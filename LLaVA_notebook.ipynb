{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "rESRfJcc5i6l",
        "outputId": "4a33f5c1-16e9-4ea2-b7ee-93d2010ee4ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n!pip install -q condacolab\\nimport condacolab\\ncondacolab.install()\\n\\n!conda create -n llava python=3.10 -y\\n!conda run -n llava pip install torch==2.0.1 torchvision==0.15.2\\n!conda run -n llava pip install transformers==4.31.0\\n!conda run -n llava pip install tokenizers==0.13.3\\n!conda run -n llava pip install numpy==1.26.0\\n!conda run -n llava pip install accelerate==0.21.0\\n\\n# Sccessfully created conda environment named llava, but the default environment is still python 3.11 without torch module, the default Colab environment\\n!python -c \"import torch, transformers; print(torch.__version__, transformers.__version__)\"\\n\\n# We have to add !conda run -n llava to excute the script in llava environment. This is a bit uncomfortable.\\n!conda run -n llava python -c \"import torch, transformers; print(torch.__version__, transformers.__version__)\"\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "\n",
        "!conda create -n llava python=3.10 -y\n",
        "!conda run -n llava pip install torch==2.0.1 torchvision==0.15.2\n",
        "!conda run -n llava pip install transformers==4.31.0\n",
        "!conda run -n llava pip install tokenizers==0.13.3\n",
        "!conda run -n llava pip install numpy==1.26.0\n",
        "!conda run -n llava pip install accelerate==0.21.0\n",
        "\n",
        "# Sccessfully created conda environment named llava, but the default environment is still python 3.11 without torch module, the default Colab environment\n",
        "!python -c \"import torch, transformers; print(torch.__version__, transformers.__version__)\"\n",
        "\n",
        "# We have to add !conda run -n llava to excute the script in llava environment. This is a bit uncomfortable.\n",
        "!conda run -n llava python -c \"import torch, transformers; print(torch.__version__, transformers.__version__)\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MgqvUKsdFkEz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.0.1 4.31.0\n"
          ]
        }
      ],
      "source": [
        "!python -c \"import torch, transformers; print(torch.__version__, transformers.__version__)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C8j7yWUt4N-C"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RmGhXE625w7R"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/venv/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
            "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
            "[2025-09-20 06:53:01,332] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/venv/lib/python3.10/site-packages/wandb/sdk/launch/builder/build.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n",
            "/opt/venv/lib/python3.10/site-packages/wandb/sdk/launch/builder/build.py:11: UserWarning: Module wandb was already imported from /opt/venv/lib/python3.10/site-packages/wandb/__init__.py, but /workspaces/LLaVA-1.2.0 is being added to sys.path\n",
            "  import pkg_resources\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class ModelArguments:\n",
        "    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n",
        "    version: Optional[str] = field(default=\"v0\")\n",
        "    freeze_backbone: bool = field(default=False)\n",
        "    tune_mm_mlp_adapter: bool = field(default=False)\n",
        "    vision_tower: Optional[str] = field(default=None) # default to None\n",
        "    mm_vision_select_layer: Optional[int] = field(default=-1)   # default to the last layer\n",
        "    pretrain_mm_mlp_adapter: Optional[str] = field(default=None)\n",
        "    mm_projector_type: Optional[str] = field(default='linear')\n",
        "    mm_use_im_start_end: bool = field(default=False)\n",
        "    mm_use_im_patch_token: bool = field(default=True)\n",
        "    mm_patch_merge_type: Optional[str] = field(default='flat')\n",
        "    mm_vision_select_feature: Optional[str] = field(default=\"patch\")\n",
        "\n",
        "@dataclass\n",
        "class DataArguments:\n",
        "    data_path: str = field(default=None,\n",
        "                           metadata={\"help\": \"Path to the training data.\"})\n",
        "    lazy_preprocess: bool = False\n",
        "    is_multimodal: bool = False\n",
        "    image_folder: Optional[str] = field(default=None)\n",
        "    image_aspect_ratio: str = 'square'\n",
        "\n",
        "@dataclass\n",
        "class TrainingArguments(transformers.TrainingArguments):\n",
        "    cache_dir: Optional[str] = field(default=None)\n",
        "    optim: str = field(default=\"adamw_torch\")\n",
        "    remove_unused_columns: bool = field(default=False)\n",
        "    freeze_mm_mlp_adapter: bool = field(default=False)\n",
        "    mpt_attn_impl: Optional[str] = field(default=\"triton\")\n",
        "    model_max_length: int = field(\n",
        "        default=512,\n",
        "        metadata={\n",
        "            \"help\":\n",
        "            \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
        "        },\n",
        "    )\n",
        "    double_quant: bool = field(\n",
        "        default=True,\n",
        "        metadata={\"help\": \"Compress the quantization statistics through double quantization.\"}\n",
        "    )\n",
        "    quant_type: str = field(\n",
        "        default=\"nf4\",\n",
        "        metadata={\"help\": \"Quantization data type to use. Should be one of `fp4` or `nf4`.\"}\n",
        "    )\n",
        "    bits: int = field(\n",
        "        default=16,\n",
        "        metadata={\"help\": \"How many bits to use.\"}\n",
        "    )\n",
        "    lora_enable: bool = False\n",
        "    lora_r: int = 64\n",
        "    lora_alpha: int = 16\n",
        "    lora_dropout: float = 0.05\n",
        "    lora_weight_path: str = \"\"\n",
        "    lora_bias: str = \"none\"\n",
        "    mm_projector_lr: Optional[float] = None\n",
        "    group_by_modality_length: bool = field(default=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTvjg0PP7za1"
      },
      "outputs": [],
      "source": [
        "from transformers import HfArgumentParser\n",
        "\n",
        "args_dict = {\n",
        "    #\"deepspeed\": \"./scripts/zero2.json\",\n",
        "    \"model_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    \"version\": \"plain\",\n",
        "    \"data_path\": \"/LLaVA-1.2.0/blip_laion_cc_sbu_1.json\",\n",
        "    \"image_folder\": \"/LLaVA-1.2.0/images/\",\n",
        "    \"vision_tower\": \"openai/clip-vit-large-patch14-336\",\n",
        "    \"mm_projector_type\": \"mlp2x_gelu\",\n",
        "    \"tune_mm_mlp_adapter\": True,\n",
        "    \"mm_vision_select_layer\": -2,\n",
        "    \"mm_use_im_start_end\": False,\n",
        "    \"mm_use_im_patch_token\": False,\n",
        "    \"bf16\": True,\n",
        "    \"output_dir\": \"./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0\",\n",
        "\n",
        "    # TrainingArguments 相当\n",
        "    \"num_train_epochs\": 1,\n",
        "    \"per_device_train_batch_size\": 1,\n",
        "    \"per_device_eval_batch_size\": 1,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"evaluation_strategy\": \"no\",\n",
        "    \"save_strategy\": \"steps\",\n",
        "    \"save_steps\": 1,\n",
        "    \"save_total_limit\": 1,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"weight_decay\": 0.0,\n",
        "    \"warmup_ratio\": 0.03,\n",
        "    \"lr_scheduler_type\": \"cosine\",\n",
        "    \"logging_steps\": 1,\n",
        "    \"tf32\": False, # switched from True for TinyLlama\n",
        "    \"model_max_length\": 2048,\n",
        "    \"gradient_checkpointing\": True,\n",
        "    \"dataloader_num_workers\": 2,\n",
        "    \"lazy_preprocess\": True,\n",
        "    \"report_to\": \"none\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dBX7g2DD-xpR",
        "outputId": "627a283d-b903-45db-a0c0-eec1b51ca0eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_args\n",
            " ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
            "data_args\n",
            " DataArguments(data_path='/content/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=False, image_folder='/content/', image_aspect_ratio='square')\n",
            "training_args\n",
            " TrainingArguments(\n",
            "_n_gpu=0,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "bits=16,\n",
            "cache_dir=None,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=2,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "double_quant=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "freeze_mm_mlp_adapter=False,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "group_by_modality_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0/runs/Sep20_06-53-03_b64d4a77cdc1,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=1,\n",
            "logging_strategy=steps,\n",
            "lora_alpha=16,\n",
            "lora_bias=none,\n",
            "lora_dropout=0.05,\n",
            "lora_enable=False,\n",
            "lora_r=64,\n",
            "lora_weight_path=,\n",
            "lr_scheduler_type=cosine,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mm_projector_lr=None,\n",
            "model_max_length=2048,\n",
            "mp_parameters=,\n",
            "mpt_attn_impl=triton,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=1,\n",
            "per_device_train_batch_size=1,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "quant_type=nf4,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=False,\n",
            "report_to=[],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=1,\n",
            "save_strategy=steps,\n",
            "save_total_limit=1,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=False,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.03,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
        "model_args, data_args, training_args = parser.parse_dict(args_dict)\n",
        "print(\"model_args\\n\", model_args)\n",
        "print(\"data_args\\n\", data_args)\n",
        "print(\"training_args\\n\", training_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oC2rbSk5QtcX"
      },
      "outputs": [],
      "source": [
        "# Model Constants\n",
        "IGNORE_INDEX = -100\n",
        "IMAGE_TOKEN_INDEX = -200\n",
        "DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
        "DEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\n",
        "DEFAULT_IM_START_TOKEN = \"<im_start>\"\n",
        "DEFAULT_IM_END_TOKEN = \"<im_end>\"\n",
        "IMAGE_PLACEHOLDER = \"<image-placeholder>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "J_5thB_J_WEi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "local_rank\n",
            " 0\n",
            "compute_dtype\n",
            " torch.bfloat16\n",
            "bnb_model_from_pretrained_args\n",
            " {}\n"
          ]
        }
      ],
      "source": [
        "local_rank = training_args.local_rank\n",
        "print(\"local_rank\\n\", local_rank)\n",
        "compute_dtype = (torch.float16 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n",
        "print(\"compute_dtype\\n\", compute_dtype)\n",
        "bnb_model_from_pretrained_args = {} # bitsandbytes\n",
        "print(\"bnb_model_from_pretrained_args\\n\", bnb_model_from_pretrained_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "t0lFK5a0INwz"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfrom transformers import CLIPModel\\n\\nnormal_clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\")\\nprint(\"normal_clip_model\\n\", normal_clip_model)\\n'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "from transformers import CLIPModel\n",
        "\n",
        "normal_clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
        "print(\"normal_clip_model\\n\", normal_clip_model)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uk84PQIHmJXO"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfrom transformers import CLIPImageProcessor\\n\\nimage_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\\nprint(\"image_processor\\n\", image_processor)\\n'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "from transformers import CLIPImageProcessor\n",
        "\n",
        "image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
        "print(\"image_processor\\n\", image_processor)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "d3WvhH4GqSht"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfrom PIL import Image\\nimport requests\\nfrom io import BytesIO\\nfrom transformers import CLIPImageProcessor\\nimport torch\\nimport torchvision.transforms as T\\nimport matplotlib.pyplot as plt\\n\\n# 画像 URL\\nurl = \"https://llava-vl.github.io/static/images/view.jpg\"\\n\\n# 画像を取得\\nresponse = requests.get(url)\\nimg = Image.open(BytesIO(response.content)).convert(\"RGB\")\\n\\n# 前処理\\nprocessor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\\nprocessed = processor(img, return_tensors=\"pt\")\\n\\n# tensor: shape (1, 3, H, W), 値は正規化済み\\npix = processed[\"pixel_values\"][0]\\n\\n# 正規化を戻す\\nmean = torch.tensor(processor.image_mean).unsqueeze(1).unsqueeze(2)\\nstd = torch.tensor(processor.image_std).unsqueeze(1).unsqueeze(2)\\npix = pix * std + mean\\n\\n# 0-1 範囲にクリップ\\npix = pix.clamp(0.0, 1.0)\\n\\n# 画像生成\\nto_pil = T.ToPILImage()\\nimg_processed = to_pil(pix)\\n\\n# ==== Colab 上で可視化 ====\\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\\n\\naxes[0].imshow(img)\\naxes[0].set_title(\"Original\")\\naxes[0].axis(\"off\")\\n\\naxes[1].imshow(img_processed)\\naxes[1].set_title(\"Processed (normalized etc.)\")\\naxes[1].axis(\"off\")\\n\\nplt.show()\\n'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from transformers import CLIPImageProcessor\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 画像 URL\n",
        "url = \"https://llava-vl.github.io/static/images/view.jpg\"\n",
        "\n",
        "# 画像を取得\n",
        "response = requests.get(url)\n",
        "img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "\n",
        "# 前処理\n",
        "processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
        "processed = processor(img, return_tensors=\"pt\")\n",
        "\n",
        "# tensor: shape (1, 3, H, W), 値は正規化済み\n",
        "pix = processed[\"pixel_values\"][0]\n",
        "\n",
        "# 正規化を戻す\n",
        "mean = torch.tensor(processor.image_mean).unsqueeze(1).unsqueeze(2)\n",
        "std = torch.tensor(processor.image_std).unsqueeze(1).unsqueeze(2)\n",
        "pix = pix * std + mean\n",
        "\n",
        "# 0-1 範囲にクリップ\n",
        "pix = pix.clamp(0.0, 1.0)\n",
        "\n",
        "# 画像生成\n",
        "to_pil = T.ToPILImage()\n",
        "img_processed = to_pil(pix)\n",
        "\n",
        "# ==== Colab 上で可視化 ====\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "axes[0].imshow(img)\n",
        "axes[0].set_title(\"Original\")\n",
        "axes[0].axis(\"off\")\n",
        "\n",
        "axes[1].imshow(img_processed)\n",
        "axes[1].set_title(\"Processed (normalized etc.)\")\n",
        "axes[1].axis(\"off\")\n",
        "\n",
        "plt.show()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sWLLumgBJbfh"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfrom transformers import CLIPVisionModel\\n\\nclip_vision_tower = CLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14-336\")\\nprint(\"clip_vision_tower\\n\", clip_vision_tower)\\n'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "from transformers import CLIPVisionModel\n",
        "\n",
        "clip_vision_tower = CLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
        "print(\"clip_vision_tower\\n\", clip_vision_tower)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_w9_49H8Cza2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nconfig_clip_vision_tower = clip_vision_tower.config\\nprint(\"config_clip_vision_tower\\n\", config_clip_vision_tower)\\n'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "config_clip_vision_tower = clip_vision_tower.config\n",
        "print(\"config_clip_vision_tower\\n\", config_clip_vision_tower)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jFp9VBciHEhH"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPVisionModel, CLIPImageProcessor, CLIPVisionConfig\n",
        "import torch.nn as nn\n",
        "# __init__\n",
        "# load_model\n",
        "\n",
        "# result = CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n",
        "class CLIPVisionTower(nn.Module):\n",
        "    def __init__(self, vision_tower, args, delay_load=False):\n",
        "        # result = CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n",
        "        print(\"current file path\", \"llava/llava/model/multimodal_encoder/clip_encoder.py\")\n",
        "        print(\"def CLIPVisionTower.__init__(self, vision_tower, args, delay_load=False)\")\n",
        "        print(\"self\\n\", type(self))\n",
        "        print(\"vision_tower\\n\", vision_tower) # openai/clip-vit-large-patch14-336\n",
        "        print(\"args\\n\", args) # ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
        "        print(\"delay_load\\n\", delay_load) # False\n",
        "        super().__init__()\n",
        "\n",
        "        self.is_loaded = False\n",
        "\n",
        "        print(\"self.is_loaded\\n\", self.is_loaded) # False\n",
        "\n",
        "        self.vision_tower_name = vision_tower\n",
        "        print(\"self.vision_tower_name\\n\", self.vision_tower_name) # openai/clip-vit-large-patch14-336\n",
        "        self.select_layer = args.mm_vision_select_layer\n",
        "        print(\"self.select_layer\\n\", self.select_layer) # -2\n",
        "        self.select_feature = getattr(args, 'mm_vision_select_feature', 'patch')\n",
        "        print(\"self.select_feature\\n\", self.select_feature) # patch\n",
        "\n",
        "        print(f\"[COND] delay_load={delay_load}\")\n",
        "        if not delay_load:\n",
        "            # 【ENTER】\n",
        "            print(\"【ENTER】if not delay_load:\")\n",
        "            self.load_model()\n",
        "        elif getattr(args, 'unfreeze_mm_vision_tower', False):\n",
        "            print(\"【ENTER】elif getattr(args, 'unfreeze_mm_vision_tower', False):\")\n",
        "            self.load_model()\n",
        "            print(\"【EXIT】elif getattr(args, 'unfreeze_mm_vision_tower', False):\")\n",
        "        else:\n",
        "            print(\"【ENTER】else of if not delay_load/elif getattr(args, 'unfreeze_mm_vision_tower', False):\")\n",
        "            self.cfg_only = CLIPVisionConfig.from_pretrained(self.vision_tower_name)\n",
        "            print(\"self.cfg_only\\n\", self.cfg_only)\n",
        "            print(\"【EXIT】else of if not delay_load/elif getattr(args, 'unfreeze_mm_vision_tower', False):\")\n",
        "\n",
        "\n",
        "    def load_model(self):\n",
        "\n",
        "        print(\"current file path\", \"llava/llava/model/multimodal_encoder/clip_encoder.py\")\n",
        "        print(\"def CLIPVisionTower.load_model(self)\")\n",
        "        print(\"self\\n\", type(self))\n",
        "        print(\"self.vision_tower_name\\n\", self.vision_tower_name) # openai/clip-vit-large-patch14-336\n",
        "        self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name)\n",
        "        print(\"self.image_processor\\n\", self.image_processor)\n",
        "        \"\"\"\n",
        "        CLIPImageProcessor {\n",
        "        \"crop_size\": {\n",
        "            \"height\": 336,\n",
        "            \"width\": 336\n",
        "        },\n",
        "        \"do_center_crop\": true,\n",
        "        \"do_convert_rgb\": true,\n",
        "        \"do_normalize\": true,\n",
        "        \"do_rescale\": true,\n",
        "        \"do_resize\": true,\n",
        "        \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n",
        "        \"image_mean\": [\n",
        "            0.48145466,\n",
        "            0.4578275,\n",
        "            0.40821073\n",
        "        ],\n",
        "        \"image_processor_type\": \"CLIPImageProcessor\",\n",
        "        \"image_std\": [\n",
        "            0.26862954,\n",
        "            0.26130258,\n",
        "            0.27577711\n",
        "        ],\n",
        "        \"resample\": 3,\n",
        "        \"rescale_factor\": 0.00392156862745098,\n",
        "        \"size\": {\n",
        "            \"shortest_edge\": 336\n",
        "        }\n",
        "        }\n",
        "        \"\"\"\n",
        "        self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name)\n",
        "        print(\"self.vision_tower\\n\", self.vision_tower)\n",
        "        \"\"\"\n",
        "        CLIPVisionModel(\n",
        "        (vision_model): CLIPVisionTransformer(\n",
        "            (embeddings): CLIPVisionEmbeddings(\n",
        "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
        "            (position_embedding): Embedding(577, 1024)\n",
        "            )\n",
        "            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
        "            (encoder): CLIPEncoder(\n",
        "            (layers): ModuleList(\n",
        "                (0-23): 24 x CLIPEncoderLayer(\n",
        "                (self_attn): CLIPAttention(\n",
        "                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "                )\n",
        "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
        "                (mlp): CLIPMLP(\n",
        "                    (activation_fn): QuickGELUActivation()\n",
        "                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
        "                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
        "                )\n",
        "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
        "                )\n",
        "            )\n",
        "            )\n",
        "            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
        "        )\n",
        "        )\n",
        "        \"\"\"\n",
        "        self.vision_tower.requires_grad_(False)\n",
        "\n",
        "        self.is_loaded = True\n",
        "        print(\"self.is_loaded\\n\", self.is_loaded) # True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5mz6UWifHfTq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def build_vision_tower(vision_tower_cfg, **kwargs):\n",
        "    # vision_tower = build_vision_tower(model_args)\n",
        "    print(\"current file path\", \"llava/llava/model/multimodal_encoder/builder.py\")\n",
        "    print(\"def build_vision_tower(vision_tower_cfg, **kwargs)\")\n",
        "    print(\"vision_tower_cfg\\n\", vision_tower_cfg) # ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
        "    print(\"kwargs\\n\", kwargs) # {}\n",
        "    vision_tower = getattr(vision_tower_cfg, 'mm_vision_tower', getattr(vision_tower_cfg, 'vision_tower', None))\n",
        "    print(\"vision_tower from vision_tower_cfg\\n\", vision_tower) # openai/clip-vit-large-patch14-336\n",
        "    # ローカルに存在しない場合はFalse。存在する場合の例: /ubuntu/home/user/model/openai/clip-vit-large-patch14-336\n",
        "    is_absolute_path_exists = os.path.exists(vision_tower)\n",
        "    print(\"is_absolute_path_exists\\n\", is_absolute_path_exists) # False\n",
        "    print(f\"[COND] is_absolute_path_exists={is_absolute_path_exists} vision_tower={vision_tower}\") # is_absolute_path_exists=False vision_tower=openai/clip-vit-large-patch14-336\n",
        "    if is_absolute_path_exists or vision_tower.startswith(\"openai\") or vision_tower.startswith(\"laion\") or \"ShareGPT4V\" in vision_tower:\n",
        "        # 【ENTER】\n",
        "        print(\"【ENTER】if is_absolute_path_exists or vision_tower.startswith('openai') or vision_tower.startswith('laion') or 'ShareGPT4V' in vision_tower:\")\n",
        "        result = CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n",
        "        print(\"result (return)\\n\", result) # CLIPVisionTowerクラスのselfに登録されたモジュール一覧を出力する\n",
        "        print(\"【EXIT】if is_absolute_path_exists or vision_tower.startswith('openai') or vision_tower.startswith('laion') or 'ShareGPT4V' in vision_tower:\")\n",
        "        return result\n",
        "\n",
        "    print(\"print(risk): print(vision_tower) disabled for safety\")\n",
        "    raise ValueError(f'Unknown vision tower: {vision_tower}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sYl9yfNWkAzW"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nbuild_vision_tower(model_args)\\n'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "build_vision_tower(model_args)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8o16cqWBL_4Q"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def build_vision_projector(config, delay_load=False, **kwargs):\n",
        "\n",
        "    print(\"current file path\", \"llava/llava/model/multimodal_projector/builder.py\")\n",
        "    print(\"def build_vision_projector(config, delay_load=False, **kwargs)\")\n",
        "    print(\"config\\n\", config)\n",
        "    \"\"\"\n",
        "    config\n",
        "    LlavaConfig {\n",
        "    \"_name_or_path\": \"lmsys/vicuna-7b-v1.5\",\n",
        "    \"architectures\": [\n",
        "        \"LlamaForCausalLM\"\n",
        "    ],\n",
        "    \"bos_token_id\": 1,\n",
        "    \"eos_token_id\": 2,\n",
        "    \"hidden_act\": \"silu\",\n",
        "    \"hidden_size\": 4096,\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"intermediate_size\": 11008,\n",
        "    \"max_position_embeddings\": 4096,\n",
        "    \"mm_hidden_size\": 1024,\n",
        "    \"mm_patch_merge_type\": \"flat\",\n",
        "    \"mm_projector_type\": \"mlp2x_gelu\",\n",
        "    \"mm_vision_select_feature\": \"patch\",\n",
        "    \"mm_vision_select_layer\": -2,\n",
        "    \"mm_vision_tower\": \"openai/clip-vit-large-patch14-336\",\n",
        "    \"model_type\": \"llava_llama\",\n",
        "    \"num_attention_heads\": 32,\n",
        "    \"num_hidden_layers\": 32,\n",
        "    \"num_key_value_heads\": 32,\n",
        "    \"pad_token_id\": 0,\n",
        "    \"pretraining_tp\": 1,\n",
        "    \"rms_norm_eps\": 1e-05,\n",
        "    \"rope_scaling\": null,\n",
        "    \"tie_word_embeddings\": false,\n",
        "    \"torch_dtype\": \"float16\",\n",
        "    \"transformers_version\": \"4.31.0\",\n",
        "    \"use_cache\": false,\n",
        "    \"use_mm_proj\": true,\n",
        "    \"vocab_size\": 32000\n",
        "    }\n",
        "    \"\"\"\n",
        "    print(\"delay_load\\n\", delay_load) # False\n",
        "    print(\"kwargs\\n\", kwargs) # {}\n",
        "    projector_type = getattr(config, 'mm_projector_type', 'linear')\n",
        "    print(\"projector_type from config\\n\", projector_type) # mlp2x_gelu\n",
        "\n",
        "    print(\"【COND】 projector_type\\n\", projector_type) # mlp2x_gelu\n",
        "    if projector_type == 'linear':\n",
        "      pass\n",
        "\n",
        "    mlp_gelu_match = re.match(r'^mlp(\\d+)x_gelu$', projector_type)\n",
        "    print(\"【COND】mlp_gelu_match\\n\", mlp_gelu_match)\n",
        "    if mlp_gelu_match:\n",
        "        #【ENTER】if mlp_gelu_match:\n",
        "        print(\"【ENTER】if mlp_gelu_match:\")\n",
        "        mlp_depth = int(mlp_gelu_match.group(1))\n",
        "        print(\"mlp_depth from mlp_gelu_match.group(1)\\n\", mlp_depth)\n",
        "        modules = [nn.Linear(config.mm_hidden_size, config.hidden_size)]\n",
        "        print(\"modules after first Linear\\n\", modules)\n",
        "        for _ in range(1, mlp_depth):\n",
        "            modules.append(nn.GELU())\n",
        "            modules.append(nn.Linear(config.hidden_size, config.hidden_size))\n",
        "        print(\"modules before Sequential\\n\", modules)\n",
        "        result = nn.Sequential(*modules) # * はリストをアンパックして引数に展開する\n",
        "        print(\"result (return)\\n\", result)\n",
        "        \"\"\"\n",
        "        Sequential(\n",
        "        (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
        "        (1): GELU(approximate='none')\n",
        "        (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
        "        )\n",
        "        \"\"\"\n",
        "        print(\"【EXIT】if mlp_gelu_match:\")\n",
        "        return result\n",
        "\n",
        "    print(\"【COND】projector_type\\n\", projector_type)\n",
        "    if projector_type == 'identity':\n",
        "      pass\n",
        "\n",
        "    print(\"print(risk): print(projector_type) disabled for safety\")\n",
        "    raise ValueError(f'Unknown projector type: {projector_type}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "VjkgxnbKFGan"
      },
      "outputs": [],
      "source": [
        "# LlavaMetaModel\n",
        "# __init__\n",
        "# get_vision_tower\n",
        "# initialize_vision_modules\n",
        "# unpad_image\n",
        "\n",
        "class LlavaMetaModel:\n",
        "\n",
        "    def __init__(self, config):\n",
        "\n",
        "        print(\"current file path\", \"llava/model/llava_arch.py\")\n",
        "        print(\"LlavaMetaModel.__init__(self, config)\")\n",
        "        print(\"config\\n\", config)\n",
        "        # LlamaModelの__init_を呼び出す\n",
        "        super(LlavaMetaModel, self).__init__(config)\n",
        "\n",
        "        print(f\"[COND] mm_vision_tower={hasattr(config, 'mm_vision_tower')}\")\n",
        "        if hasattr(config, \"mm_vision_tower\"):\n",
        "            print(\"【ENTER】if hasattr(config, 'mm_vision_tower'):\")\n",
        "            self.vision_tower = build_vision_tower(config, delay_load=True)\n",
        "            print(\"self.vision_tower\\n\", self.vision_tower)\n",
        "            self.mm_projector = build_vision_projector(config)\n",
        "            print(\"self.mm_projector\\n\", self.mm_projector)\n",
        "\n",
        "            print(\"self.config.mm_patch_merge_type\\n\", self.config.mm_patch_merge_type)\n",
        "            print(f\"[COND] unpad_in_mm_patch_merge_type={'unpad' in getattr(config, 'mm_patch_merge_type', '')}\")\n",
        "            if 'unpad' in getattr(config, 'mm_patch_merge_type', ''):\n",
        "              pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "T_YguuBGN-3R"
      },
      "outputs": [],
      "source": [
        "from transformers import LlamaConfig, LlamaModel\n",
        "\n",
        "class LlavaConfig(LlamaConfig):\n",
        "    model_type = \"llava_llama\"\n",
        "\n",
        "\n",
        "class LlavaLlamaModel(LlavaMetaModel, LlamaModel):\n",
        "    config_class = LlavaConfig\n",
        "\n",
        "    def __init__(self, config: LlamaConfig):\n",
        "\n",
        "        print(\"current file path\", \"llava/llava/model/language_model/llava_llama.py\")\n",
        "        print(\"def LlavaLlamaModel.__init__(self, config: LlamaConfig)\")\n",
        "        print(\"self\\n\", type(self))\n",
        "        print(\"config\\n\", config)\n",
        "        super(LlavaLlamaModel, self).__init__(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "V9uRX7reN0gI"
      },
      "outputs": [],
      "source": [
        "# LlavaMetaForCausalLM\n",
        "# get_vision_tower\n",
        "# encode_images\n",
        "# prepare_inputs_labels_for_multimodal\n",
        "# initialize_vision_tokenizer\n",
        "\n",
        "class LlavaMetaForCausalLM:\n",
        "\n",
        "    def get_vision_tower(self):\n",
        "        print(\"current file path\", \"llava/model/llava_arch.py\")\n",
        "        print(\"class LlavaMetaForCausalLM(ABC).get_vision_tower(self)\")\n",
        "        result = self.get_model().get_vision_tower()\n",
        "        print(\"LlavaMetaForCausalLM(ABC).get_vision_tower(self) result (return)\\n\", result)\n",
        "        \"\"\"\n",
        "        CLIPVisionTower(\n",
        "        (vision_tower): CLIPVisionModel(\n",
        "            (vision_model): CLIPVisionTransformer(\n",
        "            (embeddings): CLIPVisionEmbeddings(\n",
        "                (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
        "                (position_embedding): Embedding(577, 1024)\n",
        "            )\n",
        "            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
        "            (encoder): CLIPEncoder(\n",
        "                (layers): ModuleList(\n",
        "                (0-23): 24 x CLIPEncoderLayer(\n",
        "                    (self_attn): CLIPAttention(\n",
        "                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "                    )\n",
        "                    (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
        "                    (mlp): CLIPMLP(\n",
        "                    (activation_fn): QuickGELUActivation()\n",
        "                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
        "                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
        "                    )\n",
        "                    (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
        "                )\n",
        "                )\n",
        "            )\n",
        "            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
        "            )\n",
        "        )\n",
        "        )\n",
        "        \"\"\"\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "t4yAIwyeOEWE"
      },
      "outputs": [],
      "source": [
        "from typing import List, Optional, Tuple, Union\n",
        "from transformers.generation.utils import GenerateOutput\n",
        "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
        "from transformers import LlamaForCausalLM\n",
        "\n",
        "class LlavaLlamaForCausalLM(LlamaForCausalLM, LlavaMetaForCausalLM):\n",
        "    config_class = LlavaConfig\n",
        "\n",
        "    def __init__(self, config):\n",
        "\n",
        "        print(\"current file path\", \"llava/llava/model/language_model/llava_llama.py\")\n",
        "        print(\"def LlavaLlamaForCausalLM.__init__(self, config)\")\n",
        "        print(\"self\\n\", type(self))\n",
        "        # config は https://huggingface.co/lmsys/vicuna-7b-v1.5/blob/main/config.json\n",
        "        print(\"config\\n\", config)\n",
        "        super(LlamaForCausalLM, self).__init__(config)\n",
        "        self.model = LlavaLlamaModel(config)\n",
        "        # LlavaLlamaModelの初期化あと、LlavaMetaModelの初期化も呼ばれる。\n",
        "        self.pretraining_tp = config.pretraining_tp\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "        print(\"self.model\\n\", self.model)\n",
        "        \"\"\"\n",
        "        self.model\n",
        "        LlavaLlamaModel(\n",
        "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
        "        (layers): ModuleList(\n",
        "            (0-31): 32 x LlamaDecoderLayer(\n",
        "            (self_attn): LlamaAttention(\n",
        "                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
        "                (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
        "                (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
        "                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
        "                (rotary_emb): LlamaRotaryEmbedding()\n",
        "            )\n",
        "            (mlp): LlamaMLP(\n",
        "                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
        "                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
        "                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
        "                (act_fn): SiLUActivation()\n",
        "            )\n",
        "            (input_layernorm): LlamaRMSNorm()\n",
        "            (post_attention_layernorm): LlamaRMSNorm()\n",
        "            )\n",
        "        )\n",
        "        (norm): LlamaRMSNorm()\n",
        "        )\n",
        "        \"\"\"\n",
        "        print(\"self.pretraining_tp\\n\", self.pretraining_tp) # 1\n",
        "        print(\"self.vocab_size\\n\", self.vocab_size) # 32_000\n",
        "        print(\"self.lm_head\\n\", self.lm_head) # Linear(in_features=4096, out_features=32000, bias=False)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "2Wf2jZjizGZo",
        "outputId": "ef0ee186-8461-4560-982d-9bd4a0c530b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfrom transformers import AutoConfig\\n\\n# 公式 LLaMA-2-7B の config をロード\\nllama_config = AutoConfig.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\\n\\nprint(llama_config)\\n'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "from transformers import AutoConfig\n",
        "\n",
        "# 公式 LLaMA-2-7B の config をロード\n",
        "llama_config = AutoConfig.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "\n",
        "print(llama_config)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "RlMP4qUg0QtV",
        "outputId": "12f73ecd-9922-4d31-a0cf-a0e8bc5d88fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfrom transformers import AutoConfig\\n\\n# まず config.json をロードして Config クラスを自動判別\\nconfig = AutoConfig.from_pretrained(\\n    model_args.model_name_or_path,\\n    cache_dir=training_args.cache_dir\\n)\\n\\nprint(\"model_args.model_name_or_path\\n\", model_args.model_name_or_path)\\nprint(\"training_args.cache_dir\\n\", training_args.cache_dir)\\nprint(\"\")\\nprint(\"Loaded config:\\n\", config)\\n'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "from transformers import AutoConfig\n",
        "\n",
        "# まず config.json をロードして Config クラスを自動判別\n",
        "config = AutoConfig.from_pretrained(\n",
        "    model_args.model_name_or_path,\n",
        "    cache_dir=training_args.cache_dir\n",
        ")\n",
        "\n",
        "print(\"model_args.model_name_or_path\\n\", model_args.model_name_or_path)\n",
        "print(\"training_args.cache_dir\\n\", training_args.cache_dir)\n",
        "print(\"\")\n",
        "print(\"Loaded config:\\n\", config)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNGv8pE6SM9L",
        "outputId": "dfcf4d9a-55c6-492b-e92d-56146f36e5f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<function LlavaLlamaModel.__init__ at 0xffff31ff0940>\n"
          ]
        }
      ],
      "source": [
        "import inspect\n",
        "print(inspect.getattr_static(LlavaLlamaModel, \"__init__\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZSUn4slSEwV",
        "outputId": "84dde4c4-b051-4cea-ec3d-88ae67db5fd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MRO for LlavaLlamaModel:\n",
            "\n",
            " 0: __main__.LlavaLlamaModel\n",
            " 1: __main__.LlavaMetaModel\n",
            " 2: transformers.models.llama.modeling_llama.LlamaModel\n",
            " 3: transformers.models.llama.modeling_llama.LlamaPreTrainedModel\n",
            " 4: transformers.modeling_utils.PreTrainedModel\n",
            " 5: torch.nn.modules.module.Module\n",
            " 6: transformers.modeling_utils.ModuleUtilsMixin\n",
            " 7: transformers.generation.utils.GenerationMixin\n",
            " 8: transformers.utils.hub.PushToHubMixin\n",
            " 9: builtins.object\n"
          ]
        }
      ],
      "source": [
        "def print_mro(cls):\n",
        "    print(f\"MRO for {cls.__name__}:\\n\")\n",
        "    for i, c in enumerate(cls.mro()):\n",
        "        print(f\"{i:2d}: {c.__module__}.{c.__name__}\")\n",
        "\n",
        "print_mro(LlavaLlamaModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwLz_ubtVlti",
        "outputId": "680f292f-9756-45d4-cb80-af9692475a34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MRO for LlavaMetaModel:\n",
            "\n",
            " 0: __main__.LlavaMetaModel\n",
            " 1: builtins.object\n"
          ]
        }
      ],
      "source": [
        "print_mro(LlavaMetaModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhWiOc7BvjcV",
        "outputId": "52855dab-04f8-4ca3-81d1-fb6c7c74a379"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MRO for LlavaLlamaForCausalLM:\n",
            "\n",
            " 0: __main__.LlavaLlamaForCausalLM\n",
            " 1: transformers.models.llama.modeling_llama.LlamaForCausalLM\n",
            " 2: transformers.models.llama.modeling_llama.LlamaPreTrainedModel\n",
            " 3: transformers.modeling_utils.PreTrainedModel\n",
            " 4: torch.nn.modules.module.Module\n",
            " 5: transformers.modeling_utils.ModuleUtilsMixin\n",
            " 6: transformers.generation.utils.GenerationMixin\n",
            " 7: transformers.utils.hub.PushToHubMixin\n",
            " 8: __main__.LlavaMetaForCausalLM\n",
            " 9: builtins.object\n"
          ]
        }
      ],
      "source": [
        "print_mro(LlavaLlamaForCausalLM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "UJE07TY6Jb5K",
        "outputId": "091b04d2-adbc-49c3-ef2a-50181c53f793"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b8615fe9db94d3cbea1761a5c35414c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91887954d0b9418bb01020b831490c9f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current file path llava/llava/model/language_model/llava_llama.py\n",
            "def LlavaLlamaForCausalLM.__init__(self, config)\n",
            "self\n",
            " <class '__main__.LlavaLlamaForCausalLM'>\n",
            "config\n",
            " LlavaConfig {\n",
            "  \"_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 5632,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"llava_llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 22,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.31.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "current file path llava/llava/model/language_model/llava_llama.py\n",
            "def LlavaLlamaModel.__init__(self, config: LlamaConfig)\n",
            "self\n",
            " <class '__main__.LlavaLlamaModel'>\n",
            "config\n",
            " LlavaConfig {\n",
            "  \"_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 5632,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"llava_llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 22,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.31.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "current file path llava/model/llava_arch.py\n",
            "LlavaMetaModel.__init__(self, config)\n",
            "config\n",
            " LlavaConfig {\n",
            "  \"_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 5632,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"llava_llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 22,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.31.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[COND] mm_vision_tower=False\n",
            "self.model\n",
            " LlavaLlamaModel(\n",
            "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
            "  (layers): ModuleList(\n",
            "    (0-21): 22 x LlamaDecoderLayer(\n",
            "      (self_attn): LlamaAttention(\n",
            "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
            "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
            "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        (rotary_emb): LlamaRotaryEmbedding()\n",
            "      )\n",
            "      (mlp): LlamaMLP(\n",
            "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
            "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
            "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
            "        (act_fn): SiLUActivation()\n",
            "      )\n",
            "      (input_layernorm): LlamaRMSNorm()\n",
            "      (post_attention_layernorm): LlamaRMSNorm()\n",
            "    )\n",
            "  )\n",
            "  (norm): LlamaRMSNorm()\n",
            ")\n",
            "self.pretraining_tp\n",
            " 1\n",
            "self.vocab_size\n",
            " 32000\n",
            "self.lm_head\n",
            " Linear(in_features=2048, out_features=32000, bias=False)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0 and are newly initialized: ['model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60a0931ffb594c2fa58cd2d67a914670",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = LlavaLlamaForCausalLM.from_pretrained(\n",
        "    model_args.model_name_or_path,\n",
        "    cache_dir=training_args.cache_dir,\n",
        "    **bnb_model_from_pretrained_args\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "wodQZ9rWSyZN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model\n",
            " LlavaLlamaForCausalLM(\n",
            "  (model): LlavaLlamaModel(\n",
            "    (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-21): 22 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
            "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
            "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
            "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm()\n",
            "        (post_attention_layernorm): LlamaRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(\"model\\n\", model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "z5c1aEs4gf5L"
      },
      "outputs": [],
      "source": [
        "model.enable_input_require_grads()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "l9az2bPNccDC"
      },
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "from typing import List\n",
        "from enum import auto, Enum\n",
        "\n",
        "class SeparatorStyle(Enum):\n",
        "    \"\"\"Different separator style.\"\"\"\n",
        "    SINGLE = auto()\n",
        "    TWO = auto()\n",
        "    MPT = auto()\n",
        "    PLAIN = auto()\n",
        "    LLAMA_2 = auto()\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class Conversation:\n",
        "    \"\"\"A class that keeps all conversation history.\"\"\"\n",
        "    system: str\n",
        "    roles: List[str]\n",
        "    messages: List[List[str]]\n",
        "    offset: int\n",
        "    sep_style: SeparatorStyle = SeparatorStyle.SINGLE\n",
        "    sep: str = \"###\"\n",
        "    sep2: str = None\n",
        "    version: str = \"Unknown\"\n",
        "\n",
        "    skip_next: bool = False\n",
        "\n",
        "\n",
        "conv_llava_plain = Conversation(\n",
        "    system=\"\",\n",
        "    roles=(\"\", \"\"),\n",
        "    messages=(\n",
        "    ),\n",
        "    offset=0,\n",
        "    sep_style=SeparatorStyle.PLAIN,\n",
        "    sep=\"\\n\",\n",
        ")\n",
        "\n",
        "\n",
        "conv_templates = {\n",
        "    \"plain\": conv_llava_plain,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSs2_yNVOpOD"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "print(inspect.getattr_static(LlamaForCausalLM, \"from_pretrained\"))\n",
        "print(inspect.getattr_static(LlamaForCausalLM, \"enable_input_require_grads\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od0OYu7yBesv"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_args.model_name_or_path,\n",
        "    cache_dir=training_args.cache_dir,\n",
        "    model_max_length=training_args.model_max_length,\n",
        "    padding_side=\"right\",\n",
        "    use_fast=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M71wf9CYD_Y7"
      },
      "outputs": [],
      "source": [
        "print(\"pad_token:\", tokenizer.pad_token)\n",
        "print(\"pad_token_id:\", tokenizer.pad_token_id)\n",
        "print(\"unk_token:\", tokenizer.unk_token)\n",
        "print(\"unk_token_id:\", tokenizer.unk_token_id)\n",
        "print(\"tokenizer\\n\", tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TrZtJFODFWz"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.unk_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vbf0h2NSDoUG"
      },
      "outputs": [],
      "source": [
        "print(\"pad_token:\", tokenizer.pad_token)\n",
        "print(\"pad_token_id:\", tokenizer.pad_token_id)\n",
        "print(\"unk_token:\", tokenizer.unk_token)\n",
        "print(\"unk_token_id:\", tokenizer.unk_token_id)\n",
        "print(\"tokenizer\\n\", tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGtPYpfKDOKW"
      },
      "outputs": [],
      "source": [
        "default_conversation = conv_templates[model_args.version]\n",
        "print(\"default_conversation\\n\", default_conversation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csi6Xe_-G0_G"
      },
      "outputs": [],
      "source": [
        "print(\"model_args.vision_tower\\n\", model_args.vision_tower)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpri2EiEIatU"
      },
      "outputs": [],
      "source": [
        "def get_model(self):\n",
        "\n",
        "    print(\"current file path\", \"llava/llava/model/language_model/llava_llama.py\")\n",
        "    print(\"def LlavaLlamaForCausalLM.get_model(self)\")\n",
        "    print(\"self\\n\", type(self))\n",
        "    print(\"self.model (return)\\n\", self.model)\n",
        "    return self.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPVqYUnjJHoF"
      },
      "outputs": [],
      "source": [
        "LlavaLlamaForCausalLM.get_model = get_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqqp22MDJSJ-"
      },
      "outputs": [],
      "source": [
        "initial_model = model.get_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c_UQLwdPhjd"
      },
      "outputs": [],
      "source": [
        "def config(self):\n",
        "\n",
        "    print(\"current file path\", \"llava/llava/model/multimodal_encoder/clip_encoder.py\")\n",
        "    print(\"def CLIPVisionTower.config(self)\")\n",
        "    print(\"self\\n\", type(self))\n",
        "    print(\"self.is_loaded\\n\", self.is_loaded) # True\n",
        "    print(f\"[COND] is_loaded={self.is_loaded}\")\n",
        "    if self.is_loaded:\n",
        "        # 【ENTER】\n",
        "        print(\"【ENTER】if self.is_loaded:\")\n",
        "        result = self.vision_tower.config\n",
        "        print(\"result (return)\\n\", type(result))\n",
        "        print(\"【EXIT】if self.is_loaded:\")\n",
        "    else:\n",
        "      pass\n",
        "    print(\"result (return)\\n\", result)\n",
        "    \"\"\"\n",
        "    CLIPVisionConfig {\n",
        "    \"_name_or_path\": \"openai/clip-vit-large-patch14-336\",\n",
        "    \"attention_dropout\": 0.0,\n",
        "    \"dropout\": 0.0,\n",
        "    \"hidden_act\": \"quick_gelu\",\n",
        "    \"hidden_size\": 1024,\n",
        "    \"image_size\": 336,\n",
        "    \"initializer_factor\": 1.0,\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"intermediate_size\": 4096,\n",
        "    \"layer_norm_eps\": 1e-05,\n",
        "    \"model_type\": \"clip_vision_model\",\n",
        "    \"num_attention_heads\": 16,\n",
        "    \"num_channels\": 3,\n",
        "    \"num_hidden_layers\": 24,\n",
        "    \"patch_size\": 14,\n",
        "    \"projection_dim\": 768,\n",
        "    \"transformers_version\": \"4.31.0\"\n",
        "    }\n",
        "    \"\"\"\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YLi3g3NQMIK"
      },
      "outputs": [],
      "source": [
        "def hidden_size(self):\n",
        "\n",
        "    print(\"current file path\", \"llava/llava/model/multimodal_encoder/clip_encoder.py\")\n",
        "    print(\"def CLIPVisionTower.hidden_size(self)\")\n",
        "    print(\"self\\n\", type(self))\n",
        "    result = self.config.hidden_size\n",
        "    print(\"result (return), self.config.hidden_size\\n\", result) # 1024\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG8ocYNiPpGB"
      },
      "outputs": [],
      "source": [
        "CLIPVisionTower.config = property(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGom3d6lP40g"
      },
      "outputs": [],
      "source": [
        "CLIPVisionTower.hidden_size = property(hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrBNMbAm39zA"
      },
      "outputs": [],
      "source": [
        "def initialize_vision_modules(self, model_args, fsdp=None):\n",
        "\n",
        "  print(\"current file path\", \"llava/model/llava_arch.py\")\n",
        "  print(\"def initialize_vision_modules(self, model_args, fsdp=None)\")\n",
        "  print(\"model_args\\n\", model_args) #  ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
        "  print(\"fsdp\\n\", fsdp) # []\n",
        "  vision_tower = model_args.vision_tower\n",
        "  print(\"vision_tower from model_args\\n\", vision_tower) # openai/clip-vit-large-patch14-336\n",
        "  mm_vision_select_layer = model_args.mm_vision_select_layer\n",
        "  print(\"mm_vision_select_layer from model_args\\n\", mm_vision_select_layer) # -2\n",
        "  mm_vision_select_feature = model_args.mm_vision_select_feature\n",
        "  print(\"mm_vision_select_feature from model_args\\n\", mm_vision_select_feature) # patch\n",
        "  pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n",
        "  print(\"pretrain_mm_mlp_adapter from model_args\\n\", pretrain_mm_mlp_adapter) # None\n",
        "  mm_patch_merge_type = model_args.mm_patch_merge_type\n",
        "  # 下記はself.config.mm_vision_towerに関するもの。self.vision_towerは依然としてNone\n",
        "  self.config.mm_vision_tower = vision_tower\n",
        "  print(\"self.config.mm_vision_tower\\n\", self.config.mm_vision_tower) # None\n",
        "\n",
        "  print(\"[COND] self.get_vision_tower()\\n\", self.get_vision_tower()) # None\n",
        "  print(f\"[COND] get_vision_tower_is_None={self.get_vision_tower() is None}\")\n",
        "  if self.get_vision_tower() is None:\n",
        "      #【ENTER】self.vision_tower, self.get_vision_towerはNoneなのでこの分岐に入る。\n",
        "      print(\"【ENTER】if self.get_vision_tower() is None:\")\n",
        "      print(\"[ENTER] self.get_vision_tower() is None\")\n",
        "      # build_vision_tower(model_args) はちょっと奥の依存関係が深い\n",
        "      vision_tower = build_vision_tower(model_args)\n",
        "      print(\"vision_tower after build_vision_tower\\n\", vision_tower)\n",
        "      \"\"\"\n",
        "      CLIPVisionTower(\n",
        "      (vision_tower): CLIPVisionModel(\n",
        "      (vision_model): CLIPVisionTransformer(\n",
        "          (embeddings): CLIPVisionEmbeddings(\n",
        "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
        "          (position_embedding): Embedding(577, 1024)\n",
        "          )\n",
        "          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
        "          (encoder): CLIPEncoder(\n",
        "          (layers): ModuleList(\n",
        "              (0-23): 24 x CLIPEncoderLayer(\n",
        "              (self_attn): CLIPAttention(\n",
        "                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "              )\n",
        "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
        "              (mlp): CLIPMLP(\n",
        "                  (activation_fn): QuickGELUActivation()\n",
        "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
        "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
        "              )\n",
        "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
        "              )\n",
        "          )\n",
        "          )\n",
        "          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
        "      )\n",
        "      )\n",
        "      )\n",
        "      \"\"\"\n",
        "      # 分散学習(FSDP)を使うかどうか. 今回は [] 空のリストとなるので、Noneではないが、len(fsdp) == 0\n",
        "      print(\"[COND] fsdp\\n\", fsdp) # []\n",
        "      print(f\"[COND] fsdp_is_not_None={fsdp is not None} len_fsdp={len(fsdp) if fsdp is not None else 'N/A'}\") # fsdp_is_not_None=True len_fsdp=0\n",
        "      if fsdp is not None and len(fsdp) > 0:\n",
        "        pass\n",
        "      else:\n",
        "          # 【ENTER】else of if fsdp is not None and len(fsdp) > 0:\n",
        "          print(\"[COND] else_fsdp_is_not_None_and_len_fsdp_gt_0=True\")\n",
        "          print(\"【ENTER】else of if fsdp is not None and len(fsdp) > 0:\")\n",
        "          self.vision_tower = vision_tower\n",
        "          print(\"self.vision_tower\\n\", self.vision_tower)\n",
        "          \"\"\"\n",
        "          CLIPVisionTower(\n",
        "          (vision_tower): CLIPVisionModel(\n",
        "              (vision_model): CLIPVisionTransformer(\n",
        "              (embeddings): CLIPVisionEmbeddings(\n",
        "                  (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
        "                  (position_embedding): Embedding(577, 1024)\n",
        "              )\n",
        "              (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
        "              (encoder): CLIPEncoder(\n",
        "                  (layers): ModuleList(\n",
        "                  (0-23): 24 x CLIPEncoderLayer(\n",
        "                      (self_attn): CLIPAttention(\n",
        "                      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "                      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "                      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "                      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "                      )\n",
        "                      (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
        "                      (mlp): CLIPMLP(\n",
        "                      (activation_fn): QuickGELUActivation()\n",
        "                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
        "                      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
        "                      )\n",
        "                      (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
        "                  )\n",
        "                  )\n",
        "              )\n",
        "              (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
        "              )\n",
        "          )\n",
        "          )\n",
        "          \"\"\"\n",
        "          print(\"【EXIT】else of if fsdp is not None and len(fsdp) > 0:\")\n",
        "\n",
        "      print(\"【EXIT】if self.get_vision_tower() is None:\")\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "  self.config.use_mm_proj = True\n",
        "  print(\"self.config.use_mm_proj set to True\") # True\n",
        "  self.config.mm_projector_type = getattr(model_args, 'mm_projector_type', 'linear')\n",
        "  print(\"self.config.mm_projector_type\\n\", self.config.mm_projector_type) # mlp2x_gelu\n",
        "  self.config.mm_hidden_size = vision_tower.hidden_size\n",
        "  print(\"self.config.mm_hidden_size\\n\", self.config.mm_hidden_size) # 1024\n",
        "  self.config.mm_vision_select_layer = mm_vision_select_layer\n",
        "  print(\"self.config.mm_vision_select_layer\\n\", self.config.mm_vision_select_layer) # -2\n",
        "  self.config.mm_vision_select_feature = mm_vision_select_feature\n",
        "  print(\"self.config.mm_vision_select_feature\\n\", self.config.mm_vision_select_feature) # patch\n",
        "  self.config.mm_patch_merge_type = mm_patch_merge_type\n",
        "  print(\"self.config.mm_patch_merge_type\\n\", self.config.mm_patch_merge_type) # flat\n",
        "\n",
        "  # mm_projector_is_None=True\n",
        "  print(f\"[COND] mm_projector_is_None={getattr(self, 'mm_projector', None) is None}\")\n",
        "  if getattr(self, 'mm_projector', None) is None:\n",
        "      # 【ENTER】\n",
        "      print(\"【ENTER】if getattr(self, 'mm_projector', None) is None:\")\n",
        "      self.mm_projector = build_vision_projector(self.config)\n",
        "      \"\"\"\n",
        "      Sequential(\n",
        "        (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
        "        (1): GELU(approximate='none')\n",
        "        (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
        "      )\n",
        "      \"\"\"\n",
        "      print(\"self.mm_projector after build_vision_projector\\n\", self.mm_projector)\n",
        "      print(\"mm_patch_merge_type\\n\", mm_patch_merge_type) # flat\n",
        "      print(f\"[COND] unpad_in_mm_patch_merge_type={'unpad' in mm_patch_merge_type}\")\n",
        "      if 'unpad' in mm_patch_merge_type:\n",
        "        pass\n",
        "      print(\"【EXIT】if getattr(self, 'mm_projector', None) is None:\")\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "  print(f\"[COND] pretrain_mm_mlp_adapter_is_not_None={pretrain_mm_mlp_adapter is not None}\")\n",
        "  if pretrain_mm_mlp_adapter is not None:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6_2dvDMkr20"
      },
      "outputs": [],
      "source": [
        "LlavaMetaModel.initialize_vision_modules = initialize_vision_modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT_vXMXBlP2a"
      },
      "outputs": [],
      "source": [
        "def get_vision_tower(self):\n",
        "\n",
        "    print(\"current file path\", \"llava/model/llava_arch.py\")\n",
        "    print(\"def get_vision_tower(self)\")\n",
        "    vision_tower = getattr(self, 'vision_tower', None)\n",
        "    print(\"vision_tower (raw)\\n\", vision_tower)\n",
        "    \"\"\"\n",
        "    CLIPVisionTower(\n",
        "    (vision_tower): CLIPVisionModel(\n",
        "        (vision_model): CLIPVisionTransformer(\n",
        "        (embeddings): CLIPVisionEmbeddings(\n",
        "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
        "            (position_embedding): Embedding(577, 1024)\n",
        "        )\n",
        "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
        "        (encoder): CLIPEncoder(\n",
        "            (layers): ModuleList(\n",
        "            (0-23): 24 x CLIPEncoderLayer(\n",
        "                (self_attn): CLIPAttention(\n",
        "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "                )\n",
        "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
        "                (mlp): CLIPMLP(\n",
        "                (activation_fn): QuickGELUActivation()\n",
        "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
        "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
        "                )\n",
        "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
        "            )\n",
        "            )\n",
        "        )\n",
        "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
        "        )\n",
        "    )\n",
        "    )\n",
        "    \"\"\"\n",
        "    print(\"type(vision_tower)\\n\", type(vision_tower))\n",
        "    print(f\"[COND] type_vision_tower_is_list={type(vision_tower) is list}\")  # False\n",
        "    if type(vision_tower) is list:\n",
        "        # 【SKIP】\n",
        "        print(\"【ENTER】if type(vision_tower) is list:\")\n",
        "        vision_tower = vision_tower[0]\n",
        "        print(\"【EXIT】if type(vision_tower) is list:\")\n",
        "    print(\"vision_tower (return)\\n\", vision_tower)\n",
        "    \"\"\"\n",
        "    vision_tower (return)\n",
        "    CLIPVisionTower(\n",
        "    (vision_tower): CLIPVisionModel(\n",
        "        (vision_model): CLIPVisionTransformer(\n",
        "        (embeddings): CLIPVisionEmbeddings(\n",
        "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
        "            (position_embedding): Embedding(577, 1024)\n",
        "        )\n",
        "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
        "        (encoder): CLIPEncoder(\n",
        "            (layers): ModuleList(\n",
        "            (0-23): 24 x CLIPEncoderLayer(\n",
        "                (self_attn): CLIPAttention(\n",
        "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "                )\n",
        "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
        "                (mlp): CLIPMLP(\n",
        "                (activation_fn): QuickGELUActivation()\n",
        "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
        "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
        "                )\n",
        "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
        "            )\n",
        "            )\n",
        "        )\n",
        "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
        "        )\n",
        "    )\n",
        "    )\n",
        "    \"\"\"\n",
        "    return vision_tower"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iabxtpWflUf9"
      },
      "outputs": [],
      "source": [
        "LlavaMetaModel.get_vision_tower = get_vision_tower"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RW9YLSmBT4FG"
      },
      "outputs": [],
      "source": [
        "initial_model.initialize_vision_modules(\n",
        "    model_args=model_args,\n",
        "    fsdp=training_args.fsdp\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3-wDbzZfnfD"
      },
      "outputs": [],
      "source": [
        "vision_tower = model.get_vision_tower()\n",
        "print(\"vision_tower\\n\", vision_tower)\n",
        "vision_tower.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)\n",
        "\n",
        "data_args.image_processor = vision_tower.image_processor\n",
        "print(\"data_args.image_processor\\n\", data_args.image_processor)\n",
        "data_args.is_multimodal = True\n",
        "print(\"data_args.is_multimodal\\n\", data_args.is_multimodal) # True\n",
        "\n",
        "model.config.image_aspect_ratio = data_args.image_aspect_ratio\n",
        "print(\"model.config.image_aspect_ratio\\n\", model.config.image_aspect_ratio) # square\n",
        "model.config.tokenizer_padding_side = tokenizer.padding_side\n",
        "print(\"model.config.tokenizer_padding_side\\n\", model.config.tokenizer_padding_side) # right\n",
        "model.config.tokenizer_model_max_length = tokenizer.model_max_length\n",
        "print(\"model.config.tokenizer_model_max_length\\n\", model.config.tokenizer_model_max_length) # 2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBa5nAaWi1M7"
      },
      "outputs": [],
      "source": [
        "model.config.tune_mm_mlp_adapter = training_args.tune_mm_mlp_adapter = model_args.tune_mm_mlp_adapter\n",
        "print(f\"[COND] tune_mm_mlp_adapter={model_args.tune_mm_mlp_adapter}\") # True\n",
        "if model_args.tune_mm_mlp_adapter:\n",
        "    # 【ENTER】 tune_mm_mlp_adapter=True なので、この分岐に入る\n",
        "    print(\"【ENTER】if model_args.tune_mm_mlp_adapter:\")\n",
        "    # モデル全体の全パラメータを「学習不可（requires_grad=False）」にする\n",
        "    # これで通常の重みは全て凍結される\n",
        "    model.requires_grad_(False)\n",
        "    for p in model.get_model().mm_projector.parameters():\n",
        "        # mm_projector（画像特徴量→テキスト特徴量への変換層）の全パラメータだけを「学習可能（requires_grad=True）」に戻す\n",
        "        # これで mm_projector のみ学習されることになる\n",
        "        print(\"model.get_model().mm_projector.parameters()\", model.get_model().mm_projector.parameters())\n",
        "        p.requires_grad = True\n",
        "    print(\"【EXIT】if model_args.tune_mm_mlp_adapter:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lsCavfasZCY"
      },
      "outputs": [],
      "source": [
        "model.config.freeze_mm_mlp_adapter = training_args.freeze_mm_mlp_adapter\n",
        "print(f\"[COND] freeze_mm_mlp_adapter={training_args.freeze_mm_mlp_adapter}\") # False\n",
        "if training_args.freeze_mm_mlp_adapter:\n",
        "  pass\n",
        "\n",
        "print(f\"[COND] bits={training_args.bits}\") # 16\n",
        "if training_args.bits in [4, 8]:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjMWFAC_um1_"
      },
      "outputs": [],
      "source": [
        "def initialize_vision_tokenizer(self, model_args, tokenizer):\n",
        "    print(\"current file path\", \"llava/model/llava_arch.py\")\n",
        "    print(\"def initialize_vision_tokenizer(self, model_args, tokenizer)\")\n",
        "    print(\"model_args\\n\", model_args) # ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
        "    print(\"tokenizer\\n\", tokenizer) # LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)\n",
        "\n",
        "    print(f\"[COND] mm_use_im_patch_token={model_args.mm_use_im_patch_token}\") # False\n",
        "    if model_args.mm_use_im_patch_token:\n",
        "      pass\n",
        "\n",
        "    if model_args.mm_use_im_start_end: # False\n",
        "      pass\n",
        "\n",
        "    elif model_args.mm_use_im_patch_token: # False\n",
        "      pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnJGUH-zu5lX"
      },
      "outputs": [],
      "source": [
        "LlavaLlamaForCausalLM.initialize_vision_tokenizer = initialize_vision_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lj1_WjcpsnXB"
      },
      "outputs": [],
      "source": [
        "model.config.mm_use_im_start_end = data_args.mm_use_im_start_end = model_args.mm_use_im_start_end\n",
        "print(\"model_args.mm_use_im_start_end\", model_args.mm_use_im_start_end)\n",
        "model.config.mm_projector_lr = training_args.mm_projector_lr\n",
        "print(\"training_args.mm_projector_lr\", training_args.mm_projector_lr)\n",
        "training_args.use_im_start_end = model_args.mm_use_im_start_end\n",
        "print(\"training_args.use_im_start_end\", training_args.use_im_start_end)\n",
        "model.config.mm_use_im_patch_token = model_args.mm_use_im_patch_token\n",
        "print(\"model_args.mm_use_im_patch_token\", model_args.mm_use_im_patch_token)\n",
        "model.initialize_vision_tokenizer(model_args, tokenizer=tokenizer)\n",
        "print(\"【EXIT】if model_args.vision_tower is not None:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLUf4wqcxKI4"
      },
      "outputs": [],
      "source": [
        "def rank0_print(*args):\n",
        "\n",
        "    print(\"current file path\", \"llava/train/train.py\")\n",
        "    print(\"def rank0_print(*args)\")\n",
        "    print(\"args\\n\", args) # ('Formatting inputs...Skip in lazy mode',)\n",
        "    if local_rank == 0:\n",
        "        print(*args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "839mYcaPw2CZ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "\n",
        "class LazySupervisedDataset(Dataset):\n",
        "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    def __init__(self, data_path: str,\n",
        "                 tokenizer: transformers.PreTrainedTokenizer,\n",
        "                 data_args: DataArguments):\n",
        "\n",
        "        print(\"current file path\", \"llava/train/train.py\")\n",
        "        print(\"def LazySupervisedDataset.__init__(self, data_path, tokenizer, data_args)\")\n",
        "        print(\"data_path\\n\", data_path) # /content/LLaVA/blip_laion_cc_sbu_1.json\n",
        "        print(\"tokenizer\\n\", type(tokenizer)) # <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
        "        print(\"data_args\\n\", data_args) # DataArguments(data_path='/content/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/LLaVA/images', image_aspect_ratio='square')\n",
        "        super(LazySupervisedDataset, self).__init__()\n",
        "        list_data_dict = json.load(open(data_path, \"r\"))\n",
        "        # 今回は1サンプルだけなのでprintしても危険ではない\n",
        "        print(\"list_data_dict\", list_data_dict)\n",
        "\n",
        "        rank0_print(\"Formatting inputs...Skip in lazy mode\") # Formatting inputs...Skip in lazy mode\n",
        "        self.tokenizer = tokenizer\n",
        "        print(\"self.tokenizer\\n\", self.tokenizer)\n",
        "        self.list_data_dict = list_data_dict\n",
        "        print(\"self.list_data_dict\\n\", self.list_data_dict)\n",
        "        self.data_args = data_args\n",
        "        print(\"self.data_args\\n\", self.data_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv3nmqgMwVWa"
      },
      "outputs": [],
      "source": [
        "def __len__(self):\n",
        "\n",
        "    print(\"current file path\", \"llava/train/train.py\")\n",
        "    print(\"def LazySupervisedDataset.__len__(self)\")\n",
        "    return len(self.list_data_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64qMycFgwWrA"
      },
      "outputs": [],
      "source": [
        "LazySupervisedDataset.__len__ = __len__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2Th7s50xen5"
      },
      "outputs": [],
      "source": [
        "from typing import Sequence\n",
        "from typing import Dict\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        print(\"current file path\", \"llava/train/train.py\")\n",
        "        print(\"def DataCollatorForSupervisedDataset.__call__(self, instances)\")\n",
        "        print(\"instances\\n\", instances)\n",
        "\n",
        "        # Noneを除外\n",
        "        instances = [x for x in instances if x is not None]\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances]\n",
        "                                  for key in (\"input_ids\", \"labels\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids,\n",
        "            batch_first=True,\n",
        "            padding_value=self.tokenizer.pad_token_id)\n",
        "        labels = torch.nn.utils.rnn.pad_seqIGNORE_INDEXuence(labels,\n",
        "                                                 batch_first=True,\n",
        "                                                 padding_value=IGNORE_INDEX)\n",
        "        input_ids = input_ids[:, :self.tokenizer.model_max_length]\n",
        "        labels = labels[:, :self.tokenizer.model_max_length]\n",
        "        batch = dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )\n",
        "\n",
        "        if 'image' in instances[0]:\n",
        "            images = [instance['image'] for instance in instances]\n",
        "            if all(x is not None and x.shape == images[0].shape for x in images):\n",
        "                batch['images'] = torch.stack(images)\n",
        "            else:\n",
        "                batch['images'] = images\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "QjNj6B85wWA6"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "\n",
        "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer,\n",
        "                                data_args) -> Dict:\n",
        "\n",
        "    print(\"current file path\", \"llava/train/train.py\")\n",
        "    print(\"def make_supervised_data_module(tokenizer, data_args)\")\n",
        "    print(\"tokenizer\\n\", type(tokenizer))\n",
        "    print(\"data_args\\n\", data_args) #  DataArguments(data_path='/content/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/LLaVA/images', image_aspect_ratio='square')\n",
        "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
        "    train_dataset = LazySupervisedDataset(tokenizer=tokenizer,\n",
        "                                data_path=data_args.data_path,\n",
        "                                data_args=data_args)\n",
        "    print(\"train_dataset\\n\", train_dataset) # <llava.train.train.LazySupervisedDataset object at 0x7ed6341f4880>\n",
        "    print(\"len(train_dataset)\\n\", len(train_dataset)) # 1\n",
        "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
        "    print(\"data_collator\\n\", data_collator) # DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))\n",
        "    result = dict(train_dataset=train_dataset,\n",
        "                  eval_dataset=None,\n",
        "                  data_collator=data_collator)\n",
        "    print(\"def make_supervised_data_module: result (return)\\n\", result) # {'train_dataset': <llava.train.train.LazySupervisedDataset object at 0x7ed6341f4880>, 'eval_dataset': None, 'data_collator': DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))}\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlaRN8vG5p99",
        "outputId": "a52c09b2-84d7-4d0b-b5b1-1f94af461408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current file path llava/train/train.py\n",
            "def make_supervised_data_module(tokenizer, data_args)\n",
            "tokenizer\n",
            " <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
            "data_args\n",
            " DataArguments(data_path='/content/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/', image_aspect_ratio='square')\n",
            "current file path llava/train/train.py\n",
            "def LazySupervisedDataset.__init__(self, data_path, tokenizer, data_args)\n",
            "data_path\n",
            " /content/blip_laion_cc_sbu_1.json\n",
            "tokenizer\n",
            " <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
            "data_args\n",
            " DataArguments(data_path='/content/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/', image_aspect_ratio='square')\n",
            "list_data_dict [{'id': '000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Give a brief description of the image.\\n<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]}]\n",
            "current file path llava/train/train.py\n",
            "def rank0_print(*args)\n",
            "args\n",
            " ('Formatting inputs...Skip in lazy mode',)\n",
            "Formatting inputs...Skip in lazy mode\n",
            "self.tokenizer\n",
            " LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "self.list_data_dict\n",
            " [{'id': '000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Give a brief description of the image.\\n<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]}]\n",
            "self.data_args\n",
            " DataArguments(data_path='/content/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/', image_aspect_ratio='square')\n",
            "train_dataset\n",
            " <__main__.LazySupervisedDataset object at 0x7ba0381617f0>\n",
            "current file path llava/train/train.py\n",
            "def LazySupervisedDataset.__len__(self)\n",
            "len(train_dataset)\n",
            " 1\n",
            "data_collator\n",
            " DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "})\n",
            "def make_supervised_data_module: result (return)\n",
            " {'train_dataset': <__main__.LazySupervisedDataset object at 0x7ba0381617f0>, 'eval_dataset': None, 'data_collator': DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "})}\n",
            "data_module\n",
            " {'train_dataset': <__main__.LazySupervisedDataset object at 0x7ba0381617f0>, 'eval_dataset': None, 'data_collator': DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "})}\n"
          ]
        }
      ],
      "source": [
        "data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n",
        "print(\"data_module\\n\", data_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "z8F6sqQdxsMO",
        "outputId": "92194c34-0765-4928-9cda-23c51af0a337"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.trainer because of the following error (look up to see its traceback):\ncannot import name 'EncoderDecoderCache' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1381\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_peft_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m from .auto import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m from .peft_model import (\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mPeftModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSELoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDynamicCache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEncoderDecoderCache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHybridCache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_outputs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQuestionAnsweringModelOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequenceClassifierOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTokenClassifierOutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'EncoderDecoderCache' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1161336710.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m from transformers.trainer import (\n\u001b[1;32m      3\u001b[0m     \u001b[0mis_sagemaker_mp_enabled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mget_parameter_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhas_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1370\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1382\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1385\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\ncannot import name 'EncoderDecoderCache' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer\n",
        "from transformers.trainer import (\n",
        "    is_sagemaker_mp_enabled,\n",
        "    get_parameter_names,\n",
        "    has_length,\n",
        "    ALL_LAYERNORM_LAYERS,\n",
        "    ShardedDDPOption,\n",
        "    logger,\n",
        ")\n",
        "\n",
        "class LLaVATrainer(Trainer):\n",
        "\n",
        "    def _get_train_sampler(self) -> Optional[torch.utils.data.Sampler]:\n",
        "\n",
        "        print(\"current file path\", \"llava/train/llava_trainer.py\")\n",
        "        print(\"def _get_train_sampler(self)\")\n",
        "        print(\"self\\n\", self)\n",
        "        print(f\"[COND] train_dataset_is_None={self.train_dataset is None}, has_length={has_length(self.train_dataset) if self.train_dataset is not None else 'N/A'}\") # train_dataset_is_None=False, has_length=True\n",
        "        if self.train_dataset is None or not has_length(self.train_dataset):\n",
        "          pass\n",
        "\n",
        "        print(f\"[COND] group_by_modality_length={self.args.group_by_modality_length}\") # group_by_modality_length=False\n",
        "        if self.args.group_by_modality_length:\n",
        "          pass\n",
        "        else:\n",
        "            # 【ENTER】\n",
        "            print(\"【ENTER】else (not group_by_modality_length):\")\n",
        "            result = super()._get_train_sampler()\n",
        "            print(\"result, super()._get_train_sampler()\\n\", result) # <torch.utils.data.sampler.RandomSampler object at 0x7ed63e925e70>\n",
        "            print(\"【EXIT】else (not group_by_modality_length):\")\n",
        "            return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgAHEvYzuB0U"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "\n",
        "    print(\"current file path\", \"llava/train/train.py\")\n",
        "    print(\"def train()\")\n",
        "    global local_rank\n",
        "\n",
        "    parser = transformers.HfArgumentParser(\n",
        "        (ModelArguments, DataArguments, TrainingArguments))\n",
        "    print(\"original parser\\n\", parser)\n",
        "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
        "    print(\"model_args\\n\", model_args)\n",
        "    print(\"data_args\\n\", data_args)\n",
        "    print(\"training_args\\n\", training_args)\n",
        "    local_rank = training_args.local_rank\n",
        "    print(\"local_rank\\n\", local_rank)\n",
        "    compute_dtype = (torch.float16 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n",
        "    print(\"compute_dtype\\n\", compute_dtype)\n",
        "    bnb_model_from_pretrained_args = {}\n",
        "    print(\"bnb_model_from_pretrained_args\\n\", bnb_model_from_pretrained_args)\n",
        "    # 【SKIP】bfloat16 なので 以下の if 文はスキップされる\n",
        "    print(f\"[COND] bits={training_args.bits}\")\n",
        "    if training_args.bits in [4, 8]:\n",
        "      pass\n",
        "\n",
        "    print(f\"[COND] vision_tower={model_args.vision_tower}\")\n",
        "    # 【ENTER】 vision_tower=openai/clip-vit-large-patch14-336 なので、この分岐に入る\n",
        "    if model_args.vision_tower is not None:\n",
        "        print(\"【ENTER】if model_args.vision_tower is not None:\")\n",
        "        print(f\"[COND] mpt_in_model_name_or_path={'mpt' in model_args.model_name_or_path}\")\n",
        "        #【SKIP】model_args.model_name_or_path に mptは含まれていないので、この分岐はskipされる\n",
        "        if 'mpt' in model_args.model_name_or_path:\n",
        "          pass\n",
        "\n",
        "        #【ENTER】 model_args.model_name_or_path に mptは含まれていないので、この分岐に入る\n",
        "        else:\n",
        "            print(\"[COND] not_mpt_in_model_name_or_path={'mpt' not in model_args.model_name_or_path}\")\n",
        "            print(\"【ENTER】else of if 'mpt' in model_args.model_name_or_path:\")\n",
        "            # PreTrainedModel.from_pretrained\n",
        "            model = LlavaLlamaForCausalLM.from_pretrained(\n",
        "                model_args.model_name_or_path,\n",
        "                cache_dir=training_args.cache_dir,\n",
        "                **bnb_model_from_pretrained_args\n",
        "            )\n",
        "            print(\"model defined as LlavaLlamaForCausalLM \\n\", model)\n",
        "            print(\"【EXIT】else of if 'mpt' in model_args.model_name_or_path:\")\n",
        "        print(\"【EXIT】if model_args.vision_tower is not None:\")\n",
        "    # 【SKIP】 vision_tower=clip-vit-large-patch14-336 なので、この分岐には入らない\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    print(f\"[COND] freeze_backbone={model_args.freeze_backbone}\")\n",
        "    # 【SKIP】 freeze_backbone=False なので、この分岐はskipされる\n",
        "    if model_args.freeze_backbone:\n",
        "        pass\n",
        "\n",
        "    # 【SKIP】 bfloat16 なので 以下の if 文はスキップされる\n",
        "    print(f\"[COND] bits={training_args.bits}\")\n",
        "    if training_args.bits in [4, 8]:\n",
        "      pass\n",
        "\n",
        "    print(f\"[COND] gradient_checkpointing={training_args.gradient_checkpointing}\")\n",
        "    # 【ENTER】 gradient_checkpointing=True なので、この分岐に入る\n",
        "    if training_args.gradient_checkpointing:\n",
        "        print(\"【ENTER】if training_args.gradient_checkpointing:\")\n",
        "        print(f\"[COND] has_enable_input_require_grads={hasattr(model, 'enable_input_require_grads')}\")\n",
        "        # 【ENTER】 model に enable_input_require_grads メソッドがあるので、この分岐に入る\n",
        "        if hasattr(model, \"enable_input_require_grads\"):\n",
        "            print(\"【ENTER】if hasattr(model, 'enable_input_require_grads'):\")\n",
        "            # PreTrainedModel.enable_input_require_grads\n",
        "            # 元々 全ての重みについて True\n",
        "            model.enable_input_require_grads()\n",
        "            print(\"【EXIT】if hasattr(model, 'enable_input_require_grads'):\")\n",
        "        # 【SKIP】 model に enable_input_require_grads メソッドがあるので、この分岐はskipされる\n",
        "        else:\n",
        "          pass\n",
        "\n",
        "        print(\"【EXIT】if training_args.gradient_checkpointing:\")\n",
        "\n",
        "    print(f\"[COND] lora_enable={training_args.lora_enable}\")\n",
        "    # 【SKIP】 lora_enable=False なので、この分岐はskipされる\n",
        "    if training_args.lora_enable:\n",
        "      pass\n",
        "\n",
        "    print(f\"[COND] mpt_in_model_name_or_path={'mpt' in model_args.model_name_or_path}\")\n",
        "    # 【SKIP】model_args.model_name_or_path に mptは含まれていないので、この分岐はskipされる\n",
        "    if 'mpt' in model_args.model_name_or_path:\n",
        "      pass\n",
        "\n",
        "    #【ENTER】 model_args.model_name_or_path に mptは含まれていないので、この分岐に入る\n",
        "    else:\n",
        "        print(\"[COND] not_mpt_in_model_name_or_path={'mpt' not in model_args.model_name_or_path}\")\n",
        "        print(\"【ENTER】else of if 'mpt' in model_args.model_name_or_path:\")\n",
        "        tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            cache_dir=training_args.cache_dir,\n",
        "            model_max_length=training_args.model_max_length,\n",
        "            padding_side=\"right\",\n",
        "            use_fast=False,\n",
        "        )\n",
        "        print(\"tokenizer defined by AutoTokenizer.from_pretrained \\n\", tokenizer)\n",
        "        print(\"【EXIT】else of if 'mpt' in model_args.model_name_or_path:\")\n",
        "\n",
        "    print(f\"[COND] version={model_args.version}\")\n",
        "    # 【SKIP】 version=plain なので、この分岐はskipされる\n",
        "    if model_args.version == \"v0\":\n",
        "      pass\n",
        "\n",
        "    # 【SKIP】 version=plain なので、この分岐はskipされる\n",
        "    elif model_args.version == \"v0.5\":\n",
        "      pass\n",
        "    # 【ENTER】 version=plain なので、この分岐に入る\n",
        "    else:\n",
        "        print(\"【ENTER】else of if model_args.version == 'v0' and elif 'v0.5':\")\n",
        "        tokenizer.pad_token = tokenizer.unk_token\n",
        "        print(f\"[COND] version_in_conv_templates={model_args.version in conv_templates}\")\n",
        "        # 【ENTER】 model_args.version=plain は conversation_lib.conv_templates に含まれている（\"plain\": conv_llava_plain）ので、この分岐に入る\n",
        "        if model_args.version in conv_templates:\n",
        "            print(\"【ENTER】if model_args.version in conversation_lib.conv_templates:\")\n",
        "            default_conversation = conv_templates[model_args.version]\n",
        "            print(f\"conversation_lib.default_conversation set to {model_args.version}\")\n",
        "            print(\"【EXIT】if model_args.version in conversation_lib.conv_templates:\")\n",
        "        # 【SKIP】 model_args.version=plain は conversation_lib.conv_templates に含まれているので、この分岐はskipされる\n",
        "        else:\n",
        "          pass\n",
        "        print(\"【EXIT】else of if model_args.version == 'v0' and elif 'v0.5':\")\n",
        "\n",
        "    print(f\"[COND] vision_tower={model_args.vision_tower}\")\n",
        "    # 【ENTER】 vision_tower=openai/clip-vit-large-patch14-336 なので、この分岐に入る\n",
        "    if model_args.vision_tower is not None:\n",
        "        print(\"【ENTER】if model_args.vision_tower is not None:\")\n",
        "        model.get_model().initialize_vision_modules(\n",
        "            model_args=model_args,\n",
        "            fsdp=training_args.fsdp\n",
        "        )\n",
        "\n",
        "        vision_tower = model.get_vision_tower()\n",
        "        vision_tower.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)\n",
        "\n",
        "        data_args.image_processor = vision_tower.image_processor\n",
        "        data_args.is_multimodal = True\n",
        "\n",
        "        model.config.image_aspect_ratio = data_args.image_aspect_ratio\n",
        "        model.config.tokenizer_padding_side = tokenizer.padding_side\n",
        "        model.config.tokenizer_model_max_length = tokenizer.model_max_length\n",
        "\n",
        "        model.config.tune_mm_mlp_adapter = training_args.tune_mm_mlp_adapter = model_args.tune_mm_mlp_adapter\n",
        "        print(f\"[COND] tune_mm_mlp_adapter={model_args.tune_mm_mlp_adapter}\") # True\n",
        "        if model_args.tune_mm_mlp_adapter:\n",
        "            # 【ENTER】 tune_mm_mlp_adapter=True なので、この分岐に入る\n",
        "            print(\"【ENTER】if model_args.tune_mm_mlp_adapter:\")\n",
        "            # モデル全体の全パラメータを「学習不可（requires_grad=False）」にする\n",
        "            # これで通常の重みは全て凍結される\n",
        "            model.requires_grad_(False)\n",
        "            for p in model.get_model().mm_projector.parameters():\n",
        "                # mm_projector（画像特徴量→テキスト特徴量への変換層）の全パラメータだけを「学習可能（requires_grad=True）」に戻す\n",
        "                # これで mm_projector のみ学習されることになる\n",
        "                print(\"model.get_model().mm_projector.parameters()\", model.get_model().mm_projector.parameters())\n",
        "                p.requires_grad = True\n",
        "            print(\"【EXIT】if model_args.tune_mm_mlp_adapter:\")\n",
        "\n",
        "        model.config.freeze_mm_mlp_adapter = training_args.freeze_mm_mlp_adapter\n",
        "        print(f\"[COND] freeze_mm_mlp_adapter={training_args.freeze_mm_mlp_adapter}\") # False\n",
        "        if training_args.freeze_mm_mlp_adapter:\n",
        "          pass\n",
        "\n",
        "        print(f\"[COND] bits={training_args.bits}\") # 16\n",
        "        if training_args.bits in [4, 8]:\n",
        "          pass\n",
        "\n",
        "        model.config.mm_use_im_start_end = data_args.mm_use_im_start_end = model_args.mm_use_im_start_end\n",
        "        print(\"model_args.mm_use_im_start_end\", model_args.mm_use_im_start_end)\n",
        "        model.config.mm_projector_lr = training_args.mm_projector_lr\n",
        "        print(\"training_args.mm_projector_lr\", training_args.mm_projector_lr)\n",
        "        training_args.use_im_start_end = model_args.mm_use_im_start_end\n",
        "        print(\"training_args.use_im_start_end\", training_args.use_im_start_end)\n",
        "        model.config.mm_use_im_patch_token = model_args.mm_use_im_patch_token\n",
        "        print(\"model_args.mm_use_im_patch_token\", model_args.mm_use_im_patch_token)\n",
        "        model.initialize_vision_tokenizer(model_args, tokenizer=tokenizer)\n",
        "        print(\"【EXIT】if model_args.vision_tower is not None:\")\n",
        "\n",
        "    print(f\"[COND] bits={training_args.bits}\") # 16\n",
        "    if training_args.bits in [4, 8]:\n",
        "        pass\n",
        "\n",
        "    data_module = make_supervised_data_module(tokenizer=tokenizer,\n",
        "                                              data_args=data_args)\n",
        "    print(\"data_module\\n\", data_module) # {'train_dataset': <llava.train.train.LazySupervisedDataset object at 0x7ed6341f4880>, 'eval_dataset': None, 'data_collator': DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))}\n",
        "\n",
        "    trainer = LLaVATrainer(model=model,\n",
        "                    tokenizer=tokenizer,\n",
        "                    args=training_args,\n",
        "                    **data_module)\n",
        "    print(\"trainer\\n\", trainer) # <llava.train.llava_trainer.LLaVATrainer object at 0x7ed6341f4490>\n",
        "\n",
        "    print(\"【COND】list(pathlib.Path(training_args.output_dir).glob('checkpoint-*'))\\n\", list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\"))) # [PosixPath('checkpoints/llava-v1.5-7b-pretrain/checkpoint-250'), PosixPath('checkpoints/llava-v1.5-7b-pretrain/checkpoint-1')]\n",
        "    if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n",
        "        # 【ENTER】\n",
        "        print(\"【ENTER】if list(pathlib.Path(training_args.output_dir).glob(checkpoint-*)):\")\n",
        "        trainer.train(resume_from_checkpoint=False)\n",
        "        print(\"【EXIT】if list(pathlib.Path(training_args.output_dir).glob(checkpoint-*)):\")\n",
        "    else:\n",
        "        print(\"【ENTER】else of if list(pathlib.Path(training_args.output_dir).glob(checkpoint-*)):\")\n",
        "        trainer.train()\n",
        "        print(\"【EXIT】else of if list(pathlib.Path(training_args.output_dir).glob(checkpoint-*)):\")\n",
        "    trainer.save_state()\n",
        "\n",
        "    model.config.use_cache = True\n",
        "    print(\"model.config.use_cache = True\", model.config.use_cache) # True\n",
        "\n",
        "    print(f\"【COND】lora_enable={training_args.lora_enable}\") # False\n",
        "    if training_args.lora_enable:\n",
        "      pass\n",
        "    else:\n",
        "        # 【ENTER】\n",
        "        print(\"【ENTER】else of if training_args.lora_enable:\")\n",
        "        print(\"trainer\", trainer) # <class 'llava.train.llava_trainer.LLaVATrainer'>\n",
        "        safe_save_model_for_hf_trainer(trainer=trainer,\n",
        "                                       output_dir=training_args.output_dir)\n",
        "        print(\"【EXIT】else of if training_args.lora_enable:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5NuKyC1ZRkX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (llava)",
      "language": "python",
      "name": "llava"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
