{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "rESRfJcc5i6l",
    "outputId": "4a33f5c1-16e9-4ea2-b7ee-93d2010ee4ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip install -q condacolab\\nimport condacolab\\ncondacolab.install()\\n\\n!conda create -n llava python=3.10 -y\\n!conda run -n llava pip install torch==2.0.1 torchvision==0.15.2\\n!conda run -n llava pip install transformers==4.31.0\\n!conda run -n llava pip install tokenizers==0.13.3\\n!conda run -n llava pip install numpy==1.26.0\\n!conda run -n llava pip install accelerate==0.21.0\\n\\n# Sccessfully created conda environment named llava, but the default environment is still python 3.11 without torch module, the default Colab environment\\n!python -c \"import torch, transformers; print(torch.__version__, transformers.__version__)\"\\n\\n# We have to add !conda run -n llava to excute the script in llava environment. This is a bit uncomfortable.\\n!conda run -n llava python -c \"import torch, transformers; print(torch.__version__, transformers.__version__)\"\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install()\n",
    "\n",
    "!conda create -n llava python=3.10 -y\n",
    "!conda run -n llava pip install torch==2.0.1 torchvision==0.15.2\n",
    "!conda run -n llava pip install transformers==4.31.0\n",
    "!conda run -n llava pip install tokenizers==0.13.3\n",
    "!conda run -n llava pip install numpy==1.26.0\n",
    "!conda run -n llava pip install accelerate==0.21.0\n",
    "\n",
    "# Sccessfully created conda environment named llava, but the default environment is still python 3.11 without torch module, the default Colab environment\n",
    "!python -c \"import torch, transformers; print(torch.__version__, transformers.__version__)\"\n",
    "\n",
    "# We have to add !conda run -n llava to excute the script in llava environment. This is a bit uncomfortable.\n",
    "!conda run -n llava python -c \"import torch, transformers; print(torch.__version__, transformers.__version__)\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins\n",
    "import io\n",
    "import contextlib\n",
    "\n",
    "def normalize_key(key: str) -> str:\n",
    "    \"\"\"printキーを実行時と同じ形に揃える\"\"\"\n",
    "    return key.encode(\"utf-8\").decode(\"unicode_escape\").strip()\n",
    "\n",
    "def traced_print_factory(original_print):\n",
    "    store = {}\n",
    "\n",
    "    def traced_print(*args, **kwargs):\n",
    "        if args and isinstance(args[0], str) and len(args) > 1:\n",
    "            key = normalize_key(args[0])\n",
    "            buf = io.StringIO()\n",
    "            tmp_kwargs = dict(kwargs)\n",
    "            tmp_kwargs[\"file\"] = buf\n",
    "            original_print(*args[1:], **tmp_kwargs)\n",
    "            value = buf.getvalue().rstrip()\n",
    "            store[key] = value\n",
    "        # 画面出力は常に行う\n",
    "        original_print(*args, **kwargs)\n",
    "\n",
    "    return traced_print, store\n",
    "\n",
    "\n",
    "def run_and_capture(func, *args, **kwargs):\n",
    "    buffer = io.StringIO()\n",
    "    original_print = builtins.print\n",
    "    traced_print, store = traced_print_factory(original_print)\n",
    "\n",
    "    with contextlib.redirect_stdout(buffer):\n",
    "        builtins.print = traced_print\n",
    "        try:\n",
    "            func(*args, **kwargs)\n",
    "        finally:\n",
    "            builtins.print = original_print\n",
    "\n",
    "    logs = buffer.getvalue()\n",
    "    return logs, store\n",
    "\n",
    "def embed_print_outputs(code: str, mapping: dict[str, str]) -> str:\n",
    "    \"\"\"元のコードに print 出力を埋め込む\"\"\"\n",
    "    new_lines = []\n",
    "    for line in code.splitlines():\n",
    "        stripped = line.strip()\n",
    "        if stripped.startswith(\"print(\") and stripped[6:].startswith('\"'):\n",
    "            if \",\" not in stripped:\n",
    "                new_lines.append(line)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                raw_key = stripped.split('\"', 2)[1]\n",
    "                key = normalize_key(raw_key)\n",
    "            except IndexError:\n",
    "                key = None\n",
    "\n",
    "            indent = line[:len(line) - len(line.lstrip())]  # インデント保持\n",
    "\n",
    "            if key and key in mapping:\n",
    "                value = mapping[key]\n",
    "                if len(value) <= 40:\n",
    "                    new_lines.append(f\"{line}  # {value}\")\n",
    "                else:\n",
    "                    new_lines.append(line)\n",
    "                    new_lines.append(f'{indent}\"\"\"')\n",
    "                    for vline in value.splitlines():\n",
    "                        new_lines.append(f\"{indent}{vline}\")\n",
    "                    new_lines.append(f'{indent}\"\"\"')\n",
    "            else:\n",
    "                new_lines.append(f\"{line}  # not found\")\n",
    "        else:\n",
    "            new_lines.append(line)\n",
    "    return \"\\n\".join(new_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MgqvUKsdFkEz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117 4.31.0\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import torch, transformers; print(torch.__version__, transformers.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "C8j7yWUt4N-C"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RmGhXE625w7R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "[2025-09-23 07:57:55,357] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/wandb/sdk/launch/builder/build.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "/opt/venv/lib/python3.10/site-packages/wandb/sdk/launch/builder/build.py:11: UserWarning: Module wandb was already imported from /opt/venv/lib/python3.10/site-packages/wandb/__init__.py, but /workspaces/LLaVA is being added to sys.path\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n",
    "    version: Optional[str] = field(default=\"v0\")\n",
    "    freeze_backbone: bool = field(default=False)\n",
    "    tune_mm_mlp_adapter: bool = field(default=False)\n",
    "    vision_tower: Optional[str] = field(default=None) # default to None\n",
    "    mm_vision_select_layer: Optional[int] = field(default=-1)   # default to the last layer\n",
    "    pretrain_mm_mlp_adapter: Optional[str] = field(default=None)\n",
    "    mm_projector_type: Optional[str] = field(default='linear')\n",
    "    mm_use_im_start_end: bool = field(default=False)\n",
    "    mm_use_im_patch_token: bool = field(default=True)\n",
    "    mm_patch_merge_type: Optional[str] = field(default='flat')\n",
    "    mm_vision_select_feature: Optional[str] = field(default=\"patch\")\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(default=None,\n",
    "                           metadata={\"help\": \"Path to the training data.\"})\n",
    "    lazy_preprocess: bool = False\n",
    "    is_multimodal: bool = False\n",
    "    image_folder: Optional[str] = field(default=None)\n",
    "    image_aspect_ratio: str = 'square'\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    remove_unused_columns: bool = field(default=False)\n",
    "    freeze_mm_mlp_adapter: bool = field(default=False)\n",
    "    mpt_attn_impl: Optional[str] = field(default=\"triton\")\n",
    "    model_max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\n",
    "            \"help\":\n",
    "            \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
    "        },\n",
    "    )\n",
    "    double_quant: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Compress the quantization statistics through double quantization.\"}\n",
    "    )\n",
    "    quant_type: str = field(\n",
    "        default=\"nf4\",\n",
    "        metadata={\"help\": \"Quantization data type to use. Should be one of `fp4` or `nf4`.\"}\n",
    "    )\n",
    "    bits: int = field(\n",
    "        default=16,\n",
    "        metadata={\"help\": \"How many bits to use.\"}\n",
    "    )\n",
    "    lora_enable: bool = False\n",
    "    lora_r: int = 64\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_weight_path: str = \"\"\n",
    "    lora_bias: str = \"none\"\n",
    "    mm_projector_lr: Optional[float] = None\n",
    "    group_by_modality_length: bool = field(default=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DTvjg0PP7za1"
   },
   "outputs": [],
   "source": [
    "from transformers import HfArgumentParser\n",
    "\n",
    "args_dict = {\n",
    "    #\"deepspeed\": \"./scripts/zero2.json\",\n",
    "    \"model_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    \"version\": \"plain\",\n",
    "    \"data_path\": \"/workspaces/LLaVA/blip_laion_cc_sbu_1.json\",\n",
    "    \"image_folder\": \"/workspaces/LLaVA/images/\",\n",
    "    \"vision_tower\": \"openai/clip-vit-large-patch14-336\",\n",
    "    \"mm_projector_type\": \"mlp2x_gelu\",\n",
    "    \"tune_mm_mlp_adapter\": True,\n",
    "    \"mm_vision_select_layer\": -2,\n",
    "    \"mm_use_im_start_end\": False,\n",
    "    \"mm_use_im_patch_token\": False,\n",
    "    \"bf16\": True,\n",
    "    \"output_dir\": \"./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0\",\n",
    "\n",
    "    # TrainingArguments 相当\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 1,\n",
    "    \"save_total_limit\": 1,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"weight_decay\": 0.0, # I don't know why 0.0\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"logging_steps\": 1,\n",
    "    \"tf32\": False, # switched from True for TinyLlama\n",
    "    \"model_max_length\": 2048,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"dataloader_num_workers\": 2,\n",
    "    \"lazy_preprocess\": True,\n",
    "    \"report_to\": \"none\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dBX7g2DD-xpR",
    "outputId": "627a283d-b903-45db-a0c0-eec1b51ca0eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_args\n",
      " ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
      "data_args\n",
      " DataArguments(data_path='/workspaces/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=False, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')\n",
      "training_args\n",
      " TrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "bits=16,\n",
      "cache_dir=None,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=2,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "double_quant=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "freeze_mm_mlp_adapter=False,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "group_by_modality_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0/runs/Sep23_07-57-57_4987b947cf96,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1,\n",
      "logging_strategy=steps,\n",
      "lora_alpha=16,\n",
      "lora_bias=none,\n",
      "lora_dropout=0.05,\n",
      "lora_enable=False,\n",
      "lora_r=64,\n",
      "lora_weight_path=,\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mm_projector_lr=None,\n",
      "model_max_length=2048,\n",
      "mp_parameters=,\n",
      "mpt_attn_impl=triton,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "quant_type=nf4,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=1,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=False,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_dict(args_dict)\n",
    "print(\"model_args\\n\", model_args)\n",
    "print(\"data_args\\n\", data_args)\n",
    "print(\"training_args\\n\", training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "oC2rbSk5QtcX"
   },
   "outputs": [],
   "source": [
    "# Model Constants\n",
    "IGNORE_INDEX = -100\n",
    "IMAGE_TOKEN_INDEX = -200\n",
    "DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "DEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\n",
    "DEFAULT_IM_START_TOKEN = \"<im_start>\"\n",
    "DEFAULT_IM_END_TOKEN = \"<im_end>\"\n",
    "IMAGE_PLACEHOLDER = \"<image-placeholder>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "J_5thB_J_WEi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_rank\n",
      " 0\n",
      "compute_dtype\n",
      " torch.bfloat16\n",
      "bnb_model_from_pretrained_args\n",
      " {}\n"
     ]
    }
   ],
   "source": [
    "local_rank = training_args.local_rank\n",
    "print(\"local_rank\\n\", local_rank)\n",
    "compute_dtype = (torch.float16 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n",
    "print(\"compute_dtype\\n\", compute_dtype)\n",
    "bnb_model_from_pretrained_args = {} # bitsandbytes\n",
    "print(\"bnb_model_from_pretrained_args\\n\", bnb_model_from_pretrained_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "t0lFK5a0INwz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import CLIPModel\\n\\nnormal_clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\")\\nprint(\"normal_clip_model\\n\", normal_clip_model)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from transformers import CLIPModel\n",
    "\n",
    "normal_clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "print(\"normal_clip_model\\n\", normal_clip_model)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "uk84PQIHmJXO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import CLIPImageProcessor\\n\\nimage_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\\nprint(\"image_processor\\n\", image_processor)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from transformers import CLIPImageProcessor\n",
    "\n",
    "image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "print(\"image_processor\\n\", image_processor)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "d3WvhH4GqSht"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom PIL import Image\\nimport requests\\nfrom io import BytesIO\\nfrom transformers import CLIPImageProcessor\\nimport torch\\nimport torchvision.transforms as T\\nimport matplotlib.pyplot as plt\\n\\n# 画像 URL\\nurl = \"https://llava-vl.github.io/static/images/view.jpg\"\\n\\n# 画像を取得\\nresponse = requests.get(url)\\nimg = Image.open(BytesIO(response.content)).convert(\"RGB\")\\n\\n# 前処理\\nprocessor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\\nprocessed = processor(img, return_tensors=\"pt\")\\n\\n# tensor: shape (1, 3, H, W), 値は正規化済み\\npix = processed[\"pixel_values\"][0]\\n\\n# 正規化を戻す\\nmean = torch.tensor(processor.image_mean).unsqueeze(1).unsqueeze(2)\\nstd = torch.tensor(processor.image_std).unsqueeze(1).unsqueeze(2)\\npix = pix * std + mean\\n\\n# 0-1 範囲にクリップ\\npix = pix.clamp(0.0, 1.0)\\n\\n# 画像生成\\nto_pil = T.ToPILImage()\\nimg_processed = to_pil(pix)\\n\\n# ==== Colab 上で可視化 ====\\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\\n\\naxes[0].imshow(img)\\naxes[0].set_title(\"Original\")\\naxes[0].axis(\"off\")\\n\\naxes[1].imshow(img_processed)\\naxes[1].set_title(\"Processed (normalized etc.)\")\\naxes[1].axis(\"off\")\\n\\nplt.show()\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from transformers import CLIPImageProcessor\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 画像 URL\n",
    "url = \"https://llava-vl.github.io/static/images/view.jpg\"\n",
    "\n",
    "# 画像を取得\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "# 前処理\n",
    "processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "processed = processor(img, return_tensors=\"pt\")\n",
    "\n",
    "# tensor: shape (1, 3, H, W), 値は正規化済み\n",
    "pix = processed[\"pixel_values\"][0]\n",
    "\n",
    "# 正規化を戻す\n",
    "mean = torch.tensor(processor.image_mean).unsqueeze(1).unsqueeze(2)\n",
    "std = torch.tensor(processor.image_std).unsqueeze(1).unsqueeze(2)\n",
    "pix = pix * std + mean\n",
    "\n",
    "# 0-1 範囲にクリップ\n",
    "pix = pix.clamp(0.0, 1.0)\n",
    "\n",
    "# 画像生成\n",
    "to_pil = T.ToPILImage()\n",
    "img_processed = to_pil(pix)\n",
    "\n",
    "# ==== Colab 上で可視化 ====\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axes[0].imshow(img)\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(img_processed)\n",
    "axes[1].set_title(\"Processed (normalized etc.)\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sWLLumgBJbfh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import CLIPVisionModel\\n\\nclip_vision_tower = CLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14-336\")\\nprint(\"clip_vision_tower\\n\", clip_vision_tower)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from transformers import CLIPVisionModel\n",
    "\n",
    "clip_vision_tower = CLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "print(\"clip_vision_tower\\n\", clip_vision_tower)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_w9_49H8Cza2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nconfig_clip_vision_tower = clip_vision_tower.config\\nprint(\"config_clip_vision_tower\\n\", config_clip_vision_tower)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "config_clip_vision_tower = clip_vision_tower.config\n",
    "print(\"config_clip_vision_tower\\n\", config_clip_vision_tower)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jFp9VBciHEhH"
   },
   "outputs": [],
   "source": [
    "from transformers import CLIPVisionModel, CLIPImageProcessor, CLIPVisionConfig\n",
    "import torch.nn as nn\n",
    "# __init__\n",
    "# load_model\n",
    "\n",
    "# result = CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n",
    "class CLIPVisionTower(nn.Module):\n",
    "    def __init__(self, vision_tower, args, delay_load=False):\n",
    "        # result = CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n",
    "        print(\"current file path\", \"llava/llava/model/multimodal_encoder/clip_encoder.py\")\n",
    "        print(\"def CLIPVisionTower.__init__(self, vision_tower, args, delay_load=False)\")\n",
    "        print(\"self\\n\", type(self))\n",
    "        print(\"vision_tower\\n\", vision_tower) # openai/clip-vit-large-patch14-336\n",
    "        print(\"args\\n\", args) # ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
    "        print(\"delay_load\\n\", delay_load) # False\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_loaded = False\n",
    "\n",
    "        print(\"self.is_loaded\\n\", self.is_loaded) # False\n",
    "\n",
    "        self.vision_tower_name = vision_tower\n",
    "        print(\"self.vision_tower_name\\n\", self.vision_tower_name) # openai/clip-vit-large-patch14-336\n",
    "        self.select_layer = args.mm_vision_select_layer\n",
    "        print(\"self.select_layer\\n\", self.select_layer) # -2\n",
    "        self.select_feature = getattr(args, 'mm_vision_select_feature', 'patch')\n",
    "        print(\"self.select_feature\\n\", self.select_feature) # patch\n",
    "\n",
    "        print(f\"【COND】 delay_load={delay_load}\")\n",
    "        if not delay_load:\n",
    "            # 【ENTER】\n",
    "            print(\"【ENTER】if not delay_load:\")\n",
    "            self.load_model()\n",
    "        elif getattr(args, 'unfreeze_mm_vision_tower', False):\n",
    "            print(\"【ENTER】elif getattr(args, 'unfreeze_mm_vision_tower', False):\")\n",
    "            self.load_model()\n",
    "            print(\"【EXIT】elif getattr(args, 'unfreeze_mm_vision_tower', False):\")\n",
    "        else:\n",
    "            print(\"【ENTER】else of if not delay_load/elif getattr(args, 'unfreeze_mm_vision_tower', False):\")\n",
    "            self.cfg_only = CLIPVisionConfig.from_pretrained(self.vision_tower_name)\n",
    "            print(\"self.cfg_only\\n\", self.cfg_only)\n",
    "            print(\"【EXIT】else of if not delay_load/elif getattr(args, 'unfreeze_mm_vision_tower', False):\")\n",
    "\n",
    "\n",
    "    def load_model(self):\n",
    "\n",
    "        print(\"current file path\", \"llava/llava/model/multimodal_encoder/clip_encoder.py\")\n",
    "        print(\"def CLIPVisionTower.load_model(self)\")\n",
    "        print(\"self\\n\", type(self))\n",
    "        print(\"self.vision_tower_name\\n\", self.vision_tower_name) # openai/clip-vit-large-patch14-336\n",
    "        self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name)\n",
    "        print(\"self.image_processor\\n\", self.image_processor)\n",
    "        \"\"\"\n",
    "        CLIPImageProcessor {\n",
    "        \"crop_size\": {\n",
    "            \"height\": 336,\n",
    "            \"width\": 336\n",
    "        },\n",
    "        \"do_center_crop\": true,\n",
    "        \"do_convert_rgb\": true,\n",
    "        \"do_normalize\": true,\n",
    "        \"do_rescale\": true,\n",
    "        \"do_resize\": true,\n",
    "        \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n",
    "        \"image_mean\": [\n",
    "            0.48145466,\n",
    "            0.4578275,\n",
    "            0.40821073\n",
    "        ],\n",
    "        \"image_processor_type\": \"CLIPImageProcessor\",\n",
    "        \"image_std\": [\n",
    "            0.26862954,\n",
    "            0.26130258,\n",
    "            0.27577711\n",
    "        ],\n",
    "        \"resample\": 3,\n",
    "        \"rescale_factor\": 0.00392156862745098,\n",
    "        \"size\": {\n",
    "            \"shortest_edge\": 336\n",
    "        }\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name)\n",
    "        print(\"self.vision_tower\\n\", self.vision_tower)\n",
    "        \"\"\"\n",
    "        CLIPVisionModel(\n",
    "        (vision_model): CLIPVisionTransformer(\n",
    "            (embeddings): CLIPVisionEmbeddings(\n",
    "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "            (position_embedding): Embedding(577, 1024)\n",
    "            )\n",
    "            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "            (encoder): CLIPEncoder(\n",
    "            (layers): ModuleList(\n",
    "                (0-23): 24 x CLIPEncoderLayer(\n",
    "                (self_attn): CLIPAttention(\n",
    "                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                (mlp): CLIPMLP(\n",
    "                    (activation_fn): QuickGELUActivation()\n",
    "                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                )\n",
    "            )\n",
    "            )\n",
    "            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        )\n",
    "        )\n",
    "        \"\"\"\n",
    "        self.vision_tower.requires_grad_(False)\n",
    "\n",
    "        self.is_loaded = True\n",
    "        print(\"self.is_loaded\\n\", self.is_loaded) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5mz6UWifHfTq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def build_vision_tower(vision_tower_cfg, **kwargs):\n",
    "    # vision_tower = build_vision_tower(model_args)\n",
    "    print(\"current file path\", \"llava/llava/model/multimodal_encoder/builder.py\")\n",
    "    print(\"def build_vision_tower(vision_tower_cfg, **kwargs)\")\n",
    "    print(\"vision_tower_cfg\\n\", vision_tower_cfg) # ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
    "    print(\"kwargs\\n\", kwargs) # {}\n",
    "    vision_tower = getattr(vision_tower_cfg, 'mm_vision_tower', getattr(vision_tower_cfg, 'vision_tower', None))\n",
    "    print(\"vision_tower from vision_tower_cfg\\n\", vision_tower) # openai/clip-vit-large-patch14-336\n",
    "    # ローカルに存在しない場合はFalse。存在する場合の例: /ubuntu/home/user/model/openai/clip-vit-large-patch14-336\n",
    "    is_absolute_path_exists = os.path.exists(vision_tower)\n",
    "    print(\"is_absolute_path_exists\\n\", is_absolute_path_exists) # False\n",
    "    print(f\"【COND】 is_absolute_path_exists={is_absolute_path_exists} vision_tower={vision_tower}\") # is_absolute_path_exists=False vision_tower=openai/clip-vit-large-patch14-336\n",
    "    if is_absolute_path_exists or vision_tower.startswith(\"openai\") or vision_tower.startswith(\"laion\") or \"ShareGPT4V\" in vision_tower:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】if is_absolute_path_exists or vision_tower.startswith('openai') or vision_tower.startswith('laion') or 'ShareGPT4V' in vision_tower:\")\n",
    "        result = CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n",
    "        print(\"result (return)\\n\", result) # CLIPVisionTowerクラスのselfに登録されたモジュール一覧を出力する\n",
    "        print(\"【EXIT】if is_absolute_path_exists or vision_tower.startswith('openai') or vision_tower.startswith('laion') or 'ShareGPT4V' in vision_tower:\")\n",
    "        return result\n",
    "\n",
    "    print(\"print(risk): print(vision_tower) disabled for safety\")\n",
    "    raise ValueError(f'Unknown vision tower: {vision_tower}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "sYl9yfNWkAzW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbuild_vision_tower(model_args)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "build_vision_tower(model_args)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "8o16cqWBL_4Q"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def build_vision_projector(config, delay_load=False, **kwargs):\n",
    "\n",
    "    print(\"current file path\", \"llava/llava/model/multimodal_projector/builder.py\")\n",
    "    print(\"def build_vision_projector(config, delay_load=False, **kwargs)\")\n",
    "    print(\"config\\n\", config)\n",
    "    \"\"\"\n",
    "    config\n",
    "    LlavaConfig {\n",
    "    \"_name_or_path\": \"lmsys/vicuna-7b-v1.5\",\n",
    "    \"architectures\": [\n",
    "        \"LlamaForCausalLM\"\n",
    "    ],\n",
    "    \"bos_token_id\": 1,\n",
    "    \"eos_token_id\": 2,\n",
    "    \"hidden_act\": \"silu\",\n",
    "    \"hidden_size\": 4096,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 11008,\n",
    "    \"max_position_embeddings\": 4096,\n",
    "    \"mm_hidden_size\": 1024,\n",
    "    \"mm_patch_merge_type\": \"flat\",\n",
    "    \"mm_projector_type\": \"mlp2x_gelu\",\n",
    "    \"mm_vision_select_feature\": \"patch\",\n",
    "    \"mm_vision_select_layer\": -2,\n",
    "    \"mm_vision_tower\": \"openai/clip-vit-large-patch14-336\",\n",
    "    \"model_type\": \"llava_llama\",\n",
    "    \"num_attention_heads\": 32,\n",
    "    \"num_hidden_layers\": 32,\n",
    "    \"num_key_value_heads\": 32,\n",
    "    \"pad_token_id\": 0,\n",
    "    \"pretraining_tp\": 1,\n",
    "    \"rms_norm_eps\": 1e-05,\n",
    "    \"rope_scaling\": null,\n",
    "    \"tie_word_embeddings\": false,\n",
    "    \"torch_dtype\": \"float16\",\n",
    "    \"transformers_version\": \"4.31.0\",\n",
    "    \"use_cache\": false,\n",
    "    \"use_mm_proj\": true,\n",
    "    \"vocab_size\": 32000\n",
    "    }\n",
    "    \"\"\"\n",
    "    print(\"delay_load\\n\", delay_load) # False\n",
    "    print(\"kwargs\\n\", kwargs) # {}\n",
    "    projector_type = getattr(config, 'mm_projector_type', 'linear')\n",
    "    print(\"projector_type from config\\n\", projector_type) # mlp2x_gelu\n",
    "\n",
    "    print(\"【COND】 projector_type\\n\", projector_type) # mlp2x_gelu\n",
    "    if projector_type == 'linear':\n",
    "      pass\n",
    "\n",
    "    mlp_gelu_match = re.match(r'^mlp(\\d+)x_gelu$', projector_type)\n",
    "    print(\"【COND】mlp_gelu_match\\n\", mlp_gelu_match)\n",
    "    if mlp_gelu_match:\n",
    "        #【ENTER】if mlp_gelu_match:\n",
    "        print(\"【ENTER】if mlp_gelu_match:\")\n",
    "        mlp_depth = int(mlp_gelu_match.group(1))\n",
    "        print(\"mlp_depth from mlp_gelu_match.group(1)\\n\", mlp_depth)\n",
    "        modules = [nn.Linear(config.mm_hidden_size, config.hidden_size)]\n",
    "        print(\"modules after first Linear\\n\", modules)\n",
    "        for _ in range(1, mlp_depth):\n",
    "            modules.append(nn.GELU())\n",
    "            modules.append(nn.Linear(config.hidden_size, config.hidden_size))\n",
    "        print(\"modules before Sequential\\n\", modules)\n",
    "        result = nn.Sequential(*modules) # * はリストをアンパックして引数に展開する\n",
    "        print(\"result (return)\\n\", result)\n",
    "        \"\"\"\n",
    "        Sequential(\n",
    "        (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "        (1): GELU(approximate='none')\n",
    "        (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
    "        )\n",
    "        \"\"\"\n",
    "        print(\"【EXIT】if mlp_gelu_match:\")\n",
    "        return result\n",
    "\n",
    "    print(\"【COND】projector_type\\n\", projector_type)\n",
    "    if projector_type == 'identity':\n",
    "      pass\n",
    "\n",
    "    print(\"print(risk): print(projector_type) disabled for safety\")\n",
    "    raise ValueError(f'Unknown projector type: {projector_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "VjkgxnbKFGan"
   },
   "outputs": [],
   "source": [
    "# LlavaMetaModel\n",
    "# __init__\n",
    "# get_vision_tower\n",
    "# initialize_vision_modules\n",
    "# unpad_image\n",
    "\n",
    "class LlavaMetaModel:\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "        print(\"LlavaMetaModel.__init__(self, config)\")\n",
    "        print(\"config\\n\", config)\n",
    "        # LlamaModelの__init_を呼び出す\n",
    "        super(LlavaMetaModel, self).__init__(config)\n",
    "\n",
    "        print(f\"【COND】 mm_vision_tower={hasattr(config, 'mm_vision_tower')}\")\n",
    "        if hasattr(config, \"mm_vision_tower\"):\n",
    "            print(\"【ENTER】if hasattr(config, 'mm_vision_tower'):\")\n",
    "            self.vision_tower = build_vision_tower(config, delay_load=True)\n",
    "            print(\"self.vision_tower\\n\", self.vision_tower)\n",
    "            self.mm_projector = build_vision_projector(config)\n",
    "            print(\"self.mm_projector\\n\", self.mm_projector)\n",
    "\n",
    "            print(\"self.config.mm_patch_merge_type\\n\", self.config.mm_patch_merge_type)\n",
    "            print(f\"【COND】 unpad_in_mm_patch_merge_type={'unpad' in getattr(config, 'mm_patch_merge_type', '')}\")\n",
    "            if 'unpad' in getattr(config, 'mm_patch_merge_type', ''):\n",
    "              pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "T_YguuBGN-3R"
   },
   "outputs": [],
   "source": [
    "from transformers import LlamaConfig, LlamaModel\n",
    "\n",
    "class LlavaConfig(LlamaConfig):\n",
    "    model_type = \"llava_llama\"\n",
    "\n",
    "\n",
    "class LlavaLlamaModel(LlavaMetaModel, LlamaModel):\n",
    "    config_class = LlavaConfig\n",
    "\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "\n",
    "        print(\"current file path\", \"llava/llava/model/language_model/llava_llama.py\")\n",
    "        print(\"def LlavaLlamaModel.__init__(self, config: LlamaConfig)\")\n",
    "        print(\"self\\n\", type(self))\n",
    "        print(\"config\\n\", config)\n",
    "        super(LlavaLlamaModel, self).__init__(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "V9uRX7reN0gI"
   },
   "outputs": [],
   "source": [
    "# LlavaMetaForCausalLM\n",
    "# get_vision_tower\n",
    "# encode_images\n",
    "# prepare_inputs_labels_for_multimodal\n",
    "# initialize_vision_tokenizer\n",
    "\n",
    "class LlavaMetaForCausalLM:\n",
    "\n",
    "    def get_vision_tower(self):\n",
    "        print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "        print(\"class LlavaMetaForCausalLM(ABC).get_vision_tower(self)\")\n",
    "        result = self.get_model().get_vision_tower()\n",
    "        print(\"LlavaMetaForCausalLM(ABC).get_vision_tower(self) result (return)\\n\", result)\n",
    "        \"\"\"\n",
    "        CLIPVisionTower(\n",
    "        (vision_tower): CLIPVisionModel(\n",
    "            (vision_model): CLIPVisionTransformer(\n",
    "            (embeddings): CLIPVisionEmbeddings(\n",
    "                (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "                (position_embedding): Embedding(577, 1024)\n",
    "            )\n",
    "            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "            (encoder): CLIPEncoder(\n",
    "                (layers): ModuleList(\n",
    "                (0-23): 24 x CLIPEncoderLayer(\n",
    "                    (self_attn): CLIPAttention(\n",
    "                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    )\n",
    "                    (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                    (mlp): CLIPMLP(\n",
    "                    (activation_fn): QuickGELUActivation()\n",
    "                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "                    )\n",
    "                    (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                )\n",
    "                )\n",
    "            )\n",
    "            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "            )\n",
    "        )\n",
    "        )\n",
    "        \"\"\"\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "t4yAIwyeOEWE"
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.generation.utils import GenerateOutput\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "class LlavaLlamaForCausalLM(LlamaForCausalLM, LlavaMetaForCausalLM):\n",
    "    config_class = LlavaConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        print(\"current file path\", \"llava/llava/model/language_model/llava_llama.py\")\n",
    "        print(\"def LlavaLlamaForCausalLM.__init__(self, config)\")\n",
    "        print(\"self\\n\", type(self))\n",
    "        # config は https://huggingface.co/lmsys/vicuna-7b-v1.5/blob/main/config.json\n",
    "        print(\"config\\n\", config)\n",
    "        super(LlamaForCausalLM, self).__init__(config)\n",
    "        self.model = LlavaLlamaModel(config)\n",
    "        # LlavaLlamaModelの初期化あと、LlavaMetaModelの初期化も呼ばれる。\n",
    "        self.pretraining_tp = config.pretraining_tp\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        print(\"self.model\\n\", self.model)\n",
    "        \"\"\"\n",
    "        self.model\n",
    "        LlavaLlamaModel(\n",
    "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
    "        (layers): ModuleList(\n",
    "            (0-31): 32 x LlamaDecoderLayer(\n",
    "            (self_attn): LlamaAttention(\n",
    "                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "                (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "                (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "                (rotary_emb): LlamaRotaryEmbedding()\n",
    "            )\n",
    "            (mlp): LlamaMLP(\n",
    "                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
    "                (act_fn): SiLUActivation()\n",
    "            )\n",
    "            (input_layernorm): LlamaRMSNorm()\n",
    "            (post_attention_layernorm): LlamaRMSNorm()\n",
    "            )\n",
    "        )\n",
    "        (norm): LlamaRMSNorm()\n",
    "        )\n",
    "        \"\"\"\n",
    "        print(\"self.pretraining_tp\\n\", self.pretraining_tp) # 1\n",
    "        print(\"self.vocab_size\\n\", self.vocab_size) # 32_000\n",
    "        print(\"self.lm_head\\n\", self.lm_head) # Linear(in_features=4096, out_features=32000, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "2Wf2jZjizGZo",
    "outputId": "ef0ee186-8461-4560-982d-9bd4a0c530b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import AutoConfig\\n\\n# 公式 LLaMA-2-7B の config をロード\\nllama_config = AutoConfig.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\\n\\nprint(llama_config)\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from transformers import AutoConfig\n",
    "\n",
    "# 公式 LLaMA-2-7B の config をロード\n",
    "llama_config = AutoConfig.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "print(llama_config)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "RlMP4qUg0QtV",
    "outputId": "12f73ecd-9922-4d31-a0cf-a0e8bc5d88fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import AutoConfig\\n\\n# まず config.json をロードして Config クラスを自動判別\\nconfig = AutoConfig.from_pretrained(\\n    model_args.model_name_or_path,\\n    cache_dir=training_args.cache_dir\\n)\\n\\nprint(\"model_args.model_name_or_path\\n\", model_args.model_name_or_path)\\nprint(\"training_args.cache_dir\\n\", training_args.cache_dir)\\nprint(\"\")\\nprint(\"Loaded config:\\n\", config)\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from transformers import AutoConfig\n",
    "\n",
    "# まず config.json をロードして Config クラスを自動判別\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir\n",
    ")\n",
    "\n",
    "print(\"model_args.model_name_or_path\\n\", model_args.model_name_or_path)\n",
    "print(\"training_args.cache_dir\\n\", training_args.cache_dir)\n",
    "print(\"\")\n",
    "print(\"Loaded config:\\n\", config)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yNGv8pE6SM9L",
    "outputId": "dfcf4d9a-55c6-492b-e92d-56146f36e5f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function LlavaLlamaModel.__init__ at 0x7efff1124040>\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getattr_static(LlavaLlamaModel, \"__init__\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FZSUn4slSEwV",
    "outputId": "84dde4c4-b051-4cea-ec3d-88ae67db5fd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRO for LlavaLlamaModel:\n",
      "\n",
      " 0: __main__.LlavaLlamaModel\n",
      " 1: __main__.LlavaMetaModel\n",
      " 2: transformers.models.llama.modeling_llama.LlamaModel\n",
      " 3: transformers.models.llama.modeling_llama.LlamaPreTrainedModel\n",
      " 4: transformers.modeling_utils.PreTrainedModel\n",
      " 5: torch.nn.modules.module.Module\n",
      " 6: transformers.modeling_utils.ModuleUtilsMixin\n",
      " 7: transformers.generation.utils.GenerationMixin\n",
      " 8: transformers.utils.hub.PushToHubMixin\n",
      " 9: builtins.object\n"
     ]
    }
   ],
   "source": [
    "def print_mro(cls):\n",
    "    print(f\"MRO for {cls.__name__}:\\n\")\n",
    "    for i, c in enumerate(cls.mro()):\n",
    "        print(f\"{i:2d}: {c.__module__}.{c.__name__}\")\n",
    "\n",
    "print_mro(LlavaLlamaModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QwLz_ubtVlti",
    "outputId": "680f292f-9756-45d4-cb80-af9692475a34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRO for LlavaMetaModel:\n",
      "\n",
      " 0: __main__.LlavaMetaModel\n",
      " 1: builtins.object\n"
     ]
    }
   ],
   "source": [
    "print_mro(LlavaMetaModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JhWiOc7BvjcV",
    "outputId": "52855dab-04f8-4ca3-81d1-fb6c7c74a379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRO for LlavaLlamaForCausalLM:\n",
      "\n",
      " 0: __main__.LlavaLlamaForCausalLM\n",
      " 1: transformers.models.llama.modeling_llama.LlamaForCausalLM\n",
      " 2: transformers.models.llama.modeling_llama.LlamaPreTrainedModel\n",
      " 3: transformers.modeling_utils.PreTrainedModel\n",
      " 4: torch.nn.modules.module.Module\n",
      " 5: transformers.modeling_utils.ModuleUtilsMixin\n",
      " 6: transformers.generation.utils.GenerationMixin\n",
      " 7: transformers.utils.hub.PushToHubMixin\n",
      " 8: __main__.LlavaMetaForCausalLM\n",
      " 9: builtins.object\n"
     ]
    }
   ],
   "source": [
    "print_mro(LlavaLlamaForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "UJE07TY6Jb5K",
    "outputId": "091b04d2-adbc-49c3-ef2a-50181c53f793"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.__init__(self, config)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "config\n",
      " LlavaConfig {\n",
      "  \"_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llava_llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaModel.__init__(self, config: LlamaConfig)\n",
      "self\n",
      " <class '__main__.LlavaLlamaModel'>\n",
      "config\n",
      " LlavaConfig {\n",
      "  \"_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llava_llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "current file path llava/model/llava_arch.py\n",
      "LlavaMetaModel.__init__(self, config)\n",
      "config\n",
      " LlavaConfig {\n",
      "  \"_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llava_llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "【COND】 mm_vision_tower=False\n",
      "self.model\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      ")\n",
      "self.pretraining_tp\n",
      " 1\n",
      "self.vocab_size\n",
      " 32000\n",
      "self.lm_head\n",
      " Linear(in_features=2048, out_features=32000, bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0 and are newly initialized: ['model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    **bnb_model_from_pretrained_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "wodQZ9rWSyZN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      " LlavaLlamaForCausalLM(\n",
      "  (model): LlavaLlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-21): 22 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"model\\n\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "z5c1aEs4gf5L"
   },
   "outputs": [],
   "source": [
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "l9az2bPNccDC"
   },
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from typing import List\n",
    "from enum import auto, Enum\n",
    "\n",
    "class SeparatorStyle(Enum):\n",
    "    \"\"\"Different separator style.\"\"\"\n",
    "    SINGLE = auto()\n",
    "    TWO = auto()\n",
    "    MPT = auto()\n",
    "    PLAIN = auto()\n",
    "    LLAMA_2 = auto()\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Conversation:\n",
    "    \"\"\"A class that keeps all conversation history.\"\"\"\n",
    "    system: str\n",
    "    roles: List[str]\n",
    "    messages: List[List[str]]\n",
    "    offset: int\n",
    "    sep_style: SeparatorStyle = SeparatorStyle.SINGLE\n",
    "    sep: str = \"###\"\n",
    "    sep2: str = None\n",
    "    version: str = \"Unknown\"\n",
    "\n",
    "    skip_next: bool = False\n",
    "\n",
    "\n",
    "conv_llava_plain = Conversation(\n",
    "    system=\"\",\n",
    "    roles=(\"\", \"\"),\n",
    "    messages=(\n",
    "    ),\n",
    "    offset=0,\n",
    "    sep_style=SeparatorStyle.PLAIN,\n",
    "    sep=\"\\n\",\n",
    ")\n",
    "\n",
    "\n",
    "conv_templates = {\n",
    "    \"plain\": conv_llava_plain,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "YSs2_yNVOpOD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<classmethod(<function PreTrainedModel.from_pretrained at 0x7efff13c7f40>)>\n",
      "<function PreTrainedModel.enable_input_require_grads at 0x7efff13c70a0>\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getattr_static(LlamaForCausalLM, \"from_pretrained\"))\n",
    "print(inspect.getattr_static(LlamaForCausalLM, \"enable_input_require_grads\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "od0OYu7yBesv"
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    model_max_length=training_args.model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "M71wf9CYD_Y7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token: </s>\n",
      "pad_token_id: 2\n",
      "unk_token: <unk>\n",
      "unk_token_id: 0\n",
      "tokenizer\n",
      " LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False)}, clean_up_tokenization_spaces=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"pad_token:\", tokenizer.pad_token)\n",
    "print(\"pad_token_id:\", tokenizer.pad_token_id)\n",
    "print(\"unk_token:\", tokenizer.unk_token)\n",
    "print(\"unk_token_id:\", tokenizer.unk_token_id)\n",
    "print(\"tokenizer\\n\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "5TrZtJFODFWz"
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Vbf0h2NSDoUG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token: <unk>\n",
      "pad_token_id: 0\n",
      "unk_token: <unk>\n",
      "unk_token_id: 0\n",
      "tokenizer\n",
      " LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"pad_token:\", tokenizer.pad_token)\n",
    "print(\"pad_token_id:\", tokenizer.pad_token_id)\n",
    "print(\"unk_token:\", tokenizer.unk_token)\n",
    "print(\"unk_token_id:\", tokenizer.unk_token_id)\n",
    "print(\"tokenizer\\n\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "kGtPYpfKDOKW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default_conversation\n",
      " Conversation(system='', roles=('', ''), messages=(), offset=0, sep_style=<SeparatorStyle.PLAIN: 4>, sep='\\n', sep2=None, version='Unknown', skip_next=False)\n"
     ]
    }
   ],
   "source": [
    "default_conversation = conv_templates[model_args.version]\n",
    "print(\"default_conversation\\n\", default_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "csi6Xe_-G0_G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_args.vision_tower\n",
      " openai/clip-vit-large-patch14-336\n"
     ]
    }
   ],
   "source": [
    "print(\"model_args.vision_tower\\n\", model_args.vision_tower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "gpri2EiEIatU"
   },
   "outputs": [],
   "source": [
    "def get_model(self):\n",
    "\n",
    "    print(\"current file path\", \"llava/llava/model/language_model/llava_llama.py\")\n",
    "    print(\"def LlavaLlamaForCausalLM.get_model(self)\")\n",
    "    print(\"self\\n\", type(self))\n",
    "    print(\"self.model (return)\\n\", self.model)\n",
    "    return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "RPVqYUnjJHoF"
   },
   "outputs": [],
   "source": [
    "LlavaLlamaForCausalLM.get_model = get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Iqqp22MDJSJ-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.get_model(self)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "self.model (return)\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "initial_model = model.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "4c_UQLwdPhjd"
   },
   "outputs": [],
   "source": [
    "def config(self):\n",
    "\n",
    "    print(\"current file path\", \"llava/llava/model/multimodal_encoder/clip_encoder.py\")\n",
    "    print(\"def CLIPVisionTower.config(self)\")\n",
    "    print(\"self\\n\", type(self))\n",
    "    print(\"self.is_loaded\\n\", self.is_loaded) # True\n",
    "    print(f\"【COND】 is_loaded={self.is_loaded}\")\n",
    "    if self.is_loaded:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】if self.is_loaded:\")\n",
    "        result = self.vision_tower.config\n",
    "        print(\"result (return)\\n\", type(result))\n",
    "        print(\"【EXIT】if self.is_loaded:\")\n",
    "    else:\n",
    "      pass\n",
    "    print(\"result (return)\\n\", result)\n",
    "    \"\"\"\n",
    "    CLIPVisionConfig {\n",
    "    \"_name_or_path\": \"openai/clip-vit-large-patch14-336\",\n",
    "    \"attention_dropout\": 0.0,\n",
    "    \"dropout\": 0.0,\n",
    "    \"hidden_act\": \"quick_gelu\",\n",
    "    \"hidden_size\": 1024,\n",
    "    \"image_size\": 336,\n",
    "    \"initializer_factor\": 1.0,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 4096,\n",
    "    \"layer_norm_eps\": 1e-05,\n",
    "    \"model_type\": \"clip_vision_model\",\n",
    "    \"num_attention_heads\": 16,\n",
    "    \"num_channels\": 3,\n",
    "    \"num_hidden_layers\": 24,\n",
    "    \"patch_size\": 14,\n",
    "    \"projection_dim\": 768,\n",
    "    \"transformers_version\": \"4.31.0\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "6YLi3g3NQMIK"
   },
   "outputs": [],
   "source": [
    "def hidden_size(self):\n",
    "\n",
    "    print(\"current file path\", \"llava/llava/model/multimodal_encoder/clip_encoder.py\")\n",
    "    print(\"def CLIPVisionTower.hidden_size(self)\")\n",
    "    print(\"self\\n\", type(self))\n",
    "    result = self.config.hidden_size\n",
    "    print(\"result (return), self.config.hidden_size\\n\", result) # 1024\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "XG8ocYNiPpGB"
   },
   "outputs": [],
   "source": [
    "CLIPVisionTower.config = property(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "CGom3d6lP40g"
   },
   "outputs": [],
   "source": [
    "CLIPVisionTower.hidden_size = property(hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "HrBNMbAm39zA"
   },
   "outputs": [],
   "source": [
    "def initialize_vision_modules(self, model_args, fsdp=None):\n",
    "\n",
    "  print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "  print(\"def initialize_vision_modules(self, model_args, fsdp=None)\")\n",
    "  print(\"model_args\\n\", model_args) #  ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
    "  print(\"fsdp\\n\", fsdp) # []\n",
    "  vision_tower = model_args.vision_tower\n",
    "  print(\"vision_tower from model_args\\n\", vision_tower) # openai/clip-vit-large-patch14-336\n",
    "  mm_vision_select_layer = model_args.mm_vision_select_layer\n",
    "  print(\"mm_vision_select_layer from model_args\\n\", mm_vision_select_layer) # -2\n",
    "  mm_vision_select_feature = model_args.mm_vision_select_feature\n",
    "  print(\"mm_vision_select_feature from model_args\\n\", mm_vision_select_feature) # patch\n",
    "  pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n",
    "  print(\"pretrain_mm_mlp_adapter from model_args\\n\", pretrain_mm_mlp_adapter) # None\n",
    "  mm_patch_merge_type = model_args.mm_patch_merge_type\n",
    "  # 下記はself.config.mm_vision_towerに関するもの。self.vision_towerは依然としてNone\n",
    "  self.config.mm_vision_tower = vision_tower\n",
    "  print(\"self.config.mm_vision_tower\\n\", self.config.mm_vision_tower) # None\n",
    "\n",
    "  print(\"【COND】 self.get_vision_tower()\\n\", self.get_vision_tower()) # None\n",
    "  print(f\"【COND】 get_vision_tower_is_None={self.get_vision_tower() is None}\")\n",
    "  if self.get_vision_tower() is None:\n",
    "      #【ENTER】self.vision_tower, self.get_vision_towerはNoneなのでこの分岐に入る。\n",
    "      print(\"【ENTER】if self.get_vision_tower() is None:\")\n",
    "      print(\"[ENTER] self.get_vision_tower() is None\")\n",
    "      # build_vision_tower(model_args) はちょっと奥の依存関係が深い\n",
    "      vision_tower = build_vision_tower(model_args)\n",
    "      print(\"vision_tower after build_vision_tower\\n\", vision_tower)\n",
    "      \"\"\"\n",
    "      CLIPVisionTower(\n",
    "      (vision_tower): CLIPVisionModel(\n",
    "      (vision_model): CLIPVisionTransformer(\n",
    "          (embeddings): CLIPVisionEmbeddings(\n",
    "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "          (position_embedding): Embedding(577, 1024)\n",
    "          )\n",
    "          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "          (encoder): CLIPEncoder(\n",
    "          (layers): ModuleList(\n",
    "              (0-23): 24 x CLIPEncoderLayer(\n",
    "              (self_attn): CLIPAttention(\n",
    "                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "              )\n",
    "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "              (mlp): CLIPMLP(\n",
    "                  (activation_fn): QuickGELUActivation()\n",
    "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "              )\n",
    "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "              )\n",
    "          )\n",
    "          )\n",
    "          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "      )\n",
    "      )\n",
    "      )\n",
    "      \"\"\"\n",
    "      # 分散学習(FSDP)を使うかどうか. 今回は [] 空のリストとなるので、Noneではないが、len(fsdp) == 0\n",
    "      print(\"【COND】 fsdp\\n\", fsdp) # []\n",
    "      print(f\"【COND】 fsdp_is_not_None={fsdp is not None} len_fsdp={len(fsdp) if fsdp is not None else 'N/A'}\") # fsdp_is_not_None=True len_fsdp=0\n",
    "      if fsdp is not None and len(fsdp) > 0:\n",
    "        pass\n",
    "      else:\n",
    "          # 【ENTER】else of if fsdp is not None and len(fsdp) > 0:\n",
    "          print(\"【COND】 else_fsdp_is_not_None_and_len_fsdp_gt_0=True\")\n",
    "          print(\"【ENTER】else of if fsdp is not None and len(fsdp) > 0:\")\n",
    "          self.vision_tower = vision_tower\n",
    "          print(\"self.vision_tower\\n\", self.vision_tower)\n",
    "          \"\"\"\n",
    "          CLIPVisionTower(\n",
    "          (vision_tower): CLIPVisionModel(\n",
    "              (vision_model): CLIPVisionTransformer(\n",
    "              (embeddings): CLIPVisionEmbeddings(\n",
    "                  (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "                  (position_embedding): Embedding(577, 1024)\n",
    "              )\n",
    "              (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "              (encoder): CLIPEncoder(\n",
    "                  (layers): ModuleList(\n",
    "                  (0-23): 24 x CLIPEncoderLayer(\n",
    "                      (self_attn): CLIPAttention(\n",
    "                      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                      )\n",
    "                      (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                      (mlp): CLIPMLP(\n",
    "                      (activation_fn): QuickGELUActivation()\n",
    "                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "                      )\n",
    "                      (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                  )\n",
    "                  )\n",
    "              )\n",
    "              (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "              )\n",
    "          )\n",
    "          )\n",
    "          \"\"\"\n",
    "          print(\"【EXIT】else of if fsdp is not None and len(fsdp) > 0:\")\n",
    "\n",
    "      print(\"【EXIT】if self.get_vision_tower() is None:\")\n",
    "  else:\n",
    "    pass\n",
    "\n",
    "  self.config.use_mm_proj = True\n",
    "  print(\"self.config.use_mm_proj set to True\") # True\n",
    "  self.config.mm_projector_type = getattr(model_args, 'mm_projector_type', 'linear')\n",
    "  print(\"self.config.mm_projector_type\\n\", self.config.mm_projector_type) # mlp2x_gelu\n",
    "  self.config.mm_hidden_size = vision_tower.hidden_size\n",
    "  print(\"self.config.mm_hidden_size\\n\", self.config.mm_hidden_size) # 1024\n",
    "  self.config.mm_vision_select_layer = mm_vision_select_layer\n",
    "  print(\"self.config.mm_vision_select_layer\\n\", self.config.mm_vision_select_layer) # -2\n",
    "  self.config.mm_vision_select_feature = mm_vision_select_feature\n",
    "  print(\"self.config.mm_vision_select_feature\\n\", self.config.mm_vision_select_feature) # patch\n",
    "  self.config.mm_patch_merge_type = mm_patch_merge_type\n",
    "  print(\"self.config.mm_patch_merge_type\\n\", self.config.mm_patch_merge_type) # flat\n",
    "\n",
    "  # mm_projector_is_None=True\n",
    "  print(f\"【COND】 mm_projector_is_None={getattr(self, 'mm_projector', None) is None}\")\n",
    "  if getattr(self, 'mm_projector', None) is None:\n",
    "      # 【ENTER】\n",
    "      print(\"【ENTER】if getattr(self, 'mm_projector', None) is None:\")\n",
    "      self.mm_projector = build_vision_projector(self.config)\n",
    "      \"\"\"\n",
    "      Sequential(\n",
    "        (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
    "        (1): GELU(approximate='none')\n",
    "        (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
    "      )\n",
    "      \"\"\"\n",
    "      print(\"self.mm_projector after build_vision_projector\\n\", self.mm_projector)\n",
    "      print(\"mm_patch_merge_type\\n\", mm_patch_merge_type) # flat\n",
    "      print(f\"【COND】 unpad_in_mm_patch_merge_type={'unpad' in mm_patch_merge_type}\")\n",
    "      if 'unpad' in mm_patch_merge_type:\n",
    "        pass\n",
    "      print(\"【EXIT】if getattr(self, 'mm_projector', None) is None:\")\n",
    "  else:\n",
    "    pass\n",
    "\n",
    "  print(f\"【COND】 pretrain_mm_mlp_adapter_is_not_None={pretrain_mm_mlp_adapter is not None}\")\n",
    "  if pretrain_mm_mlp_adapter is not None:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "e6_2dvDMkr20"
   },
   "outputs": [],
   "source": [
    "LlavaMetaModel.initialize_vision_modules = initialize_vision_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "qT_vXMXBlP2a"
   },
   "outputs": [],
   "source": [
    "def get_vision_tower(self):\n",
    "\n",
    "    print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "    print(\"def get_vision_tower(self)\")\n",
    "    vision_tower = getattr(self, 'vision_tower', None)\n",
    "    print(\"vision_tower (raw)\\n\", vision_tower)\n",
    "    \"\"\"\n",
    "    CLIPVisionTower(\n",
    "    (vision_tower): CLIPVisionModel(\n",
    "        (vision_model): CLIPVisionTransformer(\n",
    "        (embeddings): CLIPVisionEmbeddings(\n",
    "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "            (position_embedding): Embedding(577, 1024)\n",
    "        )\n",
    "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        (encoder): CLIPEncoder(\n",
    "            (layers): ModuleList(\n",
    "            (0-23): 24 x CLIPEncoderLayer(\n",
    "                (self_attn): CLIPAttention(\n",
    "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                (mlp): CLIPMLP(\n",
    "                (activation_fn): QuickGELUActivation()\n",
    "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "            )\n",
    "            )\n",
    "        )\n",
    "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        )\n",
    "    )\n",
    "    )\n",
    "    \"\"\"\n",
    "    print(\"type(vision_tower)\\n\", type(vision_tower))\n",
    "    print(f\"【COND】 type_vision_tower_is_list={type(vision_tower) is list}\")  # False\n",
    "    if type(vision_tower) is list:\n",
    "        # 【SKIP】\n",
    "        print(\"【ENTER】if type(vision_tower) is list:\")\n",
    "        vision_tower = vision_tower[0]\n",
    "        print(\"【EXIT】if type(vision_tower) is list:\")\n",
    "    print(\"vision_tower (return)\\n\", vision_tower)\n",
    "    \"\"\"\n",
    "    vision_tower (return)\n",
    "    CLIPVisionTower(\n",
    "    (vision_tower): CLIPVisionModel(\n",
    "        (vision_model): CLIPVisionTransformer(\n",
    "        (embeddings): CLIPVisionEmbeddings(\n",
    "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "            (position_embedding): Embedding(577, 1024)\n",
    "        )\n",
    "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        (encoder): CLIPEncoder(\n",
    "            (layers): ModuleList(\n",
    "            (0-23): 24 x CLIPEncoderLayer(\n",
    "                (self_attn): CLIPAttention(\n",
    "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                (mlp): CLIPMLP(\n",
    "                (activation_fn): QuickGELUActivation()\n",
    "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "            )\n",
    "            )\n",
    "        )\n",
    "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        )\n",
    "    )\n",
    "    )\n",
    "    \"\"\"\n",
    "    return vision_tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "iabxtpWflUf9"
   },
   "outputs": [],
   "source": [
    "LlavaMetaModel.get_vision_tower = get_vision_tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "RW9YLSmBT4FG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/model/llava_arch.py\n",
      "def initialize_vision_modules(self, model_args, fsdp=None)\n",
      "model_args\n",
      " ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
      "fsdp\n",
      " []\n",
      "vision_tower from model_args\n",
      " openai/clip-vit-large-patch14-336\n",
      "mm_vision_select_layer from model_args\n",
      " -2\n",
      "mm_vision_select_feature from model_args\n",
      " patch\n",
      "pretrain_mm_mlp_adapter from model_args\n",
      " None\n",
      "self.config.mm_vision_tower\n",
      " openai/clip-vit-large-patch14-336\n",
      "current file path llava/model/llava_arch.py\n",
      "def get_vision_tower(self)\n",
      "vision_tower (raw)\n",
      " None\n",
      "type(vision_tower)\n",
      " <class 'NoneType'>\n",
      "【COND】 type_vision_tower_is_list=False\n",
      "vision_tower (return)\n",
      " None\n",
      "【COND】 self.get_vision_tower()\n",
      " None\n",
      "current file path llava/model/llava_arch.py\n",
      "def get_vision_tower(self)\n",
      "vision_tower (raw)\n",
      " None\n",
      "type(vision_tower)\n",
      " <class 'NoneType'>\n",
      "【COND】 type_vision_tower_is_list=False\n",
      "vision_tower (return)\n",
      " None\n",
      "【COND】 get_vision_tower_is_None=True\n",
      "current file path llava/model/llava_arch.py\n",
      "def get_vision_tower(self)\n",
      "vision_tower (raw)\n",
      " None\n",
      "type(vision_tower)\n",
      " <class 'NoneType'>\n",
      "【COND】 type_vision_tower_is_list=False\n",
      "vision_tower (return)\n",
      " None\n",
      "【ENTER】if self.get_vision_tower() is None:\n",
      "[ENTER] self.get_vision_tower() is None\n",
      "current file path llava/llava/model/multimodal_encoder/builder.py\n",
      "def build_vision_tower(vision_tower_cfg, **kwargs)\n",
      "vision_tower_cfg\n",
      " ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
      "kwargs\n",
      " {}\n",
      "vision_tower from vision_tower_cfg\n",
      " openai/clip-vit-large-patch14-336\n",
      "is_absolute_path_exists\n",
      " False\n",
      "【COND】 is_absolute_path_exists=False vision_tower=openai/clip-vit-large-patch14-336\n",
      "【ENTER】if is_absolute_path_exists or vision_tower.startswith('openai') or vision_tower.startswith('laion') or 'ShareGPT4V' in vision_tower:\n",
      "current file path llava/llava/model/multimodal_encoder/clip_encoder.py\n",
      "def CLIPVisionTower.__init__(self, vision_tower, args, delay_load=False)\n",
      "self\n",
      " <class '__main__.CLIPVisionTower'>\n",
      "vision_tower\n",
      " openai/clip-vit-large-patch14-336\n",
      "args\n",
      " ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
      "delay_load\n",
      " False\n",
      "self.is_loaded\n",
      " False\n",
      "self.vision_tower_name\n",
      " openai/clip-vit-large-patch14-336\n",
      "self.select_layer\n",
      " -2\n",
      "self.select_feature\n",
      " patch\n",
      "【COND】 delay_load=False\n",
      "【ENTER】if not delay_load:\n",
      "current file path llava/llava/model/multimodal_encoder/clip_encoder.py\n",
      "def CLIPVisionTower.load_model(self)\n",
      "self\n",
      " <class '__main__.CLIPVisionTower'>\n",
      "self.vision_tower_name\n",
      " openai/clip-vit-large-patch14-336\n",
      "self.image_processor\n",
      " CLIPImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 336,\n",
      "    \"width\": 336\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 336\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.vision_tower\n",
      " CLIPVisionModel(\n",
      "  (vision_model): CLIPVisionTransformer(\n",
      "    (embeddings): CLIPVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "      (position_embedding): Embedding(577, 1024)\n",
      "    )\n",
      "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "self.is_loaded\n",
      " True\n",
      "result (return)\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "【EXIT】if is_absolute_path_exists or vision_tower.startswith('openai') or vision_tower.startswith('laion') or 'ShareGPT4V' in vision_tower:\n",
      "vision_tower after build_vision_tower\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "【COND】 fsdp\n",
      " []\n",
      "【COND】 fsdp_is_not_None=True len_fsdp=0\n",
      "【COND】 else_fsdp_is_not_None_and_len_fsdp_gt_0=True\n",
      "【ENTER】else of if fsdp is not None and len(fsdp) > 0:\n",
      "self.vision_tower\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "【EXIT】else of if fsdp is not None and len(fsdp) > 0:\n",
      "【EXIT】if self.get_vision_tower() is None:\n",
      "self.config.use_mm_proj set to True\n",
      "self.config.mm_projector_type\n",
      " mlp2x_gelu\n",
      "current file path llava/llava/model/multimodal_encoder/clip_encoder.py\n",
      "def CLIPVisionTower.hidden_size(self)\n",
      "self\n",
      " <class '__main__.CLIPVisionTower'>\n",
      "current file path llava/llava/model/multimodal_encoder/clip_encoder.py\n",
      "def CLIPVisionTower.config(self)\n",
      "self\n",
      " <class '__main__.CLIPVisionTower'>\n",
      "self.is_loaded\n",
      " True\n",
      "【COND】 is_loaded=True\n",
      "【ENTER】if self.is_loaded:\n",
      "result (return)\n",
      " <class 'transformers.models.clip.configuration_clip.CLIPVisionConfig'>\n",
      "【EXIT】if self.is_loaded:\n",
      "result (return)\n",
      " CLIPVisionConfig {\n",
      "  \"_name_or_path\": \"openai/clip-vit-large-patch14-336\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"dropout\": 0.0,\n",
      "  \"hidden_act\": \"quick_gelu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": 336,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"model_type\": \"clip_vision_model\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"patch_size\": 14,\n",
      "  \"projection_dim\": 768,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n",
      "result (return), self.config.hidden_size\n",
      " 1024\n",
      "self.config.mm_hidden_size\n",
      " 1024\n",
      "self.config.mm_vision_select_layer\n",
      " -2\n",
      "self.config.mm_vision_select_feature\n",
      " patch\n",
      "self.config.mm_patch_merge_type\n",
      " flat\n",
      "【COND】 mm_projector_is_None=True\n",
      "【ENTER】if getattr(self, 'mm_projector', None) is None:\n",
      "current file path llava/llava/model/multimodal_projector/builder.py\n",
      "def build_vision_projector(config, delay_load=False, **kwargs)\n",
      "config\n",
      " LlavaConfig {\n",
      "  \"_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mm_hidden_size\": 1024,\n",
      "  \"mm_patch_merge_type\": \"flat\",\n",
      "  \"mm_projector_type\": \"mlp2x_gelu\",\n",
      "  \"mm_vision_select_feature\": \"patch\",\n",
      "  \"mm_vision_select_layer\": -2,\n",
      "  \"mm_vision_tower\": \"openai/clip-vit-large-patch14-336\",\n",
      "  \"model_type\": \"llava_llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_mm_proj\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "delay_load\n",
      " False\n",
      "kwargs\n",
      " {}\n",
      "projector_type from config\n",
      " mlp2x_gelu\n",
      "【COND】 projector_type\n",
      " mlp2x_gelu\n",
      "【COND】mlp_gelu_match\n",
      " <re.Match object; span=(0, 10), match='mlp2x_gelu'>\n",
      "【ENTER】if mlp_gelu_match:\n",
      "mlp_depth from mlp_gelu_match.group(1)\n",
      " 2\n",
      "modules after first Linear\n",
      " [Linear(in_features=1024, out_features=2048, bias=True)]\n",
      "modules before Sequential\n",
      " [Linear(in_features=1024, out_features=2048, bias=True), GELU(approximate='none'), Linear(in_features=2048, out_features=2048, bias=True)]\n",
      "result (return)\n",
      " Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      ")\n",
      "【EXIT】if mlp_gelu_match:\n",
      "self.mm_projector after build_vision_projector\n",
      " Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      ")\n",
      "mm_patch_merge_type\n",
      " flat\n",
      "【COND】 unpad_in_mm_patch_merge_type=False\n",
      "【EXIT】if getattr(self, 'mm_projector', None) is None:\n",
      "【COND】 pretrain_mm_mlp_adapter_is_not_None=False\n"
     ]
    }
   ],
   "source": [
    "initial_model.initialize_vision_modules(\n",
    "    model_args=model_args,\n",
    "    fsdp=training_args.fsdp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "w3-wDbzZfnfD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/model/llava_arch.py\n",
      "class LlavaMetaForCausalLM(ABC).get_vision_tower(self)\n",
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.get_model(self)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "self.model (return)\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      "  (vision_tower): CLIPVisionTower(\n",
      "    (vision_tower): CLIPVisionModel(\n",
      "      (vision_model): CLIPVisionTransformer(\n",
      "        (embeddings): CLIPVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "          (position_embedding): Embedding(577, 1024)\n",
      "        )\n",
      "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder): CLIPEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-23): 24 x CLIPEncoderLayer(\n",
      "              (self_attn): CLIPAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): CLIPMLP(\n",
      "                (activation_fn): QuickGELUActivation()\n",
      "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mm_projector): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n",
      "current file path llava/model/llava_arch.py\n",
      "def get_vision_tower(self)\n",
      "vision_tower (raw)\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "type(vision_tower)\n",
      " <class '__main__.CLIPVisionTower'>\n",
      "【COND】 type_vision_tower_is_list=False\n",
      "vision_tower (return)\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "LlavaMetaForCausalLM(ABC).get_vision_tower(self) result (return)\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "vision_tower\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "data_args.image_processor\n",
      " CLIPImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 336,\n",
      "    \"width\": 336\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 336\n",
      "  }\n",
      "}\n",
      "\n",
      "data_args.is_multimodal\n",
      " True\n",
      "model.config.image_aspect_ratio\n",
      " square\n",
      "model.config.tokenizer_padding_side\n",
      " right\n",
      "model.config.tokenizer_model_max_length\n",
      " 2048\n"
     ]
    }
   ],
   "source": [
    "vision_tower = model.get_vision_tower()\n",
    "print(\"vision_tower\\n\", vision_tower)\n",
    "vision_tower.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)\n",
    "\n",
    "data_args.image_processor = vision_tower.image_processor\n",
    "print(\"data_args.image_processor\\n\", data_args.image_processor)\n",
    "data_args.is_multimodal = True\n",
    "print(\"data_args.is_multimodal\\n\", data_args.is_multimodal) # True\n",
    "\n",
    "model.config.image_aspect_ratio = data_args.image_aspect_ratio\n",
    "print(\"model.config.image_aspect_ratio\\n\", model.config.image_aspect_ratio) # square\n",
    "model.config.tokenizer_padding_side = tokenizer.padding_side\n",
    "print(\"model.config.tokenizer_padding_side\\n\", model.config.tokenizer_padding_side) # right\n",
    "model.config.tokenizer_model_max_length = tokenizer.model_max_length\n",
    "print(\"model.config.tokenizer_model_max_length\\n\", model.config.tokenizer_model_max_length) # 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "fBa5nAaWi1M7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【COND】 tune_mm_mlp_adapter=True\n",
      "【ENTER】if model_args.tune_mm_mlp_adapter:\n",
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.get_model(self)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "self.model (return)\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      "  (vision_tower): CLIPVisionTower(\n",
      "    (vision_tower): CLIPVisionModel(\n",
      "      (vision_model): CLIPVisionTransformer(\n",
      "        (embeddings): CLIPVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "          (position_embedding): Embedding(577, 1024)\n",
      "        )\n",
      "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder): CLIPEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-23): 24 x CLIPEncoderLayer(\n",
      "              (self_attn): CLIPAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): CLIPMLP(\n",
      "                (activation_fn): QuickGELUActivation()\n",
      "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mm_projector): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n",
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.get_model(self)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "self.model (return)\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      "  (vision_tower): CLIPVisionTower(\n",
      "    (vision_tower): CLIPVisionModel(\n",
      "      (vision_model): CLIPVisionTransformer(\n",
      "        (embeddings): CLIPVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "          (position_embedding): Embedding(577, 1024)\n",
      "        )\n",
      "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder): CLIPEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-23): 24 x CLIPEncoderLayer(\n",
      "              (self_attn): CLIPAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): CLIPMLP(\n",
      "                (activation_fn): QuickGELUActivation()\n",
      "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mm_projector): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n",
      "model.get_model().mm_projector.parameters() <generator object Module.parameters at 0x7efff1193840>\n",
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.get_model(self)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "self.model (return)\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      "  (vision_tower): CLIPVisionTower(\n",
      "    (vision_tower): CLIPVisionModel(\n",
      "      (vision_model): CLIPVisionTransformer(\n",
      "        (embeddings): CLIPVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "          (position_embedding): Embedding(577, 1024)\n",
      "        )\n",
      "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder): CLIPEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-23): 24 x CLIPEncoderLayer(\n",
      "              (self_attn): CLIPAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): CLIPMLP(\n",
      "                (activation_fn): QuickGELUActivation()\n",
      "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mm_projector): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n",
      "model.get_model().mm_projector.parameters() <generator object Module.parameters at 0x7efff1193840>\n",
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.get_model(self)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "self.model (return)\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      "  (vision_tower): CLIPVisionTower(\n",
      "    (vision_tower): CLIPVisionModel(\n",
      "      (vision_model): CLIPVisionTransformer(\n",
      "        (embeddings): CLIPVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "          (position_embedding): Embedding(577, 1024)\n",
      "        )\n",
      "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder): CLIPEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-23): 24 x CLIPEncoderLayer(\n",
      "              (self_attn): CLIPAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): CLIPMLP(\n",
      "                (activation_fn): QuickGELUActivation()\n",
      "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mm_projector): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n",
      "model.get_model().mm_projector.parameters() <generator object Module.parameters at 0x7efff1193840>\n",
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.get_model(self)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "self.model (return)\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      "  (vision_tower): CLIPVisionTower(\n",
      "    (vision_tower): CLIPVisionModel(\n",
      "      (vision_model): CLIPVisionTransformer(\n",
      "        (embeddings): CLIPVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "          (position_embedding): Embedding(577, 1024)\n",
      "        )\n",
      "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder): CLIPEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-23): 24 x CLIPEncoderLayer(\n",
      "              (self_attn): CLIPAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): CLIPMLP(\n",
      "                (activation_fn): QuickGELUActivation()\n",
      "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mm_projector): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n",
      "model.get_model().mm_projector.parameters() <generator object Module.parameters at 0x7efff1193840>\n",
      "【EXIT】if model_args.tune_mm_mlp_adapter:\n"
     ]
    }
   ],
   "source": [
    "model.config.tune_mm_mlp_adapter = training_args.tune_mm_mlp_adapter = model_args.tune_mm_mlp_adapter\n",
    "print(f\"【COND】 tune_mm_mlp_adapter={model_args.tune_mm_mlp_adapter}\") # True\n",
    "if model_args.tune_mm_mlp_adapter:\n",
    "    # 【ENTER】 tune_mm_mlp_adapter=True なので、この分岐に入る\n",
    "    print(\"【ENTER】if model_args.tune_mm_mlp_adapter:\")\n",
    "    # モデル全体の全パラメータを「学習不可（requires_grad=False）」にする\n",
    "    # これで通常の重みは全て凍結される\n",
    "    model.requires_grad_(False)\n",
    "    for p in model.get_model().mm_projector.parameters():\n",
    "        # mm_projector（画像特徴量→テキスト特徴量への変換層）の全パラメータだけを「学習可能（requires_grad=True）」に戻す\n",
    "        # これで mm_projector のみ学習されることになる\n",
    "        print(\"model.get_model().mm_projector.parameters()\", model.get_model().mm_projector.parameters())\n",
    "        p.requires_grad = True\n",
    "    print(\"【EXIT】if model_args.tune_mm_mlp_adapter:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "-lsCavfasZCY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【COND】 freeze_mm_mlp_adapter=False\n",
      "【COND】 bits=16\n"
     ]
    }
   ],
   "source": [
    "model.config.freeze_mm_mlp_adapter = training_args.freeze_mm_mlp_adapter\n",
    "print(f\"【COND】 freeze_mm_mlp_adapter={training_args.freeze_mm_mlp_adapter}\") # False\n",
    "if training_args.freeze_mm_mlp_adapter:\n",
    "  pass\n",
    "\n",
    "print(f\"【COND】 bits={training_args.bits}\") # 16\n",
    "if training_args.bits in [4, 8]:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "VjMWFAC_um1_"
   },
   "outputs": [],
   "source": [
    "def initialize_vision_tokenizer(self, model_args, tokenizer):\n",
    "    print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "    print(\"def initialize_vision_tokenizer(self, model_args, tokenizer)\")\n",
    "    print(\"model_args\\n\", model_args) # ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
    "    print(\"tokenizer\\n\", tokenizer) # LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)\n",
    "\n",
    "    print(f\"【COND】 mm_use_im_patch_token={model_args.mm_use_im_patch_token}\") # False\n",
    "    if model_args.mm_use_im_patch_token:\n",
    "      pass\n",
    "\n",
    "    if model_args.mm_use_im_start_end: # False\n",
    "      pass\n",
    "\n",
    "    elif model_args.mm_use_im_patch_token: # False\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "WnJGUH-zu5lX"
   },
   "outputs": [],
   "source": [
    "LlavaLlamaForCausalLM.initialize_vision_tokenizer = initialize_vision_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "lj1_WjcpsnXB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_args.mm_use_im_start_end False\n",
      "training_args.mm_projector_lr None\n",
      "training_args.use_im_start_end False\n",
      "model_args.mm_use_im_patch_token False\n",
      "current file path llava/model/llava_arch.py\n",
      "def initialize_vision_tokenizer(self, model_args, tokenizer)\n",
      "model_args\n",
      " ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
      "tokenizer\n",
      " LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)\n",
      "【COND】 mm_use_im_patch_token=False\n",
      "【EXIT】if model_args.vision_tower is not None:\n"
     ]
    }
   ],
   "source": [
    "model.config.mm_use_im_start_end = data_args.mm_use_im_start_end = model_args.mm_use_im_start_end\n",
    "print(\"model_args.mm_use_im_start_end\", model_args.mm_use_im_start_end)\n",
    "model.config.mm_projector_lr = training_args.mm_projector_lr\n",
    "print(\"training_args.mm_projector_lr\", training_args.mm_projector_lr)\n",
    "training_args.use_im_start_end = model_args.mm_use_im_start_end\n",
    "print(\"training_args.use_im_start_end\", training_args.use_im_start_end)\n",
    "model.config.mm_use_im_patch_token = model_args.mm_use_im_patch_token\n",
    "print(\"model_args.mm_use_im_patch_token\", model_args.mm_use_im_patch_token)\n",
    "model.initialize_vision_tokenizer(model_args, tokenizer=tokenizer)\n",
    "print(\"【EXIT】if model_args.vision_tower is not None:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "zLUf4wqcxKI4"
   },
   "outputs": [],
   "source": [
    "def rank0_print(*args):\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def rank0_print(*args)\")\n",
    "    print(\"args\\n\", args) # ('Formatting inputs...Skip in lazy mode',)\n",
    "    if local_rank == 0:\n",
    "        print(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "839mYcaPw2CZ"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class LazySupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str,\n",
    "                 tokenizer: transformers.PreTrainedTokenizer,\n",
    "                 data_args: DataArguments):\n",
    "\n",
    "        print(\"current file path\", \"llava/train/train.py\")\n",
    "        print(\"def LazySupervisedDataset.__init__(self, data_path, tokenizer, data_args)\")\n",
    "        print(\"data_path\\n\", data_path) # /content/LLaVA/blip_laion_cc_sbu_1.json\n",
    "        print(\"tokenizer\\n\", type(tokenizer)) # <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
    "        print(\"data_args\\n\", data_args) # DataArguments(data_path='/content/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/LLaVA/images', image_aspect_ratio='square')\n",
    "        super(LazySupervisedDataset, self).__init__()\n",
    "        list_data_dict = json.load(open(data_path, \"r\"))\n",
    "        # 今回は1サンプルだけなのでprintしても危険ではない\n",
    "        print(\"list_data_dict\", list_data_dict)\n",
    "\n",
    "        rank0_print(\"Formatting inputs...Skip in lazy mode\") # Formatting inputs...Skip in lazy mode\n",
    "        self.tokenizer = tokenizer\n",
    "        print(\"self.tokenizer\\n\", self.tokenizer)\n",
    "        self.list_data_dict = list_data_dict\n",
    "        print(\"self.list_data_dict\\n\", self.list_data_dict)\n",
    "        self.data_args = data_args\n",
    "        print(\"self.data_args\\n\", self.data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "hv3nmqgMwVWa"
   },
   "outputs": [],
   "source": [
    "def __len__(self):\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def LazySupervisedDataset.__len__(self)\")\n",
    "    return len(self.list_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "64qMycFgwWrA"
   },
   "outputs": [],
   "source": [
    "LazySupervisedDataset.__len__ = __len__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "K2Th7s50xen5"
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from typing import Dict\n",
    "\n",
    "# Trainer > def _get_dataloader > dataloader_params = {...\"collate_fn\": data_collator,...}\n",
    "# self.accelerator.prepare(DataLoader(dataset, **dataloader_params)) で呼ばれる\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        print(\"current file path\", \"llava/train/train.py\")\n",
    "        print(\"def DataCollatorForSupervisedDataset.__call__(self, instances)\")\n",
    "        print(\"instances\\n\", instances)\n",
    "        #  [(torch.Size([24]), torch.Size([24]), torch.Size([3, 336, 336]))]\n",
    "        print(\"shape of each instance's input_ids and labels, and images(if any):\", [(x['input_ids'].shape, x['labels'].shape, x.get('image', None).shape if 'image' in x else None) for x in instances])\n",
    "        # データローダーが None を返すことがあるので、Noneのサンプルを除外。\n",
    "        instances = [x for x in instances if x is not None]\n",
    "        # input_idsとlabelsのそれぞれについてリストを作成。タプルをつくる。\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances]\n",
    "                                  for key in (\"input_ids\", \"labels\"))\n",
    "        # input_idsはtokenizerのpad_token_id(0)でパディング\n",
    "        print(\"self.tokenizer.pad_token_id\\n\", self.tokenizer.pad_token_id)\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids,\n",
    "            batch_first=True,\n",
    "            padding_value=self.tokenizer.pad_token_id)\n",
    "        # labelsはIGNORE_INDEX(-100)でパディング\n",
    "        print(\"IGNORE_INDEX\\n\", IGNORE_INDEX)\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels,\n",
    "                                                 batch_first=True,\n",
    "                                                 padding_value=IGNORE_INDEX)\n",
    "        input_ids = input_ids[:, :self.tokenizer.model_max_length]\n",
    "        print(\"input_ids.shape (after pad_sequence and truncate)\\n\", input_ids.shape)\n",
    "        print(\"input_ids (after pad_sequence and truncate)\\n\", input_ids)\n",
    "        labels = labels[:, :self.tokenizer.model_max_length]\n",
    "        print(\"labels.shape (after pad_sequence and truncate)\\n\", labels.shape)\n",
    "        print(\"labels (after pad_sequence and truncate)\\n\", labels)\n",
    "        # .ne() は \"not equal\" → pad_token_id(=0) じゃない部分を 1、pad 部分を 0 にする。モデルが pad 部分を読まないように制御するマスクです。\n",
    "        batch = dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "        if 'image' in instances[0]:\n",
    "            images = [instance['image'] for instance in instances]\n",
    "            if all(x is not None and x.shape == images[0].shape for x in images):\n",
    "                batch['images'] = torch.stack(images)\n",
    "            else:\n",
    "                batch['images'] = images\n",
    "            print(\"batch['images'].shape\\n\", batch['images'].shape)\n",
    "        \n",
    "        print(\"batch (return)\\n\", batch)\n",
    "        print(\"shape of each batch's input_ids and labels, and images(if any):\", [(batch['input_ids'].shape, batch['labels'].shape, batch.get('images', None).shape if 'images' in batch else None)])\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "QjNj6B85wWA6"
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer,\n",
    "                                data_args) -> Dict:\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def make_supervised_data_module(tokenizer, data_args)\")\n",
    "    print(\"tokenizer\\n\", type(tokenizer))\n",
    "    print(\"data_args\\n\", data_args) #  DataArguments(data_path='/content/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/LLaVA/images', image_aspect_ratio='square')\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    train_dataset = LazySupervisedDataset(tokenizer=tokenizer,\n",
    "                                data_path=data_args.data_path,\n",
    "                                data_args=data_args)\n",
    "    print(\"train_dataset\\n\", train_dataset) # <llava.train.train.LazySupervisedDataset object at 0x7ed6341f4880>\n",
    "    print(\"len(train_dataset)\\n\", len(train_dataset)) # 1\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "    print(\"data_collator\\n\", data_collator) # DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))\n",
    "    result = dict(train_dataset=train_dataset,\n",
    "                  eval_dataset=None,\n",
    "                  data_collator=data_collator)\n",
    "    print(\"def make_supervised_data_module: result (return)\\n\", result) # {'train_dataset': <llava.train.train.LazySupervisedDataset object at 0x7ed6341f4880>, 'eval_dataset': None, 'data_collator': DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TlaRN8vG5p99",
    "outputId": "a52c09b2-84d7-4d0b-b5b1-1f94af461408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/train/train.py\n",
      "def make_supervised_data_module(tokenizer, data_args)\n",
      "tokenizer\n",
      " <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "data_args\n",
      " DataArguments(data_path='/workspaces/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')\n",
      "current file path llava/train/train.py\n",
      "def LazySupervisedDataset.__init__(self, data_path, tokenizer, data_args)\n",
      "data_path\n",
      " /workspaces/LLaVA/blip_laion_cc_sbu_1.json\n",
      "tokenizer\n",
      " <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "data_args\n",
      " DataArguments(data_path='/workspaces/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')\n",
      "list_data_dict [{'id': '000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Give a brief description of the image.\\n<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]}]\n",
      "current file path llava/train/train.py\n",
      "def rank0_print(*args)\n",
      "args\n",
      " ('Formatting inputs...Skip in lazy mode',)\n",
      "Formatting inputs...Skip in lazy mode\n",
      "self.tokenizer\n",
      " LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)\n",
      "self.list_data_dict\n",
      " [{'id': '000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Give a brief description of the image.\\n<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]}]\n",
      "self.data_args\n",
      " DataArguments(data_path='/workspaces/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')\n",
      "train_dataset\n",
      " <__main__.LazySupervisedDataset object at 0x7eff83b88730>\n",
      "current file path llava/train/train.py\n",
      "def LazySupervisedDataset.__len__(self)\n",
      "len(train_dataset)\n",
      " 1\n",
      "data_collator\n",
      " DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))\n",
      "def make_supervised_data_module: result (return)\n",
      " {'train_dataset': <__main__.LazySupervisedDataset object at 0x7eff83b88730>, 'eval_dataset': None, 'data_collator': DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))}\n",
      "data_module\n",
      " {'train_dataset': <__main__.LazySupervisedDataset object at 0x7eff83b88730>, 'eval_dataset': None, 'data_collator': DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))}\n"
     ]
    }
   ],
   "source": [
    "data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n",
    "print(\"data_module\\n\", data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "id": "z8F6sqQdxsMO",
    "outputId": "92194c34-0765-4928-9cda-23c51af0a337"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from transformers.trainer import (\n",
    "    is_sagemaker_mp_enabled,\n",
    "    get_parameter_names,\n",
    "    has_length,\n",
    "    ALL_LAYERNORM_LAYERS,\n",
    "    ShardedDDPOption,\n",
    "    logger,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None):\n",
    "\n",
    "    print(\"current file path\", \"llava/mm_utils.py\")\n",
    "    print(\"def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None)\")\n",
    "    print(\"prompt\\n\", prompt) # <image>the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair\n",
    "    print(\"tokenizer\\n\", tokenizer) #  LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)\n",
    "    print(\"image_token_index\\n\", image_token_index) # -200\n",
    "    print(\"return_tensors\\n\", return_tensors) # pt\n",
    "    prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split('<image>')]\n",
    "\n",
    "    def insert_separator(X, sep):\n",
    "        return [ele for sublist in zip(X, [sep]*len(X)) for ele in sublist][:-1]\n",
    "\n",
    "    input_ids = []\n",
    "    offset = 0\n",
    "    if len(prompt_chunks) > 0 and len(prompt_chunks[0]) > 0 and prompt_chunks[0][0] == tokenizer.bos_token_id:\n",
    "        offset = 1\n",
    "        input_ids.append(prompt_chunks[0][0])\n",
    "\n",
    "    for x in insert_separator(prompt_chunks, [image_token_index] * (offset + 1)):\n",
    "        input_ids.extend(x[offset:])\n",
    "\n",
    "    if return_tensors is not None:\n",
    "        if return_tensors == 'pt':\n",
    "            return torch.tensor(input_ids, dtype=torch.long)\n",
    "        raise ValueError(f'Unsupported tensor type: {return_tensors}')\n",
    "    print(\"input_ids (return)\\n\", input_ids)\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def preprocess_plain(\n",
    "    sources: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def preprocess_plain(sources, tokenizer)\")\n",
    "    print(\"sources\\n\", sources) # [[{'from': 'human', 'value': '<image>\\nGive a brief description of the image.'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]\n",
    "    print(\"tokenizer\\n\", type(tokenizer)) # <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
    "    # add end signal and concatenate together\n",
    "    conversations = []\n",
    "    print(\"conversations initial\\n\", conversations) # []\n",
    "    for source in sources:\n",
    "        print(\"source current loop\\n\", source) \n",
    "        assert len(source) == 2\n",
    "        assert DEFAULT_IMAGE_TOKEN in source[0]['value']\n",
    "        source[0]['value'] = DEFAULT_IMAGE_TOKEN\n",
    "        conversation = source[0]['value'] + source[1]['value'] + default_conversation.sep\n",
    "        print(\"conversation current loop\\n\", conversation)\n",
    "        conversations.append(conversation)\n",
    "    print(\"conversations (final)\\n\", conversations) #  ['<image>the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair\\n']\n",
    "    # tokenize conversations\n",
    "    input_ids = [tokenizer_image_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations]\n",
    "    print(\"input_ids\\n\", input_ids) # [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114, 411,  2654, 11315,    13])]\n",
    "    for idx, tensor in enumerate(input_ids):\n",
    "        if hasattr(tensor, 'shape'):\n",
    "            print(f\"input_ids[{idx}].shape\\n\", tensor.shape) # torch.Size([24])\n",
    "    targets = copy.deepcopy(input_ids)\n",
    "    print(\"targets\\n\", targets) # [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114, 411,  2654, 11315,    13])]\n",
    "    for idx, tensor in enumerate(targets):\n",
    "        if hasattr(tensor, 'shape'):\n",
    "            print(f\"targets[{idx}].shape\\n\", tensor.shape) # torch.Size([24])\n",
    "    print(\"sources\\n\", sources) # [[{'from': 'human', 'value': '<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]\n",
    "    for target, source in zip(targets, sources):\n",
    "        tokenized_len = len(tokenizer_image_token(source[0]['value'], tokenizer)) # prompt <image>\n",
    "        target[:tokenized_len] = IGNORE_INDEX\n",
    "\n",
    "    print(\"input_ids (return)\\n\", input_ids) # [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114, 411,  2654, 11315,    13])]\n",
    "    print(\"targets (return)\\n\", targets) #  [tensor([ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114, 411,  2654, 11315,    13])]\n",
    "    return dict(input_ids=input_ids, labels=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_speaker_and_signal(header, source, get_conversation=True):\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def _add_speaker_and_signal(header, source, get_conversation=True)\")\n",
    "    print(\"header\\n\", header)\n",
    "    print(\"source\\n\", source)\n",
    "    print(\"get_conversation\\n\", get_conversation)\n",
    "    \"\"\"Add speaker and start/end signal on each round.\"\"\"\n",
    "    BEGIN_SIGNAL = \"### \"\n",
    "    END_SIGNAL = \"\\n\"\n",
    "    conversation = header\n",
    "    for sentence in source:\n",
    "        from_str = sentence[\"from\"]\n",
    "        if from_str.lower() == \"human\":\n",
    "            from_str = default_conversation.roles[0]\n",
    "        elif from_str.lower() == \"gpt\":\n",
    "            from_str = default_conversation.roles[1]\n",
    "        else:\n",
    "            from_str = 'unknown'\n",
    "        sentence[\"value\"] = (BEGIN_SIGNAL + from_str + \": \" +\n",
    "                             sentence[\"value\"] + END_SIGNAL)\n",
    "        if get_conversation:\n",
    "            conversation += sentence[\"value\"]\n",
    "    conversation += BEGIN_SIGNAL\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_fn(strings: Sequence[str],\n",
    "                 tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def _tokenize_fn(strings, tokenizer)\")\n",
    "    print(\"strings\\n\", strings)\n",
    "    print(\"tokenizer\\n\", type(tokenizer))\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        ) for text in strings\n",
    "    ]\n",
    "    input_ids = labels = [\n",
    "        tokenized.input_ids[0] for tokenized in tokenized_list\n",
    "    ]\n",
    "    for idx, tensor in enumerate(input_ids):\n",
    "        if hasattr(tensor, 'shape'):\n",
    "            print(f\"input_ids[{idx}].shape\\n\", tensor.shape)\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item()\n",
    "        for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mask_targets(target, tokenized_lens, speakers):\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def _mask_targets(target, tokenized_lens, speakers)\")\n",
    "    print(\"target\\n\", target)\n",
    "    print(\"tokenized_lens\\n\", tokenized_lens)\n",
    "    print(\"speakers\\n\", speakers)\n",
    "    # cur_idx = 0\n",
    "    cur_idx = tokenized_lens[0]\n",
    "    tokenized_lens = tokenized_lens[1:]\n",
    "    target[:cur_idx] = IGNORE_INDEX\n",
    "    for tokenized_len, speaker in zip(tokenized_lens, speakers):\n",
    "        if speaker == \"human\":\n",
    "            target[cur_idx+2:cur_idx + tokenized_len] = IGNORE_INDEX\n",
    "        cur_idx += tokenized_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    has_image: bool = False\n",
    ") -> Dict:\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def preprocess(sources, tokenizer, has_image=False)\")\n",
    "    print(\"sources\\n\", sources) # [[{'from': 'human', 'value': '<image>\\nGive a brief description of the image.'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]\n",
    "    print(\"tokenizer\\n\", type(tokenizer)) # <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
    "    print(\"has_image\\n\", has_image) # True\n",
    "    \"\"\"\n",
    "    Given a list of sources, each is a conversation list. This transform:\n",
    "    1. Add signal '### ' at the beginning each sentence, with end signal '\\n';\n",
    "    2. Concatenate conversations together;\n",
    "    3. Tokenize the concatenated conversation;\n",
    "    4. Make a deepcopy as the target. Mask human words with IGNORE_INDEX.\n",
    "    \"\"\"\n",
    "    if default_conversation.sep_style == SeparatorStyle.PLAIN:\n",
    "        return preprocess_plain(sources, tokenizer) # True\n",
    "    # add end signal and concatenate together\n",
    "    conversations = []\n",
    "    for source in sources:\n",
    "        header = f\"{default_conversation.system}\\n\\n\"\n",
    "        conversation = _add_speaker_and_signal(header, source)\n",
    "        conversations.append(conversation)\n",
    "    # tokenize conversations\n",
    "    def get_tokenize_len(prompts):\n",
    "        return [len(tokenizer_image_token(prompt, tokenizer)) for prompt in prompts]\n",
    "\n",
    "    if has_image:\n",
    "        input_ids = [tokenizer_image_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations]\n",
    "        for idx, tensor in enumerate(input_ids):\n",
    "            if hasattr(tensor, 'shape'):\n",
    "                print(f\"input_ids[{idx}].shape\\n\", tensor.shape)\n",
    "    else:\n",
    "        conversations_tokenized = _tokenize_fn(conversations, tokenizer)\n",
    "        input_ids = conversations_tokenized[\"input_ids\"]\n",
    "\n",
    "    targets = copy.deepcopy(input_ids)\n",
    "    if isinstance(targets, list):\n",
    "        for idx, tensor in enumerate(targets):\n",
    "            if hasattr(tensor, 'shape'):\n",
    "                print(f\"targets[{idx}].shape\\n\", tensor.shape)\n",
    "    elif hasattr(targets, 'shape'):\n",
    "        print(\"targets.shape\\n\", targets.shape)\n",
    "    for target, source in zip(targets, sources):\n",
    "        if has_image:\n",
    "            tokenized_lens = get_tokenize_len([header] + [s[\"value\"] for s in source])\n",
    "        else:\n",
    "            tokenized_lens = _tokenize_fn([header] + [s[\"value\"] for s in source], tokenizer)[\"input_ids_lens\"]\n",
    "        speakers = [sentence[\"from\"] for sentence in source]\n",
    "        _mask_targets(target, tokenized_lens, speakers)\n",
    "\n",
    "    print(\"return dict(input_ids=input_ids, labels=targets)\\n\", dict(input_ids=input_ids, labels=targets))\n",
    "    return dict(input_ids=input_ids, labels=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_multimodal(\n",
    "    sources: Sequence[str],\n",
    "    data_args: DataArguments\n",
    ") -> Dict:\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def preprocess_multimodal(sources, data_args)\")\n",
    "    print(\"sources\\n\", sources) # [[{'from': 'human', 'value': 'Give a brief description of the image.\\n<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]\n",
    "    print(\"data_args\\n\", data_args) # DataArguments(data_path='/content/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/LLaVA/images', image_aspect_ratio='square')\n",
    "    is_multimodal = data_args.is_multimodal \n",
    "    print(\"is_multimodal\\n\", is_multimodal) # True\n",
    "    if not is_multimodal:\n",
    "        pass\n",
    "\n",
    "    for source in sources:\n",
    "        print(\"source current loop\\n\", source)\n",
    "        for sentence in source:\n",
    "            print(\"sentence current loop\\n\", sentence)\n",
    "            print(\"【COND】 if DEFAULT_IMAGE_TOKEN in sentence['value']:\", DEFAULT_IMAGE_TOKEN in sentence['value'])\n",
    "            print(\"sentence['value']\\n\", sentence['value'])\n",
    "            print(\"DEFAULT_IMAGE_TOKEN\\n\", DEFAULT_IMAGE_TOKEN)\n",
    "            if DEFAULT_IMAGE_TOKEN in sentence['value']:\n",
    "                print(\"【ENTER】if DEFAULT_IMAGE_TOKEN in sentence['value']:\")\n",
    "                sentence['value'] = sentence['value'].replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
    "                sentence['value'] = DEFAULT_IMAGE_TOKEN + '\\n' + sentence['value']\n",
    "                sentence['value'] = sentence['value'].strip()\n",
    "                if \"mmtag\" in default_conversation.version:\n",
    "                    sentence['value'] = sentence['value'].replace(DEFAULT_IMAGE_TOKEN, '<Image>' + DEFAULT_IMAGE_TOKEN + '</Image>')\n",
    "            replace_token = DEFAULT_IMAGE_TOKEN\n",
    "            if data_args.mm_use_im_start_end:\n",
    "                replace_token = DEFAULT_IM_START_TOKEN + replace_token + DEFAULT_IM_END_TOKEN\n",
    "            sentence[\"value\"] = sentence[\"value\"].replace(DEFAULT_IMAGE_TOKEN, replace_token)\n",
    "    print(\"sources (final return)\\n\", sources)\n",
    "    return sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from PIL import Image\n",
    "\n",
    "# Trainer > def _get_dataloader > dataloader = self.accelerator.prepare(DataLoader(dataset, **dataloader_params))\n",
    "def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def LazySupervisedDataset.__getitem__(self, i)\")\n",
    "    print(\"i\\n\", i) # 0\n",
    "    sources = self.list_data_dict[i]\n",
    "    print(\"sources\\n\", sources)\n",
    "    print(\"【COND】 isinstance(i, int):\", isinstance(i, int))\n",
    "    if isinstance(i, int):\n",
    "        print(\"【ENTER】if isinstance(i, int):\")\n",
    "        sources = [sources]\n",
    "        print(\"sources (after)\\n\", sources)\n",
    "        print(\"【EXIT】if isinstance(i, int):\")\n",
    "    assert len(sources) == 1, \"Don't know why it is wrapped to a list\"  # FIXME\n",
    "    print(\"【COND】 'image' in sources[0]:\", 'image' in sources[0])\n",
    "    if 'image' in sources[0]:\n",
    "        print(\"【ENTER】if 'image' in sources[0]:\")\n",
    "        image_file = self.list_data_dict[i]['image']\n",
    "        print(\"image_file\\n\", image_file)\n",
    "        image_folder = self.data_args.image_folder\n",
    "        print(\"image_folder\\n\", image_folder)\n",
    "        processor = self.data_args.image_processor\n",
    "        print(\"processor\\n\", processor)\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        print(\"image_path\\n\", image_path)\n",
    "        try:\n",
    "            print(\"Trying to open image...\")\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            print(\"Image opened successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image: {e}\")\n",
    "            # 画像がなければこのサンプルはスキップ\n",
    "            print(\"Skipping this sample due to image loading error.\")\n",
    "            return None \n",
    "        print(\"【COND】 self.data_args.image_aspect_ratio\", self.data_args.image_aspect_ratio) # square\n",
    "        if self.data_args.image_aspect_ratio == 'pad':\n",
    "            pass\n",
    "        else:\n",
    "            print(\"【ENTER】else (self.data_args.image_aspect_ratio != 'pad')\")\n",
    "            print(\"image (before)\\n\", image)\n",
    "            image = processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n",
    "            print(\"image (after processor.preprocess)\\n\", image)\n",
    "        print(\"sources (before preprocess_multimodal)\\n\", sources)\n",
    "        sources = preprocess_multimodal(\n",
    "            copy.deepcopy([e[\"conversations\"] for e in sources]),\n",
    "            self.data_args)\n",
    "        print(\"sources (after preprocess_multimodal)\\n\", sources)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print(\"Calling preprocess...\")\n",
    "    data_dict = preprocess(\n",
    "        sources,\n",
    "        self.tokenizer,\n",
    "        has_image=('image' in self.list_data_dict[i]))\n",
    "    print(\"data_dict (after preprocess)\\n\", data_dict)\n",
    "    print(\"【COND】 isinstance(i, int):\", isinstance(i, int))\n",
    "    if isinstance(i, int):\n",
    "        data_dict = dict(input_ids=data_dict[\"input_ids\"][0],\n",
    "                            labels=data_dict[\"labels\"][0])\n",
    "\n",
    "    # image exist in the data\n",
    "    if 'image' in self.list_data_dict[i]:\n",
    "        data_dict['image'] = image\n",
    "    elif self.data_args.is_multimodal:\n",
    "        # image does not exist in the data, but the model is multimodal\n",
    "        crop_size = self.data_args.image_processor.crop_size\n",
    "        data_dict['image'] = torch.zeros(3, crop_size['height'], crop_size['width'])\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "LazySupervisedDataset.__getitem__ = __getitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer\n",
      " LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False)}, clean_up_tokenization_spaces=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    model_max_length=training_args.model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "\n",
    "print(\"tokenizer\\n\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/train/train.py\n",
      "def LazySupervisedDataset.__init__(self, data_path, tokenizer, data_args)\n",
      "data_path\n",
      " /workspaces/LLaVA/blip_laion_cc_sbu_1.json\n",
      "tokenizer\n",
      " <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "data_args\n",
      " DataArguments(data_path='/workspaces/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')\n",
      "list_data_dict [{'id': '000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Give a brief description of the image.\\n<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]}]\n",
      "current file path llava/train/train.py\n",
      "def rank0_print(*args)\n",
      "args\n",
      " ('Formatting inputs...Skip in lazy mode',)\n",
      "Formatting inputs...Skip in lazy mode\n",
      "self.tokenizer\n",
      " LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False)}, clean_up_tokenization_spaces=False)\n",
      "self.list_data_dict\n",
      " [{'id': '000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Give a brief description of the image.\\n<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]}]\n",
      "self.data_args\n",
      " DataArguments(data_path='/workspaces/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')\n"
     ]
    }
   ],
   "source": [
    "train_dataset = LazySupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path, data_args=data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/train/train.py\n",
      "def LazySupervisedDataset.__getitem__(self, i)\n",
      "i\n",
      " 0\n",
      "sources\n",
      " {'id': '000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Give a brief description of the image.\\n<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]}\n",
      "【COND】 isinstance(i, int): True\n",
      "【ENTER】if isinstance(i, int):\n",
      "sources (after)\n",
      " [{'id': '000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Give a brief description of the image.\\n<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]}]\n",
      "【EXIT】if isinstance(i, int):\n",
      "【COND】 'image' in sources[0]: True\n",
      "【ENTER】if 'image' in sources[0]:\n",
      "image_file\n",
      " GCC_train_000406392.jpg\n",
      "image_folder\n",
      " /workspaces/LLaVA/images/\n",
      "processor\n",
      " CLIPImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 336,\n",
      "    \"width\": 336\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 336\n",
      "  }\n",
      "}\n",
      "\n",
      "image_path\n",
      " /workspaces/LLaVA/images/GCC_train_000406392.jpg\n",
      "Trying to open image...\n",
      "Image opened successfully.\n",
      "【COND】 self.data_args.image_aspect_ratio square\n",
      "【ENTER】else (self.data_args.image_aspect_ratio != 'pad')\n",
      "image (before)\n",
      " <PIL.Image.Image image mode=RGB size=224x224 at 0x7EFF83B8B340>\n",
      "image (after processor.preprocess)\n",
      " tensor([[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "         [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "         [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "         ...,\n",
      "         [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "        [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "         [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "         [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "         ...,\n",
      "         [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "        [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "         [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "         [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "         ...,\n",
      "         [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "         [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "         [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]])\n",
      "sources (before preprocess_multimodal)\n",
      " [{'id': '000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Give a brief description of the image.\\n<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]}]\n",
      "current file path llava/train/train.py\n",
      "def preprocess_multimodal(sources, data_args)\n",
      "sources\n",
      " [[{'from': 'human', 'value': 'Give a brief description of the image.\\n<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]\n",
      "data_args\n",
      " DataArguments(data_path='/workspaces/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')\n",
      "is_multimodal\n",
      " True\n",
      "source current loop\n",
      " [{'from': 'human', 'value': 'Give a brief description of the image.\\n<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]\n",
      "sentence current loop\n",
      " {'from': 'human', 'value': 'Give a brief description of the image.\\n<image>'}\n",
      "【COND】 if DEFAULT_IMAGE_TOKEN in sentence['value']: True\n",
      "sentence['value']\n",
      " Give a brief description of the image.\n",
      "<image>\n",
      "DEFAULT_IMAGE_TOKEN\n",
      " <image>\n",
      "【ENTER】if DEFAULT_IMAGE_TOKEN in sentence['value']:\n",
      "sentence current loop\n",
      " {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}\n",
      "【COND】 if DEFAULT_IMAGE_TOKEN in sentence['value']: False\n",
      "sentence['value']\n",
      " the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair\n",
      "DEFAULT_IMAGE_TOKEN\n",
      " <image>\n",
      "sources (final return)\n",
      " [[{'from': 'human', 'value': '<image>\\nGive a brief description of the image.'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]\n",
      "sources (after preprocess_multimodal)\n",
      " [[{'from': 'human', 'value': '<image>\\nGive a brief description of the image.'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]\n",
      "Calling preprocess...\n",
      "current file path llava/train/train.py\n",
      "def preprocess(sources, tokenizer, has_image=False)\n",
      "sources\n",
      " [[{'from': 'human', 'value': '<image>\\nGive a brief description of the image.'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]\n",
      "tokenizer\n",
      " <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "has_image\n",
      " True\n",
      "current file path llava/train/train.py\n",
      "def preprocess_plain(sources, tokenizer)\n",
      "sources\n",
      " [[{'from': 'human', 'value': '<image>\\nGive a brief description of the image.'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]\n",
      "tokenizer\n",
      " <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "conversations initial\n",
      " []\n",
      "source current loop\n",
      " [{'from': 'human', 'value': '<image>\\nGive a brief description of the image.'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]\n",
      "conversation current loop\n",
      " <image>the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair\n",
      "\n",
      "conversations (final)\n",
      " ['<image>the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair\\n']\n",
      "current file path llava/mm_utils.py\n",
      "def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None)\n",
      "prompt\n",
      " <image>the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair\n",
      "\n",
      "tokenizer\n",
      " LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False)}, clean_up_tokenization_spaces=False)\n",
      "image_token_index\n",
      " -200\n",
      "return_tensors\n",
      " pt\n",
      "input_ids\n",
      " [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "          411,  2654, 11315,    13])]\n",
      "input_ids[0].shape\n",
      " torch.Size([24])\n",
      "targets\n",
      " [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "          411,  2654, 11315,    13])]\n",
      "targets[0].shape\n",
      " torch.Size([24])\n",
      "sources\n",
      " [[{'from': 'human', 'value': '<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]\n",
      "current file path llava/mm_utils.py\n",
      "def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None)\n",
      "prompt\n",
      " <image>\n",
      "tokenizer\n",
      " LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False)}, clean_up_tokenization_spaces=False)\n",
      "image_token_index\n",
      " -200\n",
      "return_tensors\n",
      " None\n",
      "input_ids (return)\n",
      " [1, -200]\n",
      "input_ids (return)\n",
      " [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "          411,  2654, 11315,    13])]\n",
      "targets (return)\n",
      " [tensor([ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "          411,  2654, 11315,    13])]\n",
      "data_dict (after preprocess)\n",
      " {'input_ids': [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "          411,  2654, 11315,    13])], 'labels': [tensor([ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "          411,  2654, 11315,    13])]}\n",
      "【COND】 isinstance(i, int): True\n",
      "sample_data_dict\n",
      " {'input_ids': tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "          411,  2654, 11315,    13]), 'labels': tensor([ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "          411,  2654, 11315,    13]), 'image': tensor([[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "         [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "         [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "         ...,\n",
      "         [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "        [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "         [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "         [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "         ...,\n",
      "         [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "        [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "         [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "         [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "         ...,\n",
      "         [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "         [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "         [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]])}\n"
     ]
    }
   ],
   "source": [
    "sample_data_dict = train_dataset.__getitem__(0)\n",
    "print(\"sample_data_dict\\n\", sample_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/train/train.py\n",
      "def DataCollatorForSupervisedDataset.__call__(self, instances)\n",
      "instances\n",
      " [{'input_ids': tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "          411,  2654, 11315,    13]), 'labels': tensor([ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "          411,  2654, 11315,    13]), 'image': tensor([[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "         [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "         [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "         ...,\n",
      "         [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "        [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "         [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "         [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "         ...,\n",
      "         [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "        [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "         [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "         [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "         ...,\n",
      "         [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "         [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "         [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]])}]\n",
      "shape of each instance's input_ids and labels, and images(if any): [(torch.Size([24]), torch.Size([24]), torch.Size([3, 336, 336]))]\n",
      "self.tokenizer.pad_token_id\n",
      " 2\n",
      "IGNORE_INDEX\n",
      " -100\n",
      "input_ids.shape (after pad_sequence and truncate)\n",
      " torch.Size([1, 24])\n",
      "input_ids (after pad_sequence and truncate)\n",
      " tensor([[    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "           411,  2654, 11315,    13]])\n",
      "labels.shape (after pad_sequence and truncate)\n",
      " torch.Size([1, 24])\n",
      "labels (after pad_sequence and truncate)\n",
      " tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "           411,  2654, 11315,    13]])\n",
      "batch['images'].shape\n",
      " torch.Size([1, 3, 336, 336])\n",
      "batch (return)\n",
      " {'input_ids': tensor([[    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "           411,  2654, 11315,    13]]), 'labels': tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "           411,  2654, 11315,    13]]), 'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True]]), 'images': tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "          ...,\n",
      "          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "          ...,\n",
      "          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "          ...,\n",
      "          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])}\n",
      "shape of each batch's input_ids and labels, and images(if any): [(torch.Size([1, 24]), torch.Size([1, 24]), torch.Size([1, 3, 336, 336]))]\n",
      "batch\n",
      " {'input_ids': tensor([[    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "           411,  2654, 11315,    13]]), 'labels': tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "           411,  2654, 11315,    13]]), 'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True]]), 'images': tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "          ...,\n",
      "          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "          ...,\n",
      "          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "          ...,\n",
      "          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])}\n"
     ]
    }
   ],
   "source": [
    "instances = [sample_data_dict]\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "batch = data_collator(instances)\n",
    "print(\"batch\\n\", batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape\n",
      " torch.Size([1, 3, 336, 336])\n",
      "images\n",
      " tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "          ...,\n",
      "          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "          ...,\n",
      "          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "          ...,\n",
      "          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])\n"
     ]
    }
   ],
   "source": [
    "images = batch['images']\n",
    "print(\"images shape\\n\", images.shape) # torch.Size([1, 3, 336, 336])\n",
    "print(\"images\\n\", images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images(self, images):\n",
    "    print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "    print(\"def LlavaMetaForCausalLM(ABC).encode_images(self, images)\")\n",
    "    print(\"images\\n\", images)\n",
    "    image_features = self.get_model().get_vision_tower()(images)\n",
    "    image_features = self.get_model().mm_projector(image_features)\n",
    "    print(\"image_features (return) shape\\n\", image_features.shape)\n",
    "    print(\"image_features (return)\\n\", image_features)\n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "LlavaMetaForCausalLM.encode_images = encode_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_forward_outs から、指定した層の特徴量 (B, 577, 1024) を取り出したのち、パッチ特徴量 (B, 576, 1024) のみを返す。\n",
    "def feature_select(self, image_forward_outs):\n",
    "\n",
    "    print(\"current file path\", \"llava/llava/model/multimodal_encoder/clip_encoder.py\")\n",
    "    print(\"def CLIPVisionTower.feature_select(self, image_forward_outs)\")\n",
    "    print(\"image_forward_outs\\n\", image_forward_outs) # 24層のtuple\n",
    "    image_features = image_forward_outs.hidden_states[self.select_layer]\n",
    "    print(\"image_features (after select_layer)\\n\", type(image_features))\n",
    "    if hasattr(image_features, 'shape'):\n",
    "        print(\"image_features.shape\\n\", image_features.shape) # torch.Size([1, 577, 1024])\n",
    "    print(f\"【COND】 select_feature={self.select_feature}\") # patch\n",
    "    if self.select_feature == 'patch':\n",
    "        print(\"【ENTER】if self.select_feature == 'patch':\")\n",
    "        print(\"original image_features\\n\", image_features)\n",
    "        \"\"\"\n",
    "        tensor([[[ 0.2236,  0.2432, -0.5938,  ...,  0.4863, -0.5273, -0.2041],\n",
    "                [-0.0469, -0.1836, -0.0273,  ...,  0.3535,  0.3750,  0.3047],\n",
    "                [-0.2598,  1.1484,  0.4844,  ...,  0.4961, -0.1719, -0.5117],\n",
    "                ...,\n",
    "                [ 1.7188,  0.9688,  0.8828,  ..., -0.2441, -0.8672,  1.3047],\n",
    "                [ 0.7891, -0.3984,  0.6797,  ..., -0.3594, -0.9922,  0.3164],\n",
    "                [ 1.5000,  0.6250,  0.3672,  ..., -0.5469, -0.4902,  0.9766]]],\n",
    "            device='cuda:0', dtype=torch.bfloat16)\n",
    "        \"\"\"\n",
    "        image_features = image_features[:, 1:]\n",
    "        print(\"after process\\n\", image_features)\n",
    "        \"\"\"\n",
    "        tensor([[[-0.0469, -0.1836, -0.0273,  ...,  0.3535,  0.3750,  0.3047],\n",
    "                [-0.2598,  1.1484,  0.4844,  ...,  0.4961, -0.1719, -0.5117],\n",
    "                [ 1.0625, -0.0635, -0.3730,  ...,  0.0220,  0.0820,  0.4805],\n",
    "                ...,\n",
    "                [ 1.7188,  0.9688,  0.8828,  ..., -0.2441, -0.8672,  1.3047],\n",
    "                [ 0.7891, -0.3984,  0.6797,  ..., -0.3594, -0.9922,  0.3164],\n",
    "                [ 1.5000,  0.6250,  0.3672,  ..., -0.5469, -0.4902,  0.9766]]],\n",
    "            device='cuda:0', dtype=torch.bfloat16)\n",
    "        \"\"\"\n",
    "        print(\"【EXIT】if self.select_feature == 'patch':\")\n",
    "    elif self.select_feature == 'cls_patch':\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "    print(\"selected image_feature shape\\n\", image_features.shape) \n",
    "    print(\"image_features (return)\\n\", image_features)\n",
    "    \"\"\"\n",
    "    image_features (return)\n",
    "    tensor([[[-0.0469, -0.1836, -0.0273,  ...,  0.3535,  0.3750,  0.3047],\n",
    "            [-0.2598,  1.1484,  0.4844,  ...,  0.4961, -0.1719, -0.5117],\n",
    "            [ 1.0625, -0.0635, -0.3730,  ...,  0.0220,  0.0820,  0.4805],\n",
    "            ...,\n",
    "            [ 1.7188,  0.9688,  0.8828,  ..., -0.2441, -0.8672,  1.3047],\n",
    "            [ 0.7891, -0.3984,  0.6797,  ..., -0.3594, -0.9922,  0.3164],\n",
    "            [ 1.5000,  0.6250,  0.3672,  ..., -0.5469, -0.4902,  0.9766]]],\n",
    "        device='cuda:0', dtype=torch.bfloat16)\n",
    "    \"\"\"\n",
    "    if hasattr(image_features, 'shape'):\n",
    "        print(\"image_features.shape\\n\", image_features.shape) # torch.Size([1, 576, 1024])\n",
    "    return image_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIPVisionTower.feature_select = feature_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() \n",
    "def forward(self, images):\n",
    "\n",
    "    print(\"current file path\", \"llava/llava/model/multimodal_encoder/clip_encoder.py\")\n",
    "    print(\"def CLIPVisionTower.forward(self, images)\")\n",
    "    print(\"images shape\\n\", images.shape) # torch.Size([1, 3, 336, 336])\n",
    "    print(\"images\\n\", images)\n",
    "    \n",
    "    if hasattr(images, 'shape'):\n",
    "        print(\"images.shape\\n\", images.shape) # torch.Size([1, 3, 336, 336])\n",
    "    print(f\"【COND】 type_images_is_list={type(images) is list}\") # False\n",
    "    if type(images) is list:\n",
    "        pass\n",
    "    else:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】else (type(images) is not list):\")\n",
    "        print(\"original images\\n\", images)\n",
    "        image_forward_outs = self.vision_tower(images.to(device=self.vision_tower.device, dtype=self.vision_tower.dtype), output_hidden_states=True)\n",
    "        print(\"after process image_forward_outs\\n\", type(image_forward_outs)) # 24層のtuple\n",
    "        image_features = self.feature_select(image_forward_outs).to(images.dtype)\n",
    "        print(\"after process image_features\\n\", type(image_features)) # <class 'torch.Tensor'>\n",
    "        print(\"【EXIT】else (type(images) is not list):\")\n",
    "\n",
    "    print(\"image_features (return)\\n\", image_features)\n",
    "    \"\"\"\n",
    "    image_features (return)\n",
    "    tensor([[[-0.0469, -0.1836, -0.0273,  ...,  0.3535,  0.3750,  0.3047],\n",
    "            [-0.2598,  1.1484,  0.4844,  ...,  0.4961, -0.1719, -0.5117],\n",
    "            [ 1.0625, -0.0635, -0.3730,  ...,  0.0220,  0.0820,  0.4805],\n",
    "            ...,\n",
    "            [ 1.7188,  0.9688,  0.8828,  ..., -0.2441, -0.8672,  1.3047],\n",
    "            [ 0.7891, -0.3984,  0.6797,  ..., -0.3594, -0.9922,  0.3164],\n",
    "            [ 1.5000,  0.6250,  0.3672,  ..., -0.5469, -0.4902,  0.9766]]],\n",
    "        device='cuda:0', dtype=torch.bfloat16)\n",
    "    \"\"\"\n",
    "    if hasattr(image_features, 'shape'):\n",
    "        print(\"image_features.shape\\n\", image_features.shape) # \n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIPVisionTower.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/model/llava_arch.py\n",
      "def LlavaMetaForCausalLM(ABC).encode_images(self, images)\n",
      "images\n",
      " tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "          ...,\n",
      "          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "          ...,\n",
      "          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "          ...,\n",
      "          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])\n",
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.get_model(self)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "self.model (return)\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      "  (vision_tower): CLIPVisionTower(\n",
      "    (vision_tower): CLIPVisionModel(\n",
      "      (vision_model): CLIPVisionTransformer(\n",
      "        (embeddings): CLIPVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "          (position_embedding): Embedding(577, 1024)\n",
      "        )\n",
      "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder): CLIPEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-23): 24 x CLIPEncoderLayer(\n",
      "              (self_attn): CLIPAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): CLIPMLP(\n",
      "                (activation_fn): QuickGELUActivation()\n",
      "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mm_projector): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n",
      "current file path llava/model/llava_arch.py\n",
      "def get_vision_tower(self)\n",
      "vision_tower (raw)\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "type(vision_tower)\n",
      " <class '__main__.CLIPVisionTower'>\n",
      "【COND】 type_vision_tower_is_list=False\n",
      "vision_tower (return)\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "current file path llava/llava/model/multimodal_encoder/clip_encoder.py\n",
      "def CLIPVisionTower.forward(self, images)\n",
      "images shape\n",
      " torch.Size([1, 3, 336, 336])\n",
      "images\n",
      " tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "          ...,\n",
      "          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "          ...,\n",
      "          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "          ...,\n",
      "          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])\n",
      "images.shape\n",
      " torch.Size([1, 3, 336, 336])\n",
      "【COND】 type_images_is_list=False\n",
      "【ENTER】else (type(images) is not list):\n",
      "original images\n",
      " tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "          ...,\n",
      "          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "          ...,\n",
      "          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "          ...,\n",
      "          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])\n"
     ]
    }
   ],
   "source": [
    "image_features = model.encode_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs_labels_for_multimodal(\n",
    "    self, input_ids, position_ids, attention_mask, past_key_values, labels,\n",
    "    images, image_sizes=None\n",
    "):\n",
    "    print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "    \"\"\"\n",
    "    llava/llava/model/language_model/llava_llama.py\n",
    "    \"\"\"\n",
    "    print(\"def LlavaMetaForCausalLM(ABC).prepare_inputs_labels_for_multimodal(self, input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes=None)\")  # not found\n",
    "    print(\"input_ids\\n\", input_ids)\n",
    "    \"\"\"\n",
    "    tensor([[    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "             10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "               411,  2654, 11315,    13]])\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"position_ids\\n\", position_ids)  # None\n",
    "    print(\"attention_mask\\n\", attention_mask)\n",
    "    \"\"\"\n",
    "    tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True]])\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"past_key_values\\n\", past_key_values)  # None\n",
    "    print(\"labels\\n\", labels)\n",
    "    \"\"\"\n",
    "    tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "             10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "               411,  2654, 11315,    13]])\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"images\\n\", images)\n",
    "    \"\"\"\n",
    "    tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
    "              [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
    "              [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
    "              ...,\n",
    "              [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
    "              [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
    "              [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
    "    \n",
    "             [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
    "              [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
    "              [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
    "              ...,\n",
    "              [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
    "              [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
    "              [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
    "    \n",
    "             [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
    "              [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
    "              [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
    "              ...,\n",
    "              [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
    "              [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
    "              [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"image_sizes\\n\", image_sizes)  # None\n",
    "    vision_tower = self.get_vision_tower()\n",
    "    print(\"vision_tower\\n\", vision_tower)\n",
    "    \"\"\"\n",
    "    CLIPVisionTower(\n",
    "      (vision_tower): CLIPVisionModel(\n",
    "        (vision_model): CLIPVisionTransformer(\n",
    "          (embeddings): CLIPVisionEmbeddings(\n",
    "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "            (position_embedding): Embedding(577, 1024)\n",
    "          )\n",
    "          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "          (encoder): CLIPEncoder(\n",
    "            (layers): ModuleList(\n",
    "              (0-23): 24 x CLIPEncoderLayer(\n",
    "                (self_attn): CLIPAttention(\n",
    "                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                (mlp): CLIPMLP(\n",
    "                  (activation_fn): QuickGELUActivation()\n",
    "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "              )\n",
    "            )\n",
    "          )\n",
    "          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"【COND】 vision_tower_is_None={vision_tower is None} images_is_None={images is None} input_ids_shape_1_eq_1={input_ids.shape[1] == 1}\")\n",
    "    if vision_tower is None or images is None or input_ids.shape[1] == 1:\n",
    "        pass\n",
    "\n",
    "    print(\"【COND】type(images)\\n\", type(images))  # <class 'torch.Tensor'>\n",
    "    print(\"【COND】images.ndim\\n\", images.ndim)  # 4\n",
    "    if type(images) is list or images.ndim == 5:\n",
    "        pass\n",
    "    else:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】else of if type(images) is list or images.ndim == 5:\")\n",
    "        image_features = self.encode_images(images)\n",
    "        print(\"image_features after encode_images shape \\n\", image_features.shape)  # torch.Size([1, 576, 2048])\n",
    "        print(\"image_features after encode_images\\n\", image_features)\n",
    "        \"\"\"\n",
    "        tensor([[[-0.1943,  0.1157, -0.0747,  ...,  0.0027, -0.1691, -0.3439],\n",
    "                 [ 0.0437,  0.1717, -0.0998,  ...,  0.0930, -0.1386, -0.0731],\n",
    "                 [-0.0505,  0.1592, -0.0982,  ...,  0.0866, -0.1123, -0.2177],\n",
    "                 ...,\n",
    "                 [-0.0182,  0.0850, -0.0556,  ...,  0.0622, -0.1969,  0.0129],\n",
    "                 [-0.0651,  0.0586, -0.1218,  ..., -0.0614, -0.1158, -0.0104],\n",
    "                 [ 0.0863,  0.0081, -0.1651,  ..., -0.2040, -0.0455,  0.0618]]],\n",
    "               grad_fn=<ViewBackward0>)\n",
    "        \"\"\"\n",
    "        print(\"【EXIT】else of if type(images) is list or images.ndim == 5:\")\n",
    "\n",
    "    # TODO: image start / end is not implemented here to support pretraining.\n",
    "    if getattr(self.config, 'tune_mm_mlp_adapter', False) and getattr(self.config, 'mm_use_im_start_end', False):\n",
    "        print(\"【ENTER】if getattr(self.config, 'tune_mm_mlp_adapter', False) and getattr(self.config, 'mm_use_im_start_end', False):\")  # not found\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Let's just add dummy tensors if they do not exist,\n",
    "    # it is a headache to deal with None all the time.\n",
    "    # But it is not ideal, and if you have a better idea,\n",
    "    # please open an issue / submit a PR, thanks.\n",
    "\n",
    "    print(\"labels before\\n\", labels)\n",
    "    \"\"\"\n",
    "    tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "             10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "               411,  2654, 11315,    13]])\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"position_ids before\\n\", position_ids)  # None\n",
    "\n",
    "    print(\"attention_mask before\\n\", attention_mask)\n",
    "    \"\"\"\n",
    "    tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True]])\n",
    "    \"\"\"\n",
    "\n",
    "    _labels = labels\n",
    "    _position_ids = position_ids\n",
    "    _attention_mask = attention_mask\n",
    "    if attention_mask is None:\n",
    "        pass\n",
    "    else:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】else of if attention_mask is None:\")\n",
    "        attention_mask = attention_mask.bool()\n",
    " \n",
    "        print(\"attention_mask（after）shape \\n\", attention_mask.shape)  # torch.Size([1, 24])\n",
    "        print(\"attention_mask (after)\\n\", attention_mask)\n",
    "        \"\"\"\n",
    "        tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True]])\n",
    "        \"\"\"\n",
    "        print(\"【EXIT】else of if attention_mask is None:\")\n",
    "    if position_ids is None:\n",
    "        print(\"【ENTER】if position_ids is None:\")\n",
    "        position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "        print(\"position_ids (after) shape \\n\", position_ids.shape)  # torch.Size([24])\n",
    "        print(\"position_ids (after)\\n\", position_ids)\n",
    "        \"\"\"\n",
    "        tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
    "                18, 19, 20, 21, 22, 23])\n",
    "        \"\"\"\n",
    "        print(\"【EXIT】if position_ids is None:\")\n",
    "    print(f\"【COND】 labels_is_None={labels is None}\")\n",
    "    if labels is None:\n",
    "        pass\n",
    "\n",
    "    # remove the padding using attention_mask -- FIXME\n",
    "    _input_ids = input_ids\n",
    "    input_ids = [cur_input_ids[cur_attention_mask] for cur_input_ids, cur_attention_mask in zip(input_ids, attention_mask)]\n",
    "    labels = [cur_labels[cur_attention_mask] for cur_labels, cur_attention_mask in zip(labels, attention_mask)]\n",
    "    print(\"input_ids after removing padding\\n\", input_ids)\n",
    "    \"\"\"\n",
    "    [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "            10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "              411,  2654, 11315,    13])]\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"labels after removing padding\\n\", labels)\n",
    "    \"\"\"\n",
    "    [tensor([ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "            10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "              411,  2654, 11315,    13])]\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    new_input_embeds = []\n",
    "    new_labels = []\n",
    "    cur_image_idx = 0\n",
    "    for batch_idx, cur_input_ids in enumerate(input_ids):\n",
    "        print(\"cur_input_ids shape\\n\", cur_input_ids.shape)   # torch.Size([24])\n",
    "        print(\"cur_input_ids\\n\", cur_input_ids)\n",
    "        \"\"\"\n",
    "        tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "                10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "                  411,  2654, 11315,    13])\n",
    "        \"\"\"\n",
    "        num_images = (cur_input_ids == IMAGE_TOKEN_INDEX).sum()\n",
    "        print(\"【COND】num_images:\", num_images)  # tensor(1)\n",
    "        if num_images == 0:\n",
    "            print(\"【ENTER】if num_images == 0:\")\n",
    "            cur_image_features = image_features[cur_image_idx]\n",
    "            cur_input_embeds_1 = self.get_model().embed_tokens(cur_input_ids)\n",
    "            cur_input_embeds = torch.cat([cur_input_embeds_1, cur_image_features[0:0]], dim=0)\n",
    "            new_input_embeds.append(cur_input_embeds)\n",
    "            new_labels.append(labels[batch_idx])\n",
    "            cur_image_idx += 1\n",
    "            print(\"【EXIT】if num_images == 0:\")\n",
    "            continue\n",
    "\n",
    "        image_token_indices = [-1] + torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)[0].tolist() + [cur_input_ids.shape[0]]\n",
    "        print(\"image_token_indices\\n\", image_token_indices)  # [-1, 1, 24]\n",
    "        print(\"len image_token_indices\", len(image_token_indices))   # 3\n",
    "        cur_input_ids_noim = []\n",
    "        cur_labels = labels[batch_idx]\n",
    "        print(\"cur_labels\\n\", cur_labels)\n",
    "        \"\"\"\n",
    "        tensor([ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "                10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "                  411,  2654, 11315,    13])\n",
    "        \"\"\"\n",
    "        cur_labels_noim = []\n",
    "        for i in range(len(image_token_indices) - 1): # 2回ループ。1回目 START から IMAGE_TOKEN_INDEXの手前まで、2回目はIMAGE_TOKEN_INDEX より先から 最後まで\n",
    "            cur_input_ids_noim.append(cur_input_ids[image_token_indices[i]+1:image_token_indices[i+1]])\n",
    "            cur_labels_noim.append(cur_labels[image_token_indices[i]+1:image_token_indices[i+1]])\n",
    "        print(\"cur_input_ids_noim (after)\\n\", cur_input_ids_noim)\n",
    "        \"\"\"\n",
    "        [tensor([1]), tensor([  278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596,\n",
    "                23425,   278,  3700,   322,  6567,   310,   263,  6114,   411,  2654,\n",
    "                11315,    13])]\n",
    "        \"\"\"\n",
    "        print(\"cur_labels_noim (after) \\n\", cur_labels_noim)\n",
    "        \"\"\"\n",
    "        [tensor([-100]), tensor([  278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596,\n",
    "                23425,   278,  3700,   322,  6567,   310,   263,  6114,   411,  2654,\n",
    "                11315,    13])]\n",
    "        \"\"\"\n",
    "        split_sizes = [x.shape[0] for x in cur_labels_noim]\n",
    "        print(\"split_sizes\\n\", split_sizes)  # [1, 22]\n",
    "        cur_input_embeds = self.get_model().embed_tokens(torch.cat(cur_input_ids_noim))\n",
    "        print(\"cur_input_embeds shape\\n\", cur_input_embeds.shape)  # torch.Size([23, 2048])\n",
    "        print(\"cur_input_embeds\\n\", cur_input_embeds)\n",
    "        \"\"\"\n",
    "        tensor([[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
    "                 -6.5231e-04, -4.9973e-04],\n",
    "                [ 7.0801e-03,  1.0452e-03,  6.0425e-03,  ...,  3.9673e-03,\n",
    "                  1.2817e-03, -1.1215e-03],\n",
    "                [-2.2949e-02, -2.6226e-05,  6.8359e-03,  ..., -2.4658e-02,\n",
    "                 -9.4604e-03,  1.5869e-02],\n",
    "                ...,\n",
    "                [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
    "                 -8.3618e-03, -9.4604e-03],\n",
    "                [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
    "                 -2.5177e-03, -8.0566e-03],\n",
    "                [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
    "                 -7.3242e-04,  2.7924e-03]], requires_grad=True)\n",
    "        \"\"\"\n",
    "        cur_input_embeds_no_im = torch.split(cur_input_embeds, split_sizes, dim=0)\n",
    "        print(\"cur_input_embeds_no_im\\n\", cur_input_embeds_no_im)\n",
    "        \"\"\"\n",
    "        (tensor([[-0.0011,  0.0019, -0.0017,  ...,  0.0002, -0.0007, -0.0005]],\n",
    "               grad_fn=<SplitWithSizesBackward0>), tensor([[ 7.0801e-03,  1.0452e-03,  6.0425e-03,  ...,  3.9673e-03,\n",
    "                  1.2817e-03, -1.1215e-03],\n",
    "                [-2.2949e-02, -2.6226e-05,  6.8359e-03,  ..., -2.4658e-02,\n",
    "                 -9.4604e-03,  1.5869e-02],\n",
    "                [-2.3499e-03,  1.4893e-02, -2.0447e-03,  ..., -8.6060e-03,\n",
    "                  2.3193e-03,  3.0670e-03],\n",
    "                ...,\n",
    "                [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
    "                 -8.3618e-03, -9.4604e-03],\n",
    "                [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
    "                 -2.5177e-03, -8.0566e-03],\n",
    "                [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
    "                 -7.3242e-04,  2.7924e-03]], grad_fn=<SplitWithSizesBackward0>))\n",
    "        \"\"\"\n",
    "        cur_new_input_embeds = []\n",
    "        cur_new_labels = []\n",
    "\n",
    "        for i in range(num_images + 1):\n",
    "            cur_new_input_embeds.append(cur_input_embeds_no_im[i])\n",
    "            cur_new_labels.append(cur_labels_noim[i])\n",
    "            print(f\"【COND】 i={i} num_images={num_images}\")\n",
    "            if i < num_images:\n",
    "                print(\"【ENTER】if i < num_images:\")\n",
    "                cur_image_features = image_features[cur_image_idx]\n",
    "                cur_image_idx += 1\n",
    "                cur_new_input_embeds.append(cur_image_features)\n",
    "                cur_new_labels.append(torch.full((cur_image_features.shape[0],), IGNORE_INDEX, device=cur_labels.device, dtype=cur_labels.dtype))\n",
    "                print(\"【EXIT】if i < num_images:\")\n",
    "\n",
    "        print(\"cur_new_input_embeds (before cat) shape\\n\", [x.shape for x in cur_new_input_embeds])\n",
    "        \"\"\"\n",
    "        [torch.Size([1, 2048]), torch.Size([576, 2048]), torch.Size([22, 2048])]\n",
    "        \"\"\"\n",
    "        print(\"cur_new_input_embeds (before cat)\\n\", cur_new_input_embeds)\n",
    "        \"\"\"\n",
    "        [tensor([[-0.0011,  0.0019, -0.0017,  ...,  0.0002, -0.0007, -0.0005]],\n",
    "               grad_fn=<SplitWithSizesBackward0>), tensor([[-0.1943,  0.1157, -0.0747,  ...,  0.0027, -0.1691, -0.3439],\n",
    "                [ 0.0437,  0.1717, -0.0998,  ...,  0.0930, -0.1386, -0.0731],\n",
    "                [-0.0505,  0.1592, -0.0982,  ...,  0.0866, -0.1123, -0.2177],\n",
    "                ...,\n",
    "                [-0.0182,  0.0850, -0.0556,  ...,  0.0622, -0.1969,  0.0129],\n",
    "                [-0.0651,  0.0586, -0.1218,  ..., -0.0614, -0.1158, -0.0104],\n",
    "                [ 0.0863,  0.0081, -0.1651,  ..., -0.2040, -0.0455,  0.0618]],\n",
    "               grad_fn=<SelectBackward0>), tensor([[ 7.0801e-03,  1.0452e-03,  6.0425e-03,  ...,  3.9673e-03,\n",
    "                  1.2817e-03, -1.1215e-03],\n",
    "                [-2.2949e-02, -2.6226e-05,  6.8359e-03,  ..., -2.4658e-02,\n",
    "                 -9.4604e-03,  1.5869e-02],\n",
    "                [-2.3499e-03,  1.4893e-02, -2.0447e-03,  ..., -8.6060e-03,\n",
    "                  2.3193e-03,  3.0670e-03],\n",
    "                ...,\n",
    "                [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
    "                 -8.3618e-03, -9.4604e-03],\n",
    "                [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
    "                 -2.5177e-03, -8.0566e-03],\n",
    "                [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
    "                 -7.3242e-04,  2.7924e-03]], grad_fn=<SplitWithSizesBackward0>)]\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"cur_new_labels (before cat) shape\\n\", [x.shape for x in cur_new_labels])\n",
    "        \"\"\"\n",
    "        [torch.Size([1]), torch.Size([576]), torch.Size([22])]\n",
    "        \"\"\"\n",
    "        print(\"cur_new_labels (before cat)\\n\", cur_new_labels)\n",
    "        \"\"\"\n",
    "        [tensor([-100]), tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]), tensor([  278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596,\n",
    "                23425,   278,  3700,   322,  6567,   310,   263,  6114,   411,  2654,\n",
    "                11315,    13])]\n",
    "        \"\"\"\n",
    "        cur_new_input_embeds = [x.to(self.device) for x in cur_new_input_embeds]\n",
    "\n",
    "        cur_new_input_embeds = torch.cat(cur_new_input_embeds)\n",
    "        cur_new_labels = torch.cat(cur_new_labels)\n",
    "\n",
    "        print(\"cur_new_input_embeds (after cat) shape\\n\", cur_new_input_embeds.shape)  # torch.Size([599, 2048])\n",
    "        print(\"cur_new_input_embeds (after cat)\\n\", cur_new_input_embeds)\n",
    "        \"\"\"\n",
    "        tensor([[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
    "                 -6.5231e-04, -4.9973e-04],\n",
    "                [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
    "                 -1.6907e-01, -3.4387e-01],\n",
    "                [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
    "                 -1.3859e-01, -7.3106e-02],\n",
    "                ...,\n",
    "                [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
    "                 -8.3618e-03, -9.4604e-03],\n",
    "                [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
    "                 -2.5177e-03, -8.0566e-03],\n",
    "                [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
    "                 -7.3242e-04,  2.7924e-03]], grad_fn=<CatBackward0>)\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"cur_new_labels (after cat) shape\\n\", cur_new_labels.shape)  # torch.Size([599])\n",
    "        print(\"cur_new_labels (after cat)\\n\", cur_new_labels)\n",
    "        \"\"\"\n",
    "        tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
    "                  297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
    "                  322,  6567,   310,   263,  6114,   411,  2654, 11315,    13])\n",
    "        \"\"\"\n",
    "\n",
    "        new_input_embeds.append(cur_new_input_embeds)\n",
    "        new_labels.append(cur_new_labels)\n",
    "        print(\"new_input_embeds (so far) shape\\n\", [x.shape for x in new_input_embeds])  # [torch.Size([599, 2048])]\n",
    "        print(\"new_input_embeds (so far)\\n\", new_input_embeds)\n",
    "        \"\"\"\n",
    "        [tensor([[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
    "                 -6.5231e-04, -4.9973e-04],\n",
    "                [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
    "                 -1.6907e-01, -3.4387e-01],\n",
    "                [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
    "                 -1.3859e-01, -7.3106e-02],\n",
    "                ...,\n",
    "                [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
    "                 -8.3618e-03, -9.4604e-03],\n",
    "                [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
    "                 -2.5177e-03, -8.0566e-03],\n",
    "                [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
    "                 -7.3242e-04,  2.7924e-03]], grad_fn=<CatBackward0>)]\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"new_labels (so far) shape\\n\", [x.shape for x in new_labels])  # [torch.Size([599])]\n",
    "        print(\"new_labels (so far)\\n\", new_labels)\n",
    "        \"\"\"\n",
    "        [tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
    "                  297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
    "                  322,  6567,   310,   263,  6114,   411,  2654, 11315,    13])]\n",
    "        \"\"\"\n",
    "\n",
    "    # Truncate sequences to max length as image embeddings can make the sequence longer\n",
    "    tokenizer_model_max_length = getattr(self.config, 'tokenizer_model_max_length', None)\n",
    "    print(f\"【COND】 tokenizer_model_max_length_is_not_None={tokenizer_model_max_length is not None}\")\n",
    "    if tokenizer_model_max_length is not None:\n",
    "        print(\"【ENTER】if tokenizer_model_max_length is not None:\")\n",
    "        new_input_embeds = [x[:tokenizer_model_max_length] for x in new_input_embeds]\n",
    "        new_labels = [x[:tokenizer_model_max_length] for x in new_labels]\n",
    "        print(\"【EXIT】if tokenizer_model_max_length is not None:\")\n",
    "\n",
    "    # Combine them\n",
    "    max_len = max(x.shape[0] for x in new_input_embeds)\n",
    "    print(\"max_len\\n\", max_len)  # 599\n",
    "    batch_size = len(new_input_embeds)\n",
    "    print(\"batch_size\\n\", batch_size)  # 1\n",
    "\n",
    "    new_input_embeds_padded = []\n",
    "    new_labels_padded = torch.full((batch_size, max_len), IGNORE_INDEX, dtype=new_labels[0].dtype, device=new_labels[0].device)\n",
    "    print(\"new_labels_padded (before) shape\\n\", new_labels_padded.shape)  # torch.Size([1, 599])\n",
    "    print(\"new_labels_padded (before)\\n\", new_labels_padded)\n",
    "    \"\"\"\n",
    "    tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]])\n",
    "    \"\"\"\n",
    "    attention_mask = torch.zeros((batch_size, max_len), dtype=attention_mask.dtype, device=attention_mask.device)\n",
    "    print(\"attention_mask (before) shape\\n\", attention_mask.shape)  # torch.Size([1, 599])\n",
    "    print(\"attention_mask (before)\\n\", attention_mask)\n",
    "    \"\"\"\n",
    "    tensor([[False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False]])\n",
    "    \"\"\"\n",
    "    position_ids = torch.zeros((batch_size, max_len), dtype=position_ids.dtype, device=position_ids.device)\n",
    "    print(\"position_ids (before) shape\\n\", position_ids.shape)  # torch.Size([1, 599])\n",
    "    print(\"position_ids (before)\\n\", position_ids)\n",
    "    \"\"\"\n",
    "    tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "    \"\"\"\n",
    "\n",
    "    for i, (cur_new_embed, cur_new_labels) in enumerate(zip(new_input_embeds, new_labels)):\n",
    "        cur_len = cur_new_embed.shape[0]\n",
    "        print(f\"【COND】 padding_side={getattr(self.config, 'tokenizer_padding_side', 'right')} cur_len={cur_len} max_len={max_len}\")\n",
    "        if getattr(self.config, 'tokenizer_padding_side', 'right') == \"left\":\n",
    "            pass\n",
    "        else:\n",
    "            print(\"【ENTER】else (padding_side != 'left'):\")\n",
    "            #【ENTER】\n",
    "            new_input_embeds_padded.append(torch.cat((\n",
    "                cur_new_embed,\n",
    "                torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device)\n",
    "            ), dim=0))\n",
    "            if cur_len > 0:\n",
    "                # :cur_len に、代入\n",
    "                new_labels_padded[i, :cur_len] = cur_new_labels \n",
    "                attention_mask[i, :cur_len] = True\n",
    "                position_ids[i, :cur_len] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\n",
    "            print(\"new_input_embeds_padded (so far) shape\\n\", [x.shape for x in new_input_embeds_padded])  # [torch.Size([599, 2048])]\n",
    "            print(\"new_input_embeds_padded (so far)\\n\", new_input_embeds_padded)\n",
    "            \"\"\"\n",
    "            [tensor([[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
    "                     -6.5231e-04, -4.9973e-04],\n",
    "                    [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
    "                     -1.6907e-01, -3.4387e-01],\n",
    "                    [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
    "                     -1.3859e-01, -7.3106e-02],\n",
    "                    ...,\n",
    "                    [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
    "                     -8.3618e-03, -9.4604e-03],\n",
    "                    [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
    "                     -2.5177e-03, -8.0566e-03],\n",
    "                    [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
    "                     -7.3242e-04,  2.7924e-03]], grad_fn=<CatBackward0>)]\n",
    "            \"\"\"\n",
    "\n",
    "            print(\"new_labels_padded (so far) shape\\n\", new_labels_padded.shape)  # torch.Size([1, 599])\n",
    "            print(\"new_labels_padded (so far)\\n\", new_labels_padded)\n",
    "            \"\"\"\n",
    "            tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
    "                       297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
    "                       322,  6567,   310,   263,  6114,   411,  2654, 11315,    13]])\n",
    "            \"\"\"\n",
    "\n",
    "            print(\"attention_mask (so far) shape\\n\", attention_mask.shape)  # torch.Size([1, 599])\n",
    "            print(\"attention_mask (so far)\\n\", attention_mask)\n",
    "            \"\"\"\n",
    "            tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True]])\n",
    "            \"\"\"\n",
    "\n",
    "            print(\"position_ids (so far) shape\\n\", position_ids.shape)  # torch.Size([1, 599])\n",
    "            print(\"position_ids (so far)\\n\", position_ids)\n",
    "            \"\"\"\n",
    "            tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
    "                      14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
    "                      28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
    "                      42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
    "                      56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
    "                      70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
    "                      84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
    "                      98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
    "                     112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
    "                     126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
    "                     140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
    "                     154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
    "                     168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
    "                     182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
    "                     196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
    "                     210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
    "                     224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
    "                     238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
    "                     252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
    "                     266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
    "                     280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
    "                     294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
    "                     308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
    "                     322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
    "                     336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
    "                     350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
    "                     364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
    "                     378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
    "                     392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
    "                     406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
    "                     420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
    "                     434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
    "                     448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
    "                     462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
    "                     476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
    "                     490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
    "                     504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517,\n",
    "                     518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531,\n",
    "                     532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
    "                     546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559,\n",
    "                     560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573,\n",
    "                     574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587,\n",
    "                     588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598]])\n",
    "            \"\"\"\n",
    "            print(\"【EXIT】else (padding_side != 'left'):\")\n",
    "\n",
    "    new_input_embeds = torch.stack(new_input_embeds_padded, dim=0)\n",
    "    print(\"new_input_embeds (after) shape\\n\", new_input_embeds.shape)  # torch.Size([1, 599, 2048])\n",
    "    print(\"new_input_embeds (after)\\n\", new_input_embeds)\n",
    "    \"\"\"\n",
    "    tensor([[[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
    "              -6.5231e-04, -4.9973e-04],\n",
    "             [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
    "              -1.6907e-01, -3.4387e-01],\n",
    "             [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
    "              -1.3859e-01, -7.3106e-02],\n",
    "             ...,\n",
    "             [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
    "              -8.3618e-03, -9.4604e-03],\n",
    "             [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
    "              -2.5177e-03, -8.0566e-03],\n",
    "             [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
    "              -7.3242e-04,  2.7924e-03]]], grad_fn=<StackBackward0>)\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"【COND】 _labels_is_None={_labels is None}\") \n",
    "    if _labels is None:\n",
    "        #【SKIP】\n",
    "        print(\"【ENTER】if _labels is None:\")\n",
    "        new_labels = None\n",
    "        print(\"【EXIT】if _labels is None:\")\n",
    "    else:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】else of if _labels is None:\")\n",
    "        new_labels = new_labels_padded\n",
    "        print(\"new_labels (after)\\n\", new_labels)\n",
    "        \"\"\"\n",
    "        tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
    "                   297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
    "                   322,  6567,   310,   263,  6114,   411,  2654, 11315,    13]])\n",
    "        \"\"\"\n",
    "        print(\"【EXIT】else of if _labels is None:\")\n",
    "\n",
    "    print(f\"【COND】 _attention_mask_is_None={_attention_mask is None}\") \n",
    "    if _attention_mask is None:\n",
    "        # 【SKIP】\n",
    "        print(\"【ENTER】if _attention_mask is None:\")\n",
    "        attention_mask = None\n",
    "        print(\"【EXIT】if _attention_mask is None:\")\n",
    "    else:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】else of if _attention_mask is None:\")\n",
    "        attention_mask = attention_mask.to(dtype=_attention_mask.dtype)\n",
    "        print(\"attention_mask (after)2\\n\", attention_mask)\n",
    "        \"\"\"\n",
    "        tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True]])\n",
    "        \"\"\"\n",
    "\n",
    "    print(f\"【COND】 _position_ids_is_None={_position_ids is None}\")\n",
    "    if _position_ids is None:\n",
    "        print(\"【ENTER】if _position_ids is None:\")\n",
    "        position_ids = None\n",
    "        print(\"【EXIT】if _position_ids is None:\")\n",
    "\n",
    "    print(\"position_ids (return)\\n\", position_ids)  # None\n",
    "    print(\"attention_mask (return)\\n\", attention_mask)\n",
    "    \"\"\"\n",
    "    tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True]])\n",
    "    \"\"\"\n",
    "    print(\"past_key_values (return)\\n\", past_key_values)  # None\n",
    "    print(\"new_input_embeds (return)\\n\", new_input_embeds)\n",
    "    \"\"\"\n",
    "    tensor([[[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
    "              -6.5231e-04, -4.9973e-04],\n",
    "             [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
    "              -1.6907e-01, -3.4387e-01],\n",
    "             [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
    "              -1.3859e-01, -7.3106e-02],\n",
    "             ...,\n",
    "             [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
    "              -8.3618e-03, -9.4604e-03],\n",
    "             [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
    "              -2.5177e-03, -8.0566e-03],\n",
    "             [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
    "              -7.3242e-04,  2.7924e-03]]], grad_fn=<StackBackward0>)\n",
    "    \"\"\"\n",
    "    print(\"new_labels (return)\\n\", new_labels)\n",
    "    \"\"\"\n",
    "    tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
    "               297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
    "               322,  6567,   310,   263,  6114,   411,  2654, 11315,    13]])\n",
    "    \"\"\"\n",
    "    return None, position_ids, attention_mask, past_key_values, new_input_embeds, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LlavaMetaForCausalLM.prepare_inputs_labels_for_multimodal = prepare_inputs_labels_for_multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_device\n",
      " cpu\n",
      "input_ids shape\n",
      " torch.Size([1, 24])\n",
      "input_ids\n",
      " tensor([[    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "           411,  2654, 11315,    13]])\n",
      "attention_mask shape\n",
      " torch.Size([1, 24])\n",
      "attention_mask\n",
      " tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True]])\n",
      "labels shape\n",
      " torch.Size([1, 24])\n",
      "labels\n",
      " tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "           411,  2654, 11315,    13]])\n",
      "images shape\n",
      " torch.Size([1, 3, 336, 336])\n",
      "images\n",
      " tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "          ...,\n",
      "          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "          ...,\n",
      "          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "          ...,\n",
      "          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\\n          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\\n          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\\n          ...,\\n          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\\n          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\\n          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\\n\\n         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\\n          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\\n          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\\n          ...,\\n          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\\n          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\\n          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\\n\\n         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\\n          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\\n          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\\n          ...,\\n          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\\n          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\\n          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])\\n'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"model_device\\n\", model.device) # cpu\n",
    "\n",
    "input_ids = batch['input_ids'].to(device=model.device)\n",
    "print(\"input_ids shape\\n\", input_ids.shape) # torch.Size([1, 24])\n",
    "print(\"input_ids\\n\", input_ids)\n",
    "\"\"\"\n",
    "tensor([[    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "           411,  2654, 11315,    13]])\n",
    "\"\"\"\n",
    "\n",
    "attention_mask = batch['attention_mask'].to(device=model.device)\n",
    "print(\"attention_mask shape\\n\", attention_mask.shape) # torch.Size([1, 24])\n",
    "print(\"attention_mask\\n\", attention_mask)\n",
    "\"\"\"\n",
    "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True]])\n",
    "\"\"\"\n",
    "\n",
    "labels = batch['labels'].to(device=model.device)\n",
    "print(\"labels shape\\n\", labels.shape) # torch.Size([1, 3, 336, 336])\n",
    "print(\"labels\\n\", labels)\n",
    "\"\"\"\n",
    "tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "           411,  2654, 11315,    13]])\n",
    "\"\"\"\n",
    "\n",
    "images = batch['images'].to(device=model.device)\n",
    "print(\"images shape\\n\", images.shape)\n",
    "print(\"images\\n\", images)\n",
    "\"\"\"\n",
    "tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
    "          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
    "          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
    "          ...,\n",
    "          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
    "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
    "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
    "\n",
    "         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
    "          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
    "          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
    "          ...,\n",
    "          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
    "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
    "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
    "\n",
    "         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
    "          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
    "          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
    "          ...,\n",
    "          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
    "          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
    "          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids = None\n",
    "past_key_values = None\n",
    "image_sizes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "from print_factory.print_factory import run_and_capture, embed_print_outputs\n",
    "\n",
    "logs, mapping = run_and_capture(\n",
    "    model.prepare_inputs_labels_for_multimodal,\n",
    "    input_ids,\n",
    "    position_ids,\n",
    "    attention_mask,\n",
    "    past_key_values,\n",
    "    labels,\n",
    "    images,\n",
    "    image_sizes\n",
    ")\n",
    "\n",
    "with open(\"print_factory/original_code.py\") as f:\n",
    "    code = f.read()\n",
    "\n",
    "new_code = embed_print_outputs(code, mapping)\n",
    "\n",
    "with open(\"print_factory/after_code.py\", \"w\") as f:\n",
    "    f.write(new_code)\n",
    "\n",
    "print(\"done\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/model/llava_arch.py\n",
      "def LlavaMetaForCausalLM(ABC).prepare_inputs_labels_for_multimodal(self, input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes=None)\n",
      "input_ids\n",
      " tensor([[    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "           411,  2654, 11315,    13]])\n",
      "position_ids\n",
      " None\n",
      "attention_mask\n",
      " tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True]])\n",
      "past_key_values\n",
      " None\n",
      "labels\n",
      " tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "           411,  2654, 11315,    13]])\n",
      "images\n",
      " tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "          ...,\n",
      "          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "          ...,\n",
      "          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "          ...,\n",
      "          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])\n",
      "image_sizes\n",
      " None\n",
      "current file path llava/model/llava_arch.py\n",
      "class LlavaMetaForCausalLM(ABC).get_vision_tower(self)\n",
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.get_model(self)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "self.model (return)\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      "  (vision_tower): CLIPVisionTower(\n",
      "    (vision_tower): CLIPVisionModel(\n",
      "      (vision_model): CLIPVisionTransformer(\n",
      "        (embeddings): CLIPVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "          (position_embedding): Embedding(577, 1024)\n",
      "        )\n",
      "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder): CLIPEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-23): 24 x CLIPEncoderLayer(\n",
      "              (self_attn): CLIPAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): CLIPMLP(\n",
      "                (activation_fn): QuickGELUActivation()\n",
      "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mm_projector): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n",
      "current file path llava/model/llava_arch.py\n",
      "def get_vision_tower(self)\n",
      "vision_tower (raw)\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "type(vision_tower)\n",
      " <class '__main__.CLIPVisionTower'>\n",
      "【COND】 type_vision_tower_is_list=False\n",
      "vision_tower (return)\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "LlavaMetaForCausalLM(ABC).get_vision_tower(self) result (return)\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "vision_tower\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "【COND】 vision_tower_is_None=False images_is_None=False input_ids_shape_1_eq_1=False\n",
      "【COND】type(images)\n",
      " <class 'torch.Tensor'>\n",
      "【COND】images.ndim\n",
      " 4\n",
      "【ENTER】else of if type(images) is list or images.ndim == 5:\n",
      "current file path llava/model/llava_arch.py\n",
      "def LlavaMetaForCausalLM(ABC).encode_images(self, images)\n",
      "images\n",
      " tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "          ...,\n",
      "          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "          ...,\n",
      "          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "          ...,\n",
      "          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])\n",
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.get_model(self)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "self.model (return)\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      "  (vision_tower): CLIPVisionTower(\n",
      "    (vision_tower): CLIPVisionModel(\n",
      "      (vision_model): CLIPVisionTransformer(\n",
      "        (embeddings): CLIPVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "          (position_embedding): Embedding(577, 1024)\n",
      "        )\n",
      "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder): CLIPEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-23): 24 x CLIPEncoderLayer(\n",
      "              (self_attn): CLIPAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): CLIPMLP(\n",
      "                (activation_fn): QuickGELUActivation()\n",
      "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mm_projector): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n",
      "current file path llava/model/llava_arch.py\n",
      "def get_vision_tower(self)\n",
      "vision_tower (raw)\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "type(vision_tower)\n",
      " <class '__main__.CLIPVisionTower'>\n",
      "【COND】 type_vision_tower_is_list=False\n",
      "vision_tower (return)\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "current file path llava/llava/model/multimodal_encoder/clip_encoder.py\n",
      "def CLIPVisionTower.forward(self, images)\n",
      "images shape\n",
      " torch.Size([1, 3, 336, 336])\n",
      "images\n",
      " tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "          ...,\n",
      "          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "          ...,\n",
      "          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "          ...,\n",
      "          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])\n",
      "images.shape\n",
      " torch.Size([1, 3, 336, 336])\n",
      "【COND】 type_images_is_list=False\n",
      "【ENTER】else (type(images) is not list):\n",
      "original images\n",
      " tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "          ...,\n",
      "          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "          ...,\n",
      "          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "          ...,\n",
      "          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after process image_forward_outs\n",
      " <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
      "current file path llava/llava/model/multimodal_encoder/clip_encoder.py\n",
      "def CLIPVisionTower.feature_select(self, image_forward_outs)\n",
      "image_forward_outs\n",
      " BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.0879, -0.1445, -0.2949,  ...,  0.3906, -0.7188,  0.0356],\n",
      "         [ 0.4629,  0.0078,  0.0859,  ..., -0.0815,  0.1113, -0.3906],\n",
      "         [-0.2188,  1.0938,  1.0469,  ...,  0.2119, -0.3477, -0.8477],\n",
      "         ...,\n",
      "         [ 1.7500,  1.0859,  1.6719,  ..., -0.2891, -0.6797,  1.1016],\n",
      "         [ 0.9141, -0.1050,  1.3594,  ..., -0.4141, -1.2031,  0.4043],\n",
      "         [ 1.4531,  1.1641,  1.0156,  ..., -0.3867, -0.6094,  0.6719]]],\n",
      "       dtype=torch.bfloat16), pooler_output=tensor([[ 0.1738, -0.1426, -0.6133,  ...,  0.9453, -1.4844,  0.0967]],\n",
      "       dtype=torch.bfloat16), hidden_states=(tensor([[[ 0.0342, -0.0408, -0.1670,  ...,  0.3203, -0.1475, -0.0201],\n",
      "         [-0.1187, -0.0493, -0.2656,  ...,  0.5859, -0.1318, -0.0098],\n",
      "         [ 0.2393, -0.0437, -0.6172,  ...,  0.3438, -0.1445, -0.0098],\n",
      "         ...,\n",
      "         [ 0.1108, -0.0439, -0.1240,  ..., -0.0601, -0.1357, -0.0162],\n",
      "         [ 0.1855, -0.0454, -0.1118,  ..., -0.0903, -0.1357, -0.0112],\n",
      "         [ 0.0308, -0.0486, -0.0287,  ..., -0.0150, -0.1436, -0.0153]]],\n",
      "       dtype=torch.bfloat16), tensor([[[ 0.0530,  0.1074, -0.1055,  ...,  0.1094,  0.0781,  0.0308],\n",
      "         [ 0.0371,  0.0625, -0.1621,  ...,  0.3711, -0.0166, -0.0398],\n",
      "         [ 0.2490,  0.0527, -0.4141,  ...,  0.1621,  0.1074,  0.1143],\n",
      "         ...,\n",
      "         [ 0.0698, -0.0698,  0.0410,  ..., -0.2988, -0.0195, -0.0435],\n",
      "         [ 0.1904, -0.1289,  0.0171,  ..., -0.2988,  0.0146, -0.0525],\n",
      "         [ 0.1396,  0.0537, -0.0117,  ..., -0.0732, -0.0156, -0.0083]]],\n",
      "       dtype=torch.bfloat16), tensor([[[ 0.0408,  0.0098, -0.0869,  ...,  0.0674, -0.0288,  0.0486],\n",
      "         [ 0.1133,  0.0054, -0.0918,  ...,  0.2412,  0.1250, -0.0337],\n",
      "         [ 0.2070, -0.0588, -0.2832,  ...,  0.1523,  0.1006,  0.2051],\n",
      "         ...,\n",
      "         [-0.0547, -0.1973,  0.0098,  ..., -0.4551, -0.1118, -0.1055],\n",
      "         [ 0.0205, -0.3027, -0.0288,  ..., -0.4922, -0.0635, -0.1338],\n",
      "         [ 0.1348,  0.1006, -0.0991,  ..., -0.1621, -0.0354, -0.0364]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.0488,  0.0015, -0.0503,  ...,  0.0510,  0.0056,  0.0938],\n",
      "         [ 0.1680, -0.0400, -0.0239,  ...,  0.2852,  0.3242, -0.0232],\n",
      "         [ 0.2305,  0.0479, -0.0625,  ...,  0.1836,  0.1514,  0.3594],\n",
      "         ...,\n",
      "         [-0.1992, -0.2344, -0.0156,  ..., -0.4023, -0.1602, -0.0654],\n",
      "         [-0.0586, -0.2539,  0.0259,  ..., -0.4316, -0.1309, -0.2139],\n",
      "         [-0.0352,  0.3262, -0.1660,  ..., -0.1484,  0.0164, -0.0408]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.0933, -0.0166,  0.0188,  ...,  0.0811, -0.0063,  0.1025],\n",
      "         [ 0.2500,  0.0225, -0.0117,  ...,  0.3145,  0.3789, -0.0177],\n",
      "         [ 0.0527, -0.1914, -0.0776,  ...,  0.2598,  0.0928,  0.4062],\n",
      "         ...,\n",
      "         [-0.2158, -0.3867, -0.1240,  ..., -0.3145, -0.2236, -0.1523],\n",
      "         [ 0.0273, -0.4551, -0.0640,  ..., -0.4160, -0.2324, -0.3340],\n",
      "         [-0.0315,  0.3945, -0.0322,  ..., -0.0762, -0.0112, -0.0608]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.0942, -0.0542,  0.0381,  ...,  0.0903,  0.1113,  0.1201],\n",
      "         [ 0.0859,  0.0083,  0.1484,  ...,  0.3008,  0.3086, -0.0874],\n",
      "         [-0.0786, -0.2197,  0.0293,  ...,  0.3516,  0.0562,  0.4531],\n",
      "         ...,\n",
      "         [-0.1406, -0.4141,  0.0762,  ..., -0.2148, -0.1348, -0.1699],\n",
      "         [ 0.0317, -0.5781,  0.1592,  ..., -0.3750, -0.2480, -0.2344],\n",
      "         [-0.0894,  0.3516, -0.1387,  ..., -0.1084, -0.0137,  0.0728]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.0308,  0.0203,  0.0535,  ...,  0.0515,  0.1348,  0.0703],\n",
      "         [-0.1006,  0.1211,  0.1147,  ...,  0.4551,  0.3164, -0.0635],\n",
      "         [-0.0159, -0.1777,  0.0645,  ...,  0.4121, -0.0640,  0.4297],\n",
      "         ...,\n",
      "         [-0.2422, -0.3672,  0.2422,  ..., -0.2158, -0.2656, -0.2969],\n",
      "         [-0.0569, -0.6250,  0.1982,  ..., -0.3145, -0.2832, -0.3008],\n",
      "         [ 0.0088,  0.3867, -0.2041,  ..., -0.0801,  0.0625,  0.0391]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.0103,  0.0128,  0.0366,  ...,  0.0708,  0.0625,  0.0525],\n",
      "         [-0.3164,  0.0747,  0.1641,  ...,  0.7266,  0.2988, -0.0088],\n",
      "         [ 0.1602, -0.2061,  0.1816,  ...,  0.6445, -0.1777,  0.0928],\n",
      "         ...,\n",
      "         [-0.3145, -0.1406,  0.4531,  ..., -0.0132, -0.3164, -0.0225],\n",
      "         [-0.1211, -0.5859,  0.3164,  ..., -0.1279, -0.3125, -0.1680],\n",
      "         [ 0.0259,  0.3809, -0.0156,  ...,  0.0625,  0.1445,  0.1680]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.0405, -0.0493,  0.0444,  ...,  0.1738,  0.0366,  0.0183],\n",
      "         [-0.1943, -0.0244,  0.0264,  ...,  0.8086,  0.1758, -0.1436],\n",
      "         [ 0.0830, -0.3789,  0.3184,  ...,  0.8594, -0.1553, -0.1123],\n",
      "         ...,\n",
      "         [-0.3125, -0.2754,  0.4668,  ..., -0.1162, -0.4883,  0.2266],\n",
      "         [-0.3027, -0.6406,  0.2930,  ..., -0.2217, -0.4023, -0.1172],\n",
      "         [-0.0498,  0.3848, -0.3711,  ...,  0.0718, -0.1895, -0.0547]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.1196, -0.0562, -0.0332,  ...,  0.1270, -0.0732, -0.0430],\n",
      "         [-0.0430, -0.1641, -0.0894,  ...,  0.8047,  0.3125, -0.0527],\n",
      "         [ 0.0708, -0.3242,  0.3379,  ...,  0.8828, -0.2617,  0.0908],\n",
      "         ...,\n",
      "         [-0.3320, -0.4219,  0.1191,  ..., -0.2988, -0.6133,  0.4238],\n",
      "         [-0.4551, -0.8320, -0.0947,  ..., -0.4375, -0.8203,  0.0026],\n",
      "         [ 0.0371,  0.3203, -0.3223,  ..., -0.1279, -0.1338, -0.1406]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.0908, -0.0972,  0.0366,  ...,  0.1235, -0.0913,  0.0771],\n",
      "         [ 0.1089,  0.1504, -0.1641,  ...,  0.5195,  0.3262,  0.1123],\n",
      "         [ 0.0762,  0.0742,  0.1279,  ...,  0.8398, -0.1484,  0.0156],\n",
      "         ...,\n",
      "         [-0.3594, -0.3535,  0.2910,  ..., -0.1055, -0.6250,  0.3281],\n",
      "         [-0.3555, -0.5977,  0.2578,  ..., -0.3359, -0.5781,  0.0245],\n",
      "         [ 0.1494,  0.3438, -0.0767,  ..., -0.0713, -0.0029, -0.2080]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.0205, -0.0469,  0.0039,  ...,  0.0718, -0.0884,  0.1406],\n",
      "         [ 0.0825,  0.2061, -0.0703,  ...,  0.5234,  0.5195,  0.0884],\n",
      "         [ 0.0100,  0.1836,  0.0386,  ...,  0.9297, -0.2080, -0.0530],\n",
      "         ...,\n",
      "         [-0.2354, -0.3633,  0.2852,  ...,  0.0576, -0.5312,  0.0786],\n",
      "         [-0.2227, -0.4297,  0.0977,  ..., -0.2578, -0.4961, -0.3027],\n",
      "         [-0.0957,  0.2129,  0.2832,  ..., -0.0073, -0.1260, -0.3750]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.0339, -0.1240, -0.1602,  ...,  0.0713, -0.1895,  0.1230],\n",
      "         [-0.0312,  0.2812, -0.2451,  ...,  0.5273,  0.3281,  0.1660],\n",
      "         [-0.0645,  0.1914,  0.0801,  ...,  0.9336, -0.1758,  0.0018],\n",
      "         ...,\n",
      "         [-0.2734, -0.1035,  0.3262,  ..., -0.0674, -0.5469,  0.0713],\n",
      "         [-0.0542, -0.5039,  0.1240,  ..., -0.2168, -0.5195, -0.5586],\n",
      "         [-0.0098,  0.2012,  0.3828,  ..., -0.1279, -0.0874, -0.1807]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.0840, -0.0293, -0.0859,  ...,  0.1201, -0.2158,  0.1787],\n",
      "         [-0.0459,  0.2490, -0.0205,  ...,  0.4004,  0.3223,  0.1426],\n",
      "         [-0.1108,  0.1357, -0.0801,  ...,  0.7109, -0.2734, -0.2148],\n",
      "         ...,\n",
      "         [-0.2168,  0.0405,  0.2490,  ..., -0.0068, -0.1289,  0.0217],\n",
      "         [ 0.0571, -0.3633,  0.0942,  ..., -0.2949, -0.1094, -0.4590],\n",
      "         [ 0.2148,  0.1943,  0.3262,  ..., -0.1768,  0.0806, -0.2988]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.1719, -0.0371, -0.1592,  ...,  0.1055, -0.2656,  0.0127],\n",
      "         [ 0.0337,  0.2451,  0.1396,  ...,  0.1426,  0.2158,  0.1562],\n",
      "         [-0.1445,  0.1455, -0.2773,  ...,  0.3652, -0.1572, -0.1777],\n",
      "         ...,\n",
      "         [ 0.1152,  0.0645,  0.3359,  ..., -0.3438,  0.0118,  0.0430],\n",
      "         [ 0.0591, -0.3750,  0.1045,  ..., -0.5664, -0.0610, -0.2119],\n",
      "         [ 0.2578,  0.0405,  0.5352,  ..., -0.4746,  0.0566, -0.2871]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.1240, -0.1602, -0.0991,  ...,  0.1123, -0.2578,  0.1191],\n",
      "         [-0.0027,  0.2715,  0.1855,  ..., -0.1982,  0.3320,  0.3789],\n",
      "         [-0.3613,  0.2500, -0.1035,  ...,  0.1357, -0.0771,  0.0137],\n",
      "         ...,\n",
      "         [ 0.2578,  0.1133,  0.3965,  ...,  0.0923, -0.1484,  0.3008],\n",
      "         [-0.0366, -0.4805,  0.2217,  ..., -0.0608, -0.3340, -0.1592],\n",
      "         [ 0.3828,  0.0098,  0.5312,  ..., -0.4355, -0.0300, -0.0439]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.1157, -0.1279, -0.1006,  ..., -0.0732, -0.2637,  0.1235],\n",
      "         [-0.0928,  0.4102,  0.2002,  ...,  0.0062,  0.2402,  0.4551],\n",
      "         [-0.3125,  0.4824, -0.2100,  ...,  0.2832, -0.0742,  0.0564],\n",
      "         ...,\n",
      "         [ 0.3867,  0.3066,  0.2236,  ...,  0.2695, -0.1406,  0.2871],\n",
      "         [-0.0635, -0.2080,  0.1748,  ...,  0.0251, -0.3496, -0.3457],\n",
      "         [ 0.5234,  0.2676,  0.4531,  ..., -0.5312, -0.0742, -0.0635]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.1152,  0.1089,  0.0088,  ..., -0.1250, -0.3516,  0.1035],\n",
      "         [-0.1348, -0.1104,  0.1709,  ..., -0.1475,  0.1621,  0.2812],\n",
      "         [-0.1963,  0.2695, -0.3086,  ...,  0.1504,  0.0527, -0.1533],\n",
      "         ...,\n",
      "         [ 0.4785,  0.4980,  0.2236,  ...,  0.1719, -0.0986,  0.2500],\n",
      "         [ 0.0625, -0.0176,  0.1934,  ..., -0.2207, -0.3770, -0.3965],\n",
      "         [ 0.6055,  0.2334,  0.4629,  ..., -0.4199, -0.2832, -0.1426]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.1533,  0.1006,  0.0013,  ..., -0.0967, -0.2002,  0.0542],\n",
      "         [-0.2363, -0.1387, -0.3262,  ..., -0.2988,  0.1406,  0.2891],\n",
      "         [-0.2695,  0.3008, -0.2773,  ...,  0.0967,  0.2891, -0.1094],\n",
      "         ...,\n",
      "         [ 0.5508,  0.6797,  0.4336,  ...,  0.1338, -0.2422,  0.2656],\n",
      "         [-0.1777, -0.0898, -0.0527,  ..., -0.1875, -0.3691, -0.5117],\n",
      "         [ 0.5820,  0.2217,  0.2910,  ..., -0.4141, -0.2402, -0.0972]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-2.6367e-01,  1.0547e-01, -6.9336e-02,  ..., -5.2734e-02,\n",
      "          -3.1250e-01,  1.6602e-02],\n",
      "         [-3.7695e-01, -5.4688e-01, -3.8281e-01,  ...,  8.7891e-03,\n",
      "           4.9805e-01,  8.5938e-02],\n",
      "         [-4.5898e-01,  2.7344e-01, -8.0078e-02,  ...,  9.2773e-02,\n",
      "           3.1641e-01, -6.5918e-02],\n",
      "         ...,\n",
      "         [ 5.3906e-01,  7.6172e-01,  4.7266e-01,  ...,  2.0703e-01,\n",
      "          -2.8320e-01,  1.7383e-01],\n",
      "         [-2.3828e-01, -4.2383e-01, -4.8828e-04,  ..., -1.7383e-01,\n",
      "          -4.9609e-01, -3.5156e-01],\n",
      "         [ 5.3516e-01,  1.7969e-01,  3.1641e-01,  ..., -2.1582e-01,\n",
      "          -4.9219e-01, -2.4414e-03]]], dtype=torch.bfloat16), tensor([[[ 0.0703,  0.1992, -0.1992,  ..., -0.0293, -0.3535,  0.2363],\n",
      "         [-0.2383, -0.2031, -0.0596,  ..., -0.1074,  0.4258,  0.0693],\n",
      "         [-0.2188,  0.5391,  0.2070,  ...,  0.1021,  0.0771, -0.0388],\n",
      "         ...,\n",
      "         [ 0.8633,  0.7344,  0.5742,  ...,  0.1094, -0.3906,  0.2051],\n",
      "         [ 0.2002, -0.5156,  0.1436,  ..., -0.2891, -0.6172, -0.3027],\n",
      "         [ 0.9883,  0.4512,  0.5039,  ..., -0.3164, -0.5156,  0.2002]]],\n",
      "       dtype=torch.bfloat16), tensor([[[ 0.0449,  0.2178, -0.2490,  ...,  0.1367, -0.3242,  0.4492],\n",
      "         [-0.3594, -0.3027,  0.4336,  ...,  0.0020,  0.4023,  0.0400],\n",
      "         [ 0.2754,  0.8047, -0.0332,  ...,  0.0479, -0.3086, -0.3945],\n",
      "         ...,\n",
      "         [ 1.6094,  1.1562,  0.6641,  ..., -0.5430, -0.8320,  0.4023],\n",
      "         [ 1.1328, -0.2031,  0.7227,  ..., -0.9531, -0.7656, -0.3398],\n",
      "         [ 1.8672,  0.9336,  0.6797,  ..., -0.9922, -0.4082,  0.0059]]],\n",
      "       dtype=torch.bfloat16), tensor([[[ 0.0347,  0.3184, -0.2930,  ...,  0.3086, -0.3535,  0.1572],\n",
      "         [ 0.2930, -0.0312,  0.3555,  ...,  0.3984,  0.5000, -0.2578],\n",
      "         [ 0.0977,  0.7734,  0.4414,  ...,  0.4375, -0.1816, -0.5898],\n",
      "         ...,\n",
      "         [ 1.4297,  1.2031,  1.2656,  ..., -0.1641, -0.8711,  0.9180],\n",
      "         [ 0.9609, -0.0039,  1.2344,  ..., -0.3906, -0.9922, -0.0430],\n",
      "         [ 1.5625,  1.1016,  1.1250,  ..., -0.4961, -0.5000,  0.4141]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.0054,  0.2051, -0.7422,  ...,  0.4414, -0.5078, -0.1128],\n",
      "         [ 0.1074,  0.1348, -0.4238,  ...,  0.1475,  0.3477, -0.4414],\n",
      "         [-0.3164,  0.9922,  0.3301,  ...,  0.4297, -0.2090, -0.7070],\n",
      "         ...,\n",
      "         [ 1.6094,  0.9805,  0.8906,  ..., -0.2812, -1.0391,  1.2031],\n",
      "         [ 0.8398, -0.3633,  0.6953,  ..., -0.2754, -1.2266,  0.4453],\n",
      "         [ 1.5391,  0.7812,  0.3848,  ..., -0.2246, -0.7266,  0.8047]]],\n",
      "       dtype=torch.bfloat16), tensor([[[ 0.0879, -0.1445, -0.2949,  ...,  0.3906, -0.7188,  0.0356],\n",
      "         [ 0.4629,  0.0078,  0.0859,  ..., -0.0815,  0.1113, -0.3906],\n",
      "         [-0.2188,  1.0938,  1.0469,  ...,  0.2119, -0.3477, -0.8477],\n",
      "         ...,\n",
      "         [ 1.7500,  1.0859,  1.6719,  ..., -0.2891, -0.6797,  1.1016],\n",
      "         [ 0.9141, -0.1050,  1.3594,  ..., -0.4141, -1.2031,  0.4043],\n",
      "         [ 1.4531,  1.1641,  1.0156,  ..., -0.3867, -0.6094,  0.6719]]],\n",
      "       dtype=torch.bfloat16)), attentions=None)\n",
      "image_features (after select_layer)\n",
      " <class 'torch.Tensor'>\n",
      "image_features.shape\n",
      " torch.Size([1, 577, 1024])\n",
      "【COND】 select_feature=patch\n",
      "【ENTER】if self.select_feature == 'patch':\n",
      "original image_features\n",
      " tensor([[[-0.0054,  0.2051, -0.7422,  ...,  0.4414, -0.5078, -0.1128],\n",
      "         [ 0.1074,  0.1348, -0.4238,  ...,  0.1475,  0.3477, -0.4414],\n",
      "         [-0.3164,  0.9922,  0.3301,  ...,  0.4297, -0.2090, -0.7070],\n",
      "         ...,\n",
      "         [ 1.6094,  0.9805,  0.8906,  ..., -0.2812, -1.0391,  1.2031],\n",
      "         [ 0.8398, -0.3633,  0.6953,  ..., -0.2754, -1.2266,  0.4453],\n",
      "         [ 1.5391,  0.7812,  0.3848,  ..., -0.2246, -0.7266,  0.8047]]],\n",
      "       dtype=torch.bfloat16)\n",
      "after process\n",
      " tensor([[[ 0.1074,  0.1348, -0.4238,  ...,  0.1475,  0.3477, -0.4414],\n",
      "         [-0.3164,  0.9922,  0.3301,  ...,  0.4297, -0.2090, -0.7070],\n",
      "         [ 1.2656, -0.0146, -0.6602,  ...,  0.2432,  0.3555,  0.3438],\n",
      "         ...,\n",
      "         [ 1.6094,  0.9805,  0.8906,  ..., -0.2812, -1.0391,  1.2031],\n",
      "         [ 0.8398, -0.3633,  0.6953,  ..., -0.2754, -1.2266,  0.4453],\n",
      "         [ 1.5391,  0.7812,  0.3848,  ..., -0.2246, -0.7266,  0.8047]]],\n",
      "       dtype=torch.bfloat16)\n",
      "【EXIT】if self.select_feature == 'patch':\n",
      "selected image_feature shape\n",
      " torch.Size([1, 576, 1024])\n",
      "image_features (return)\n",
      " tensor([[[ 0.1074,  0.1348, -0.4238,  ...,  0.1475,  0.3477, -0.4414],\n",
      "         [-0.3164,  0.9922,  0.3301,  ...,  0.4297, -0.2090, -0.7070],\n",
      "         [ 1.2656, -0.0146, -0.6602,  ...,  0.2432,  0.3555,  0.3438],\n",
      "         ...,\n",
      "         [ 1.6094,  0.9805,  0.8906,  ..., -0.2812, -1.0391,  1.2031],\n",
      "         [ 0.8398, -0.3633,  0.6953,  ..., -0.2754, -1.2266,  0.4453],\n",
      "         [ 1.5391,  0.7812,  0.3848,  ..., -0.2246, -0.7266,  0.8047]]],\n",
      "       dtype=torch.bfloat16)\n",
      "image_features.shape\n",
      " torch.Size([1, 576, 1024])\n",
      "after process image_features\n",
      " <class 'torch.Tensor'>\n",
      "【EXIT】else (type(images) is not list):\n",
      "image_features (return)\n",
      " tensor([[[ 0.1074,  0.1348, -0.4238,  ...,  0.1475,  0.3477, -0.4414],\n",
      "         [-0.3164,  0.9922,  0.3301,  ...,  0.4297, -0.2090, -0.7070],\n",
      "         [ 1.2656, -0.0146, -0.6602,  ...,  0.2432,  0.3555,  0.3438],\n",
      "         ...,\n",
      "         [ 1.6094,  0.9805,  0.8906,  ..., -0.2812, -1.0391,  1.2031],\n",
      "         [ 0.8398, -0.3633,  0.6953,  ..., -0.2754, -1.2266,  0.4453],\n",
      "         [ 1.5391,  0.7812,  0.3848,  ..., -0.2246, -0.7266,  0.8047]]])\n",
      "image_features.shape\n",
      " torch.Size([1, 576, 1024])\n",
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.get_model(self)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "self.model (return)\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      "  (vision_tower): CLIPVisionTower(\n",
      "    (vision_tower): CLIPVisionModel(\n",
      "      (vision_model): CLIPVisionTransformer(\n",
      "        (embeddings): CLIPVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "          (position_embedding): Embedding(577, 1024)\n",
      "        )\n",
      "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder): CLIPEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-23): 24 x CLIPEncoderLayer(\n",
      "              (self_attn): CLIPAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): CLIPMLP(\n",
      "                (activation_fn): QuickGELUActivation()\n",
      "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mm_projector): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n",
      "image_features (return) shape\n",
      " torch.Size([1, 576, 2048])\n",
      "image_features (return)\n",
      " tensor([[[-0.1943,  0.1157, -0.0747,  ...,  0.0027, -0.1691, -0.3439],\n",
      "         [ 0.0437,  0.1717, -0.0998,  ...,  0.0930, -0.1386, -0.0731],\n",
      "         [-0.0505,  0.1592, -0.0982,  ...,  0.0866, -0.1123, -0.2177],\n",
      "         ...,\n",
      "         [-0.0182,  0.0850, -0.0556,  ...,  0.0622, -0.1969,  0.0129],\n",
      "         [-0.0651,  0.0586, -0.1218,  ..., -0.0614, -0.1158, -0.0104],\n",
      "         [ 0.0863,  0.0081, -0.1651,  ..., -0.2040, -0.0455,  0.0618]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "image_features after encode_images shape \n",
      " torch.Size([1, 576, 2048])\n",
      "image_features after encode_images\n",
      " tensor([[[-0.1943,  0.1157, -0.0747,  ...,  0.0027, -0.1691, -0.3439],\n",
      "         [ 0.0437,  0.1717, -0.0998,  ...,  0.0930, -0.1386, -0.0731],\n",
      "         [-0.0505,  0.1592, -0.0982,  ...,  0.0866, -0.1123, -0.2177],\n",
      "         ...,\n",
      "         [-0.0182,  0.0850, -0.0556,  ...,  0.0622, -0.1969,  0.0129],\n",
      "         [-0.0651,  0.0586, -0.1218,  ..., -0.0614, -0.1158, -0.0104],\n",
      "         [ 0.0863,  0.0081, -0.1651,  ..., -0.2040, -0.0455,  0.0618]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "【EXIT】else of if type(images) is list or images.ndim == 5:\n",
      "labels before\n",
      " tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "           411,  2654, 11315,    13]])\n",
      "position_ids before\n",
      " None\n",
      "attention_mask before\n",
      " tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True]])\n",
      "【ENTER】else of if attention_mask is None:\n",
      "attention_mask（after）shape \n",
      " torch.Size([1, 24])\n",
      "attention_mask (after)\n",
      " tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True]])\n",
      "【EXIT】else of if attention_mask is None:\n",
      "【ENTER】if position_ids is None:\n",
      "position_ids (after) shape \n",
      " torch.Size([24])\n",
      "position_ids (after)\n",
      " tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23])\n",
      "【EXIT】if position_ids is None:\n",
      "【COND】 labels_is_None=False\n",
      "input_ids after removing padding\n",
      " [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "          411,  2654, 11315,    13])]\n",
      "labels after removing padding\n",
      " [tensor([ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "          411,  2654, 11315,    13])]\n",
      "cur_input_ids shape\n",
      " torch.Size([24])\n",
      "cur_input_ids\n",
      " tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "          411,  2654, 11315,    13])\n",
      "【COND】num_images: tensor(1)\n",
      "image_token_indices\n",
      " [-1, 1, 24]\n",
      "len image_token_indices 3\n",
      "cur_labels\n",
      " tensor([ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "          411,  2654, 11315,    13])\n",
      "cur_input_ids_noim (after)\n",
      " [tensor([1]), tensor([  278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596,\n",
      "        23425,   278,  3700,   322,  6567,   310,   263,  6114,   411,  2654,\n",
      "        11315,    13])]\n",
      "cur_labels_noim (after) \n",
      " [tensor([-100]), tensor([  278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596,\n",
      "        23425,   278,  3700,   322,  6567,   310,   263,  6114,   411,  2654,\n",
      "        11315,    13])]\n",
      "split_sizes\n",
      " [1, 22]\n",
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.get_model(self)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "self.model (return)\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      "  (vision_tower): CLIPVisionTower(\n",
      "    (vision_tower): CLIPVisionModel(\n",
      "      (vision_model): CLIPVisionTransformer(\n",
      "        (embeddings): CLIPVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "          (position_embedding): Embedding(577, 1024)\n",
      "        )\n",
      "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder): CLIPEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-23): 24 x CLIPEncoderLayer(\n",
      "              (self_attn): CLIPAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): CLIPMLP(\n",
      "                (activation_fn): QuickGELUActivation()\n",
      "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mm_projector): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n",
      "cur_input_embeds shape\n",
      " torch.Size([23, 2048])\n",
      "cur_input_embeds\n",
      " tensor([[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
      "         -6.5231e-04, -4.9973e-04],\n",
      "        [ 7.0801e-03,  1.0452e-03,  6.0425e-03,  ...,  3.9673e-03,\n",
      "          1.2817e-03, -1.1215e-03],\n",
      "        [-2.2949e-02, -2.6226e-05,  6.8359e-03,  ..., -2.4658e-02,\n",
      "         -9.4604e-03,  1.5869e-02],\n",
      "        ...,\n",
      "        [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
      "         -8.3618e-03, -9.4604e-03],\n",
      "        [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
      "         -2.5177e-03, -8.0566e-03],\n",
      "        [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
      "         -7.3242e-04,  2.7924e-03]], requires_grad=True)\n",
      "cur_input_embeds_no_im\n",
      " (tensor([[-0.0011,  0.0019, -0.0017,  ...,  0.0002, -0.0007, -0.0005]],\n",
      "       grad_fn=<SplitWithSizesBackward0>), tensor([[ 7.0801e-03,  1.0452e-03,  6.0425e-03,  ...,  3.9673e-03,\n",
      "          1.2817e-03, -1.1215e-03],\n",
      "        [-2.2949e-02, -2.6226e-05,  6.8359e-03,  ..., -2.4658e-02,\n",
      "         -9.4604e-03,  1.5869e-02],\n",
      "        [-2.3499e-03,  1.4893e-02, -2.0447e-03,  ..., -8.6060e-03,\n",
      "          2.3193e-03,  3.0670e-03],\n",
      "        ...,\n",
      "        [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
      "         -8.3618e-03, -9.4604e-03],\n",
      "        [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
      "         -2.5177e-03, -8.0566e-03],\n",
      "        [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
      "         -7.3242e-04,  2.7924e-03]], grad_fn=<SplitWithSizesBackward0>))\n",
      "【COND】 i=0 num_images=1\n",
      "【ENTER】if i < num_images:\n",
      "【EXIT】if i < num_images:\n",
      "【COND】 i=1 num_images=1\n",
      "cur_new_input_embeds (before cat) shape\n",
      " [torch.Size([1, 2048]), torch.Size([576, 2048]), torch.Size([22, 2048])]\n",
      "cur_new_input_embeds (before cat)\n",
      " [tensor([[-0.0011,  0.0019, -0.0017,  ...,  0.0002, -0.0007, -0.0005]],\n",
      "       grad_fn=<SplitWithSizesBackward0>), tensor([[-0.1943,  0.1157, -0.0747,  ...,  0.0027, -0.1691, -0.3439],\n",
      "        [ 0.0437,  0.1717, -0.0998,  ...,  0.0930, -0.1386, -0.0731],\n",
      "        [-0.0505,  0.1592, -0.0982,  ...,  0.0866, -0.1123, -0.2177],\n",
      "        ...,\n",
      "        [-0.0182,  0.0850, -0.0556,  ...,  0.0622, -0.1969,  0.0129],\n",
      "        [-0.0651,  0.0586, -0.1218,  ..., -0.0614, -0.1158, -0.0104],\n",
      "        [ 0.0863,  0.0081, -0.1651,  ..., -0.2040, -0.0455,  0.0618]],\n",
      "       grad_fn=<SelectBackward0>), tensor([[ 7.0801e-03,  1.0452e-03,  6.0425e-03,  ...,  3.9673e-03,\n",
      "          1.2817e-03, -1.1215e-03],\n",
      "        [-2.2949e-02, -2.6226e-05,  6.8359e-03,  ..., -2.4658e-02,\n",
      "         -9.4604e-03,  1.5869e-02],\n",
      "        [-2.3499e-03,  1.4893e-02, -2.0447e-03,  ..., -8.6060e-03,\n",
      "          2.3193e-03,  3.0670e-03],\n",
      "        ...,\n",
      "        [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
      "         -8.3618e-03, -9.4604e-03],\n",
      "        [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
      "         -2.5177e-03, -8.0566e-03],\n",
      "        [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
      "         -7.3242e-04,  2.7924e-03]], grad_fn=<SplitWithSizesBackward0>)]\n",
      "cur_new_labels (before cat) shape\n",
      " [torch.Size([1]), torch.Size([576]), torch.Size([22])]\n",
      "cur_new_labels (before cat)\n",
      " [tensor([-100]), tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]), tensor([  278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596,\n",
      "        23425,   278,  3700,   322,  6567,   310,   263,  6114,   411,  2654,\n",
      "        11315,    13])]\n",
      "cur_new_input_embeds (after cat) shape\n",
      " torch.Size([599, 2048])\n",
      "cur_new_input_embeds (after cat)\n",
      " tensor([[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
      "         -6.5231e-04, -4.9973e-04],\n",
      "        [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
      "         -1.6907e-01, -3.4387e-01],\n",
      "        [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
      "         -1.3859e-01, -7.3106e-02],\n",
      "        ...,\n",
      "        [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
      "         -8.3618e-03, -9.4604e-03],\n",
      "        [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
      "         -2.5177e-03, -8.0566e-03],\n",
      "        [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
      "         -7.3242e-04,  2.7924e-03]], grad_fn=<CatBackward0>)\n",
      "cur_new_labels (after cat) shape\n",
      " torch.Size([599])\n",
      "cur_new_labels (after cat)\n",
      " tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
      "          297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
      "          322,  6567,   310,   263,  6114,   411,  2654, 11315,    13])\n",
      "new_input_embeds (so far) shape\n",
      " [torch.Size([599, 2048])]\n",
      "new_input_embeds (so far)\n",
      " [tensor([[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
      "         -6.5231e-04, -4.9973e-04],\n",
      "        [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
      "         -1.6907e-01, -3.4387e-01],\n",
      "        [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
      "         -1.3859e-01, -7.3106e-02],\n",
      "        ...,\n",
      "        [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
      "         -8.3618e-03, -9.4604e-03],\n",
      "        [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
      "         -2.5177e-03, -8.0566e-03],\n",
      "        [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
      "         -7.3242e-04,  2.7924e-03]], grad_fn=<CatBackward0>)]\n",
      "new_labels (so far) shape\n",
      " [torch.Size([599])]\n",
      "new_labels (so far)\n",
      " [tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
      "          297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
      "          322,  6567,   310,   263,  6114,   411,  2654, 11315,    13])]\n",
      "【COND】 tokenizer_model_max_length_is_not_None=True\n",
      "【ENTER】if tokenizer_model_max_length is not None:\n",
      "【EXIT】if tokenizer_model_max_length is not None:\n",
      "max_len\n",
      " 599\n",
      "batch_size\n",
      " 1\n",
      "new_labels_padded (before) shape\n",
      " torch.Size([1, 599])\n",
      "new_labels_padded (before)\n",
      " tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]])\n",
      "attention_mask (before) shape\n",
      " torch.Size([1, 599])\n",
      "attention_mask (before)\n",
      " tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False]])\n",
      "position_ids (before) shape\n",
      " torch.Size([1, 599])\n",
      "position_ids (before)\n",
      " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "【COND】 padding_side=right cur_len=599 max_len=599\n",
      "【ENTER】else (padding_side != 'left'):\n",
      "new_input_embeds_padded (so far) shape\n",
      " [torch.Size([599, 2048])]\n",
      "new_input_embeds_padded (so far)\n",
      " [tensor([[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
      "         -6.5231e-04, -4.9973e-04],\n",
      "        [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
      "         -1.6907e-01, -3.4387e-01],\n",
      "        [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
      "         -1.3859e-01, -7.3106e-02],\n",
      "        ...,\n",
      "        [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
      "         -8.3618e-03, -9.4604e-03],\n",
      "        [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
      "         -2.5177e-03, -8.0566e-03],\n",
      "        [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
      "         -7.3242e-04,  2.7924e-03]], grad_fn=<CatBackward0>)]\n",
      "new_labels_padded (so far) shape\n",
      " torch.Size([1, 599])\n",
      "new_labels_padded (so far)\n",
      " tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
      "           297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
      "           322,  6567,   310,   263,  6114,   411,  2654, 11315,    13]])\n",
      "attention_mask (so far) shape\n",
      " torch.Size([1, 599])\n",
      "attention_mask (so far)\n",
      " tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True]])\n",
      "position_ids (so far) shape\n",
      " torch.Size([1, 599])\n",
      "position_ids (so far)\n",
      " tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "         126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "         140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "         154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "         168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "         182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "         196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "         210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "         224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "         238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "         252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "         266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "         280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "         294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "         308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "         322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "         336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "         350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "         364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "         378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "         392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "         406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "         420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "         434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "         448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
      "         462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
      "         476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
      "         490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
      "         504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517,\n",
      "         518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531,\n",
      "         532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
      "         546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559,\n",
      "         560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573,\n",
      "         574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587,\n",
      "         588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598]])\n",
      "【EXIT】else (padding_side != 'left'):\n",
      "new_input_embeds (after) shape\n",
      " torch.Size([1, 599, 2048])\n",
      "new_input_embeds (after)\n",
      " tensor([[[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
      "          -6.5231e-04, -4.9973e-04],\n",
      "         [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
      "          -1.6907e-01, -3.4387e-01],\n",
      "         [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
      "          -1.3859e-01, -7.3106e-02],\n",
      "         ...,\n",
      "         [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
      "          -8.3618e-03, -9.4604e-03],\n",
      "         [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
      "          -2.5177e-03, -8.0566e-03],\n",
      "         [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
      "          -7.3242e-04,  2.7924e-03]]], grad_fn=<StackBackward0>)\n",
      "【COND】 _labels_is_None=False\n",
      "【ENTER】else of if _labels is None:\n",
      "new_labels (after)\n",
      " tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
      "           297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
      "           322,  6567,   310,   263,  6114,   411,  2654, 11315,    13]])\n",
      "【EXIT】else of if _labels is None:\n",
      "【COND】 _attention_mask_is_None=False\n",
      "【ENTER】else of if _attention_mask is None:\n",
      "attention_mask (after)\n",
      " tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True]])\n",
      "【COND】 _position_ids_is_None=True\n",
      "【ENTER】if _position_ids is None:\n",
      "【EXIT】if _position_ids is None:\n",
      "position_ids (return)\n",
      " None\n",
      "attention_mask (return)\n",
      " tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True]])\n",
      "past_key_values (return)\n",
      " None\n",
      "new_input_embeds (return)\n",
      " tensor([[[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
      "          -6.5231e-04, -4.9973e-04],\n",
      "         [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
      "          -1.6907e-01, -3.4387e-01],\n",
      "         [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
      "          -1.3859e-01, -7.3106e-02],\n",
      "         ...,\n",
      "         [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
      "          -8.3618e-03, -9.4604e-03],\n",
      "         [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
      "          -2.5177e-03, -8.0566e-03],\n",
      "         [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
      "          -7.3242e-04,  2.7924e-03]]], grad_fn=<StackBackward0>)\n",
      "new_labels (return)\n",
      " tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
      "           297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
      "           322,  6567,   310,   263,  6114,   411,  2654, 11315,    13]])\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    input_ids,\n",
    "    position_ids,\n",
    "    attention_mask,\n",
    "    past_key_values,\n",
    "    inputs_embeds,\n",
    "    labels\n",
    ") = model.prepare_inputs_labels_for_multimodal(\n",
    "    input_ids,\n",
    "    position_ids,\n",
    "    attention_mask,\n",
    "    past_key_values,\n",
    "    labels,\n",
    "    images,\n",
    "    image_sizes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m----> 3\u001b[0m     input_ids: \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m     attention_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m     position_ids: Optional[torch\u001b[38;5;241m.\u001b[39mLongTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     past_key_values: Optional[List[torch\u001b[38;5;241m.\u001b[39mFloatTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     inputs_embeds: Optional[torch\u001b[38;5;241m.\u001b[39mFloatTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     labels: Optional[torch\u001b[38;5;241m.\u001b[39mLongTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     use_cache: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m     output_hidden_states: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m     images: Optional[torch\u001b[38;5;241m.\u001b[39mFloatTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m     image_sizes: Optional[List[List[\u001b[38;5;28mint\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     return_dict: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, CausalLMOutputWithPast]:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent file path\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllava/llava/model/language_model/llava_llama.py\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdef LlavaLlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, image_sizes, return_dict)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def forward(\n",
    "    self,\n",
    "    input_ids: torch.LongTensor = None,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    labels: Optional[torch.LongTensor] = None,\n",
    "    use_cache: Optional[bool] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    images: Optional[torch.FloatTensor] = None,\n",
    "    image_sizes: Optional[List[List[int]]] = None,\n",
    "    return_dict: Optional[bool] = None,\n",
    ") -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "\n",
    "    print(\"current file path\", \"llava/llava/model/language_model/llava_llama.py\")\n",
    "    print(\"def LlavaLlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, image_sizes, return_dict)\")\n",
    "    print(\"input_ids\\n\", input_ids)\n",
    "    \"\"\"\n",
    "    tensor([[    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "            10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "            411,  2654, 11315,    13]], device='cuda:0')        \n",
    "    \"\"\"\n",
    "    if hasattr(input_ids, 'shape'):\n",
    "        print(\"input_ids.shape\\n\", input_ids.shape) # torch.Size([1, 24])\n",
    "    print(\"attention_mask\\n\", attention_mask)\n",
    "    \"\"\"\n",
    "    tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "            True, True, True, True, True, True, True, True, True, True, True, True]],\n",
    "        device='cuda:0')\n",
    "    \"\"\"\n",
    "    print(\"position_ids\\n\", position_ids) # None\n",
    "    print(\"past_key_values\\n\", past_key_values) # None\n",
    "    print(\"inputs_embeds\\n\", inputs_embeds) # None\n",
    "    if hasattr(inputs_embeds, 'shape'):\n",
    "        print(\"inputs_embeds.shape\\n\", inputs_embeds.shape)\n",
    "    print(\"labels\\n\", labels)\n",
    "    \"\"\"\n",
    "    tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "            10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "            411,  2654, 11315,    13]], device='cuda:0')\n",
    "    \"\"\"\n",
    "    print(\"use_cache\\n\", use_cache) # None\n",
    "    print(\"output_attentions\\n\", output_attentions) # None\n",
    "    print(\"output_hidden_states\\n\", output_hidden_states) # None\n",
    "    print(\"images\\n\", images)\n",
    "    \"\"\"\n",
    "    tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7109, -0.3613, -0.1279],\n",
    "            [ 0.0325,  0.0325,  0.0325,  ..., -0.3906, -0.1719, -0.0259],\n",
    "            [ 0.0325,  0.0325,  0.0325,  ..., -0.0112,  0.0471,  0.0908],\n",
    "            ...,\n",
    "            [-1.0312, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],\n",
    "            [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],\n",
    "            [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625]],\n",
    "\n",
    "            [[ 0.3184,  0.3184,  0.3184,  ..., -0.3867, -0.0112,  0.2139],\n",
    "            [ 0.3184,  0.3184,  0.3184,  ..., -0.0713,  0.1543,  0.3184],\n",
    "            [ 0.3184,  0.3184,  0.3184,  ...,  0.2891,  0.3633,  0.4395],\n",
    "            ...,\n",
    "            [-1.0156, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],\n",
    "            [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],\n",
    "            [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000]],\n",
    "\n",
    "            [[ 0.9648,  0.9648,  0.9648,  ...,  0.0981,  0.4531,  0.6680],\n",
    "            [ 0.9648,  0.9648,  0.9648,  ...,  0.3965,  0.6094,  0.7539],\n",
    "            [ 0.9648,  0.9648,  0.9648,  ...,  0.7539,  0.8086,  0.8359],\n",
    "            ...,\n",
    "            [-0.3711, -0.3848, -0.4004,  ..., -0.4277, -0.4277, -0.4277],\n",
    "            [-0.3711, -0.3711, -0.3848,  ..., -0.4277, -0.4277, -0.4277],\n",
    "            [-0.3848, -0.3711, -0.3711,  ..., -0.4277, -0.4277, -0.4277]]]],\n",
    "        device='cuda:0', dtype=torch.bfloat16)\n",
    "    \"\"\"\n",
    "    if hasattr(images, 'shape'):\n",
    "        print(\"images.shape\\n\", images.shape) # torch.Size([1, 3, 336, 336])\n",
    "    print(\"image_sizes\\n\", image_sizes) # None\n",
    "    print(\"return_dict\\n\", return_dict) # None\n",
    "\n",
    "    print(f\"【COND】 inputs_embeds_is_None={inputs_embeds is None}\") # True\n",
    "    if inputs_embeds is None:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】if inputs_embeds is None:\")\n",
    "        (\n",
    "            input_ids,\n",
    "            position_ids,\n",
    "            attention_mask,\n",
    "            past_key_values,\n",
    "            inputs_embeds,\n",
    "            labels\n",
    "        ) = self.prepare_inputs_labels_for_multimodal(\n",
    "            input_ids,\n",
    "            position_ids,\n",
    "            attention_mask,\n",
    "            past_key_values,\n",
    "            labels,\n",
    "            images,\n",
    "            image_sizes\n",
    "        )\n",
    "        print(\"【EXIT】if inputs_embeds is None:\")\n",
    "\n",
    "    print(\"input_ids (after prepare_inputs_labels_for_multimodal)\\n\", input_ids)\n",
    "\n",
    "    print(\"position_ids (after prepare_inputs_labels_for_multimodal)\\n\", position_ids)\n",
    "\n",
    "    print(\"attention_mask shape (after prepare_inputs_labels_for_multimodal)\\n\", attention_mask.shape)\n",
    "    print(\"attention_mask (after prepare_inputs_labels_for_multimodal)\\n\", attention_mask)\n",
    "\n",
    "\n",
    "    print(\"past_key_values (after prepare_inputs_labels_for_multimodal)\\n\", past_key_values)\n",
    "\n",
    "    print(\"inputs_embeds shape (after prepare_inputs_labels_for_multimodal)\\n\", None if inputs_embeds is None else inputs_embeds.shape)\n",
    "    print(\"inputs_embeds (after prepare_inputs_labels_for_multimodal)\\n\", inputs_embeds)\n",
    "\n",
    "    print(\"labels shape (after prepare_inputs_labels_for_multimodal)\\n\", labels.shape)\n",
    "    print(\"labels (after prepare_inputs_labels_for_multimodal)\\n\", labels)\n",
    "\n",
    "    #  LlamaForCausalLM.forward(self, ...)で明示\n",
    "    # Trainer > def train > def inner_training_loop > def training_step > model(**inputs) > model.forward\n",
    "    result = LlamaForCausalLM.forward(\n",
    "        self,\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_values=past_key_values,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        labels=labels,\n",
    "        use_cache=use_cache,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict\n",
    "    )\n",
    "    print(\"Return of def LlavaLlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, image_sizes, return_dict)\")\n",
    "    #print(\"result of LlavaLlamaForCausalLM.forward (return)\\n\", result)\n",
    "    print(\"logits tensor shape  LlavaLlamaForCausalLM.forward\\n\", result.logits.shape)\n",
    "    print(\"logits tensor (first 10 tokens)  LlavaLlamaForCausalLM.forward\\n\", result.logits[0, :10, :])\n",
    "    print(\"loss (return)  LlavaLlamaForCausalLM.forward \\n\", result.loss)\n",
    "    \"\"\"\n",
    "    CausalLMOutputWithPast(loss=tensor(7.3094, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[  0.8242,   0.1855,  -0.7031,  ...,   1.6719,   2.6719,   1.1875],\n",
    "            [ -9.0000,  -2.1562,   8.9375,  ...,  -6.4375,  -6.9688,  -5.9375],\n",
    "            [-12.4375,  -7.8750,   3.5625,  ..., -10.2500, -10.3750, -11.1875],\n",
    "            ...,\n",
    "            [ -6.7812,  -3.1406,   4.2188,  ...,  -4.6562,  -3.5312,  -4.8750],\n",
    "            [ -7.5312,  -4.7188,   4.1562,  ...,  -4.6250,  -4.5625,  -5.5000],\n",
    "            [ -4.3438,  -0.9023,   2.0625,  ...,  -3.5312,  -4.0625,  -2.5469]]],\n",
    "        device='cuda:0', grad_fn=<ToCopyBackward0>), past_key_values=None, hidden_states=None, attentions=None)\n",
    "    \"\"\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'forward' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m LlavaLlamaForCausalLM\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'forward' is not defined"
     ]
    }
   ],
   "source": [
    "LlavaLlamaForCausalLM.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"input_ids\": batch['input_ids'].to(device=model.device),\n",
    "    \"attention_mask\": batch['attention_mask'].to(device=model.device),\n",
    "    \"labels\": batch['labels'].to(device=model.device),\n",
    "    \"images\": batch['images'].to(device=model.device),\n",
    "    \"position_ids\": None,\n",
    "    \"past_key_values\": None,\n",
    "    \"image_sizes\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, image_sizes, return_dict)\n",
      "input_ids\n",
      " tensor([[    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "           411,  2654, 11315,    13]])\n",
      "input_ids.shape\n",
      " torch.Size([1, 24])\n",
      "attention_mask\n",
      " tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True]])\n",
      "position_ids\n",
      " None\n",
      "past_key_values\n",
      " None\n",
      "inputs_embeds\n",
      " None\n",
      "labels\n",
      " tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "           411,  2654, 11315,    13]])\n",
      "use_cache\n",
      " None\n",
      "output_attentions\n",
      " None\n",
      "output_hidden_states\n",
      " None\n",
      "images\n",
      " tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "          ...,\n",
      "          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "          ...,\n",
      "          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "          ...,\n",
      "          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])\n",
      "images.shape\n",
      " torch.Size([1, 3, 336, 336])\n",
      "image_sizes\n",
      " None\n",
      "return_dict\n",
      " None\n",
      "【COND】 inputs_embeds_is_None=True\n",
      "【ENTER】if inputs_embeds is None:\n",
      "current file path llava/model/llava_arch.py\n",
      "def LlavaMetaForCausalLM(ABC).prepare_inputs_labels_for_multimodal(self, input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes=None)\n",
      "input_ids\n",
      " tensor([[    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "           411,  2654, 11315,    13]])\n",
      "position_ids\n",
      " None\n",
      "attention_mask\n",
      " tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True]])\n",
      "past_key_values\n",
      " None\n",
      "labels\n",
      " tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "           411,  2654, 11315,    13]])\n",
      "images\n",
      " tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "          ...,\n",
      "          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "          ...,\n",
      "          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "          ...,\n",
      "          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])\n",
      "image_sizes\n",
      " None\n",
      "current file path llava/model/llava_arch.py\n",
      "class LlavaMetaForCausalLM(ABC).get_vision_tower(self)\n",
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.get_model(self)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "self.model (return)\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      "  (vision_tower): CLIPVisionTower(\n",
      "    (vision_tower): CLIPVisionModel(\n",
      "      (vision_model): CLIPVisionTransformer(\n",
      "        (embeddings): CLIPVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "          (position_embedding): Embedding(577, 1024)\n",
      "        )\n",
      "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder): CLIPEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-23): 24 x CLIPEncoderLayer(\n",
      "              (self_attn): CLIPAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): CLIPMLP(\n",
      "                (activation_fn): QuickGELUActivation()\n",
      "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mm_projector): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n",
      "current file path llava/model/llava_arch.py\n",
      "def get_vision_tower(self)\n",
      "vision_tower (raw)\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "type(vision_tower)\n",
      " <class '__main__.CLIPVisionTower'>\n",
      "【COND】 type_vision_tower_is_list=False\n",
      "vision_tower (return)\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "LlavaMetaForCausalLM(ABC).get_vision_tower(self) result (return)\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "vision_tower\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "【COND】 vision_tower_is_None=False images_is_None=False input_ids_shape_1_eq_1=False\n",
      "【COND】type(images)\n",
      " <class 'torch.Tensor'>\n",
      "【COND】images.ndim\n",
      " 4\n",
      "【ENTER】else of if type(images) is list or images.ndim == 5:\n",
      "current file path llava/model/llava_arch.py\n",
      "def LlavaMetaForCausalLM(ABC).encode_images(self, images)\n",
      "images\n",
      " tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "          ...,\n",
      "          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "          ...,\n",
      "          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "          ...,\n",
      "          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])\n",
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.get_model(self)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "self.model (return)\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      "  (vision_tower): CLIPVisionTower(\n",
      "    (vision_tower): CLIPVisionModel(\n",
      "      (vision_model): CLIPVisionTransformer(\n",
      "        (embeddings): CLIPVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "          (position_embedding): Embedding(577, 1024)\n",
      "        )\n",
      "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder): CLIPEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-23): 24 x CLIPEncoderLayer(\n",
      "              (self_attn): CLIPAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): CLIPMLP(\n",
      "                (activation_fn): QuickGELUActivation()\n",
      "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mm_projector): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n",
      "current file path llava/model/llava_arch.py\n",
      "def get_vision_tower(self)\n",
      "vision_tower (raw)\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "type(vision_tower)\n",
      " <class '__main__.CLIPVisionTower'>\n",
      "【COND】 type_vision_tower_is_list=False\n",
      "vision_tower (return)\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "current file path llava/llava/model/multimodal_encoder/clip_encoder.py\n",
      "def CLIPVisionTower.forward(self, images)\n",
      "images shape\n",
      " torch.Size([1, 3, 336, 336])\n",
      "images\n",
      " tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "          ...,\n",
      "          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "          ...,\n",
      "          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "          ...,\n",
      "          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])\n",
      "images.shape\n",
      " torch.Size([1, 3, 336, 336])\n",
      "【COND】 type_images_is_list=False\n",
      "【ENTER】else (type(images) is not list):\n",
      "original images\n",
      " tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "          ...,\n",
      "          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "          ...,\n",
      "          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "          ...,\n",
      "          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])\n",
      "after process image_forward_outs\n",
      " <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
      "current file path llava/llava/model/multimodal_encoder/clip_encoder.py\n",
      "def CLIPVisionTower.feature_select(self, image_forward_outs)\n",
      "image_forward_outs\n",
      " BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.0879, -0.1445, -0.2949,  ...,  0.3906, -0.7188,  0.0356],\n",
      "         [ 0.4629,  0.0078,  0.0859,  ..., -0.0815,  0.1113, -0.3906],\n",
      "         [-0.2188,  1.0938,  1.0469,  ...,  0.2119, -0.3477, -0.8477],\n",
      "         ...,\n",
      "         [ 1.7500,  1.0859,  1.6719,  ..., -0.2891, -0.6797,  1.1016],\n",
      "         [ 0.9141, -0.1050,  1.3594,  ..., -0.4141, -1.2031,  0.4043],\n",
      "         [ 1.4531,  1.1641,  1.0156,  ..., -0.3867, -0.6094,  0.6719]]],\n",
      "       dtype=torch.bfloat16), pooler_output=tensor([[ 0.1738, -0.1426, -0.6133,  ...,  0.9453, -1.4844,  0.0967]],\n",
      "       dtype=torch.bfloat16), hidden_states=(tensor([[[ 0.0342, -0.0408, -0.1670,  ...,  0.3203, -0.1475, -0.0201],\n",
      "         [-0.1187, -0.0493, -0.2656,  ...,  0.5859, -0.1318, -0.0098],\n",
      "         [ 0.2393, -0.0437, -0.6172,  ...,  0.3438, -0.1445, -0.0098],\n",
      "         ...,\n",
      "         [ 0.1108, -0.0439, -0.1240,  ..., -0.0601, -0.1357, -0.0162],\n",
      "         [ 0.1855, -0.0454, -0.1118,  ..., -0.0903, -0.1357, -0.0112],\n",
      "         [ 0.0308, -0.0486, -0.0287,  ..., -0.0150, -0.1436, -0.0153]]],\n",
      "       dtype=torch.bfloat16), tensor([[[ 0.0530,  0.1074, -0.1055,  ...,  0.1094,  0.0781,  0.0308],\n",
      "         [ 0.0371,  0.0625, -0.1621,  ...,  0.3711, -0.0166, -0.0398],\n",
      "         [ 0.2490,  0.0527, -0.4141,  ...,  0.1621,  0.1074,  0.1143],\n",
      "         ...,\n",
      "         [ 0.0698, -0.0698,  0.0410,  ..., -0.2988, -0.0195, -0.0435],\n",
      "         [ 0.1904, -0.1289,  0.0171,  ..., -0.2988,  0.0146, -0.0525],\n",
      "         [ 0.1396,  0.0537, -0.0117,  ..., -0.0732, -0.0156, -0.0083]]],\n",
      "       dtype=torch.bfloat16), tensor([[[ 0.0408,  0.0098, -0.0869,  ...,  0.0674, -0.0288,  0.0486],\n",
      "         [ 0.1133,  0.0054, -0.0918,  ...,  0.2412,  0.1250, -0.0337],\n",
      "         [ 0.2070, -0.0588, -0.2832,  ...,  0.1523,  0.1006,  0.2051],\n",
      "         ...,\n",
      "         [-0.0547, -0.1973,  0.0098,  ..., -0.4551, -0.1118, -0.1055],\n",
      "         [ 0.0205, -0.3027, -0.0288,  ..., -0.4922, -0.0635, -0.1338],\n",
      "         [ 0.1348,  0.1006, -0.0991,  ..., -0.1621, -0.0354, -0.0364]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.0488,  0.0015, -0.0503,  ...,  0.0510,  0.0056,  0.0938],\n",
      "         [ 0.1680, -0.0400, -0.0239,  ...,  0.2852,  0.3242, -0.0232],\n",
      "         [ 0.2305,  0.0479, -0.0625,  ...,  0.1836,  0.1514,  0.3594],\n",
      "         ...,\n",
      "         [-0.1992, -0.2344, -0.0156,  ..., -0.4023, -0.1602, -0.0654],\n",
      "         [-0.0586, -0.2539,  0.0259,  ..., -0.4316, -0.1309, -0.2139],\n",
      "         [-0.0352,  0.3262, -0.1660,  ..., -0.1484,  0.0164, -0.0408]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.0933, -0.0166,  0.0188,  ...,  0.0811, -0.0063,  0.1025],\n",
      "         [ 0.2500,  0.0225, -0.0117,  ...,  0.3145,  0.3789, -0.0177],\n",
      "         [ 0.0527, -0.1914, -0.0776,  ...,  0.2598,  0.0928,  0.4062],\n",
      "         ...,\n",
      "         [-0.2158, -0.3867, -0.1240,  ..., -0.3145, -0.2236, -0.1523],\n",
      "         [ 0.0273, -0.4551, -0.0640,  ..., -0.4160, -0.2324, -0.3340],\n",
      "         [-0.0315,  0.3945, -0.0322,  ..., -0.0762, -0.0112, -0.0608]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.0942, -0.0542,  0.0381,  ...,  0.0903,  0.1113,  0.1201],\n",
      "         [ 0.0859,  0.0083,  0.1484,  ...,  0.3008,  0.3086, -0.0874],\n",
      "         [-0.0786, -0.2197,  0.0293,  ...,  0.3516,  0.0562,  0.4531],\n",
      "         ...,\n",
      "         [-0.1406, -0.4141,  0.0762,  ..., -0.2148, -0.1348, -0.1699],\n",
      "         [ 0.0317, -0.5781,  0.1592,  ..., -0.3750, -0.2480, -0.2344],\n",
      "         [-0.0894,  0.3516, -0.1387,  ..., -0.1084, -0.0137,  0.0728]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.0308,  0.0203,  0.0535,  ...,  0.0515,  0.1348,  0.0703],\n",
      "         [-0.1006,  0.1211,  0.1147,  ...,  0.4551,  0.3164, -0.0635],\n",
      "         [-0.0159, -0.1777,  0.0645,  ...,  0.4121, -0.0640,  0.4297],\n",
      "         ...,\n",
      "         [-0.2422, -0.3672,  0.2422,  ..., -0.2158, -0.2656, -0.2969],\n",
      "         [-0.0569, -0.6250,  0.1982,  ..., -0.3145, -0.2832, -0.3008],\n",
      "         [ 0.0088,  0.3867, -0.2041,  ..., -0.0801,  0.0625,  0.0391]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.0103,  0.0128,  0.0366,  ...,  0.0708,  0.0625,  0.0525],\n",
      "         [-0.3164,  0.0747,  0.1641,  ...,  0.7266,  0.2988, -0.0088],\n",
      "         [ 0.1602, -0.2061,  0.1816,  ...,  0.6445, -0.1777,  0.0928],\n",
      "         ...,\n",
      "         [-0.3145, -0.1406,  0.4531,  ..., -0.0132, -0.3164, -0.0225],\n",
      "         [-0.1211, -0.5859,  0.3164,  ..., -0.1279, -0.3125, -0.1680],\n",
      "         [ 0.0259,  0.3809, -0.0156,  ...,  0.0625,  0.1445,  0.1680]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.0405, -0.0493,  0.0444,  ...,  0.1738,  0.0366,  0.0183],\n",
      "         [-0.1943, -0.0244,  0.0264,  ...,  0.8086,  0.1758, -0.1436],\n",
      "         [ 0.0830, -0.3789,  0.3184,  ...,  0.8594, -0.1553, -0.1123],\n",
      "         ...,\n",
      "         [-0.3125, -0.2754,  0.4668,  ..., -0.1162, -0.4883,  0.2266],\n",
      "         [-0.3027, -0.6406,  0.2930,  ..., -0.2217, -0.4023, -0.1172],\n",
      "         [-0.0498,  0.3848, -0.3711,  ...,  0.0718, -0.1895, -0.0547]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.1196, -0.0562, -0.0332,  ...,  0.1270, -0.0732, -0.0430],\n",
      "         [-0.0430, -0.1641, -0.0894,  ...,  0.8047,  0.3125, -0.0527],\n",
      "         [ 0.0708, -0.3242,  0.3379,  ...,  0.8828, -0.2617,  0.0908],\n",
      "         ...,\n",
      "         [-0.3320, -0.4219,  0.1191,  ..., -0.2988, -0.6133,  0.4238],\n",
      "         [-0.4551, -0.8320, -0.0947,  ..., -0.4375, -0.8203,  0.0026],\n",
      "         [ 0.0371,  0.3203, -0.3223,  ..., -0.1279, -0.1338, -0.1406]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.0908, -0.0972,  0.0366,  ...,  0.1235, -0.0913,  0.0771],\n",
      "         [ 0.1089,  0.1504, -0.1641,  ...,  0.5195,  0.3262,  0.1123],\n",
      "         [ 0.0762,  0.0742,  0.1279,  ...,  0.8398, -0.1484,  0.0156],\n",
      "         ...,\n",
      "         [-0.3594, -0.3535,  0.2910,  ..., -0.1055, -0.6250,  0.3281],\n",
      "         [-0.3555, -0.5977,  0.2578,  ..., -0.3359, -0.5781,  0.0245],\n",
      "         [ 0.1494,  0.3438, -0.0767,  ..., -0.0713, -0.0029, -0.2080]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.0205, -0.0469,  0.0039,  ...,  0.0718, -0.0884,  0.1406],\n",
      "         [ 0.0825,  0.2061, -0.0703,  ...,  0.5234,  0.5195,  0.0884],\n",
      "         [ 0.0100,  0.1836,  0.0386,  ...,  0.9297, -0.2080, -0.0530],\n",
      "         ...,\n",
      "         [-0.2354, -0.3633,  0.2852,  ...,  0.0576, -0.5312,  0.0786],\n",
      "         [-0.2227, -0.4297,  0.0977,  ..., -0.2578, -0.4961, -0.3027],\n",
      "         [-0.0957,  0.2129,  0.2832,  ..., -0.0073, -0.1260, -0.3750]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.0339, -0.1240, -0.1602,  ...,  0.0713, -0.1895,  0.1230],\n",
      "         [-0.0312,  0.2812, -0.2451,  ...,  0.5273,  0.3281,  0.1660],\n",
      "         [-0.0645,  0.1914,  0.0801,  ...,  0.9336, -0.1758,  0.0018],\n",
      "         ...,\n",
      "         [-0.2734, -0.1035,  0.3262,  ..., -0.0674, -0.5469,  0.0713],\n",
      "         [-0.0542, -0.5039,  0.1240,  ..., -0.2168, -0.5195, -0.5586],\n",
      "         [-0.0098,  0.2012,  0.3828,  ..., -0.1279, -0.0874, -0.1807]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.0840, -0.0293, -0.0859,  ...,  0.1201, -0.2158,  0.1787],\n",
      "         [-0.0459,  0.2490, -0.0205,  ...,  0.4004,  0.3223,  0.1426],\n",
      "         [-0.1108,  0.1357, -0.0801,  ...,  0.7109, -0.2734, -0.2148],\n",
      "         ...,\n",
      "         [-0.2168,  0.0405,  0.2490,  ..., -0.0068, -0.1289,  0.0217],\n",
      "         [ 0.0571, -0.3633,  0.0942,  ..., -0.2949, -0.1094, -0.4590],\n",
      "         [ 0.2148,  0.1943,  0.3262,  ..., -0.1768,  0.0806, -0.2988]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.1719, -0.0371, -0.1592,  ...,  0.1055, -0.2656,  0.0127],\n",
      "         [ 0.0337,  0.2451,  0.1396,  ...,  0.1426,  0.2158,  0.1562],\n",
      "         [-0.1445,  0.1455, -0.2773,  ...,  0.3652, -0.1572, -0.1777],\n",
      "         ...,\n",
      "         [ 0.1152,  0.0645,  0.3359,  ..., -0.3438,  0.0118,  0.0430],\n",
      "         [ 0.0591, -0.3750,  0.1045,  ..., -0.5664, -0.0610, -0.2119],\n",
      "         [ 0.2578,  0.0405,  0.5352,  ..., -0.4746,  0.0566, -0.2871]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.1240, -0.1602, -0.0991,  ...,  0.1123, -0.2578,  0.1191],\n",
      "         [-0.0027,  0.2715,  0.1855,  ..., -0.1982,  0.3320,  0.3789],\n",
      "         [-0.3613,  0.2500, -0.1035,  ...,  0.1357, -0.0771,  0.0137],\n",
      "         ...,\n",
      "         [ 0.2578,  0.1133,  0.3965,  ...,  0.0923, -0.1484,  0.3008],\n",
      "         [-0.0366, -0.4805,  0.2217,  ..., -0.0608, -0.3340, -0.1592],\n",
      "         [ 0.3828,  0.0098,  0.5312,  ..., -0.4355, -0.0300, -0.0439]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.1157, -0.1279, -0.1006,  ..., -0.0732, -0.2637,  0.1235],\n",
      "         [-0.0928,  0.4102,  0.2002,  ...,  0.0062,  0.2402,  0.4551],\n",
      "         [-0.3125,  0.4824, -0.2100,  ...,  0.2832, -0.0742,  0.0564],\n",
      "         ...,\n",
      "         [ 0.3867,  0.3066,  0.2236,  ...,  0.2695, -0.1406,  0.2871],\n",
      "         [-0.0635, -0.2080,  0.1748,  ...,  0.0251, -0.3496, -0.3457],\n",
      "         [ 0.5234,  0.2676,  0.4531,  ..., -0.5312, -0.0742, -0.0635]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.1152,  0.1089,  0.0088,  ..., -0.1250, -0.3516,  0.1035],\n",
      "         [-0.1348, -0.1104,  0.1709,  ..., -0.1475,  0.1621,  0.2812],\n",
      "         [-0.1963,  0.2695, -0.3086,  ...,  0.1504,  0.0527, -0.1533],\n",
      "         ...,\n",
      "         [ 0.4785,  0.4980,  0.2236,  ...,  0.1719, -0.0986,  0.2500],\n",
      "         [ 0.0625, -0.0176,  0.1934,  ..., -0.2207, -0.3770, -0.3965],\n",
      "         [ 0.6055,  0.2334,  0.4629,  ..., -0.4199, -0.2832, -0.1426]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.1533,  0.1006,  0.0013,  ..., -0.0967, -0.2002,  0.0542],\n",
      "         [-0.2363, -0.1387, -0.3262,  ..., -0.2988,  0.1406,  0.2891],\n",
      "         [-0.2695,  0.3008, -0.2773,  ...,  0.0967,  0.2891, -0.1094],\n",
      "         ...,\n",
      "         [ 0.5508,  0.6797,  0.4336,  ...,  0.1338, -0.2422,  0.2656],\n",
      "         [-0.1777, -0.0898, -0.0527,  ..., -0.1875, -0.3691, -0.5117],\n",
      "         [ 0.5820,  0.2217,  0.2910,  ..., -0.4141, -0.2402, -0.0972]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-2.6367e-01,  1.0547e-01, -6.9336e-02,  ..., -5.2734e-02,\n",
      "          -3.1250e-01,  1.6602e-02],\n",
      "         [-3.7695e-01, -5.4688e-01, -3.8281e-01,  ...,  8.7891e-03,\n",
      "           4.9805e-01,  8.5938e-02],\n",
      "         [-4.5898e-01,  2.7344e-01, -8.0078e-02,  ...,  9.2773e-02,\n",
      "           3.1641e-01, -6.5918e-02],\n",
      "         ...,\n",
      "         [ 5.3906e-01,  7.6172e-01,  4.7266e-01,  ...,  2.0703e-01,\n",
      "          -2.8320e-01,  1.7383e-01],\n",
      "         [-2.3828e-01, -4.2383e-01, -4.8828e-04,  ..., -1.7383e-01,\n",
      "          -4.9609e-01, -3.5156e-01],\n",
      "         [ 5.3516e-01,  1.7969e-01,  3.1641e-01,  ..., -2.1582e-01,\n",
      "          -4.9219e-01, -2.4414e-03]]], dtype=torch.bfloat16), tensor([[[ 0.0703,  0.1992, -0.1992,  ..., -0.0293, -0.3535,  0.2363],\n",
      "         [-0.2383, -0.2031, -0.0596,  ..., -0.1074,  0.4258,  0.0693],\n",
      "         [-0.2188,  0.5391,  0.2070,  ...,  0.1021,  0.0771, -0.0388],\n",
      "         ...,\n",
      "         [ 0.8633,  0.7344,  0.5742,  ...,  0.1094, -0.3906,  0.2051],\n",
      "         [ 0.2002, -0.5156,  0.1436,  ..., -0.2891, -0.6172, -0.3027],\n",
      "         [ 0.9883,  0.4512,  0.5039,  ..., -0.3164, -0.5156,  0.2002]]],\n",
      "       dtype=torch.bfloat16), tensor([[[ 0.0449,  0.2178, -0.2490,  ...,  0.1367, -0.3242,  0.4492],\n",
      "         [-0.3594, -0.3027,  0.4336,  ...,  0.0020,  0.4023,  0.0400],\n",
      "         [ 0.2754,  0.8047, -0.0332,  ...,  0.0479, -0.3086, -0.3945],\n",
      "         ...,\n",
      "         [ 1.6094,  1.1562,  0.6641,  ..., -0.5430, -0.8320,  0.4023],\n",
      "         [ 1.1328, -0.2031,  0.7227,  ..., -0.9531, -0.7656, -0.3398],\n",
      "         [ 1.8672,  0.9336,  0.6797,  ..., -0.9922, -0.4082,  0.0059]]],\n",
      "       dtype=torch.bfloat16), tensor([[[ 0.0347,  0.3184, -0.2930,  ...,  0.3086, -0.3535,  0.1572],\n",
      "         [ 0.2930, -0.0312,  0.3555,  ...,  0.3984,  0.5000, -0.2578],\n",
      "         [ 0.0977,  0.7734,  0.4414,  ...,  0.4375, -0.1816, -0.5898],\n",
      "         ...,\n",
      "         [ 1.4297,  1.2031,  1.2656,  ..., -0.1641, -0.8711,  0.9180],\n",
      "         [ 0.9609, -0.0039,  1.2344,  ..., -0.3906, -0.9922, -0.0430],\n",
      "         [ 1.5625,  1.1016,  1.1250,  ..., -0.4961, -0.5000,  0.4141]]],\n",
      "       dtype=torch.bfloat16), tensor([[[-0.0054,  0.2051, -0.7422,  ...,  0.4414, -0.5078, -0.1128],\n",
      "         [ 0.1074,  0.1348, -0.4238,  ...,  0.1475,  0.3477, -0.4414],\n",
      "         [-0.3164,  0.9922,  0.3301,  ...,  0.4297, -0.2090, -0.7070],\n",
      "         ...,\n",
      "         [ 1.6094,  0.9805,  0.8906,  ..., -0.2812, -1.0391,  1.2031],\n",
      "         [ 0.8398, -0.3633,  0.6953,  ..., -0.2754, -1.2266,  0.4453],\n",
      "         [ 1.5391,  0.7812,  0.3848,  ..., -0.2246, -0.7266,  0.8047]]],\n",
      "       dtype=torch.bfloat16), tensor([[[ 0.0879, -0.1445, -0.2949,  ...,  0.3906, -0.7188,  0.0356],\n",
      "         [ 0.4629,  0.0078,  0.0859,  ..., -0.0815,  0.1113, -0.3906],\n",
      "         [-0.2188,  1.0938,  1.0469,  ...,  0.2119, -0.3477, -0.8477],\n",
      "         ...,\n",
      "         [ 1.7500,  1.0859,  1.6719,  ..., -0.2891, -0.6797,  1.1016],\n",
      "         [ 0.9141, -0.1050,  1.3594,  ..., -0.4141, -1.2031,  0.4043],\n",
      "         [ 1.4531,  1.1641,  1.0156,  ..., -0.3867, -0.6094,  0.6719]]],\n",
      "       dtype=torch.bfloat16)), attentions=None)\n",
      "image_features (after select_layer)\n",
      " <class 'torch.Tensor'>\n",
      "image_features.shape\n",
      " torch.Size([1, 577, 1024])\n",
      "【COND】 select_feature=patch\n",
      "【ENTER】if self.select_feature == 'patch':\n",
      "original image_features\n",
      " tensor([[[-0.0054,  0.2051, -0.7422,  ...,  0.4414, -0.5078, -0.1128],\n",
      "         [ 0.1074,  0.1348, -0.4238,  ...,  0.1475,  0.3477, -0.4414],\n",
      "         [-0.3164,  0.9922,  0.3301,  ...,  0.4297, -0.2090, -0.7070],\n",
      "         ...,\n",
      "         [ 1.6094,  0.9805,  0.8906,  ..., -0.2812, -1.0391,  1.2031],\n",
      "         [ 0.8398, -0.3633,  0.6953,  ..., -0.2754, -1.2266,  0.4453],\n",
      "         [ 1.5391,  0.7812,  0.3848,  ..., -0.2246, -0.7266,  0.8047]]],\n",
      "       dtype=torch.bfloat16)\n",
      "after process\n",
      " tensor([[[ 0.1074,  0.1348, -0.4238,  ...,  0.1475,  0.3477, -0.4414],\n",
      "         [-0.3164,  0.9922,  0.3301,  ...,  0.4297, -0.2090, -0.7070],\n",
      "         [ 1.2656, -0.0146, -0.6602,  ...,  0.2432,  0.3555,  0.3438],\n",
      "         ...,\n",
      "         [ 1.6094,  0.9805,  0.8906,  ..., -0.2812, -1.0391,  1.2031],\n",
      "         [ 0.8398, -0.3633,  0.6953,  ..., -0.2754, -1.2266,  0.4453],\n",
      "         [ 1.5391,  0.7812,  0.3848,  ..., -0.2246, -0.7266,  0.8047]]],\n",
      "       dtype=torch.bfloat16)\n",
      "【EXIT】if self.select_feature == 'patch':\n",
      "selected image_feature shape\n",
      " torch.Size([1, 576, 1024])\n",
      "image_features (return)\n",
      " tensor([[[ 0.1074,  0.1348, -0.4238,  ...,  0.1475,  0.3477, -0.4414],\n",
      "         [-0.3164,  0.9922,  0.3301,  ...,  0.4297, -0.2090, -0.7070],\n",
      "         [ 1.2656, -0.0146, -0.6602,  ...,  0.2432,  0.3555,  0.3438],\n",
      "         ...,\n",
      "         [ 1.6094,  0.9805,  0.8906,  ..., -0.2812, -1.0391,  1.2031],\n",
      "         [ 0.8398, -0.3633,  0.6953,  ..., -0.2754, -1.2266,  0.4453],\n",
      "         [ 1.5391,  0.7812,  0.3848,  ..., -0.2246, -0.7266,  0.8047]]],\n",
      "       dtype=torch.bfloat16)\n",
      "image_features.shape\n",
      " torch.Size([1, 576, 1024])\n",
      "after process image_features\n",
      " <class 'torch.Tensor'>\n",
      "【EXIT】else (type(images) is not list):\n",
      "image_features (return)\n",
      " tensor([[[ 0.1074,  0.1348, -0.4238,  ...,  0.1475,  0.3477, -0.4414],\n",
      "         [-0.3164,  0.9922,  0.3301,  ...,  0.4297, -0.2090, -0.7070],\n",
      "         [ 1.2656, -0.0146, -0.6602,  ...,  0.2432,  0.3555,  0.3438],\n",
      "         ...,\n",
      "         [ 1.6094,  0.9805,  0.8906,  ..., -0.2812, -1.0391,  1.2031],\n",
      "         [ 0.8398, -0.3633,  0.6953,  ..., -0.2754, -1.2266,  0.4453],\n",
      "         [ 1.5391,  0.7812,  0.3848,  ..., -0.2246, -0.7266,  0.8047]]])\n",
      "image_features.shape\n",
      " torch.Size([1, 576, 1024])\n",
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.get_model(self)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "self.model (return)\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      "  (vision_tower): CLIPVisionTower(\n",
      "    (vision_tower): CLIPVisionModel(\n",
      "      (vision_model): CLIPVisionTransformer(\n",
      "        (embeddings): CLIPVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "          (position_embedding): Embedding(577, 1024)\n",
      "        )\n",
      "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder): CLIPEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-23): 24 x CLIPEncoderLayer(\n",
      "              (self_attn): CLIPAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): CLIPMLP(\n",
      "                (activation_fn): QuickGELUActivation()\n",
      "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mm_projector): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n",
      "image_features (return) shape\n",
      " torch.Size([1, 576, 2048])\n",
      "image_features (return)\n",
      " tensor([[[-0.1943,  0.1157, -0.0747,  ...,  0.0027, -0.1691, -0.3439],\n",
      "         [ 0.0437,  0.1717, -0.0998,  ...,  0.0930, -0.1386, -0.0731],\n",
      "         [-0.0505,  0.1592, -0.0982,  ...,  0.0866, -0.1123, -0.2177],\n",
      "         ...,\n",
      "         [-0.0182,  0.0850, -0.0556,  ...,  0.0622, -0.1969,  0.0129],\n",
      "         [-0.0651,  0.0586, -0.1218,  ..., -0.0614, -0.1158, -0.0104],\n",
      "         [ 0.0863,  0.0081, -0.1651,  ..., -0.2040, -0.0455,  0.0618]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "image_features after encode_images shape \n",
      " torch.Size([1, 576, 2048])\n",
      "image_features after encode_images\n",
      " tensor([[[-0.1943,  0.1157, -0.0747,  ...,  0.0027, -0.1691, -0.3439],\n",
      "         [ 0.0437,  0.1717, -0.0998,  ...,  0.0930, -0.1386, -0.0731],\n",
      "         [-0.0505,  0.1592, -0.0982,  ...,  0.0866, -0.1123, -0.2177],\n",
      "         ...,\n",
      "         [-0.0182,  0.0850, -0.0556,  ...,  0.0622, -0.1969,  0.0129],\n",
      "         [-0.0651,  0.0586, -0.1218,  ..., -0.0614, -0.1158, -0.0104],\n",
      "         [ 0.0863,  0.0081, -0.1651,  ..., -0.2040, -0.0455,  0.0618]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "【EXIT】else of if type(images) is list or images.ndim == 5:\n",
      "labels before\n",
      " tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "           411,  2654, 11315,    13]])\n",
      "position_ids before\n",
      " None\n",
      "attention_mask before\n",
      " tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True]])\n",
      "【ENTER】else of if attention_mask is None:\n",
      "attention_mask（after）shape \n",
      " torch.Size([1, 24])\n",
      "attention_mask (after)\n",
      " tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True]])\n",
      "【EXIT】else of if attention_mask is None:\n",
      "【ENTER】if position_ids is None:\n",
      "position_ids (after) shape \n",
      " torch.Size([24])\n",
      "position_ids (after)\n",
      " tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23])\n",
      "【EXIT】if position_ids is None:\n",
      "【COND】 labels_is_None=False\n",
      "input_ids after removing padding\n",
      " [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "          411,  2654, 11315,    13])]\n",
      "labels after removing padding\n",
      " [tensor([ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "          411,  2654, 11315,    13])]\n",
      "cur_input_ids shape\n",
      " torch.Size([24])\n",
      "cur_input_ids\n",
      " tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "          411,  2654, 11315,    13])\n",
      "【COND】num_images: tensor(1)\n",
      "image_token_indices\n",
      " [-1, 1, 24]\n",
      "len image_token_indices 3\n",
      "cur_labels\n",
      " tensor([ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
      "        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
      "          411,  2654, 11315,    13])\n",
      "cur_input_ids_noim (after)\n",
      " [tensor([1]), tensor([  278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596,\n",
      "        23425,   278,  3700,   322,  6567,   310,   263,  6114,   411,  2654,\n",
      "        11315,    13])]\n",
      "cur_labels_noim (after) \n",
      " [tensor([-100]), tensor([  278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596,\n",
      "        23425,   278,  3700,   322,  6567,   310,   263,  6114,   411,  2654,\n",
      "        11315,    13])]\n",
      "split_sizes\n",
      " [1, 22]\n",
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.get_model(self)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "self.model (return)\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      "  (vision_tower): CLIPVisionTower(\n",
      "    (vision_tower): CLIPVisionModel(\n",
      "      (vision_model): CLIPVisionTransformer(\n",
      "        (embeddings): CLIPVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "          (position_embedding): Embedding(577, 1024)\n",
      "        )\n",
      "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder): CLIPEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-23): 24 x CLIPEncoderLayer(\n",
      "              (self_attn): CLIPAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): CLIPMLP(\n",
      "                (activation_fn): QuickGELUActivation()\n",
      "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mm_projector): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n",
      "cur_input_embeds shape\n",
      " torch.Size([23, 2048])\n",
      "cur_input_embeds\n",
      " tensor([[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
      "         -6.5231e-04, -4.9973e-04],\n",
      "        [ 7.0801e-03,  1.0452e-03,  6.0425e-03,  ...,  3.9673e-03,\n",
      "          1.2817e-03, -1.1215e-03],\n",
      "        [-2.2949e-02, -2.6226e-05,  6.8359e-03,  ..., -2.4658e-02,\n",
      "         -9.4604e-03,  1.5869e-02],\n",
      "        ...,\n",
      "        [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
      "         -8.3618e-03, -9.4604e-03],\n",
      "        [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
      "         -2.5177e-03, -8.0566e-03],\n",
      "        [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
      "         -7.3242e-04,  2.7924e-03]], requires_grad=True)\n",
      "cur_input_embeds_no_im\n",
      " (tensor([[-0.0011,  0.0019, -0.0017,  ...,  0.0002, -0.0007, -0.0005]],\n",
      "       grad_fn=<SplitWithSizesBackward0>), tensor([[ 7.0801e-03,  1.0452e-03,  6.0425e-03,  ...,  3.9673e-03,\n",
      "          1.2817e-03, -1.1215e-03],\n",
      "        [-2.2949e-02, -2.6226e-05,  6.8359e-03,  ..., -2.4658e-02,\n",
      "         -9.4604e-03,  1.5869e-02],\n",
      "        [-2.3499e-03,  1.4893e-02, -2.0447e-03,  ..., -8.6060e-03,\n",
      "          2.3193e-03,  3.0670e-03],\n",
      "        ...,\n",
      "        [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
      "         -8.3618e-03, -9.4604e-03],\n",
      "        [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
      "         -2.5177e-03, -8.0566e-03],\n",
      "        [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
      "         -7.3242e-04,  2.7924e-03]], grad_fn=<SplitWithSizesBackward0>))\n",
      "【COND】 i=0 num_images=1\n",
      "【ENTER】if i < num_images:\n",
      "【EXIT】if i < num_images:\n",
      "【COND】 i=1 num_images=1\n",
      "cur_new_input_embeds (before cat) shape\n",
      " [torch.Size([1, 2048]), torch.Size([576, 2048]), torch.Size([22, 2048])]\n",
      "cur_new_input_embeds (before cat)\n",
      " [tensor([[-0.0011,  0.0019, -0.0017,  ...,  0.0002, -0.0007, -0.0005]],\n",
      "       grad_fn=<SplitWithSizesBackward0>), tensor([[-0.1943,  0.1157, -0.0747,  ...,  0.0027, -0.1691, -0.3439],\n",
      "        [ 0.0437,  0.1717, -0.0998,  ...,  0.0930, -0.1386, -0.0731],\n",
      "        [-0.0505,  0.1592, -0.0982,  ...,  0.0866, -0.1123, -0.2177],\n",
      "        ...,\n",
      "        [-0.0182,  0.0850, -0.0556,  ...,  0.0622, -0.1969,  0.0129],\n",
      "        [-0.0651,  0.0586, -0.1218,  ..., -0.0614, -0.1158, -0.0104],\n",
      "        [ 0.0863,  0.0081, -0.1651,  ..., -0.2040, -0.0455,  0.0618]],\n",
      "       grad_fn=<SelectBackward0>), tensor([[ 7.0801e-03,  1.0452e-03,  6.0425e-03,  ...,  3.9673e-03,\n",
      "          1.2817e-03, -1.1215e-03],\n",
      "        [-2.2949e-02, -2.6226e-05,  6.8359e-03,  ..., -2.4658e-02,\n",
      "         -9.4604e-03,  1.5869e-02],\n",
      "        [-2.3499e-03,  1.4893e-02, -2.0447e-03,  ..., -8.6060e-03,\n",
      "          2.3193e-03,  3.0670e-03],\n",
      "        ...,\n",
      "        [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
      "         -8.3618e-03, -9.4604e-03],\n",
      "        [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
      "         -2.5177e-03, -8.0566e-03],\n",
      "        [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
      "         -7.3242e-04,  2.7924e-03]], grad_fn=<SplitWithSizesBackward0>)]\n",
      "cur_new_labels (before cat) shape\n",
      " [torch.Size([1]), torch.Size([576]), torch.Size([22])]\n",
      "cur_new_labels (before cat)\n",
      " [tensor([-100]), tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]), tensor([  278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596,\n",
      "        23425,   278,  3700,   322,  6567,   310,   263,  6114,   411,  2654,\n",
      "        11315,    13])]\n",
      "cur_new_input_embeds (after cat) shape\n",
      " torch.Size([599, 2048])\n",
      "cur_new_input_embeds (after cat)\n",
      " tensor([[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
      "         -6.5231e-04, -4.9973e-04],\n",
      "        [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
      "         -1.6907e-01, -3.4387e-01],\n",
      "        [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
      "         -1.3859e-01, -7.3106e-02],\n",
      "        ...,\n",
      "        [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
      "         -8.3618e-03, -9.4604e-03],\n",
      "        [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
      "         -2.5177e-03, -8.0566e-03],\n",
      "        [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
      "         -7.3242e-04,  2.7924e-03]], grad_fn=<CatBackward0>)\n",
      "cur_new_labels (after cat) shape\n",
      " torch.Size([599])\n",
      "cur_new_labels (after cat)\n",
      " tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
      "          297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
      "          322,  6567,   310,   263,  6114,   411,  2654, 11315,    13])\n",
      "new_input_embeds (so far) shape\n",
      " [torch.Size([599, 2048])]\n",
      "new_input_embeds (so far)\n",
      " [tensor([[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
      "         -6.5231e-04, -4.9973e-04],\n",
      "        [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
      "         -1.6907e-01, -3.4387e-01],\n",
      "        [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
      "         -1.3859e-01, -7.3106e-02],\n",
      "        ...,\n",
      "        [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
      "         -8.3618e-03, -9.4604e-03],\n",
      "        [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
      "         -2.5177e-03, -8.0566e-03],\n",
      "        [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
      "         -7.3242e-04,  2.7924e-03]], grad_fn=<CatBackward0>)]\n",
      "new_labels (so far) shape\n",
      " [torch.Size([599])]\n",
      "new_labels (so far)\n",
      " [tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
      "          297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
      "          322,  6567,   310,   263,  6114,   411,  2654, 11315,    13])]\n",
      "【COND】 tokenizer_model_max_length_is_not_None=True\n",
      "【ENTER】if tokenizer_model_max_length is not None:\n",
      "【EXIT】if tokenizer_model_max_length is not None:\n",
      "max_len\n",
      " 599\n",
      "batch_size\n",
      " 1\n",
      "new_labels_padded (before) shape\n",
      " torch.Size([1, 599])\n",
      "new_labels_padded (before)\n",
      " tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]])\n",
      "attention_mask (before) shape\n",
      " torch.Size([1, 599])\n",
      "attention_mask (before)\n",
      " tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False]])\n",
      "position_ids (before) shape\n",
      " torch.Size([1, 599])\n",
      "position_ids (before)\n",
      " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "【COND】 padding_side=right cur_len=599 max_len=599\n",
      "【ENTER】else (padding_side != 'left'):\n",
      "new_input_embeds_padded (so far) shape\n",
      " [torch.Size([599, 2048])]\n",
      "new_input_embeds_padded (so far)\n",
      " [tensor([[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
      "         -6.5231e-04, -4.9973e-04],\n",
      "        [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
      "         -1.6907e-01, -3.4387e-01],\n",
      "        [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
      "         -1.3859e-01, -7.3106e-02],\n",
      "        ...,\n",
      "        [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
      "         -8.3618e-03, -9.4604e-03],\n",
      "        [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
      "         -2.5177e-03, -8.0566e-03],\n",
      "        [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
      "         -7.3242e-04,  2.7924e-03]], grad_fn=<CatBackward0>)]\n",
      "new_labels_padded (so far) shape\n",
      " torch.Size([1, 599])\n",
      "new_labels_padded (so far)\n",
      " tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
      "           297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
      "           322,  6567,   310,   263,  6114,   411,  2654, 11315,    13]])\n",
      "attention_mask (so far) shape\n",
      " torch.Size([1, 599])\n",
      "attention_mask (so far)\n",
      " tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True]])\n",
      "position_ids (so far) shape\n",
      " torch.Size([1, 599])\n",
      "position_ids (so far)\n",
      " tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "         126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "         140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "         154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "         168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "         182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "         196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "         210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "         224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "         238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "         252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "         266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "         280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "         294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "         308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "         322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "         336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "         350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "         364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "         378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "         392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "         406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "         420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "         434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "         448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
      "         462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
      "         476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
      "         490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
      "         504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517,\n",
      "         518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531,\n",
      "         532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
      "         546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559,\n",
      "         560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573,\n",
      "         574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587,\n",
      "         588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598]])\n",
      "【EXIT】else (padding_side != 'left'):\n",
      "new_input_embeds (after) shape\n",
      " torch.Size([1, 599, 2048])\n",
      "new_input_embeds (after)\n",
      " tensor([[[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
      "          -6.5231e-04, -4.9973e-04],\n",
      "         [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
      "          -1.6907e-01, -3.4387e-01],\n",
      "         [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
      "          -1.3859e-01, -7.3106e-02],\n",
      "         ...,\n",
      "         [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
      "          -8.3618e-03, -9.4604e-03],\n",
      "         [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
      "          -2.5177e-03, -8.0566e-03],\n",
      "         [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
      "          -7.3242e-04,  2.7924e-03]]], grad_fn=<StackBackward0>)\n",
      "【COND】 _labels_is_None=False\n",
      "【ENTER】else of if _labels is None:\n",
      "new_labels (after)\n",
      " tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
      "           297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
      "           322,  6567,   310,   263,  6114,   411,  2654, 11315,    13]])\n",
      "【EXIT】else of if _labels is None:\n",
      "【COND】 _attention_mask_is_None=False\n",
      "【ENTER】else of if _attention_mask is None:\n",
      "attention_mask (after)2\n",
      " tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True]])\n",
      "【COND】 _position_ids_is_None=True\n",
      "【ENTER】if _position_ids is None:\n",
      "【EXIT】if _position_ids is None:\n",
      "position_ids (return)\n",
      " None\n",
      "attention_mask (return)\n",
      " tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True]])\n",
      "past_key_values (return)\n",
      " None\n",
      "new_input_embeds (return)\n",
      " tensor([[[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
      "          -6.5231e-04, -4.9973e-04],\n",
      "         [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
      "          -1.6907e-01, -3.4387e-01],\n",
      "         [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
      "          -1.3859e-01, -7.3106e-02],\n",
      "         ...,\n",
      "         [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
      "          -8.3618e-03, -9.4604e-03],\n",
      "         [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
      "          -2.5177e-03, -8.0566e-03],\n",
      "         [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
      "          -7.3242e-04,  2.7924e-03]]], grad_fn=<StackBackward0>)\n",
      "new_labels (return)\n",
      " tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
      "           297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
      "           322,  6567,   310,   263,  6114,   411,  2654, 11315,    13]])\n",
      "【EXIT】if inputs_embeds is None:\n",
      "input_ids (after prepare_inputs_labels_for_multimodal)\n",
      " None\n",
      "position_ids (after prepare_inputs_labels_for_multimodal)\n",
      " None\n",
      "attention_mask shape (after prepare_inputs_labels_for_multimodal)\n",
      " torch.Size([1, 599])\n",
      "attention_mask (after prepare_inputs_labels_for_multimodal)\n",
      " tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True]])\n",
      "past_key_values (after prepare_inputs_labels_for_multimodal)\n",
      " None\n",
      "inputs_embeds shape (after prepare_inputs_labels_for_multimodal)\n",
      " torch.Size([1, 599, 2048])\n",
      "inputs_embeds (after prepare_inputs_labels_for_multimodal)\n",
      " tensor([[[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
      "          -6.5231e-04, -4.9973e-04],\n",
      "         [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
      "          -1.6907e-01, -3.4387e-01],\n",
      "         [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
      "          -1.3859e-01, -7.3106e-02],\n",
      "         ...,\n",
      "         [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
      "          -8.3618e-03, -9.4604e-03],\n",
      "         [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
      "          -2.5177e-03, -8.0566e-03],\n",
      "         [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
      "          -7.3242e-04,  2.7924e-03]]], grad_fn=<StackBackward0>)\n",
      "labels shape (after prepare_inputs_labels_for_multimodal)\n",
      " torch.Size([1, 599])\n",
      "labels (after prepare_inputs_labels_for_multimodal)\n",
      " tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
      "           297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
      "           322,  6567,   310,   263,  6114,   411,  2654, 11315,    13]])\n",
      "Return of def LlavaLlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, image_sizes, return_dict)\n",
      "result of LlavaLlamaForCausalLM.forward (return)\n",
      " CausalLMOutputWithPast(loss=tensor(8.4862, grad_fn=<NllLossBackward0>), logits=tensor([[[-4.6822,  0.9866,  4.5126,  ..., -5.2010, -2.1646, -4.2286],\n",
      "         [-5.7232, -5.6142,  6.5636,  ..., -4.1083, -8.1069, -4.7190],\n",
      "         [-2.0816, -1.5998,  3.0980,  ..., -0.4457, -6.7922, -0.6046],\n",
      "         ...,\n",
      "         [-3.5149, -3.5361,  3.4583,  ..., -3.8208, -4.0502, -6.5584],\n",
      "         [-4.6837, -4.5297,  2.6598,  ..., -1.8726, -6.8239, -5.4091],\n",
      "         [-4.9032, -4.4957,  3.3181,  ..., -1.4921, -4.5011, -5.8604]]],\n",
      "       grad_fn=<UnsafeViewBackward0>), past_key_values=((tensor([[[[ 1.9216e-02, -3.6968e-02,  1.1219e-02,  ..., -1.2727e-02,\n",
      "            3.8287e-04, -9.5638e-03],\n",
      "          [ 1.2562e-01,  2.4079e-01, -3.6072e-02,  ..., -1.0651e-01,\n",
      "           -1.1563e-01, -1.0832e-01],\n",
      "          [ 2.6599e-01,  3.5478e-02,  1.4940e-01,  ..., -1.0300e-01,\n",
      "           -9.4172e-02, -9.8763e-02],\n",
      "          ...,\n",
      "          [-4.6643e-01,  2.8532e-01,  2.2707e-01,  ..., -3.6099e-02,\n",
      "            2.1460e-02, -2.2348e-02],\n",
      "          [ 9.7321e-04, -1.7182e-02, -2.7302e-03,  ...,  1.1178e-01,\n",
      "            1.1538e-01,  1.1549e-01],\n",
      "          [ 5.9351e+00, -2.7762e+00, -1.7167e-01,  ..., -3.4404e+00,\n",
      "           -2.2593e+00, -3.2202e+00]],\n",
      "\n",
      "         [[-2.0199e-01, -1.9005e-01, -2.8824e-02,  ..., -2.6949e-02,\n",
      "            1.0124e-01, -9.5581e-02],\n",
      "          [ 5.5658e-01,  9.4093e-01,  6.5539e-01,  ...,  8.2628e-01,\n",
      "           -5.6919e-01,  1.1197e+00],\n",
      "          [-6.1816e-02,  1.0681e-02,  1.4987e-01,  ...,  1.1017e-01,\n",
      "           -1.0854e-01,  1.5724e-01],\n",
      "          ...,\n",
      "          [-4.5582e-01, -6.1458e-01, -1.0091e-01,  ..., -1.1683e+00,\n",
      "            1.4774e-01, -1.1727e+00],\n",
      "          [-6.9729e-01, -2.6213e-01,  1.3151e-01,  ..., -9.2325e-01,\n",
      "            1.4513e-01, -1.0322e+00],\n",
      "          [ 1.6250e+00, -8.7929e-01, -5.9226e-01,  ...,  2.4438e+00,\n",
      "           -3.6248e+00,  3.8577e+00]],\n",
      "\n",
      "         [[ 4.0899e-01, -1.6014e-01,  6.9950e-02,  ...,  3.0311e-01,\n",
      "           -1.0522e-01,  7.7642e-02],\n",
      "          [-1.1824e-01, -4.9489e-01, -1.0558e+00,  ..., -7.8293e-01,\n",
      "            2.6601e-01,  2.8419e-01],\n",
      "          [ 3.2234e-01,  3.3078e-01,  3.7688e-02,  ...,  2.9983e-01,\n",
      "            2.8452e-01,  1.5405e-01],\n",
      "          ...,\n",
      "          [-1.5696e+00,  3.0222e-01,  9.5557e-01,  ..., -5.9129e-01,\n",
      "            3.0503e-01,  1.1685e-02],\n",
      "          [ 4.7735e-01, -1.5690e-01,  9.9102e-01,  ...,  2.0701e-01,\n",
      "           -1.6774e-01, -4.0988e-01],\n",
      "          [-2.6533e+00, -4.8490e+00,  1.8934e+00,  ..., -3.3075e+00,\n",
      "           -2.0509e+00, -1.2994e+00]],\n",
      "\n",
      "         [[-1.0084e-01,  1.1619e-02, -2.5743e-02,  ...,  4.2536e-03,\n",
      "           -3.5207e-02, -3.2801e-02],\n",
      "          [ 1.2713e+00,  4.2432e-01,  5.0689e-01,  ..., -5.1707e-01,\n",
      "            8.2997e-01,  8.0495e-01],\n",
      "          [ 7.8038e-02, -7.0127e-02, -8.3425e-02,  ..., -1.5208e-01,\n",
      "            2.2083e-01,  2.1296e-01],\n",
      "          ...,\n",
      "          [ 1.1702e+00, -5.5030e-01,  1.1395e+00,  ...,  4.7378e-01,\n",
      "           -9.4074e-01, -9.0116e-01],\n",
      "          [-4.8180e-01,  4.6490e-01,  1.3428e+00,  ...,  2.7867e-01,\n",
      "           -6.0061e-01, -5.7115e-01],\n",
      "          [ 2.9499e+00, -1.7768e+00, -1.6681e+00,  ..., -7.1877e-01,\n",
      "            3.4090e+00,  3.3434e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-1.2109e-03, -1.4629e-04,  1.0833e-03,  ..., -2.1259e-03,\n",
      "           -2.6191e-04, -6.8051e-04],\n",
      "          [-2.6336e-03,  3.3643e-03,  1.9360e-03,  ...,  2.7080e-03,\n",
      "           -4.2484e-03,  1.9837e-03],\n",
      "          [-1.6460e-04,  2.6689e-03,  7.5009e-04,  ..., -1.2653e-03,\n",
      "           -3.7402e-03,  1.1857e-03],\n",
      "          ...,\n",
      "          [-3.7837e-03,  6.3820e-03,  1.2965e-03,  ...,  4.2498e-04,\n",
      "            9.5963e-04,  4.0690e-03],\n",
      "          [-3.8042e-04, -1.5876e-03, -2.5234e-04,  ..., -1.3467e-03,\n",
      "           -1.5804e-03, -1.4101e-03],\n",
      "          [ 4.6591e-03,  5.3128e-03,  6.7420e-03,  ...,  1.0318e-02,\n",
      "            8.3115e-03, -6.3561e-03]],\n",
      "\n",
      "         [[ 2.3483e-03,  1.7052e-04,  4.2270e-03,  ...,  8.9260e-04,\n",
      "           -1.9698e-03, -1.0090e-03],\n",
      "          [-3.4400e-03, -3.3442e-03, -4.4836e-03,  ...,  2.3106e-03,\n",
      "            1.3018e-03,  6.0001e-04],\n",
      "          [ 7.1232e-04, -2.1454e-03, -4.2633e-03,  ...,  1.5744e-03,\n",
      "           -2.9651e-03, -7.4607e-04],\n",
      "          ...,\n",
      "          [ 5.7570e-03, -2.1445e-03, -1.4089e-03,  ..., -1.2255e-03,\n",
      "           -1.2679e-03,  3.9718e-03],\n",
      "          [-1.8690e-03,  1.0728e-03, -6.8348e-03,  ...,  1.8936e-03,\n",
      "            6.6694e-04, -6.4786e-04],\n",
      "          [-1.3391e-02,  1.0770e-02,  8.3045e-03,  ...,  7.7911e-04,\n",
      "           -1.0178e-02, -5.8963e-03]],\n",
      "\n",
      "         [[-6.1501e-04,  4.4648e-03, -5.8925e-03,  ..., -9.9052e-03,\n",
      "           -2.5577e-03,  1.9986e-03],\n",
      "          [ 3.8935e-03,  1.4992e-03, -1.3253e-03,  ...,  4.3478e-03,\n",
      "           -8.3074e-03, -1.9107e-02],\n",
      "          [-1.8246e-02, -8.6674e-03, -1.2355e-02,  ...,  4.5145e-03,\n",
      "           -1.0351e-02, -4.1616e-03],\n",
      "          ...,\n",
      "          [-1.0935e-02,  9.2776e-03,  6.6386e-04,  ...,  5.2391e-03,\n",
      "            1.1243e-02,  4.2271e-03],\n",
      "          [-1.1005e-02,  1.4795e-02,  1.6315e-03,  ..., -5.2155e-04,\n",
      "            3.5352e-03,  1.4452e-02],\n",
      "          [-2.5535e-03,  3.5211e-05,  5.7577e-03,  ..., -2.2032e-03,\n",
      "            8.9331e-04,  1.8453e-02]],\n",
      "\n",
      "         [[ 4.5752e-04, -2.1883e-03,  1.2693e-04,  ...,  4.2488e-03,\n",
      "           -9.0289e-04,  4.4423e-03],\n",
      "          [ 3.8893e-03,  2.1047e-03, -1.5443e-03,  ...,  2.6456e-03,\n",
      "           -3.3067e-02,  1.3323e-03],\n",
      "          [ 3.7073e-03,  7.5709e-04, -7.9642e-03,  ..., -9.6639e-05,\n",
      "            4.1746e-03,  2.3436e-03],\n",
      "          ...,\n",
      "          [ 1.6199e-03,  3.0380e-03, -4.0387e-03,  ..., -3.3192e-03,\n",
      "            1.5066e-02, -1.0455e-03],\n",
      "          [ 2.8596e-03, -3.5583e-03, -4.2818e-03,  ..., -9.1034e-04,\n",
      "            3.8747e-02,  1.7626e-03],\n",
      "          [ 6.8523e-04, -1.1690e-02,  2.0276e-04,  ..., -4.7451e-03,\n",
      "           -1.7154e-02,  8.6700e-04]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.8828e-01,  3.6189e-01, -7.4322e-01,  ..., -1.5805e+00,\n",
      "            2.7620e-02, -7.1771e-01],\n",
      "          [-1.5587e-01,  3.3290e-01, -4.5352e-01,  ...,  2.8992e-01,\n",
      "           -1.8913e-01,  5.1905e-01],\n",
      "          [ 1.6057e-01, -4.7396e-01,  4.7885e-04,  ..., -3.9514e-01,\n",
      "            6.9952e-02, -1.0926e-01],\n",
      "          ...,\n",
      "          [-3.5205e+00, -1.2623e+00,  2.4637e+00,  ..., -2.9999e+00,\n",
      "           -1.0156e+00, -2.2946e+00],\n",
      "          [ 3.5167e-01, -2.7187e+00,  1.7930e+00,  ..., -2.6951e+00,\n",
      "           -7.5776e-01, -1.3944e+00],\n",
      "          [ 5.7728e-01, -1.3194e+00, -5.0249e-01,  ..., -2.0844e+00,\n",
      "            1.3479e+00, -1.6619e+00]],\n",
      "\n",
      "         [[-1.4724e+00,  4.0073e-01, -9.1775e-01,  ...,  6.7995e-01,\n",
      "            6.2759e-01,  9.0185e-01],\n",
      "          [ 6.8421e-02,  4.8862e-02,  1.2790e-01,  ...,  4.4229e-01,\n",
      "           -1.7637e-01, -9.1588e-01],\n",
      "          [ 1.9398e-01,  8.2052e-03, -1.5185e-02,  ...,  1.3227e-01,\n",
      "           -7.7849e-02, -7.4548e-02],\n",
      "          ...,\n",
      "          [-1.8379e+00,  1.9643e+00,  1.7427e+00,  ..., -5.6919e-01,\n",
      "            9.5655e-01,  2.8202e+00],\n",
      "          [-1.6060e+00,  1.3336e+00,  1.5479e+00,  ...,  8.7757e-02,\n",
      "            8.0458e-01,  2.2474e+00],\n",
      "          [ 3.3782e-01,  2.3505e-01,  8.0742e-01,  ..., -6.3467e-01,\n",
      "            1.7830e+00,  4.4176e-01]],\n",
      "\n",
      "         [[ 3.8637e+00,  3.9051e-01, -1.2857e+00,  ...,  2.6924e-01,\n",
      "           -9.8622e-01, -9.1514e-01],\n",
      "          [-1.2557e-01,  3.9198e-01,  2.9482e-01,  ...,  1.7083e-01,\n",
      "           -4.0618e-01,  1.7431e-01],\n",
      "          [-1.9605e-01, -6.5054e-01, -1.5878e-03,  ...,  2.5123e-01,\n",
      "            1.9969e-01, -4.6797e-01],\n",
      "          ...,\n",
      "          [ 2.6455e+00, -2.3920e+00,  1.6965e+00,  ...,  1.1466e+00,\n",
      "           -7.7580e-01, -2.1566e+00],\n",
      "          [ 5.5772e+00, -3.1370e+00,  1.7268e+00,  ...,  1.7333e+00,\n",
      "           -7.3606e-01, -1.7792e+00],\n",
      "          [-3.7778e-01,  7.1351e-01, -4.6760e-01,  ...,  3.0380e+00,\n",
      "           -2.6382e+00, -1.9358e-01]],\n",
      "\n",
      "         [[-1.3111e+00, -1.9730e+00,  2.5173e-01,  ..., -1.2096e+00,\n",
      "           -1.0444e-01, -1.6182e-01],\n",
      "          [ 4.5064e-01,  1.8737e-01,  5.2465e-01,  ...,  4.0178e-01,\n",
      "           -4.5566e-01, -7.9022e-02],\n",
      "          [-2.9939e-01,  6.1774e-01, -2.6221e-01,  ..., -3.6967e-01,\n",
      "           -3.3125e-01, -3.0725e-01],\n",
      "          ...,\n",
      "          [ 2.2580e+00, -9.2128e-01, -2.4585e+00,  ..., -2.4700e+00,\n",
      "           -6.7977e-01, -2.1241e+00],\n",
      "          [-1.7508e+00,  7.0537e-01, -1.7225e+00,  ..., -1.8488e+00,\n",
      "           -9.9721e-01, -3.5431e+00],\n",
      "          [-1.5381e+00,  3.7267e-01,  4.0707e-01,  ..., -9.2705e-01,\n",
      "           -1.8271e+00,  4.0538e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 2.4596e-02,  1.6263e-02, -3.7252e-03,  ..., -7.0240e-03,\n",
      "            3.9153e-04,  2.4442e-02],\n",
      "          [-4.1183e-02,  3.4897e-03,  1.7885e-02,  ...,  1.2059e-02,\n",
      "            5.0366e-03,  3.7503e-03],\n",
      "          [-7.1177e-03, -6.6925e-03,  2.5682e-03,  ...,  1.0355e-02,\n",
      "            5.8049e-04, -1.0922e-02],\n",
      "          ...,\n",
      "          [ 4.4948e-02, -4.6060e-03,  1.4028e-03,  ..., -2.1439e-02,\n",
      "            8.5929e-03, -3.3836e-02],\n",
      "          [ 2.3723e-02,  2.5135e-02, -1.5455e-02,  ..., -9.6655e-03,\n",
      "            5.6073e-03, -4.2973e-02],\n",
      "          [ 2.6522e-02, -1.5756e-02, -8.0801e-03,  ..., -1.2468e-03,\n",
      "            1.5274e-03,  2.0137e-01]],\n",
      "\n",
      "         [[ 4.1935e-02, -1.2841e-02,  2.1867e-02,  ...,  8.0583e-03,\n",
      "            1.4139e-02,  3.1715e-02],\n",
      "          [-7.8963e-03,  3.0904e-02,  2.8409e-02,  ...,  4.1278e-03,\n",
      "            1.6955e-02, -1.2393e-02],\n",
      "          [ 3.4721e-02, -1.1511e-02,  1.0652e-02,  ...,  9.7135e-03,\n",
      "            3.3756e-02,  8.1413e-03],\n",
      "          ...,\n",
      "          [-3.3967e-03, -1.6769e-02,  1.6981e-02,  ..., -1.6806e-02,\n",
      "           -8.5401e-03, -1.5971e-03],\n",
      "          [-3.2158e-03, -1.2493e-02,  6.1833e-05,  ..., -6.9840e-03,\n",
      "           -3.9560e-02,  1.8718e-02],\n",
      "          [-4.5060e-03, -2.7989e-02,  7.9221e-03,  ..., -3.6239e-02,\n",
      "            2.4100e-02, -7.1335e-03]],\n",
      "\n",
      "         [[-6.6033e-03,  1.3917e-03, -2.1958e-02,  ...,  3.9891e-03,\n",
      "           -2.0989e-02, -4.6576e-02],\n",
      "          [ 1.8968e-02,  6.8218e-02,  3.1840e-02,  ..., -7.7986e-02,\n",
      "            2.9015e-02,  2.0428e-02],\n",
      "          [ 2.1357e-02,  1.3187e-02, -2.5681e-02,  ..., -3.1769e-02,\n",
      "            8.1464e-03, -2.5347e-03],\n",
      "          ...,\n",
      "          [ 8.6113e-03,  1.0603e-01, -7.7951e-03,  ...,  6.8258e-02,\n",
      "           -4.9274e-02, -9.1846e-02],\n",
      "          [ 7.8552e-02, -1.3545e-02, -7.1152e-02,  ...,  2.7050e-02,\n",
      "           -7.8826e-02,  1.5942e-02],\n",
      "          [ 2.9238e-02,  1.5937e-02, -1.7085e-02,  ...,  6.4697e-03,\n",
      "            2.6842e-03,  9.3733e-03]],\n",
      "\n",
      "         [[ 1.0752e-02,  6.3511e-03, -1.0990e-02,  ...,  6.5208e-03,\n",
      "           -2.7217e-02,  4.7827e-03],\n",
      "          [ 3.1303e-02,  1.9895e-02, -3.2524e-02,  ...,  4.5992e-02,\n",
      "           -4.2994e-02, -4.6226e-03],\n",
      "          [-3.3725e-03, -1.0910e-02, -9.6673e-03,  ...,  1.2246e-02,\n",
      "           -4.1391e-02, -1.7211e-02],\n",
      "          ...,\n",
      "          [ 2.1265e-02,  5.1040e-02, -1.7221e-02,  ...,  3.5942e-02,\n",
      "           -4.3871e-02, -4.2120e-02],\n",
      "          [ 1.9180e-02,  1.9322e-02,  1.5258e-02,  ..., -8.6714e-03,\n",
      "            2.0454e-02, -4.4702e-02],\n",
      "          [-1.6891e-03, -4.7610e-03, -2.3047e-02,  ..., -1.6590e-02,\n",
      "           -3.1413e-02, -5.6593e-03]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 5.5442e-01, -1.8435e+00, -1.4437e+00,  ...,  2.8764e+00,\n",
      "            3.8010e+00, -2.9420e+00],\n",
      "          [ 8.5110e-02, -2.6468e-01, -1.8583e-03,  ...,  2.8575e-01,\n",
      "           -3.5326e-02,  1.4652e-01],\n",
      "          [ 5.7609e-01, -3.7163e-02, -3.1284e-01,  ...,  3.3423e-01,\n",
      "            1.3840e-01, -2.6068e-01],\n",
      "          ...,\n",
      "          [-3.0656e+00, -2.8964e+00,  2.4484e+00,  ...,  1.8744e+00,\n",
      "            1.9157e+00, -1.2422e+00],\n",
      "          [ 2.9219e+00, -9.6149e-01,  2.7091e+00,  ...,  1.5318e+00,\n",
      "            1.4817e+00, -7.1262e-01],\n",
      "          [ 2.7508e+00,  5.1084e-01,  1.0535e+00,  ...,  2.0040e+00,\n",
      "            1.0536e+00, -1.2221e+00]],\n",
      "\n",
      "         [[-3.0268e-01, -1.6073e+00, -2.9060e+00,  ..., -2.0678e-01,\n",
      "           -7.8521e-01,  9.1557e-01],\n",
      "          [-3.6592e-01, -4.6722e-01,  1.5187e-02,  ...,  7.0855e-01,\n",
      "           -9.2782e-02,  2.0148e-01],\n",
      "          [ 1.0149e+00, -1.7804e-01,  2.1724e-01,  ...,  1.6646e-01,\n",
      "            4.1720e-02, -1.0240e-02],\n",
      "          ...,\n",
      "          [-2.6633e+00, -8.6602e-01,  9.0273e-01,  ...,  5.4788e-01,\n",
      "            1.3395e+00, -1.4666e+00],\n",
      "          [ 1.4418e+00,  9.0951e-01,  9.8322e-01,  ..., -4.5584e-02,\n",
      "            8.0058e-01, -1.2173e+00],\n",
      "          [ 3.6479e+00,  1.7421e+00,  9.4359e-01,  ...,  3.5450e-01,\n",
      "            2.4122e+00,  1.3084e-01]],\n",
      "\n",
      "         [[ 3.0784e-01, -1.7152e+00,  1.8357e+00,  ...,  7.6916e-01,\n",
      "           -1.7731e+00,  4.9822e-01],\n",
      "          [ 2.3151e-01,  7.1734e-02, -2.3305e-01,  ..., -1.0451e-01,\n",
      "           -8.5864e-01,  5.6641e-01],\n",
      "          [-4.6888e-01,  5.5542e-01, -6.7776e-01,  ...,  1.6941e-01,\n",
      "            4.4597e-02, -1.2314e-01],\n",
      "          ...,\n",
      "          [ 3.2662e+00,  9.1614e-01, -3.6031e+00,  ...,  2.3327e+00,\n",
      "            1.6384e+00,  4.5414e-01],\n",
      "          [-3.3040e+00,  2.8554e+00, -3.0731e+00,  ...,  9.2744e-01,\n",
      "            9.9036e-01,  1.0006e+00],\n",
      "          [-2.5280e+00,  1.3647e+00, -6.1120e-01,  ..., -1.0667e+00,\n",
      "           -1.4245e+00,  9.5720e-01]],\n",
      "\n",
      "         [[ 1.2333e+00,  2.0779e+00, -9.7051e-01,  ...,  6.6698e-01,\n",
      "           -1.7950e-01,  1.3153e-01],\n",
      "          [ 5.6079e-01,  7.1671e-02, -1.7099e-01,  ...,  6.5467e-01,\n",
      "            6.4507e-02,  4.7223e-01],\n",
      "          [-3.5956e-01, -4.2555e-01, -3.5480e-01,  ...,  4.1157e-01,\n",
      "            1.2279e-01,  1.8102e-01],\n",
      "          ...,\n",
      "          [ 2.9004e+00,  3.2962e-01,  1.0576e+00,  ...,  1.4593e+00,\n",
      "           -8.3586e-02, -1.3857e+00],\n",
      "          [-1.5415e+00, -1.9773e+00,  2.6792e+00,  ...,  7.8202e-01,\n",
      "            7.7247e-02, -1.1006e+00],\n",
      "          [-2.6240e+00, -9.1290e-01,  7.7854e-01,  ...,  3.8972e+00,\n",
      "           -2.8280e-01, -7.9706e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-0.0284, -0.0347,  0.0111,  ...,  0.0174, -0.0167,  0.0202],\n",
      "          [ 0.0384, -0.0233, -0.0581,  ...,  0.0018, -0.0055,  0.0072],\n",
      "          [ 0.0433, -0.0197,  0.0129,  ..., -0.0522,  0.0103, -0.0177],\n",
      "          ...,\n",
      "          [-0.0834, -0.0299, -0.0600,  ..., -0.0718,  0.0211, -0.0196],\n",
      "          [-0.0766,  0.0305, -0.0217,  ..., -0.0240,  0.0592, -0.0037],\n",
      "          [-0.0029, -0.0118, -0.0167,  ...,  0.0377, -0.0205,  0.0177]],\n",
      "\n",
      "         [[ 0.0056, -0.0154, -0.0050,  ..., -0.0009, -0.0213,  0.0230],\n",
      "          [-0.0010,  0.0136, -0.0217,  ..., -0.0129, -0.0488, -0.0029],\n",
      "          [-0.0322, -0.0229, -0.0586,  ..., -0.0820,  0.0369,  0.0175],\n",
      "          ...,\n",
      "          [ 0.0153,  0.0205, -0.0086,  ...,  0.0533,  0.0503, -0.0126],\n",
      "          [ 0.0055,  0.0632, -0.0530,  ...,  0.0277, -0.0197, -0.0687],\n",
      "          [-0.0320,  0.0259, -0.0137,  ...,  0.0022, -0.0285,  0.0085]],\n",
      "\n",
      "         [[ 0.0468, -0.0655,  0.0525,  ...,  0.0235,  0.0094, -0.0222],\n",
      "          [ 0.0453,  0.0364, -0.0008,  ...,  0.0153, -0.0209,  0.0102],\n",
      "          [ 0.0429, -0.0437, -0.0191,  ...,  0.0147, -0.0367,  0.0439],\n",
      "          ...,\n",
      "          [ 0.0521,  0.0077,  0.1361,  ..., -0.2123,  0.1012, -0.1312],\n",
      "          [ 0.0188,  0.0292, -0.0119,  ...,  0.1147, -0.0810, -0.0842],\n",
      "          [ 0.0231,  0.0586,  0.0651,  ...,  0.0808, -0.0604,  0.0227]],\n",
      "\n",
      "         [[-0.0311,  0.0011,  0.0571,  ...,  0.0022,  0.0125,  0.0064],\n",
      "          [-0.0413, -0.0391, -0.1330,  ...,  0.0385, -0.0049,  0.0489],\n",
      "          [ 0.0049, -0.0673, -0.0900,  ..., -0.0017, -0.0323, -0.0030],\n",
      "          ...,\n",
      "          [-0.0725,  0.0160,  0.0472,  ..., -0.1241, -0.0722, -0.0143],\n",
      "          [-0.0383,  0.0172, -0.0255,  ...,  0.0181,  0.0404,  0.1125],\n",
      "          [ 0.0282, -0.0177,  0.0099,  ...,  0.0322, -0.0305, -0.0071]]]],\n",
      "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.6426e-03, -2.3357e-02, -6.7909e-03,  ...,  1.0284e+00,\n",
      "           -1.4063e-01, -1.0271e+00],\n",
      "          [ 5.8247e-01, -1.3224e-01,  8.8182e-01,  ..., -3.9781e-01,\n",
      "           -8.5340e-02, -3.6313e-01],\n",
      "          [-2.6019e-02, -7.1060e-02,  1.8801e-01,  ..., -2.6266e-01,\n",
      "            1.0177e+00,  8.9037e-01],\n",
      "          ...,\n",
      "          [ 2.7235e+00, -1.3992e-01, -1.7106e+00,  ..., -1.1654e+00,\n",
      "            7.6604e-01,  6.4390e-01],\n",
      "          [ 3.3380e+00,  1.0089e+00, -2.2079e+00,  ..., -2.5073e+00,\n",
      "            2.4365e-01,  1.7527e+00],\n",
      "          [ 1.2110e-01,  3.5785e-01, -1.5346e-01,  ...,  4.8589e-01,\n",
      "            8.2333e-01, -6.4778e-01]],\n",
      "\n",
      "         [[ 1.0056e-03, -3.7680e-03,  1.6718e-03,  ..., -5.8772e-01,\n",
      "           -1.3528e-01, -6.1934e-01],\n",
      "          [ 4.4221e-02, -9.6585e-02,  3.5939e-01,  ..., -2.5028e-01,\n",
      "            8.7894e-01, -7.1294e-01],\n",
      "          [-2.6183e-02,  2.2363e-01,  1.8126e-02,  ..., -3.5082e-01,\n",
      "           -3.6977e-01,  2.9925e-01],\n",
      "          ...,\n",
      "          [-1.2066e+00, -2.6221e+00, -8.5245e-01,  ...,  5.8797e-01,\n",
      "            7.4287e-02,  1.2248e+00],\n",
      "          [ 8.3912e-01, -1.2909e+00, -9.7289e-01,  ...,  7.6938e-01,\n",
      "            8.9809e-01,  1.0103e+00],\n",
      "          [ 4.5135e-01, -2.6745e-01,  3.5366e-02,  ...,  1.6230e-01,\n",
      "           -1.1128e+00,  3.6106e-01]],\n",
      "\n",
      "         [[ 3.2599e-03,  1.0708e-02, -1.6174e-03,  ...,  6.2076e-01,\n",
      "           -1.0226e+00, -8.5013e-01],\n",
      "          [-1.2782e+00,  4.5638e-01, -6.6100e-01,  ..., -3.2778e-01,\n",
      "           -3.0086e-02, -4.7583e-01],\n",
      "          [-2.1961e-01,  4.6943e-01, -6.0362e-02,  ..., -6.1710e-01,\n",
      "            1.8385e-01, -3.4040e-01],\n",
      "          ...,\n",
      "          [ 9.4857e-01, -5.2858e-01, -3.5549e-01,  ..., -3.0602e-01,\n",
      "            6.7957e-01, -9.8662e-02],\n",
      "          [-1.0426e+00,  1.3979e-01,  1.4197e-01,  ..., -2.5316e+00,\n",
      "            1.5042e+00,  2.4391e-01],\n",
      "          [-5.5108e-01,  6.8431e-01,  4.3364e-01,  ...,  5.4587e-01,\n",
      "           -3.1428e-01,  6.9930e-01]],\n",
      "\n",
      "         [[-2.0082e-02,  1.9534e-02, -1.5234e-02,  ..., -4.0062e-01,\n",
      "            3.6710e-01, -3.6143e-01],\n",
      "          [-1.3734e-02, -2.6440e-01, -3.2130e-01,  ..., -5.8455e-01,\n",
      "            1.0574e+00, -9.1211e-01],\n",
      "          [-2.3972e-01, -1.6478e-01,  3.9981e-01,  ...,  5.9528e-01,\n",
      "            2.9405e-01, -6.3005e-01],\n",
      "          ...,\n",
      "          [ 1.3017e+00, -6.5372e-01,  5.9083e-01,  ...,  6.5508e-01,\n",
      "           -1.4169e-01, -2.4524e-01],\n",
      "          [ 8.3175e-01,  6.8536e-01,  3.5419e-01,  ..., -2.0422e-01,\n",
      "            1.0377e+00, -2.8465e-01],\n",
      "          [ 2.7587e-01, -4.6551e-03, -1.9069e-02,  ..., -6.5441e-01,\n",
      "            5.2257e-01, -5.8605e-01]]]], grad_fn=<AddBackward0>), tensor([[[[ 1.0722e-03, -4.5086e-04,  1.8110e-04,  ...,  1.1413e-04,\n",
      "           -1.5732e-03,  3.0321e-03],\n",
      "          [-2.7212e-02,  1.1572e-01, -6.7970e-02,  ..., -1.0785e-01,\n",
      "           -1.1044e-01,  9.2458e-05],\n",
      "          [-3.0353e-02,  2.1332e-02, -9.8092e-02,  ..., -1.5120e-02,\n",
      "           -6.3443e-02,  2.4295e-02],\n",
      "          ...,\n",
      "          [ 6.5300e-02, -5.6595e-02, -1.7817e-02,  ..., -5.6078e-02,\n",
      "           -5.3269e-02,  8.9255e-03],\n",
      "          [-7.3471e-02,  3.7725e-02,  5.4441e-02,  ...,  7.7377e-02,\n",
      "           -1.5481e-01, -1.3703e-01],\n",
      "          [-1.9059e-02,  2.2660e-03, -7.3378e-02,  ...,  2.1056e-03,\n",
      "            3.8945e-02,  7.0924e-02]],\n",
      "\n",
      "         [[ 4.2027e-02, -1.3180e-03,  4.6068e-05,  ...,  2.0639e-03,\n",
      "            1.0695e-05,  6.3597e-04],\n",
      "          [ 3.6075e-02, -1.3682e-01, -4.0070e-02,  ..., -1.0509e-01,\n",
      "            1.1925e-01,  2.7718e-03],\n",
      "          [ 3.7855e-02, -4.0514e-02, -1.3992e-01,  ..., -1.0805e-01,\n",
      "            1.8910e-01, -4.8389e-02],\n",
      "          ...,\n",
      "          [-2.1587e-02,  2.6893e-02, -4.5308e-02,  ...,  2.9528e-02,\n",
      "           -4.4160e-03,  5.8012e-03],\n",
      "          [-1.7076e-01, -1.3714e-01, -1.2425e-01,  ..., -1.5519e-01,\n",
      "            1.7905e-02, -2.2862e-01],\n",
      "          [-3.2525e-02, -4.2362e-02,  1.8854e-02,  ...,  1.8362e-02,\n",
      "            1.8633e-02, -8.8210e-03]],\n",
      "\n",
      "         [[ 3.5215e-04,  7.4184e-03, -3.8533e-05,  ..., -2.5333e-03,\n",
      "           -1.0099e-03, -3.5661e-04],\n",
      "          [-1.0594e-01, -1.2533e-01,  1.1490e-01,  ..., -9.0990e-02,\n",
      "            4.1433e-02,  1.6522e-01],\n",
      "          [-1.1614e-01, -3.2553e-02, -8.7965e-02,  ...,  1.0648e-01,\n",
      "            1.1424e-01,  6.3952e-02],\n",
      "          ...,\n",
      "          [-2.6344e-02, -1.2469e-01, -3.5640e-02,  ...,  8.3530e-02,\n",
      "            1.0990e-01, -5.9204e-02],\n",
      "          [-1.3280e-02, -1.0806e-01, -8.5255e-02,  ...,  3.4497e-02,\n",
      "            1.3211e-01,  6.6688e-02],\n",
      "          [ 2.2176e-02,  6.4654e-02,  1.5789e-02,  ...,  2.2132e-02,\n",
      "            1.7037e-02, -4.3054e-02]],\n",
      "\n",
      "         [[ 2.5606e-03, -1.3853e-02, -1.1069e-03,  ..., -1.7222e-03,\n",
      "           -2.2425e-03, -3.4573e-03],\n",
      "          [-1.8204e-02,  1.3922e-01,  1.6155e-01,  ..., -1.7892e-01,\n",
      "            7.9912e-02,  1.6213e-01],\n",
      "          [-7.9276e-02,  3.0074e-02, -1.9054e-01,  ..., -9.8541e-02,\n",
      "           -1.5553e-02, -2.2631e-01],\n",
      "          ...,\n",
      "          [ 4.5499e-02,  1.2241e-01,  5.2964e-02,  ...,  1.9468e-01,\n",
      "            1.8942e-01, -5.3050e-02],\n",
      "          [-1.2180e-01,  5.0507e-02,  7.9089e-02,  ...,  5.7812e-02,\n",
      "            7.0893e-02,  8.2904e-02],\n",
      "          [-1.7372e-02, -1.0323e-02,  3.6963e-02,  ..., -2.6989e-02,\n",
      "           -7.7030e-03,  5.5933e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 3.4631e-04,  1.4307e-02,  4.8545e-03,  ...,  1.7930e-01,\n",
      "            5.1566e-01, -1.5301e+00],\n",
      "          [ 4.9182e-01,  8.4678e-01,  2.4854e-01,  ...,  5.7271e-01,\n",
      "            6.4550e-01, -2.1840e-01],\n",
      "          [-1.9742e-01,  7.1504e-01, -1.5201e-01,  ...,  3.1304e-01,\n",
      "            3.7166e-01, -1.6158e-01],\n",
      "          ...,\n",
      "          [ 2.3820e+00, -4.8218e-01, -1.2118e+00,  ..., -8.7556e-01,\n",
      "           -3.3174e-02,  1.6507e+00],\n",
      "          [ 3.1240e-01,  3.9574e-01, -1.6433e+00,  ..., -5.7726e-01,\n",
      "           -1.6646e+00,  1.9905e+00],\n",
      "          [-9.0467e-01,  8.7366e-01, -8.0806e-01,  ..., -2.6019e-01,\n",
      "            1.6856e-02, -1.9604e-02]],\n",
      "\n",
      "         [[ 2.5637e-02, -7.3711e-03, -1.6213e-02,  ...,  4.3829e-01,\n",
      "           -1.3504e+00,  4.4426e-01],\n",
      "          [-9.2364e-03, -2.5064e-01,  7.2828e-03,  ...,  1.9425e+00,\n",
      "            5.3335e-01,  3.8515e-01],\n",
      "          [ 9.3758e-01,  4.9096e-01, -9.3375e-01,  ...,  2.2445e-01,\n",
      "            8.6364e-01,  9.8798e-02],\n",
      "          ...,\n",
      "          [-3.2619e+00,  9.0084e-01, -3.2713e+00,  ..., -6.1875e-01,\n",
      "            1.1750e+00,  1.9410e-02],\n",
      "          [-3.5696e+00,  1.6031e+00, -3.5832e+00,  ..., -1.1293e+00,\n",
      "            1.8111e+00, -1.2226e+00],\n",
      "          [-7.8949e-01,  1.5519e+00, -1.2523e-01,  ..., -3.1279e-01,\n",
      "            2.8177e-01, -8.7160e-02]],\n",
      "\n",
      "         [[-2.4978e-02,  2.6575e-02, -2.3356e-02,  ..., -5.3256e-01,\n",
      "            5.6723e-01,  5.4703e-01],\n",
      "          [ 1.3282e+00, -7.7359e-01,  2.8646e-02,  ..., -5.0593e-01,\n",
      "            7.4410e-01, -4.0656e-01],\n",
      "          [-3.7524e-01,  4.0374e-01, -4.4070e-01,  ..., -7.8406e-01,\n",
      "            3.0978e-01, -5.3467e-01],\n",
      "          ...,\n",
      "          [-9.1938e-02, -1.0250e-01, -1.1700e+00,  ..., -9.5224e-01,\n",
      "           -5.7417e-01, -5.2600e-01],\n",
      "          [ 1.3947e+00,  4.4789e-01, -1.4948e+00,  ..., -1.3142e+00,\n",
      "           -5.9247e-01, -1.0886e+00],\n",
      "          [ 2.7871e-01,  3.4825e-01, -4.3236e-01,  ..., -2.8331e-01,\n",
      "            5.6304e-01,  8.1469e-01]],\n",
      "\n",
      "         [[-3.5636e-03,  6.3377e-03,  2.6891e-02,  ...,  1.1159e+00,\n",
      "           -1.7820e-01,  1.1679e+00],\n",
      "          [ 5.4594e-01,  5.1857e-01,  5.3777e-01,  ...,  6.7464e-01,\n",
      "           -9.3837e-01, -3.3653e-03],\n",
      "          [ 8.0944e-01,  1.0323e-01,  6.7828e-02,  ..., -2.9690e-02,\n",
      "            1.2149e-02,  4.3519e-01],\n",
      "          ...,\n",
      "          [-2.5157e+00,  2.2429e+00,  1.2051e+00,  ...,  4.4277e-01,\n",
      "            4.9459e-01, -4.4653e+00],\n",
      "          [-7.8235e-01,  1.6099e+00,  1.0413e+00,  ...,  9.5295e-01,\n",
      "            1.7553e+00, -3.2438e+00],\n",
      "          [ 8.8150e-01, -3.2653e-01,  1.7093e-01,  ...,  5.9467e-01,\n",
      "            3.0292e-02, -1.1145e-01]]]], grad_fn=<AddBackward0>), tensor([[[[ 2.7111e-03,  6.4958e-04,  1.5430e-04,  ...,  2.4138e-03,\n",
      "           -1.6449e-03, -4.7978e-04],\n",
      "          [ 1.6603e-01,  4.5314e-02, -1.8363e-01,  ...,  1.2799e-01,\n",
      "            1.6205e-01,  3.6653e-02],\n",
      "          [-1.1494e-02, -1.3288e-02, -7.7786e-02,  ..., -7.4274e-02,\n",
      "           -1.0176e-01, -1.5379e-02],\n",
      "          ...,\n",
      "          [ 9.4858e-03, -2.1345e-01, -1.0743e-01,  ...,  5.2648e-02,\n",
      "           -1.9227e-02,  5.6468e-02],\n",
      "          [ 1.5184e-01, -1.0312e-01, -5.9897e-02,  ...,  1.1001e-01,\n",
      "           -2.6888e-02,  6.9540e-02],\n",
      "          [-4.1640e-02, -2.2088e-02,  8.4614e-02,  ...,  3.8877e-02,\n",
      "           -5.5017e-02,  2.4303e-02]],\n",
      "\n",
      "         [[-1.7244e-03, -1.0580e-03, -1.8629e-03,  ...,  3.5375e-04,\n",
      "            6.7257e-04,  2.0380e-03],\n",
      "          [-2.5811e-01, -2.0996e-02, -8.4310e-02,  ...,  1.2947e-01,\n",
      "           -1.1864e-01, -8.7277e-02],\n",
      "          [-1.1051e-01, -1.3822e-02, -9.9797e-02,  ...,  1.3240e-01,\n",
      "           -2.9258e-01, -1.1785e-01],\n",
      "          ...,\n",
      "          [-5.6160e-02, -8.4525e-02, -6.7218e-02,  ...,  1.7226e-01,\n",
      "           -1.4958e-01,  2.0617e-02],\n",
      "          [ 2.5432e-01, -2.2505e-01, -1.2813e-01,  ...,  1.0363e-01,\n",
      "            2.0066e-01, -2.6235e-02],\n",
      "          [ 3.1879e-02, -3.8109e-02, -3.2745e-02,  ..., -3.0837e-02,\n",
      "            3.1486e-02, -2.7302e-03]],\n",
      "\n",
      "         [[-6.8515e-04, -2.4757e-04, -1.8518e-03,  ..., -7.1999e-04,\n",
      "           -1.8820e-03, -1.3466e-03],\n",
      "          [-2.9146e-02,  4.1545e-02,  9.0080e-02,  ...,  2.5171e-02,\n",
      "            2.0161e-02,  1.2939e-01],\n",
      "          [-1.4967e-01, -7.0518e-02,  2.5890e-02,  ..., -1.9509e-01,\n",
      "           -1.4993e-01,  4.4399e-02],\n",
      "          ...,\n",
      "          [-9.3013e-02, -2.4131e-01,  7.5294e-02,  ...,  3.4654e-02,\n",
      "            1.0313e-01,  2.0463e-02],\n",
      "          [-2.0442e-01, -1.8314e-01,  1.2451e-01,  ...,  5.2845e-02,\n",
      "            1.0850e-01, -1.2034e-01],\n",
      "          [-2.8671e-02,  5.3337e-02, -8.6852e-02,  ...,  2.3434e-02,\n",
      "           -3.2438e-02,  6.4249e-03]],\n",
      "\n",
      "         [[ 2.3756e-04,  9.6300e-04, -2.0795e-03,  ...,  7.0104e-04,\n",
      "           -3.9614e-03, -2.9002e-04],\n",
      "          [-2.3067e-01,  9.4506e-02, -3.1505e-02,  ..., -1.7225e-02,\n",
      "           -4.3149e-02,  1.0371e-01],\n",
      "          [ 7.1201e-02, -9.3311e-02, -1.6600e-01,  ..., -5.7708e-03,\n",
      "            1.3415e-01, -1.9475e-01],\n",
      "          ...,\n",
      "          [ 1.5024e-01, -4.1949e-03,  5.5109e-02,  ..., -8.5108e-03,\n",
      "            4.4147e-02,  2.3074e-02],\n",
      "          [ 6.8811e-02, -4.1445e-02,  8.9817e-02,  ..., -2.1105e-01,\n",
      "           -4.4649e-02,  1.0626e-01],\n",
      "          [ 1.7075e-02,  1.5129e-02,  4.4294e-03,  ...,  2.9330e-02,\n",
      "            3.2623e-02, -3.5875e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 3.1695e-03, -3.8940e-03, -4.7405e-03,  ...,  3.0931e-02,\n",
      "           -1.2691e+00,  1.4704e+00],\n",
      "          [-2.0171e-01, -8.5569e-02,  1.0417e-01,  ...,  8.5160e-01,\n",
      "            4.6011e-01, -6.8031e-01],\n",
      "          [-6.3685e-01,  7.1802e-01, -3.2386e-01,  ...,  1.0564e-01,\n",
      "            3.5766e-01, -8.1449e-01],\n",
      "          ...,\n",
      "          [ 2.4139e+00,  2.2142e+00, -1.2213e+00,  ..., -2.6331e-01,\n",
      "            2.0932e+00, -2.8782e+00],\n",
      "          [-5.2624e-01,  1.9941e+00, -6.8248e-01,  ..., -4.0465e-01,\n",
      "            1.9470e+00, -3.3101e+00],\n",
      "          [-1.3669e+00,  2.4603e-01, -4.3450e-01,  ..., -4.2822e-01,\n",
      "            4.9800e-01, -8.8663e-01]],\n",
      "\n",
      "         [[-1.2825e-02, -5.3718e-03,  9.9045e-03,  ...,  1.2701e-01,\n",
      "            2.1490e-01,  1.5056e-01],\n",
      "          [ 3.9705e-01, -1.2519e+00,  3.1780e-01,  ..., -1.0643e+00,\n",
      "            4.4525e-01, -8.2515e-01],\n",
      "          [ 9.5340e-01, -1.1154e+00, -8.3748e-02,  ..., -7.3213e-01,\n",
      "            1.1280e+00, -5.5808e-02],\n",
      "          ...,\n",
      "          [-3.6204e+00, -6.4596e-01, -2.4775e+00,  ..., -2.3710e-01,\n",
      "           -3.0249e-01, -2.0323e-01],\n",
      "          [-3.5367e+00, -2.9491e+00, -2.5103e+00,  ..., -1.4906e-01,\n",
      "           -2.5881e-01, -1.6129e-02],\n",
      "          [ 1.7927e-01, -1.8448e+00, -6.7133e-01,  ..., -2.5125e-01,\n",
      "            6.6146e-02,  5.0068e-01]],\n",
      "\n",
      "         [[ 6.6384e-03, -1.1701e-02,  5.2739e-03,  ..., -1.2947e+00,\n",
      "           -3.4950e-03, -7.9600e-02],\n",
      "          [-6.6914e-01,  2.9493e-01,  1.4781e-01,  ...,  4.4147e-01,\n",
      "           -6.8412e-01, -3.9277e-02],\n",
      "          [-1.1591e+00,  1.3685e-01,  5.6443e-01,  ...,  6.6275e-01,\n",
      "            1.3934e+00, -2.0989e-01],\n",
      "          ...,\n",
      "          [ 2.8594e+00,  2.5554e+00,  7.0344e-01,  ...,  1.8893e+00,\n",
      "           -9.3450e-01, -3.6989e-01],\n",
      "          [-1.1616e+00,  1.2827e+00,  1.1194e+00,  ...,  2.1328e+00,\n",
      "            4.2432e-02,  7.7307e-01],\n",
      "          [-2.0841e+00,  3.2261e-01, -7.9120e-01,  ...,  8.5610e-01,\n",
      "           -2.5165e-01,  2.6593e-02]],\n",
      "\n",
      "         [[-2.5343e-02, -2.0777e-02, -1.2890e-02,  ...,  2.6611e-01,\n",
      "           -1.3869e+00, -7.7375e-01],\n",
      "          [ 5.0793e-02, -5.3095e-01, -9.3641e-02,  ...,  2.4104e-01,\n",
      "           -2.6519e-02,  1.7226e+00],\n",
      "          [ 9.8945e-01,  6.3579e-01, -2.6775e-01,  ..., -1.3978e+00,\n",
      "            1.7149e-01,  1.1820e+00],\n",
      "          ...,\n",
      "          [-2.6011e+00,  4.1430e-01, -2.1462e-01,  ..., -6.5096e-01,\n",
      "            1.3148e+00,  1.1267e+00],\n",
      "          [-2.1281e+00,  1.6814e+00,  7.9706e-01,  ...,  5.7670e-01,\n",
      "            1.4613e+00,  2.0012e+00],\n",
      "          [ 2.4445e-01,  8.3197e-01,  8.4380e-01,  ...,  1.8398e-01,\n",
      "            1.1499e+00,  8.2760e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-0.0007, -0.0019,  0.0020,  ...,  0.0010,  0.0004, -0.0025],\n",
      "          [ 0.1808,  0.1408, -0.0291,  ..., -0.2090, -0.0973,  0.1622],\n",
      "          [ 0.2088,  0.1464, -0.0243,  ..., -0.1426, -0.0277,  0.0607],\n",
      "          ...,\n",
      "          [-0.0307,  0.0075, -0.0309,  ..., -0.0674, -0.0165,  0.0017],\n",
      "          [ 0.0554,  0.0372, -0.1121,  ..., -0.0270, -0.0455,  0.0118],\n",
      "          [ 0.0210,  0.0343, -0.0302,  ..., -0.0282, -0.0731, -0.0133]],\n",
      "\n",
      "         [[ 0.0062, -0.0021, -0.0028,  ...,  0.0017, -0.0074, -0.0032],\n",
      "          [-0.0281,  0.0938, -0.1469,  ...,  0.1942,  0.1314, -0.0923],\n",
      "          [-0.0119, -0.0570, -0.1161,  ..., -0.0654,  0.0822,  0.0016],\n",
      "          ...,\n",
      "          [-0.1776, -0.2204, -0.0459,  ..., -0.0280,  0.1015, -0.1596],\n",
      "          [-0.1116,  0.0989,  0.0321,  ..., -0.1084,  0.0922, -0.2289],\n",
      "          [-0.2523,  0.0113, -0.0220,  ..., -0.0335,  0.0767, -0.1329]],\n",
      "\n",
      "         [[-0.0013,  0.0051,  0.0026,  ..., -0.0267,  0.0023, -0.0011],\n",
      "          [-0.0812, -0.0437, -0.2103,  ...,  0.0760, -0.0008,  0.1496],\n",
      "          [-0.0057, -0.0727, -0.2615,  ...,  0.2145,  0.1069,  0.0051],\n",
      "          ...,\n",
      "          [-0.0759,  0.1120, -0.0370,  ...,  0.1632, -0.1646, -0.0158],\n",
      "          [-0.1456,  0.1772, -0.0091,  ...,  0.2205, -0.1746, -0.1826],\n",
      "          [-0.0538,  0.1437, -0.0901,  ...,  0.1974, -0.1629, -0.0519]],\n",
      "\n",
      "         [[-0.0005, -0.0012, -0.0005,  ..., -0.0019,  0.0008,  0.0024],\n",
      "          [-0.0103,  0.1686, -0.1285,  ...,  0.1093,  0.1387,  0.0964],\n",
      "          [-0.1206,  0.3321,  0.0580,  ..., -0.0540,  0.0048,  0.0511],\n",
      "          ...,\n",
      "          [-0.0258,  0.0193, -0.0556,  ...,  0.0758,  0.0251,  0.0117],\n",
      "          [-0.0218,  0.0442, -0.0183,  ..., -0.1038,  0.1077, -0.1124],\n",
      "          [-0.0500,  0.0713,  0.0138,  ...,  0.0583,  0.0636, -0.1154]]]],\n",
      "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.1991e-02,  1.5245e-04,  9.4955e-03,  ...,  4.5953e-01,\n",
      "            6.8838e-01,  7.5729e-01],\n",
      "          [-8.3051e-01, -7.7000e-01,  4.3197e-01,  ...,  2.2524e+00,\n",
      "            4.8856e-01,  3.2182e-01],\n",
      "          [-2.7636e-01, -4.0072e-01,  7.9331e-01,  ...,  1.2024e+00,\n",
      "            4.2242e-02, -1.0283e-01],\n",
      "          ...,\n",
      "          [ 2.5098e+00, -1.9330e+00,  2.1349e+00,  ...,  4.0293e-01,\n",
      "           -2.4172e-01,  6.2033e-01],\n",
      "          [-5.3892e-01, -1.8711e+00,  1.7277e+00,  ...,  2.3345e-01,\n",
      "           -1.1410e+00,  3.5800e-01],\n",
      "          [-1.6434e+00, -1.1556e-01,  1.1961e+00,  ...,  8.1988e-01,\n",
      "           -4.2560e-02,  9.5215e-01]],\n",
      "\n",
      "         [[-2.9994e-03, -1.4975e-02, -1.1991e-02,  ..., -1.2132e-01,\n",
      "           -2.9154e-02,  3.4619e-01],\n",
      "          [ 7.4281e-01,  2.4038e-01, -1.0209e+00,  ...,  2.1543e+00,\n",
      "           -4.4418e-01,  4.6687e-01],\n",
      "          [ 1.5619e+00, -2.5479e-01, -4.5561e-01,  ...,  1.3586e+00,\n",
      "            6.0222e-01,  5.9348e-01],\n",
      "          ...,\n",
      "          [-1.3172e+00, -2.5701e+00,  1.2285e+00,  ...,  1.0977e+00,\n",
      "           -1.3128e+00,  3.1261e-01],\n",
      "          [ 1.6249e-01, -2.0120e+00,  1.7341e+00,  ...,  1.0685e+00,\n",
      "           -1.1155e+00, -3.6354e-02],\n",
      "          [ 1.0425e+00, -1.0937e-01,  1.4388e+00,  ...,  1.1418e+00,\n",
      "           -1.4139e+00,  1.0429e-01]],\n",
      "\n",
      "         [[ 5.2733e-03,  7.4820e-03, -6.8778e-03,  ...,  1.3464e-01,\n",
      "           -2.0955e-02, -9.1280e-02],\n",
      "          [ 2.1056e+00,  7.4554e-01, -1.5940e+00,  ...,  1.0040e+00,\n",
      "           -8.2271e-01,  3.2326e-01],\n",
      "          [ 1.8845e+00,  1.4217e+00, -1.3876e+00,  ...,  9.2035e-01,\n",
      "           -7.8891e-01,  4.5283e-01],\n",
      "          ...,\n",
      "          [-4.0486e+00,  2.2651e+00, -1.3070e+00,  ..., -1.3413e+00,\n",
      "           -7.2835e-01,  3.8201e-01],\n",
      "          [ 2.0702e+00,  3.1460e+00,  6.9420e-01,  ..., -2.3315e+00,\n",
      "           -6.1998e-01,  4.8293e-01],\n",
      "          [ 4.6597e+00,  1.3383e+00,  1.2279e+00,  ..., -1.3693e+00,\n",
      "           -3.9053e-01,  4.8369e-01]],\n",
      "\n",
      "         [[ 2.8953e-03,  1.0954e-02,  1.5773e-03,  ...,  1.2502e+00,\n",
      "           -1.5621e+00, -6.9291e-02],\n",
      "          [-4.4651e-01, -6.3231e-01,  3.6742e-02,  ...,  9.7561e-01,\n",
      "            2.0992e+00,  1.0369e+00],\n",
      "          [ 5.0057e-01, -4.2472e-01, -5.2578e-01,  ...,  6.7653e-01,\n",
      "            8.0750e-01,  2.5365e+00],\n",
      "          ...,\n",
      "          [-2.2012e+00, -2.5340e+00, -2.3085e+00,  ..., -2.7000e+00,\n",
      "            5.1113e+00,  1.8872e+00],\n",
      "          [-3.1377e+00, -1.3429e+00, -2.2503e+00,  ..., -2.6135e+00,\n",
      "            5.2981e+00,  2.7699e+00],\n",
      "          [-8.4949e-01,  8.2578e-01, -5.5849e-01,  ..., -8.8972e-02,\n",
      "            2.5623e+00,  2.2915e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 2.7269e-03,  1.8422e-03, -7.4942e-04,  ..., -4.8351e-03,\n",
      "            1.5781e-03, -2.6535e-03],\n",
      "          [ 1.9490e-01, -2.0353e-01,  2.8358e-01,  ...,  1.8831e-01,\n",
      "            1.7002e-02,  2.3421e-01],\n",
      "          [-3.7987e-02, -1.5450e-03,  2.0220e-01,  ...,  2.5784e-01,\n",
      "           -8.0829e-02, -4.4091e-02],\n",
      "          ...,\n",
      "          [ 4.3597e-03, -7.5374e-02,  5.5701e-02,  ...,  3.6005e-02,\n",
      "           -6.7219e-03,  1.0686e-01],\n",
      "          [-2.4659e-02,  1.5087e-01,  2.5726e-01,  ...,  4.6351e-02,\n",
      "           -1.3133e-01,  1.6870e-01],\n",
      "          [-2.1162e-02, -3.7243e-02,  6.9046e-02,  ..., -1.7662e-02,\n",
      "           -4.2678e-02,  1.3710e-01]],\n",
      "\n",
      "         [[-2.7409e-04,  4.5685e-04, -2.1493e-03,  ..., -1.5816e-04,\n",
      "            2.4730e-03,  2.7887e-03],\n",
      "          [ 9.3411e-02,  6.2804e-02,  1.3381e-01,  ...,  9.9665e-02,\n",
      "            2.5315e-02,  9.0730e-02],\n",
      "          [-8.1474e-02, -1.6686e-01,  2.3439e-01,  ...,  3.0520e-02,\n",
      "           -4.7767e-02,  4.6927e-03],\n",
      "          ...,\n",
      "          [-1.1084e-01,  4.6220e-02,  8.6808e-02,  ...,  1.0774e-01,\n",
      "            1.1070e-01, -2.8309e-01],\n",
      "          [ 2.7821e-02,  9.7738e-02,  6.1615e-02,  ...,  8.6979e-02,\n",
      "            1.5267e-01, -1.8611e-01],\n",
      "          [-2.1902e-02, -8.1657e-03, -4.3012e-02,  ..., -4.8925e-03,\n",
      "            1.1854e-01, -1.8208e-01]],\n",
      "\n",
      "         [[-1.7423e-02, -1.7803e-03,  2.5829e-03,  ...,  1.7613e-04,\n",
      "            2.2554e-03, -6.9004e-04],\n",
      "          [ 3.5697e-02,  5.4348e-02, -3.8024e-02,  ...,  1.3000e-01,\n",
      "           -2.5446e-01,  1.7410e-01],\n",
      "          [-1.2516e-03,  9.4747e-02,  3.6923e-02,  ..., -3.5763e-02,\n",
      "           -2.5652e-01,  8.7143e-03],\n",
      "          ...,\n",
      "          [ 1.5856e-03, -6.0736e-02,  1.3951e-01,  ..., -1.4430e-01,\n",
      "           -2.2426e-01, -6.0272e-02],\n",
      "          [ 3.8897e-02, -2.2352e-02,  1.3954e-01,  ..., -1.6821e-01,\n",
      "           -1.9830e-01, -1.9949e-01],\n",
      "          [ 2.6974e-01, -6.5836e-02,  1.7217e-01,  ..., -7.5391e-02,\n",
      "           -1.8536e-01, -1.3054e-01]],\n",
      "\n",
      "         [[ 1.0662e-03, -1.9989e-03, -1.1739e-03,  ..., -2.8751e-03,\n",
      "           -1.6661e-03,  2.3947e-03],\n",
      "          [ 1.6696e-01, -1.3347e-01,  8.3266e-02,  ...,  3.8293e-03,\n",
      "           -4.9930e-02, -2.9248e-02],\n",
      "          [-8.6941e-03, -1.4615e-01,  6.5733e-02,  ..., -2.9713e-03,\n",
      "           -2.8591e-02,  4.5035e-02],\n",
      "          ...,\n",
      "          [ 5.2319e-02, -9.4903e-02, -4.5992e-02,  ..., -2.3437e-02,\n",
      "            8.1771e-02,  8.0781e-02],\n",
      "          [ 4.7183e-02, -8.1933e-02, -4.7701e-03,  ..., -6.7130e-02,\n",
      "            1.2916e-01,  9.4459e-02],\n",
      "          [ 2.1647e-02, -1.1709e-01, -4.3327e-02,  ..., -6.0214e-03,\n",
      "           -6.3893e-02,  3.5389e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 4.1381e-03, -3.1488e-03, -1.4203e-02,  ..., -1.6697e-01,\n",
      "            3.8361e-01, -1.8616e+00],\n",
      "          [ 7.2438e-01,  1.8248e-01,  6.3291e-01,  ...,  1.9329e-01,\n",
      "           -6.1870e-01, -6.0599e-01],\n",
      "          [ 8.4986e-01, -4.4171e-01,  1.2662e+00,  ...,  1.1360e-01,\n",
      "           -7.6260e-01, -3.7286e-01],\n",
      "          ...,\n",
      "          [-9.1801e-01,  3.7006e-01,  1.7934e-01,  ..., -2.8780e-01,\n",
      "            4.5871e-01,  2.0291e+00],\n",
      "          [ 1.0689e+00, -1.0313e+00, -5.3496e-01,  ..., -4.3191e-01,\n",
      "            2.7760e-01,  2.0736e+00],\n",
      "          [ 1.3838e+00, -1.1172e+00, -1.0157e+00,  ..., -5.0748e-02,\n",
      "            9.1875e-01,  1.2715e+00]],\n",
      "\n",
      "         [[ 7.0465e-03, -1.6245e-02,  2.9662e-03,  ...,  3.2687e-01,\n",
      "            2.9341e-02, -2.3847e+00],\n",
      "          [ 6.3019e-01, -2.2855e-01,  1.8245e-01,  ...,  7.7723e-02,\n",
      "           -1.5636e+00,  2.6990e-01],\n",
      "          [-1.7959e-01, -1.0242e+00,  1.4594e-01,  ..., -2.5045e+00,\n",
      "           -2.0427e+00,  1.4075e+00],\n",
      "          ...,\n",
      "          [-4.3674e-01, -8.0460e-01,  2.4341e+00,  ..., -4.1521e+00,\n",
      "            3.2510e-01,  4.6996e+00],\n",
      "          [ 2.5156e+00, -2.3882e+00,  1.7084e+00,  ..., -4.1144e+00,\n",
      "            1.8134e-01,  4.7059e+00],\n",
      "          [ 2.2319e+00, -2.0682e+00,  1.0934e-01,  ..., -4.4631e+00,\n",
      "            1.2505e-01,  3.5329e+00]],\n",
      "\n",
      "         [[-7.8543e-03, -3.3968e-02, -2.2614e-02,  ...,  1.1253e+00,\n",
      "           -5.4290e-01,  1.5537e+00],\n",
      "          [-6.8685e-01, -1.3367e-01,  4.9840e-02,  ...,  2.3092e+00,\n",
      "           -3.2789e-01,  3.1500e-02],\n",
      "          [-3.5988e-01, -5.2533e-01,  1.6850e-01,  ...,  1.6503e+00,\n",
      "            1.1488e-01, -5.5508e-01],\n",
      "          ...,\n",
      "          [ 1.4148e+00, -9.1710e-01, -1.8082e+00,  ..., -1.3163e+00,\n",
      "            6.9538e-02, -2.3952e+00],\n",
      "          [-1.8186e-01, -1.3950e+00, -1.7499e+00,  ..., -1.7846e+00,\n",
      "           -2.8780e-01, -2.8041e+00],\n",
      "          [-1.2525e+00, -5.6614e-01, -9.9406e-01,  ..., -1.3282e+00,\n",
      "            5.1014e-02, -1.5517e+00]],\n",
      "\n",
      "         [[ 1.7481e-02, -4.7006e-03,  7.3111e-03,  ...,  7.5355e-01,\n",
      "           -2.8627e+00, -1.3464e+00],\n",
      "          [-1.9560e-01,  5.1132e-01, -3.8607e-01,  ..., -2.1207e-01,\n",
      "           -9.8035e-01,  7.9891e-01],\n",
      "          [ 2.3010e-01,  6.1415e-01, -2.3067e-01,  ..., -9.1489e-01,\n",
      "            1.9395e-01,  6.3942e-02],\n",
      "          ...,\n",
      "          [-1.5055e+00,  1.8628e+00, -1.2961e+00,  ..., -2.1566e+00,\n",
      "            1.8881e+00,  3.0764e+00],\n",
      "          [-1.6697e+00,  7.1614e-01, -7.3605e-01,  ..., -2.0690e+00,\n",
      "            2.4076e+00,  2.7017e+00],\n",
      "          [-6.7540e-01, -5.9125e-01, -3.7540e-01,  ..., -2.4988e+00,\n",
      "            9.9813e-01,  2.1776e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 2.0173e-03,  6.0756e-04, -8.4807e-04,  ...,  5.6875e-03,\n",
      "            1.3032e-02,  4.7011e-03],\n",
      "          [-9.1968e-02,  1.8930e-01,  1.6172e-01,  ..., -1.9879e-01,\n",
      "            3.0534e-01, -3.9670e-01],\n",
      "          [-4.4485e-01, -1.8106e-01, -7.2426e-02,  ..., -2.4603e-01,\n",
      "            1.5529e-01, -2.9384e-01],\n",
      "          ...,\n",
      "          [ 1.4175e-01,  1.7578e-01,  1.6682e-01,  ..., -1.6632e-01,\n",
      "           -5.5541e-02, -4.9867e-01],\n",
      "          [-7.6171e-02,  1.8284e-01,  2.5642e-01,  ..., -1.5431e-01,\n",
      "            7.9805e-02, -4.4213e-01],\n",
      "          [ 7.6594e-02,  1.2608e-01,  2.4565e-01,  ..., -1.5056e-01,\n",
      "            1.1855e-02, -2.4719e-01]],\n",
      "\n",
      "         [[ 2.8113e-03,  5.9525e-04,  1.3361e-03,  ..., -3.4663e-04,\n",
      "           -2.9381e-03,  9.4140e-04],\n",
      "          [ 3.1906e-01,  2.9991e-01, -1.9872e-02,  ..., -3.8637e-02,\n",
      "           -1.7047e-01,  5.9798e-02],\n",
      "          [ 2.6986e-01,  6.1068e-02,  1.9577e-03,  ...,  6.3233e-03,\n",
      "            2.5369e-01,  3.5607e-01],\n",
      "          ...,\n",
      "          [ 3.8969e-03, -1.6913e-01,  8.0009e-02,  ...,  4.3010e-03,\n",
      "           -1.7956e-01,  1.7246e-01],\n",
      "          [-1.0903e-01, -1.3113e-01,  1.5231e-01,  ..., -2.6797e-02,\n",
      "           -3.0185e-01,  2.6434e-01],\n",
      "          [ 5.1014e-04, -1.0285e-01,  8.9947e-02,  ..., -2.6748e-02,\n",
      "           -1.4320e-01,  2.1312e-01]],\n",
      "\n",
      "         [[-6.1816e-03, -3.2478e-03,  2.9683e-05,  ...,  3.7073e-03,\n",
      "           -6.1424e-03,  4.6241e-03],\n",
      "          [ 1.4130e-01,  1.2116e-01,  3.7530e-02,  ..., -2.2070e-01,\n",
      "            2.7444e-02,  1.0726e-01],\n",
      "          [ 1.5982e-02,  8.5448e-02,  4.3929e-02,  ..., -2.6447e-02,\n",
      "            2.2698e-01,  2.0057e-02],\n",
      "          ...,\n",
      "          [-1.1481e-01, -1.4273e-02, -1.8018e-02,  ...,  6.4144e-02,\n",
      "            2.7924e-01,  6.2736e-02],\n",
      "          [-2.2141e-01, -6.2739e-02, -4.7459e-03,  ..., -3.0034e-02,\n",
      "            2.7559e-01,  9.0439e-02],\n",
      "          [-1.8331e-01, -8.0891e-02, -4.7975e-02,  ..., -3.4614e-02,\n",
      "            2.5762e-01, -3.5990e-02]],\n",
      "\n",
      "         [[-2.6432e-03, -6.7103e-04,  7.8744e-02,  ...,  3.5473e-03,\n",
      "            8.2038e-04, -1.3276e-03],\n",
      "          [-1.1006e-01, -2.7429e-01, -6.7271e-02,  ..., -1.3140e-01,\n",
      "            7.5693e-02,  2.0447e-01],\n",
      "          [-7.6803e-02, -4.3991e-02, -6.3628e-02,  ..., -3.3948e-01,\n",
      "           -2.2270e-02,  2.3388e-01],\n",
      "          ...,\n",
      "          [ 5.2251e-02, -4.0205e-02,  1.1040e-01,  ...,  1.2598e-02,\n",
      "           -1.5463e-02, -4.2599e-02],\n",
      "          [ 1.1238e-01, -4.6984e-02, -3.3950e-02,  ..., -7.1850e-02,\n",
      "           -9.4524e-02, -1.8095e-01],\n",
      "          [ 7.6933e-02,  3.4311e-03,  1.8685e-01,  ...,  8.1268e-04,\n",
      "           -3.6005e-02,  2.4970e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-2.4062e-03, -1.8663e-03, -8.8815e-03,  ..., -4.7154e-02,\n",
      "            4.9732e-02, -1.5398e-01],\n",
      "          [ 9.6829e-01, -6.0671e-01, -8.4084e-01,  ...,  1.1984e+00,\n",
      "           -1.1425e+00, -2.1874e+00],\n",
      "          [ 2.1586e+00, -1.7205e+00, -1.9554e+00,  ...,  5.8393e-01,\n",
      "           -5.4466e-01, -8.4404e-01],\n",
      "          ...,\n",
      "          [-4.2479e+00, -2.6480e+00, -1.7518e+00,  ..., -3.5392e-01,\n",
      "            3.7495e-03, -2.2708e+00],\n",
      "          [-5.8095e-01, -3.2993e+00,  2.2509e-03,  ..., -2.4965e-01,\n",
      "           -3.9442e-01, -2.7280e+00],\n",
      "          [ 2.8625e+00, -2.2929e+00,  1.6667e+00,  ..., -4.5939e-01,\n",
      "            2.1074e-01, -2.3458e+00]],\n",
      "\n",
      "         [[ 2.9169e-03, -6.8844e-03,  7.4502e-04,  ...,  4.5993e-01,\n",
      "            1.5264e+00,  1.5507e+00],\n",
      "          [-8.2061e-02,  1.6511e-01, -1.0156e+00,  ..., -8.8053e-01,\n",
      "           -4.3702e+00, -2.6596e+00],\n",
      "          [-5.9017e-01,  3.2150e-01, -5.9029e-01,  ..., -7.1127e-01,\n",
      "           -3.0115e+00, -3.8407e+00],\n",
      "          ...,\n",
      "          [ 1.1411e+00,  6.3392e-01, -3.6181e-01,  ...,  1.6357e-01,\n",
      "           -4.7341e+00, -7.4062e+00],\n",
      "          [ 6.2452e-01,  6.3958e-01,  2.2096e-01,  ..., -7.6149e-01,\n",
      "           -5.5078e+00, -5.0928e+00],\n",
      "          [-4.2721e-01,  2.3525e-01,  5.5319e-01,  ..., -1.7302e+00,\n",
      "           -3.2195e+00, -5.2727e+00]],\n",
      "\n",
      "         [[-1.4856e-02,  6.7577e-03,  3.6167e-02,  ...,  1.1305e+00,\n",
      "           -3.2681e-01,  6.8020e-01],\n",
      "          [-1.2992e+00,  1.5143e+00,  1.0874e+00,  ..., -8.6695e-01,\n",
      "            8.9096e-01,  4.6879e-01],\n",
      "          [-2.9715e+00,  1.6147e+00,  1.8252e+00,  ..., -2.0127e+00,\n",
      "            6.8360e-01, -1.8487e-01],\n",
      "          ...,\n",
      "          [ 4.1072e+00,  3.5822e+00,  1.3607e+00,  ..., -3.7140e+00,\n",
      "            1.3421e+00, -3.8265e-01],\n",
      "          [ 2.3219e-01,  2.4863e+00, -1.1924e+00,  ..., -3.5097e+00,\n",
      "            1.2188e+00, -4.5864e-01],\n",
      "          [-3.0177e+00, -4.1362e-03, -2.7807e+00,  ..., -2.9441e+00,\n",
      "            2.3774e+00,  6.4061e-01]],\n",
      "\n",
      "         [[-6.1277e-03,  2.4817e-04,  2.0344e-02,  ...,  2.7340e-02,\n",
      "           -9.0499e-02,  2.3263e-01],\n",
      "          [ 8.0061e-01, -5.0203e-01, -3.2862e-01,  ..., -6.5838e-01,\n",
      "            1.2209e+00,  2.5389e-01],\n",
      "          [ 1.9164e+00, -1.5906e+00,  3.0783e-01,  ..., -8.9318e-01,\n",
      "            7.9651e-01,  9.5811e-01],\n",
      "          ...,\n",
      "          [-3.1246e+00, -8.5484e-01,  1.7631e+00,  ..., -1.0414e+00,\n",
      "           -4.5362e-01,  2.1134e-01],\n",
      "          [-1.6734e+00, -2.0679e+00,  1.1507e+00,  ..., -2.7448e-01,\n",
      "           -4.5516e-01,  3.3430e-01],\n",
      "          [ 1.1101e+00, -9.7450e-01,  5.1687e-01,  ..., -6.4941e-01,\n",
      "           -6.2992e-01,  3.4845e-01]]]], grad_fn=<AddBackward0>), tensor([[[[ 2.9910e-03, -9.7400e-04,  2.6428e-03,  ...,  5.9068e-03,\n",
      "           -1.0227e-03, -1.3702e-03],\n",
      "          [ 1.6341e-01,  1.1356e-01, -7.7466e-02,  ..., -1.0813e-01,\n",
      "            2.4105e-01, -2.7721e-01],\n",
      "          [-1.7205e-02,  5.2880e-02, -2.0914e-02,  ..., -1.3707e-01,\n",
      "           -1.7004e-01, -3.3832e-01],\n",
      "          ...,\n",
      "          [ 4.3035e-02, -1.1577e-01,  5.3843e-02,  ..., -3.1912e-01,\n",
      "            1.5608e-01, -1.6797e-01],\n",
      "          [ 7.1515e-02, -1.9513e-01,  1.0399e-01,  ..., -3.1026e-01,\n",
      "            6.3865e-02, -1.8665e-01],\n",
      "          [-3.4353e-02, -6.4701e-02,  1.1551e-01,  ..., -2.9029e-01,\n",
      "            1.5547e-01, -1.1812e-01]],\n",
      "\n",
      "         [[-2.6684e-03, -2.5892e-03,  2.6636e-04,  ...,  3.9769e-03,\n",
      "            5.3761e-04,  5.6468e-04],\n",
      "          [-7.3457e-03,  2.0544e-01, -2.2465e-02,  ...,  1.1562e-01,\n",
      "           -2.2371e-01, -1.0073e-01],\n",
      "          [ 1.9337e-01,  1.6595e-01,  1.0301e-01,  ...,  3.1891e-02,\n",
      "           -7.9178e-02, -6.9461e-02],\n",
      "          ...,\n",
      "          [-1.4334e-01,  1.8951e-01, -1.0412e-01,  ...,  1.3022e-01,\n",
      "           -1.3416e-01, -8.7437e-02],\n",
      "          [-1.3966e-01,  2.2545e-01, -7.4099e-02,  ...,  2.0802e-01,\n",
      "           -1.3573e-01, -7.1305e-02],\n",
      "          [-1.8255e-01,  2.0517e-01, -9.8727e-02,  ...,  1.2224e-04,\n",
      "           -6.8716e-03, -8.9559e-02]],\n",
      "\n",
      "         [[-1.6999e-04,  5.9515e-05,  1.1052e-03,  ..., -6.3309e-04,\n",
      "           -1.4694e-04,  3.1643e-03],\n",
      "          [-4.5776e-02, -3.2543e-01, -4.2809e-01,  ...,  1.5161e-01,\n",
      "           -7.2149e-03,  7.0098e-02],\n",
      "          [-9.3779e-02, -3.1502e-01, -1.4625e-01,  ..., -9.6508e-02,\n",
      "            3.3335e-02,  4.0247e-02],\n",
      "          ...,\n",
      "          [-2.5713e-03, -6.4427e-02,  1.2815e-01,  ..., -1.8736e-01,\n",
      "            8.5802e-03,  1.6145e-01],\n",
      "          [-7.0631e-02, -1.2957e-01,  8.5985e-02,  ..., -1.0776e-01,\n",
      "           -7.6723e-04,  2.4867e-01],\n",
      "          [-6.2968e-02, -1.1251e-01,  1.0099e-01,  ..., -1.6365e-01,\n",
      "            5.0411e-02,  2.5083e-01]],\n",
      "\n",
      "         [[ 3.6064e-03, -5.3748e-03, -2.6360e-03,  ..., -6.2820e-04,\n",
      "           -6.8743e-03,  2.1284e-03],\n",
      "          [ 1.4555e-02,  5.7211e-02,  1.2508e-01,  ...,  2.3225e-02,\n",
      "            9.8262e-02,  4.9671e-01],\n",
      "          [-1.1086e-01,  1.0001e-01, -8.4500e-02,  ...,  2.3097e-01,\n",
      "            1.1734e-01,  3.9575e-01],\n",
      "          ...,\n",
      "          [-2.1945e-01, -2.6755e-01,  1.3179e-01,  ...,  3.2887e-01,\n",
      "           -1.5398e-01,  1.1970e-02],\n",
      "          [-1.2355e-01, -2.0766e-01,  1.0219e-01,  ...,  3.0112e-01,\n",
      "           -1.1381e-01,  1.8631e-01],\n",
      "          [-1.7439e-01, -1.8271e-01,  2.3238e-01,  ...,  3.3812e-01,\n",
      "           -2.1212e-01,  2.7723e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-2.7367e-02,  9.2359e-04, -1.0823e-02,  ..., -2.4137e-01,\n",
      "           -5.6168e-02,  4.5574e-01],\n",
      "          [-8.5872e-01, -4.6492e-01, -6.2832e-02,  ..., -2.9358e+00,\n",
      "           -8.5419e-01, -7.8091e-02],\n",
      "          [ 9.0020e-01, -7.8887e-01,  9.8404e-02,  ..., -1.6219e+00,\n",
      "           -8.1409e-01, -1.2041e-02],\n",
      "          ...,\n",
      "          [-2.5161e+00, -2.3188e+00,  2.4018e+00,  ..., -2.2388e+00,\n",
      "           -1.2025e+00, -7.5111e-01],\n",
      "          [-4.3164e+00, -1.0060e+00,  2.8289e+00,  ..., -1.6091e+00,\n",
      "           -1.4525e+00, -8.0596e-01],\n",
      "          [-1.8050e+00,  3.0609e-01,  1.8189e+00,  ..., -2.2791e+00,\n",
      "           -1.3121e+00, -2.3531e-01]],\n",
      "\n",
      "         [[-1.1507e-02, -6.1662e-05,  3.5832e-03,  ...,  7.7159e-02,\n",
      "           -8.2763e-01,  1.4691e-01],\n",
      "          [ 5.4236e-01,  9.4482e-01, -2.0475e-01,  ..., -1.1988e+00,\n",
      "           -2.7831e-02,  1.6037e+00],\n",
      "          [-7.6565e-01,  1.6221e+00, -1.2385e+00,  ..., -1.1291e-04,\n",
      "            7.4488e-01, -1.0868e+00],\n",
      "          ...,\n",
      "          [ 1.9032e+00,  2.7845e+00, -6.3665e-01,  ..., -9.2353e-01,\n",
      "            1.5417e+00, -5.3649e-01],\n",
      "          [ 3.9595e+00,  2.3596e+00,  2.0239e-01,  ..., -3.8874e-01,\n",
      "            1.3778e+00, -2.9654e-01],\n",
      "          [ 2.3558e+00,  6.4933e-01,  4.8960e-01,  ..., -5.3722e-01,\n",
      "            1.4056e+00,  2.6186e-01]],\n",
      "\n",
      "         [[-8.3351e-03,  6.3418e-03, -3.1162e-02,  ..., -5.2716e-02,\n",
      "           -3.1088e-02, -6.9405e-02],\n",
      "          [-2.0099e+00,  2.0133e+00,  7.7213e-01,  ..., -2.2977e-01,\n",
      "            9.0161e-01, -1.8316e+00],\n",
      "          [-2.4194e+00,  1.0103e+00,  1.1287e+00,  ..., -2.9434e-02,\n",
      "            4.1864e-01, -2.4221e-01],\n",
      "          ...,\n",
      "          [ 3.2710e+00,  3.2830e+00,  1.6194e+00,  ...,  3.1131e-01,\n",
      "           -3.7598e-01,  3.7199e-01],\n",
      "          [-2.7457e+00,  1.6345e+00, -2.2637e-02,  ..., -1.1079e-01,\n",
      "            1.9877e-01, -1.3922e-01],\n",
      "          [-5.0614e+00, -8.7910e-01, -3.4389e-01,  ...,  5.9516e-01,\n",
      "           -6.9091e-01, -9.6909e-02]],\n",
      "\n",
      "         [[-7.8842e-03, -2.4509e-02,  5.3377e-03,  ..., -4.6200e-01,\n",
      "            3.3985e-01,  2.1361e-01],\n",
      "          [ 2.8422e-02,  8.0471e-01,  5.1370e-01,  ...,  4.1843e-01,\n",
      "            4.6484e-01, -8.3901e-02],\n",
      "          [-7.2090e-01,  4.0450e-01,  1.3262e+00,  ...,  3.4667e-01,\n",
      "           -4.3333e-01,  1.5905e+00],\n",
      "          ...,\n",
      "          [ 5.3284e+00,  8.0814e-01,  1.5882e+00,  ...,  1.8161e+00,\n",
      "            6.1517e-01,  6.3684e-01],\n",
      "          [ 4.1857e+00, -7.9267e-01, -2.5929e-01,  ...,  2.1871e+00,\n",
      "            9.9035e-01,  4.6206e-01],\n",
      "          [-5.9027e-01, -1.9607e+00, -1.5363e+00,  ...,  1.8742e+00,\n",
      "            7.1552e-01,  5.0206e-01]]]], grad_fn=<AddBackward0>), tensor([[[[ 4.0290e-03,  4.5730e-04, -1.0895e-03,  ...,  1.3968e-03,\n",
      "           -2.0639e-03,  6.9478e-03],\n",
      "          [ 2.7724e-01, -6.8194e-02, -4.3784e-01,  ...,  8.3264e-02,\n",
      "            3.8486e-01, -1.1167e-01],\n",
      "          [ 2.3146e-01,  1.2643e-01, -2.8778e-02,  ...,  2.2219e-01,\n",
      "            1.0866e-01, -3.0138e-02],\n",
      "          ...,\n",
      "          [ 6.2828e-03, -1.2806e-02, -1.5280e-01,  ..., -1.0585e-01,\n",
      "            1.4337e-01, -2.5678e-03],\n",
      "          [ 6.9439e-03,  3.1911e-02, -1.6314e-01,  ..., -1.3182e-01,\n",
      "            8.0492e-02, -9.5485e-02],\n",
      "          [-4.6036e-02,  5.0394e-03, -1.6989e-01,  ..., -3.4323e-02,\n",
      "            1.2221e-01, -1.1622e-02]],\n",
      "\n",
      "         [[-3.0558e-03,  1.3350e-02, -6.0896e-03,  ..., -1.0797e-02,\n",
      "           -3.2567e-03, -7.1497e-04],\n",
      "          [ 1.1291e-01, -1.9282e-02,  6.6240e-02,  ...,  2.1295e-01,\n",
      "            1.1800e-01, -1.8793e-01],\n",
      "          [ 1.5018e-01,  7.8190e-02, -1.5167e-01,  ...,  5.5272e-02,\n",
      "           -5.0505e-02,  1.9520e-02],\n",
      "          ...,\n",
      "          [ 1.0252e-01,  8.9763e-02,  8.8341e-02,  ...,  6.3732e-02,\n",
      "           -2.2483e-02,  3.2720e-02],\n",
      "          [ 1.2290e-01,  6.3900e-02,  4.6146e-02,  ...,  1.2540e-01,\n",
      "           -5.2219e-02, -6.5135e-02],\n",
      "          [ 1.3191e-01,  1.4068e-01,  9.2041e-02,  ...,  6.6631e-02,\n",
      "            5.8564e-02,  1.6558e-02]],\n",
      "\n",
      "         [[-4.0697e-04,  2.4603e-03,  2.5439e-04,  ..., -5.1456e-04,\n",
      "            2.7173e-03,  5.6904e-02],\n",
      "          [-1.7111e-01, -2.9551e-01,  2.5209e-02,  ..., -1.4960e-02,\n",
      "            4.5370e-02, -1.5726e-01],\n",
      "          [-3.2557e-01,  6.9135e-02, -1.3148e-01,  ...,  2.0503e-02,\n",
      "           -4.1294e-04,  3.3721e-02],\n",
      "          ...,\n",
      "          [-1.5274e-01, -1.1675e-01, -2.0675e-01,  ...,  1.6576e-01,\n",
      "            4.3967e-03, -4.0586e-01],\n",
      "          [-2.0321e-01, -1.3950e-01, -2.3734e-01,  ...,  2.1785e-01,\n",
      "           -6.9002e-02, -4.3876e-01],\n",
      "          [-1.4641e-01, -9.1230e-02, -8.3741e-02,  ...,  1.9543e-01,\n",
      "            7.4762e-02, -3.5349e-01]],\n",
      "\n",
      "         [[-6.8225e-04,  5.7466e-03,  5.0373e-03,  ...,  2.2938e-03,\n",
      "           -5.3986e-03,  5.5351e-03],\n",
      "          [ 3.8582e-03,  2.4725e-01, -1.5076e-01,  ...,  1.2135e-03,\n",
      "           -2.4294e-01,  2.4399e-01],\n",
      "          [ 2.4947e-01,  3.2365e-01, -1.3360e-01,  ..., -7.8526e-02,\n",
      "           -5.0452e-01,  1.4473e-01],\n",
      "          ...,\n",
      "          [-3.0490e-02, -1.3194e-01, -1.5889e-01,  ...,  6.4644e-02,\n",
      "            1.9367e-01,  1.6094e-01],\n",
      "          [-3.0135e-02, -6.3990e-02,  1.5954e-02,  ..., -8.0542e-02,\n",
      "            2.2030e-01,  1.1900e-01],\n",
      "          [-1.0932e-01, -7.2075e-03, -2.3245e-02,  ...,  4.4208e-02,\n",
      "            1.9635e-01,  7.2394e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.5932e-02, -2.6673e-02,  1.6353e-02,  ...,  9.0364e-01,\n",
      "           -1.4951e+00, -7.4898e-01],\n",
      "          [-1.8051e+00, -9.0580e-01,  4.8907e-01,  ..., -1.1792e+00,\n",
      "           -3.4074e-01,  1.0938e+00],\n",
      "          [-1.5807e+00, -7.2112e-01, -9.4651e-01,  ..., -4.6964e-01,\n",
      "           -7.1771e-01,  6.3545e-01],\n",
      "          ...,\n",
      "          [ 3.1754e+00, -2.6950e+00, -2.2495e+00,  ..., -1.7864e+00,\n",
      "            1.5721e+00,  8.3708e-01],\n",
      "          [-1.6138e+00, -1.1028e+00, -1.6418e+00,  ..., -1.7569e+00,\n",
      "            1.7012e+00,  7.2090e-01],\n",
      "          [-3.9383e+00,  1.2290e+00, -5.9275e-01,  ..., -1.4041e+00,\n",
      "            2.2965e+00,  2.2949e-01]],\n",
      "\n",
      "         [[-2.6257e-02,  2.4369e-02, -7.3768e-03,  ...,  3.4298e-01,\n",
      "           -1.7374e+00,  1.9473e-01],\n",
      "          [-7.8469e-01,  4.7542e-02,  1.4006e+00,  ...,  4.4883e-01,\n",
      "            1.3780e+00,  1.7208e-01],\n",
      "          [ 1.5636e+00,  7.4129e-01,  1.0264e+00,  ..., -1.8586e+00,\n",
      "            2.4705e+00, -5.7214e-01],\n",
      "          ...,\n",
      "          [-5.5368e+00,  7.5900e-01, -2.2907e+00,  ..., -7.6936e-01,\n",
      "            4.8578e+00, -1.3768e+00],\n",
      "          [-6.9764e+00, -6.6349e-01, -3.0889e+00,  ..., -7.7862e-01,\n",
      "            4.9391e+00, -6.0542e-01],\n",
      "          [-1.8286e+00, -1.8941e+00, -2.6823e+00,  ..., -4.9098e-01,\n",
      "            4.2962e+00, -1.4754e+00]],\n",
      "\n",
      "         [[ 8.5054e-03, -3.3030e-03, -1.8419e-02,  ...,  5.9614e-01,\n",
      "           -3.6321e-01,  5.7168e-01],\n",
      "          [ 6.0819e-01,  2.4556e-01,  2.6143e-01,  ...,  8.6921e-01,\n",
      "            2.6251e+00, -8.6773e-01],\n",
      "          [ 2.9706e-01, -1.0333e+00,  5.8583e-01,  ..., -1.8528e-01,\n",
      "            1.9299e+00, -1.2512e+00],\n",
      "          ...,\n",
      "          [ 1.9359e+00, -8.9954e-01, -2.5351e-01,  ..., -2.4076e+00,\n",
      "            3.3401e+00, -3.0514e+00],\n",
      "          [ 4.3039e+00, -2.9395e+00, -2.1305e+00,  ..., -2.9069e+00,\n",
      "            3.4397e+00, -2.6044e+00],\n",
      "          [ 2.5934e+00, -3.1273e+00, -2.3777e+00,  ..., -2.6557e+00,\n",
      "            3.2640e+00, -2.9718e+00]],\n",
      "\n",
      "         [[ 1.8120e-03, -3.9221e-03, -1.1759e-03,  ...,  5.7375e-01,\n",
      "            2.9918e-01, -7.6861e-02],\n",
      "          [-3.4861e-01,  1.6960e-01,  1.0809e+00,  ...,  1.9942e+00,\n",
      "            6.9754e-01, -6.5097e-01],\n",
      "          [ 4.3298e-01,  1.4567e+00,  8.5923e-01,  ...,  1.4289e+00,\n",
      "           -6.3730e-02, -1.2984e+00],\n",
      "          ...,\n",
      "          [-1.8090e+00,  2.8629e+00, -1.6667e+00,  ...,  2.4165e+00,\n",
      "           -2.8671e+00,  7.6433e-01],\n",
      "          [-3.1730e+00,  2.5983e+00, -2.4701e+00,  ...,  2.1137e+00,\n",
      "           -2.4200e+00,  8.9003e-01],\n",
      "          [-1.3952e+00,  1.2488e+00, -2.1718e+00,  ...,  2.7220e+00,\n",
      "           -3.0546e+00,  2.6110e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-0.0018, -0.0061,  0.0044,  ..., -0.0072, -0.0023,  0.0073],\n",
      "          [ 0.0343,  0.1252, -0.4863,  ...,  0.1079, -0.2292, -0.0866],\n",
      "          [ 0.2199, -0.1500,  0.2063,  ...,  0.2198, -0.1411,  0.2195],\n",
      "          ...,\n",
      "          [ 0.1657,  0.0319,  0.0688,  ..., -0.1965, -0.0379, -0.0940],\n",
      "          [ 0.1709, -0.0192,  0.0794,  ..., -0.2502, -0.0171, -0.1521],\n",
      "          [ 0.1664, -0.0186,  0.0500,  ..., -0.2230, -0.0256, -0.0793]],\n",
      "\n",
      "         [[-0.0047, -0.0013,  0.0014,  ..., -0.0065, -0.0071, -0.0023],\n",
      "          [ 0.0471, -0.0254,  0.0105,  ..., -0.0862,  0.0039, -0.0628],\n",
      "          [-0.2972,  0.1927,  0.0985,  ..., -0.1282,  0.2612, -0.2196],\n",
      "          ...,\n",
      "          [ 0.2361, -0.1945, -0.1691,  ...,  0.1482,  0.0939, -0.0064],\n",
      "          [ 0.1989, -0.1383, -0.0519,  ...,  0.1795,  0.0841, -0.0232],\n",
      "          [ 0.1719, -0.2101, -0.0601,  ...,  0.2376,  0.0961,  0.0390]],\n",
      "\n",
      "         [[-0.0098,  0.0102, -0.0063,  ..., -0.0021,  0.0046, -0.0050],\n",
      "          [ 0.0855,  0.2178,  0.2472,  ..., -0.1339,  0.1728, -0.3170],\n",
      "          [-0.0971,  0.0415,  0.2202,  ..., -0.0993,  0.0237, -0.1439],\n",
      "          ...,\n",
      "          [-0.0062,  0.0442, -0.0021,  ..., -0.0300, -0.1069, -0.0234],\n",
      "          [ 0.0813,  0.0326, -0.0112,  ..., -0.0349, -0.0972,  0.0585],\n",
      "          [ 0.0290,  0.0420, -0.0719,  ...,  0.0589, -0.0804,  0.0442]],\n",
      "\n",
      "         [[-0.0021, -0.0043,  0.0008,  ...,  0.0103, -0.0062,  0.0016],\n",
      "          [ 0.1252, -0.0262,  0.1859,  ...,  0.1236, -0.1522,  0.0414],\n",
      "          [ 0.0812, -0.2303,  0.0439,  ...,  0.0222,  0.0803,  0.1370],\n",
      "          ...,\n",
      "          [-0.0940,  0.0269,  0.0146,  ..., -0.0963,  0.0591,  0.1511],\n",
      "          [-0.1648,  0.0787, -0.0423,  ..., -0.0123,  0.1225,  0.1355],\n",
      "          [-0.0995,  0.0414, -0.0967,  ..., -0.0251,  0.0230,  0.1143]]]],\n",
      "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.7972e-03,  8.9892e-03,  1.9108e-02,  ..., -3.4788e-01,\n",
      "           -4.0792e-01,  1.5716e-01],\n",
      "          [ 6.7184e-02,  4.2607e-02,  1.4182e-01,  ...,  1.3206e+00,\n",
      "            1.2158e+00, -2.2641e+00],\n",
      "          [ 3.6912e-01,  2.2427e-01, -5.6911e-01,  ...,  2.4467e+00,\n",
      "            1.6896e+00, -8.1772e-01],\n",
      "          ...,\n",
      "          [-1.0438e+00,  1.5841e+00, -1.9736e+00,  ..., -2.7828e-01,\n",
      "            3.7409e+00, -7.3825e-01],\n",
      "          [-1.7717e+00,  8.2889e-01, -1.2478e+00,  ..., -4.6287e-01,\n",
      "            3.4269e+00, -1.3926e+00],\n",
      "          [-9.8650e-01, -5.7637e-01, -2.8969e-01,  ..., -1.8474e-01,\n",
      "            3.0447e+00, -8.0462e-01]],\n",
      "\n",
      "         [[-1.5796e-02, -9.8251e-03,  1.5017e-02,  ..., -2.3967e-01,\n",
      "           -2.0837e+00,  1.8741e+00],\n",
      "          [-5.6856e-02, -3.9680e-01, -5.2908e-01,  ...,  1.2518e+00,\n",
      "            1.9756e+00, -5.6562e+00],\n",
      "          [-2.1510e-01,  4.5215e-01, -5.7099e-01,  ...,  7.3291e-01,\n",
      "            4.4124e+00, -5.7966e+00],\n",
      "          ...,\n",
      "          [ 1.3150e+00, -2.1487e-01, -4.4933e-01,  ...,  1.4834e+00,\n",
      "            5.6999e+00, -7.9509e+00],\n",
      "          [-6.5797e-01,  9.4279e-01,  2.8563e-01,  ...,  2.1022e+00,\n",
      "            5.2001e+00, -8.5761e+00],\n",
      "          [-1.6861e+00,  1.3186e+00,  7.9974e-01,  ...,  1.1643e+00,\n",
      "            5.0768e+00, -7.2745e+00]],\n",
      "\n",
      "         [[-7.2259e-03,  2.0220e-02, -9.3295e-03,  ..., -1.2444e-01,\n",
      "           -7.6500e-01, -3.2126e+00],\n",
      "          [ 2.5724e+00,  1.0440e+00,  3.1288e-01,  ..., -3.1857e-01,\n",
      "            1.9087e+00,  1.4461e+00],\n",
      "          [ 2.1806e+00,  1.7844e+00,  1.0386e+00,  ...,  2.3381e-02,\n",
      "            2.1712e+00,  2.5965e+00],\n",
      "          ...,\n",
      "          [-2.3263e+00,  2.8486e+00,  2.3787e+00,  ..., -4.2349e-01,\n",
      "            6.7383e-01,  3.8151e+00],\n",
      "          [ 2.2359e+00,  3.4648e+00,  1.7132e+00,  ..., -3.3055e-01,\n",
      "            8.4934e-01,  3.7828e+00],\n",
      "          [ 4.1480e+00,  2.0334e+00,  2.8562e-01,  ..., -2.9384e-03,\n",
      "            8.4647e-01,  2.9100e+00]],\n",
      "\n",
      "         [[ 1.7710e-02, -4.1676e-03,  4.3311e-02,  ...,  4.6303e-01,\n",
      "            1.7934e+00, -2.5265e-01],\n",
      "          [-1.1546e+00,  2.2978e-01, -9.0288e-01,  ...,  3.5784e-01,\n",
      "           -1.3783e+00,  8.5397e-02],\n",
      "          [-3.1098e+00, -8.4406e-01, -1.4681e+00,  ..., -2.5906e-01,\n",
      "           -2.7249e+00,  4.0086e-01],\n",
      "          ...,\n",
      "          [ 6.2184e+00, -1.2068e+00,  4.3373e-01,  ...,  1.1140e-01,\n",
      "           -5.8173e+00, -6.9551e-01],\n",
      "          [ 2.7179e+00, -2.7142e+00,  1.6472e+00,  ...,  2.1793e-01,\n",
      "           -5.9535e+00, -4.9793e-01],\n",
      "          [-2.7226e+00, -2.7392e+00,  1.9775e+00,  ...,  6.1454e-02,\n",
      "           -4.8761e+00, -3.0506e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-6.3189e-03,  6.1498e-03,  5.2548e-04,  ..., -3.5917e-03,\n",
      "            4.7928e-03,  1.7975e-03],\n",
      "          [-4.4115e-01, -3.1694e-01, -6.6011e-02,  ..., -2.9605e-01,\n",
      "           -3.9548e-01,  2.6435e-01],\n",
      "          [ 5.7955e-02, -5.2414e-02, -2.7345e-01,  ..., -2.5663e-01,\n",
      "           -2.7885e-01,  1.0759e-01],\n",
      "          ...,\n",
      "          [ 1.3810e-01, -1.8400e-01,  2.0675e-02,  ..., -8.2175e-02,\n",
      "           -5.8997e-03, -5.2252e-02],\n",
      "          [ 1.8170e-01, -2.7227e-01,  2.9896e-02,  ..., -2.1835e-02,\n",
      "           -1.1913e-01,  3.1971e-02],\n",
      "          [ 2.8377e-01, -1.4182e-01, -2.3593e-02,  ..., -6.0581e-02,\n",
      "           -2.2807e-02, -6.4259e-02]],\n",
      "\n",
      "         [[ 4.6890e-03,  1.7528e-02,  3.3132e-03,  ..., -1.6531e-03,\n",
      "           -1.0768e-03,  3.6090e-04],\n",
      "          [ 9.9668e-02, -3.3083e-01,  1.8326e-02,  ..., -1.6748e-01,\n",
      "           -1.8358e-01,  3.0463e-02],\n",
      "          [ 1.9512e-02, -3.7148e-01,  4.0182e-01,  ..., -2.6289e-01,\n",
      "            2.4417e-01,  2.6415e-01],\n",
      "          ...,\n",
      "          [-1.2665e-02, -2.0765e-01, -9.5585e-02,  ..., -7.7500e-03,\n",
      "            1.4841e-03,  5.6144e-02],\n",
      "          [ 5.9115e-03, -2.0472e-01, -4.3425e-02,  ..., -7.1936e-03,\n",
      "            3.0226e-02,  6.8236e-02],\n",
      "          [-2.1218e-02, -1.7353e-01, -5.6638e-02,  ...,  7.6468e-02,\n",
      "           -7.5913e-03,  1.2126e-01]],\n",
      "\n",
      "         [[ 1.6793e-03, -9.9028e-04,  5.0992e-03,  ...,  2.7365e-03,\n",
      "            1.7185e-03,  1.1659e-03],\n",
      "          [-9.9815e-02,  1.5584e-01,  2.9656e-02,  ...,  1.0069e-01,\n",
      "           -3.6914e-02,  9.4206e-02],\n",
      "          [-1.5959e-01, -1.3991e-01,  1.5854e-01,  ..., -1.1624e-01,\n",
      "            1.0726e-01, -1.3811e-01],\n",
      "          ...,\n",
      "          [ 1.2694e-02, -1.2591e-01,  5.7752e-02,  ..., -1.9721e-01,\n",
      "           -2.8159e-02, -3.7087e-02],\n",
      "          [-6.5788e-02, -6.7934e-02,  3.5934e-02,  ..., -1.9100e-01,\n",
      "           -1.2494e-01, -5.5157e-02],\n",
      "          [ 2.4414e-02, -1.3035e-01,  5.9464e-02,  ..., -2.1526e-01,\n",
      "           -4.1859e-02, -1.2689e-01]],\n",
      "\n",
      "         [[ 6.4660e-03, -2.4541e-03, -3.8210e-03,  ...,  1.3047e-02,\n",
      "            2.6163e-03,  3.7800e-03],\n",
      "          [ 2.9099e-01,  6.2637e-01,  4.7181e-01,  ..., -3.7230e-02,\n",
      "           -7.6282e-02, -6.6091e-02],\n",
      "          [-5.4360e-02, -4.6255e-02, -1.2352e-01,  ...,  1.7559e-01,\n",
      "           -8.7376e-04, -1.2821e-02],\n",
      "          ...,\n",
      "          [-1.1876e-01, -8.8826e-02,  7.2619e-02,  ...,  2.2176e-01,\n",
      "            6.5287e-02, -2.5488e-02],\n",
      "          [-1.6793e-01, -1.1767e-01,  6.1810e-02,  ...,  2.0496e-01,\n",
      "           -6.9114e-03, -3.2373e-02],\n",
      "          [-1.8078e-01, -5.8370e-02,  6.3682e-03,  ...,  2.4541e-01,\n",
      "           -9.5319e-03,  2.0382e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.8318e-02,  1.1074e-02,  1.4122e-02,  ...,  1.6686e+00,\n",
      "            1.7064e+00,  2.5543e-01],\n",
      "          [-1.3350e+00,  6.8861e-01, -2.1663e-01,  ..., -4.9991e-01,\n",
      "           -2.7317e-01,  8.1411e-01],\n",
      "          [-2.1338e+00,  1.3853e+00,  4.2366e-01,  ..., -4.9870e-01,\n",
      "            7.0529e-01,  2.2160e-02],\n",
      "          ...,\n",
      "          [ 3.4060e+00,  7.0506e-01,  1.0980e+00,  ..., -6.1994e-01,\n",
      "           -5.9298e-01,  1.7498e+00],\n",
      "          [-8.6045e-01,  1.7653e+00,  2.6730e-01,  ..., -6.3761e-01,\n",
      "           -6.5105e-01,  1.7556e+00],\n",
      "          [-3.5858e+00,  1.7641e+00, -9.5706e-01,  ..., -5.9717e-01,\n",
      "           -9.1016e-01,  1.5953e+00]],\n",
      "\n",
      "         [[ 1.7154e-03,  1.9425e-02,  1.2534e-02,  ..., -7.0740e-01,\n",
      "            2.0464e+00,  1.7065e+00],\n",
      "          [-4.4993e-01,  3.0353e-01,  6.2170e-01,  ...,  9.3218e-01,\n",
      "            7.4120e-01, -5.1411e-01],\n",
      "          [-2.1246e-01,  3.1923e-01,  1.0469e+00,  ..., -6.8868e-01,\n",
      "            2.1031e-01, -1.9004e+00],\n",
      "          ...,\n",
      "          [-2.3980e-01,  2.6523e+00, -1.2444e+00,  ..., -1.9539e+00,\n",
      "           -1.8259e+00,  3.5607e-01],\n",
      "          [-3.6561e+00,  1.3849e+00, -1.8683e+00,  ..., -1.9350e+00,\n",
      "           -1.8595e+00,  7.6298e-01],\n",
      "          [-3.4127e+00, -1.1081e+00, -1.6820e+00,  ..., -1.4168e+00,\n",
      "           -1.3665e+00,  5.0100e-01]],\n",
      "\n",
      "         [[ 3.0924e-03, -1.8991e-03,  5.1443e-03,  ..., -3.9493e-01,\n",
      "            2.2166e-01, -1.0681e+00],\n",
      "          [ 8.3091e-01,  1.6599e+00,  6.4810e-01,  ..., -8.5982e-01,\n",
      "           -7.1741e-01, -3.2817e-01],\n",
      "          [ 1.8986e+00, -1.5887e+00,  6.3213e-01,  ..., -1.0862e+00,\n",
      "            7.3372e-01,  2.3416e+00],\n",
      "          ...,\n",
      "          [-2.8488e+00,  3.3552e-01, -3.9778e+00,  ..., -2.4677e-02,\n",
      "            6.7127e-01,  4.6818e+00],\n",
      "          [ 1.9150e+00, -1.8503e+00, -3.7980e+00,  ...,  1.1190e-02,\n",
      "            1.2890e+00,  4.7563e+00],\n",
      "          [ 3.7762e+00, -2.7929e+00, -2.0691e+00,  ..., -4.5799e-01,\n",
      "            9.9551e-01,  3.6921e+00]],\n",
      "\n",
      "         [[-2.3281e-02, -2.3551e-03, -1.2342e-02,  ..., -1.9394e+00,\n",
      "            3.3575e-01,  1.6210e+00],\n",
      "          [ 4.2190e-01,  1.2593e-01, -5.7544e-02,  ...,  2.1009e+00,\n",
      "            1.6480e+00, -4.5760e-01],\n",
      "          [-8.8126e-01,  8.0993e-01, -2.3296e-01,  ...,  3.1413e+00,\n",
      "            1.7594e+00, -1.5189e+00],\n",
      "          ...,\n",
      "          [ 3.2691e+00,  2.5944e-01, -1.4463e+00,  ...,  4.4107e+00,\n",
      "           -7.8022e-03, -8.7087e-01],\n",
      "          [ 2.6679e+00,  1.9179e+00, -1.1839e+00,  ...,  4.7069e+00,\n",
      "            4.1590e-02, -7.3475e-01],\n",
      "          [-1.8460e-02,  2.5347e+00, -1.3948e-01,  ...,  3.4714e+00,\n",
      "            1.2374e-01, -5.3970e-01]]]], grad_fn=<AddBackward0>), tensor([[[[ 0.0052, -0.0021,  0.0008,  ...,  0.0011,  0.0067, -0.0019],\n",
      "          [ 0.1812,  0.2064,  0.0013,  ...,  0.1773, -0.4634, -0.1366],\n",
      "          [ 0.2317,  0.0739, -0.1206,  ...,  0.2544,  0.1300,  0.2495],\n",
      "          ...,\n",
      "          [ 0.0407,  0.1028, -0.3123,  ..., -0.2459,  0.0172, -0.1257],\n",
      "          [ 0.0101,  0.1144, -0.3269,  ..., -0.2586, -0.0836, -0.1003],\n",
      "          [ 0.0277,  0.0707, -0.2624,  ..., -0.2869, -0.1063, -0.0338]],\n",
      "\n",
      "         [[ 0.0072, -0.0062, -0.0074,  ...,  0.0092, -0.0126,  0.0042],\n",
      "          [-0.2006,  0.0547, -0.2866,  ..., -0.1008,  0.0766,  0.2906],\n",
      "          [-0.1107,  0.0304, -0.2037,  ...,  0.3401, -0.1323,  0.0952],\n",
      "          ...,\n",
      "          [ 0.0676,  0.0540,  0.3166,  ..., -0.1560,  0.1347,  0.0953],\n",
      "          [ 0.0918,  0.0546,  0.3185,  ..., -0.2257,  0.1251,  0.1356],\n",
      "          [ 0.0473,  0.0342,  0.3885,  ..., -0.1889,  0.1276,  0.1036]],\n",
      "\n",
      "         [[ 0.0021,  0.0087,  0.0062,  ...,  0.0034, -0.0028,  0.0074],\n",
      "          [ 0.1252, -0.2169, -0.1165,  ..., -0.2034, -0.1127, -0.1904],\n",
      "          [ 0.0769, -0.1323, -0.0744,  ..., -0.2733, -0.0366,  0.2797],\n",
      "          ...,\n",
      "          [ 0.1793,  0.0693,  0.2157,  ..., -0.0821,  0.0648,  0.1240],\n",
      "          [ 0.1421, -0.0446,  0.2002,  ..., -0.0434,  0.0834,  0.2262],\n",
      "          [ 0.1668,  0.1943,  0.2566,  ..., -0.1072,  0.0100,  0.1237]],\n",
      "\n",
      "         [[-0.0151,  0.0018,  0.0323,  ..., -0.0077, -0.0084, -0.0027],\n",
      "          [-0.0584,  0.4005, -0.3267,  ..., -0.3166,  0.0532, -0.1983],\n",
      "          [-0.3886,  0.3211, -0.0234,  ..., -0.1398, -0.1599,  0.0471],\n",
      "          ...,\n",
      "          [-0.0521,  0.1925,  0.0818,  ..., -0.4286, -0.1019, -0.1650],\n",
      "          [-0.0944,  0.2353,  0.0931,  ..., -0.4800, -0.1122, -0.3048],\n",
      "          [-0.2158,  0.3136,  0.1779,  ..., -0.3604, -0.1433, -0.2291]]]],\n",
      "       grad_fn=<TransposeBackward0>)), (tensor([[[[-1.9406e-02,  2.0666e-02, -3.4164e-03,  ...,  1.4495e-01,\n",
      "            4.6577e-01,  2.1639e-01],\n",
      "          [ 1.4075e+00, -6.0827e-01,  1.2684e+00,  ..., -5.2311e-02,\n",
      "            3.2874e-01,  9.1647e-01],\n",
      "          [-2.6341e+00, -3.1657e+00, -2.3846e-01,  ...,  4.8388e-01,\n",
      "           -1.2378e-01,  7.5187e-01],\n",
      "          ...,\n",
      "          [ 6.7516e+00, -8.6897e-01, -5.5630e+00,  ...,  7.7296e-01,\n",
      "            6.0702e-01,  4.8451e-01],\n",
      "          [ 7.1356e+00, -2.8668e+00, -5.0067e+00,  ...,  8.9317e-01,\n",
      "            5.1484e-01,  4.0558e-01],\n",
      "          [ 1.2319e+00, -3.0148e+00, -2.3416e+00,  ...,  6.6849e-01,\n",
      "            3.1909e-01,  1.5130e-01]],\n",
      "\n",
      "         [[ 2.1955e-03, -1.2801e-02, -3.5818e-03,  ..., -1.4672e+00,\n",
      "           -7.0420e-01, -5.8759e-01],\n",
      "          [-4.1888e-02,  1.6004e-03,  8.7981e-01,  ...,  9.6875e-01,\n",
      "           -7.8546e-02,  3.4364e-01],\n",
      "          [-4.1573e-01,  6.8337e-01,  1.3845e+00,  ...,  8.3460e-01,\n",
      "            4.2889e-01, -8.2769e-01],\n",
      "          ...,\n",
      "          [ 5.1137e-01,  2.7798e+00, -1.0760e+00,  ..., -1.3919e+00,\n",
      "            3.6881e+00, -4.8018e-01],\n",
      "          [ 2.1618e+00,  2.1972e+00, -1.8375e+00,  ..., -1.5201e+00,\n",
      "            3.7620e+00,  7.8442e-02],\n",
      "          [ 1.7362e+00,  4.8667e-01, -1.8570e+00,  ..., -1.1130e+00,\n",
      "            3.0102e+00, -1.8146e-01]],\n",
      "\n",
      "         [[ 5.7060e-03,  2.3837e-02,  2.7784e-02,  ..., -4.5725e-01,\n",
      "            1.9227e+00, -1.3755e+00],\n",
      "          [-7.2023e-01, -8.9402e-01,  5.6908e-01,  ...,  1.6251e+00,\n",
      "            3.5019e-02,  1.5706e-01],\n",
      "          [-6.1358e-01, -9.6106e-01, -1.2375e+00,  ...,  1.5857e+00,\n",
      "           -6.4861e-01,  3.1490e-01],\n",
      "          ...,\n",
      "          [ 3.4997e+00, -2.4321e+00, -6.1686e-01,  ...,  3.0960e+00,\n",
      "            1.3724e+00,  2.6319e+00],\n",
      "          [ 6.6676e-01, -1.4248e+00,  4.0885e-01,  ...,  3.2861e+00,\n",
      "            1.2908e+00,  2.6795e+00],\n",
      "          [-2.4566e+00, -3.3753e-02,  9.1838e-01,  ...,  3.1971e+00,\n",
      "            1.3779e+00,  1.7850e+00]],\n",
      "\n",
      "         [[-1.1314e-02,  9.0625e-03, -1.2549e-02,  ...,  1.8762e-01,\n",
      "           -1.7673e+00, -1.6688e+00],\n",
      "          [-2.1124e-02, -7.9493e-02, -1.2448e-01,  ...,  3.3330e+00,\n",
      "            1.5484e+00,  2.8723e-01],\n",
      "          [-2.7578e-01, -6.6915e-01,  7.1047e-02,  ...,  2.6386e+00,\n",
      "            9.5316e-01,  4.5081e-01],\n",
      "          ...,\n",
      "          [ 1.1932e+00, -7.6025e-01, -1.6402e+00,  ...,  4.1918e-02,\n",
      "            1.6035e+00, -9.2746e-01],\n",
      "          [ 2.1860e+00, -1.8211e+00, -2.2832e+00,  ..., -9.4580e-01,\n",
      "            1.6385e+00, -1.5013e+00],\n",
      "          [ 6.5218e-01, -1.5283e+00, -1.8390e+00,  ...,  1.3726e-01,\n",
      "            1.6877e+00, -1.1036e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-8.0284e-03, -4.5055e-02, -8.3293e-03,  ..., -1.3266e-01,\n",
      "            2.0353e-03,  2.1049e-03],\n",
      "          [ 1.5011e-01,  4.7904e-02,  7.6847e-02,  ...,  1.6465e-01,\n",
      "           -1.5743e-01, -7.6507e-02],\n",
      "          [ 4.4051e-02, -1.1532e-01, -2.5750e-02,  ..., -2.5593e-03,\n",
      "            3.9629e-01,  1.4421e-02],\n",
      "          ...,\n",
      "          [-1.2018e-01,  7.8258e-01,  2.5405e-01,  ..., -8.3572e-02,\n",
      "           -3.3827e-01,  1.3938e-01],\n",
      "          [-1.2055e-01,  7.6425e-01,  2.3212e-01,  ..., -2.1152e-01,\n",
      "           -3.5037e-01,  1.9582e-01],\n",
      "          [-2.0907e-01,  5.1756e-01,  2.2860e-01,  ..., -1.4396e-01,\n",
      "           -3.4998e-01,  1.7178e-01]],\n",
      "\n",
      "         [[ 2.5838e-03,  4.5711e-03,  1.6453e-02,  ...,  1.2609e-03,\n",
      "            2.3867e-03,  1.5003e-02],\n",
      "          [ 1.2243e-01, -1.0539e-01, -5.1686e-01,  ..., -7.3911e-02,\n",
      "            1.4669e-01,  4.5437e-01],\n",
      "          [ 1.9962e-01,  4.8074e-02, -2.1049e-01,  ...,  1.1889e-01,\n",
      "           -1.3509e-02,  2.8816e-01],\n",
      "          ...,\n",
      "          [-2.4573e-01,  1.3542e-01,  1.3924e-01,  ...,  2.5999e-02,\n",
      "            2.2318e-01,  5.7375e-02],\n",
      "          [-7.4390e-02,  1.9672e-01,  1.0583e-01,  ..., -1.2988e-01,\n",
      "            3.6977e-01,  2.7757e-02],\n",
      "          [-2.2502e-01,  2.1836e-01,  8.7835e-02,  ..., -1.3978e-01,\n",
      "            2.3056e-01,  3.3624e-02]],\n",
      "\n",
      "         [[ 8.8864e-04, -1.0037e-02,  1.0756e-02,  ...,  3.0364e-03,\n",
      "            1.2832e-02, -2.7831e-03],\n",
      "          [-2.6344e-02,  1.1770e-01, -2.0381e-01,  ...,  1.1371e-01,\n",
      "            7.2533e-03, -2.7242e-02],\n",
      "          [-4.2450e-02,  1.2117e-02,  8.2339e-02,  ..., -2.1146e-01,\n",
      "            1.4917e-01,  1.6386e-01],\n",
      "          ...,\n",
      "          [-5.6249e-02, -5.6423e-02,  1.9763e-01,  ..., -1.0276e-01,\n",
      "            2.4201e-01,  6.8588e-03],\n",
      "          [-9.1707e-02, -7.6114e-02,  1.1634e-01,  ..., -1.1645e-01,\n",
      "            1.4572e-01,  3.0762e-02],\n",
      "          [-3.1201e-02, -2.1721e-02,  7.8535e-02,  ..., -3.5984e-02,\n",
      "            2.9527e-01, -8.8552e-02]],\n",
      "\n",
      "         [[-2.9809e-03,  1.4665e-03, -1.2274e-03,  ...,  1.5584e-05,\n",
      "            5.3056e-03,  7.5231e-03],\n",
      "          [-2.3521e-01,  1.5309e-01,  1.5830e-01,  ...,  1.2805e-01,\n",
      "            1.3717e-01,  3.1731e-02],\n",
      "          [-6.0597e-02,  8.0695e-02,  1.5486e-01,  ...,  3.4788e-02,\n",
      "            1.0874e-01, -1.4243e-01],\n",
      "          ...,\n",
      "          [-1.4556e-01, -4.0957e-01, -7.8471e-02,  ..., -9.7430e-03,\n",
      "            9.7375e-02, -3.9576e-01],\n",
      "          [-1.8400e-01, -3.4791e-01, -4.0213e-02,  ..., -2.0295e-02,\n",
      "           -3.7176e-02, -3.6434e-01],\n",
      "          [-2.4143e-01, -3.7589e-01, -1.1352e-02,  ..., -4.7206e-02,\n",
      "            6.5894e-02, -3.8668e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.4389e-02, -4.8216e-03,  1.0411e-02,  ..., -1.7991e-02,\n",
      "           -1.7300e+00, -1.4130e+00],\n",
      "          [ 4.4704e-01, -1.3242e+00, -1.3328e-01,  ..., -9.9507e-01,\n",
      "            1.0637e-01, -1.1175e+00],\n",
      "          [ 1.1522e+00,  7.3519e-01, -3.9656e-01,  ..., -1.8449e+00,\n",
      "           -1.8603e-02, -9.0072e-01],\n",
      "          ...,\n",
      "          [-2.2110e+00, -9.8414e-01, -2.3900e+00,  ..., -4.6712e+00,\n",
      "           -1.7448e+00, -4.8023e-01],\n",
      "          [ 4.0435e-01,  1.7366e+00, -1.4992e+00,  ..., -5.0467e+00,\n",
      "           -8.3019e-01, -6.9940e-01],\n",
      "          [ 1.9635e+00,  2.9423e+00,  2.5028e-01,  ..., -4.4255e+00,\n",
      "           -7.4058e-01, -1.2232e+00]],\n",
      "\n",
      "         [[ 8.5994e-03,  3.6883e-03, -2.6468e-03,  ..., -3.5220e-01,\n",
      "           -1.4124e+00,  3.0381e+00],\n",
      "          [ 6.0543e-02,  2.0003e-01,  3.2977e-01,  ..., -2.4113e-01,\n",
      "            1.3480e+00, -2.0612e+00],\n",
      "          [ 2.8903e-01,  6.3372e-01,  7.2870e-01,  ...,  9.5837e-01,\n",
      "            2.6902e+00, -3.1900e+00],\n",
      "          ...,\n",
      "          [-1.7902e+00,  1.7296e+00,  6.4478e-01,  ..., -3.1142e-01,\n",
      "            1.5611e+00, -1.7815e+00],\n",
      "          [-2.0390e+00,  2.0510e+00, -5.0412e-01,  ...,  5.3946e-01,\n",
      "            8.5593e-01, -1.0969e+00],\n",
      "          [-5.9575e-01,  1.5045e+00, -1.3724e+00,  ...,  3.6770e-02,\n",
      "            1.1807e+00, -1.8801e+00]],\n",
      "\n",
      "         [[-1.2891e-02,  3.7967e-03,  7.9196e-03,  ..., -2.1225e-02,\n",
      "           -3.7610e-01, -7.4026e-01],\n",
      "          [ 1.6920e+00, -7.6522e-01, -6.2870e-01,  ...,  9.7433e-01,\n",
      "            3.9025e-01, -2.1483e-01],\n",
      "          [ 1.7981e+00, -1.5863e+00, -2.1254e+00,  ..., -3.6740e-01,\n",
      "            1.7129e-01,  6.9685e-01],\n",
      "          ...,\n",
      "          [-2.2208e+00, -3.4342e-01, -2.5947e+00,  ..., -2.9922e+00,\n",
      "           -9.3285e-01,  1.4966e-01],\n",
      "          [ 2.1531e+00, -1.9353e-01, -6.2522e-01,  ..., -2.9763e+00,\n",
      "           -9.4766e-01,  2.3062e-01],\n",
      "          [ 3.6338e+00, -9.5930e-01,  1.0873e+00,  ..., -2.3381e+00,\n",
      "           -9.7543e-01,  4.0764e-01]],\n",
      "\n",
      "         [[ 7.1290e-03,  9.6911e-03, -1.4273e-02,  ..., -7.5510e-01,\n",
      "            4.3844e-01, -3.5290e-02],\n",
      "          [-1.9463e+00,  1.4567e+00,  6.1913e-01,  ...,  8.6053e-01,\n",
      "           -1.4064e+00,  5.3089e-01],\n",
      "          [-4.4131e+00,  2.3808e+00,  4.9181e-01,  ...,  1.5542e+00,\n",
      "           -6.7488e-01,  1.7217e+00],\n",
      "          ...,\n",
      "          [ 5.8340e+00,  2.3518e+00, -4.7501e+00,  ...,  9.9049e-02,\n",
      "           -8.0826e-01,  1.1334e+00],\n",
      "          [ 7.6424e-01,  3.9704e+00, -4.6745e+00,  ...,  4.3346e-01,\n",
      "           -3.0402e-01,  1.4739e+00],\n",
      "          [-4.4338e+00,  3.1026e+00, -2.9618e+00,  ...,  1.1395e+00,\n",
      "           -7.6600e-01,  1.3522e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 0.0041, -0.0077, -0.0018,  ..., -0.0020,  0.0036, -0.0054],\n",
      "          [-0.1491, -0.1759, -0.1388,  ..., -0.1974, -0.1375, -0.1957],\n",
      "          [-0.1793,  0.1840, -0.1640,  ...,  0.2183, -0.0258, -0.1245],\n",
      "          ...,\n",
      "          [-0.1711,  0.2104, -0.0181,  ..., -0.1109,  0.0598,  0.0183],\n",
      "          [-0.1234,  0.1874, -0.0104,  ..., -0.1241, -0.0757, -0.1414],\n",
      "          [-0.1898,  0.1618,  0.0587,  ..., -0.1165,  0.0511, -0.0546]],\n",
      "\n",
      "         [[ 0.0026,  0.0026, -0.0059,  ..., -0.0098,  0.0008,  0.0015],\n",
      "          [ 0.0510,  0.0749, -0.4178,  ...,  0.0753, -0.1349, -0.0751],\n",
      "          [-0.2705,  0.1983, -0.1835,  ...,  0.0523,  0.3856, -0.2941],\n",
      "          ...,\n",
      "          [-0.2451, -0.1779,  0.1323,  ...,  0.1130, -0.1300,  0.0143],\n",
      "          [ 0.0561, -0.0369,  0.1865,  ...,  0.0043, -0.1605, -0.0528],\n",
      "          [ 0.0406,  0.0091,  0.0197,  ..., -0.1457, -0.2032, -0.0888]],\n",
      "\n",
      "         [[ 0.2184, -0.0290, -0.0268,  ..., -0.0079,  0.0122,  0.0039],\n",
      "          [-0.0125, -0.1380,  0.0984,  ...,  0.2224,  0.2166, -0.0821],\n",
      "          [ 0.0336, -0.2633,  0.1323,  ...,  0.0628,  0.3125,  0.1279],\n",
      "          ...,\n",
      "          [-0.0872, -0.1195, -0.0031,  ..., -0.1707, -0.4018,  0.3048],\n",
      "          [-0.0369, -0.0144,  0.0015,  ...,  0.0483, -0.3410,  0.3104],\n",
      "          [-0.0210, -0.0831,  0.0755,  ..., -0.0905, -0.3233,  0.2898]],\n",
      "\n",
      "         [[-0.0014,  0.0055, -0.0114,  ..., -0.0041,  0.0079,  0.0194],\n",
      "          [-0.2199,  0.2347,  0.0945,  ..., -0.0026,  0.0289, -0.2657],\n",
      "          [ 0.1304,  0.2213,  0.0883,  ...,  0.2096,  0.1123,  0.0749],\n",
      "          ...,\n",
      "          [-0.3376,  0.1283,  0.0110,  ...,  0.1823,  0.0095,  0.2130],\n",
      "          [-0.2843,  0.1254,  0.1490,  ...,  0.1411,  0.2365,  0.3025],\n",
      "          [-0.2426,  0.2260,  0.2179,  ...,  0.2284, -0.0052,  0.2922]]]],\n",
      "       grad_fn=<TransposeBackward0>)), (tensor([[[[-1.6882e-02,  8.6641e-03,  3.8325e-03,  ...,  2.5184e-01,\n",
      "            3.1251e-02,  5.1940e-01],\n",
      "          [ 5.3757e-01,  6.6686e-01,  2.6218e-01,  ..., -1.5658e+00,\n",
      "            1.8880e-01, -9.5804e-01],\n",
      "          [ 9.8201e-01,  2.3533e+00,  1.0918e+00,  ..., -2.5755e+00,\n",
      "           -3.2136e-01, -3.4367e-02],\n",
      "          ...,\n",
      "          [-3.0206e+00,  3.9833e+00,  5.3341e-01,  ..., -1.3484e+00,\n",
      "            3.9234e-01,  7.6434e-02],\n",
      "          [ 3.8866e-01,  2.7760e+00,  9.4302e-01,  ..., -2.0071e+00,\n",
      "            1.4542e-01,  5.1329e-01],\n",
      "          [ 2.4119e+00,  9.6786e-01,  6.9682e-01,  ..., -1.4053e+00,\n",
      "            4.5385e-01, -5.5186e-02]],\n",
      "\n",
      "         [[-5.6620e-03,  1.0222e-02, -5.8047e-03,  ...,  1.9074e-01,\n",
      "           -7.3173e-02,  1.0759e-01],\n",
      "          [-1.1119e-02,  1.5755e-01, -2.5465e-02,  ..., -3.8306e-01,\n",
      "            2.1843e+00, -1.9003e-01],\n",
      "          [ 1.4329e-01,  3.6865e-02, -6.0714e-01,  ...,  4.1117e-01,\n",
      "            2.0497e+00, -2.6751e+00],\n",
      "          ...,\n",
      "          [-9.3809e-01,  2.3238e-02, -2.3815e+00,  ..., -1.8131e-01,\n",
      "            1.9278e+00, -1.9096e+00],\n",
      "          [-3.3972e+00,  3.9501e-01, -3.1997e+00,  ...,  4.2485e-01,\n",
      "            1.2588e+00, -1.1334e+00],\n",
      "          [-2.2790e+00,  1.2023e+00, -2.6137e+00,  ...,  3.5926e-01,\n",
      "            1.1126e+00, -1.1642e+00]],\n",
      "\n",
      "         [[-1.1018e-03, -1.8401e-02, -1.0051e-02,  ...,  1.7199e-02,\n",
      "            8.3031e-01, -1.2891e+00],\n",
      "          [-1.6043e-01,  2.9187e-01, -2.0854e-01,  ..., -9.0599e-02,\n",
      "           -2.6524e+00,  3.0668e+00],\n",
      "          [-8.1800e-01, -1.6202e-01, -8.3191e-01,  ...,  8.3234e-01,\n",
      "           -4.0279e+00,  4.0560e+00],\n",
      "          ...,\n",
      "          [ 3.7266e-01, -2.9494e-01, -4.6708e-01,  ...,  3.7321e-01,\n",
      "           -1.9389e+00,  5.2110e+00],\n",
      "          [-7.7637e-01, -1.1657e+00,  3.6003e-01,  ..., -3.4646e-01,\n",
      "           -2.4747e+00,  1.9512e+00],\n",
      "          [-1.3667e+00, -1.3413e+00,  8.9707e-01,  ...,  1.1403e+00,\n",
      "           -2.3307e+00,  3.1350e+00]],\n",
      "\n",
      "         [[-5.6279e-03,  7.4370e-04,  1.9109e-02,  ...,  8.7968e-02,\n",
      "            6.4287e-01, -9.9071e-01],\n",
      "          [-5.7095e-01,  1.4726e-01, -6.8721e-01,  ..., -3.4129e-01,\n",
      "            1.7474e+00,  1.1328e+00],\n",
      "          [ 6.4338e-01, -1.5156e+00, -2.7415e-01,  ..., -1.5078e+00,\n",
      "           -2.7703e-01, -2.2625e+00],\n",
      "          ...,\n",
      "          [-3.5107e+00, -7.8944e-01,  1.1223e+00,  ..., -1.0942e+00,\n",
      "           -3.3561e+00, -2.6352e+00],\n",
      "          [-2.6191e+00, -2.4899e+00,  5.6598e-01,  ..., -4.3767e-01,\n",
      "           -3.2998e+00, -3.0779e+00],\n",
      "          [-8.4707e-01, -2.7898e+00, -6.2830e-01,  ..., -1.9148e-03,\n",
      "           -3.0316e+00, -1.5509e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 1.0958e-02,  1.5167e-02,  3.6830e-03,  ...,  4.8793e-03,\n",
      "           -7.4567e-03,  1.6907e-02],\n",
      "          [ 5.5163e-02, -3.7018e-01,  5.4901e-02,  ..., -1.2582e-01,\n",
      "           -1.3437e-01, -1.1078e-01],\n",
      "          [ 2.4362e-01,  1.9351e-02,  9.4825e-02,  ...,  7.4685e-02,\n",
      "           -7.0956e-02,  2.2497e-01],\n",
      "          ...,\n",
      "          [-2.0208e-01, -3.5565e-01,  2.7339e-01,  ...,  7.7953e-02,\n",
      "           -1.9296e-01,  5.4628e-01],\n",
      "          [ 3.5289e-02, -4.5780e-01,  3.7769e-01,  ...,  1.3629e-02,\n",
      "           -3.8192e-01,  2.9762e-01],\n",
      "          [-1.6414e-01, -4.2250e-01,  3.3342e-01,  ...,  6.0201e-02,\n",
      "           -1.6135e-01,  2.7446e-01]],\n",
      "\n",
      "         [[ 1.2633e-02,  1.6635e-03, -1.2526e-03,  ..., -4.8991e-03,\n",
      "           -3.1791e-03,  3.2508e-03],\n",
      "          [ 4.3457e-02, -2.3004e-02, -1.5826e-01,  ..., -1.3555e-01,\n",
      "            4.1254e-01,  7.3985e-02],\n",
      "          [-1.1912e-01, -3.7685e-01,  4.8406e-03,  ..., -2.6839e-01,\n",
      "            5.9865e-02, -8.3015e-02],\n",
      "          ...,\n",
      "          [ 3.1395e-01, -1.4637e-01,  1.2995e-01,  ...,  2.5694e-01,\n",
      "            1.7251e-01,  2.5268e-01],\n",
      "          [ 2.2023e-01, -1.1542e-01,  3.6430e-02,  ...,  1.8280e-01,\n",
      "            1.3067e-01,  3.0561e-01],\n",
      "          [ 1.8203e-01, -2.6611e-02,  4.5776e-02,  ...,  5.2241e-02,\n",
      "            5.3676e-02,  1.3580e-01]],\n",
      "\n",
      "         [[-1.3063e-03, -1.2905e-03,  1.5391e-03,  ...,  2.7983e-03,\n",
      "           -2.7140e-03,  1.1319e-03],\n",
      "          [-3.2465e-01,  2.9214e-02,  6.5880e-02,  ..., -5.2533e-02,\n",
      "           -1.2452e-01,  2.1628e-01],\n",
      "          [-1.0336e-01, -1.3712e-02,  9.1674e-02,  ..., -2.8343e-01,\n",
      "           -6.6129e-02,  1.7998e-02],\n",
      "          ...,\n",
      "          [ 1.5848e-01, -6.0975e-02,  2.6280e-01,  ..., -6.9981e-02,\n",
      "           -3.5917e-01, -1.9394e-02],\n",
      "          [ 1.2675e-01,  6.6432e-02,  3.9100e-01,  ..., -3.6260e-01,\n",
      "           -1.9949e-01, -4.2289e-02],\n",
      "          [ 6.8800e-02,  2.6107e-02,  4.8528e-01,  ..., -4.1591e-01,\n",
      "           -3.6354e-01, -8.1395e-02]],\n",
      "\n",
      "         [[ 4.5109e-03, -1.2154e-02, -6.9094e-03,  ..., -1.5882e-04,\n",
      "            6.5721e-03,  3.0040e-02],\n",
      "          [-1.7457e-01, -1.6461e-01, -3.7003e-01,  ...,  1.1932e-01,\n",
      "            4.5142e-01, -3.4291e-01],\n",
      "          [ 9.5924e-02,  1.6486e-01,  2.8528e-02,  ...,  1.0290e-01,\n",
      "            4.1894e-01, -2.3389e-01],\n",
      "          ...,\n",
      "          [ 2.1374e-01, -2.8714e-02,  1.4175e-01,  ...,  2.2721e-01,\n",
      "           -3.3540e-01,  2.6643e-01],\n",
      "          [ 1.3357e-01, -1.4928e-01,  5.5102e-02,  ...,  8.8788e-02,\n",
      "           -1.7984e-01,  1.3295e-01],\n",
      "          [ 2.2586e-01, -1.2311e-01,  5.5131e-02,  ...,  2.2157e-01,\n",
      "           -2.8649e-01,  1.1570e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.9041e-02, -2.1963e-03,  3.1788e-02,  ..., -3.9829e-01,\n",
      "            6.2742e-01,  2.1842e+00],\n",
      "          [-9.8458e-01, -1.0082e+00,  1.1099e+00,  ..., -3.0213e-01,\n",
      "            2.0999e-01, -1.9367e+00],\n",
      "          [-2.9314e+00, -2.6463e+00,  4.4245e-01,  ..., -2.1068e-01,\n",
      "            1.1811e+00, -3.0661e+00],\n",
      "          ...,\n",
      "          [ 3.7874e+00, -4.8384e+00, -1.7989e+00,  ...,  2.1173e-01,\n",
      "            1.3640e+00, -5.1484e+00],\n",
      "          [-3.1278e-01, -3.5173e+00, -3.3096e+00,  ...,  3.1397e-01,\n",
      "            1.6105e+00, -5.0891e+00],\n",
      "          [-4.1064e+00, -3.5246e-01, -3.2859e+00,  ...,  3.6683e-01,\n",
      "            6.0120e-01, -3.8279e+00]],\n",
      "\n",
      "         [[ 6.1270e-03,  3.6100e-02, -2.8169e-02,  ..., -2.1099e-02,\n",
      "           -6.2613e-01, -1.4030e+00],\n",
      "          [ 7.4468e-01, -4.2969e-01, -1.0789e+00,  ...,  1.9928e+00,\n",
      "           -1.3744e+00,  5.8978e-01],\n",
      "          [-7.2632e-01, -3.7428e-01, -1.2679e-01,  ...,  1.3252e+00,\n",
      "           -6.2342e-01,  6.0359e-02],\n",
      "          ...,\n",
      "          [ 2.5468e+00, -1.5982e+00,  3.3330e+00,  ...,  2.0157e+00,\n",
      "           -2.8187e-01,  7.0932e-01],\n",
      "          [ 1.5096e+00, -2.5026e+00,  2.2950e+00,  ...,  2.3635e+00,\n",
      "           -2.7030e+00,  1.1067e+00],\n",
      "          [ 7.5171e-01, -1.1428e+00,  2.2790e+00,  ...,  1.7741e+00,\n",
      "           -1.3215e+00,  1.0057e+00]],\n",
      "\n",
      "         [[ 1.2380e-02, -1.7374e-02,  3.9737e-02,  ...,  5.0050e-01,\n",
      "           -2.4470e-01, -6.2374e-01],\n",
      "          [ 1.7170e+00, -3.5760e-01,  1.4363e+00,  ..., -1.0527e+00,\n",
      "           -4.1525e-01, -2.4889e+00],\n",
      "          [ 5.3981e+00,  1.5252e+00,  1.2141e+00,  ..., -9.5331e-02,\n",
      "           -3.1159e-01, -3.1673e+00],\n",
      "          ...,\n",
      "          [-8.3246e+00,  9.4058e-02,  1.0762e+00,  ...,  1.6986e+00,\n",
      "           -1.5213e+00, -5.9774e+00],\n",
      "          [-2.9844e+00,  3.0010e+00, -1.6343e+00,  ...,  1.4254e+00,\n",
      "           -1.4522e+00, -5.8336e+00],\n",
      "          [ 4.1105e+00,  4.4310e+00, -3.8916e+00,  ...,  1.5892e+00,\n",
      "           -9.1096e-01, -5.6221e+00]],\n",
      "\n",
      "         [[ 1.3775e-03, -2.4264e-03,  1.0413e-02,  ...,  4.9309e-02,\n",
      "            5.4892e-01, -1.7659e+00],\n",
      "          [-4.5238e-02,  4.5564e-01,  3.2249e-01,  ..., -2.7455e+00,\n",
      "           -4.3104e+00,  3.7494e+00],\n",
      "          [-6.8569e-01,  5.5880e-01,  1.0153e+00,  ..., -3.8003e+00,\n",
      "           -4.0061e+00,  5.2598e+00],\n",
      "          ...,\n",
      "          [-9.8283e-01,  1.4349e+00,  1.1719e+00,  ...,  6.1256e-01,\n",
      "           -3.2998e+00,  5.8489e+00],\n",
      "          [ 3.3851e-01,  1.5146e+00,  7.2609e-02,  ...,  6.1595e-01,\n",
      "           -3.0633e+00,  6.5991e+00],\n",
      "          [ 1.0614e+00,  1.0466e+00, -1.0110e+00,  ...,  7.7625e-01,\n",
      "           -2.0558e+00,  6.8541e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-0.0046,  0.0020, -0.0140,  ...,  0.0082,  0.0081, -0.0015],\n",
      "          [ 0.0832, -0.0413, -0.1715,  ...,  0.1178, -0.0066,  0.1292],\n",
      "          [-0.1238,  0.1765,  0.0550,  ...,  0.0925,  0.1317,  0.3535],\n",
      "          ...,\n",
      "          [-0.3325,  0.1242, -0.0833,  ..., -0.0387, -0.0642,  0.0976],\n",
      "          [-0.1648, -0.0363, -0.2476,  ..., -0.0960,  0.0903,  0.1393],\n",
      "          [-0.1879, -0.0344, -0.1273,  ..., -0.0079,  0.0381,  0.1353]],\n",
      "\n",
      "         [[-0.0038, -0.0091,  0.0083,  ..., -0.0054,  0.0071, -0.0047],\n",
      "          [ 0.1566,  0.1351,  0.0050,  ..., -0.4548, -0.3239,  0.0680],\n",
      "          [ 0.1477,  0.0966, -0.0670,  ..., -0.5770, -0.0011, -0.1576],\n",
      "          ...,\n",
      "          [-0.1758,  0.1400,  0.2836,  ..., -0.1574,  0.0606,  0.1341],\n",
      "          [-0.0904,  0.0665,  0.2797,  ...,  0.1177,  0.1326,  0.1111],\n",
      "          [-0.1174,  0.0107,  0.3672,  ..., -0.0994, -0.1035, -0.0251]],\n",
      "\n",
      "         [[ 0.0008, -0.0039, -0.0039,  ...,  0.0026,  0.0038, -0.0024],\n",
      "          [ 0.0183, -0.1471, -0.3235,  ..., -0.1289,  0.1862, -0.4419],\n",
      "          [ 0.1391,  0.0976, -0.2344,  ...,  0.0246, -0.0504, -0.1171],\n",
      "          ...,\n",
      "          [ 0.1105, -0.0224,  0.3630,  ...,  0.0398, -0.1578, -0.0530],\n",
      "          [ 0.0235,  0.0601,  0.2273,  ...,  0.0200,  0.0553,  0.0124],\n",
      "          [ 0.1680, -0.0751,  0.1612,  ...,  0.0185, -0.0534,  0.2135]],\n",
      "\n",
      "         [[ 0.0023,  0.0022,  0.0085,  ...,  0.0011, -0.0032, -0.0044],\n",
      "          [-0.1708, -0.3404,  0.0911,  ..., -0.1357, -0.2974,  0.2315],\n",
      "          [-0.2444,  0.2014,  0.0115,  ...,  0.2007,  0.1298, -0.2523],\n",
      "          ...,\n",
      "          [-0.3095, -0.6333, -0.0640,  ...,  0.1772,  0.3814,  0.0414],\n",
      "          [-0.2200, -0.6508, -0.3625,  ...,  0.0367,  0.1266, -0.0427],\n",
      "          [-0.3224, -0.6427,  0.0147,  ..., -0.0377,  0.1240,  0.1798]]]],\n",
      "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 2.4661e-03, -1.8564e-02, -1.3659e-03,  ..., -8.8163e-01,\n",
      "            1.8388e+00,  9.5980e-01],\n",
      "          [ 1.0614e-01, -1.4248e-01,  1.1014e-01,  ..., -1.3144e-01,\n",
      "           -1.4971e+00, -9.9642e-01],\n",
      "          [ 5.0465e-01, -1.2756e+00,  3.2766e-01,  ...,  1.0839e+00,\n",
      "           -2.3777e+00, -7.6284e-01],\n",
      "          ...,\n",
      "          [-9.4601e-01, -1.4567e+00,  1.5070e+00,  ..., -1.0254e+00,\n",
      "           -2.8964e+00, -5.5158e+00],\n",
      "          [-1.7954e+00, -8.5272e-02,  4.8491e-01,  ..., -8.1928e-01,\n",
      "           -1.9701e-01, -5.3832e+00],\n",
      "          [-6.8741e-01,  6.3748e-01, -6.5793e-01,  ...,  1.4303e-01,\n",
      "           -3.0139e+00, -4.4349e+00]],\n",
      "\n",
      "         [[-2.7702e-03, -2.5485e-02,  1.5394e-02,  ..., -1.1661e+00,\n",
      "           -4.5075e-01, -1.6335e+00],\n",
      "          [ 1.4510e+00, -5.5608e-01, -1.3763e+00,  ...,  1.8605e+00,\n",
      "            6.5836e-01,  1.7125e+00],\n",
      "          [-1.6550e+00,  4.1245e-01, -5.1286e-01,  ..., -9.2647e-01,\n",
      "            2.1693e+00,  3.8255e+00],\n",
      "          ...,\n",
      "          [ 3.0174e+00, -1.2515e+00,  1.7032e+00,  ...,  2.5093e+00,\n",
      "           -1.1873e+00,  5.5565e+00],\n",
      "          [ 3.3223e+00,  8.8909e-01,  2.1768e+00,  ..., -3.1638e-01,\n",
      "           -2.7397e+00,  5.3065e+00],\n",
      "          [ 1.0706e+00,  2.4861e+00,  2.6390e+00,  ...,  1.7795e+00,\n",
      "           -1.8623e+00,  4.1346e+00]],\n",
      "\n",
      "         [[-6.0043e-03,  1.3508e-03, -1.6365e-02,  ...,  2.1125e-01,\n",
      "            4.6633e-01, -8.9323e-01],\n",
      "          [-2.2639e-01,  5.0410e-02, -6.2642e-01,  ...,  6.1170e-01,\n",
      "           -2.1945e+00, -3.5135e-01],\n",
      "          [-5.4990e-01, -5.3723e-01,  7.0894e-01,  ...,  2.6226e+00,\n",
      "           -2.6710e+00, -2.1397e+00],\n",
      "          ...,\n",
      "          [ 3.4651e+00,  4.3972e-01,  7.4567e-01,  ...,  4.3014e-01,\n",
      "            2.6000e-01, -2.7249e+00],\n",
      "          [ 1.0769e+00, -1.3743e+00,  8.0074e-01,  ..., -3.0158e-01,\n",
      "            8.9183e-01, -4.5810e+00],\n",
      "          [-1.6066e+00, -3.2668e+00,  1.9343e+00,  ..., -2.6129e-02,\n",
      "            2.2832e-01, -3.2140e+00]],\n",
      "\n",
      "         [[ 2.2572e-02, -3.9020e-02,  3.2312e-02,  ...,  3.0697e-01,\n",
      "           -1.6622e-01, -1.7126e+00],\n",
      "          [-1.8620e+00,  7.0151e-01,  1.5229e+00,  ..., -1.0829e+00,\n",
      "            1.3682e-01,  3.1425e+00],\n",
      "          [-4.2841e+00,  3.8916e-01, -4.5938e-01,  ...,  1.8416e-02,\n",
      "            8.5055e-01,  4.2689e+00],\n",
      "          ...,\n",
      "          [ 4.7799e+00, -4.1817e+00, -5.7875e+00,  ...,  9.8206e-01,\n",
      "           -2.2573e+00,  8.4250e+00],\n",
      "          [-1.3937e+00,  2.2204e-01, -5.4135e+00,  ...,  3.7657e-01,\n",
      "           -1.4145e+00,  7.7025e+00],\n",
      "          [-5.9324e+00,  1.0002e+00, -3.3352e+00,  ...,  8.1003e-01,\n",
      "           -1.5868e+00,  6.7471e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 1.1140e-02,  6.8977e-03,  9.7502e-03,  ...,  9.1542e-03,\n",
      "           -6.7721e-03,  9.2524e-03],\n",
      "          [ 8.5405e-02,  3.2543e-01,  2.1310e-01,  ..., -3.5495e-01,\n",
      "           -8.1388e-01, -5.9049e-02],\n",
      "          [-3.5068e-01, -4.7571e-01, -3.5204e-01,  ...,  3.3781e-01,\n",
      "           -6.7878e-01,  4.5822e-01],\n",
      "          ...,\n",
      "          [-3.3034e-01,  3.6530e-01, -2.9839e-01,  ..., -6.4024e-03,\n",
      "           -5.2561e-01, -1.8826e-01],\n",
      "          [-1.3977e-01,  1.4854e-01, -8.6878e-01,  ..., -4.5177e-01,\n",
      "            3.2727e-01, -4.2894e-02],\n",
      "          [-4.2225e-01,  6.6036e-01, -6.7464e-01,  ..., -2.4498e-01,\n",
      "            1.0003e-01, -7.3074e-02]],\n",
      "\n",
      "         [[ 1.7854e-02, -7.6131e-03, -5.8683e-03,  ..., -1.9090e-03,\n",
      "           -1.1749e-03, -7.3016e-03],\n",
      "          [ 2.9106e-01, -1.8873e-01, -2.5724e-01,  ..., -3.5195e-02,\n",
      "           -8.8147e-02,  7.4759e-02],\n",
      "          [ 7.8790e-01, -4.6717e-01, -5.5272e-01,  ..., -4.9451e-01,\n",
      "           -1.1177e-01,  3.4864e-01],\n",
      "          ...,\n",
      "          [ 1.4055e-01, -2.9709e-02, -1.7221e-02,  ...,  2.6746e-02,\n",
      "            5.6936e-01, -2.1871e-01],\n",
      "          [ 6.2514e-02,  3.1959e-01,  8.8892e-02,  ..., -1.6380e-01,\n",
      "            4.4886e-01, -1.1738e-01],\n",
      "          [ 1.1815e-01,  1.9875e-01, -4.1465e-02,  ...,  7.0857e-02,\n",
      "            6.2998e-01, -6.2760e-02]],\n",
      "\n",
      "         [[ 3.3264e-03, -1.0257e-04,  7.0035e-03,  ...,  6.0381e-03,\n",
      "           -1.1853e-03,  1.9587e-03],\n",
      "          [-1.6028e-01, -8.2520e-02,  3.4590e-01,  ...,  8.2714e-01,\n",
      "            1.1847e-01,  3.0451e-01],\n",
      "          [ 2.0191e-01,  1.8862e-01,  3.1520e-01,  ...,  4.1978e-01,\n",
      "           -4.3229e-01, -1.9802e-01],\n",
      "          ...,\n",
      "          [-5.7128e-02,  1.9051e-01,  9.7140e-02,  ..., -4.2837e-02,\n",
      "            4.2435e-01,  1.2291e-01],\n",
      "          [ 1.5772e-01, -5.7168e-03, -2.4640e-01,  ..., -6.8629e-01,\n",
      "           -6.8990e-02,  2.6560e-01],\n",
      "          [ 1.2323e-01,  2.7381e-01, -2.0192e-01,  ..., -6.9884e-01,\n",
      "            1.3701e-01,  2.6164e-01]],\n",
      "\n",
      "         [[-7.5595e-03,  1.2732e-05,  4.0150e-03,  ..., -9.7901e-03,\n",
      "            2.9049e-03,  7.0048e-03],\n",
      "          [-5.7048e-01,  1.0472e-01,  7.6206e-01,  ...,  4.1111e-01,\n",
      "            1.9349e-01, -3.6886e-01],\n",
      "          [-3.9402e-01,  4.9979e-01,  2.8634e-01,  ..., -3.3591e-02,\n",
      "           -1.4821e-01, -2.5100e-01],\n",
      "          ...,\n",
      "          [-1.7777e-01,  1.2317e-01,  2.2742e-02,  ..., -2.8640e-02,\n",
      "            2.7760e-02, -4.9987e-01],\n",
      "          [-1.7903e-01,  3.3327e-01, -2.4841e-01,  ...,  8.4489e-02,\n",
      "            1.6849e-01, -1.2432e-01],\n",
      "          [-5.3567e-02,  5.0856e-01, -2.2376e-01,  ...,  4.8256e-02,\n",
      "            2.3687e-02, -2.0771e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.9954e-03, -9.6235e-03, -6.3216e-03,  ..., -1.0267e+00,\n",
      "            4.9953e-01, -2.5523e-01],\n",
      "          [-1.6714e-01, -2.4055e-01,  1.9856e-01,  ...,  1.9516e+00,\n",
      "           -4.8267e+00,  3.9901e-01],\n",
      "          [ 3.2069e-01, -2.0514e-01,  2.1353e-01,  ...,  6.3179e-01,\n",
      "           -4.2436e+00, -2.1853e+00],\n",
      "          ...,\n",
      "          [-9.2501e-01, -1.1946e+00,  1.1051e+00,  ...,  1.4416e+00,\n",
      "           -7.9827e+00,  2.9901e+00],\n",
      "          [-1.3613e+00, -7.2447e-01,  1.0263e+00,  ...,  6.9149e-01,\n",
      "           -1.0083e+01,  3.0373e+00],\n",
      "          [-3.4929e-01,  5.0467e-01,  1.0882e+00,  ...,  1.1906e+00,\n",
      "           -9.8026e+00,  2.8800e+00]],\n",
      "\n",
      "         [[ 5.1886e-03,  1.0575e-02, -2.5615e-05,  ...,  9.4034e-02,\n",
      "            2.2087e+00, -5.8272e-01],\n",
      "          [ 5.3163e-01, -1.0974e+00, -4.5727e-01,  ...,  4.6140e-01,\n",
      "           -1.4754e+00,  1.0680e+00],\n",
      "          [ 2.3118e+00, -1.9615e-01, -2.2171e-01,  ..., -1.3534e+00,\n",
      "           -2.3936e+00,  9.1419e-01],\n",
      "          ...,\n",
      "          [-4.5156e+00, -2.0765e+00, -7.8882e-01,  ..., -2.6777e+00,\n",
      "           -3.6507e+00,  1.9572e+00],\n",
      "          [-1.5483e+00,  1.0156e-01,  4.4876e-01,  ..., -1.7221e+00,\n",
      "           -2.9879e+00, -3.7017e-01],\n",
      "          [ 3.3559e+00,  1.9136e+00,  1.9710e+00,  ..., -1.5732e+00,\n",
      "           -3.5153e+00,  9.3864e-01]],\n",
      "\n",
      "         [[ 1.2755e-02, -3.7288e-03,  2.5611e-02,  ...,  1.5196e+00,\n",
      "           -1.0030e-01,  2.1827e+00],\n",
      "          [-2.0290e-01,  9.4977e-01,  3.0841e-01,  ..., -1.6258e+00,\n",
      "           -7.1305e-01, -2.7522e+00],\n",
      "          [-1.2964e-01,  7.2508e-01,  6.4916e-01,  ..., -3.2470e+00,\n",
      "           -6.8937e-01, -3.4955e+00],\n",
      "          ...,\n",
      "          [ 1.4678e-01,  9.3096e-01,  1.9306e+00,  ..., -4.3792e+00,\n",
      "            6.2345e-01, -5.4662e+00],\n",
      "          [-1.7694e+00,  1.0352e+00,  1.2766e+00,  ..., -5.1710e+00,\n",
      "           -1.2772e+00, -4.7764e+00],\n",
      "          [-1.8993e+00,  7.2949e-01, -7.5588e-02,  ..., -3.3490e+00,\n",
      "           -8.9197e-01, -7.1716e+00]],\n",
      "\n",
      "         [[ 6.9784e-03,  1.1896e-02, -9.7730e-03,  ...,  6.0794e-02,\n",
      "            2.1409e-01, -2.4456e-01],\n",
      "          [ 2.2934e-01,  2.8422e-01, -4.8159e-01,  ..., -1.9595e-01,\n",
      "            1.9792e+00, -2.0637e-01],\n",
      "          [-1.5222e+00, -4.2509e-01, -1.4838e+00,  ...,  1.2865e+00,\n",
      "            4.4940e-01, -3.5297e-01],\n",
      "          ...,\n",
      "          [ 3.3654e+00, -8.8159e-01, -2.3807e-01,  ...,  1.8183e+00,\n",
      "            3.9896e+00, -2.9428e+00],\n",
      "          [ 2.1786e+00,  7.5718e-02,  1.2958e+00,  ...,  3.2222e+00,\n",
      "            3.3337e+00, -4.2685e+00],\n",
      "          [ 3.2084e-01, -2.0314e+00,  2.2115e+00,  ...,  1.8660e+00,\n",
      "            2.3107e+00, -2.4417e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-1.1397e-02,  9.6104e-04,  6.3185e-03,  ..., -1.5067e-02,\n",
      "            2.8815e-03,  8.0872e-04],\n",
      "          [-3.3234e-01, -1.4828e-01, -1.9586e-01,  ..., -2.5293e-01,\n",
      "           -1.1428e-02,  1.0253e-01],\n",
      "          [ 5.0500e-01, -1.4986e-01,  5.1304e-02,  ...,  2.6112e-01,\n",
      "            9.1815e-02,  3.8045e-01],\n",
      "          ...,\n",
      "          [-3.7712e-01, -1.3353e+00, -1.2929e-01,  ...,  3.5227e-01,\n",
      "            3.2060e-02, -4.2664e-01],\n",
      "          [-2.0780e-02, -8.8940e-01,  1.7877e-02,  ...,  7.4211e-01,\n",
      "            4.0908e-01, -4.8355e-01],\n",
      "          [ 2.1539e-01, -3.8701e-01,  3.2595e-01,  ...,  4.6627e-01,\n",
      "            3.5300e-01,  3.1051e-02]],\n",
      "\n",
      "         [[-1.1260e-02,  9.2822e-03, -4.8466e-03,  ...,  4.3775e-03,\n",
      "           -1.8696e-02, -4.6843e-03],\n",
      "          [ 2.9219e-01,  2.4368e-01,  1.0288e-01,  ...,  9.5423e-03,\n",
      "           -5.8030e-02, -3.1206e-02],\n",
      "          [ 1.3990e-01,  2.2406e-01, -4.4883e-01,  ..., -1.6995e-01,\n",
      "            2.8030e-01,  3.2196e-01],\n",
      "          ...,\n",
      "          [ 5.2192e-02,  1.8601e-01, -3.8292e-02,  ...,  1.6040e-01,\n",
      "            4.1952e-01, -3.5459e-01],\n",
      "          [-7.9849e-02,  3.2760e-01, -2.0371e-01,  ...,  3.2842e-01,\n",
      "            2.0520e-01, -3.4863e-01],\n",
      "          [-1.5608e-01,  3.1804e-01, -2.4261e-01,  ...,  8.2761e-02,\n",
      "            2.9562e-01, -1.4675e-01]],\n",
      "\n",
      "         [[ 1.8537e-02, -2.7435e-03,  2.1794e-02,  ..., -9.7100e-03,\n",
      "           -7.2893e-03, -3.1297e-03],\n",
      "          [ 5.2184e-02,  8.5849e-02, -3.7058e-01,  ..., -2.5165e-01,\n",
      "           -1.1442e-01, -5.5814e-01],\n",
      "          [ 3.7817e-01,  6.6229e-02, -2.4632e-01,  ..., -1.2936e-01,\n",
      "           -1.4512e-01, -3.8921e-01],\n",
      "          ...,\n",
      "          [-2.6872e-01, -1.0854e-01,  1.1156e-01,  ...,  5.5822e-03,\n",
      "           -2.9391e-01,  3.3517e-01],\n",
      "          [-1.8234e-02,  2.3703e-01, -1.5749e-01,  ...,  4.1612e-03,\n",
      "           -4.5119e-01,  4.4633e-01],\n",
      "          [-3.1796e-01, -8.2739e-02, -1.5416e-01,  ..., -2.8459e-01,\n",
      "           -5.3528e-01,  1.6930e-01]],\n",
      "\n",
      "         [[-1.0722e-02,  9.6770e-03, -1.1596e-02,  ...,  1.0901e-02,\n",
      "           -1.6652e-03,  3.2180e-03],\n",
      "          [-3.3385e-02,  2.9989e-02, -2.8583e-01,  ...,  5.8913e-02,\n",
      "            3.2415e-01,  1.8707e-01],\n",
      "          [-3.2095e-01,  2.9242e-01, -1.7437e-01,  ...,  5.6438e-01,\n",
      "           -2.3419e-01, -1.5592e-01],\n",
      "          ...,\n",
      "          [ 1.3522e-01, -5.1883e-01,  2.8244e-02,  ...,  6.8938e-02,\n",
      "            8.8331e-03, -1.7099e-01],\n",
      "          [-4.2349e-01, -1.7209e-02, -3.8570e-01,  ...,  2.5231e-01,\n",
      "            2.9307e-01,  5.1908e-02],\n",
      "          [ 3.9892e-01, -2.5810e-01, -1.7718e-01,  ...,  6.5979e-02,\n",
      "            3.9956e-01, -2.3054e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 6.0531e-03, -1.6843e-03, -2.2802e-02,  ...,  7.2813e-03,\n",
      "            1.3498e-01,  3.5676e-01],\n",
      "          [ 6.3897e-01, -1.6476e+00, -1.3601e+00,  ...,  4.2053e-01,\n",
      "           -1.4852e+00, -2.0078e-01],\n",
      "          [ 1.7760e+00, -2.2245e+00,  1.0481e+00,  ..., -1.3995e+00,\n",
      "            1.2341e-01, -1.9736e-01],\n",
      "          ...,\n",
      "          [-1.7813e-01, -2.0452e+00,  2.4864e+00,  ..., -3.5336e-01,\n",
      "            2.1860e+00,  2.0447e+00],\n",
      "          [-1.4376e+00, -7.3373e-01,  3.0251e+00,  ...,  1.0671e+00,\n",
      "            2.1650e+00,  2.5631e+00],\n",
      "          [-7.2687e-01,  8.3094e-01,  1.6654e+00,  ...,  8.6775e-01,\n",
      "            1.4442e+00,  1.8303e+00]],\n",
      "\n",
      "         [[ 6.2736e-03, -2.6301e-02, -2.3428e-02,  ..., -7.0280e-01,\n",
      "           -6.9706e-01,  2.6269e+00],\n",
      "          [-5.0304e-01, -3.3020e-02, -9.2811e-02,  ...,  8.3682e-01,\n",
      "           -3.4116e-01, -3.3033e+00],\n",
      "          [-6.1074e-01,  7.1242e-01, -9.7847e-01,  ..., -1.1976e+00,\n",
      "           -4.8795e-01, -3.7915e+00],\n",
      "          ...,\n",
      "          [-4.1447e-01,  5.4956e-01, -7.9947e-01,  ...,  2.1872e+00,\n",
      "           -2.8077e+00, -4.4597e+00],\n",
      "          [-7.3732e-01,  1.0663e+00, -1.2821e-01,  ...,  4.1126e-01,\n",
      "           -1.4797e+00, -3.2174e+00],\n",
      "          [-6.5104e-01, -1.3397e-01,  5.6116e-01,  ...,  4.9056e+00,\n",
      "           -1.1052e+00, -5.6208e+00]],\n",
      "\n",
      "         [[ 2.1531e-02, -6.2467e-03,  5.3069e-03,  ..., -2.9258e-02,\n",
      "           -2.1898e+00, -6.3233e-02],\n",
      "          [-7.8528e-01, -9.3715e-01,  7.9430e-01,  ..., -1.0407e-01,\n",
      "            3.5328e-01,  1.1499e-01],\n",
      "          [ 9.7893e-01, -1.8718e+00,  1.4143e-01,  ..., -1.9626e-01,\n",
      "            1.3065e+00, -3.8367e-01],\n",
      "          ...,\n",
      "          [-2.1176e+00, -2.5022e+00, -1.1472e+00,  ...,  2.9393e+00,\n",
      "            2.3586e+00, -1.8919e+00],\n",
      "          [-2.2941e+00, -3.4175e+00, -8.9065e-01,  ...,  2.8734e+00,\n",
      "            1.2468e+00, -1.4328e+00],\n",
      "          [-1.3190e+00, -1.9177e+00, -1.7678e+00,  ...,  2.2773e+00,\n",
      "            2.4642e+00, -1.2488e+00]],\n",
      "\n",
      "         [[-2.6541e-02, -1.9358e-02,  1.8092e-02,  ...,  5.3540e-01,\n",
      "           -1.2998e+00, -6.6691e-01],\n",
      "          [ 2.2826e+00,  2.0473e+00,  1.2192e+00,  ..., -3.5795e-01,\n",
      "            1.5956e+00,  1.4873e+00],\n",
      "          [ 4.2898e+00,  3.8062e-02,  1.3424e+00,  ...,  1.5016e+00,\n",
      "            2.3447e+00,  2.9443e-02],\n",
      "          ...,\n",
      "          [-4.3024e+00,  1.9458e+00,  1.4687e+00,  ..., -5.8122e-01,\n",
      "            2.5928e+00, -4.1854e-01],\n",
      "          [ 2.0994e+00, -9.3463e-01, -5.7620e-01,  ..., -6.9324e-01,\n",
      "            2.4752e+00, -7.2743e-01],\n",
      "          [ 6.6399e+00, -2.5312e+00, -2.8712e+00,  ..., -1.6669e+00,\n",
      "            2.4019e+00,  2.0406e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-4.9946e-03, -1.2770e-02, -8.3554e-03,  ..., -1.0779e-02,\n",
      "            5.7142e-03,  3.4078e-03],\n",
      "          [ 1.6546e-01, -8.5717e-03,  5.5637e-01,  ..., -1.9585e-01,\n",
      "            6.6426e-01, -1.1668e-01],\n",
      "          [-4.1997e-02, -2.6105e-01, -5.3636e-02,  ..., -5.5807e-01,\n",
      "            1.1235e+00, -4.8474e-01],\n",
      "          ...,\n",
      "          [ 3.8565e-01,  5.8103e-02,  3.9509e-01,  ..., -3.7878e-02,\n",
      "            1.0914e-01,  4.2151e-02],\n",
      "          [ 1.3017e-01,  2.6874e-01,  5.2454e-01,  ..., -3.1955e-02,\n",
      "            4.3526e-01,  1.2479e-03],\n",
      "          [ 1.2686e-01,  7.3299e-02,  8.6711e-01,  ..., -1.4032e-01,\n",
      "            3.6867e-01,  7.3779e-02]],\n",
      "\n",
      "         [[ 1.4238e-02, -1.4174e-02,  1.4938e-03,  ...,  1.2901e-02,\n",
      "           -2.3884e-02,  2.0822e-02],\n",
      "          [-2.1949e-01, -1.0940e+00,  2.7181e-01,  ...,  2.5042e-01,\n",
      "           -3.8343e-02,  5.4662e-01],\n",
      "          [-6.1444e-01, -6.0960e-01, -3.3741e-01,  ...,  2.6548e-02,\n",
      "           -6.5740e-01, -2.4832e-01],\n",
      "          ...,\n",
      "          [ 1.9268e-01, -3.1044e-01, -5.1792e-01,  ...,  1.4264e-01,\n",
      "            1.1090e+00,  9.6642e-01],\n",
      "          [ 7.7565e-01, -1.3218e-02, -2.7477e-01,  ...,  7.6761e-01,\n",
      "           -9.1341e-02,  1.3486e-02],\n",
      "          [ 1.2717e-01, -1.3473e-01,  5.4477e-02,  ...,  9.6178e-01,\n",
      "           -8.3613e-02, -4.2287e-01]],\n",
      "\n",
      "         [[ 3.1102e-02,  1.4212e-02,  1.9913e-03,  ..., -6.8422e-03,\n",
      "            9.7062e-03,  1.9131e-03],\n",
      "          [-2.0527e-01, -9.6940e-02,  4.3870e-01,  ..., -6.1471e-02,\n",
      "           -2.1203e-01, -6.3271e-01],\n",
      "          [-2.8992e-01, -6.9784e-02,  5.9453e-02,  ..., -8.6157e-01,\n",
      "           -1.5753e-01, -4.8789e-01],\n",
      "          ...,\n",
      "          [-1.1177e-01, -5.9315e-01,  5.0657e-01,  ..., -1.0375e-01,\n",
      "           -2.2932e-01, -1.1203e-01],\n",
      "          [ 1.9014e-01,  1.7211e-01,  3.1432e-01,  ..., -4.5455e-01,\n",
      "           -3.1787e-01, -1.4730e-01],\n",
      "          [ 3.5610e-01,  2.3451e-01,  3.7778e-01,  ..., -4.5026e-01,\n",
      "           -1.0585e-01,  1.5188e-01]],\n",
      "\n",
      "         [[-1.9263e-04, -4.2217e-03,  7.7507e-03,  ...,  1.4083e-02,\n",
      "           -4.2847e-03,  3.2424e-04],\n",
      "          [-3.9066e-01, -5.4990e-01, -4.1808e-01,  ...,  7.1012e-01,\n",
      "           -1.4400e-01,  1.1345e-01],\n",
      "          [-1.1891e-01, -4.4064e-01, -5.4295e-01,  ...,  5.7324e-01,\n",
      "            3.3752e-02,  3.4630e-01],\n",
      "          ...,\n",
      "          [-3.1757e-01, -1.2609e-01,  4.7504e-02,  ...,  1.3513e-01,\n",
      "           -1.9213e-01,  1.8857e-02],\n",
      "          [ 1.0842e-01,  9.2164e-02,  1.1144e-01,  ...,  5.9027e-02,\n",
      "           -1.1920e-01, -2.1452e-01],\n",
      "          [-4.1245e-02, -8.5121e-02, -9.2024e-02,  ...,  3.4148e-01,\n",
      "           -3.2316e-01, -3.2687e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.2958e-02, -1.3435e-02,  1.3358e-02,  ...,  8.2715e-02,\n",
      "            3.1325e-01,  3.7420e-01],\n",
      "          [-1.1284e+00,  1.7642e+00,  5.9895e-01,  ..., -8.1060e-01,\n",
      "            1.5369e+00, -6.4685e-01],\n",
      "          [ 3.5242e-01,  1.6556e+00,  7.0153e-01,  ..., -1.0341e-01,\n",
      "           -8.1201e-01,  8.6927e-01],\n",
      "          ...,\n",
      "          [-2.9910e+00,  2.9688e+00, -1.9483e+00,  ..., -7.6188e-01,\n",
      "            1.2829e+00, -1.2443e+00],\n",
      "          [-3.9728e+00,  2.7850e+00, -2.3051e+00,  ...,  8.3688e-01,\n",
      "            7.5208e-01, -1.4415e+00],\n",
      "          [-1.8046e+00,  9.3749e-01, -1.8116e+00,  ...,  2.3825e-01,\n",
      "            3.9551e-01, -3.1506e-01]],\n",
      "\n",
      "         [[-1.7936e-02,  4.9792e-04, -2.0992e-02,  ...,  2.6243e+00,\n",
      "           -5.4919e-01, -1.6646e+00],\n",
      "          [ 2.9155e+00,  4.3326e-01,  1.5935e+00,  ..., -5.8566e-02,\n",
      "            5.8397e-01, -1.0406e-01],\n",
      "          [ 1.6597e+00, -2.7655e-01,  7.0676e-01,  ..., -1.5865e+00,\n",
      "           -9.6807e-02,  1.0145e+00],\n",
      "          ...,\n",
      "          [-7.8946e-01, -7.5685e-01,  2.2365e-01,  ..., -1.2915e+00,\n",
      "            4.4691e-01,  1.3224e+00],\n",
      "          [ 8.7610e-01, -1.3393e-01,  1.8813e-01,  ..., -3.1987e-01,\n",
      "           -4.0428e-01,  2.1418e+00],\n",
      "          [ 2.8783e+00,  3.4945e-01, -4.1454e-01,  ..., -2.1668e+00,\n",
      "           -1.3759e-01,  8.9078e-01]],\n",
      "\n",
      "         [[-1.1523e-02,  2.8151e-02,  4.9991e-04,  ..., -1.0703e+00,\n",
      "            9.4870e-01,  2.2386e+00],\n",
      "          [ 5.8558e-01,  2.1301e+00, -1.3182e+00,  ...,  2.1632e+00,\n",
      "           -2.4449e+00, -5.2549e-01],\n",
      "          [-9.3870e-01,  2.1944e+00, -8.9647e-01,  ...,  2.3847e+00,\n",
      "            2.1249e-01, -1.7982e+00],\n",
      "          ...,\n",
      "          [ 3.8996e+00,  1.9547e+00,  5.6531e-01,  ...,  4.0404e+00,\n",
      "           -2.6404e-01, -1.7466e+00],\n",
      "          [ 3.5359e+00,  9.5112e-01,  2.0774e+00,  ...,  3.2738e+00,\n",
      "           -1.3987e+00, -1.7700e+00],\n",
      "          [ 6.8079e-01, -8.9599e-01,  2.8293e+00,  ...,  2.3569e+00,\n",
      "           -1.7642e+00, -2.6738e+00]],\n",
      "\n",
      "         [[ 9.8798e-03, -6.9373e-03, -3.3795e-03,  ..., -4.0260e-01,\n",
      "           -2.6256e-01, -2.0730e+00],\n",
      "          [-9.7356e-01,  4.1302e-01, -6.2369e-01,  ...,  2.1914e-02,\n",
      "            2.2953e+00,  1.4437e+00],\n",
      "          [-2.8761e+00, -1.7145e+00,  1.7134e+00,  ..., -5.2525e-01,\n",
      "            2.0461e-01,  1.9573e+00],\n",
      "          ...,\n",
      "          [ 2.5021e+00,  7.9660e-01,  8.5578e-01,  ..., -6.3939e-01,\n",
      "           -1.6471e+00,  8.2961e-01],\n",
      "          [ 7.3855e-01, -1.6751e+00,  3.4103e-01,  ..., -7.1065e-01,\n",
      "           -2.8440e+00,  1.9091e+00],\n",
      "          [-2.7482e+00, -1.8782e+00, -1.0423e+00,  ...,  3.7454e-01,\n",
      "           -2.0917e+00,  2.3345e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 1.4337e-02,  2.4794e-02, -1.2252e-02,  ...,  9.2560e-03,\n",
      "            2.6094e-02,  4.2547e-03],\n",
      "          [ 1.1465e-01, -9.0175e-01, -1.0552e-01,  ...,  4.3786e-01,\n",
      "           -8.6173e-02, -1.4423e-01],\n",
      "          [-1.2411e-01,  5.1480e-01, -3.6395e-01,  ..., -3.0042e-01,\n",
      "           -2.8877e-03, -2.4539e-02],\n",
      "          ...,\n",
      "          [-2.8849e-01,  2.7034e-02, -4.8947e-02,  ..., -1.1287e-02,\n",
      "            5.3364e-01, -2.0132e-01],\n",
      "          [-5.9395e-03,  5.2480e-01, -1.3575e-01,  ..., -5.9137e-01,\n",
      "            8.6996e-01, -7.0474e-01],\n",
      "          [-1.3610e-01,  4.2862e-01, -2.8137e-01,  ...,  8.5990e-02,\n",
      "            2.2022e-01, -5.5732e-01]],\n",
      "\n",
      "         [[ 5.6272e-03, -8.0819e-03,  8.4504e-02,  ...,  1.3539e-02,\n",
      "           -9.9371e-03, -7.4089e-03],\n",
      "          [-5.5949e-01, -9.2309e-02, -2.7099e-02,  ...,  1.6765e-01,\n",
      "           -8.4053e-02, -4.3129e-01],\n",
      "          [ 1.7989e-01,  1.0243e-02,  2.1584e-01,  ..., -6.6775e-01,\n",
      "            5.9847e-01, -1.2101e-01],\n",
      "          ...,\n",
      "          [ 1.9399e-01,  7.6127e-02,  1.4349e-01,  ..., -3.9708e-01,\n",
      "            2.2283e-01,  5.5724e-01],\n",
      "          [ 6.8564e-01,  2.9989e-01,  4.3090e-03,  ..., -1.0109e+00,\n",
      "            1.7150e-01,  1.6542e-01],\n",
      "          [ 1.0490e-01,  6.0702e-01,  5.4224e-01,  ..., -8.4806e-01,\n",
      "            5.1004e-02,  4.8177e-01]],\n",
      "\n",
      "         [[ 4.2572e-03,  7.8646e-03,  1.1700e-02,  ...,  6.2216e-04,\n",
      "           -8.8579e-03,  1.3695e-02],\n",
      "          [ 9.5575e-01, -2.1558e-01, -1.9383e-01,  ...,  7.8577e-02,\n",
      "            1.2843e-01,  1.8485e-01],\n",
      "          [ 3.0734e-01, -3.9086e-02, -2.2824e-02,  ..., -1.9988e-01,\n",
      "            6.9472e-01,  4.4025e-01],\n",
      "          ...,\n",
      "          [-6.5847e-02,  6.0940e-01,  2.3587e-01,  ...,  8.0236e-01,\n",
      "           -1.9366e-01, -1.7277e-01],\n",
      "          [-1.9722e-01,  4.4928e-01,  4.1058e-02,  ..., -1.3220e-02,\n",
      "            2.0121e-01,  1.6845e-03],\n",
      "          [-4.3909e-01,  3.3761e-01, -5.3368e-03,  ...,  3.1068e-01,\n",
      "            2.7124e-01,  1.4263e-01]],\n",
      "\n",
      "         [[ 2.8908e-03,  1.1318e-03, -1.6554e-03,  ...,  1.5156e-02,\n",
      "           -8.1936e-03,  7.9789e-03],\n",
      "          [ 6.6073e-01,  5.3455e-02,  3.5004e-01,  ..., -4.8393e-02,\n",
      "            2.9338e-02, -8.3894e-01],\n",
      "          [-2.8494e-01, -3.7048e-01,  1.1792e-01,  ...,  3.0484e-01,\n",
      "            6.3163e-01, -5.3605e-01],\n",
      "          ...,\n",
      "          [-1.4982e-01, -1.2449e-01, -1.7760e-01,  ..., -2.4108e-01,\n",
      "            2.4800e-02,  1.9490e-01],\n",
      "          [ 1.2984e-01, -7.4501e-02, -2.6272e-01,  ...,  4.9621e-01,\n",
      "           -9.2911e-02, -1.6656e-01],\n",
      "          [ 2.2095e-01, -9.2432e-02, -3.9515e-01,  ...,  5.7623e-01,\n",
      "            1.3406e-01,  1.6984e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.0282e-02,  1.4592e-02, -2.5244e-02,  ..., -7.4899e-01,\n",
      "           -1.0360e-02,  2.2914e+00],\n",
      "          [-1.8532e+00, -1.9383e+00, -9.2336e-01,  ...,  2.7433e+00,\n",
      "            6.9333e-01, -1.1890e+00],\n",
      "          [-2.8141e+00, -1.0892e+00, -2.6936e+00,  ...,  1.2897e+00,\n",
      "            8.1126e-01, -2.2856e+00],\n",
      "          ...,\n",
      "          [ 2.6279e+00, -1.1971e+00,  1.6212e+00,  ...,  4.4078e-01,\n",
      "            3.6539e-01, -1.4259e+00],\n",
      "          [-7.1244e-01, -6.3403e-01,  2.2850e+00,  ...,  6.6662e-01,\n",
      "            8.4796e-01, -2.4895e+00],\n",
      "          [-3.4351e+00,  8.4664e-01,  2.2510e+00,  ...,  1.2922e+00,\n",
      "            7.4847e-02, -2.3632e+00]],\n",
      "\n",
      "         [[ 2.4240e-03, -3.9585e-02, -6.9607e-02,  ...,  2.5384e-01,\n",
      "            1.6081e+00,  2.3556e+00],\n",
      "          [-3.4870e-01, -5.4627e-01, -6.4921e-01,  ...,  1.5821e+00,\n",
      "           -2.2307e+00, -1.0911e+00],\n",
      "          [-1.0963e+00,  1.5969e+00, -1.5643e+00,  ...,  1.2606e+00,\n",
      "           -1.2617e+00, -1.2123e+00],\n",
      "          ...,\n",
      "          [ 3.1796e+00,  3.9662e-01,  1.4726e+00,  ..., -7.0138e-02,\n",
      "            3.4769e-01, -3.9489e+00],\n",
      "          [ 3.5421e+00,  1.4535e+00,  1.7632e+00,  ..., -4.3044e-01,\n",
      "            1.0543e+00, -4.5131e+00],\n",
      "          [ 1.3950e+00,  1.3420e+00,  1.5801e+00,  ..., -4.9902e-01,\n",
      "            1.1282e-01, -3.6143e+00]],\n",
      "\n",
      "         [[ 3.5188e-02,  4.3313e-03, -2.2852e-02,  ..., -1.1033e-02,\n",
      "           -8.1488e-01, -2.3063e+00],\n",
      "          [-2.8626e+00, -1.6150e+00,  7.7825e-01,  ..., -1.5449e-01,\n",
      "            9.3909e-01, -2.1709e-01],\n",
      "          [-3.0186e+00,  2.0916e+00,  3.4770e+00,  ...,  1.2037e+00,\n",
      "            3.8469e-01,  2.5290e-01],\n",
      "          ...,\n",
      "          [ 3.1156e+00, -5.0818e-01, -1.4769e+00,  ..., -8.5071e-01,\n",
      "            1.2081e+00, -8.9950e-01],\n",
      "          [-2.1135e-01,  1.3422e+00, -2.1498e+00,  ..., -1.5972e-01,\n",
      "            1.1688e+00, -7.3887e-01],\n",
      "          [-3.5420e+00,  2.3192e+00, -2.2005e+00,  ..., -1.2564e+00,\n",
      "            2.3737e+00, -7.5622e-01]],\n",
      "\n",
      "         [[ 4.0084e-03, -3.4686e-02, -1.2474e-02,  ..., -2.5728e+00,\n",
      "           -1.9084e-01,  6.7127e-01],\n",
      "          [-9.8710e-01, -1.1193e+00, -1.7057e+00,  ...,  1.1192e+00,\n",
      "           -3.9661e-01,  1.6847e-01],\n",
      "          [-3.2676e-01,  9.4322e-01, -1.4393e+00,  ...,  2.9685e+00,\n",
      "            2.3802e+00, -3.4925e-01],\n",
      "          ...,\n",
      "          [-8.8605e-01,  3.7265e-02, -1.2119e+00,  ...,  6.1131e-01,\n",
      "           -5.7617e-01,  1.4207e+00],\n",
      "          [-2.3267e+00,  1.0649e+00, -3.9994e-01,  ...,  3.3097e+00,\n",
      "           -1.4568e-01,  1.2311e-01],\n",
      "          [-1.3236e+00,  2.4029e+00,  7.2727e-01,  ...,  3.3881e+00,\n",
      "           -1.2964e+00,  8.8704e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-2.0198e-02, -1.5066e-02,  6.3782e-03,  ..., -3.2834e-01,\n",
      "           -1.8483e-02,  1.1651e-02],\n",
      "          [ 9.2262e-02,  1.1608e+00,  5.2930e-01,  ..., -6.3493e-01,\n",
      "           -3.0110e-01, -2.5537e-01],\n",
      "          [ 2.7814e-01, -3.9992e-01, -3.3647e-01,  ..., -1.0538e+00,\n",
      "            7.1493e-01, -4.5124e-01],\n",
      "          ...,\n",
      "          [-2.9471e-01,  4.2099e-01, -2.2800e-01,  ..., -1.5672e+00,\n",
      "           -8.4987e-02, -1.0680e+00],\n",
      "          [-7.8973e-02,  1.8461e-01, -5.2449e-01,  ..., -1.7966e+00,\n",
      "           -2.8569e-01, -4.6808e-01],\n",
      "          [-2.9401e-01, -1.2680e-02, -5.8957e-01,  ..., -1.7353e+00,\n",
      "           -3.9601e-01, -8.9407e-01]],\n",
      "\n",
      "         [[ 3.5878e-02, -4.7212e-02, -1.5277e-03,  ..., -1.4034e-02,\n",
      "            3.4530e-02,  3.0004e-03],\n",
      "          [-9.6447e-01, -7.5368e-01,  8.1469e-01,  ..., -9.5809e-01,\n",
      "           -8.5290e-02, -5.4097e-01],\n",
      "          [-1.1200e+00,  4.7840e-01,  1.0423e+00,  ...,  1.0437e+00,\n",
      "            1.7162e-01,  5.1325e-01],\n",
      "          ...,\n",
      "          [-6.3619e-01, -6.4893e-01, -8.6752e-01,  ..., -1.5191e+00,\n",
      "            6.5412e-01, -2.1378e-01],\n",
      "          [ 1.1472e-01,  6.5780e-02, -8.0493e-01,  ..., -2.9696e+00,\n",
      "           -7.3866e-01,  6.1800e-01],\n",
      "          [ 1.2969e-01, -3.3408e-01, -1.2548e+00,  ..., -3.0845e+00,\n",
      "           -1.6216e-01,  2.9601e-01]],\n",
      "\n",
      "         [[-3.3412e-02,  2.8859e-02,  1.0661e-02,  ..., -1.9743e-02,\n",
      "           -7.5216e-03, -2.5582e-03],\n",
      "          [-2.8175e-02,  1.7185e-01,  1.5113e-01,  ...,  5.4975e-02,\n",
      "           -4.6247e-01,  1.3967e-01],\n",
      "          [ 1.2498e-01, -3.7112e-01, -1.8297e-01,  ..., -1.5777e-01,\n",
      "            9.0413e-01, -1.9701e-01],\n",
      "          ...,\n",
      "          [-1.5207e-02, -4.5669e-01, -4.7214e-01,  ...,  6.1973e-01,\n",
      "           -2.4275e-01,  9.1221e-01],\n",
      "          [ 2.6086e-01,  3.9537e-01, -3.5703e-01,  ...,  6.9387e-01,\n",
      "            7.5693e-02,  3.3926e-01],\n",
      "          [-2.2063e-02,  5.3849e-02, -1.7258e-01,  ...,  4.2600e-01,\n",
      "           -3.5404e-01,  2.7270e-01]],\n",
      "\n",
      "         [[-1.4925e-03, -4.6463e-02, -1.9524e-02,  ...,  1.7764e-02,\n",
      "           -4.2118e-03,  2.2416e-03],\n",
      "          [ 1.0589e-02,  9.0933e-01, -4.5077e-01,  ..., -1.0641e-01,\n",
      "            7.6094e-01,  7.4887e-02],\n",
      "          [ 3.8330e-01,  6.7610e-01, -5.5002e-01,  ...,  8.8103e-02,\n",
      "            1.7395e-01,  3.1617e-01],\n",
      "          ...,\n",
      "          [ 3.0730e-01, -5.7105e-01,  1.9247e-01,  ...,  2.1445e-02,\n",
      "            2.4395e-01,  2.7333e-01],\n",
      "          [-7.2407e-01, -6.7754e-01, -5.9187e-01,  ...,  5.7826e-01,\n",
      "            2.7944e-01,  3.8045e-01],\n",
      "          [ 7.8550e-02, -5.6771e-01,  1.1925e-01,  ...,  1.9575e-01,\n",
      "            2.7474e-01,  2.4867e-01]]]], grad_fn=<TransposeBackward0>))), hidden_states=None, attentions=None)\n",
      "logits tensor shape  LlavaLlamaForCausalLM.forward\n",
      " torch.Size([1, 599, 32000])\n",
      "logits tensor (first 2 tokens)  LlavaLlamaForCausalLM.forward\n",
      " tensor([[ -4.6822,   0.9866,   4.5126,  ...,  -5.2010,  -2.1646,  -4.2286],\n",
      "        [ -5.7232,  -5.6142,   6.5636,  ...,  -4.1083,  -8.1069,  -4.7190],\n",
      "        [ -2.0816,  -1.5998,   3.0980,  ...,  -0.4457,  -6.7922,  -0.6046],\n",
      "        ...,\n",
      "        [ -5.4379,  -5.2867,   0.3890,  ...,  -3.6321,  -5.3582,  -1.5054],\n",
      "        [ -6.4225,  -5.3943,   4.8081,  ...,  -5.0760, -10.5771,  -5.4899],\n",
      "        [ -7.9309,  -7.4418,   6.2761,  ...,  -2.6605, -11.1020,  -5.4067]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "loss (return)  LlavaLlamaForCausalLM.forward \n",
      " tensor(8.4862, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'print_factory.print_factory'; 'print_factory' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprint_factory\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprint_factory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_and_capture, embed_print_outputs\n\u001b[1;32m      3\u001b[0m logs, mapping \u001b[38;5;241m=\u001b[39m run_and_capture(\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprint_factory/original_code.py\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'print_factory.print_factory'; 'print_factory' is not a package"
     ]
    }
   ],
   "source": [
    "logs, mapping = run_and_capture(\n",
    "    model.forward,\n",
    "    **inputs\n",
    ")\n",
    "\n",
    "with open(\"print_factory/original_code.py\") as f:\n",
    "    code = f.read()\n",
    "\n",
    "new_code = embed_print_outputs(code, mapping)\n",
    "\n",
    "with open(\"print_factory/after_code.py\", \"w\") as f:\n",
    "    f.write(new_code)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_zero_3(param, ignore_status=False, name=None):\n",
    "\n",
    "    print(\"current file path\", \"llava/train/llava_trainer.py\")\n",
    "    print(\"def maybe_zero_3(param, ignore_status=False, name=None)\")\n",
    "    print(\"param maybe_zero_3\\n\", param)\n",
    "    print(\"ignore_status maybe_zero_3\\n\", ignore_status)\n",
    "    print(\"name maybe_zero_3\\n\", name)\n",
    "    from deepspeed import zero\n",
    "    from deepspeed.runtime.zero.partition_parameters import ZeroParamStatus\n",
    "    print(f\"【COND】 hasattr_ds_id={hasattr(param, 'ds_id')}\")\n",
    "    if hasattr(param, \"ds_id\"):\n",
    "        print(\"【ENTER】if hasattr(param, 'ds_id'):\")\n",
    "        print(f\"【COND】 ds_status={getattr(param, 'ds_status', None)}, ignore_status={ignore_status}\")\n",
    "        if param.ds_status == ZeroParamStatus.NOT_AVAILABLE:\n",
    "            print(\"【ENTER】if param.ds_status == ZeroParamStatus.NOT_AVAILABLE:\")\n",
    "            print(f\"【COND】 ignore_status={ignore_status}\")\n",
    "            if not ignore_status:\n",
    "                print(\"【ENTER】if not ignore_status:\")\n",
    "                print(name, 'no ignore status')\n",
    "                print(\"【EXIT】if not ignore_status:\")\n",
    "            print(\"【EXIT】if param.ds_status == ZeroParamStatus.NOT_AVAILABLE:\")\n",
    "        with zero.GatheredParameters([param]):\n",
    "            param = param.data.detach().cpu().clone()\n",
    "            print(\"param (after GatheredParameters)\\n\", param)\n",
    "        print(\"【EXIT】if hasattr(param, 'ds_id'):\")\n",
    "    else:\n",
    "        print(\"【ENTER】else (not hasattr(param, 'ds_id')):\")\n",
    "        param = param.detach().cpu().clone()\n",
    "        print(\"param (after else)\\n\", param)\n",
    "        print(\"【EXIT】else (not hasattr(param, 'ds_id')):\")\n",
    "    print(\"param (def maybe_zero_3 at llava_trainer.py return)\\n\", param)\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mm_adapter_state_maybe_zero_3(named_params, keys_to_match):\n",
    "\n",
    "    print(\"current file path\", \"llava/train/llava_trainer.py\")\n",
    "    print(\"def get_mm_adapter_state_maybe_zero_3(named_params, keys_to_match)\")\n",
    "    print(\"named_params get_mm_adapter_state_maybe_zero_3\\n\", named_params)\n",
    "    print(\"keys_to_match get_mm_adapter_state_maybe_zero_3\\n\", keys_to_match)\n",
    "    to_return = {k: t for k, t in named_params if any(key_match in k for key_match in keys_to_match)}\n",
    "    to_return = {k: maybe_zero_3(v, ignore_status=True, name=k).cpu() for k, v in to_return.items()}\n",
    "    print(\"to_return def get_mm_adapter_state_maybe_zero_3 \\n\", to_return)\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_checkpoint(self, model, trial, metrics=None):\n",
    "\n",
    "    print(\"current file path\", \"llava/train/llava_trainer.py\")\n",
    "    print(\"def _save_checkpoint(self, model, trial, metrics=None)\")\n",
    "    print(\"self\\n\", self) # <llava.train.llava_trainer.LLaVATrainer object at 0x7ed6341f4490>\n",
    "    print(\"model\\n\", model)\n",
    "\n",
    "    print(\"trial\\n\", trial) # None\n",
    "    print(\"metrics\\n\", metrics) # None\n",
    "    print(f\"【COND】 tune_mm_mlp_adapter={getattr(self.args, 'tune_mm_mlp_adapter', False)}\") # True\n",
    "    if getattr(self.args, 'tune_mm_mlp_adapter', False):\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】if getattr(self.args, 'tune_mm_mlp_adapter', False):\")\n",
    "        from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "        checkpoint_folder = f\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\"\n",
    "        print(\"checkpoint_folder = f\\\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\\\"\\n\", checkpoint_folder)\n",
    "\n",
    "        run_dir = self._get_output_dir(trial=trial)\n",
    "        print(\"run_dir = self._get_output_dir(trial=trial)\", run_dir)\n",
    "        output_dir = os.path.join(run_dir, checkpoint_folder)\n",
    "        print(\"output_dir = os.path.join(run_dir, checkpoint_folder)\", output_dir)\n",
    "\n",
    "        # Only save Adapter\n",
    "        keys_to_match = ['mm_projector', 'vision_resampler']\n",
    "        print(f\"【COND】 use_im_start_end={getattr(self.args, 'use_im_start_end', False)}\") # False\n",
    "        if getattr(self.args, \"use_im_start_end\", False):\n",
    "            pass\n",
    "\n",
    "        weight_to_save = get_mm_adapter_state_maybe_zero_3(self.model.named_parameters(), keys_to_match)\n",
    "\n",
    "        print(f\"【COND】 local_rank={self.args.local_rank}\") # 0\n",
    "        if self.args.local_rank == 0 or self.args.local_rank == -1:\n",
    "            # 【ENTER】\n",
    "            print(\"【ENTER】if self.args.local_rank == 0 or self.args.local_rank == -1:\")\n",
    "            self.model.config.save_pretrained(output_dir)\n",
    "            torch.save(weight_to_save, os.path.join(output_dir, f'mm_projector.bin'))\n",
    "            print(\"【EXIT】if self.args.local_rank == 0 or self.args.local_rank == -1:\")\n",
    "        print(\"【EXIT】if getattr(self.args, 'tune_mm_mlp_adapter', False):\")\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgAHEvYzuB0U"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def train()\")\n",
    "    global local_rank\n",
    "\n",
    "    parser = transformers.HfArgumentParser(\n",
    "        (ModelArguments, DataArguments, TrainingArguments))\n",
    "    print(\"original parser\\n\", parser)\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    print(\"model_args\\n\", model_args)\n",
    "    print(\"data_args\\n\", data_args)\n",
    "    print(\"training_args\\n\", training_args)\n",
    "    local_rank = training_args.local_rank\n",
    "    print(\"local_rank\\n\", local_rank)\n",
    "    compute_dtype = (torch.float16 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n",
    "    print(\"compute_dtype\\n\", compute_dtype)\n",
    "    bnb_model_from_pretrained_args = {}\n",
    "    print(\"bnb_model_from_pretrained_args\\n\", bnb_model_from_pretrained_args)\n",
    "    # 【SKIP】bfloat16 なので 以下の if 文はスキップされる\n",
    "    print(f\"【COND】 bits={training_args.bits}\")\n",
    "    if training_args.bits in [4, 8]:\n",
    "      pass\n",
    "\n",
    "    print(f\"【COND】 vision_tower={model_args.vision_tower}\")\n",
    "    # 【ENTER】 vision_tower=openai/clip-vit-large-patch14-336 なので、この分岐に入る\n",
    "    if model_args.vision_tower is not None:\n",
    "        print(\"【ENTER】if model_args.vision_tower is not None:\")\n",
    "        print(f\"【COND】 mpt_in_model_name_or_path={'mpt' in model_args.model_name_or_path}\")\n",
    "        #【SKIP】model_args.model_name_or_path に mptは含まれていないので、この分岐はskipされる\n",
    "        if 'mpt' in model_args.model_name_or_path:\n",
    "          pass\n",
    "\n",
    "        #【ENTER】 model_args.model_name_or_path に mptは含まれていないので、この分岐に入る\n",
    "        else:\n",
    "            print(\"【COND】 not_mpt_in_model_name_or_path={'mpt' not in model_args.model_name_or_path}\")\n",
    "            print(\"【ENTER】else of if 'mpt' in model_args.model_name_or_path:\")\n",
    "            # PreTrainedModel.from_pretrained\n",
    "            model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "                model_args.model_name_or_path,\n",
    "                cache_dir=training_args.cache_dir,\n",
    "                **bnb_model_from_pretrained_args\n",
    "            )\n",
    "            print(\"model defined as LlavaLlamaForCausalLM \\n\", model)\n",
    "            print(\"【EXIT】else of if 'mpt' in model_args.model_name_or_path:\")\n",
    "        print(\"【EXIT】if model_args.vision_tower is not None:\")\n",
    "    # 【SKIP】 vision_tower=clip-vit-large-patch14-336 なので、この分岐には入らない\n",
    "    else:\n",
    "      pass\n",
    "\n",
    "    print(f\"【COND】 freeze_backbone={model_args.freeze_backbone}\")\n",
    "    # 【SKIP】 freeze_backbone=False なので、この分岐はskipされる\n",
    "    if model_args.freeze_backbone:\n",
    "        pass\n",
    "\n",
    "    # 【SKIP】 bfloat16 なので 以下の if 文はスキップされる\n",
    "    print(f\"【COND】 bits={training_args.bits}\")\n",
    "    if training_args.bits in [4, 8]:\n",
    "      pass\n",
    "\n",
    "    print(f\"【COND】 gradient_checkpointing={training_args.gradient_checkpointing}\")\n",
    "    # 【ENTER】 gradient_checkpointing=True なので、この分岐に入る\n",
    "    if training_args.gradient_checkpointing:\n",
    "        print(\"【ENTER】if training_args.gradient_checkpointing:\")\n",
    "        print(f\"【COND】 has_enable_input_require_grads={hasattr(model, 'enable_input_require_grads')}\")\n",
    "        # 【ENTER】 model に enable_input_require_grads メソッドがあるので、この分岐に入る\n",
    "        if hasattr(model, \"enable_input_require_grads\"):\n",
    "            print(\"【ENTER】if hasattr(model, 'enable_input_require_grads'):\")\n",
    "            # PreTrainedModel.enable_input_require_grads\n",
    "            # 元々 全ての重みについて True\n",
    "            model.enable_input_require_grads()\n",
    "            print(\"【EXIT】if hasattr(model, 'enable_input_require_grads'):\")\n",
    "        # 【SKIP】 model に enable_input_require_grads メソッドがあるので、この分岐はskipされる\n",
    "        else:\n",
    "          pass\n",
    "\n",
    "        print(\"【EXIT】if training_args.gradient_checkpointing:\")\n",
    "\n",
    "    print(f\"【COND】 lora_enable={training_args.lora_enable}\")\n",
    "    # 【SKIP】 lora_enable=False なので、この分岐はskipされる\n",
    "    if training_args.lora_enable:\n",
    "      pass\n",
    "\n",
    "    print(f\"【COND】 mpt_in_model_name_or_path={'mpt' in model_args.model_name_or_path}\")\n",
    "    # 【SKIP】model_args.model_name_or_path に mptは含まれていないので、この分岐はskipされる\n",
    "    if 'mpt' in model_args.model_name_or_path:\n",
    "      pass\n",
    "\n",
    "    #【ENTER】 model_args.model_name_or_path に mptは含まれていないので、この分岐に入る\n",
    "    else:\n",
    "        print(\"【COND】 not_mpt_in_model_name_or_path={'mpt' not in model_args.model_name_or_path}\")\n",
    "        print(\"【ENTER】else of if 'mpt' in model_args.model_name_or_path:\")\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            cache_dir=training_args.cache_dir,\n",
    "            model_max_length=training_args.model_max_length,\n",
    "            padding_side=\"right\",\n",
    "            use_fast=False,\n",
    "        )\n",
    "        print(\"tokenizer defined by AutoTokenizer.from_pretrained \\n\", tokenizer)\n",
    "        print(\"【EXIT】else of if 'mpt' in model_args.model_name_or_path:\")\n",
    "\n",
    "    print(f\"【COND】 version={model_args.version}\")\n",
    "    # 【SKIP】 version=plain なので、この分岐はskipされる\n",
    "    if model_args.version == \"v0\":\n",
    "      pass\n",
    "\n",
    "    # 【SKIP】 version=plain なので、この分岐はskipされる\n",
    "    elif model_args.version == \"v0.5\":\n",
    "      pass\n",
    "    # 【ENTER】 version=plain なので、この分岐に入る\n",
    "    else:\n",
    "        print(\"【ENTER】else of if model_args.version == 'v0' and elif 'v0.5':\")\n",
    "        tokenizer.pad_token = tokenizer.unk_token\n",
    "        print(f\"【COND】 version_in_conv_templates={model_args.version in conv_templates}\")\n",
    "        # 【ENTER】 model_args.version=plain は conversation_lib.conv_templates に含まれている（\"plain\": conv_llava_plain）ので、この分岐に入る\n",
    "        if model_args.version in conv_templates:\n",
    "            print(\"【ENTER】if model_args.version in conversation_lib.conv_templates:\")\n",
    "            default_conversation = conv_templates[model_args.version]\n",
    "            print(f\"conversation_lib.default_conversation set to {model_args.version}\")\n",
    "            print(\"【EXIT】if model_args.version in conversation_lib.conv_templates:\")\n",
    "        # 【SKIP】 model_args.version=plain は conversation_lib.conv_templates に含まれているので、この分岐はskipされる\n",
    "        else:\n",
    "          pass\n",
    "        print(\"【EXIT】else of if model_args.version == 'v0' and elif 'v0.5':\")\n",
    "\n",
    "    print(f\"【COND】 vision_tower={model_args.vision_tower}\")\n",
    "    # 【ENTER】 vision_tower=openai/clip-vit-large-patch14-336 なので、この分岐に入る\n",
    "    if model_args.vision_tower is not None:\n",
    "        print(\"【ENTER】if model_args.vision_tower is not None:\")\n",
    "        model.get_model().initialize_vision_modules(\n",
    "            model_args=model_args,\n",
    "            fsdp=training_args.fsdp\n",
    "        )\n",
    "\n",
    "        vision_tower = model.get_vision_tower()\n",
    "        vision_tower.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)\n",
    "\n",
    "        data_args.image_processor = vision_tower.image_processor\n",
    "        data_args.is_multimodal = True\n",
    "\n",
    "        model.config.image_aspect_ratio = data_args.image_aspect_ratio\n",
    "        model.config.tokenizer_padding_side = tokenizer.padding_side\n",
    "        model.config.tokenizer_model_max_length = tokenizer.model_max_length\n",
    "\n",
    "        model.config.tune_mm_mlp_adapter = training_args.tune_mm_mlp_adapter = model_args.tune_mm_mlp_adapter\n",
    "        print(f\"【COND】 tune_mm_mlp_adapter={model_args.tune_mm_mlp_adapter}\") # True\n",
    "        if model_args.tune_mm_mlp_adapter:\n",
    "            # 【ENTER】 tune_mm_mlp_adapter=True なので、この分岐に入る\n",
    "            print(\"【ENTER】if model_args.tune_mm_mlp_adapter:\")\n",
    "            # モデル全体の全パラメータを「学習不可（requires_grad=False）」にする\n",
    "            # これで通常の重みは全て凍結される\n",
    "            model.requires_grad_(False)\n",
    "            for p in model.get_model().mm_projector.parameters():\n",
    "                # mm_projector（画像特徴量→テキスト特徴量への変換層）の全パラメータだけを「学習可能（requires_grad=True）」に戻す\n",
    "                # これで mm_projector のみ学習されることになる\n",
    "                print(\"model.get_model().mm_projector.parameters()\", model.get_model().mm_projector.parameters())\n",
    "                p.requires_grad = True\n",
    "            print(\"【EXIT】if model_args.tune_mm_mlp_adapter:\")\n",
    "\n",
    "        model.config.freeze_mm_mlp_adapter = training_args.freeze_mm_mlp_adapter\n",
    "        print(f\"【COND】 freeze_mm_mlp_adapter={training_args.freeze_mm_mlp_adapter}\") # False\n",
    "        if training_args.freeze_mm_mlp_adapter:\n",
    "          pass\n",
    "\n",
    "        print(f\"【COND】 bits={training_args.bits}\") # 16\n",
    "        if training_args.bits in [4, 8]:\n",
    "          pass\n",
    "\n",
    "        model.config.mm_use_im_start_end = data_args.mm_use_im_start_end = model_args.mm_use_im_start_end\n",
    "        print(\"model_args.mm_use_im_start_end\", model_args.mm_use_im_start_end)\n",
    "        model.config.mm_projector_lr = training_args.mm_projector_lr\n",
    "        print(\"training_args.mm_projector_lr\", training_args.mm_projector_lr)\n",
    "        training_args.use_im_start_end = model_args.mm_use_im_start_end\n",
    "        print(\"training_args.use_im_start_end\", training_args.use_im_start_end)\n",
    "        model.config.mm_use_im_patch_token = model_args.mm_use_im_patch_token\n",
    "        print(\"model_args.mm_use_im_patch_token\", model_args.mm_use_im_patch_token)\n",
    "        model.initialize_vision_tokenizer(model_args, tokenizer=tokenizer)\n",
    "        print(\"【EXIT】if model_args.vision_tower is not None:\")\n",
    "\n",
    "    print(f\"【COND】 bits={training_args.bits}\") # 16\n",
    "    if training_args.bits in [4, 8]:\n",
    "        pass\n",
    "\n",
    "    data_module = make_supervised_data_module(tokenizer=tokenizer,\n",
    "                                              data_args=data_args)\n",
    "    print(\"data_module\\n\", data_module) # {'train_dataset': <llava.train.train.LazySupervisedDataset object at 0x7ed6341f4880>, 'eval_dataset': None, 'data_collator': DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))}\n",
    "\n",
    "    trainer = LLaVATrainer(model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    args=training_args,\n",
    "                    **data_module)\n",
    "    print(\"trainer\\n\", trainer) # <llava.train.llava_trainer.LLaVATrainer object at 0x7ed6341f4490>\n",
    "\n",
    "    print(\"【COND】list(pathlib.Path(training_args.output_dir).glob('checkpoint-*'))\\n\", list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\"))) # [PosixPath('checkpoints/llava-v1.5-7b-pretrain/checkpoint-250'), PosixPath('checkpoints/llava-v1.5-7b-pretrain/checkpoint-1')]\n",
    "    if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】if list(pathlib.Path(training_args.output_dir).glob(checkpoint-*)):\")\n",
    "        trainer.train(resume_from_checkpoint=False)\n",
    "        print(\"【EXIT】if list(pathlib.Path(training_args.output_dir).glob(checkpoint-*)):\")\n",
    "    else:\n",
    "        print(\"【ENTER】else of if list(pathlib.Path(training_args.output_dir).glob(checkpoint-*)):\")\n",
    "        trainer.train()\n",
    "        print(\"【EXIT】else of if list(pathlib.Path(training_args.output_dir).glob(checkpoint-*)):\")\n",
    "    trainer.save_state()\n",
    "\n",
    "    model.config.use_cache = True\n",
    "    print(\"model.config.use_cache = True\", model.config.use_cache) # True\n",
    "\n",
    "    print(f\"【COND】lora_enable={training_args.lora_enable}\") # False\n",
    "    if training_args.lora_enable:\n",
    "      pass\n",
    "    else:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】else of if training_args.lora_enable:\")\n",
    "        print(\"trainer\", trainer) # <class 'llava.train.llava_trainer.LLaVATrainer'>\n",
    "        safe_save_model_for_hf_trainer(trainer=trainer,\n",
    "                                       output_dir=training_args.output_dir)\n",
    "        print(\"【EXIT】else of if training_args.lora_enable:\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
