[2025-10-05 09:15:39,077] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-05 09:15:41,121] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-10-05 09:15:41,121] [INFO] [runner.py:555:main] cmd = /root/miniconda3/envs/llava/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero2.json --model_name_or_path TinyLlama/TinyLlama-1.1B-Chat-v1.0 --version plain --data_path /content/LLaVA/CC3M_2.json --image_folder /content/LLaVA/images --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --tune_mm_mlp_adapter True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir ./checkpoints/Tinyllava-v1.5-7b-pretrain --num_train_epochs 1 --per_device_train_batch_size 2 --per_device_eval_batch_size 2 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 1 --save_total_limit 1 --learning_rate 1e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 2 --lazy_preprocess True --report_to none
[2025-10-05 09:15:42,508] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-05 09:15:44,495] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.22.3-1+cuda12.5
[2025-10-05 09:15:44,495] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.22.3-1
[2025-10-05 09:15:44,495] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.22.3-1
[2025-10-05 09:15:44,495] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2025-10-05 09:15:44,495] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.22.3-1+cuda12.5
[2025-10-05 09:15:44,495] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2025-10-05 09:15:44,495] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.22.3-1
[2025-10-05 09:15:44,495] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2025-10-05 09:15:44,496] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-10-05 09:15:44,496] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-10-05 09:15:44,496] [INFO] [launch.py:163:main] dist_world_size=1
[2025-10-05 09:15:44,496] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-10-05 09:15:47,344] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
current file path llava/train/train.py
def train()
original parser
 HfArgumentParser(prog='train_mem.py', usage=None, description=None, formatter_class=<class 'argparse.ArgumentDefaultsHelpFormatter'>, conflict_handler='error', add_help=True)
[2025-10-05 09:15:48,023] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2025-10-05 09:15:48,023] [INFO] [comm.py:594:init_distributed] cdb=None
[2025-10-05 09:15:48,023] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
model_args
 ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')
data_args
 DataArguments(data_path='/content/LLaVA/CC3M_2.json', lazy_preprocess=True, is_multimodal=False, image_folder='/content/LLaVA/images', image_aspect_ratio='square')
training_args
 TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
bits=16,
cache_dir=None,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=2,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=./scripts/zero2.json,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=False,
double_quant=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
freeze_mm_mlp_adapter=False,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
group_by_modality_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./checkpoints/Tinyllava-v1.5-7b-pretrain/runs/Oct05_09-15-48_d580ed7cead7,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lora_alpha=16,
lora_bias=none,
lora_dropout=0.05,
lora_enable=False,
lora_r=64,
lora_weight_path=,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mm_projector_lr=None,
model_max_length=2048,
mp_parameters=,
mpt_attn_impl=triton,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=./checkpoints/Tinyllava-v1.5-7b-pretrain,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=2,
per_device_train_batch_size=2,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
quant_type=nf4,
ray_scope=last,
remove_unused_columns=False,
report_to=[],
resume_from_checkpoint=None,
run_name=./checkpoints/Tinyllava-v1.5-7b-pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=1.0,
save_strategy=steps,
save_total_limit=1,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
local_rank
 0
compute_dtype
 torch.bfloat16
bnb_model_from_pretrained_args
 {}
【COND】 bits=16
【COND】 vision_tower=openai/clip-vit-large-patch14-336
【ENTER】if model_args.vision_tower is not None:
【COND】 mpt_in_model_name_or_path=False
【COND】 not_mpt_in_model_name_or_path={'mpt' not in model_args.model_name_or_path}
【ENTER】else of if 'mpt' in model_args.model_name_or_path:
/root/miniconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.__init__(self, config)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
config
 LlavaConfig {
  "_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llava_llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.31.0",
  "use_cache": true,
  "vocab_size": 32000
}

current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaModel.__init__(self, config: LlamaConfig)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaModel'>
config
 LlavaConfig {
  "_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llava_llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.31.0",
  "use_cache": true,
  "vocab_size": 32000
}

current file path llava/model/llava_arch.py
LlavaMetaModel.__init__(self, config)
config
 LlavaConfig {
  "_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llava_llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.31.0",
  "use_cache": true,
  "vocab_size": 32000
}

【COND】 mm_vision_tower=False
self.model
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
)
self.pretraining_tp
 1
self.vocab_size
 32000
self.lm_head
 Linear(in_features=2048, out_features=32000, bias=False)
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0 and are newly initialized: ['model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
model defined as LlavaLlamaForCausalLM 
 LlavaLlamaForCausalLM(
  (model): LlavaLlamaModel(
    (embed_tokens): Embedding(32000, 2048, padding_idx=0)
    (layers): ModuleList(
      (0-21): 22 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (k_proj): Linear(in_features=2048, out_features=256, bias=False)
          (v_proj): Linear(in_features=2048, out_features=256, bias=False)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)
)
【EXIT】else of if 'mpt' in model_args.model_name_or_path:
【EXIT】if model_args.vision_tower is not None:
model.config.use_cache
 False
【COND】 freeze_backbone=False
【COND】 bits=16
【COND】 gradient_checkpointing=True
【ENTER】if training_args.gradient_checkpointing:
【COND】 has_enable_input_require_grads=True
【ENTER】if hasattr(model, 'enable_input_require_grads'):
【EXIT】if hasattr(model, 'enable_input_require_grads'):
【EXIT】if training_args.gradient_checkpointing:
【COND】 lora_enable=False
【COND】 mpt_in_model_name_or_path=False
【COND】 not_mpt_in_model_name_or_path={'mpt' not in model_args.model_name_or_path}
【ENTER】else of if 'mpt' in model_args.model_name_or_path:
tokenizer defined by AutoTokenizer.from_pretrained 
 LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False)}, clean_up_tokenization_spaces=False)
【EXIT】else of if 'mpt' in model_args.model_name_or_path:
【COND】 version=plain
【ENTER】else of if model_args.version == 'v0' and elif 'v0.5':
【COND】 version_in_conv_templates=True
【ENTER】if model_args.version in conversation_lib.conv_templates:
conversation_lib.default_conversation set to plain
【EXIT】if model_args.version in conversation_lib.conv_templates:
【EXIT】else of if model_args.version == 'v0' and elif 'v0.5':
【COND】 vision_tower=openai/clip-vit-large-patch14-336
【ENTER】if model_args.vision_tower is not None:
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
)
current file path llava/model/llava_arch.py
def initialize_vision_modules(self, model_args, fsdp=None)
model_args
 ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')
fsdp
 []
vision_tower from model_args
 openai/clip-vit-large-patch14-336
mm_vision_select_layer from model_args
 -2
mm_vision_select_feature from model_args
 patch
pretrain_mm_mlp_adapter from model_args
 None
self.config.mm_vision_tower
 openai/clip-vit-large-patch14-336
current file path llava/model/llava_arch.py
def get_vision_tower(self)
vision_tower (raw)
 None
type(vision_tower)
 <class 'NoneType'>
【COND】 type_vision_tower_is_list=False
vision_tower (return)
 None
【COND】 self.get_vision_tower()
 None
current file path llava/model/llava_arch.py
def get_vision_tower(self)
vision_tower (raw)
 None
type(vision_tower)
 <class 'NoneType'>
【COND】 type_vision_tower_is_list=False
vision_tower (return)
 None
【COND】 get_vision_tower_is_None=True
current file path llava/model/llava_arch.py
def get_vision_tower(self)
vision_tower (raw)
 None
type(vision_tower)
 <class 'NoneType'>
【COND】 type_vision_tower_is_list=False
vision_tower (return)
 None
【ENTER】if self.get_vision_tower() is None:
[ENTER] self.get_vision_tower() is None
current file path llava/llava/model/multimodal_encoder/builder.py
def build_vision_tower(vision_tower_cfg, **kwargs)
vision_tower_cfg
 ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')
kwargs
 {}
vision_tower from vision_tower_cfg
 openai/clip-vit-large-patch14-336
is_absolute_path_exists
 False
【COND】 is_absolute_path_exists=False vision_tower=openai/clip-vit-large-patch14-336
【ENTER】if is_absolute_path_exists or vision_tower.startswith('openai') or vision_tower.startswith('laion') or 'ShareGPT4V' in vision_tower:
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.__init__(self, vision_tower, args, delay_load=False)
self
 <class 'llava.model.multimodal_encoder.clip_encoder.CLIPVisionTower'>
vision_tower
 openai/clip-vit-large-patch14-336
args
 ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')
delay_load
 False
self.is_loaded
 False
self.vision_tower_name
 openai/clip-vit-large-patch14-336
self.select_layer
 -2
self.select_feature
 patch
【COND】 delay_load=False
【ENTER】if not delay_load:
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.load_model(self)
self
 <class 'llava.model.multimodal_encoder.clip_encoder.CLIPVisionTower'>
self.vision_tower_name
 openai/clip-vit-large-patch14-336
self.image_processor
 CLIPImageProcessor {
  "crop_size": {
    "height": 336,
    "width": 336
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "CLIPFeatureExtractor",
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 336
  }
}

self.vision_tower
 CLIPVisionModel(
  (vision_model): CLIPVisionTransformer(
    (embeddings): CLIPVisionEmbeddings(
      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
      (position_embedding): Embedding(577, 1024)
    )
    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (encoder): CLIPEncoder(
      (layers): ModuleList(
        (0-23): 24 x CLIPEncoderLayer(
          (self_attn): CLIPAttention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): CLIPMLP(
            (activation_fn): QuickGELUActivation()
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
)
self.is_loaded
 True
result (return)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
【EXIT】if is_absolute_path_exists or vision_tower.startswith('openai') or vision_tower.startswith('laion') or 'ShareGPT4V' in vision_tower:
vision_tower after build_vision_tower
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
【COND】 fsdp
 []
【COND】 fsdp_is_not_None=True len_fsdp=0
【COND】 else_fsdp_is_not_None_and_len_fsdp_gt_0=True
【ENTER】else of if fsdp is not None and len(fsdp) > 0:
self.vision_tower
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
【EXIT】else of if fsdp is not None and len(fsdp) > 0:
【EXIT】if self.get_vision_tower() is None:
self.config.use_mm_proj set to True
self.config.mm_projector_type
 mlp2x_gelu
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.hidden_size(self)
self
 <class 'llava.model.multimodal_encoder.clip_encoder.CLIPVisionTower'>
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.config(self)
self
 <class 'llava.model.multimodal_encoder.clip_encoder.CLIPVisionTower'>
self.is_loaded
 True
【COND】 is_loaded=True
【ENTER】if self.is_loaded:
result (return)
 <class 'transformers.models.clip.configuration_clip.CLIPVisionConfig'>
【EXIT】if self.is_loaded:
result (return)
 CLIPVisionConfig {
  "_name_or_path": "openai/clip-vit-large-patch14-336",
  "attention_dropout": 0.0,
  "dropout": 0.0,
  "hidden_act": "quick_gelu",
  "hidden_size": 1024,
  "image_size": 336,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "model_type": "clip_vision_model",
  "num_attention_heads": 16,
  "num_channels": 3,
  "num_hidden_layers": 24,
  "patch_size": 14,
  "projection_dim": 768,
  "transformers_version": "4.31.0"
}

result (return), self.config.hidden_size
 1024
self.config.mm_hidden_size
 1024
self.config.mm_vision_select_layer
 -2
self.config.mm_vision_select_feature
 patch
self.config.mm_patch_merge_type
 flat
【COND】 mm_projector_is_None=True
【ENTER】if getattr(self, 'mm_projector', None) is None:
current file path llava/llava/model/multimodal_projector/builder.py
def build_vision_projector(config, delay_load=False, **kwargs)
config
 LlavaConfig {
  "_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "mm_hidden_size": 1024,
  "mm_patch_merge_type": "flat",
  "mm_projector_type": "mlp2x_gelu",
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14-336",
  "model_type": "llava_llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.31.0",
  "use_cache": false,
  "use_mm_proj": true,
  "vocab_size": 32000
}

delay_load
 False
kwargs
 {}
projector_type from config
 mlp2x_gelu
【COND】 projector_type
 mlp2x_gelu
【COND】mlp_gelu_match
 <re.Match object; span=(0, 10), match='mlp2x_gelu'>
【ENTER】if mlp_gelu_match:
mlp_depth from mlp_gelu_match.group(1)
 2
modules after first Linear
 [Linear(in_features=1024, out_features=2048, bias=True)]
modules before Sequential
 [Linear(in_features=1024, out_features=2048, bias=True), GELU(approximate='none'), Linear(in_features=2048, out_features=2048, bias=True)]
result (return)
 Sequential(
  (0): Linear(in_features=1024, out_features=2048, bias=True)
  (1): GELU(approximate='none')
  (2): Linear(in_features=2048, out_features=2048, bias=True)
)
【EXIT】if mlp_gelu_match:
self.mm_projector after build_vision_projector
 Sequential(
  (0): Linear(in_features=1024, out_features=2048, bias=True)
  (1): GELU(approximate='none')
  (2): Linear(in_features=2048, out_features=2048, bias=True)
)
mm_patch_merge_type
 flat
【COND】 unpad_in_mm_patch_merge_type=False
【EXIT】if getattr(self, 'mm_projector', None) is None:
【COND】 pretrain_mm_mlp_adapter_is_not_None=False
current file path llava/model/llava_arch.py
class LlavaMetaForCausalLM(ABC).get_vision_tower(self)
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=2048, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=2048, out_features=2048, bias=True)
  )
)
current file path llava/model/llava_arch.py
def get_vision_tower(self)
vision_tower (raw)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
type(vision_tower)
 <class 'llava.model.multimodal_encoder.clip_encoder.CLIPVisionTower'>
【COND】 type_vision_tower_is_list=False
vision_tower (return)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
LlavaMetaForCausalLM(ABC).get_vision_tower(self) result (return)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
【COND】 tune_mm_mlp_adapter=True
【ENTER】if model_args.tune_mm_mlp_adapter:
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=2048, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=2048, out_features=2048, bias=True)
  )
)
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=2048, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=2048, out_features=2048, bias=True)
  )
)
model.get_model().mm_projector.parameters() <generator object Module.parameters at 0x7a463827c350>
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=2048, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=2048, out_features=2048, bias=True)
  )
)
model.get_model().mm_projector.parameters() <generator object Module.parameters at 0x7a463827c350>
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=2048, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=2048, out_features=2048, bias=True)
  )
)
model.get_model().mm_projector.parameters() <generator object Module.parameters at 0x7a463827c350>
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=2048, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=2048, out_features=2048, bias=True)
  )
)
model.get_model().mm_projector.parameters() <generator object Module.parameters at 0x7a463827c350>
【EXIT】if model_args.tune_mm_mlp_adapter:
【COND】 freeze_mm_mlp_adapter=False
【COND】 bits=16
model_args.mm_use_im_start_end False
training_args.mm_projector_lr None
training_args.use_im_start_end False
model_args.mm_use_im_patch_token False
current file path llava/model/llava_arch.py
def initialize_vision_tokenizer(self, model_args, tokenizer)
model_args
 ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')
tokenizer
 LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)
【COND】 mm_use_im_patch_token=False
【EXIT】if model_args.vision_tower is not None:
【COND】 bits=16
current file path llava/train/train.py
def make_supervised_data_module(tokenizer, data_args)
tokenizer
 <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>
data_args
 DataArguments(data_path='/content/LLaVA/CC3M_2.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/LLaVA/images', image_aspect_ratio='square')
current file path llava/train/train.py
def LazySupervisedDataset.__init__(self, data_path, tokenizer, data_args)
data_path
 /content/LLaVA/CC3M_2.json
tokenizer
 <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>
data_args
 DataArguments(data_path='/content/LLaVA/CC3M_2.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/LLaVA/images', image_aspect_ratio='square')
list_data_dict [{'id': 'GCC_train_000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': '<image>\nShare a concise interpretation of the image provided.'}, {'from': 'gpt', 'value': 'water pollution in the city'}]}, {'id': 'GCC_train_000196242', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Render a clear and concise summary of the photo.\n<image>'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]}]
current file path llava/train/train.py
def rank0_print(*args)
args
 ('Formatting inputs...Skip in lazy mode',)
Formatting inputs...Skip in lazy mode
self.tokenizer
 LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)
self.list_data_dict
 [{'id': 'GCC_train_000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': '<image>\nShare a concise interpretation of the image provided.'}, {'from': 'gpt', 'value': 'water pollution in the city'}]}, {'id': 'GCC_train_000196242', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Render a clear and concise summary of the photo.\n<image>'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]}]
self.data_args
 DataArguments(data_path='/content/LLaVA/CC3M_2.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/LLaVA/images', image_aspect_ratio='square')
train_dataset
 <llava.train.train.LazySupervisedDataset object at 0x7a4663927010>
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
len(train_dataset)
 2
data_collator
 DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))
def make_supervised_data_module: result (return)
 {'train_dataset': <llava.train.train.LazySupervisedDataset object at 0x7a4663927010>, 'eval_dataset': None, 'data_collator': DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))}
data_module
 {'train_dataset': <llava.train.train.LazySupervisedDataset object at 0x7a4663927010>, 'eval_dataset': None, 'data_collator': DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))}
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
trainer
 <llava.train.llava_trainer.LLaVATrainer object at 0x7a4663927070>
【COND】list(pathlib.Path(training_args.output_dir).glob('checkpoint-*'))
 [PosixPath('checkpoints/Tinyllava-v1.5-7b-pretrain/checkpoint-1'), PosixPath('checkpoints/Tinyllava-v1.5-7b-pretrain/checkpoint-2')]
【ENTER】if list(pathlib.Path(training_args.output_dir).glob(checkpoint-*)):
current file path llava/train/llava_trainer.py
def _get_train_sampler(self)
self
 <llava.train.llava_trainer.LLaVATrainer object at 0x7a4663927070>
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
【COND】 train_dataset_is_None=False, has_length=True
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
【COND】 group_by_modality_length=False
【ENTER】else (not group_by_modality_length):
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
result, super()._get_train_sampler()
 <torch.utils.data.sampler.RandomSampler object at 0x7a4638270700>
【EXIT】else (not group_by_modality_length):
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/llava_trainer.py
def create_optimizer(self)
self
 <llava.train.llava_trainer.LLaVATrainer object at 0x7a4663927070>
【COND】 sagemaker_mp_enabled=False
【COND】 sharded_ddp=None, SHARDED_DDP_SIMPLE=simple
opt_model
 LlavaLlamaForCausalLM(
  (model): LlavaLlamaModel(
    (embed_tokens): Embedding(32000, 2048, padding_idx=0)
    (layers): ModuleList(
      (0-21): 22 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (k_proj): Linear(in_features=2048, out_features=256, bias=False)
          (v_proj): Linear(in_features=2048, out_features=256, bias=False)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (vision_tower): CLIPVisionTower(
      (vision_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=2048, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=2048, out_features=2048, bias=True)
    )
  )
  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)
)
【COND】 optimizer_is_None=True
【ENTER】if self.optimizer is None:
print(risk): print(self.args) disabled for safety
print(risk): print(opt_model) disabled for safety
print(risk): print(optimizer_grouped_parameters) disabled for safety
print(risk): print(optimizer_cls) disabled for safety
print(risk): print(optimizer_kwargs) disabled for safety
ALL_LAYERNORM_LAYERS
 [<class 'torch.nn.modules.normalization.LayerNorm'>]
decay_parameters (before removing bias)
 ['model.embed_tokens.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.mlp.up_proj.bias', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.down_proj.bias', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.mlp.up_proj.bias', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.mlp.up_proj.bias', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.mlp.up_proj.bias', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.mlp.up_proj.bias', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.mlp.up_proj.bias', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.mlp.up_proj.bias', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.bias', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.mlp.up_proj.bias', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.bias', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.mlp.up_proj.bias', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.mlp.up_proj.bias', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.input_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.mlp.up_proj.bias', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.mlp.up_proj.bias', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.down_proj.bias', 'model.layers.11.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.mlp.up_proj.bias', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.down_proj.bias', 'model.layers.12.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.mlp.up_proj.bias', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.mlp.up_proj.bias', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.down_proj.bias', 'model.layers.14.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.mlp.up_proj.bias', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.down_proj.bias', 'model.layers.15.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.mlp.up_proj.bias', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.down_proj.bias', 'model.layers.16.input_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.mlp.up_proj.bias', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.down_proj.bias', 'model.layers.17.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.mlp.up_proj.bias', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.down_proj.bias', 'model.layers.18.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.mlp.up_proj.bias', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.input_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.mlp.up_proj.bias', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.down_proj.bias', 'model.layers.20.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.mlp.up_proj.bias', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.down_proj.bias', 'model.layers.21.input_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.norm.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.mm_projector.0.weight', 'model.mm_projector.0.bias', 'model.mm_projector.2.weight', 'model.mm_projector.2.bias', 'lm_head.weight', 'lm_head.bias']
decay_parameters
 ['model.embed_tokens.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.norm.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.mm_projector.0.weight', 'model.mm_projector.2.weight', 'lm_head.weight']
【COND】 mm_projector_lr=None
【ENTER】else (mm_projector_lr is None):
optimizer_cls
 <class 'torch.optim.adamw.AdamW'>
optimizer_kwargs
 {'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08}
【COND】 sharded_ddp=None, SHARDED_DDP_SIMPLE=simple
【ENTER】else (not sharded_ddp SIMPLE):
【COND】 optimizer_cls_name=AdamW
【EXIT】if self.optimizer is None:
self.optimizer
 AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
current file path llava/train/llava_trainer.py
def create_optimizer(self)
self
 <llava.train.llava_trainer.LLaVATrainer object at 0x7a4663927070>
【COND】 sagemaker_mp_enabled=False
【COND】 sharded_ddp=None, SHARDED_DDP_SIMPLE=simple
opt_model
 LlavaLlamaForCausalLM(
  (model): LlavaLlamaModel(
    (embed_tokens): Embedding(32000, 2048, padding_idx=0)
    (layers): ModuleList(
      (0-21): 22 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (k_proj): Linear(in_features=2048, out_features=256, bias=False)
          (v_proj): Linear(in_features=2048, out_features=256, bias=False)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (vision_tower): CLIPVisionTower(
      (vision_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=2048, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=2048, out_features=2048, bias=True)
    )
  )
  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)
)
【COND】 optimizer_is_None=False
self.optimizer
 AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.0
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.0
    maximize: False
    weight_decay: 0.0
)
Rank: 0 partition count [1, 1] and sizes[(6291456, False), (4096, False)] 

  0%|          | 0/1 [00:00<?, ?it/s]current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__getitem__(self, i)
i
 0
sources
 {'id': 'GCC_train_000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': '<image>\nShare a concise interpretation of the image provided.'}, {'from': 'gpt', 'value': 'water pollution in the city'}]}
【COND】 isinstance(i, int): True
【ENTER】if isinstance(i, int):
sources (after)
 [{'id': 'GCC_train_000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': '<image>\nShare a concise interpretation of the image provided.'}, {'from': 'gpt', 'value': 'water pollution in the city'}]}]
【EXIT】if isinstance(i, int):
【COND】 'image' in sources[0]: True
【ENTER】if 'image' in sources[0]:
image_file
 GCC_train_000406392.jpg
image_folder
 /content/LLaVA/images
processor
 CLIPImageProcessor {
  "crop_size": {
    "height": 336,
    "width": 336
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "CLIPFeatureExtractor",
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 336
  }
}

image_path
 /content/LLaVA/images/GCC_train_000406392.jpg
Trying to open image...
Image opened successfully.
【COND】 self.data_args.image_aspect_ratio square
【ENTER】else (self.data_args.image_aspect_ratio != 'pad')
image (before)
 <PIL.Image.Image image mode=RGB size=224x224 at 0x7A4638242260>
image (after processor.preprocess)
 tensor([[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],
         [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],
         [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],
         ...,
         [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],

        [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],
         [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],
         [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],
         ...,
         [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],

        [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],
         [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],
         [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],
         ...,
         [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],
         [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],
         [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]])
sources (before preprocess_multimodal)
 [{'id': 'GCC_train_000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': '<image>\nShare a concise interpretation of the image provided.'}, {'from': 'gpt', 'value': 'water pollution in the city'}]}]
current file path llava/train/train.py
def preprocess_multimodal(sources, data_args)
sources
 [[{'from': 'human', 'value': '<image>\nShare a concise interpretation of the image provided.'}, {'from': 'gpt', 'value': 'water pollution in the city'}]]
data_args
 DataArguments(data_path='/content/LLaVA/CC3M_2.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/LLaVA/images', image_aspect_ratio='square')
is_multimodal
 True
source current loop
 [{'from': 'human', 'value': '<image>\nShare a concise interpretation of the image provided.'}, {'from': 'gpt', 'value': 'water pollution in the city'}]
sentence current loop
 {'from': 'human', 'value': '<image>\nShare a concise interpretation of the image provided.'}
【COND】 if DEFAULT_IMAGE_TOKEN in sentence['value']: True
sentence['value']
 <image>
Share a concise interpretation of the image provided.
DEFAULT_IMAGE_TOKEN
 <image>
【ENTER】if DEFAULT_IMAGE_TOKEN in sentence['value']:
sentence current loop
 {'from': 'gpt', 'value': 'water pollution in the city'}
【COND】 if DEFAULT_IMAGE_TOKEN in sentence['value']: False
sentence['value']
 water pollution in the city
DEFAULT_IMAGE_TOKEN
 <image>
sources (final return)
 [[{'from': 'human', 'value': '<image>\nShare a concise interpretation of the image provided.'}, {'from': 'gpt', 'value': 'water pollution in the city'}]]
sources (after preprocess_multimodal)
 [[{'from': 'human', 'value': '<image>\nShare a concise interpretation of the image provided.'}, {'from': 'gpt', 'value': 'water pollution in the city'}]]
Calling preprocess...
current file path llava/train/train.py
def preprocess(sources, tokenizer, has_image=False)
sources
 [[{'from': 'human', 'value': '<image>\nShare a concise interpretation of the image provided.'}, {'from': 'gpt', 'value': 'water pollution in the city'}]]
tokenizer
 <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>
has_image
 True
current file path llava/train/train.py
def preprocess_plain(sources, tokenizer)
sources
 [[{'from': 'human', 'value': '<image>\nShare a concise interpretation of the image provided.'}, {'from': 'gpt', 'value': 'water pollution in the city'}]]
tokenizer
 <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>
conversations initial
 []
source current loop
 [{'from': 'human', 'value': '<image>\nShare a concise interpretation of the image provided.'}, {'from': 'gpt', 'value': 'water pollution in the city'}]
conversation current loop
 <image>water pollution in the city

conversations (final)
 ['<image>water pollution in the city\n']
current file path llava/mm_utils.py
def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None)
prompt
 <image>water pollution in the city

tokenizer
 LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)
image_token_index
 -200
return_tensors
 pt
input_ids
 [tensor([    1,  -200,  4094, 21180,   918,   297,   278,  4272,    13])]
input_ids[0].shape
 torch.Size([9])
targets
 [tensor([    1,  -200,  4094, 21180,   918,   297,   278,  4272,    13])]
targets[0].shape
 torch.Size([9])
sources
 [[{'from': 'human', 'value': '<image>'}, {'from': 'gpt', 'value': 'water pollution in the city'}]]
current file path llava/mm_utils.py
def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None)
prompt
 <image>
tokenizer
 LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)
image_token_index
 -200
return_tensors
 None
input_ids (return)
 [1, -200]
input_ids (return)
 [tensor([    1,  -200,  4094, 21180,   918,   297,   278,  4272,    13])]
targets (return)
 [tensor([ -100,  -100,  4094, 21180,   918,   297,   278,  4272,    13])]
data_dict (after preprocess)
 {'input_ids': [tensor([    1,  -200,  4094, 21180,   918,   297,   278,  4272,    13])], 'labels': [tensor([ -100,  -100,  4094, 21180,   918,   297,   278,  4272,    13])]}
【COND】 isinstance(i, int): True
【ENTER】if isinstance(i, int) is True
data_dict
 {'input_ids': tensor([    1,  -200,  4094, 21180,   918,   297,   278,  4272,    13]), 'labels': tensor([ -100,  -100,  4094, 21180,   918,   297,   278,  4272,    13])}
【EXIT】if isinstance(i, int) is True
【COND】'image' in self.list_data_dict[i]: True
【ENTER】if 'image' in self.list_data_dict[i] is True
data_dict
 {'input_ids': tensor([    1,  -200,  4094, 21180,   918,   297,   278,  4272,    13]), 'labels': tensor([ -100,  -100,  4094, 21180,   918,   297,   278,  4272,    13]), 'image': tensor([[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],
         [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],
         [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],
         ...,
         [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],

        [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],
         [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],
         [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],
         ...,
         [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],

        [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],
         [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],
         [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],
         ...,
         [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],
         [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],
         [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]])}
【EXIT】if 'image' in self.list_data_dict[i] is True
data_dict (return)
 {'input_ids': tensor([    1,  -200,  4094, 21180,   918,   297,   278,  4272,    13]), 'labels': tensor([ -100,  -100,  4094, 21180,   918,   297,   278,  4272,    13]), 'image': tensor([[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],
         [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],
         [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],
         ...,
         [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],

        [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],
         [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],
         [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],
         ...,
         [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],

        [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],
         [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],
         [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],
         ...,
         [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],
         [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],
         [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]])}
current file path llava/train/train.py
def LazySupervisedDataset.__getitem__(self, i)
i
 1
sources
 {'id': 'GCC_train_000196242', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Render a clear and concise summary of the photo.\n<image>'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]}
【COND】 isinstance(i, int): True
【ENTER】if isinstance(i, int):
sources (after)
 [{'id': 'GCC_train_000196242', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Render a clear and concise summary of the photo.\n<image>'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]}]
【EXIT】if isinstance(i, int):
【COND】 'image' in sources[0]: True
【ENTER】if 'image' in sources[0]:
image_file
 GCC_train_000406392.jpg
image_folder
 /content/LLaVA/images
processor
 CLIPImageProcessor {
  "crop_size": {
    "height": 336,
    "width": 336
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "CLIPFeatureExtractor",
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 336
  }
}

image_path
 /content/LLaVA/images/GCC_train_000406392.jpg
Trying to open image...
Image opened successfully.
【COND】 self.data_args.image_aspect_ratio square
【ENTER】else (self.data_args.image_aspect_ratio != 'pad')
image (before)
 <PIL.Image.Image image mode=RGB size=224x224 at 0x7A46D17C69B0>
image (after processor.preprocess)
 tensor([[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],
         [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],
         [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],
         ...,
         [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],

        [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],
         [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],
         [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],
         ...,
         [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],

        [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],
         [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],
         [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],
         ...,
         [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],
         [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],
         [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]])
sources (before preprocess_multimodal)
 [{'id': 'GCC_train_000196242', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Render a clear and concise summary of the photo.\n<image>'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]}]
current file path llava/train/train.py
def preprocess_multimodal(sources, data_args)
sources
 [[{'from': 'human', 'value': 'Render a clear and concise summary of the photo.\n<image>'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]]
data_args
 DataArguments(data_path='/content/LLaVA/CC3M_2.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/LLaVA/images', image_aspect_ratio='square')
is_multimodal
 True
source current loop
 [{'from': 'human', 'value': 'Render a clear and concise summary of the photo.\n<image>'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]
sentence current loop
 {'from': 'human', 'value': 'Render a clear and concise summary of the photo.\n<image>'}
【COND】 if DEFAULT_IMAGE_TOKEN in sentence['value']: True
sentence['value']
 Render a clear and concise summary of the photo.
<image>
DEFAULT_IMAGE_TOKEN
 <image>
【ENTER】if DEFAULT_IMAGE_TOKEN in sentence['value']:
sentence current loop
 {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}
【COND】 if DEFAULT_IMAGE_TOKEN in sentence['value']: False
sentence['value']
 illustration of a summer holiday in bright colors .
DEFAULT_IMAGE_TOKEN
 <image>
sources (final return)
 [[{'from': 'human', 'value': '<image>\nRender a clear and concise summary of the photo.'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]]
sources (after preprocess_multimodal)
 [[{'from': 'human', 'value': '<image>\nRender a clear and concise summary of the photo.'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]]
Calling preprocess...
current file path llava/train/train.py
def preprocess(sources, tokenizer, has_image=False)
sources
 [[{'from': 'human', 'value': '<image>\nRender a clear and concise summary of the photo.'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]]
tokenizer
 <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>
has_image
 True
current file path llava/train/train.py
def preprocess_plain(sources, tokenizer)
sources
 [[{'from': 'human', 'value': '<image>\nRender a clear and concise summary of the photo.'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]]
tokenizer
 <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>
conversations initial
 []
source current loop
 [{'from': 'human', 'value': '<image>\nRender a clear and concise summary of the photo.'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]
conversation current loop
 <image>illustration of a summer holiday in bright colors .

conversations (final)
 ['<image>illustration of a summer holiday in bright colors .\n']
current file path llava/mm_utils.py
def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None)
prompt
 <image>illustration of a summer holiday in bright colors .

tokenizer
 LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)
image_token_index
 -200
return_tensors
 pt
input_ids
 [tensor([    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,
        11785, 11955,   869,    13])]
input_ids[0].shape
 torch.Size([14])
targets
 [tensor([    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,
        11785, 11955,   869,    13])]
targets[0].shape
 torch.Size([14])
sources
 [[{'from': 'human', 'value': '<image>'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]]
current file path llava/mm_utils.py
def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None)
prompt
 <image>
tokenizer
 LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)
image_token_index
 -200
return_tensors
 None
input_ids (return)
 [1, -200]
input_ids (return)
 [tensor([    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,
        11785, 11955,   869,    13])]
targets (return)
 [tensor([ -100,  -100,  8632,   362,   310,   263, 11801,  8753, 22394,   297,
        11785, 11955,   869,    13])]
data_dict (after preprocess)
 {'input_ids': [tensor([    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,
        11785, 11955,   869,    13])], 'labels': [tensor([ -100,  -100,  8632,   362,   310,   263, 11801,  8753, 22394,   297,
        11785, 11955,   869,    13])]}
【COND】 isinstance(i, int): True
【ENTER】if isinstance(i, int) is True
data_dict
 {'input_ids': tensor([    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,
        11785, 11955,   869,    13]), 'labels': tensor([ -100,  -100,  8632,   362,   310,   263, 11801,  8753, 22394,   297,
        11785, 11955,   869,    13])}
【EXIT】if isinstance(i, int) is True
【COND】'image' in self.list_data_dict[i]: True
【ENTER】if 'image' in self.list_data_dict[i] is True
data_dict
 {'input_ids': tensor([    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,
        11785, 11955,   869,    13]), 'labels': tensor([ -100,  -100,  8632,   362,   310,   263, 11801,  8753, 22394,   297,
        11785, 11955,   869,    13]), 'image': tensor([[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],
         [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],
         [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],
         ...,
         [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],

        [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],
         [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],
         [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],
         ...,
         [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],

        [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],
         [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],
         [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],
         ...,
         [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],
         [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],
         [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]])}
【EXIT】if 'image' in self.list_data_dict[i] is True
data_dict (return)
 {'input_ids': tensor([    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,
        11785, 11955,   869,    13]), 'labels': tensor([ -100,  -100,  8632,   362,   310,   263, 11801,  8753, 22394,   297,
        11785, 11955,   869,    13]), 'image': tensor([[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],
         [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],
         [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],
         ...,
         [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],

        [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],
         [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],
         [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],
         ...,
         [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],

        [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],
         [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],
         [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],
         ...,
         [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],
         [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],
         [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]])}
current file path llava/train/train.py
def DataCollatorForSupervisedDataset.__call__(self, instances)
instances
 [{'input_ids': tensor([    1,  -200,  4094, 21180,   918,   297,   278,  4272,    13]), 'labels': tensor([ -100,  -100,  4094, 21180,   918,   297,   278,  4272,    13]), 'image': tensor([[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],
         [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],
         [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],
         ...,
         [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],

        [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],
         [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],
         [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],
         ...,
         [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],

        [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],
         [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],
         [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],
         ...,
         [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],
         [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],
         [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]])}, {'input_ids': tensor([    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,
        11785, 11955,   869,    13]), 'labels': tensor([ -100,  -100,  8632,   362,   310,   263, 11801,  8753, 22394,   297,
        11785, 11955,   869,    13]), 'image': tensor([[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],
         [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],
         [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],
         ...,
         [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],

        [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],
         [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],
         [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],
         ...,
         [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],

        [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],
         [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],
         [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],
         ...,
         [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],
         [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],
         [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]])}]
shape of each instance's input_ids and labels, and images(if any): [(torch.Size([9]), torch.Size([9]), torch.Size([3, 336, 336])), (torch.Size([14]), torch.Size([14]), torch.Size([3, 336, 336]))]
self.tokenizer.pad_token_id
 0
IGNORE_INDEX
 -100
input_ids.shape (after pad_sequence and truncate)
 torch.Size([2, 14])
input_ids (after pad_sequence and truncate)
 tensor([[    1,  -200,  4094, 21180,   918,   297,   278,  4272,    13,     0,
             0,     0,     0,     0],
        [    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,
         11785, 11955,   869,    13]])
labels.shape (after pad_sequence and truncate)
 torch.Size([2, 14])
labels (after pad_sequence and truncate)
 tensor([[ -100,  -100,  4094, 21180,   918,   297,   278,  4272,    13,  -100,
          -100,  -100,  -100,  -100],
        [ -100,  -100,  8632,   362,   310,   263, 11801,  8753, 22394,   297,
         11785, 11955,   869,    13]])
batch['images'].shape
 torch.Size([2, 3, 336, 336])
batch (return)
 {'input_ids': tensor([[    1,  -200,  4094, 21180,   918,   297,   278,  4272,    13,     0,
             0,     0,     0,     0],
        [    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,
         11785, 11955,   869,    13]]), 'labels': tensor([[ -100,  -100,  4094, 21180,   918,   297,   278,  4272,    13,  -100,
          -100,  -100,  -100,  -100],
        [ -100,  -100,  8632,   362,   310,   263, 11801,  8753, 22394,   297,
         11785, 11955,   869,    13]]), 'attention_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True, False,
         False, False, False, False],
        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True]]), 'images': tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],
          ...,
          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],

         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],
          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],
          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],
          ...,
          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],

         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],
          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],
          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],
          ...,
          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],
          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],
          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]],


        [[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],
          ...,
          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],

         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],
          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],
          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],
          ...,
          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],

         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],
          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],
          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],
          ...,
          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],
          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],
          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])}
shape of each batch's input_ids and labels, and images(if any): [(torch.Size([2, 14]), torch.Size([2, 14]), torch.Size([2, 3, 336, 336]))]
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, image_sizes, return_dict)
input_ids
 tensor([[    1,  -200,  4094, 21180,   918,   297,   278,  4272,    13,     0,
             0,     0,     0,     0],
        [    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,
         11785, 11955,   869,    13]], device='cuda:0')
input_ids.shape
 torch.Size([2, 14])
attention_mask
 tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True, False,
         False, False, False, False],
        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True]], device='cuda:0')
position_ids
 None
past_key_values
 None
inputs_embeds
 None
labels
 tensor([[ -100,  -100,  4094, 21180,   918,   297,   278,  4272,    13,  -100,
          -100,  -100,  -100,  -100],
        [ -100,  -100,  8632,   362,   310,   263, 11801,  8753, 22394,   297,
         11785, 11955,   869,    13]], device='cuda:0')
use_cache
 None
output_attentions
 None
output_hidden_states
 None
images
 tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7109, -0.3613, -0.1279],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3906, -0.1719, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0112,  0.0471,  0.0908],
          ...,
          [-1.0312, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625]],

         [[ 0.3184,  0.3184,  0.3184,  ..., -0.3867, -0.0112,  0.2139],
          [ 0.3184,  0.3184,  0.3184,  ..., -0.0713,  0.1543,  0.3184],
          [ 0.3184,  0.3184,  0.3184,  ...,  0.2891,  0.3633,  0.4395],
          ...,
          [-1.0156, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000]],

         [[ 0.9648,  0.9648,  0.9648,  ...,  0.0981,  0.4531,  0.6680],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.3965,  0.6094,  0.7539],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.7539,  0.8086,  0.8359],
          ...,
          [-0.3711, -0.3848, -0.4004,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3711, -0.3711, -0.3848,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3848, -0.3711, -0.3711,  ..., -0.4277, -0.4277, -0.4277]]],


        [[[ 0.0325,  0.0325,  0.0325,  ..., -0.7109, -0.3613, -0.1279],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3906, -0.1719, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0112,  0.0471,  0.0908],
          ...,
          [-1.0312, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625]],

         [[ 0.3184,  0.3184,  0.3184,  ..., -0.3867, -0.0112,  0.2139],
          [ 0.3184,  0.3184,  0.3184,  ..., -0.0713,  0.1543,  0.3184],
          [ 0.3184,  0.3184,  0.3184,  ...,  0.2891,  0.3633,  0.4395],
          ...,
          [-1.0156, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000]],

         [[ 0.9648,  0.9648,  0.9648,  ...,  0.0981,  0.4531,  0.6680],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.3965,  0.6094,  0.7539],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.7539,  0.8086,  0.8359],
          ...,
          [-0.3711, -0.3848, -0.4004,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3711, -0.3711, -0.3848,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3848, -0.3711, -0.3711,  ..., -0.4277, -0.4277, -0.4277]]]],
       device='cuda:0', dtype=torch.bfloat16)
images.shape
 torch.Size([2, 3, 336, 336])
image_sizes
 None
return_dict
 None
【COND】 inputs_embeds_is_None=True
【ENTER】if inputs_embeds is None:
current file path llava/model/llava_arch.py
def LlavaMetaForCausalLM(ABC).prepare_inputs_labels_for_multimodal(self, input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes=None)
input_ids
 tensor([[    1,  -200,  4094, 21180,   918,   297,   278,  4272,    13,     0,
             0,     0,     0,     0],
        [    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,
         11785, 11955,   869,    13]], device='cuda:0')
position_ids
 None
attention_mask
 tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True, False,
         False, False, False, False],
        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True]], device='cuda:0')
past_key_values
 None
labels
 tensor([[ -100,  -100,  4094, 21180,   918,   297,   278,  4272,    13,  -100,
          -100,  -100,  -100,  -100],
        [ -100,  -100,  8632,   362,   310,   263, 11801,  8753, 22394,   297,
         11785, 11955,   869,    13]], device='cuda:0')
images
 tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7109, -0.3613, -0.1279],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3906, -0.1719, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0112,  0.0471,  0.0908],
          ...,
          [-1.0312, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625]],

         [[ 0.3184,  0.3184,  0.3184,  ..., -0.3867, -0.0112,  0.2139],
          [ 0.3184,  0.3184,  0.3184,  ..., -0.0713,  0.1543,  0.3184],
          [ 0.3184,  0.3184,  0.3184,  ...,  0.2891,  0.3633,  0.4395],
          ...,
          [-1.0156, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000]],

         [[ 0.9648,  0.9648,  0.9648,  ...,  0.0981,  0.4531,  0.6680],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.3965,  0.6094,  0.7539],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.7539,  0.8086,  0.8359],
          ...,
          [-0.3711, -0.3848, -0.4004,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3711, -0.3711, -0.3848,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3848, -0.3711, -0.3711,  ..., -0.4277, -0.4277, -0.4277]]],


        [[[ 0.0325,  0.0325,  0.0325,  ..., -0.7109, -0.3613, -0.1279],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3906, -0.1719, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0112,  0.0471,  0.0908],
          ...,
          [-1.0312, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625]],

         [[ 0.3184,  0.3184,  0.3184,  ..., -0.3867, -0.0112,  0.2139],
          [ 0.3184,  0.3184,  0.3184,  ..., -0.0713,  0.1543,  0.3184],
          [ 0.3184,  0.3184,  0.3184,  ...,  0.2891,  0.3633,  0.4395],
          ...,
          [-1.0156, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000]],

         [[ 0.9648,  0.9648,  0.9648,  ...,  0.0981,  0.4531,  0.6680],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.3965,  0.6094,  0.7539],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.7539,  0.8086,  0.8359],
          ...,
          [-0.3711, -0.3848, -0.4004,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3711, -0.3711, -0.3848,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3848, -0.3711, -0.3711,  ..., -0.4277, -0.4277, -0.4277]]]],
       device='cuda:0', dtype=torch.bfloat16)
image_sizes
 None
current file path llava/model/llava_arch.py
class LlavaMetaForCausalLM(ABC).get_vision_tower(self)
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=2048, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=2048, out_features=2048, bias=True)
  )
)
current file path llava/model/llava_arch.py
def get_vision_tower(self)
vision_tower (raw)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
type(vision_tower)
 <class 'llava.model.multimodal_encoder.clip_encoder.CLIPVisionTower'>
【COND】 type_vision_tower_is_list=False
vision_tower (return)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
LlavaMetaForCausalLM(ABC).get_vision_tower(self) result (return)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
vision_tower
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
【COND】 vision_tower_is_None=False images_is_None=False input_ids_shape_1_eq_1=False
【COND】type(images)
 <class 'torch.Tensor'>
【COND】images.ndim
 4
【ENTER】else of if type(images) is list or images.ndim == 5:
current file path llava/model/llava_arch.py
def LlavaMetaForCausalLM(ABC).encode_images(self, images)
images
 tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7109, -0.3613, -0.1279],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3906, -0.1719, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0112,  0.0471,  0.0908],
          ...,
          [-1.0312, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625]],

         [[ 0.3184,  0.3184,  0.3184,  ..., -0.3867, -0.0112,  0.2139],
          [ 0.3184,  0.3184,  0.3184,  ..., -0.0713,  0.1543,  0.3184],
          [ 0.3184,  0.3184,  0.3184,  ...,  0.2891,  0.3633,  0.4395],
          ...,
          [-1.0156, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000]],

         [[ 0.9648,  0.9648,  0.9648,  ...,  0.0981,  0.4531,  0.6680],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.3965,  0.6094,  0.7539],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.7539,  0.8086,  0.8359],
          ...,
          [-0.3711, -0.3848, -0.4004,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3711, -0.3711, -0.3848,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3848, -0.3711, -0.3711,  ..., -0.4277, -0.4277, -0.4277]]],


        [[[ 0.0325,  0.0325,  0.0325,  ..., -0.7109, -0.3613, -0.1279],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3906, -0.1719, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0112,  0.0471,  0.0908],
          ...,
          [-1.0312, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625]],

         [[ 0.3184,  0.3184,  0.3184,  ..., -0.3867, -0.0112,  0.2139],
          [ 0.3184,  0.3184,  0.3184,  ..., -0.0713,  0.1543,  0.3184],
          [ 0.3184,  0.3184,  0.3184,  ...,  0.2891,  0.3633,  0.4395],
          ...,
          [-1.0156, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000]],

         [[ 0.9648,  0.9648,  0.9648,  ...,  0.0981,  0.4531,  0.6680],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.3965,  0.6094,  0.7539],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.7539,  0.8086,  0.8359],
          ...,
          [-0.3711, -0.3848, -0.4004,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3711, -0.3711, -0.3848,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3848, -0.3711, -0.3711,  ..., -0.4277, -0.4277, -0.4277]]]],
       device='cuda:0', dtype=torch.bfloat16)
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=2048, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=2048, out_features=2048, bias=True)
  )
)
current file path llava/model/llava_arch.py
def get_vision_tower(self)
vision_tower (raw)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
type(vision_tower)
 <class 'llava.model.multimodal_encoder.clip_encoder.CLIPVisionTower'>
【COND】 type_vision_tower_is_list=False
vision_tower (return)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.forward(self, images)
images
 tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7109, -0.3613, -0.1279],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3906, -0.1719, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0112,  0.0471,  0.0908],
          ...,
          [-1.0312, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625]],

         [[ 0.3184,  0.3184,  0.3184,  ..., -0.3867, -0.0112,  0.2139],
          [ 0.3184,  0.3184,  0.3184,  ..., -0.0713,  0.1543,  0.3184],
          [ 0.3184,  0.3184,  0.3184,  ...,  0.2891,  0.3633,  0.4395],
          ...,
          [-1.0156, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000]],

         [[ 0.9648,  0.9648,  0.9648,  ...,  0.0981,  0.4531,  0.6680],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.3965,  0.6094,  0.7539],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.7539,  0.8086,  0.8359],
          ...,
          [-0.3711, -0.3848, -0.4004,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3711, -0.3711, -0.3848,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3848, -0.3711, -0.3711,  ..., -0.4277, -0.4277, -0.4277]]],


        [[[ 0.0325,  0.0325,  0.0325,  ..., -0.7109, -0.3613, -0.1279],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3906, -0.1719, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0112,  0.0471,  0.0908],
          ...,
          [-1.0312, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625]],

         [[ 0.3184,  0.3184,  0.3184,  ..., -0.3867, -0.0112,  0.2139],
          [ 0.3184,  0.3184,  0.3184,  ..., -0.0713,  0.1543,  0.3184],
          [ 0.3184,  0.3184,  0.3184,  ...,  0.2891,  0.3633,  0.4395],
          ...,
          [-1.0156, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000]],

         [[ 0.9648,  0.9648,  0.9648,  ...,  0.0981,  0.4531,  0.6680],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.3965,  0.6094,  0.7539],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.7539,  0.8086,  0.8359],
          ...,
          [-0.3711, -0.3848, -0.4004,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3711, -0.3711, -0.3848,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3848, -0.3711, -0.3711,  ..., -0.4277, -0.4277, -0.4277]]]],
       device='cuda:0', dtype=torch.bfloat16)
images.shape
 torch.Size([2, 3, 336, 336])
【COND】 type_images_is_list=False
【ENTER】else (type(images) is not list):
original images
 tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7109, -0.3613, -0.1279],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3906, -0.1719, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0112,  0.0471,  0.0908],
          ...,
          [-1.0312, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625]],

         [[ 0.3184,  0.3184,  0.3184,  ..., -0.3867, -0.0112,  0.2139],
          [ 0.3184,  0.3184,  0.3184,  ..., -0.0713,  0.1543,  0.3184],
          [ 0.3184,  0.3184,  0.3184,  ...,  0.2891,  0.3633,  0.4395],
          ...,
          [-1.0156, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000]],

         [[ 0.9648,  0.9648,  0.9648,  ...,  0.0981,  0.4531,  0.6680],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.3965,  0.6094,  0.7539],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.7539,  0.8086,  0.8359],
          ...,
          [-0.3711, -0.3848, -0.4004,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3711, -0.3711, -0.3848,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3848, -0.3711, -0.3711,  ..., -0.4277, -0.4277, -0.4277]]],


        [[[ 0.0325,  0.0325,  0.0325,  ..., -0.7109, -0.3613, -0.1279],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3906, -0.1719, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0112,  0.0471,  0.0908],
          ...,
          [-1.0312, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625]],

         [[ 0.3184,  0.3184,  0.3184,  ..., -0.3867, -0.0112,  0.2139],
          [ 0.3184,  0.3184,  0.3184,  ..., -0.0713,  0.1543,  0.3184],
          [ 0.3184,  0.3184,  0.3184,  ...,  0.2891,  0.3633,  0.4395],
          ...,
          [-1.0156, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000]],

         [[ 0.9648,  0.9648,  0.9648,  ...,  0.0981,  0.4531,  0.6680],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.3965,  0.6094,  0.7539],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.7539,  0.8086,  0.8359],
          ...,
          [-0.3711, -0.3848, -0.4004,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3711, -0.3711, -0.3848,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3848, -0.3711, -0.3711,  ..., -0.4277, -0.4277, -0.4277]]]],
       device='cuda:0', dtype=torch.bfloat16)
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.device(self)
self
 <class 'llava.model.multimodal_encoder.clip_encoder.CLIPVisionTower'>
result (return)
 cuda:0
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.dtype(self)
self
 <class 'llava.model.multimodal_encoder.clip_encoder.CLIPVisionTower'>
result (return)
 torch.bfloat16
after process image_forward_outs
 <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.feature_select(self, image_forward_outs)
image_forward_outs
 BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.3828, -0.0547, -0.2070,  ...,  0.5078, -0.7695, -0.0635],
         [ 0.3984, -0.3086,  0.3965,  ...,  0.0371,  0.1406,  0.2930],
         [-0.0039,  1.1406,  1.0625,  ...,  0.1426, -0.1250, -0.7305],
         ...,
         [ 1.9219,  0.9102,  1.6562,  ..., -0.2695, -0.5781,  1.2734],
         [ 0.9805, -0.1562,  1.3281,  ..., -0.5117, -0.9766,  0.3457],
         [ 1.6016,  1.0469,  1.1250,  ..., -0.7148, -0.3203,  1.1328]],

        [[ 0.3828, -0.0547, -0.2070,  ...,  0.5078, -0.7695, -0.0635],
         [ 0.3984, -0.3086,  0.3965,  ...,  0.0371,  0.1406,  0.2930],
         [-0.0039,  1.1406,  1.0625,  ...,  0.1426, -0.1250, -0.7305],
         ...,
         [ 1.9219,  0.9102,  1.6562,  ..., -0.2695, -0.5781,  1.2734],
         [ 0.9805, -0.1562,  1.3281,  ..., -0.5117, -0.9766,  0.3457],
         [ 1.6016,  1.0469,  1.1250,  ..., -0.7148, -0.3203,  1.1328]]],
       device='cuda:0', dtype=torch.bfloat16), pooler_output=tensor([[ 0.7461,  0.0444, -0.4473,  ...,  1.1484, -1.6094, -0.1191],
        [ 0.7461,  0.0444, -0.4473,  ...,  1.1484, -1.6094, -0.1191]],
       device='cuda:0', dtype=torch.bfloat16), hidden_states=(tensor([[[ 0.0342, -0.0408, -0.1670,  ...,  0.3203, -0.1475, -0.0201],
         [-0.1167, -0.0491, -0.2637,  ...,  0.5859, -0.1318, -0.0099],
         [ 0.2451, -0.0437, -0.6133,  ...,  0.3477, -0.1445, -0.0098],
         ...,
         [ 0.1074, -0.0439, -0.1309,  ..., -0.0601, -0.1357, -0.0164],
         [ 0.1797, -0.0452, -0.1074,  ..., -0.0811, -0.1348, -0.0111],
         [ 0.0266, -0.0486, -0.0238,  ..., -0.0114, -0.1445, -0.0155]],

        [[ 0.0342, -0.0408, -0.1670,  ...,  0.3203, -0.1475, -0.0201],
         [-0.1167, -0.0491, -0.2637,  ...,  0.5859, -0.1318, -0.0099],
         [ 0.2451, -0.0437, -0.6133,  ...,  0.3477, -0.1445, -0.0098],
         ...,
         [ 0.1074, -0.0439, -0.1309,  ..., -0.0601, -0.1357, -0.0164],
         [ 0.1797, -0.0452, -0.1074,  ..., -0.0811, -0.1348, -0.0111],
         [ 0.0266, -0.0486, -0.0238,  ..., -0.0114, -0.1445, -0.0155]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.0513,  0.1055, -0.1074,  ...,  0.1152,  0.0723,  0.0320],
         [ 0.0391,  0.0635, -0.1592,  ...,  0.3496, -0.0371, -0.0439],
         [ 0.2480,  0.0557, -0.4141,  ...,  0.1582,  0.0957,  0.1108],
         ...,
         [ 0.0640, -0.0659,  0.0264,  ..., -0.2852, -0.0283, -0.0415],
         [ 0.1729, -0.1162,  0.0181,  ..., -0.2891, -0.0049, -0.0574],
         [ 0.0977,  0.0254, -0.0293,  ..., -0.0508,  0.0107, -0.0073]],

        [[ 0.0513,  0.1055, -0.1074,  ...,  0.1152,  0.0723,  0.0320],
         [ 0.0391,  0.0635, -0.1592,  ...,  0.3496, -0.0371, -0.0439],
         [ 0.2480,  0.0557, -0.4141,  ...,  0.1582,  0.0957,  0.1108],
         ...,
         [ 0.0640, -0.0659,  0.0264,  ..., -0.2852, -0.0283, -0.0415],
         [ 0.1729, -0.1162,  0.0181,  ..., -0.2891, -0.0049, -0.0574],
         [ 0.0977,  0.0254, -0.0293,  ..., -0.0508,  0.0107, -0.0073]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.0417,  0.0078, -0.0894,  ...,  0.0664, -0.0337,  0.0474],
         [ 0.1201,  0.0049, -0.0908,  ...,  0.2324,  0.1152, -0.0432],
         [ 0.2109, -0.0591, -0.2812,  ...,  0.1523,  0.0967,  0.1992],
         ...,
         [-0.0532, -0.1914, -0.0005,  ..., -0.4453, -0.1157, -0.1035],
         [ 0.0127, -0.2812, -0.0063,  ..., -0.4883, -0.0732, -0.1416],
         [ 0.0811,  0.0698, -0.0776,  ..., -0.1523,  0.0593, -0.0081]],

        [[ 0.0417,  0.0078, -0.0894,  ...,  0.0664, -0.0337,  0.0474],
         [ 0.1201,  0.0049, -0.0908,  ...,  0.2324,  0.1152, -0.0432],
         [ 0.2109, -0.0591, -0.2812,  ...,  0.1523,  0.0967,  0.1992],
         ...,
         [-0.0532, -0.1914, -0.0005,  ..., -0.4453, -0.1157, -0.1035],
         [ 0.0127, -0.2812, -0.0063,  ..., -0.4883, -0.0732, -0.1416],
         [ 0.0811,  0.0698, -0.0776,  ..., -0.1523,  0.0593, -0.0081]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0435, -0.0015, -0.0520,  ...,  0.0474,  0.0000,  0.0942],
         [ 0.1836, -0.0381, -0.0181,  ...,  0.2871,  0.2988, -0.0303],
         [ 0.2500,  0.0371, -0.0635,  ...,  0.1914,  0.1504,  0.3477],
         ...,
         [-0.1924, -0.2168, -0.0371,  ..., -0.4121, -0.1562, -0.0535],
         [-0.0479, -0.2734,  0.0381,  ..., -0.4219, -0.1367, -0.2324],
         [-0.0732,  0.2793, -0.1709,  ..., -0.1533,  0.1777,  0.0540]],

        [[-0.0435, -0.0015, -0.0520,  ...,  0.0474,  0.0000,  0.0942],
         [ 0.1836, -0.0381, -0.0181,  ...,  0.2871,  0.2988, -0.0303],
         [ 0.2500,  0.0371, -0.0635,  ...,  0.1914,  0.1504,  0.3477],
         ...,
         [-0.1924, -0.2168, -0.0371,  ..., -0.4121, -0.1562, -0.0535],
         [-0.0479, -0.2734,  0.0381,  ..., -0.4219, -0.1367, -0.2324],
         [-0.0732,  0.2793, -0.1709,  ..., -0.1533,  0.1777,  0.0540]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0791, -0.0142,  0.0145,  ...,  0.0796, -0.0127,  0.1055],
         [ 0.2617,  0.0215, -0.0098,  ...,  0.3125,  0.3438, -0.0250],
         [ 0.0742, -0.2021, -0.0752,  ...,  0.2676,  0.0850,  0.3906],
         ...,
         [-0.2090, -0.3691, -0.1562,  ..., -0.3242, -0.2324, -0.1309],
         [ 0.0347, -0.4668, -0.0566,  ..., -0.3926, -0.2500, -0.2852],
         [-0.0178,  0.3535, -0.1064,  ..., -0.1074,  0.1133,  0.0393]],

        [[-0.0791, -0.0142,  0.0145,  ...,  0.0796, -0.0127,  0.1055],
         [ 0.2617,  0.0215, -0.0098,  ...,  0.3125,  0.3438, -0.0250],
         [ 0.0742, -0.2021, -0.0752,  ...,  0.2676,  0.0850,  0.3906],
         ...,
         [-0.2090, -0.3691, -0.1562,  ..., -0.3242, -0.2324, -0.1309],
         [ 0.0347, -0.4668, -0.0566,  ..., -0.3926, -0.2500, -0.2852],
         [-0.0178,  0.3535, -0.1064,  ..., -0.1074,  0.1133,  0.0393]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0786, -0.0430,  0.0459,  ...,  0.0972,  0.1113,  0.1338],
         [ 0.0850,  0.0142,  0.1514,  ...,  0.2988,  0.2578, -0.1040],
         [-0.0591, -0.2324,  0.0593,  ...,  0.3613,  0.0518,  0.4453],
         ...,
         [-0.1030, -0.3906,  0.0291,  ..., -0.2021, -0.1162, -0.1406],
         [ 0.0723, -0.5820,  0.1592,  ..., -0.3438, -0.2598, -0.1680],
         [ 0.0222,  0.3027, -0.1924,  ..., -0.1250,  0.1357,  0.1660]],

        [[-0.0786, -0.0430,  0.0459,  ...,  0.0972,  0.1113,  0.1338],
         [ 0.0850,  0.0142,  0.1514,  ...,  0.2988,  0.2578, -0.1040],
         [-0.0591, -0.2324,  0.0593,  ...,  0.3613,  0.0518,  0.4453],
         ...,
         [-0.1030, -0.3906,  0.0291,  ..., -0.2021, -0.1162, -0.1406],
         [ 0.0723, -0.5820,  0.1592,  ..., -0.3438, -0.2598, -0.1680],
         [ 0.0222,  0.3027, -0.1924,  ..., -0.1250,  0.1357,  0.1660]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0229,  0.0454,  0.0522,  ...,  0.0542,  0.1299,  0.0645],
         [-0.1387,  0.1514,  0.0610,  ...,  0.4531,  0.2617, -0.1108],
         [-0.0271, -0.1758,  0.0413,  ...,  0.4043, -0.0879,  0.4023],
         ...,
         [-0.2490, -0.3125,  0.1816,  ..., -0.2285, -0.2988, -0.3125],
         [-0.0142, -0.6250,  0.2090,  ..., -0.3066, -0.2734, -0.2246],
         [ 0.0771,  0.3438, -0.2637,  ..., -0.1074,  0.1777,  0.1064]],

        [[-0.0229,  0.0454,  0.0522,  ...,  0.0542,  0.1299,  0.0645],
         [-0.1387,  0.1514,  0.0610,  ...,  0.4531,  0.2617, -0.1108],
         [-0.0271, -0.1758,  0.0413,  ...,  0.4043, -0.0879,  0.4023],
         ...,
         [-0.2490, -0.3125,  0.1816,  ..., -0.2285, -0.2988, -0.3125],
         [-0.0142, -0.6250,  0.2090,  ..., -0.3066, -0.2734, -0.2246],
         [ 0.0771,  0.3438, -0.2637,  ..., -0.1074,  0.1777,  0.1064]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.0332,  0.0159,  0.0554,  ...,  0.0767,  0.0830,  0.0530],
         [-0.3906,  0.0645,  0.0547,  ...,  0.7266,  0.2168, -0.0879],
         [ 0.1533, -0.2070,  0.1934,  ...,  0.6523, -0.2402,  0.0332],
         ...,
         [-0.2412, -0.1270,  0.4062,  ..., -0.0156, -0.3926, -0.0410],
         [ 0.0188, -0.6016,  0.3516,  ..., -0.1050, -0.3242, -0.1162],
         [ 0.1367,  0.2812,  0.0000,  ...,  0.0283,  0.3008,  0.1895]],

        [[ 0.0332,  0.0159,  0.0554,  ...,  0.0767,  0.0830,  0.0530],
         [-0.3906,  0.0645,  0.0547,  ...,  0.7266,  0.2168, -0.0879],
         [ 0.1533, -0.2070,  0.1934,  ...,  0.6523, -0.2402,  0.0332],
         ...,
         [-0.2412, -0.1270,  0.4062,  ..., -0.0156, -0.3926, -0.0410],
         [ 0.0188, -0.6016,  0.3516,  ..., -0.1050, -0.3242, -0.1162],
         [ 0.1367,  0.2812,  0.0000,  ...,  0.0283,  0.3008,  0.1895]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.0083, -0.0354,  0.0620,  ...,  0.1719,  0.0796,  0.0493],
         [-0.3086, -0.0400, -0.0708,  ...,  0.8047,  0.1377, -0.2148],
         [ 0.0767, -0.4121,  0.3398,  ...,  0.8555, -0.1562, -0.1582],
         ...,
         [-0.2363, -0.2852,  0.4414,  ..., -0.1211, -0.5547,  0.2148],
         [-0.1963, -0.6445,  0.3359,  ..., -0.2061, -0.3789, -0.0957],
         [ 0.0078,  0.3105, -0.3223,  ...,  0.0227, -0.0654,  0.0052]],

        [[ 0.0083, -0.0354,  0.0620,  ...,  0.1719,  0.0796,  0.0493],
         [-0.3086, -0.0400, -0.0708,  ...,  0.8047,  0.1377, -0.2148],
         [ 0.0767, -0.4121,  0.3398,  ...,  0.8555, -0.1562, -0.1582],
         ...,
         [-0.2363, -0.2852,  0.4414,  ..., -0.1211, -0.5547,  0.2148],
         [-0.1963, -0.6445,  0.3359,  ..., -0.2061, -0.3789, -0.0957],
         [ 0.0078,  0.3105, -0.3223,  ...,  0.0227, -0.0654,  0.0052]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0281, -0.0154,  0.0244,  ...,  0.1299,  0.0173, -0.0381],
         [-0.1050, -0.1895, -0.1289,  ...,  0.8047,  0.3203, -0.1235],
         [ 0.0591, -0.3496,  0.3574,  ...,  0.8867, -0.2393,  0.0645],
         ...,
         [-0.2422, -0.4062,  0.0859,  ..., -0.3184, -0.6680,  0.3867],
         [-0.3770, -0.7930, -0.0449,  ..., -0.4043, -0.7734,  0.0143],
         [ 0.1416,  0.2490, -0.3672,  ..., -0.1904, -0.0420, -0.0830]],

        [[-0.0281, -0.0154,  0.0244,  ...,  0.1299,  0.0173, -0.0381],
         [-0.1050, -0.1895, -0.1289,  ...,  0.8047,  0.3203, -0.1235],
         [ 0.0591, -0.3496,  0.3574,  ...,  0.8867, -0.2393,  0.0645],
         ...,
         [-0.2422, -0.4062,  0.0859,  ..., -0.3184, -0.6680,  0.3867],
         [-0.3770, -0.7930, -0.0449,  ..., -0.4043, -0.7734,  0.0143],
         [ 0.1416,  0.2490, -0.3672,  ..., -0.1904, -0.0420, -0.0830]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0189, -0.0498,  0.0957,  ...,  0.1191, -0.0156,  0.0889],
         [ 0.0830,  0.1934, -0.1973,  ...,  0.5586,  0.3340,  0.0698],
         [ 0.0640,  0.0352,  0.1406,  ...,  0.8516, -0.0806,  0.0137],
         ...,
         [-0.3262, -0.3281,  0.2539,  ..., -0.1562, -0.6250,  0.3203],
         [-0.3164, -0.5781,  0.2520,  ..., -0.3223, -0.5391,  0.0510],
         [ 0.2285,  0.2441, -0.0898,  ..., -0.1719,  0.0957, -0.1172]],

        [[-0.0189, -0.0498,  0.0957,  ...,  0.1191, -0.0156,  0.0889],
         [ 0.0830,  0.1934, -0.1973,  ...,  0.5586,  0.3340,  0.0698],
         [ 0.0640,  0.0352,  0.1406,  ...,  0.8516, -0.0806,  0.0137],
         ...,
         [-0.3262, -0.3281,  0.2539,  ..., -0.1562, -0.6250,  0.3203],
         [-0.3164, -0.5781,  0.2520,  ..., -0.3223, -0.5391,  0.0510],
         [ 0.2285,  0.2441, -0.0898,  ..., -0.1719,  0.0957, -0.1172]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.0635, -0.0762,  0.0554,  ...,  0.0659,  0.0364,  0.0674],
         [ 0.0530,  0.2256, -0.0469,  ...,  0.5039,  0.5234,  0.0449],
         [ 0.0122,  0.1719,  0.0518,  ...,  0.9219, -0.1289, -0.0669],
         ...,
         [-0.2441, -0.3535,  0.2500,  ...,  0.0176, -0.4980,  0.0693],
         [-0.2227, -0.4277,  0.0967,  ..., -0.2578, -0.4727, -0.3086],
         [-0.0078,  0.1631,  0.3164,  ..., -0.0938,  0.0000, -0.3027]],

        [[ 0.0635, -0.0762,  0.0554,  ...,  0.0659,  0.0364,  0.0674],
         [ 0.0530,  0.2256, -0.0469,  ...,  0.5039,  0.5234,  0.0449],
         [ 0.0122,  0.1719,  0.0518,  ...,  0.9219, -0.1289, -0.0669],
         ...,
         [-0.2441, -0.3535,  0.2500,  ...,  0.0176, -0.4980,  0.0693],
         [-0.2227, -0.4277,  0.0967,  ..., -0.2578, -0.4727, -0.3086],
         [-0.0078,  0.1631,  0.3164,  ..., -0.0938,  0.0000, -0.3027]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0020, -0.1914, -0.1514,  ...,  0.0933, -0.0635,  0.0186],
         [-0.0630,  0.2412, -0.1953,  ...,  0.5039,  0.2969,  0.1963],
         [-0.0469,  0.2158,  0.0605,  ...,  0.9336, -0.1426, -0.0077],
         ...,
         [-0.2891, -0.1025,  0.3086,  ..., -0.1001, -0.5117,  0.0659],
         [-0.0356, -0.5117,  0.1128,  ..., -0.2246, -0.5117, -0.5586],
         [ 0.0117,  0.1719,  0.3262,  ..., -0.2441,  0.0212, -0.1553]],

        [[-0.0020, -0.1914, -0.1514,  ...,  0.0933, -0.0635,  0.0186],
         [-0.0630,  0.2412, -0.1953,  ...,  0.5039,  0.2969,  0.1963],
         [-0.0469,  0.2158,  0.0605,  ...,  0.9336, -0.1426, -0.0077],
         ...,
         [-0.2891, -0.1025,  0.3086,  ..., -0.1001, -0.5117,  0.0659],
         [-0.0356, -0.5117,  0.1128,  ..., -0.2246, -0.5117, -0.5586],
         [ 0.0117,  0.1719,  0.3262,  ..., -0.2441,  0.0212, -0.1553]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0942, -0.1641, -0.0879,  ...,  0.1123, -0.0430,  0.0640],
         [-0.0327,  0.1875,  0.0547,  ...,  0.3848,  0.2520,  0.2051],
         [-0.0786,  0.1602, -0.0996,  ...,  0.7109, -0.2305, -0.2090],
         ...,
         [-0.2236,  0.0317,  0.2451,  ..., -0.0234, -0.0928, -0.0032],
         [ 0.0669, -0.3516,  0.0830,  ..., -0.2871, -0.1035, -0.4531],
         [ 0.2578,  0.2441,  0.3340,  ..., -0.2500,  0.1445, -0.2275]],

        [[-0.0942, -0.1641, -0.0879,  ...,  0.1123, -0.0430,  0.0640],
         [-0.0327,  0.1875,  0.0547,  ...,  0.3848,  0.2520,  0.2051],
         [-0.0786,  0.1602, -0.0996,  ...,  0.7109, -0.2305, -0.2090],
         ...,
         [-0.2236,  0.0317,  0.2451,  ..., -0.0234, -0.0928, -0.0032],
         [ 0.0669, -0.3516,  0.0830,  ..., -0.2871, -0.1035, -0.4531],
         [ 0.2578,  0.2441,  0.3340,  ..., -0.2500,  0.1445, -0.2275]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0820, -0.1104, -0.1094,  ...,  0.1074, -0.0601, -0.0293],
         [ 0.0454,  0.1699,  0.2324,  ...,  0.1045,  0.1836,  0.2070],
         [-0.1177,  0.1514, -0.3008,  ...,  0.3340, -0.1270, -0.1875],
         ...,
         [ 0.1182,  0.0259,  0.3301,  ..., -0.3652,  0.0378,  0.0059],
         [ 0.0874, -0.3770,  0.0889,  ..., -0.5586, -0.0713, -0.2158],
         [ 0.2812,  0.1152,  0.5000,  ..., -0.5352,  0.1758, -0.2021]],

        [[-0.0820, -0.1104, -0.1094,  ...,  0.1074, -0.0601, -0.0293],
         [ 0.0454,  0.1699,  0.2324,  ...,  0.1045,  0.1836,  0.2070],
         [-0.1177,  0.1514, -0.3008,  ...,  0.3340, -0.1270, -0.1875],
         ...,
         [ 0.1182,  0.0259,  0.3301,  ..., -0.3652,  0.0378,  0.0059],
         [ 0.0874, -0.3770,  0.0889,  ..., -0.5586, -0.0713, -0.2158],
         [ 0.2812,  0.1152,  0.5000,  ..., -0.5352,  0.1758, -0.2021]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0850, -0.1621, -0.0791,  ...,  0.1406, -0.0354,  0.0869],
         [ 0.0518,  0.2129,  0.2676,  ..., -0.2246,  0.3535,  0.4434],
         [-0.2930,  0.2490, -0.1445,  ...,  0.1172, -0.0430,  0.0381],
         ...,
         [ 0.3047,  0.0781,  0.3828,  ...,  0.0742, -0.1245,  0.3066],
         [-0.0217, -0.4805,  0.2207,  ..., -0.0217, -0.3438, -0.1699],
         [ 0.3770,  0.0522,  0.5820,  ..., -0.5000,  0.0215,  0.0093]],

        [[-0.0850, -0.1621, -0.0791,  ...,  0.1406, -0.0354,  0.0869],
         [ 0.0518,  0.2129,  0.2676,  ..., -0.2246,  0.3535,  0.4434],
         [-0.2930,  0.2490, -0.1445,  ...,  0.1172, -0.0430,  0.0381],
         ...,
         [ 0.3047,  0.0781,  0.3828,  ...,  0.0742, -0.1245,  0.3066],
         [-0.0217, -0.4805,  0.2207,  ..., -0.0217, -0.3438, -0.1699],
         [ 0.3770,  0.0522,  0.5820,  ..., -0.5000,  0.0215,  0.0093]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0205, -0.1299, -0.1084,  ...,  0.0039, -0.0698,  0.1465],
         [-0.0225,  0.3398,  0.2500,  ...,  0.0012,  0.2656,  0.5078],
         [-0.2617,  0.4570, -0.2578,  ...,  0.2930, -0.0339,  0.0928],
         ...,
         [ 0.4473,  0.2910,  0.2266,  ...,  0.2617, -0.1221,  0.2930],
         [-0.0771, -0.2168,  0.2012,  ...,  0.0508, -0.3828, -0.3789],
         [ 0.5391,  0.2539,  0.5430,  ..., -0.5508,  0.0208,  0.1187]],

        [[-0.0205, -0.1299, -0.1084,  ...,  0.0039, -0.0698,  0.1465],
         [-0.0225,  0.3398,  0.2500,  ...,  0.0012,  0.2656,  0.5078],
         [-0.2617,  0.4570, -0.2578,  ...,  0.2930, -0.0339,  0.0928],
         ...,
         [ 0.4473,  0.2910,  0.2266,  ...,  0.2617, -0.1221,  0.2930],
         [-0.0771, -0.2168,  0.2012,  ...,  0.0508, -0.3828, -0.3789],
         [ 0.5391,  0.2539,  0.5430,  ..., -0.5508,  0.0208,  0.1187]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0437,  0.0947,  0.0215,  ..., -0.1367, -0.2324,  0.0713],
         [-0.0618, -0.1738,  0.2119,  ..., -0.1670,  0.3164,  0.3984],
         [-0.1709,  0.2461, -0.3398,  ...,  0.1079,  0.0957, -0.1016],
         ...,
         [ 0.5195,  0.4844,  0.2080,  ...,  0.1738, -0.0518,  0.2676],
         [ 0.0371, -0.0068,  0.1807,  ..., -0.2188, -0.3809, -0.3965],
         [ 0.5352,  0.2637,  0.4316,  ..., -0.5156, -0.1611,  0.0195]],

        [[-0.0437,  0.0947,  0.0215,  ..., -0.1367, -0.2324,  0.0713],
         [-0.0618, -0.1738,  0.2119,  ..., -0.1670,  0.3164,  0.3984],
         [-0.1709,  0.2461, -0.3398,  ...,  0.1079,  0.0957, -0.1016],
         ...,
         [ 0.5195,  0.4844,  0.2080,  ...,  0.1738, -0.0518,  0.2676],
         [ 0.0371, -0.0068,  0.1807,  ..., -0.2188, -0.3809, -0.3965],
         [ 0.5352,  0.2637,  0.4316,  ..., -0.5156, -0.1611,  0.0195]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.1309,  0.0918, -0.0084,  ..., -0.0732, -0.0283,  0.0757],
         [-0.1328, -0.1660, -0.1748,  ..., -0.2812,  0.2656,  0.4219],
         [-0.2139,  0.2695, -0.3086,  ...,  0.0581,  0.3105, -0.0645],
         ...,
         [ 0.5547,  0.6602,  0.4180,  ...,  0.1318, -0.1729,  0.3066],
         [-0.1582, -0.1221, -0.0352,  ..., -0.2090, -0.3633, -0.5039],
         [ 0.5195,  0.2139,  0.1426,  ..., -0.4883, -0.0781,  0.1553]],

        [[-0.1309,  0.0918, -0.0084,  ..., -0.0732, -0.0283,  0.0757],
         [-0.1328, -0.1660, -0.1748,  ..., -0.2812,  0.2656,  0.4219],
         [-0.2139,  0.2695, -0.3086,  ...,  0.0581,  0.3105, -0.0645],
         ...,
         [ 0.5547,  0.6602,  0.4180,  ...,  0.1318, -0.1729,  0.3066],
         [-0.1582, -0.1221, -0.0352,  ..., -0.2090, -0.3633, -0.5039],
         [ 0.5195,  0.2139,  0.1426,  ..., -0.4883, -0.0781,  0.1553]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.1719,  0.1279, -0.0304,  ...,  0.0122, -0.1260, -0.0078],
         [-0.2539, -0.6641, -0.2754,  ..., -0.0127,  0.6211,  0.1924],
         [-0.5078,  0.1953, -0.1436,  ...,  0.0498,  0.3984, -0.0786],
         ...,
         [ 0.5391,  0.7344,  0.4648,  ...,  0.2910, -0.1973,  0.2656],
         [-0.1177, -0.4492,  0.0469,  ..., -0.1416, -0.4395, -0.3281],
         [ 0.6094,  0.0703,  0.1895,  ..., -0.2100, -0.3301,  0.3477]],

        [[-0.1719,  0.1279, -0.0304,  ...,  0.0122, -0.1260, -0.0078],
         [-0.2539, -0.6641, -0.2754,  ..., -0.0127,  0.6211,  0.1924],
         [-0.5078,  0.1953, -0.1436,  ...,  0.0498,  0.3984, -0.0786],
         ...,
         [ 0.5391,  0.7344,  0.4648,  ...,  0.2910, -0.1973,  0.2656],
         [-0.1177, -0.4492,  0.0469,  ..., -0.1416, -0.4395, -0.3281],
         [ 0.6094,  0.0703,  0.1895,  ..., -0.2100, -0.3301,  0.3477]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.2168,  0.3340, -0.1992,  ..., -0.0198, -0.1553,  0.1934],
         [-0.2363, -0.4023, -0.0713,  ..., -0.1963,  0.4668,  0.2061],
         [-0.2402,  0.4648,  0.2031,  ...,  0.0654,  0.1221, -0.0051],
         ...,
         [ 0.7812,  0.7500,  0.5391,  ...,  0.2324, -0.3184,  0.3301],
         [ 0.2363, -0.5391,  0.1602,  ..., -0.2344, -0.5586, -0.2695],
         [ 1.0938,  0.2598,  0.2852,  ..., -0.2715, -0.5664,  0.6367]],

        [[ 0.2168,  0.3340, -0.1992,  ..., -0.0198, -0.1553,  0.1934],
         [-0.2363, -0.4023, -0.0713,  ..., -0.1963,  0.4668,  0.2061],
         [-0.2402,  0.4648,  0.2031,  ...,  0.0654,  0.1221, -0.0051],
         ...,
         [ 0.7812,  0.7500,  0.5391,  ...,  0.2324, -0.3184,  0.3301],
         [ 0.2363, -0.5391,  0.1602,  ..., -0.2344, -0.5586, -0.2695],
         [ 1.0938,  0.2598,  0.2852,  ..., -0.2715, -0.5664,  0.6367]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.2363,  0.2930, -0.2734,  ...,  0.2393, -0.2148,  0.3535],
         [-0.3105, -0.4004,  0.5352,  ..., -0.0137,  0.5039,  0.1826],
         [ 0.3242,  0.8438,  0.0234,  ..., -0.0195, -0.3809, -0.2500],
         ...,
         [ 1.4531,  1.2656,  1.0312,  ..., -0.4336, -0.7422,  0.4609],
         [ 1.1016, -0.0781,  1.0781,  ..., -0.9961, -0.6758, -0.3945],
         [ 1.7812,  0.8203,  0.9141,  ..., -1.1250, -0.2969,  0.3359]],

        [[ 0.2363,  0.2930, -0.2734,  ...,  0.2393, -0.2148,  0.3535],
         [-0.3105, -0.4004,  0.5352,  ..., -0.0137,  0.5039,  0.1826],
         [ 0.3242,  0.8438,  0.0234,  ..., -0.0195, -0.3809, -0.2500],
         ...,
         [ 1.4531,  1.2656,  1.0312,  ..., -0.4336, -0.7422,  0.4609],
         [ 1.1016, -0.0781,  1.0781,  ..., -0.9961, -0.6758, -0.3945],
         [ 1.7812,  0.8203,  0.9141,  ..., -1.1250, -0.2969,  0.3359]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.1650,  0.3242, -0.2402,  ...,  0.3848, -0.3477,  0.0620],
         [-0.0703, -0.1250,  0.4004,  ...,  0.3242,  0.4727,  0.3945],
         [ 0.1123,  0.8672,  0.5586,  ...,  0.4102, -0.1982, -0.4043],
         ...,
         [ 1.3984,  1.1641,  1.4375,  ...,  0.0234, -0.6484,  1.0391],
         [ 0.9453, -0.0430,  1.3984,  ..., -0.3711, -0.7188, -0.1348],
         [ 1.4844,  0.9219,  0.9219,  ..., -0.6406, -0.3496,  0.8594]],

        [[ 0.1650,  0.3242, -0.2402,  ...,  0.3848, -0.3477,  0.0620],
         [-0.0703, -0.1250,  0.4004,  ...,  0.3242,  0.4727,  0.3945],
         [ 0.1123,  0.8672,  0.5586,  ...,  0.4102, -0.1982, -0.4043],
         ...,
         [ 1.3984,  1.1641,  1.4375,  ...,  0.0234, -0.6484,  1.0391],
         [ 0.9453, -0.0430,  1.3984,  ..., -0.3711, -0.7188, -0.1348],
         [ 1.4844,  0.9219,  0.9219,  ..., -0.6406, -0.3496,  0.8594]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.2178,  0.2402, -0.6055,  ...,  0.4902, -0.5273, -0.2090],
         [-0.0371, -0.2070, -0.0283,  ...,  0.3555,  0.3535,  0.3105],
         [-0.1816,  1.1875,  0.4512,  ...,  0.3730, -0.0486, -0.4531],
         ...,
         [ 1.7031,  0.9688,  0.8516,  ..., -0.2656, -0.8828,  1.2969],
         [ 0.7734, -0.3535,  0.6719,  ..., -0.3320, -1.0078,  0.3242],
         [ 1.4062,  0.6992,  0.3789,  ..., -0.5195, -0.4805,  1.1641]],

        [[ 0.2178,  0.2402, -0.6055,  ...,  0.4902, -0.5273, -0.2090],
         [-0.0371, -0.2070, -0.0283,  ...,  0.3555,  0.3535,  0.3105],
         [-0.1816,  1.1875,  0.4512,  ...,  0.3730, -0.0486, -0.4531],
         ...,
         [ 1.7031,  0.9688,  0.8516,  ..., -0.2656, -0.8828,  1.2969],
         [ 0.7734, -0.3535,  0.6719,  ..., -0.3320, -1.0078,  0.3242],
         [ 1.4062,  0.6992,  0.3789,  ..., -0.5195, -0.4805,  1.1641]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.3828, -0.0547, -0.2070,  ...,  0.5078, -0.7695, -0.0635],
         [ 0.3984, -0.3086,  0.3965,  ...,  0.0371,  0.1406,  0.2930],
         [-0.0039,  1.1406,  1.0625,  ...,  0.1426, -0.1250, -0.7305],
         ...,
         [ 1.9219,  0.9102,  1.6562,  ..., -0.2695, -0.5781,  1.2734],
         [ 0.9805, -0.1562,  1.3281,  ..., -0.5117, -0.9766,  0.3457],
         [ 1.6016,  1.0469,  1.1250,  ..., -0.7148, -0.3203,  1.1328]],

        [[ 0.3828, -0.0547, -0.2070,  ...,  0.5078, -0.7695, -0.0635],
         [ 0.3984, -0.3086,  0.3965,  ...,  0.0371,  0.1406,  0.2930],
         [-0.0039,  1.1406,  1.0625,  ...,  0.1426, -0.1250, -0.7305],
         ...,
         [ 1.9219,  0.9102,  1.6562,  ..., -0.2695, -0.5781,  1.2734],
         [ 0.9805, -0.1562,  1.3281,  ..., -0.5117, -0.9766,  0.3457],
         [ 1.6016,  1.0469,  1.1250,  ..., -0.7148, -0.3203,  1.1328]]],
       device='cuda:0', dtype=torch.bfloat16)), attentions=None)
image_features (after select_layer)
 <class 'torch.Tensor'>
image_features.shape
 torch.Size([2, 577, 1024])
【COND】 select_feature=patch
【ENTER】if self.select_feature == 'patch':
original image_features
 tensor([[[ 0.2178,  0.2402, -0.6055,  ...,  0.4902, -0.5273, -0.2090],
         [-0.0371, -0.2070, -0.0283,  ...,  0.3555,  0.3535,  0.3105],
         [-0.1816,  1.1875,  0.4512,  ...,  0.3730, -0.0486, -0.4531],
         ...,
         [ 1.7031,  0.9688,  0.8516,  ..., -0.2656, -0.8828,  1.2969],
         [ 0.7734, -0.3535,  0.6719,  ..., -0.3320, -1.0078,  0.3242],
         [ 1.4062,  0.6992,  0.3789,  ..., -0.5195, -0.4805,  1.1641]],

        [[ 0.2178,  0.2402, -0.6055,  ...,  0.4902, -0.5273, -0.2090],
         [-0.0371, -0.2070, -0.0283,  ...,  0.3555,  0.3535,  0.3105],
         [-0.1816,  1.1875,  0.4512,  ...,  0.3730, -0.0486, -0.4531],
         ...,
         [ 1.7031,  0.9688,  0.8516,  ..., -0.2656, -0.8828,  1.2969],
         [ 0.7734, -0.3535,  0.6719,  ..., -0.3320, -1.0078,  0.3242],
         [ 1.4062,  0.6992,  0.3789,  ..., -0.5195, -0.4805,  1.1641]]],
       device='cuda:0', dtype=torch.bfloat16)
after process
 tensor([[[-0.0371, -0.2070, -0.0283,  ...,  0.3555,  0.3535,  0.3105],
         [-0.1816,  1.1875,  0.4512,  ...,  0.3730, -0.0486, -0.4531],
         [ 1.0781, -0.1250, -0.3457,  ..., -0.1240,  0.1230,  0.4746],
         ...,
         [ 1.7031,  0.9688,  0.8516,  ..., -0.2656, -0.8828,  1.2969],
         [ 0.7734, -0.3535,  0.6719,  ..., -0.3320, -1.0078,  0.3242],
         [ 1.4062,  0.6992,  0.3789,  ..., -0.5195, -0.4805,  1.1641]],

        [[-0.0371, -0.2070, -0.0283,  ...,  0.3555,  0.3535,  0.3105],
         [-0.1816,  1.1875,  0.4512,  ...,  0.3730, -0.0486, -0.4531],
         [ 1.0781, -0.1250, -0.3457,  ..., -0.1240,  0.1230,  0.4746],
         ...,
         [ 1.7031,  0.9688,  0.8516,  ..., -0.2656, -0.8828,  1.2969],
         [ 0.7734, -0.3535,  0.6719,  ..., -0.3320, -1.0078,  0.3242],
         [ 1.4062,  0.6992,  0.3789,  ..., -0.5195, -0.4805,  1.1641]]],
       device='cuda:0', dtype=torch.bfloat16)
【EXIT】if self.select_feature == 'patch':
image_features (return)
 tensor([[[-0.0371, -0.2070, -0.0283,  ...,  0.3555,  0.3535,  0.3105],
         [-0.1816,  1.1875,  0.4512,  ...,  0.3730, -0.0486, -0.4531],
         [ 1.0781, -0.1250, -0.3457,  ..., -0.1240,  0.1230,  0.4746],
         ...,
         [ 1.7031,  0.9688,  0.8516,  ..., -0.2656, -0.8828,  1.2969],
         [ 0.7734, -0.3535,  0.6719,  ..., -0.3320, -1.0078,  0.3242],
         [ 1.4062,  0.6992,  0.3789,  ..., -0.5195, -0.4805,  1.1641]],

        [[-0.0371, -0.2070, -0.0283,  ...,  0.3555,  0.3535,  0.3105],
         [-0.1816,  1.1875,  0.4512,  ...,  0.3730, -0.0486, -0.4531],
         [ 1.0781, -0.1250, -0.3457,  ..., -0.1240,  0.1230,  0.4746],
         ...,
         [ 1.7031,  0.9688,  0.8516,  ..., -0.2656, -0.8828,  1.2969],
         [ 0.7734, -0.3535,  0.6719,  ..., -0.3320, -1.0078,  0.3242],
         [ 1.4062,  0.6992,  0.3789,  ..., -0.5195, -0.4805,  1.1641]]],
       device='cuda:0', dtype=torch.bfloat16)
image_features.shape
 torch.Size([2, 576, 1024])
after process image_features
 <class 'torch.Tensor'>
【EXIT】else (type(images) is not list):
image_features (return)
 tensor([[[-0.0371, -0.2070, -0.0283,  ...,  0.3555,  0.3535,  0.3105],
         [-0.1816,  1.1875,  0.4512,  ...,  0.3730, -0.0486, -0.4531],
         [ 1.0781, -0.1250, -0.3457,  ..., -0.1240,  0.1230,  0.4746],
         ...,
         [ 1.7031,  0.9688,  0.8516,  ..., -0.2656, -0.8828,  1.2969],
         [ 0.7734, -0.3535,  0.6719,  ..., -0.3320, -1.0078,  0.3242],
         [ 1.4062,  0.6992,  0.3789,  ..., -0.5195, -0.4805,  1.1641]],

        [[-0.0371, -0.2070, -0.0283,  ...,  0.3555,  0.3535,  0.3105],
         [-0.1816,  1.1875,  0.4512,  ...,  0.3730, -0.0486, -0.4531],
         [ 1.0781, -0.1250, -0.3457,  ..., -0.1240,  0.1230,  0.4746],
         ...,
         [ 1.7031,  0.9688,  0.8516,  ..., -0.2656, -0.8828,  1.2969],
         [ 0.7734, -0.3535,  0.6719,  ..., -0.3320, -1.0078,  0.3242],
         [ 1.4062,  0.6992,  0.3789,  ..., -0.5195, -0.4805,  1.1641]]],
       device='cuda:0', dtype=torch.bfloat16)
image_features.shape
 torch.Size([2, 576, 1024])
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=2048, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=2048, out_features=2048, bias=True)
  )
)
image_features (return)
 tensor([[[ 0.0073,  0.1001, -0.1709,  ...,  0.0075, -0.0596,  0.0742],
         [ 0.0933,  0.1211, -0.1621,  ...,  0.0376,  0.0845, -0.1602],
         [-0.1426, -0.0488, -0.0269,  ..., -0.0483,  0.1514,  0.0486],
         ...,
         [-0.1230,  0.1079, -0.0013,  ..., -0.1621,  0.0659, -0.1328],
         [-0.0708,  0.0898, -0.1777,  ..., -0.2363,  0.2207, -0.0564],
         [ 0.0542,  0.0388, -0.0381,  ...,  0.0188, -0.0728, -0.2334]],

        [[ 0.0073,  0.1001, -0.1709,  ...,  0.0075, -0.0596,  0.0742],
         [ 0.0933,  0.1211, -0.1621,  ...,  0.0376,  0.0845, -0.1602],
         [-0.1426, -0.0488, -0.0269,  ..., -0.0483,  0.1514,  0.0486],
         ...,
         [-0.1230,  0.1079, -0.0013,  ..., -0.1621,  0.0659, -0.1328],
         [-0.0708,  0.0898, -0.1777,  ..., -0.2363,  0.2207, -0.0564],
         [ 0.0542,  0.0388, -0.0381,  ...,  0.0188, -0.0728, -0.2334]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ViewBackward0>)
image_features after encode_images
 tensor([[[ 0.0073,  0.1001, -0.1709,  ...,  0.0075, -0.0596,  0.0742],
         [ 0.0933,  0.1211, -0.1621,  ...,  0.0376,  0.0845, -0.1602],
         [-0.1426, -0.0488, -0.0269,  ..., -0.0483,  0.1514,  0.0486],
         ...,
         [-0.1230,  0.1079, -0.0013,  ..., -0.1621,  0.0659, -0.1328],
         [-0.0708,  0.0898, -0.1777,  ..., -0.2363,  0.2207, -0.0564],
         [ 0.0542,  0.0388, -0.0381,  ...,  0.0188, -0.0728, -0.2334]],

        [[ 0.0073,  0.1001, -0.1709,  ...,  0.0075, -0.0596,  0.0742],
         [ 0.0933,  0.1211, -0.1621,  ...,  0.0376,  0.0845, -0.1602],
         [-0.1426, -0.0488, -0.0269,  ..., -0.0483,  0.1514,  0.0486],
         ...,
         [-0.1230,  0.1079, -0.0013,  ..., -0.1621,  0.0659, -0.1328],
         [-0.0708,  0.0898, -0.1777,  ..., -0.2363,  0.2207, -0.0564],
         [ 0.0542,  0.0388, -0.0381,  ...,  0.0188, -0.0728, -0.2334]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ViewBackward0>)
【EXIT】else of if type(images) is list or images.ndim == 5:
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=2048, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=2048, out_features=2048, bias=True)
  )
)
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=2048, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=2048, out_features=2048, bias=True)
  )
)
position_ids (return)
 None
attention_mask (return)
 tensor([[ True,  True,  True,  ..., False, False, False],
        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')
past_key_values (return)
 None
new_input_embeds (return)
 tensor([[[-0.0011,  0.0019, -0.0017,  ...,  0.0002, -0.0007, -0.0005],
         [ 0.0073,  0.1001, -0.1709,  ...,  0.0075, -0.0596,  0.0742],
         [ 0.0933,  0.1211, -0.1621,  ...,  0.0376,  0.0845, -0.1602],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        [[-0.0011,  0.0019, -0.0017,  ...,  0.0002, -0.0007, -0.0005],
         [ 0.0073,  0.1001, -0.1709,  ...,  0.0075, -0.0596,  0.0742],
         [ 0.0933,  0.1211, -0.1621,  ...,  0.0376,  0.0845, -0.1602],
         ...,
         [-0.0293,  0.0125,  0.0122,  ..., -0.0022,  0.0060, -0.0070],
         [-0.0014,  0.0050,  0.0133,  ..., -0.0005,  0.0037, -0.0054],
         [-0.0006, -0.0011, -0.0118,  ...,  0.0002, -0.0007,  0.0028]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<StackBackward0>)
new_labels (return)
 tensor([[ -100,  -100,  -100,  ...,  -100,  -100,  -100],
        [ -100,  -100,  -100,  ..., 11955,   869,    13]], device='cuda:0')
【EXIT】if inputs_embeds is None:
Return of def LlavaLlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, image_sizes, return_dict)
result (return)
 CausalLMOutputWithPast(loss=tensor(8.4151, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[-4.8125,  0.8125,  4.4062,  ..., -5.2188, -2.4688, -4.2500],
         [-5.7812, -4.7812,  4.6875,  ..., -0.5625, -3.0938, -0.7891],
         [-6.6562, -6.7188,  4.5625,  ...,  5.0938, -3.9375,  2.7969],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        [[-4.8125,  0.8125,  4.4062,  ..., -5.2188, -2.4688, -4.2500],
         [-5.7812, -4.7812,  4.6875,  ..., -0.5625, -3.0938, -0.7891],
         [-6.6562, -6.7188,  4.5625,  ...,  5.0938, -3.9375,  2.7969],
         ...,
         [-9.0625, -9.3750,  1.5781,  ..., -5.3125, -6.8438, -3.1250],
         [-9.0625, -9.3125,  3.8906,  ..., -4.3438, -6.0938, -2.8906],
         [-8.2500, -8.5625,  1.3906,  ..., -3.7812, -5.9688, -2.4219]]],
       device='cuda:0', grad_fn=<ToCopyBackward0>), past_key_values=None, hidden_states=None, attentions=None)

100%|██████████| 1/1 [00:01<00:00,  1.46s/it]
                                             
{'loss': 8.4151, 'learning_rate': 0.001, 'epoch': 1.0}

100%|██████████| 1/1 [00:01<00:00,  1.46s/it]current file path llava/train/llava_trainer.py
def _save_checkpoint(self, model, trial, metrics=None)
self
 <llava.train.llava_trainer.LLaVATrainer object at 0x7a4663927070>
model
 DeepSpeedEngine(
  (module): LlavaLlamaForCausalLM(
    (model): LlavaLlamaModel(
      (embed_tokens): Embedding(32000, 2048, padding_idx=0)
      (layers): ModuleList(
        (0-21): 22 x LlamaDecoderLayer(
          (self_attn): LlamaAttention(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (k_proj): Linear(in_features=2048, out_features=256, bias=False)
            (v_proj): Linear(in_features=2048, out_features=256, bias=False)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
            (act_fn): SiLUActivation()
          )
          (input_layernorm): LlamaRMSNorm()
          (post_attention_layernorm): LlamaRMSNorm()
        )
      )
      (norm): LlamaRMSNorm()
      (vision_tower): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (mm_projector): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
    )
    (lm_head): Linear(in_features=2048, out_features=32000, bias=False)
  )
)
trial
 None
metrics
 None
【COND】 tune_mm_mlp_adapter=True
【ENTER】if getattr(self.args, 'tune_mm_mlp_adapter', False):
【COND】 use_im_start_end=False
current file path llava/train/llava_trainer.py
def get_mm_adapter_state_maybe_zero_3(named_params, keys_to_match)
named_params
 <generator object Module.named_parameters at 0x7a463827d540>
keys_to_match
 ['mm_projector', 'vision_resampler']
current file path llava/train/llava_trainer.py
def maybe_zero_3(param, ignore_status=False, name=None)
param
 Parameter containing:
tensor([[-0.0058,  0.0097, -0.0162,  ..., -0.0008, -0.0182, -0.0002],
        [-0.0209, -0.0105,  0.0015,  ..., -0.0160, -0.0190,  0.0212],
        [ 0.0182,  0.0133, -0.0121,  ...,  0.0275,  0.0132,  0.0165],
        ...,
        [ 0.0105,  0.0216,  0.0195,  ..., -0.0142, -0.0258,  0.0069],
        [-0.0052, -0.0130,  0.0054,  ..., -0.0063,  0.0295, -0.0258],
        [-0.0078,  0.0276,  0.0109,  ...,  0.0057,  0.0137,  0.0150]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
ignore_status
 True
name
 model.mm_projector.0.weight
【COND】 hasattr_ds_id=False
【ENTER】else (not hasattr(param, 'ds_id')):
【EXIT】else (not hasattr(param, 'ds_id')):
param (to return)
 tensor([[-0.0058,  0.0097, -0.0162,  ..., -0.0008, -0.0182, -0.0002],
        [-0.0209, -0.0105,  0.0015,  ..., -0.0160, -0.0190,  0.0212],
        [ 0.0182,  0.0133, -0.0121,  ...,  0.0275,  0.0132,  0.0165],
        ...,
        [ 0.0105,  0.0216,  0.0195,  ..., -0.0142, -0.0258,  0.0069],
        [-0.0052, -0.0130,  0.0054,  ..., -0.0063,  0.0295, -0.0258],
        [-0.0078,  0.0276,  0.0109,  ...,  0.0057,  0.0137,  0.0150]],
       dtype=torch.bfloat16)
current file path llava/train/llava_trainer.py
def maybe_zero_3(param, ignore_status=False, name=None)
param
 Parameter containing:
tensor([ 0.0254,  0.0092,  0.0159,  ..., -0.0168,  0.0093, -0.0087],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
ignore_status
 True
name
 model.mm_projector.0.bias
【COND】 hasattr_ds_id=False
【ENTER】else (not hasattr(param, 'ds_id')):
【EXIT】else (not hasattr(param, 'ds_id')):
param (to return)
 tensor([ 0.0254,  0.0092,  0.0159,  ..., -0.0168,  0.0093, -0.0087],
       dtype=torch.bfloat16)
current file path llava/train/llava_trainer.py
def maybe_zero_3(param, ignore_status=False, name=None)
param
 Parameter containing:
tensor([[ 0.0093,  0.0210,  0.0216,  ...,  0.0201,  0.0175,  0.0042],
        [-0.0040,  0.0070,  0.0067,  ..., -0.0138,  0.0179,  0.0096],
        [-0.0026,  0.0109, -0.0131,  ...,  0.0044,  0.0019, -0.0137],
        ...,
        [ 0.0067, -0.0159, -0.0011,  ..., -0.0114, -0.0026, -0.0144],
        [ 0.0079,  0.0139, -0.0110,  ...,  0.0199,  0.0026, -0.0043],
        [-0.0143,  0.0159,  0.0099,  ..., -0.0047,  0.0195, -0.0098]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
ignore_status
 True
name
 model.mm_projector.2.weight
【COND】 hasattr_ds_id=False
【ENTER】else (not hasattr(param, 'ds_id')):
【EXIT】else (not hasattr(param, 'ds_id')):
param (to return)
 tensor([[ 0.0093,  0.0210,  0.0216,  ...,  0.0201,  0.0175,  0.0042],
        [-0.0040,  0.0070,  0.0067,  ..., -0.0138,  0.0179,  0.0096],
        [-0.0026,  0.0109, -0.0131,  ...,  0.0044,  0.0019, -0.0137],
        ...,
        [ 0.0067, -0.0159, -0.0011,  ..., -0.0114, -0.0026, -0.0144],
        [ 0.0079,  0.0139, -0.0110,  ...,  0.0199,  0.0026, -0.0043],
        [-0.0143,  0.0159,  0.0099,  ..., -0.0047,  0.0195, -0.0098]],
       dtype=torch.bfloat16)
current file path llava/train/llava_trainer.py
def maybe_zero_3(param, ignore_status=False, name=None)
param
 Parameter containing:
tensor([-0.0117,  0.0208, -0.0112,  ..., -0.0096,  0.0084, -0.0159],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
ignore_status
 True
name
 model.mm_projector.2.bias
【COND】 hasattr_ds_id=False
【ENTER】else (not hasattr(param, 'ds_id')):
【EXIT】else (not hasattr(param, 'ds_id')):
param (to return)
 tensor([-0.0117,  0.0208, -0.0112,  ..., -0.0096,  0.0084, -0.0159],
       dtype=torch.bfloat16)
to_return
 {'model.mm_projector.0.weight': tensor([[-0.0058,  0.0097, -0.0162,  ..., -0.0008, -0.0182, -0.0002],
        [-0.0209, -0.0105,  0.0015,  ..., -0.0160, -0.0190,  0.0212],
        [ 0.0182,  0.0133, -0.0121,  ...,  0.0275,  0.0132,  0.0165],
        ...,
        [ 0.0105,  0.0216,  0.0195,  ..., -0.0142, -0.0258,  0.0069],
        [-0.0052, -0.0130,  0.0054,  ..., -0.0063,  0.0295, -0.0258],
        [-0.0078,  0.0276,  0.0109,  ...,  0.0057,  0.0137,  0.0150]],
       dtype=torch.bfloat16), 'model.mm_projector.0.bias': tensor([ 0.0254,  0.0092,  0.0159,  ..., -0.0168,  0.0093, -0.0087],
       dtype=torch.bfloat16), 'model.mm_projector.2.weight': tensor([[ 0.0093,  0.0210,  0.0216,  ...,  0.0201,  0.0175,  0.0042],
        [-0.0040,  0.0070,  0.0067,  ..., -0.0138,  0.0179,  0.0096],
        [-0.0026,  0.0109, -0.0131,  ...,  0.0044,  0.0019, -0.0137],
        ...,
        [ 0.0067, -0.0159, -0.0011,  ..., -0.0114, -0.0026, -0.0144],
        [ 0.0079,  0.0139, -0.0110,  ...,  0.0199,  0.0026, -0.0043],
        [-0.0143,  0.0159,  0.0099,  ..., -0.0047,  0.0195, -0.0098]],
       dtype=torch.bfloat16), 'model.mm_projector.2.bias': tensor([-0.0117,  0.0208, -0.0112,  ..., -0.0096,  0.0084, -0.0159],
       dtype=torch.bfloat16)}
【COND】 local_rank=0
【ENTER】if self.args.local_rank == 0 or self.args.local_rank == -1:
【EXIT】if self.args.local_rank == 0 or self.args.local_rank == -1:
【EXIT】if getattr(self.args, 'tune_mm_mlp_adapter', False):

                                             
{'train_runtime': 1.51, 'train_samples_per_second': 1.325, 'train_steps_per_second': 0.662, 'train_loss': 8.415122985839844, 'epoch': 1.0}

100%|██████████| 1/1 [00:01<00:00,  1.46s/it]
100%|██████████| 1/1 [00:01<00:00,  1.51s/it]
【EXIT】if list(pathlib.Path(training_args.output_dir).glob(checkpoint-*)):
model.config.use_cache = True True
【COND】lora_enable=False
【ENTER】else of if training_args.lora_enable:
trainer <llava.train.llava_trainer.LLaVATrainer object at 0x7a4663927070>
current file path llava/train/train.py
def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str)
trainer
 <class 'llava.train.llava_trainer.LLaVATrainer'>
output_dir
 ./checkpoints/Tinyllava-v1.5-7b-pretrain
trainer.args
 TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
bits=16,
cache_dir=None,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=2,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=./scripts/zero2.json,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=False,
double_quant=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
freeze_mm_mlp_adapter=False,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
group_by_modality_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./checkpoints/Tinyllava-v1.5-7b-pretrain/runs/Oct05_09-15-48_d580ed7cead7,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lora_alpha=16,
lora_bias=none,
lora_dropout=0.05,
lora_enable=False,
lora_r=64,
lora_weight_path=,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mm_projector_lr=None,
model_max_length=2048,
mp_parameters=,
mpt_attn_impl=triton,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=./checkpoints/Tinyllava-v1.5-7b-pretrain,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=2,
per_device_train_batch_size=2,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
quant_type=nf4,
ray_scope=last,
remove_unused_columns=False,
report_to=[],
resume_from_checkpoint=None,
run_name=./checkpoints/Tinyllava-v1.5-7b-pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=1.0,
save_strategy=steps,
save_total_limit=1,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
【COND】tune_mm_mlp_adapter= True
【ENTER】if getattr(trainer.args, 'tune_mm_mlp_adapter', False):
current file path llava/train/train.py
def get_mm_adapter_state_maybe_zero_3(named_params, keys_to_match)
named_params
 <generator object Module.named_parameters at 0x7a463827d5b0>
keys_to_match
 ['mm_projector']
current file path llava/train/train.py
def maybe_zero_3(param, ignore_status=False, name=None)
param
 Parameter containing:
tensor([[-0.0058,  0.0097, -0.0162,  ..., -0.0008, -0.0182, -0.0002],
        [-0.0209, -0.0105,  0.0015,  ..., -0.0160, -0.0190,  0.0212],
        [ 0.0182,  0.0133, -0.0121,  ...,  0.0275,  0.0132,  0.0165],
        ...,
        [ 0.0105,  0.0216,  0.0195,  ..., -0.0142, -0.0258,  0.0069],
        [-0.0052, -0.0130,  0.0054,  ..., -0.0063,  0.0295, -0.0258],
        [-0.0078,  0.0276,  0.0109,  ...,  0.0057,  0.0137,  0.0150]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
param.shape
 torch.Size([2048, 1024])
ignore_status
 True
name
 None
param (return)
 tensor([[-0.0058,  0.0097, -0.0162,  ..., -0.0008, -0.0182, -0.0002],
        [-0.0209, -0.0105,  0.0015,  ..., -0.0160, -0.0190,  0.0212],
        [ 0.0182,  0.0133, -0.0121,  ...,  0.0275,  0.0132,  0.0165],
        ...,
        [ 0.0105,  0.0216,  0.0195,  ..., -0.0142, -0.0258,  0.0069],
        [-0.0052, -0.0130,  0.0054,  ..., -0.0063,  0.0295, -0.0258],
        [-0.0078,  0.0276,  0.0109,  ...,  0.0057,  0.0137,  0.0150]],
       dtype=torch.bfloat16)
param (return).shape
 torch.Size([2048, 1024])
current file path llava/train/train.py
def maybe_zero_3(param, ignore_status=False, name=None)
param
 Parameter containing:
tensor([ 0.0254,  0.0092,  0.0159,  ..., -0.0168,  0.0093, -0.0087],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
param.shape
 torch.Size([2048])
ignore_status
 True
name
 None
param (return)
 tensor([ 0.0254,  0.0092,  0.0159,  ..., -0.0168,  0.0093, -0.0087],
       dtype=torch.bfloat16)
param (return).shape
 torch.Size([2048])
current file path llava/train/train.py
def maybe_zero_3(param, ignore_status=False, name=None)
param
 Parameter containing:
tensor([[ 0.0093,  0.0210,  0.0216,  ...,  0.0201,  0.0175,  0.0042],
        [-0.0040,  0.0070,  0.0067,  ..., -0.0138,  0.0179,  0.0096],
        [-0.0026,  0.0109, -0.0131,  ...,  0.0044,  0.0019, -0.0137],
        ...,
        [ 0.0067, -0.0159, -0.0011,  ..., -0.0114, -0.0026, -0.0144],
        [ 0.0079,  0.0139, -0.0110,  ...,  0.0199,  0.0026, -0.0043],
        [-0.0143,  0.0159,  0.0099,  ..., -0.0047,  0.0195, -0.0098]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
param.shape
 torch.Size([2048, 2048])
ignore_status
 True
name
 None
param (return)
 tensor([[ 0.0093,  0.0210,  0.0216,  ...,  0.0201,  0.0175,  0.0042],
        [-0.0040,  0.0070,  0.0067,  ..., -0.0138,  0.0179,  0.0096],
        [-0.0026,  0.0109, -0.0131,  ...,  0.0044,  0.0019, -0.0137],
        ...,
        [ 0.0067, -0.0159, -0.0011,  ..., -0.0114, -0.0026, -0.0144],
        [ 0.0079,  0.0139, -0.0110,  ...,  0.0199,  0.0026, -0.0043],
        [-0.0143,  0.0159,  0.0099,  ..., -0.0047,  0.0195, -0.0098]],
       dtype=torch.bfloat16)
param (return).shape
 torch.Size([2048, 2048])
current file path llava/train/train.py
def maybe_zero_3(param, ignore_status=False, name=None)
param
 Parameter containing:
tensor([-0.0117,  0.0208, -0.0112,  ..., -0.0096,  0.0084, -0.0159],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
param.shape
 torch.Size([2048])
ignore_status
 True
name
 None
param (return)
 tensor([-0.0117,  0.0208, -0.0112,  ..., -0.0096,  0.0084, -0.0159],
       dtype=torch.bfloat16)
param (return).shape
 torch.Size([2048])
to_return (return)
 {'model.mm_projector.0.weight': tensor([[-0.0058,  0.0097, -0.0162,  ..., -0.0008, -0.0182, -0.0002],
        [-0.0209, -0.0105,  0.0015,  ..., -0.0160, -0.0190,  0.0212],
        [ 0.0182,  0.0133, -0.0121,  ...,  0.0275,  0.0132,  0.0165],
        ...,
        [ 0.0105,  0.0216,  0.0195,  ..., -0.0142, -0.0258,  0.0069],
        [-0.0052, -0.0130,  0.0054,  ..., -0.0063,  0.0295, -0.0258],
        [-0.0078,  0.0276,  0.0109,  ...,  0.0057,  0.0137,  0.0150]],
       dtype=torch.bfloat16), 'model.mm_projector.0.bias': tensor([ 0.0254,  0.0092,  0.0159,  ..., -0.0168,  0.0093, -0.0087],
       dtype=torch.bfloat16), 'model.mm_projector.2.weight': tensor([[ 0.0093,  0.0210,  0.0216,  ...,  0.0201,  0.0175,  0.0042],
        [-0.0040,  0.0070,  0.0067,  ..., -0.0138,  0.0179,  0.0096],
        [-0.0026,  0.0109, -0.0131,  ...,  0.0044,  0.0019, -0.0137],
        ...,
        [ 0.0067, -0.0159, -0.0011,  ..., -0.0114, -0.0026, -0.0144],
        [ 0.0079,  0.0139, -0.0110,  ...,  0.0199,  0.0026, -0.0043],
        [-0.0143,  0.0159,  0.0099,  ..., -0.0047,  0.0195, -0.0098]],
       dtype=torch.bfloat16), 'model.mm_projector.2.bias': tensor([-0.0117,  0.0208, -0.0112,  ..., -0.0096,  0.0084, -0.0159],
       dtype=torch.bfloat16)}
to_return['model.mm_projector.0.weight'].shape
 torch.Size([2048, 1024])
to_return['model.mm_projector.0.bias'].shape
 torch.Size([2048])
to_return['model.mm_projector.2.weight'].shape
 torch.Size([2048, 2048])
to_return['model.mm_projector.2.bias'].shape
 torch.Size([2048])
current_folder
 Tinyllava-v1.5-7b-pretrain
parent_folder
 ./checkpoints
【COND】trainer.args.local_rank= 0
【ENTER】if trainer.args.local_rank == 0 or trainer.args.local_rank == -1:
Adapter weights saved to ./checkpoints/Tinyllava-v1.5-7b-pretrain/mm_projector.bin
【EXIT】if trainer.args.local_rank == 0 or trainer.args.local_rank == -1:
【EXIT】if getattr(trainer.args, 'tune_mm_mlp_adapter', False):
【EXIT】else of if training_args.lora_enable:
[2025-10-05 09:16:29,543] [INFO] [launch.py:347:main] Process 10695 exits successfully.
