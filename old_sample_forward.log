[2025-09-12 08:50:49,796] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-12 08:50:51,823] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-09-12 08:50:51,823] [INFO] [runner.py:555:main] cmd = /root/miniconda3/envs/llava/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero2.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version plain --data_path /content/LLaVA/blip_laion_cc_sbu_1.json --image_folder /content/LLaVA/images --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --tune_mm_mlp_adapter True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir ./checkpoints/llava-v1.5-7b-pretrain --num_train_epochs 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 1 --save_total_limit 1 --learning_rate 1e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 2 --lazy_preprocess True --report_to none
[2025-09-12 08:50:53,217] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-12 08:50:55,220] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.22.3-1+cuda12.5
[2025-09-12 08:50:55,220] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.22.3-1
[2025-09-12 08:50:55,220] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.22.3-1
[2025-09-12 08:50:55,220] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2025-09-12 08:50:55,220] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.22.3-1+cuda12.5
[2025-09-12 08:50:55,220] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2025-09-12 08:50:55,220] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.22.3-1
[2025-09-12 08:50:55,220] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2025-09-12 08:50:55,220] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-09-12 08:50:55,220] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-09-12 08:50:55,220] [INFO] [launch.py:163:main] dist_world_size=1
[2025-09-12 08:50:55,220] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-09-12 08:50:58,082] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
current file path llava/train/train.py
def train()
original parser
 HfArgumentParser(prog='train_mem.py', usage=None, description=None, formatter_class=<class 'argparse.ArgumentDefaultsHelpFormatter'>, conflict_handler='error', add_help=True)
[2025-09-12 08:50:58,768] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2025-09-12 08:50:58,768] [INFO] [comm.py:594:init_distributed] cdb=None
[2025-09-12 08:50:58,768] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
model_args
 ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')
data_args
 DataArguments(data_path='/content/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=False, image_folder='/content/LLaVA/images', image_aspect_ratio='square')
training_args
 TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
bits=16,
cache_dir=None,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=2,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=./scripts/zero2.json,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=False,
double_quant=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
freeze_mm_mlp_adapter=False,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
group_by_modality_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./checkpoints/llava-v1.5-7b-pretrain/runs/Sep12_08-50-58_6938f967efda,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lora_alpha=16,
lora_bias=none,
lora_dropout=0.05,
lora_enable=False,
lora_r=64,
lora_weight_path=,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mm_projector_lr=None,
model_max_length=2048,
mp_parameters=,
mpt_attn_impl=triton,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=./checkpoints/llava-v1.5-7b-pretrain,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
quant_type=nf4,
ray_scope=last,
remove_unused_columns=False,
report_to=[],
resume_from_checkpoint=None,
run_name=./checkpoints/llava-v1.5-7b-pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=1.0,
save_strategy=steps,
save_total_limit=1,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
local_rank
 0
compute_dtype
 torch.bfloat16
bnb_model_from_pretrained_args
 {}
[COND] bits=16
[COND] vision_tower=openai/clip-vit-large-patch14-336
【ENTER】if model_args.vision_tower is not None:
[COND] mpt_in_model_name_or_path=False
[COND] not_mpt_in_model_name_or_path={'mpt' not in model_args.model_name_or_path}
【ENTER】else of if 'mpt' in model_args.model_name_or_path:
/root/miniconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.__init__(self, config)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
config
 LlavaConfig {
  "_name_or_path": "lmsys/vicuna-7b-v1.5",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llava_llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "use_cache": true,
  "vocab_size": 32000
}

current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaModel.__init__(self, config: LlamaConfig)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaModel'>
config
 LlavaConfig {
  "_name_or_path": "lmsys/vicuna-7b-v1.5",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llava_llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "use_cache": true,
  "vocab_size": 32000
}

current file path llava/model/llava_arch.py
LlavaMetaModel.__init__(self, config)
config
 LlavaConfig {
  "_name_or_path": "lmsys/vicuna-7b-v1.5",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llava_llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "use_cache": true,
  "vocab_size": 32000
}

[COND] mm_vision_tower=False
self.model
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 4096, padding_idx=0)
  (layers): ModuleList(
    (0-31): 32 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)n
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
)
self.pretraining_tp
 1
self.vocab_size
 32000
self.lm_head
 Linear(in_features=4096, out_features=32000, bias=False)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.72s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.19s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.57s/it]
model defined as LlavaLlamaForCausalLM 
 LlavaLlamaForCausalLM(
  (model): LlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
【EXIT】else of if 'mpt' in model_args.model_name_or_path:
【EXIT】if model_args.vision_tower is not None:
model.config.use_cache
 False
[COND] freeze_backbone=False
[COND] bits=16
[COND] gradient_checkpointing=True
【ENTER】if training_args.gradient_checkpointing:
[COND] has_enable_input_require_grads=True
【ENTER】if hasattr(model, 'enable_input_require_grads'):
【EXIT】if hasattr(model, 'enable_input_require_grads'):
【EXIT】if training_args.gradient_checkpointing:
[COND] lora_enable=False
[COND] mpt_in_model_name_or_path=False
[COND] not_mpt_in_model_name_or_path={'mpt' not in model_args.model_name_or_path}
【ENTER】else of if 'mpt' in model_args.model_name_or_path:
tokenizer defined by AutoTokenizer.from_pretrained 
 LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)
【EXIT】else of if 'mpt' in model_args.model_name_or_path:
[COND] version=plain
【ENTER】else of if model_args.version == 'v0' and elif 'v0.5':
[COND] version_in_conv_templates=True
【ENTER】if model_args.version in conversation_lib.conv_templates:
conversation_lib.default_conversation set to plain
【EXIT】if model_args.version in conversation_lib.conv_templates:
【EXIT】else of if model_args.version == 'v0' and elif 'v0.5':
[COND] vision_tower=openai/clip-vit-large-patch14-336
【ENTER】if model_args.vision_tower is not None:
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 4096, padding_idx=0)
  (layers): ModuleList(
    (0-31): 32 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
)
current file path llava/model/llava_arch.py
def initialize_vision_modules(self, model_args, fsdp=None)
model_args
 ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')
fsdp
 []
vision_tower from model_args
 openai/clip-vit-large-patch14-336
mm_vision_select_layer from model_args
 -2
mm_vision_select_feature from model_args
 patch
pretrain_mm_mlp_adapter from model_args
 None
self.config.mm_vision_tower
 openai/clip-vit-large-patch14-336
current file path llava/model/llava_arch.py
def get_vision_tower(self)
vision_tower (raw)
 None
type(vision_tower)
 <class 'NoneType'>
[COND] type_vision_tower_is_list=False
vision_tower (return)
 None
[COND] self.get_vision_tower()
 None
current file path llava/model/llava_arch.py
def get_vision_tower(self)
vision_tower (raw)
 None
type(vision_tower)
 <class 'NoneType'>
[COND] type_vision_tower_is_list=False
vision_tower (return)
 None
[COND] get_vision_tower_is_None=True
current file path llava/model/llava_arch.py
def get_vision_tower(self)
vision_tower (raw)
 None
type(vision_tower)
 <class 'NoneType'>
[COND] type_vision_tower_is_list=False
vision_tower (return)
 None
【ENTER】if self.get_vision_tower() is None:
[ENTER] self.get_vision_tower() is None
current file path llava/llava/model/multimodal_encoder/builder.py
def build_vision_tower(vision_tower_cfg, **kwargs)
vision_tower_cfg
 ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')
kwargs
 {}
vision_tower from vision_tower_cfg
 openai/clip-vit-large-patch14-336
is_absolute_path_exists
 False
[COND] is_absolute_path_exists=False vision_tower=openai/clip-vit-large-patch14-336
【ENTER】if is_absolute_path_exists or vision_tower.startswith('openai') or vision_tower.startswith('laion') or 'ShareGPT4V' in vision_tower:
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.__init__(self, vision_tower, args, delay_load=False)
self
 <class 'llava.model.multimodal_encoder.clip_encoder.CLIPVisionTower'>
vision_tower
 openai/clip-vit-large-patch14-336
args
 ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')
delay_load
 False
self.is_loaded
 False
self.vision_tower_name
 openai/clip-vit-large-patch14-336
self.select_layer
 -2
self.select_feature
 patch
[COND] delay_load=False
【ENTER】if not delay_load:
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.load_model(self)
self
 <class 'llava.model.multimodal_encoder.clip_encoder.CLIPVisionTower'>
self.vision_tower_name
 openai/clip-vit-large-patch14-336
self.image_processor
 CLIPImageProcessor {
  "crop_size": {
    "height": 336,
    "width": 336
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "CLIPFeatureExtractor",
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 336
  }
}

self.vision_tower
 CLIPVisionModel(
  (vision_model): CLIPVisionTransformer(
    (embeddings): CLIPVisionEmbeddings(
      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
      (position_embedding): Embedding(577, 1024)
    )
    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (encoder): CLIPEncoder(
      (layers): ModuleList(
        (0-23): 24 x CLIPEncoderLayer(
          (self_attn): CLIPAttention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): CLIPMLP(
            (activation_fn): QuickGELUActivation()
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
)
self.is_loaded
 True
result (return)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
【EXIT】if is_absolute_path_exists or vision_tower.startswith('openai') or vision_tower.startswith('laion') or 'ShareGPT4V' in vision_tower:
vision_tower after build_vision_tower
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
[COND] fsdp
 []
[COND] fsdp_is_not_None=True len_fsdp=0
[COND] else_fsdp_is_not_None_and_len_fsdp_gt_0=True
【ENTER】else of if fsdp is not None and len(fsdp) > 0:
self.vision_tower
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
【EXIT】else of if fsdp is not None and len(fsdp) > 0:
【EXIT】if self.get_vision_tower() is None:
self.config.use_mm_proj set to True
self.config.mm_projector_type
 mlp2x_gelu
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.hidden_size(self)
self
 <class 'llava.model.multimodal_encoder.clip_encoder.CLIPVisionTower'>
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.config(self)
self
 <class 'llava.model.multimodal_encoder.clip_encoder.CLIPVisionTower'>
self.is_loaded
 True
[COND] is_loaded=True
【ENTER】if self.is_loaded:
result (return)
 <class 'transformers.models.clip.configuration_clip.CLIPVisionConfig'>
【EXIT】if self.is_loaded:
result (return)
 CLIPVisionConfig {
  "_name_or_path": "openai/clip-vit-large-patch14-336",
  "attention_dropout": 0.0,
  "dropout": 0.0,
  "hidden_act": "quick_gelu",
  "hidden_size": 1024,
  "image_size": 336,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "model_type": "clip_vision_model",
  "num_attention_heads": 16,
  "num_channels": 3,
  "num_hidden_layers": 24,
  "patch_size": 14,
  "projection_dim": 768,
  "transformers_version": "4.31.0"
}

result (return), self.config.hidden_size
 1024
self.config.mm_hidden_size
 1024
self.config.mm_vision_select_layer
 -2
self.config.mm_vision_select_feature
 patch
self.config.mm_patch_merge_type
 flat
[COND] mm_projector_is_None=True
【ENTER】if getattr(self, 'mm_projector', None) is None:
current file path llava/llava/model/multimodal_projector/builder.py
def build_vision_projector(config, delay_load=False, **kwargs)
config
 LlavaConfig {
  "_name_or_path": "lmsys/vicuna-7b-v1.5",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_patch_merge_type": "flat",
  "mm_projector_type": "mlp2x_gelu",
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14-336",
  "model_type": "llava_llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "use_cache": false,
  "use_mm_proj": true,
  "vocab_size": 32000
}

delay_load
 False
kwargs
 {}
projector_type from config
 mlp2x_gelu
【COND】 projector_type
 mlp2x_gelu
【COND】mlp_gelu_match
 <re.Match object; span=(0, 10), match='mlp2x_gelu'>
【ENTER】if mlp_gelu_match:
mlp_depth from mlp_gelu_match.group(1)
 2
modules after first Linear
 [Linear(in_features=1024, out_features=4096, bias=True)]
modules before Sequential
 [Linear(in_features=1024, out_features=4096, bias=True), GELU(approximate='none'), Linear(in_features=4096, out_features=4096, bias=True)]
result (return)
 Sequential(
  (0): Linear(in_features=1024, out_features=4096, bias=True)
  (1): GELU(approximate='none')
  (2): Linear(in_features=4096, out_features=4096, bias=True)
)
【EXIT】if mlp_gelu_match:
self.mm_projector after build_vision_projector
 Sequential(
  (0): Linear(in_features=1024, out_features=4096, bias=True)
  (1): GELU(approximate='none')
  (2): Linear(in_features=4096, out_features=4096, bias=True)
)
mm_patch_merge_type
 flat
[COND] unpad_in_mm_patch_merge_type=False
【EXIT】if getattr(self, 'mm_projector', None) is None:
[COND] pretrain_mm_mlp_adapter_is_not_None=False
current file path llava/model/llava_arch.py
class LlavaMetaForCausalLM(ABC).get_vision_tower(self)
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 4096, padding_idx=0)
  (layers): ModuleList(
    (0-31): 32 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=4096, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=4096, out_features=4096, bias=True)
  )
)
current file path llava/model/llava_arch.py
def get_vision_tower(self)
vision_tower (raw)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
type(vision_tower)
 <class 'llava.model.multimodal_encoder.clip_encoder.CLIPVisionTower'>
[COND] type_vision_tower_is_list=False
vision_tower (return)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
LlavaMetaForCausalLM(ABC).get_vision_tower(self) result (return)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
[COND] tune_mm_mlp_adapter=True
【ENTER】if model_args.tune_mm_mlp_adapter:
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 4096, padding_idx=0)
  (layers): ModuleList(
    (0-31): 32 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=4096, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=4096, out_features=4096, bias=True)
  )
)
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 4096, padding_idx=0)
  (layers): ModuleList(
    (0-31): 32 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=4096, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=4096, out_features=4096, bias=True)
  )
)
model.get_model().mm_projector.parameters() <generator object Module.parameters at 0x7ed637a75af0>
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 4096, padding_idx=0)
  (layers): ModuleList(
    (0-31): 32 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=4096, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=4096, out_features=4096, bias=True)
  )
)
model.get_model().mm_projector.parameters() <generator object Module.parameters at 0x7ed637a75af0>
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 4096, padding_idx=0)
  (layers): ModuleList(
    (0-31): 32 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=4096, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=4096, out_features=4096, bias=True)
  )
)
model.get_model().mm_projector.parameters() <generator object Module.parameters at 0x7ed637a75af0>
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 4096, padding_idx=0)
  (layers): ModuleList(
    (0-31): 32 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=4096, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=4096, out_features=4096, bias=True)
  )
)
model.get_model().mm_projector.parameters() <generator object Module.parameters at 0x7ed637a75af0>
【EXIT】if model_args.tune_mm_mlp_adapter:
[COND] freeze_mm_mlp_adapter=False
[COND] bits=16
model_args.mm_use_im_start_end False
training_args.mm_projector_lr None
training_args.use_im_start_end False
model_args.mm_use_im_patch_token False
current file path llava/model/llava_arch.py
def initialize_vision_tokenizer(self, model_args, tokenizer)
model_args
 ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')
tokenizer
 LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)
[COND] mm_use_im_patch_token=False
【EXIT】if model_args.vision_tower is not None:
[COND] bits=16
current file path llava/train/train.py
def make_supervised_data_module(tokenizer, data_args)
tokenizer
 <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>
data_args
 DataArguments(data_path='/content/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/LLaVA/images', image_aspect_ratio='square')
current file path llava/train/train.py
def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer, data_args) -> Dict
current file path llava/train/train.py
def LazySupervisedDataset.__init__(self, data_path, tokenizer, data_args)
data_path
 /content/LLaVA/blip_laion_cc_sbu_1.json
tokenizer
 <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>
data_args
 DataArguments(data_path='/content/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/LLaVA/images', image_aspect_ratio='square')
current file path llava/train/train.py
def rank0_print(*args)
args
 ('Formatting inputs...Skip in lazy mode',)
Formatting inputs...Skip in lazy mode
train_dataset
 <llava.train.train.LazySupervisedDataset object at 0x7ed6341f4880>
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
len(train_dataset)
 1
data_collator
 DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))
def make_supervised_data_module: result (return)
 {'train_dataset': <llava.train.train.LazySupervisedDataset object at 0x7ed6341f4880>, 'eval_dataset': None, 'data_collator': DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))}
data_module
 {'train_dataset': <llava.train.train.LazySupervisedDataset object at 0x7ed6341f4880>, 'eval_dataset': None, 'data_collator': DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))}
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
trainer
 <llava.train.llava_trainer.LLaVATrainer object at 0x7ed6341f4490>
【COND】list(pathlib.Path(training_args.output_dir).glob('checkpoint-*'))
### 再開
 [PosixPath('checkpoints/llava-v1.5-7b-pretrain/checkpoint-250'), PosixPath('checkpoints/llava-v1.5-7b-pretrain/checkpoint-1')]
【ENTER】if list(pathlib.Path(training_args.output_dir).glob(checkpoint-*)):
# なんとここから 2759行目まで trainer.train(resume_from_checkpoint=False) の処理！
current file path llava/train/llava_trainer.py
def _get_train_sampler(self)
self
 <llava.train.llava_trainer.LLaVATrainer object at 0x7ed6341f4490>
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
[COND] train_dataset_is_None=False, has_length=True
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
[COND] group_by_modality_length=False
【ENTER】else (not group_by_modality_length):
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
result, super()._get_train_sampler()
 <torch.utils.data.sampler.RandomSampler object at 0x7ed63e925e70>
【EXIT】else (not group_by_modality_length):
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/llava_trainer.py
def create_optimizer(self)
self
 <llava.train.llava_trainer.LLaVATrainer object at 0x7ed6341f4490>
[COND] sagemaker_mp_enabled=False
[COND] sharded_ddp=None, SHARDED_DDP_SIMPLE=simple
opt_model
 LlavaLlamaForCausalLM(
  (model): LlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (vision_tower): CLIPVisionTower(
      (vision_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
[COND] optimizer_is_None=True
【ENTER】if self.optimizer is None:
print(risk): print(self.args) disabled for safety
print(risk): print(opt_model) disabled for safety
print(risk): print(optimizer_grouped_parameters) disabled for safety
print(risk): print(optimizer_cls) disabled for safety
print(risk): print(optimizer_kwargs) disabled for safety
[COND] mm_projector_lr=None
【ENTER】else (mm_projector_lr is None):
[COND] sharded_ddp=None, SHARDED_DDP_SIMPLE=simple
【ENTER】else (not sharded_ddp SIMPLE):
[COND] optimizer_cls_name=AdamW
【EXIT】if self.optimizer is None:
self.optimizer
 AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
current file path llava/train/llava_trainer.py
def create_optimizer(self)
self
 <llava.train.llava_trainer.LLaVATrainer object at 0x7ed6341f4490>
[COND] sagemaker_mp_enabled=False
[COND] sharded_ddp=None, SHARDED_DDP_SIMPLE=simple
opt_model
 LlavaLlamaForCausalLM(
  (model): LlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (vision_tower): CLIPVisionTower(
      (vision_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
[COND] optimizer_is_None=False
self.optimizer
 AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.0
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.0
    maximize: False
    weight_decay: 0.0
)
Rank: 0 partition count [1, 1] and sizes[(20971520, False), (8192, False)] 

  0%|          | 0/1 [00:00<?, ?it/s]current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__getitem__(self, i)
i
 0
current file path llava/train/train.py
def preprocess_multimodal(sources, data_args)
sources
 [[{'from': 'human', 'value': 'Give a brief description of the image.\n<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]
data_args
 DataArguments(data_path='/content/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/LLaVA/images', image_aspect_ratio='square')
current file path llava/train/train.py
def preprocess(sources, tokenizer, has_image=False)
sources
 [[{'from': 'human', 'value': '<image>\nGive a brief description of the image.'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]
tokenizer
 <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>
has_image
 True
current file path llava/train/train.py
def preprocess_plain(sources, tokenizer)
sources
 [[{'from': 'human', 'value': '<image>\nGive a brief description of the image.'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]
tokenizer
 <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>
current file path llava/mm_utils.py
def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None)
prompt
 <image>the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair

tokenizer
 LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)
image_token_index
 -200
return_tensors
 pt
input_ids[0].shape
 torch.Size([24])
targets[0].shape
 torch.Size([24])
current file path llava/mm_utils.py
def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None)
prompt
 <image>
tokenizer
 LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)
image_token_index
 -200
return_tensors
 None
input_ids (return)
 [1, -200]
current file path llava/train/train.py
def DataCollatorForSupervisedDataset.__call__(self, instances)
instances
 [{'input_ids': tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
          411,  2654, 11315,    13]), 'labels': tensor([ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
          411,  2654, 11315,    13]), 'image': tensor([[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],
         [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],
         [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],
         ...,
         [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],

        [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],
         [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],
         [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],
         ...,
         [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],

        [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],
         [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],
         [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],
         ...,
         [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],
         [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],
         [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]])}]
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, image_sizes, return_dict)
input_ids
 tensor([[    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
           411,  2654, 11315,    13]], device='cuda:0')
input_ids.shape
 torch.Size([1, 24])
attention_mask
 tensor([[True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True]],
       device='cuda:0')
position_ids
 None
past_key_values
 None
inputs_embeds
 None
labels
 tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
           411,  2654, 11315,    13]], device='cuda:0')
use_cache
 None
output_attentions
 None
output_hidden_states
 None
images
 tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7109, -0.3613, -0.1279],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3906, -0.1719, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0112,  0.0471,  0.0908],
          ...,
          [-1.0312, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625]],

         [[ 0.3184,  0.3184,  0.3184,  ..., -0.3867, -0.0112,  0.2139],
          [ 0.3184,  0.3184,  0.3184,  ..., -0.0713,  0.1543,  0.3184],
          [ 0.3184,  0.3184,  0.3184,  ...,  0.2891,  0.3633,  0.4395],
          ...,
          [-1.0156, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000]],

         [[ 0.9648,  0.9648,  0.9648,  ...,  0.0981,  0.4531,  0.6680],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.3965,  0.6094,  0.7539],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.7539,  0.8086,  0.8359],
          ...,
          [-0.3711, -0.3848, -0.4004,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3711, -0.3711, -0.3848,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3848, -0.3711, -0.3711,  ..., -0.4277, -0.4277, -0.4277]]]],
       device='cuda:0', dtype=torch.bfloat16)
images.shape
 torch.Size([1, 3, 336, 336])
image_sizes
 None
return_dict
 None
[COND] inputs_embeds_is_None=True
【ENTER】if inputs_embeds is None:
current file path llava/model/llava_arch.py
def LlavaMetaForCausalLM(ABC).prepare_inputs_labels_for_multimodal(self, input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes=None)
input_ids
 tensor([[    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
           411,  2654, 11315,    13]], device='cuda:0')
position_ids
 None
attention_mask
 tensor([[True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True]],
       device='cuda:0')
past_key_values
 None
labels
 tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
           411,  2654, 11315,    13]], device='cuda:0')
images
 tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7109, -0.3613, -0.1279],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3906, -0.1719, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0112,  0.0471,  0.0908],
          ...,
          [-1.0312, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625]],

         [[ 0.3184,  0.3184,  0.3184,  ..., -0.3867, -0.0112,  0.2139],
          [ 0.3184,  0.3184,  0.3184,  ..., -0.0713,  0.1543,  0.3184],
          [ 0.3184,  0.3184,  0.3184,  ...,  0.2891,  0.3633,  0.4395],
          ...,
          [-1.0156, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000]],

         [[ 0.9648,  0.9648,  0.9648,  ...,  0.0981,  0.4531,  0.6680],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.3965,  0.6094,  0.7539],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.7539,  0.8086,  0.8359],
          ...,
          [-0.3711, -0.3848, -0.4004,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3711, -0.3711, -0.3848,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3848, -0.3711, -0.3711,  ..., -0.4277, -0.4277, -0.4277]]]],
       device='cuda:0', dtype=torch.bfloat16)
image_sizes
 None
current file path llava/model/llava_arch.py
class LlavaMetaForCausalLM(ABC).get_vision_tower(self)
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 4096, padding_idx=0)
  (layers): ModuleList(
    (0-31): 32 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=4096, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=4096, out_features=4096, bias=True)
  )
)
current file path llava/model/llava_arch.py
def get_vision_tower(self)
vision_tower (raw)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
type(vision_tower)
 <class 'llava.model.multimodal_encoder.clip_encoder.CLIPVisionTower'>
[COND] type_vision_tower_is_list=False
vision_tower (return)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
LlavaMetaForCausalLM(ABC).get_vision_tower(self) result (return)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
current file path llava/model/llava_arch.py
def LlavaMetaForCausalLM(ABC).encode_images(self, images)
images
 tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7109, -0.3613, -0.1279],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3906, -0.1719, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0112,  0.0471,  0.0908],
          ...,
          [-1.0312, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625]],

         [[ 0.3184,  0.3184,  0.3184,  ..., -0.3867, -0.0112,  0.2139],
          [ 0.3184,  0.3184,  0.3184,  ..., -0.0713,  0.1543,  0.3184],
          [ 0.3184,  0.3184,  0.3184,  ...,  0.2891,  0.3633,  0.4395],
          ...,
          [-1.0156, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000]],

         [[ 0.9648,  0.9648,  0.9648,  ...,  0.0981,  0.4531,  0.6680],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.3965,  0.6094,  0.7539],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.7539,  0.8086,  0.8359],
          ...,
          [-0.3711, -0.3848, -0.4004,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3711, -0.3711, -0.3848,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3848, -0.3711, -0.3711,  ..., -0.4277, -0.4277, -0.4277]]]],
       device='cuda:0', dtype=torch.bfloat16)
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 4096, padding_idx=0)
  (layers): ModuleList(
    (0-31): 32 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=4096, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=4096, out_features=4096, bias=True)
  )
)
current file path llava/model/llava_arch.py
def get_vision_tower(self)
vision_tower (raw)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
type(vision_tower)
 <class 'llava.model.multimodal_encoder.clip_encoder.CLIPVisionTower'>
[COND] type_vision_tower_is_list=False
vision_tower (return)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.forward(self, images)
images
 tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7109, -0.3613, -0.1279],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3906, -0.1719, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0112,  0.0471,  0.0908],
          ...,
          [-1.0312, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625]],

         [[ 0.3184,  0.3184,  0.3184,  ..., -0.3867, -0.0112,  0.2139],
          [ 0.3184,  0.3184,  0.3184,  ..., -0.0713,  0.1543,  0.3184],
          [ 0.3184,  0.3184,  0.3184,  ...,  0.2891,  0.3633,  0.4395],
          ...,
          [-1.0156, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000]],

         [[ 0.9648,  0.9648,  0.9648,  ...,  0.0981,  0.4531,  0.6680],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.3965,  0.6094,  0.7539],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.7539,  0.8086,  0.8359],
          ...,
          [-0.3711, -0.3848, -0.4004,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3711, -0.3711, -0.3848,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3848, -0.3711, -0.3711,  ..., -0.4277, -0.4277, -0.4277]]]],
       device='cuda:0', dtype=torch.bfloat16)
images.shape
 torch.Size([1, 3, 336, 336])
[COND] type_images_is_list=False
【ENTER】else (type(images) is not list):
original images
 tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7109, -0.3613, -0.1279],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3906, -0.1719, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0112,  0.0471,  0.0908],
          ...,
          [-1.0312, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625]],

         [[ 0.3184,  0.3184,  0.3184,  ..., -0.3867, -0.0112,  0.2139],
          [ 0.3184,  0.3184,  0.3184,  ..., -0.0713,  0.1543,  0.3184],
          [ 0.3184,  0.3184,  0.3184,  ...,  0.2891,  0.3633,  0.4395],
          ...,
          [-1.0156, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000]],

         [[ 0.9648,  0.9648,  0.9648,  ...,  0.0981,  0.4531,  0.6680],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.3965,  0.6094,  0.7539],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.7539,  0.8086,  0.8359],
          ...,
          [-0.3711, -0.3848, -0.4004,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3711, -0.3711, -0.3848,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3848, -0.3711, -0.3711,  ..., -0.4277, -0.4277, -0.4277]]]],
       device='cuda:0', dtype=torch.bfloat16)
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.device(self)
self
 <class 'llava.model.multimodal_encoder.clip_encoder.CLIPVisionTower'>
result (return)
 cuda:0
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.dtype(self)
self
 <class 'llava.model.multimodal_encoder.clip_encoder.CLIPVisionTower'>
result (return)
 torch.bfloat16
after process image_forward_outs
 <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.feature_select(self, image_forward_outs)
image_forward_outs
 BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.3926, -0.0566, -0.2051,  ...,  0.4980, -0.7695, -0.0625],
         [ 0.3828, -0.2754,  0.3906,  ...,  0.0303,  0.1455,  0.2754],
         [-0.1113,  1.1250,  1.0859,  ...,  0.2656, -0.2520, -0.8203],
         ...,
         [ 1.9453,  0.9219,  1.6719,  ..., -0.2334, -0.5625,  1.2812],
         [ 1.0078, -0.2295,  1.3359,  ..., -0.5312, -0.9922,  0.3477],
         [ 1.6719,  0.9570,  1.1094,  ..., -0.7812, -0.3750,  0.9492]]],
       device='cuda:0', dtype=torch.bfloat16), pooler_output=tensor([[ 0.7617,  0.0408, -0.4414,  ...,  1.1250, -1.6016, -0.1167]],
       device='cuda:0', dtype=torch.bfloat16), hidden_states=(tensor([[[ 0.0342, -0.0408, -0.1670,  ...,  0.3203, -0.1475, -0.0201],
         [-0.1167, -0.0491, -0.2637,  ...,  0.5859, -0.1318, -0.0099],
         [ 0.2451, -0.0437, -0.6133,  ...,  0.3477, -0.1445, -0.0098],
         ...,
         [ 0.1074, -0.0439, -0.1309,  ..., -0.0601, -0.1357, -0.0164],
         [ 0.1797, -0.0452, -0.1074,  ..., -0.0811, -0.1348, -0.0111],
         [ 0.0266, -0.0486, -0.0238,  ..., -0.0114, -0.1445, -0.0155]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.0513,  0.1045, -0.1074,  ...,  0.1152,  0.0723,  0.0322],
         [ 0.0391,  0.0635, -0.1592,  ...,  0.3496, -0.0361, -0.0442],
         [ 0.2480,  0.0557, -0.4141,  ...,  0.1582,  0.0967,  0.1113],
         ...,
         [ 0.0640, -0.0659,  0.0269,  ..., -0.2852, -0.0283, -0.0420],
         [ 0.1729, -0.1162,  0.0181,  ..., -0.2891, -0.0049, -0.0574],
         [ 0.0977,  0.0244, -0.0283,  ..., -0.0527,  0.0107, -0.0073]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.0415,  0.0073, -0.0889,  ...,  0.0664, -0.0334,  0.0476],
         [ 0.1201,  0.0059, -0.0898,  ...,  0.2324,  0.1152, -0.0430],
         [ 0.2119, -0.0593, -0.2832,  ...,  0.1514,  0.0977,  0.1992],
         ...,
         [-0.0527, -0.1914,  0.0000,  ..., -0.4453, -0.1162, -0.1050],
         [ 0.0117, -0.2793, -0.0059,  ..., -0.4883, -0.0732, -0.1416],
         [ 0.0815,  0.0679, -0.0757,  ..., -0.1523,  0.0603, -0.0087]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-4.3701e-02, -1.9531e-03, -5.1270e-02,  ...,  4.8096e-02,
           2.4414e-04,  9.4727e-02],
         [ 1.8262e-01, -3.6133e-02, -1.7090e-02,  ...,  2.8516e-01,
           3.0078e-01, -2.8809e-02],
         [ 2.5000e-01,  3.6133e-02, -6.6406e-02,  ...,  1.9141e-01,
           1.5039e-01,  3.4766e-01],
         ...,
         [-1.9336e-01, -2.1484e-01, -3.6621e-02,  ..., -4.1016e-01,
          -1.5430e-01, -5.4688e-02],
         [-4.7852e-02, -2.7539e-01,  3.8574e-02,  ..., -4.1992e-01,
          -1.3672e-01, -2.3242e-01],
         [-7.3242e-02,  2.7930e-01, -1.6895e-01,  ..., -1.5430e-01,
           1.7969e-01,  5.5176e-02]]], device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0791, -0.0151,  0.0153,  ...,  0.0811, -0.0127,  0.1055],
         [ 0.2617,  0.0234, -0.0078,  ...,  0.3105,  0.3457, -0.0234],
         [ 0.0728, -0.2021, -0.0781,  ...,  0.2695,  0.0840,  0.3906],
         ...,
         [-0.2119, -0.3672, -0.1543,  ..., -0.3223, -0.2314, -0.1309],
         [ 0.0337, -0.4688, -0.0571,  ..., -0.3926, -0.2500, -0.2852],
         [-0.0190,  0.3535, -0.1060,  ..., -0.1089,  0.1133,  0.0396]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0791, -0.0437,  0.0469,  ...,  0.0986,  0.1113,  0.1328],
         [ 0.0850,  0.0151,  0.1533,  ...,  0.2988,  0.2598, -0.1006],
         [-0.0603, -0.2324,  0.0549,  ...,  0.3633,  0.0493,  0.4473],
         ...,
         [-0.1055, -0.3867,  0.0304,  ..., -0.2012, -0.1152, -0.1406],
         [ 0.0718, -0.5859,  0.1582,  ..., -0.3438, -0.2617, -0.1680],
         [ 0.0227,  0.3047, -0.1895,  ..., -0.1270,  0.1348,  0.1660]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0229,  0.0454,  0.0537,  ...,  0.0559,  0.1309,  0.0645],
         [-0.1396,  0.1514,  0.0649,  ...,  0.4512,  0.2656, -0.1074],
         [-0.0283, -0.1748,  0.0381,  ...,  0.4082, -0.0879,  0.4023],
         ...,
         [-0.2500, -0.3066,  0.1836,  ..., -0.2266, -0.3008, -0.3145],
         [-0.0181, -0.6250,  0.2070,  ..., -0.3047, -0.2773, -0.2246],
         [ 0.0781,  0.3477, -0.2637,  ..., -0.1079,  0.1768,  0.1074]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 3.2959e-02,  1.5381e-02,  5.6396e-02,  ...,  7.9102e-02,
           8.3984e-02,  5.3223e-02],
         [-3.9062e-01,  6.4453e-02,  5.8594e-02,  ...,  7.2656e-01,
           2.1973e-01, -8.3496e-02],
         [ 1.5137e-01, -2.0605e-01,  1.8945e-01,  ...,  6.5625e-01,
          -2.4023e-01,  3.1250e-02],
         ...,
         [-2.4414e-01, -1.2207e-01,  4.0625e-01,  ..., -1.3184e-02,
          -3.9844e-01, -4.2969e-02],
         [ 1.3184e-02, -6.0156e-01,  3.4766e-01,  ..., -1.0156e-01,
          -3.2812e-01, -1.1475e-01],
         [ 1.3770e-01,  2.8516e-01, -4.8828e-04,  ...,  2.9053e-02,
           3.0078e-01,  1.9043e-01]]], device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.0078, -0.0359,  0.0630,  ...,  0.1738,  0.0801,  0.0483],
         [-0.3105, -0.0420, -0.0649,  ...,  0.8047,  0.1406, -0.2080],
         [ 0.0747, -0.4062,  0.3340,  ...,  0.8594, -0.1592, -0.1582],
         ...,
         [-0.2324, -0.2832,  0.4414,  ..., -0.1187, -0.5625,  0.2139],
         [-0.2002, -0.6484,  0.3379,  ..., -0.2012, -0.3828, -0.0962],
         [ 0.0078,  0.3164, -0.3223,  ...,  0.0223, -0.0674,  0.0049]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0293, -0.0152,  0.0259,  ...,  0.1309,  0.0183, -0.0393],
         [-0.1050, -0.1914, -0.1230,  ...,  0.8047,  0.3242, -0.1162],
         [ 0.0601, -0.3457,  0.3594,  ...,  0.8906, -0.2422,  0.0596],
         ...,
         [-0.2451, -0.4023,  0.0840,  ..., -0.3184, -0.6719,  0.3848],
         [-0.3750, -0.7930, -0.0449,  ..., -0.4004, -0.7734,  0.0133],
         [ 0.1377,  0.2539, -0.3672,  ..., -0.1914, -0.0449, -0.0830]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0199, -0.0488,  0.0977,  ...,  0.1201, -0.0154,  0.0889],
         [ 0.0820,  0.1895, -0.1914,  ...,  0.5625,  0.3359,  0.0737],
         [ 0.0618,  0.0371,  0.1416,  ...,  0.8516, -0.0825,  0.0029],
         ...,
         [-0.3281, -0.3242,  0.2539,  ..., -0.1562, -0.6289,  0.3203],
         [-0.3145, -0.5742,  0.2559,  ..., -0.3203, -0.5352,  0.0498],
         [ 0.2266,  0.2480, -0.0879,  ..., -0.1699,  0.0913, -0.1147]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.0635, -0.0757,  0.0557,  ...,  0.0669,  0.0361,  0.0664],
         [ 0.0525,  0.2217, -0.0425,  ...,  0.5039,  0.5273,  0.0464],
         [ 0.0085,  0.1768,  0.0605,  ...,  0.9219, -0.1309, -0.0786],
         ...,
         [-0.2471, -0.3516,  0.2539,  ...,  0.0156, -0.5000,  0.0684],
         [-0.2246, -0.4219,  0.0957,  ..., -0.2578, -0.4688, -0.3086],
         [-0.0059,  0.1660,  0.3242,  ..., -0.0918,  0.0000, -0.3086]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0020, -0.1914, -0.1504,  ...,  0.0952, -0.0635,  0.0156],
         [-0.0620,  0.2363, -0.1914,  ...,  0.5000,  0.3008,  0.1992],
         [-0.0479,  0.2188,  0.0669,  ...,  0.9297, -0.1436, -0.0142],
         ...,
         [-0.2891, -0.1006,  0.3105,  ..., -0.1040, -0.5195,  0.0713],
         [-0.0388, -0.5039,  0.1133,  ..., -0.2266, -0.5117, -0.5586],
         [ 0.0127,  0.1758,  0.3340,  ..., -0.2500,  0.0188, -0.1582]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0957, -0.1621, -0.0874,  ...,  0.1128, -0.0439,  0.0625],
         [-0.0347,  0.1836,  0.0596,  ...,  0.3828,  0.2559,  0.2090],
         [-0.0791,  0.1621, -0.0967,  ...,  0.7031, -0.2305, -0.2109],
         ...,
         [-0.2236,  0.0317,  0.2412,  ..., -0.0254, -0.0986, -0.0027],
         [ 0.0630, -0.3438,  0.0820,  ..., -0.2891, -0.1074, -0.4551],
         [ 0.2578,  0.2363,  0.3359,  ..., -0.2559,  0.1455, -0.2344]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0840, -0.1104, -0.1099,  ...,  0.1074, -0.0608, -0.0303],
         [ 0.0471,  0.1641,  0.2354,  ...,  0.1045,  0.1836,  0.2070],
         [-0.1152,  0.1582, -0.2969,  ...,  0.3242, -0.1279, -0.1875],
         ...,
         [ 0.1152,  0.0225,  0.3242,  ..., -0.3711,  0.0330,  0.0049],
         [ 0.0859, -0.3750,  0.0898,  ..., -0.5625, -0.0723, -0.2197],
         [ 0.2871,  0.1045,  0.5117,  ..., -0.5391,  0.1758, -0.2070]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0874, -0.1621, -0.0811,  ...,  0.1396, -0.0361,  0.0850],
         [ 0.0552,  0.2070,  0.2715,  ..., -0.2266,  0.3477,  0.4434],
         [-0.2910,  0.2559, -0.1426,  ...,  0.1074, -0.0417,  0.0420],
         ...,
         [ 0.3047,  0.0737,  0.3789,  ...,  0.0664, -0.1260,  0.3086],
         [-0.0255, -0.4805,  0.2227,  ..., -0.0139, -0.3477, -0.1689],
         [ 0.3887,  0.0400,  0.5898,  ..., -0.5078,  0.0264,  0.0015]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-2.0508e-02, -1.2793e-01, -1.0938e-01,  ...,  3.9062e-03,
          -7.0312e-02,  1.4648e-01],
         [-1.9531e-02,  3.3203e-01,  2.5000e-01,  ..., -1.2207e-04,
           2.6562e-01,  5.1172e-01],
         [-2.6367e-01,  4.5898e-01, -2.5586e-01,  ...,  2.8711e-01,
          -3.0762e-02,  9.8633e-02],
         ...,
         [ 4.4531e-01,  2.8906e-01,  2.2559e-01,  ...,  2.5586e-01,
          -1.2891e-01,  2.9492e-01],
         [-7.8125e-02, -2.1582e-01,  2.0312e-01,  ...,  5.5664e-02,
          -3.8281e-01, -3.7891e-01],
         [ 5.5469e-01,  2.4609e-01,  5.4688e-01,  ..., -5.5469e-01,
           2.5391e-02,  1.0547e-01]]], device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0444,  0.0947,  0.0210,  ..., -0.1357, -0.2324,  0.0718],
         [-0.0559, -0.1777,  0.2119,  ..., -0.1641,  0.3203,  0.3984],
         [-0.1748,  0.2471, -0.3320,  ...,  0.0986,  0.1021, -0.0972],
         ...,
         [ 0.5234,  0.4746,  0.2051,  ...,  0.1660, -0.0527,  0.2793],
         [ 0.0400,  0.0029,  0.1826,  ..., -0.2158, -0.3750, -0.3984],
         [ 0.5469,  0.2656,  0.4375,  ..., -0.5234, -0.1641,  0.0054]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.1318,  0.0918, -0.0099,  ..., -0.0718, -0.0293,  0.0776],
         [-0.1289, -0.1719, -0.1719,  ..., -0.2773,  0.2715,  0.4258],
         [-0.2227,  0.2695, -0.3027,  ...,  0.0540,  0.3203, -0.0581],
         ...,
         [ 0.5547,  0.6562,  0.4102,  ...,  0.1245, -0.1777,  0.3164],
         [-0.1562, -0.1143, -0.0254,  ..., -0.2119, -0.3633, -0.5078],
         [ 0.5273,  0.2178,  0.1523,  ..., -0.5000, -0.0762,  0.1221]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.1719,  0.1270, -0.0293,  ...,  0.0132, -0.1270, -0.0059],
         [-0.2500, -0.6719, -0.2773,  ..., -0.0088,  0.6289,  0.1934],
         [-0.5195,  0.1963, -0.1387,  ...,  0.0435,  0.4082, -0.0762],
         ...,
         [ 0.5391,  0.7188,  0.4590,  ...,  0.2910, -0.1982,  0.2793],
         [-0.1108, -0.4355,  0.0649,  ..., -0.1396, -0.4512, -0.3320],
         [ 0.6250,  0.0820,  0.2100,  ..., -0.2158, -0.3320,  0.3203]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.2207,  0.3359, -0.2021,  ..., -0.0181, -0.1562,  0.1973],
         [-0.2256, -0.4062, -0.0879,  ..., -0.2031,  0.4688,  0.2012],
         [-0.2539,  0.4688,  0.2051,  ...,  0.0532,  0.1289,  0.0061],
         ...,
         [ 0.7891,  0.7305,  0.5352,  ...,  0.2344, -0.3203,  0.3359],
         [ 0.2324, -0.5312,  0.1602,  ..., -0.2402, -0.5781, -0.2695],
         [ 1.1016,  0.2715,  0.3008,  ..., -0.2754, -0.5703,  0.6055]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.2412,  0.2930, -0.2734,  ...,  0.2412, -0.2119,  0.3594],
         [-0.3008, -0.4062,  0.5195,  ..., -0.0208,  0.5156,  0.1719],
         [ 0.3242,  0.8516,  0.0449,  ..., -0.0198, -0.3672, -0.2520],
         ...,
         [ 1.4766,  1.2656,  1.0312,  ..., -0.4336, -0.7344,  0.4648],
         [ 1.0781, -0.0664,  1.0781,  ..., -0.9922, -0.6836, -0.4004],
         [ 1.7891,  0.8359,  0.8945,  ..., -1.1172, -0.3340,  0.3203]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.1650,  0.3223, -0.2383,  ...,  0.3867, -0.3438,  0.0703],
         [-0.0684, -0.1299,  0.4023,  ...,  0.3145,  0.4844,  0.3926],
         [ 0.1445,  0.8867,  0.5703,  ...,  0.4219, -0.1836, -0.4023],
         ...,
         [ 1.4062,  1.1797,  1.4688,  ...,  0.0469, -0.6367,  1.0234],
         [ 0.9453, -0.0439,  1.4062,  ..., -0.3633, -0.7383, -0.1465],
         [ 1.5000,  0.9766,  0.9531,  ..., -0.6289, -0.3906,  0.8281]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.2236,  0.2432, -0.5938,  ...,  0.4863, -0.5273, -0.2041],
         [-0.0469, -0.1836, -0.0273,  ...,  0.3535,  0.3750,  0.3047],
         [-0.2598,  1.1484,  0.4844,  ...,  0.4961, -0.1719, -0.5117],
         ...,
         [ 1.7188,  0.9688,  0.8828,  ..., -0.2441, -0.8672,  1.3047],
         [ 0.7891, -0.3984,  0.6797,  ..., -0.3594, -0.9922,  0.3164],
         [ 1.5000,  0.6250,  0.3672,  ..., -0.5469, -0.4902,  0.9766]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.3926, -0.0566, -0.2051,  ...,  0.4980, -0.7695, -0.0625],
         [ 0.3828, -0.2754,  0.3906,  ...,  0.0303,  0.1455,  0.2754],
         [-0.1113,  1.1250,  1.0859,  ...,  0.2656, -0.2520, -0.8203],
         ...,
         [ 1.9453,  0.9219,  1.6719,  ..., -0.2334, -0.5625,  1.2812],
         [ 1.0078, -0.2295,  1.3359,  ..., -0.5312, -0.9922,  0.3477],
         [ 1.6719,  0.9570,  1.1094,  ..., -0.7812, -0.3750,  0.9492]]],
       device='cuda:0', dtype=torch.bfloat16)), attentions=None)
image_features (after select_layer)
 <class 'torch.Tensor'>
image_features.shape
 torch.Size([1, 577, 1024])
[COND] select_feature=patch
【ENTER】if self.select_feature == 'patch':
original image_features
 tensor([[[ 0.2236,  0.2432, -0.5938,  ...,  0.4863, -0.5273, -0.2041],
         [-0.0469, -0.1836, -0.0273,  ...,  0.3535,  0.3750,  0.3047],
         [-0.2598,  1.1484,  0.4844,  ...,  0.4961, -0.1719, -0.5117],
         ...,
         [ 1.7188,  0.9688,  0.8828,  ..., -0.2441, -0.8672,  1.3047],
         [ 0.7891, -0.3984,  0.6797,  ..., -0.3594, -0.9922,  0.3164],
         [ 1.5000,  0.6250,  0.3672,  ..., -0.5469, -0.4902,  0.9766]]],
       device='cuda:0', dtype=torch.bfloat16)
after process
 tensor([[[-0.0469, -0.1836, -0.0273,  ...,  0.3535,  0.3750,  0.3047],
         [-0.2598,  1.1484,  0.4844,  ...,  0.4961, -0.1719, -0.5117],
         [ 1.0625, -0.0635, -0.3730,  ...,  0.0220,  0.0820,  0.4805],
         ...,
         [ 1.7188,  0.9688,  0.8828,  ..., -0.2441, -0.8672,  1.3047],
         [ 0.7891, -0.3984,  0.6797,  ..., -0.3594, -0.9922,  0.3164],
         [ 1.5000,  0.6250,  0.3672,  ..., -0.5469, -0.4902,  0.9766]]],
       device='cuda:0', dtype=torch.bfloat16)
【EXIT】if self.select_feature == 'patch':
image_features (return)
 tensor([[[-0.0469, -0.1836, -0.0273,  ...,  0.3535,  0.3750,  0.3047],
         [-0.2598,  1.1484,  0.4844,  ...,  0.4961, -0.1719, -0.5117],
         [ 1.0625, -0.0635, -0.3730,  ...,  0.0220,  0.0820,  0.4805],
         ...,
         [ 1.7188,  0.9688,  0.8828,  ..., -0.2441, -0.8672,  1.3047],
         [ 0.7891, -0.3984,  0.6797,  ..., -0.3594, -0.9922,  0.3164],
         [ 1.5000,  0.6250,  0.3672,  ..., -0.5469, -0.4902,  0.9766]]],
       device='cuda:0', dtype=torch.bfloat16)
image_features.shape
 torch.Size([1, 576, 1024])
after process image_features
 <class 'torch.Tensor'>
【EXIT】else (type(images) is not list):
image_features (return)
 tensor([[[-0.0469, -0.1836, -0.0273,  ...,  0.3535,  0.3750,  0.3047],
         [-0.2598,  1.1484,  0.4844,  ...,  0.4961, -0.1719, -0.5117],
         [ 1.0625, -0.0635, -0.3730,  ...,  0.0220,  0.0820,  0.4805],
         ...,
         [ 1.7188,  0.9688,  0.8828,  ..., -0.2441, -0.8672,  1.3047],
         [ 0.7891, -0.3984,  0.6797,  ..., -0.3594, -0.9922,  0.3164],
         [ 1.5000,  0.6250,  0.3672,  ..., -0.5469, -0.4902,  0.9766]]],
       device='cuda:0', dtype=torch.bfloat16)
image_features.shape
 torch.Size([1, 576, 1024])
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 4096, padding_idx=0)
  (layers): ModuleList(
    (0-31): 32 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=4096, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=4096, out_features=4096, bias=True)
  )
)
image_features (return)
 tensor([[[-0.1021,  0.0383, -0.0674,  ..., -0.1118, -0.0444, -0.0223],
         [-0.1348,  0.0366, -0.1045,  ...,  0.0403,  0.0635, -0.0654],
         [-0.1299, -0.0815, -0.3594,  ...,  0.2119,  0.1533, -0.0269],
         ...,
         [ 0.0115, -0.0767, -0.0732,  ...,  0.4551, -0.0674,  0.0060],
         [-0.0214, -0.0635,  0.0972,  ...,  0.4102, -0.1279, -0.0070],
         [ 0.0024, -0.1406,  0.0972,  ...,  0.3594, -0.0815, -0.1582]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ViewBackward0>)
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 4096, padding_idx=0)
  (layers): ModuleList(
    (0-31): 32 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=4096, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=4096, out_features=4096, bias=True)
  )
)
【EXIT】if inputs_embeds is None:
Return of 
def LlavaLlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, image_sizes, return_dict)
result (return)
 CausalLMOutputWithPast(loss=tensor(7.3094, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[  0.8242,   0.1855,  -0.7031,  ...,   1.6719,   2.6719,   1.1875],
         [ -9.0000,  -2.1562,   8.9375,  ...,  -6.4375,  -6.9688,  -5.9375],
         [-12.4375,  -7.8750,   3.5625,  ..., -10.2500, -10.3750, -11.1875],
         ...,
         [ -6.7812,  -3.1406,   4.2188,  ...,  -4.6562,  -3.5312,  -4.8750],
         [ -7.5312,  -4.7188,   4.1562,  ...,  -4.6250,  -4.5625,  -5.5000],
         [ -4.3438,  -0.9023,   2.0625,  ...,  -3.5312,  -4.0625,  -2.5469]]],
       device='cuda:0', grad_fn=<ToCopyBackward0>), past_key_values=None, hidden_states=None, attentions=None)

100%|██████████| 1/1 [00:01<00:00,  1.60s/it]
                                             
{'loss': 7.3094, 'learning_rate': 0.001, 'epoch': 1.0}

100%|██████████| 1/1 [00:01<00:00,  1.60s/it]current file path llava/train/llava_trainer.py
def _save_checkpoint(self, model, trial, metrics=None)
self
 <llava.train.llava_trainer.LLaVATrainer object at 0x7ed6341f4490>
model
 DeepSpeedEngine(
  (module): LlavaLlamaForCausalLM(
    (model): LlavaLlamaModel(
      (embed_tokens): Embedding(32000, 4096, padding_idx=0)
      (layers): ModuleList(
        (0-31): 32 x LlamaDecoderLayer(
          (self_attn): LlamaAttention(
            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
            (act_fn): SiLUActivation()
          )
          (input_layernorm): LlamaRMSNorm()
          (post_attention_layernorm): LlamaRMSNorm()
        )
      )
      (norm): LlamaRMSNorm()
      (vision_tower): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (mm_projector): Sequential(
        (0): Linear(in_features=1024, out_features=4096, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=4096, out_features=4096, bias=True)
      )
    )
    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
  )
)
trial
 None
metrics
 None
[COND] tune_mm_mlp_adapter=True
【ENTER】if getattr(self.args, 'tune_mm_mlp_adapter', False):
[COND] use_im_start_end=False
current file path llava/train/llava_trainer.py
def get_mm_adapter_state_maybe_zero_3(named_params, keys_to_match)
named_params
 <generator object Module.named_parameters at 0x7ed637a75000>
keys_to_match
 ['mm_projector', 'vision_resampler']
current file path llava/train/llava_trainer.py
def maybe_zero_3(param, ignore_status=False, name=None)
param
 Parameter containing:
tensor([[-0.0299, -0.0014,  0.0156,  ...,  0.0070,  0.0259, -0.0071],
        [-0.0011, -0.0205,  0.0048,  ..., -0.0051,  0.0042,  0.0079],
        [ 0.0115, -0.0201,  0.0222,  ..., -0.0037, -0.0095, -0.0099],
        ...,
        [ 0.0006, -0.0216,  0.0028,  ..., -0.0090,  0.0134,  0.0194],
        [-0.0238,  0.0164, -0.0289,  ...,  0.0141,  0.0113,  0.0054],
        [ 0.0259,  0.0069, -0.0188,  ...,  0.0201, -0.0157,  0.0148]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
ignore_status
 True
name
 model.mm_projector.0.weight
[COND] hasattr_ds_id=False
【ENTER】else (not hasattr(param, 'ds_id')):
【EXIT】else (not hasattr(param, 'ds_id')):
param (to return)
 tensor([[-0.0299, -0.0014,  0.0156,  ...,  0.0070,  0.0259, -0.0071],
        [-0.0011, -0.0205,  0.0048,  ..., -0.0051,  0.0042,  0.0079],
        [ 0.0115, -0.0201,  0.0222,  ..., -0.0037, -0.0095, -0.0099],
        ...,
        [ 0.0006, -0.0216,  0.0028,  ..., -0.0090,  0.0134,  0.0194],
        [-0.0238,  0.0164, -0.0289,  ...,  0.0141,  0.0113,  0.0054],
        [ 0.0259,  0.0069, -0.0188,  ...,  0.0201, -0.0157,  0.0148]],
       dtype=torch.bfloat16)
current file path llava/train/llava_trainer.py
def maybe_zero_3(param, ignore_status=False, name=None)
param
 Parameter containing:
tensor([-0.0119,  0.0082,  0.0117,  ...,  0.0188, -0.0243,  0.0286],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
ignore_status
 True
name
 model.mm_projector.0.bias
[COND] hasattr_ds_id=False
【ENTER】else (not hasattr(param, 'ds_id')):
【EXIT】else (not hasattr(param, 'ds_id')):
param (to return)
 tensor([-0.0119,  0.0082,  0.0117,  ...,  0.0188, -0.0243,  0.0286],
       dtype=torch.bfloat16)
current file path llava/train/llava_trainer.py
def maybe_zero_3(param, ignore_status=False, name=None)
param
 Parameter containing:
tensor([[-1.3123e-02, -1.5442e-02, -1.3733e-02,  ..., -9.8419e-04,
          1.7700e-03, -1.4709e-02],
        [-2.3346e-03, -1.3855e-02,  9.5215e-03,  ...,  7.8125e-03,
          4.2419e-03, -8.4229e-03],
        [-1.1841e-02,  1.4709e-02,  1.3794e-02,  ...,  8.4839e-03,
         -1.1597e-02, -6.6833e-03],
        ...,
        [-1.4282e-02,  1.9073e-03,  5.0049e-03,  ...,  1.3672e-02,
         -4.3945e-03,  1.3000e-02],
        [-8.7280e-03,  1.4893e-02, -7.4768e-03,  ...,  5.2185e-03,
         -5.2452e-05,  1.0910e-03],
        [-8.9722e-03,  3.6011e-03, -1.4587e-02,  ..., -1.0803e-02,
         -5.3711e-03,  9.6436e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
ignore_status
 True
name
 model.mm_projector.2.weight
[COND] hasattr_ds_id=False
【ENTER】else (not hasattr(param, 'ds_id')):
【EXIT】else (not hasattr(param, 'ds_id')):
param (to return)
 tensor([[-1.3123e-02, -1.5442e-02, -1.3733e-02,  ..., -9.8419e-04,
          1.7700e-03, -1.4709e-02],
        [-2.3346e-03, -1.3855e-02,  9.5215e-03,  ...,  7.8125e-03,
          4.2419e-03, -8.4229e-03],
        [-1.1841e-02,  1.4709e-02,  1.3794e-02,  ...,  8.4839e-03,
         -1.1597e-02, -6.6833e-03],
        ...,
        [-1.4282e-02,  1.9073e-03,  5.0049e-03,  ...,  1.3672e-02,
         -4.3945e-03,  1.3000e-02],
        [-8.7280e-03,  1.4893e-02, -7.4768e-03,  ...,  5.2185e-03,
         -5.2452e-05,  1.0910e-03],
        [-8.9722e-03,  3.6011e-03, -1.4587e-02,  ..., -1.0803e-02,
         -5.3711e-03,  9.6436e-03]], dtype=torch.bfloat16)
current file path llava/train/llava_trainer.py
def maybe_zero_3(param, ignore_status=False, name=None)
param
 Parameter containing:
tensor([ 0.0039, -0.0108, -0.0074,  ...,  0.0064, -0.0076, -0.0034],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
ignore_status
 True
name
 model.mm_projector.2.bias
[COND] hasattr_ds_id=False
【ENTER】else (not hasattr(param, 'ds_id')):
【EXIT】else (not hasattr(param, 'ds_id')):
param (to return)
 tensor([ 0.0039, -0.0108, -0.0074,  ...,  0.0064, -0.0076, -0.0034],
       dtype=torch.bfloat16)
to_return
 {'model.mm_projector.0.weight': tensor([[-0.0299, -0.0014,  0.0156,  ...,  0.0070,  0.0259, -0.0071],
        [-0.0011, -0.0205,  0.0048,  ..., -0.0051,  0.0042,  0.0079],
        [ 0.0115, -0.0201,  0.0222,  ..., -0.0037, -0.0095, -0.0099],
        ...,
        [ 0.0006, -0.0216,  0.0028,  ..., -0.0090,  0.0134,  0.0194],
        [-0.0238,  0.0164, -0.0289,  ...,  0.0141,  0.0113,  0.0054],
        [ 0.0259,  0.0069, -0.0188,  ...,  0.0201, -0.0157,  0.0148]],
       dtype=torch.bfloat16), 'model.mm_projector.0.bias': tensor([-0.0119,  0.0082,  0.0117,  ...,  0.0188, -0.0243,  0.0286],
       dtype=torch.bfloat16), 'model.mm_projector.2.weight': tensor([[-1.3123e-02, -1.5442e-02, -1.3733e-02,  ..., -9.8419e-04,
          1.7700e-03, -1.4709e-02],
        [-2.3346e-03, -1.3855e-02,  9.5215e-03,  ...,  7.8125e-03,
          4.2419e-03, -8.4229e-03],
        [-1.1841e-02,  1.4709e-02,  1.3794e-02,  ...,  8.4839e-03,
         -1.1597e-02, -6.6833e-03],
        ...,
        [-1.4282e-02,  1.9073e-03,  5.0049e-03,  ...,  1.3672e-02,
         -4.3945e-03,  1.3000e-02],
        [-8.7280e-03,  1.4893e-02, -7.4768e-03,  ...,  5.2185e-03,
         -5.2452e-05,  1.0910e-03],
        [-8.9722e-03,  3.6011e-03, -1.4587e-02,  ..., -1.0803e-02,
         -5.3711e-03,  9.6436e-03]], dtype=torch.bfloat16), 'model.mm_projector.2.bias': tensor([ 0.0039, -0.0108, -0.0074,  ...,  0.0064, -0.0076, -0.0034],
       dtype=torch.bfloat16)}
[COND] local_rank=0
【ENTER】if self.args.local_rank == 0 or self.args.local_rank == -1:
【EXIT】if self.args.local_rank == 0 or self.args.local_rank == -1:
【EXIT】if getattr(self.args, 'tune_mm_mlp_adapter', False):

                                             
{'train_runtime': 1.7562, 'train_samples_per_second': 0.569, 'train_steps_per_second': 0.569, 'train_loss': 7.30941915512085, 'epoch': 1.0}

100%|██████████| 1/1 [00:01<00:00,  1.60s/it]
100%|██████████| 1/1 [00:01<00:00,  1.76s/it]
【EXIT】if list(pathlib.Path(training_args.output_dir).glob(checkpoint-*)):
model.config.use_cache = True True
【COND】lora_enable=False
【ENTER】else of if training_args.lora_enable:
trainer <llava.train.llava_trainer.LLaVATrainer object at 0x7ed6341f4490>
current file path llava/train/train.py
def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str)
trainer
 <class 'llava.train.llava_trainer.LLaVATrainer'>
output_dir
 ./checkpoints/llava-v1.5-7b-pretrain
current file path llava/train/train.py
def get_mm_adapter_state_maybe_zero_3(named_params, keys_to_match)
named_params
 <generator object Module.named_parameters at 0x7ed637a74ba0>
keys_to_match
 ['mm_projector']
current file path llava/train/train.py
def maybe_zero_3(param, ignore_status=False, name=None)
param
 Parameter containing:
tensor([[-0.0299, -0.0014,  0.0156,  ...,  0.0070,  0.0259, -0.0071],
        [-0.0011, -0.0205,  0.0048,  ..., -0.0051,  0.0042,  0.0079],
        [ 0.0115, -0.0201,  0.0222,  ..., -0.0037, -0.0095, -0.0099],
        ...,
        [ 0.0006, -0.0216,  0.0028,  ..., -0.0090,  0.0134,  0.0194],
        [-0.0238,  0.0164, -0.0289,  ...,  0.0141,  0.0113,  0.0054],
        [ 0.0259,  0.0069, -0.0188,  ...,  0.0201, -0.0157,  0.0148]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
param.shape
 torch.Size([4096, 1024])
ignore_status
 True
name
 None
param (return)
 tensor([[-0.0299, -0.0014,  0.0156,  ...,  0.0070,  0.0259, -0.0071],
        [-0.0011, -0.0205,  0.0048,  ..., -0.0051,  0.0042,  0.0079],
        [ 0.0115, -0.0201,  0.0222,  ..., -0.0037, -0.0095, -0.0099],
        ...,
        [ 0.0006, -0.0216,  0.0028,  ..., -0.0090,  0.0134,  0.0194],
        [-0.0238,  0.0164, -0.0289,  ...,  0.0141,  0.0113,  0.0054],
        [ 0.0259,  0.0069, -0.0188,  ...,  0.0201, -0.0157,  0.0148]],
       dtype=torch.bfloat16)
param (return).shape
 torch.Size([4096, 1024])
current file path llava/train/train.py
def maybe_zero_3(param, ignore_status=False, name=None)
param
 Parameter containing:
tensor([-0.0119,  0.0082,  0.0117,  ...,  0.0188, -0.0243,  0.0286],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
param.shape
 torch.Size([4096])
ignore_status
 True
name
 None
param (return)
 tensor([-0.0119,  0.0082,  0.0117,  ...,  0.0188, -0.0243,  0.0286],
       dtype=torch.bfloat16)
param (return).shape
 torch.Size([4096])
current file path llava/train/train.py
def maybe_zero_3(param, ignore_status=False, name=None)
param
 Parameter containing:
tensor([[-1.3123e-02, -1.5442e-02, -1.3733e-02,  ..., -9.8419e-04,
          1.7700e-03, -1.4709e-02],
        [-2.3346e-03, -1.3855e-02,  9.5215e-03,  ...,  7.8125e-03,
          4.2419e-03, -8.4229e-03],
        [-1.1841e-02,  1.4709e-02,  1.3794e-02,  ...,  8.4839e-03,
         -1.1597e-02, -6.6833e-03],
        ...,
        [-1.4282e-02,  1.9073e-03,  5.0049e-03,  ...,  1.3672e-02,
         -4.3945e-03,  1.3000e-02],
        [-8.7280e-03,  1.4893e-02, -7.4768e-03,  ...,  5.2185e-03,
         -5.2452e-05,  1.0910e-03],
        [-8.9722e-03,  3.6011e-03, -1.4587e-02,  ..., -1.0803e-02,
         -5.3711e-03,  9.6436e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
param.shape
 torch.Size([4096, 4096])
ignore_status
 True
name
 None
param (return)
 tensor([[-1.3123e-02, -1.5442e-02, -1.3733e-02,  ..., -9.8419e-04,
          1.7700e-03, -1.4709e-02],
        [-2.3346e-03, -1.3855e-02,  9.5215e-03,  ...,  7.8125e-03,
          4.2419e-03, -8.4229e-03],
        [-1.1841e-02,  1.4709e-02,  1.3794e-02,  ...,  8.4839e-03,
         -1.1597e-02, -6.6833e-03],
        ...,
        [-1.4282e-02,  1.9073e-03,  5.0049e-03,  ...,  1.3672e-02,
         -4.3945e-03,  1.3000e-02],
        [-8.7280e-03,  1.4893e-02, -7.4768e-03,  ...,  5.2185e-03,
         -5.2452e-05,  1.0910e-03],
        [-8.9722e-03,  3.6011e-03, -1.4587e-02,  ..., -1.0803e-02,
         -5.3711e-03,  9.6436e-03]], dtype=torch.bfloat16)
param (return).shape
 torch.Size([4096, 4096])
current file path llava/train/train.py
def maybe_zero_3(param, ignore_status=False, name=None)
param
 Parameter containing:
tensor([ 0.0039, -0.0108, -0.0074,  ...,  0.0064, -0.0076, -0.0034],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
param.shape
 torch.Size([4096])
ignore_status
 True
name
 None
param (return)
 tensor([ 0.0039, -0.0108, -0.0074,  ...,  0.0064, -0.0076, -0.0034],
       dtype=torch.bfloat16)
param (return).shape
 torch.Size([4096])
to_return (return)
 {'model.mm_projector.0.weight': tensor([[-0.0299, -0.0014,  0.0156,  ...,  0.0070,  0.0259, -0.0071],
        [-0.0011, -0.0205,  0.0048,  ..., -0.0051,  0.0042,  0.0079],
        [ 0.0115, -0.0201,  0.0222,  ..., -0.0037, -0.0095, -0.0099],
        ...,
        [ 0.0006, -0.0216,  0.0028,  ..., -0.0090,  0.0134,  0.0194],
        [-0.0238,  0.0164, -0.0289,  ...,  0.0141,  0.0113,  0.0054],
        [ 0.0259,  0.0069, -0.0188,  ...,  0.0201, -0.0157,  0.0148]],
       dtype=torch.bfloat16), 'model.mm_projector.0.bias': tensor([-0.0119,  0.0082,  0.0117,  ...,  0.0188, -0.0243,  0.0286],
       dtype=torch.bfloat16), 'model.mm_projector.2.weight': tensor([[-1.3123e-02, -1.5442e-02, -1.3733e-02,  ..., -9.8419e-04,
          1.7700e-03, -1.4709e-02],
        [-2.3346e-03, -1.3855e-02,  9.5215e-03,  ...,  7.8125e-03,
          4.2419e-03, -8.4229e-03],
        [-1.1841e-02,  1.4709e-02,  1.3794e-02,  ...,  8.4839e-03,
         -1.1597e-02, -6.6833e-03],
        ...,
        [-1.4282e-02,  1.9073e-03,  5.0049e-03,  ...,  1.3672e-02,
         -4.3945e-03,  1.3000e-02],
        [-8.7280e-03,  1.4893e-02, -7.4768e-03,  ...,  5.2185e-03,
         -5.2452e-05,  1.0910e-03],
        [-8.9722e-03,  3.6011e-03, -1.4587e-02,  ..., -1.0803e-02,
         -5.3711e-03,  9.6436e-03]], dtype=torch.bfloat16), 'model.mm_projector.2.bias': tensor([ 0.0039, -0.0108, -0.0074,  ...,  0.0064, -0.0076, -0.0034],
       dtype=torch.bfloat16)}
to_return['model.mm_projector.0.weight'].shape
 torch.Size([4096, 1024])
to_return['model.mm_projector.0.bias'].shape
 torch.Size([4096])
to_return['model.mm_projector.2.weight'].shape
 torch.Size([4096, 4096])
to_return['model.mm_projector.2.bias'].shape
 torch.Size([4096])
【EXIT】else of if training_args.lora_enable:
[2025-09-12 08:52:17,305] [INFO] [launch.py:347:main] Process 19203 exits successfully.
