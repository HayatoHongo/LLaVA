[
  {
    "issue_number": 737,
    "state": "open",
    "created_by": "daiqing98",
    "created_at": "2023-11-02T06:04:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/737</URL>\n\n<TITLE>cpu_adam cuda is missing or is incompatible with installed torch[Question]</TITLE>\n\n<BODY>### Question\n\nWhen I replaced zero3.json with zero3_offload.json for finetuning, I receive this error:\r\n\r\n\"cpu_adam cuda is missing or is incompatible with installed torch\"\r\n\r\nMy CUDA version is 12.2 and torch version is 2.0.1 (+cu117)\r\n\r\nMay I ask any solutions?</BODY>\n\n<COMMENTS>\n<Comment by ybsu at 2024-06-03T10:21:21Z>\n> ### Question\r\n> When I replaced zero3.json with zero3_offload.json for finetuning, I receive this error:\r\n> \r\n> \"cpu_adam cuda is missing or is incompatible with installed torch\"\r\n> \r\n> My CUDA version is 12.2 and torch version is 2.0.1 (+cu117)\r\n> \r\n> May I ask any solutions?\r\n\r\nI met the same issue, have you solved it ? Thank you.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 736,
    "state": "open",
    "created_by": "kindasweetgal",
    "created_at": "2023-11-02T03:31:39Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/736</URL>\n\n<TITLE>[Usage] An  size mismatch error loading projector model during fine tuning。</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nWhen there was a shape mismatch error when loading weights, I found that when linear was initialized in debug, the weight would be empty.\r\n\r\nLog: \r\n![image](https://github.com/haotian-liu/LLaVA/assets/60832987/1f8bddab-d72a-483f-b2fa-f81df031b8f4)\r\n\r\n\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/60832987/3e681a6a-7d20-4ad4-91a0-6d3588a34ff0)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-05T01:01:05Z>\nHi, this is due to the DeepSpeed ZeRO-3. Please do not use that for debugging.\r\n\r\nFor debugging purposes, and if you do not have enough VRAM, you can turn on `--freeze_backbone True` to save VRAM (ONLY for debug) and do not forget to turn that off after you do the real training.\n</Comment>\n<Comment by Stardust-y at 2023-12-08T08:32:37Z>\nI've encountered the same error when using deepspeed zero3, so which script should be used for debugging?\n</Comment>\n<Comment by Junxiao-Ma at 2024-02-24T12:28:12Z>\nZero2 is Ok,\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 735,
    "state": "open",
    "created_by": "dotieuthien",
    "created_at": "2023-11-02T02:52:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/735</URL>\n\n<TITLE>[Question] Improve quality of short sentence output</TITLE>\n\n<BODY>### Question\n\nWhen I set the max new token param to 77, I think the result is just a cut of a long sentence with the sequence length = 77, and the result is quite bad at the end. How can I improve it?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 734,
    "state": "closed",
    "created_by": "dotieuthien",
    "created_at": "2023-11-02T02:50:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/734</URL>\n\n<TITLE>[Question] When I set the max new token param to 77, I think the result is just a cut of a long sentence with the sequence length = 77, and the result is quite bad at the end.</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 733,
    "state": "closed",
    "created_by": "ahmedhshahinn",
    "created_at": "2023-11-01T14:19:14Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/733</URL>\n\n<TITLE>[Question] How to load finetuned LoRA and projector model parts  from a checkpoint?</TITLE>\n\n<BODY>### Question\r\nAfter finetuning the LLM (using LoRA) and the `mm_projector`, how to load these weights for inference?\r\nMy understanding is that this is done by the function[`load_pretrained_model`](https://github.com/haotian-liu/LLaVA/blob/785f766fcddc86ffeaa62cd51cf7834a11c04e6d/llava/model/builder.py#L26), specifically[`this if condition`](https://github.com/haotian-liu/LLaVA/blob/785f766fcddc86ffeaa62cd51cf7834a11c04e6d/llava/model/builder.py#L49). However, this doesn't load the `mm_projector_weights`, right?\r\n\r\nHappy to submit a PR fix if my understanding is correct.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-01T16:27:08Z>\nHi. The projector weights are included in `non_lora_trainables.bin` and are loaded here: https://github.com/haotian-liu/LLaVA/blob/785f766fcddc86ffeaa62cd51cf7834a11c04e6d/llava/model/builder.py#L72-L75\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 732,
    "state": "closed",
    "created_by": "yjt-okkk",
    "created_at": "2023-11-01T10:59:27Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/732</URL>\n\n<TITLE>[Usage] IndexErrors while finetuning on my own dataset, cur_image_idx out of bounds</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nIndexErrors while finetuning on my own dataset\r\n\r\nCommand:\r\n```\r\ndeepspeed /home/jovyan/work/llava/LLaVA/llava/train.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed /home/jovyan/work/llava/LLaVA/scripts/zero3.json \\\r\n    --model_name_or_path /home/jovyan/work/model/llava-med/finetune_e2e_on_instruct_caption_in_text_cleaned-60k-3epoch \\\r\n    --version v1 \\\r\n    --data_path /home/jovyan/work/llava_data/llava_data_train.json \\\r\n    --image_folder /home/jovyan/work/ln_data \\\r\n    --vision_tower /home/jovyan/work/model/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-med-7b-task-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 2 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 0 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nLog: \r\n```\r\n File \"/home/jovyan/work/llava/LLaVA/llava/model/llava_arch.py\", line 154, in prepare_inputs_labels_for_multimodal\r\n    cur_image_features = image_features[cur_image_idx]\r\nIndexError: index 2 is out of bounds for dimension 0 with size 2\r\n\r\n```\r\n\r\nScreenshots:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/100284767/165b0607-854b-46ab-a8c7-cf60a3eddb42)</BODY>\n\n<COMMENTS>\n<Comment by White1973 at 2023-12-07T11:44:16Z>\nI have the same issue, could you tell me how you solve it.\n</Comment>\n<Comment by saeedkhaki92 at 2024-02-23T15:52:44Z>\n@White1973 @yjt-okkk , where you able to solve the issue? I think it is related to images with the same name in the data.\n</Comment>\n<Comment by accordtsai at 2024-03-13T15:06:28Z>\nI had this issue also, and solved it by fixing my prompt.\r\n\r\nDuring training, the prompt is formatted `<image>\\nHow many people are in this picture?`\r\nHowever, during inference, the `<image>` tag is not needed, and should be formatted like `How many people are in this picture?`\r\n\r\nFixing the prompt solved the IndexError issue for me.\n</Comment>\n<Comment by ayensujeremiah at 2024-05-07T11:28:32Z>\nDid any one here resolve this issue?\r\n\r\nmy data is a json that has both image-text samples and text-only samples…(samples with only conversation) mixed\r\n\r\ninitially I thought it was the mixture causing the error, so I regenerated one with all samples being image-text and still had this same error\r\n\r\nQuite annoying because when I try some dataset it works, but my main dataset I want to use doesn’t.\n</Comment>\n<Comment by ayensujeremiah at 2024-05-07T11:29:42Z>\nAnd yeah\r\n\r\ni have the image token <image> in all image samples, so I am not sure why it is not working\n</Comment>\n<Comment by SWHL at 2024-05-23T03:08:07Z>\nI met the same problem. \r\nI successfully reproduce it by repeating the data which has `<image>` strings more than one. \r\nThe data is as following:\r\n```json\r\n[\r\n    {\r\n        \"id\": \"xxxx\",\r\n        \"image\": \"xxxx\",\r\n        \"conversations\": [\r\n            {\r\n                \"from\": \"human\",\r\n                \"value\": \"<image>\\nRead the following text: <doc> Группа GO \\n Элемент   <coverpage> \\n Описание - Содержит ссылку на графическое изображение обложки книги. \\n Нет атрибутов. Должен содержать элементы изображений: \\n <image> 1..n (один или несколько, один обязателен). \\n Может содержаться в следующих элементах: \\n <title-info> 0..1 (один, опционально); \\n <src-title-info; 0..1 (один, опционально). \\n Пример использования \\n <coverpage><image l:href=\\\"#cover.jpg\\\"/></coverpage> \\n Элемент    <image> \\n Описание - Картинка, иллюстрация в тексте. \\n Определены два типа картинок: обычные (imageType) и внутри текста (inlineImageType). \\n Атрибуты \\n xlink:type (опциональный) \\n xlink:href - ссылка на собственно графические данные, обычно содержащиеся в элементе \\n <binary>; \\n alt (опциональный); \\n title (опциональный, для inline недопустимый) - подпись к картинке; \\n id (опциональный, для inline недопустимый) - для ссылок на картинку. \\n Нет подчиненных элементов, обычно нет закрывающего тэга (пустой элемент). \\n Может содержаться в следующих элементах: \\n Для обычных:\"\r\n            },\r\n            {\r\n                \"from\": \"gpt\",\r\n                \"value\": \"<body>; \\n <section>. \\n Для inline: \\n <coverpage>; \\n <p>; \\n <v>; \\n <subtitle>; \\n <th> (с версии 2.1); \\n <td> (с версии 2.1); \\n <text-author> (с версии 2.1). </doc>\"\r\n            }\r\n        ]\r\n    }\r\n]\r\n```\r\n\r\nMaybe that's the reason for the error.\n</Comment>\n<Comment by dcompgriff at 2024-05-31T22:46:25Z>\nI think this is a bug when there are no images in a batch. I think it's this line https://github.com/haotian-liu/LLaVA/blob/main/llava/model/llava_arch.py#L240. For the life of me I cannot figure our why 'cur_image_idx += 1' when there are no images in the list of tokens since cur_image_idx is the location of the next image feature to be processed. \r\n\r\nTheir code gets cur_image_features as 'cur_image_features = image_features[cur_image_idx]', but then a few lines later do 'cur_input_embeds = torch.cat([cur_input_embeds_1, cur_image_features[0:0]], dim=0)'. But cur_image_features[0:0] results in just an empty tensor. So, why?\n</Comment>\n<Comment by KejiaZhang-Robust at 2024-11-14T02:25:55Z>\nIt seems that the current iteration process in mPLUG-OWL2 results in issues with reusing the conv object. Each time a new message is appended to conv, it accumulates all previous messages, leading to unintended results during inference with multiple text pairs.\r\n\r\nTo address this, it’s important to reset conv at the start of each iteration rather than continually appending messages. This way, each inference run begins with a fresh conversation state, avoiding the problem of carrying over previous context.\r\n\r\nHere’s a suggested approach to resolve this issue:\r\n\t1.\tReinitialize conv at each iteration: Instead of appending new messages to conv, reset conv at the start of each iteration by reassigning it from conv_templates[\"mplug_owl2\"].copy().\r\n\t2.\tSet the new input prompt each time: After resetting conv, set inp = DEFAULT_IMAGE_TOKEN + args.prompt, then add messages using conv.append_message() as before.\r\n\r\nHere’s an example for clarity:\r\n```\r\nfor text_pair in text_pairs:\r\n    # Reinitialize conv for a clean conversation state\r\n    conv = conv_templates[\"mplug_owl2\"].copy()\r\n    \r\n    # Set up the prompt and conversation messages\r\n    inp = DEFAULT_IMAGE_TOKEN + text_pair[\"prompt\"]\r\n    conv.append_message(conv.roles[0], inp)\r\n    conv.append_message(conv.roles[1], None)\r\n    prompt = conv.get_prompt()\r\n    \r\n    # Proceed with inference as before\r\n    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').to(model.device)\r\n    \r\n    # [Additional inference code...]\r\n```\r\nThis method ensures that conv doesn’t accumulate previous messages, maintaining the intended behavior for each text pair in your iterations.\r\n\r\nHope this helps resolve the issue!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 730,
    "state": "open",
    "created_by": "pipilurj",
    "created_at": "2023-11-01T08:06:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/730</URL>\n\n<TITLE>[Usage] How to load fine-tuned lora checkpoint for CLI inference</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nHi! Could you explain how to load the model weights for lora after fine-tuning? Also the mm_projector seems to need be separately loaded? I did not find an argument to specify the path to the mm_projector for cli inference. Thanks for your help!</BODY>\n\n<COMMENTS>\n<Comment by baiyuting at 2023-11-01T11:12:45Z>\ndoes the scripts/finetune_lora.sh tune the mm_projector ?  or Just the LLM? I am also trying to fine-tuned with lora, but a little bit confused with the setting\n</Comment>\n<Comment by Nomiluks at 2023-11-27T10:05:08Z>\nHave you got any solution to load the fine-tuned model using LORA?\n</Comment>\n<Comment by Nomiluks at 2023-11-27T11:46:09Z>\nI am able to load the model using the following command:\r\n\r\n```\r\npython -m llava.serve.cli --model-path /home/user/LLaVA/checkpoints/llava-v1.5-13b-task-lora --model-base /home/user/llava-v1.5-7b --image-file \"/home/user/LLaVA/digital_tampering/test_data/image.jpeg\"\r\n```\n</Comment>\n<Comment by z3ugma at 2023-11-29T14:47:47Z>\n@Nomiluks could you produce a slightly more full example than what is found in https://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md ? It looks like you were successful at training a LoRA and saving a checkpoint. \r\n\r\nI would like to run https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task_lora.sh \r\n\r\nBut there are no instructions provided as to which parameter is the images directory, and then which JSON is your custom dataset as indicated in Finetune_Custom_Data.md  \r\n\r\n What is  deepspeed ./scripts/zero3.json \\ \r\n \r\n And is there documentation on the 37 command line flags here? I'd love to see just a minimal tutorial that I could follow since I was able to successfully prepare a custom dataset\n</Comment>\n<Comment by SoniaGrh at 2024-01-04T14:01:29Z>\nTry to use the Predictor class in the file [predict.py](https://github.com/haotian-liu/LLaVA/blob/main/predict.py) and replace the setup function of the class with this:\r\n\r\n`def setup(self) -> None:`\r\n`\r\n        self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\"checkpoints/llava-v1.5-13b-task-lora\", model_name=\"llava-v1.5-13b-lora-finetuned\", model_base=\"liuhaotian/llava-v1.5-13b\", load_8bit=False, load_4bit=False)\r\n`\r\n\r\n(If you have 'lora' in the model_name, you don't need the mm_projector)\r\n\r\nIt is working for me after launching this finetune script [scripts/v1_5/finetune_task_lora.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task_lora.sh) with my own parameters.\n</Comment>\n<Comment by hai-qi at 2024-01-09T11:15:56Z>\n> 我可以使用以下命令加载模型：\r\n> \r\n> ```\r\n> python -m llava.serve.cli --model-path /home/user/LLaVA/checkpoints/llava-v1.5-13b-task-lora --model-base /home/user/llava-v1.5-7b --image-file \"/home/user/LLaVA/digital_tampering/test_data/image.jpeg\"\r\n> ```\r\n\r\nWhen I use this command for inference, the following error occurs, but this file is missing from the fine-tuned file. What is the reason?\r\nFileNotFoundError: [Errno 2] No such file or directory: './checkpoints/llava-data_know/mm_projector.bin'\n</Comment>\n<Comment by SoniaGrh at 2024-01-09T13:24:50Z>\n> > 我可以使用以下命令加载模型：\r\n> > ```\r\n> > python -m llava.serve.cli --model-path /home/user/LLaVA/checkpoints/llava-v1.5-13b-task-lora --model-base /home/user/llava-v1.5-7b --image-file \"/home/user/LLaVA/digital_tampering/test_data/image.jpeg\"\r\n> > ```\r\n> \r\n> When I use this command for inference, the following error occurs, but this file is missing from the fine-tuned file. What is the reason? FileNotFoundError: [Errno 2] No such file or directory: './checkpoints/llava-data_know/mm_projector.bin'\r\n\r\nYou need to give the parameter model-name with the string 'lora' in the name, for example model_name=\"llava-v1.5-13b-lora-finetuned\". Then the script will understand that it is a different type of model.\n</Comment>\n<Comment by hai-qi at 2024-01-10T04:42:33Z>\n> > > 我可以使用以下命令加载模型：\r\n> > > ```\r\n> > > python -m llava.serve.cli --model-path /home/user/LLaVA/checkpoints/llava-v1.5-13b-task-lora --model-base /home/user/llava-v1.5-7b --image-file \"/home/user/LLaVA/digital_tampering/test_data/image.jpeg\"\r\n> > > ```\r\n> > \r\n> > \r\n> > When I use this command for inference, the following error occurs, but this file is missing from the fine-tuned file. What is the reason? FileNotFoundError: [Errno 2] No such file or directory: './checkpoints/llava-data_know/mm_projector.bin'\r\n> \r\n> You need to give the parameter model-name with the string 'lora' in the name, for example model_name=\"llava-v1.5-13b-lora-finetuned\". Then the script will understand that it is a different type of model.\r\n\r\nThank you very much for your reply, I will be able to use it after adding it.\n</Comment>\n<Comment by Nomiluks at 2024-01-11T05:23:26Z>\n> @Nomiluks could you produce a slightly more full example than what is found in https://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md ? It looks like you were successful at training a LoRA and saving a checkpoint.\r\n> \r\n> I would like to run https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task_lora.sh\r\n> \r\n> But there are no instructions provided as to which parameter is the images directory, and then which JSON is your custom dataset as indicated in Finetune_Custom_Data.md\r\n> \r\n> What is deepspeed ./scripts/zero3.json \\\r\n> \r\n> And is there documentation on the 37 command line flags here? I'd love to see just a minimal tutorial that I could follow since I was able to successfully prepare a custom dataset\r\n\r\n\r\n**Following things I did:**\r\nnote: Please make sure you have installed the necessary dependencies and you have an activated conda environment.\r\n1. Update the `finetune_task_lora.sh`:\r\n>  --data_path ./playground/data/complete_normalized_training_with_paths.json\r\n> --image_folder ./playground/data/images \r\n> --output_dir ./checkpoints/llava-v1.5-7b-task-lora\r\n> --per_device_train_batch_size 12 ----> Due to memory constraints\r\n\r\n2. conda activate llava\r\n \r\n3. Run the script : bash scripts/v1_5/finetune_task_lora.sh\r\n\r\n4. Model test: python -m llava.serve.cli --model-path \"checkpoints/llava-v1.5-7b-task-lora\" --model-base \"llava-v1.5-7b\" --image-file \"training_data/test_cropped_document_samples/6599560327fb1c3dc1155689/front/front.jpg\"\n</Comment>\n<Comment by Nomiluks at 2024-01-11T06:26:22Z>\n> Try to use the Predictor class in the file [predict.py](https://github.com/haotian-liu/LLaVA/blob/main/predict.py) and replace the setup function of the class with this:\r\n> \r\n> `def setup(self) -> None:` `self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\"checkpoints/llava-v1.5-13b-task-lora\", model_name=\"llava-v1.5-13b-lora-finetuned\", model_base=\"liuhaotian/llava-v1.5-13b\", load_8bit=False, load_4bit=False)`\r\n> \r\n> (If you have 'lora' in the model_name, you don't need the mm_projector)\r\n> \r\n> It is working for me after launching this finetune script [scripts/v1_5/finetune_task_lora.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task_lora.sh) with my own parameters.\r\n\r\nI want to load the model predict without the conversation behavior. \r\nI used your suggestion, but I am facing the following error:\r\n\r\n`ModuleNotFoundError: No module named 'cog'`\r\n\r\nCould you please help me to resolve this error?\n</Comment>\n<Comment by SoniaGrh at 2024-01-11T14:13:06Z>\n> > Try to use the Predictor class in the file [predict.py](https://github.com/haotian-liu/LLaVA/blob/main/predict.py) and replace the setup function of the class with this:\r\n> > `def setup(self) -> None:` `self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\"checkpoints/llava-v1.5-13b-task-lora\", model_name=\"llava-v1.5-13b-lora-finetuned\", model_base=\"liuhaotian/llava-v1.5-13b\", load_8bit=False, load_4bit=False)`\r\n> > (If you have 'lora' in the model_name, you don't need the mm_projector)\r\n> > It is working for me after launching this finetune script [scripts/v1_5/finetune_task_lora.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task_lora.sh) with my own parameters.\r\n> \r\n> I want to load the model predict without the conversation behavior. I used your suggestion, but I am facing the following error:\r\n> \r\n> `ModuleNotFoundError: No module named 'cog'`\r\n> \r\n> Could you please help me to resolve this error?\r\n\r\nIt doesn't resolve the error to install this package: [cog](https://pypi.org/project/cog/)?\n</Comment>\n<Comment by conheaven at 2024-03-03T14:44:46Z>\n> > @Nomiluks could you produce a slightly more full example than what is found in https://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md ? It looks like you were successful at training a LoRA and saving a checkpoint.你能提供一个比https://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md更完整的例子吗?看起来你成功地训练了一个LoRA并保存了一个检查点。\r\n> > I would like to run https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task_lora.sh我想跑https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task_lora.sh\r\n> > But there are no instructions provided as to which parameter is the images directory, and then which JSON is your custom dataset as indicated in Finetune_Custom_Data.md但是没有说明哪个参数是图像目录，然后哪个JSON是您的自定义数据集，如Finetune_Custom_Data.md所示\r\n> > What is deepspeed ./scripts/zero3.json \\什么是deepspeed ./scripts/zero3。json \\\r\n> > And is there documentation on the 37 command line flags here? I'd love to see just a minimal tutorial that I could follow since I was able to successfully prepare a custom dataset这里有关于37个命令行标志的文档吗?我希望看到一个最小的教程，我可以遵循，因为我能够成功地准备一个自定义数据集\r\n> \r\n> **Following things I did: 我做了以下事情:** note: Please make sure you have installed the necessary dependencies and you have an activated conda environment.注意:请确保您已经安装了必要的依赖项，并且您有一个激活的conda环境。\r\n> \r\n> 1. Update the `finetune_task_lora.sh`:   更新:\r\n> \r\n> > --data_path ./playground/data/complete_normalized_training_with_paths.json——data_path。/操场/数据/ complete_normalized_training_with_paths.json\r\n> > --image_folder ./playground/data/images——image_folder。/操场/数据/图像\r\n> > --output_dir ./checkpoints/llava-v1.5-7b-task-lora——output_dir。/检查点/ llava-v1.5-7b-task-lora\r\n> > --per_device_train_batch_size 12 ----> Due to memory constraints——per_device_train_batch_size 12 ---->由于内存限制\r\n> \r\n> 2. conda activate llava   康达激活熔岩\r\n> 3. Run the script : bash scripts/v1_5/finetune_task_lora.sh执行如下脚本:bash scripts/v1_5/finetune_task_lora.sh\r\n> 4. Model test: python -m llava.serve.cli --model-path \"checkpoints/llava-v1.5-7b-task-lora\" --model-base \"llava-v1.5-7b\" --image-file \"training_data/test_cropped_document_samples/6599560327fb1c3dc1155689/front/front.jpg\"模型测试:python -m llava. server .cli——模型路径“检查点/llava-v1.5-7b-任务-lora”——模型基础“llava-v1.5-7b”——图像文件“training_data/test_cropped_document_samples/6599560327fb1c3dc1155689/front/front.jpg”\r\n\r\nin the 4. my command is \"python -m llava.serve.cli \\\r\n    --model-base /data1/khw/llava \\\r\n    --model-path /data1/khw/output_llava/merge_model-lora \\\r\n    --image-file /data1/khw/img/1.jpg \\\"\r\n    but it show\r\n    Traceback (most recent call last):\r\n  File \"/home/khw/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/khw/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/khw/llava/LLaVA/llava/serve/cli.py\", line 126, in <module>\r\n    main(args)\r\n  File \"/home/khw/llava/LLaVA/llava/serve/cli.py\", line 32, in main\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n  File \"/home/khw/llava/LLaVA/llava/model/builder.py\", line 122, in load_pretrained_model\r\n    model = AutoModelForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, **kwargs)\r\n  File \"/home/khw/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\r\n    raise ValueError(\r\nValueError: Unrecognized configuration class <class 'transformers.models.llava.configuration_llava.LlavaConfig'> for this kind of AutoModel: AutoModelForCausalLM.\r\nModel type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, ElectraConfig, ErnieConfig, FalconConfig, FuyuConfig, GemmaConfig, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, LlamaConfig, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MptConfig, MusicgenConfig, MvpConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, LlavaConfig, LlavaMptConfig, LlavaMistralConfig.\r\n\r\ncould you help me? thank you in advance\n</Comment>\n<Comment by iremonur at 2024-05-19T15:56:38Z>\nHi @conheaven,\r\n\r\nI got the same error, did you find a solution?\n</Comment>\n<Comment by conheaven at 2024-05-21T06:52:13Z>\n> Hi @conheaven,\r\n> \r\n> I got the same error, did you find a solution?\r\n\r\nsorry, i didn't find any feasible solution\n</Comment>\n<Comment by lowestbuaaer at 2024-08-08T08:45:06Z>\nJust use the \"scripts/merge_lora_weights.py\" to merge lora checkpoints into models before inference.\r\nGuidance:https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 729,
    "state": "open",
    "created_by": "yangzian035210",
    "created_at": "2023-11-01T06:57:23Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/729</URL>\n\n<TITLE>[Question] finetune LLaVA-1.5 with LoRA.: does not appear to have a file named config.json.</TITLE>\n\n<BODY>### Question\n\nThanks for your great work! I have a question about finetune LLaVA-1.5 with LoRA:\r\nOSError: /checkpoints/llava-v1.5-13b-lora-v2/checkpoint-6000 does not appear to have a file named config.json.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/129492824/4b54f165-0d43-4908-b283-66d37ced69e9)</BODY>\n\n<COMMENTS>\n<Comment by dhifafaz at 2023-11-02T13:37:46Z>\nhave you figured it out?\n</Comment>\n<Comment by CiaoHe at 2023-11-07T13:54:02Z>\nIt seems like we need add a trigger when saving intermediate checkpoint, like the ones after train finished:\r\n```\r\nif training_args.lora_enable:\r\n    state_dict = get_peft_state_maybe_zero_3(\r\n        model.named_parameters(), training_args.lora_bias\r\n    )\r\n    non_lora_state_dict = get_peft_state_non_lora_maybe_zero_3(\r\n        model.named_parameters()\r\n    )\r\n    if training_args.local_rank == 0 or training_args.local_rank == -1:\r\n        model.config.save_pretrained(training_args.output_dir)\r\n        model.save_pretrained(training_args.output_dir, state_dict=state_dict)\r\n        torch.save(non_lora_state_dict, os.path.join(training_args.output_dir, 'non_lora_trainables.bin'))\r\nelse:\r\n    safe_save_model_for_hf_trainer(trainer=trainer,\r\n                                   output_dir=training_args.output_dir)\r\n```\n</Comment>\n<Comment by CiaoHe at 2023-11-07T15:28:06Z>\nI added a custom callback just above the trainer initialize\r\n```python\r\n# save callback\r\nfrom transformers import TrainerCallback\r\nclass SaveCallback(TrainerCallback):\r\n    def on_save(self, args, state, control, **kwargs):\r\n        checkpoint_dir = os.path.join(args.output_dir, 'checkpoint-{}'.format(state.global_step))\r\n        if args.lora_enable:\r\n            state_dict = get_peft_state_maybe_zero_3(\r\n                model.named_parameters(), training_args.lora_bias\r\n            )\r\n            non_lora_state_dict = get_peft_state_non_lora_maybe_zero_3(\r\n                model.named_parameters()\r\n            )\r\n            if args.local_rank in [-1, 0]:\r\n                model.config.save_pretrained(checkpoint_dir)\r\n                model.save_pretrained(checkpoint_dir, state_dict=state_dict)\r\n                torch.save(non_lora_state_dict, os.path.join(checkpoint_dir, 'non_lora_trainables.bin'))\r\n```\r\nand pass it to the trainer initializing\r\n```python\r\ntrainer = LLaVATrainer(model=model,\r\n                tokenizer=tokenizer,\r\n                args=training_args,\r\n                callbacks=[SaveCallback()],\r\n                **data_module)\r\n```\n</Comment>\n<Comment by wuwu-C at 2024-04-20T12:41:28Z>\nhey I met the same problem. Did you solve this?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 728,
    "state": "open",
    "created_by": "Road2Redemption",
    "created_at": "2023-11-01T04:32:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/728</URL>\n\n<TITLE>[Usage] RuntimeError: Error(s) in loading state_dict for Linear:</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nThanks to your contribution to the work! I met some issues while training.\r\nI want to finetune the llava-llama2-7b-chat model on custom data. I used a two step finetuning process: I firstly did finetune-lora on one dataset, and got the lora weights and then merged them as llava-llama2-7b-chat-MERGE. Then I wanted to use the MERGE model to train on another dataset, and when I did this, there is something wrong with my adaptor.\r\nCommand:\r\n```\r\n#!/bin/bash\r\n\r\n# Uncomment and set the following variables correspondingly to run this script:\r\n\r\n################## VICUNA ##################\r\n# PROMPT_VERSION=v1\r\n# MODEL_VERSION=\"vicuna-v1-3-7b\"\r\n################## VICUNA ##################\r\n\r\n################## LLaMA-2 ##################\r\nPROMPT_VERSION=\"llava_llama_2\"\r\nMODEL_VERSION=\"llama-2-7b-chat\"\r\n################## LLaMA-2 ##################\r\n\r\ndeepspeed --num_gpus=2 llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3_offload.json \\\r\n    --lora_enable True \\\r\n    --model_name_or_path /data6/xyc/chartcaption/LLaVA/checkpoints/llava-llama-2-7b-chat-finetune_lora-MERGE \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path ./playground/data/vistext_train_full.json \\\r\n    --image_folder /data6/xyc/chartcaption/vistext/data/baseline_data-v2/train \\\r\n    --vision_tower ./openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter /data6/xyc/models/llava-$MODEL_VERSION-pretrain/mm_projector.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-$MODEL_VERSION-finetune_lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nLog: \r\n```\r\nAdding LoRA adapters...\r\n[2023-11-01 04:16:24,003] [WARNING] [partition_parameters.py:836:_post_init_method] param `class_embedding` in CLIPVisionEmbeddings not on GPU so was not broadcasted from rank 0\r\n[2023-11-01 04:16:25,785] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 7.05B parameters\r\nTraceback (most recent call last):\r\n  File \"/data6/xyc/chartcaption/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/data6/xyc/chartcaption/LLaVA/llava/train/train.py\", line 875, in train\r\n    model.get_model().initialize_vision_modules(\r\n  File \"/data6/xyc/chartcaption/LLaVA/llava/model/llava_arch.py\", line 82, in initialize_vision_modules\r\n    self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'))\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2041, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for Linear:\r\n\tsize mismatch for weight: copying a param with shape torch.Size([4096, 1024]) from checkpoint, the shape in current model is torch.Size([0]).\r\n\tsize mismatch for bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([0]).\r\nTraceback (most recent call last):\r\n  File \"/data6/xyc/chartcaption/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/data6/xyc/chartcaption/LLaVA/llava/train/train.py\", line 875, in train\r\n    model.get_model().initialize_vision_modules(\r\n  File \"/data6/xyc/chartcaption/LLaVA/llava/model/llava_arch.py\", line 82, in initialize_vision_modules\r\n    self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'))\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2041, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for Linear:\r\n\tsize mismatch for weight: copying a param with shape torch.Size([4096, 1024]) from checkpoint, the shape in current model is torch.Size([0]).\r\n\tsize mismatch for bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([0]).\r\n[2023-11-01 04:16:31,040] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1912392\r\n[2023-11-01 04:16:31,136] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1912393\r\n[2023-11-01 04:16:31,136] [ERROR] [launch.py:321:sigkill_handler] ['/opt/conda/envs/llava/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero3_offload.json', '--lora_enable', 'True', '--model_name_or_path', '/data6/xyc/chartcaption/LLaVA/checkpoints/llava-llama-2-7b-chat-finetune_lora-MERGE', '--version', 'llava_llama_2', '--data_path', './playground/data/vistext_train_full.json', '--image_folder', '/data6/xyc/chartcaption/vistext/data/baseline_data-v2/train', '--vision_tower', './openai/clip-vit-large-patch14', '--pretrain_mm_mlp_adapter', '/data6/xyc/models/llava-llama-2-7b-chat-pretrain/mm_projector.bin', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './checkpoints/llava-llama-2-7b-chat-finetune_lora', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = 1\r\n```\r\n![image](https://github.com/haotian-liu/LLaVA/assets/114891845/bffeb5e8-10aa-4344-943d-3dc8e5e9bdc6)\r\n![image](https://github.com/haotian-liu/LLaVA/assets/114891845/159e9781-4e00-4cbf-968b-6897634886b5)\r\n\r\nI am not quite familiar about how the adaptor works, is there any solution to this? Thanks a lot!</BODY>\n\n<COMMENTS>\n<Comment by Road2Redemption at 2023-11-01T04:37:28Z>\n--pretrain_mm_mlp_adapter /data6/xyc/models/llava-$MODEL_VERSION-pretrain/mm_projector.bin \\\r\nwhen I deleted this line, the training works, but I am afraid that the performance will be not as good as that with the adaptor.\n</Comment>\n<Comment by Road2Redemption at 2023-11-01T06:34:34Z>\nummm I changed zero3_offload to zero2 and it also worked\n</Comment>\n<Comment by kindasweetgal at 2023-11-02T04:30:01Z>\nI have the same question, I don't understand why. I changed zero3_offload to zero2  or comment out the deepspeed parameter,  the program will stop automatically. Is this related to my memory size?\n</Comment>\n<Comment by JJJoeha at 2023-12-18T10:05:43Z>\nsame issue here, any updates?\n</Comment>\n<Comment by 459737087 at 2024-01-12T04:55:36Z>\nsame issue\n</Comment>\n<Comment by zhang9302002 at 2024-02-04T15:07:47Z>\nsame issue, dun\n</Comment>\n<Comment by Road2Redemption at 2024-03-02T06:27:45Z>\nI saw on other issues that the --pretrain_mm_mlp_adapter is used only for fine tuning after LLaVA's own pretraining. If continuous task fine tuning on our own custom dataset, this parameter should not be included.\r\n\r\nHowever I am still not sure how the deepseed zero2/zero3_offload affects this problem T_T\n</Comment>\n<Comment by zhang9302002 at 2024-03-03T01:38:54Z>\nGuys, It seems that deepspeed zero3 will do a lazy parameter initialization. So in our main() function, the parameters of model are set to [ ]. It cannot be reload from --pretrain_mm_mlp_adapter.\r\nI solve this by use full deepspeed ckpt, instead of --pretrain_mm_mlp_adapter, Have a try.\n</Comment>\n<Comment by xinyuliu-jeffrey at 2024-04-06T05:28:47Z>\n@zhang9302002 Could you please share how to load pretrain mlp without --pretrain_mm_mlp_adapter? Thanks.\n</Comment>\n<Comment by zhang9302002 at 2024-04-08T02:47:34Z>\n@xinyuliu-jeffrey If you are using deepspeed, use `--model_name_or_path your_model/checkpoint-3000` argument to let deepspeed load the full model itself.\r\nBTW, I suggest comment `def _save_checkpoint()` and `def _save()` in llava_trainer.py to avoid overriding original deepspeed implementation.\n</Comment>\n<Comment by xinyuliu-jeffrey at 2024-04-08T03:32:45Z>\n@zhang9302002 Thanks! I saved full model with deepseed and the pretrained weights can be successfully loaded. Only adapter will be saved when `--tune_mm_mlp_adapter` is True, so these functions shall be indeed commented.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 727,
    "state": "open",
    "created_by": "taikai-zz",
    "created_at": "2023-11-01T01:47:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/727</URL>\n\n<TITLE>Does AMDGPU support</TITLE>\n\n<BODY>### Question\n\nDoes AMDGPU support</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 726,
    "state": "open",
    "created_by": "unmo",
    "created_at": "2023-11-01T01:17:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/726</URL>\n\n<TITLE>[Usage] RuntimeError: probability tensor contains either `inf`, `nan` or element < 0</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nI executed cli script in the following command. I have encountered a problem \"RuntimeError: probability tensor contains either `inf`, `nan` or element < 0\".\r\n\r\nUsing model is a trained on custom data.\r\nWhat is the problem?\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.cli \\\r\n    --model-path ./checkpoints/llava_vicuna1.5-7b_clip-vit-l-336_task_epoch20\\\r\n    --image-file ./playground/data/LLaVA-Pretrain/images/test2/4.png\\\r\n```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/app/LLaVA/llava/serve/cli.py\", line 125, in <module>\r\n    main(args)\r\n  File \"/app/LLaVA/llava/serve/cli.py\", line 94, in main\r\n    output_ids = model.generate(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 1588, in generate\r\n    return self.sample(\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 2678, in sample\r\n    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n```</BODY>\n\n<COMMENTS>\n<Comment by crazycth at 2023-11-09T08:12:25Z>\n+1\n</Comment>\n<Comment by LiWentomng at 2023-11-11T13:08:25Z>\n@unmo @crazycth \r\nHello,  any update?  I also encounter this problem.\n</Comment>\n<Comment by unmo at 2023-11-13T00:49:43Z>\nSorry, I have not been able to resolve it yet either.\n</Comment>\n<Comment by haotian-liu at 2023-11-13T01:23:00Z>\nHi,\r\n\r\n1. The first thing to try is to see if it helps by setting `--conv-mode llava_v1`?\r\n2. If not, please share: what script did you run to train the model (by providing the exact command)? Also, please share the wandb link to see if the loss curves are normal.\n</Comment>\n<Comment by Junxiao-Ma at 2024-02-24T12:44:37Z>\nI'm having the same issue: when I train the model to run the run_llava.py, I also get this error, I find that the output is all NAN, but I don't know why it's happening\n</Comment>\n<Comment by ghazalsaheb at 2024-07-30T04:18:08Z>\nFacing the same issue here.  The output is nan although my w&B loss looks fine.\n</Comment>\n<Comment by ghazalsaheb at 2024-08-06T19:46:56Z>\nUpdate: I was able to resolve the issue by changing the base model from hugging face's \"llava-hf/llava-1.5-7b-hf\"to \"liuhaotian/llava-v1.5-7b\". It resolved the NaN issue and the training performance got much better.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 725,
    "state": "open",
    "created_by": "SuperMaximus1984",
    "created_at": "2023-10-31T20:47:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/725</URL>\n\n<TITLE>Use LLaVA CLI with initial prompt</TITLE>\n\n<BODY>### Question\n\nHow can I use LLaVA CLI and pass an initial prompt so that I could get a response without typing this prompt?\r\n\r\n```\r\npython -m llava.serve.cli \\\r\n    --model-path liuhaotian/llava-v1.5-7b \\\r\n    --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n    --load-4bit\r\n```\r\n\r\nIs it possible to exit CLI worker after grabbing the text response?\r\n\r\nThanks!</BODY>\n\n<COMMENTS>\n<Comment by SuperMaximus1984 at 2023-11-01T21:50:17Z>\nupd: Is it possible to pass the initial prompt via CLI, receive one-time response and exit the script? That would be an ideal case. How can I realize it?\n</Comment>\n<Comment by haotian-liu at 2023-11-02T00:37:23Z>\nYou may be looking for https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/run_llava.py\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 724,
    "state": "open",
    "created_by": "Deaddawn",
    "created_at": "2023-10-31T18:35:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/724</URL>\n\n<TITLE>[Question] Merging lora weights got strange vision_tower weight in pytorch_model.bin.index.json</TITLE>\n\n<BODY>### Question\n\nHi, there. I used merge_lora_weights.py to merge lora fintuning weights and base model. But got some strange weights in the final output pytorch_model.bin.index.json, like this:\r\n\"model.vision_tower.vision_tower.vision_model.embeddings.class_embedding\": \"pytorch_model-00002-of-00002.bin\",\r\n    \"model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight\": \"pytorch_model-00002-of-00002.bin\",\r\n    \"model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight\": \"pytorch_model-00002-of-00002.bin\",\r\n    \"model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias\": \"pytorch_model-00002-of-00002.bin\",\r\n    \"model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight\": \"pytorch_model-00002-of-00002.bin\",\r\n    \"model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias\": \"pytorch_model-00002-of-00002.bin\",\r\n    \"model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight\": \"pytorch_model-00002-of-00002.bin\",\r\n    \"model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias\": \"pytorch_model-00002-of-00002.bin\",\r\n    \"model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight\": \"pytorch_model-00002-of-00002.bin\",\r\n    \"model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias\": \"pytorch_model-00002-of-00002.bin\",\r\n    \"model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight\": \"pytorch_model-00002-of-00002.bin\",\r\n\r\nIs this expected?</BODY>\n\n<COMMENTS>\n<Comment by jerett at 2023-12-26T06:30:46Z>\nsame issue.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 723,
    "state": "closed",
    "created_by": "ahmedhshahin",
    "created_at": "2023-10-31T16:02:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/723</URL>\n\n<TITLE>[Question] Can you clarify the intended behavior of `lora_enable` and `freeze_backbone`?</TITLE>\n\n<BODY>### Question\n\nI am finding these two flags a bit confusing. Specifically, what will happen if `lora_enable=True` and `freeze_backbone=False`? Is that full fine-tuning or LoRA fine-tuning? If it is the latter, then what will happen if we set `freeze_backbone=True` and `lora_enable=True`? \r\n\r\nThanks</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-31T16:22:16Z>\nHi @ahmedhshahin \r\n\r\nThe flag `freeze_backbone` is not for training purposes, it is just for saving the memory for debugging the code on a single GPU.\r\n\r\nIf there is a particular reason for using this flag to train the model, please let me know, thanks.\n</Comment>\n<Comment by ahmedhshahin at 2023-10-31T16:24:21Z>\n@haotian-liu \r\nThanks for the clarification. I imagine you can use it when you are training the projector network only, so you are freezing the LLM and the visual encoder, right?\n</Comment>\n<Comment by haotian-liu at 2023-10-31T16:26:08Z>\nFor that, you should use `--tune_mm_mlp_adapter True`. The name may be a bit confusing, but it will freeze the language model / vision encoder and unfreeze the projector. It does not freeze the lora weights.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/pretrain.sh#L11C5-L11C5\n</Comment>\n<Comment by ahmedhshahin at 2023-10-31T16:27:28Z>\nWhat if I want to finetune both the projector and the LLM (LoRA)?\n</Comment>\n<Comment by haotian-liu at 2023-10-31T16:28:38Z>\nBy default the vision encoder is always frozen. So you can just use the LoRA scripts.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_lora.sh\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task_lora.sh\n</Comment>\n<Comment by ahmedhshahin at 2023-10-31T16:29:56Z>\nGreat, Thanks! That's very helpful.\n</Comment>\n<Comment by St3p99 at 2024-04-05T09:34:21Z>\n> By default the vision encoder is always frozen. So you can just use the LoRA scripts.\r\n> \r\n> https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_lora.sh https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task_lora.sh\r\n\r\nUsing following FT script you're training both projector and LLM. However, since tune_mm_mlp_adapter is False the mm_projector.bin will not be saved! \r\n\r\n[https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_lora.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_lora.sh)\n</Comment>\n<Comment by SFaegheh at 2025-05-03T21:05:13Z>\nWhat if we want to finetune both the Vision encode and projector?\n</Comment>\n<Comment by YoungLer at 2025-08-03T12:18:40Z>\n> What if we want to finetune both the Vision encode and projector?\n\nI have the same question. Maybe we can refer to:  [LLaMA-Factory/issues/2046](https://github.com/hiyouga/LLaMA-Factory/issues/2046) ,setting the parameter `--lora_target`  if you want to use lora, setting `--additional_target` if you want to use full parameter fine-tuning.\nI am not sure if it is correct. If you have any ideas, please feel free to discuss. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 719,
    "state": "open",
    "created_by": "miznchimaki",
    "created_at": "2023-10-31T10:44:56Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/719</URL>\n\n<TITLE>[Question] A question about the LLaVA-v1.5 checkpoint's float number precision</TITLE>\n\n<BODY>### Question\r\n\r\nHello, Thanks for your great work! I have a question about the checkpoint's float number precision. I used torch.load function to view the chekpoint's float precision on your huggingface hub, i.e. `pytorch_model-00001-of-00003.bin`, `pytorch_model-00002-of-00003`.bin and `pytorch_model-00003-of-00003.bin`. Their precision can be either `float16` or `float32`.\r\nWhen I reproduced LLaVA-v1.5 pretraining and finetuning phases according to your instruction, I also got my own checkpoint files. But I used torch.load function to view the float precision of my own checkpoints, all float numbers' precision is `bfloat16`. I have two question about this phenomenon:\r\n(1) What causes the difference of float precision between my and your checkpoint files?\r\n(2) Does the bfloat16 checkpoint files have negative influence on evaluation benchmark?\r\n\r\nLooking forward to your reply.</BODY>\n\n<COMMENTS>\n<Comment by tingxueronghua at 2023-10-31T11:10:41Z>\nbfloat16 is better float16 and could only be utilized on GPUs with Ampere architecture.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 718,
    "state": "closed",
    "created_by": "qiuhaining",
    "created_at": "2023-10-31T09:00:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/718</URL>\n\n<TITLE>[Usage]  finetune_task_lora in custom datasets, but KeyError: 'LlavaConfig'</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: KeyError: 'LlavaConfig'\r\n\r\nCommand:\r\n./scripts/v1_5/finetune_task_lora.sh\r\n\r\nLog: \r\n\r\n[2023-10-31 16:49:49,791] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-10-31 16:51:00,501] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\n[2023-10-31 16:51:00,501] [INFO] [runner.py:555:main] cmd = /home/luban/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero3.json --model_name_or_path /nfs/dataset-adas/qiushi_data/LLM/llava-v1.5-13b --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_tower /mnt/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b-task-lora --num_train_epochs 1 --per_device_train_batch_size 2 --per_device_eval_batch_size 2 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb\r\n[2023-10-31 16:51:09,706] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-10-31 16:51:25,892] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}\r\n[2023-10-31 16:51:25,892] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0\r\n[2023-10-31 16:51:25,892] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\r\n[2023-10-31 16:51:25,892] [INFO] [launch.py:163:main] dist_world_size=1\r\n[2023-10-31 16:51:25,892] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0\r\n[2023-10-31 16:51:43,162] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-10-31 16:51:58,497] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-10-31 16:51:58,497] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-10-31 16:51:58,497] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n[W socket.cpp:426] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).\r\n[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).\r\n[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).\r\n[2023-10-31 16:52:24,341] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 13.05B parameters\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [02:25<00:00, 48.62s/it]\r\nAdding LoRA adapters...\r\nTraceback (most recent call last):\r\n  File \"/nfs/volume-379-4/qiushi/codes/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/nfs/volume-379-4/qiushi/codes/LLaVA/llava/train/train.py\", line 850, in train\r\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\r\n  File \"/home/luban/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 718, in from_pretrained\r\n    tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]\r\n  File \"/home/luban/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 663, in __getitem__\r\n    model_type = self._reverse_config_mapping[key.__name__]\r\nKeyError: 'LlavaConfig'\r\n[2023-10-31 16:58:30,440] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2885847\r\n[2023-10-31 16:58:30,440] [ERROR] [launch.py:321:sigkill_handler] ['/home/luban/miniconda3/envs/llava/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=0', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', '/nfs/dataset-adas/qiushi_data/LLM/llava-v1.5-13b', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--vision_tower', '/mnt/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b-task-lora', '--num_train_epochs', '1', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '2', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = 1\r\n\r\nScreenshots:\r\n<img width=\"738\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/23293902/0612ad4e-4f43-41cb-9804-89132f585583\"></BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-05T00:13:57Z>\nCan you please check if you have the proper tokenizer config in the local folder? Thanks.\r\n\r\nhttps://huggingface.co/liuhaotian/llava-v1.5-13b/blob/main/tokenizer_config.json\n</Comment>\n<Comment by qiuhaining at 2023-11-05T00:21:35Z>\n> Can you please check if you have the proper tokenizer config in the local folder? Thanks.\r\n> \r\n> https://huggingface.co/liuhaotian/llava-v1.5-13b/blob/main/tokenizer_config.json\r\n\r\n Yes，this is my tokenizer_config.json , the same to yours\r\n\r\n\r\n{\r\n  \"add_bos_token\": true,\r\n  \"add_eos_token\": false,\r\n  \"bos_token\": {\r\n    \"__type\": \"AddedToken\",\r\n    \"content\": \"<s>\",\r\n    \"lstrip\": false,\r\n    \"normalized\": false,\r\n    \"rstrip\": false,\r\n    \"single_word\": false\r\n  },\r\n  \"clean_up_tokenization_spaces\": false,\r\n  \"eos_token\": {\r\n    \"__type\": \"AddedToken\",\r\n    \"content\": \"</s>\",\r\n    \"lstrip\": false,\r\n    \"normalized\": false,\r\n    \"rstrip\": false,\r\n    \"single_word\": false\r\n  },\r\n  \"legacy\": false,\r\n  \"model_max_length\": 2048,\r\n  \"pad_token\": null,\r\n  \"padding_side\": \"right\",\r\n  \"sp_model_kwargs\": {},\r\n  \"tokenizer_class\": \"LlamaTokenizer\",\r\n  \"unk_token\": {\r\n    \"__type\": \"AddedToken\",\r\n    \"content\": \"<unk>\",\r\n    \"lstrip\": false,\r\n    \"normalized\": false,\r\n    \"rstrip\": false,\r\n    \"single_word\": false\r\n  }\r\n}\n</Comment>\n<Comment by haotian-liu at 2023-11-05T00:53:02Z>\nThis is strange. What if you manually load from this directory: `transformers.AutoTokenizer.from_pretrained('xxx')`? I don't see why it will try to find something related to `llava`\r\n\r\n> tokenizer = transformers.AutoTokenizer.from_pretrained(\r\n> File \"/home/luban/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 718, in from_pretrained\r\n> tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]\r\n> File \"/home/luban/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 663, in getitem\r\n> model_type = self._reverse_config_mapping[key.name]\r\n> KeyError: 'LlavaConfig'\n</Comment>\n<Comment by qiuhaining at 2023-11-06T03:44:02Z>\n> This is strange. What if you manually load from this directory: `transformers.AutoTokenizer.from_pretrained('xxx')`? I don't see why it will try to find something related to `llava`\r\n> \r\n> > tokenizer = transformers.AutoTokenizer.from_pretrained(\r\n> > File \"/home/luban/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 718, in from_pretrained\r\n> > tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]\r\n> > File \"/home/luban/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 663, in getitem\r\n> > model_type = self._reverse_config_mapping[key.name]\r\n> > KeyError: 'LlavaConfig'\r\n\r\nI found something maybe related to this problem.\r\n1、  tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)， tokenizer_config == {}\r\n2、“llava” not in TOKENIZER_MAPPING\n</Comment>\n<Comment by qiuhaining at 2023-11-06T06:09:02Z>\nits my fault for not have “tokenizer_config.json” in local dir\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 717,
    "state": "closed",
    "created_by": "chagmgang",
    "created_at": "2023-10-31T07:10:57Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/717</URL>\n\n<TITLE>[Question] How to convert finetuned qlora model to llava/server/cli.py available format?</TITLE>\n\n<BODY>### Question\n\nI finetune the llava model by script below\r\n```\r\n################## VICUNA ##################\r\n# PROMPT_VERSION=v1\r\n# MODEL_VERSION=\"vicuna-v1-3-7b\"\r\n################## VICUNA ##################\r\n\r\n################## LLaMA-2 ##################\r\nexport PROMPT_VERSION=\"llava_llama_2\"\r\nexport MODEL_VERSION=\"llama-2-7b-chat\"\r\n################## LLaMA-2 ##################\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --lora_enable True \\\r\n    --bits 4 \\\r\n    --model_name_or_path ./checkpoints/$MODEL_VERSION \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path ./playground/data/llava_instruct_80k.json \\\r\n    --image_folder /nas/Dataset/COCO2017Det/train2017 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava-$MODEL_VERSION-pretrain/mm_projector.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-$MODEL_VERSION-finetune_lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 16 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 500 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --dataloader_num_workers 4 \\\r\n    --report_to none\r\n```\r\n\r\nIn checkpoint directory, below files are generated.\r\n```\r\n-rw-r--r-- 1 nobody 4294967294       435 Oct 31 06:29 README.md\r\n-rw-r--r-- 1 nobody 4294967294       529 Oct 31 06:29 adapter_config.json\r\n-rw-r--r-- 1 nobody 4294967294 319970957 Oct 31 06:29 adapter_model.bin\r\ndrwxr-xr-x 2 nobody 4294967294       172 Oct 31 06:29 global_step500\r\n-rw-r--r-- 1 nobody 4294967294        14 Oct 31 06:30 latest\r\n-rw-r--r-- 1 nobody 4294967294     15607 Oct 31 06:30 rng_state_0.pth\r\n-rw-r--r-- 1 nobody 4294967294     15607 Oct 31 06:30 rng_state_1.pth\r\n-rw-r--r-- 1 nobody 4294967294       435 Oct 31 06:29 special_tokens_map.json\r\n-rw-r--r-- 1 nobody 4294967294    499723 Oct 31 06:29 tokenizer.model\r\n-rw-r--r-- 1 nobody 4294967294       745 Oct 31 06:29 tokenizer_config.json\r\n-rw-r--r-- 1 nobody 4294967294     60340 Oct 31 06:30 trainer_state.json\r\n-rw-r--r-- 1 nobody 4294967294      5499 Oct 31 06:29 training_args.bin\r\n-rwxr--r-- 1 nobody 4294967294     23607 Oct 31 06:30 zero_to_fp32.py\r\n```\r\n\r\nAfter making checkpoint, I want to test my model with llava/server.cli.py.\r\nHowever, there are not manual to inference with my finetuned model.\r\n\r\nThank you.</BODY>\n\n<COMMENTS>\n<Comment by CrazyBrick at 2023-11-27T14:46:53Z>\nI met the same problem, have you solved? @chagmgang\n</Comment>\n<Comment by haotian-liu at 2023-11-27T15:55:18Z>\nYou can either [serve it directly](https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md#launch-a-model-worker), or [merge them](https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md#create-merged-checkpoints).\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 716,
    "state": "closed",
    "created_by": "Ryan-ZL-Lin",
    "created_at": "2023-10-31T04:04:39Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/716</URL>\n\n<TITLE>[Usage] Error while pip install -e \".[train]\"</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nError while pip install -e \".[train]\" even though I setup CUDA_HOME in system environment and reboot, has anybody encountered the same error? \r\n\r\nBTW, I'm using WSL2 to setup the conda environment.\r\n\r\nCommand:\r\n```\r\npip install -e \".[train]\"\r\n```\r\n\r\nLog: \r\n```\r\nCollecting deepspeed==0.9.5\r\n  Using cached deepspeed-0.9.5.tar.gz (809 kB)\r\n  Preparing metadata (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n\r\n  × python setup.py egg_info did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> [8 lines of output]\r\n      Traceback (most recent call last):\r\n        File \"<string>\", line 2, in <module>\r\n        File \"<pip-setuptools-caller>\", line 34, in <module>\r\n        File \"/tmp/pip-install-jig9096n/deepspeed_807b83ecd8ea4b0a9adf8e88dd84130d/setup.py\", line 82, in <module>\r\n          cuda_major_ver, cuda_minor_ver = installed_cuda_version()\r\n        File \"/tmp/pip-install-jig9096n/deepspeed_807b83ecd8ea4b0a9adf8e88dd84130d/op_builder/builder.py\", line 41, in installed_cuda_version\r\n          assert cuda_home is not None, \"CUDA_HOME does not exist, unable to compile CUDA op(s)\"\r\n      AssertionError: CUDA_HOME does not exist, unable to compile CUDA op(s)\r\n      [end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n× Encountered error while generating package metadata.\r\n╰─> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details.\r\n```\r\n\r\nScreenshots:\r\n<img width=\"467\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/33056320/a7e1ae87-6f28-4ec9-a026-4a476ba431fa\"></BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-04T23:41:59Z>\nHi, training is currently not supported on Windows. Please try with WSL2 (which is confirmed by some other users), thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 715,
    "state": "closed",
    "created_by": "mjkmain",
    "created_at": "2023-10-31T02:31:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/715</URL>\n\n<TITLE>[Question] Issue with get_modality_length_grouped_indices when using only multimodal input</TITLE>\n\n<BODY>### Question\n\nHello @haotian-liu , \r\nI've been working on visual instruction tuning using image-text pair data and came across the following error:\r\n\r\n```bash\r\n  File \"/LLaVA/llava/train/llava_trainer.py\", line 127, in __iter__\r\n    indices = get_modality_length_grouped_indices(self.lengths, self.batch_size, self.world_size, generator=self.generator)\r\n  File \"/LLaVA/llava/train/llava_trainer.py\", line 64, in get_modality_length_grouped_indices\r\n    lang_indices, lang_lengths = zip(*[(i, -l) for i, l in enumerate(lengths) if l < 0])\r\nValueError: not enough values to unpack (expected 2, got 0)\r\n```\r\n\r\nUpon inspecting the code, I believe the issue arises from the function get_modality_length_grouped_indices:\r\n\r\n```python\r\ndef get_modality_length_grouped_indices(lengths, batch_size, world_size, generator=None):\r\n    mm_indices, mm_lengths = zip(*[(i, l) for i, l in enumerate(lengths) if l > 0])\r\n    lang_indices, lang_lengths = zip(*[(i, -l) for i, l in enumerate(lengths) if l < 0])\r\n\r\n    assert len(mm_indices) > 0, \"Should have at least one multimodal sample.\"\r\n    assert len(lang_indices) > 0, \"Should have at least one language sample.\"\r\n```\r\n\r\nIt seems that the implementation requires at least one text-to-text data sample, leading to the error when only using multimodal data. Is there a particular reason for this design choice? I'm specifically interested in multimodal data (image, text) to text tasks, and I'm wondering how to proceed. One solution I considered was adding at least one text-to-text data sample. Would this be the best approach, or is there a more optimal solution?\r\n\r\nThank you for your time and guidance.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-31T02:53:26Z>\nHi, you can simply set that option to `False`. Or you can pull the latest repo, as I have just updated the repo in https://github.com/haotian-liu/LLaVA/commit/a546269905945b732289c19c764fec34f4338989 so that this is handled more gracefully. Thanks.\n</Comment>\n<Comment by mjkmain at 2023-10-31T04:41:34Z>\nThank you !\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 714,
    "state": "open",
    "created_by": "nj159",
    "created_at": "2023-10-31T02:02:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/714</URL>\n\n<TITLE>GPT-assisted Evaluation and Launch a model worker (LoRA weights, unmerged)[Question]</TITLE>\n\n<BODY>### Question\r\n\r\nexcuse me, i want to know that which 7b model corresponds to llava-v1-0719-336px-lora-merge-vicuna-13b-v1.3，and does not vicuna-7b-v1-1 correspond to liuhaotian/LLaVA-Lightning-7B-delta-v1-1？</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 709,
    "state": "open",
    "created_by": "kimihailv",
    "created_at": "2023-10-30T16:28:29Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/709</URL>\n\n<TITLE>[Usage] Batch inference with Llava 1.5</TITLE>\n\n<BODY>### Describe the issue\n\nCurrenty, only inference with batch_size=1 is possible. If I undestood correctly, these things should be changed to make batch inference:\r\n\r\n1. position_ids should be shifted, because of left padding\r\n2. Attention mask should be passed and transformed for multimodal forward\r\n\r\nMaybe someone has managed to adapt the code?</BODY>\n\n<COMMENTS>\n<Comment by rabiulcste at 2023-11-02T16:40:38Z>\nHere's a processor that I wrote to make it work.\r\n```\r\nfrom LLaVA.llava.constants import DEFAULT_IM_END_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IMAGE_TOKEN, IMAGE_TOKEN_INDEX\r\nfrom LLaVA.llava.conversation import conv_templates\r\nfrom LLaVA.llava.mm_utils import tokenizer_image_token\r\n\r\n\r\nclass LlaVaProcessor:\r\n    def __init__(self, tokenizer, image_processor, mm_use_im_start_end):\r\n        self.mm_use_im_start_end = mm_use_im_start_end\r\n        self.tokenizer = tokenizer\r\n        self.image_processor = image_processor\r\n        self.conv_mode = \"llava_v1\"\r\n\r\n    def load_demo_images(image_files: Union[List[str], str]):\r\n        if type(image_files) is list:\r\n            out = []\r\n            for image_file in image_files:\r\n                image = Image.open(image_file).convert(\"RGB\")\r\n                out.append(image)\r\n        else:\r\n            out = Image.open(image_files).convert(\"RGB\")\r\n        return out\r\n\r\n    # TODO: refactor this, not working\r\n    def get_processed_tokens_demo(self, text: str, image_files: Union[List[str], str]):\r\n        if self.mm_use_im_start_end:\r\n            qs = (\r\n                qs\r\n                + \"\\n\"\r\n                + DEFAULT_IM_START_TOKEN\r\n                + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len\r\n                + DEFAULT_IM_END_TOKEN\r\n                + \"\\n\"\r\n                + DEFAULT_IM_START_TOKEN\r\n                + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len\r\n                + DEFAULT_IM_END_TOKEN\r\n            )\r\n        else:\r\n            qs = (\r\n                qs\r\n                + \"\\n\"\r\n                + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len\r\n                + \"\\n\"\r\n                + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len\r\n            )\r\n\r\n        conv = conv_templates[self.conv_mode].copy()\r\n        conv.append_message(conv.roles[0], text)\r\n        conv.append_message(conv.roles[1], None)\r\n        prompt = conv.get_prompt()\r\n\r\n        images = self.load_demo_images(image_files)\r\n        image_tensor = torch.stack(\r\n            [self.image_processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"][0] for image in images]\r\n        )\r\n\r\n        input_ids = (\r\n            tokenizer_image_token(text, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).cuda()\r\n        )\r\n\r\n        return image_tensor, input_ids\r\n\r\n    def format_text(self, text: str):\r\n        if self.mm_use_im_start_end:\r\n            text = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + \"\\n\" + text\r\n        else:\r\n            text = DEFAULT_IMAGE_TOKEN + \"\\n\" + text\r\n\r\n        conv = conv_templates[self.conv_mode].copy()\r\n        conv.append_message(conv.roles[0], text)\r\n        conv.append_message(conv.roles[1], None)\r\n        text = conv.get_prompt()\r\n\r\n        return text\r\n\r\n    def load_image(self, image_path: str):\r\n        return Image.open(image_path).convert(\"RGB\")\r\n\r\n    @staticmethod\r\n    def pad_sequence_to_max_length(sequence, max_length, padding_value=0):\r\n        \"\"\"Pad a sequence to the desired max length.\"\"\"\r\n        if len(sequence) >= max_length:\r\n            return sequence\r\n        return torch.cat([torch.full((max_length - len(sequence),), padding_value, dtype=sequence.dtype), sequence])\r\n\r\n    def get_processed_tokens(self, text: str, image_path: str):\r\n        prompt = self.format_text(text)\r\n        image = self.load_image(image_path)\r\n\r\n        input_ids = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0)\r\n        image_tensor = self.image_processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"][0]\r\n\r\n        return image_tensor, input_ids\r\n\r\n    def get_processed_tokens_batch(self, batch_text: List[str], image_paths: List[str]):\r\n        prompt = [self.format_text(text) for text in batch_text]\r\n        images = [self.load_image(image_path) for image_path in image_paths]\r\n\r\n        batch_input_ids = [\r\n            tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\") for prompt in prompt\r\n        ]\r\n\r\n        # Determine the maximum length of input_ids in the batch\r\n        max_len = max([len(seq) for seq in batch_input_ids])\r\n        # Pad each sequence in input_ids to the max_len\r\n        padded_input_ids = [self.pad_sequence_to_max_length(seq.squeeze(), max_len) for seq in batch_input_ids]\r\n        batch_input_ids = torch.stack(padded_input_ids)\r\n\r\n        batch_image_tensor = self.image_processor(images, return_tensors=\"pt\")[\"pixel_values\"]\r\n\r\n        return batch_image_tensor, batch_input_ids\r\n```\r\n\r\nYou can now do inference\r\n```\r\n                from LLaVA.llava.conversation import (SeparatorStyle,\r\n                                                      conv_templates)\r\n                from LLaVA.llava.mm_utils import KeywordsStoppingCriteria\r\n\r\n                conv = conv_templates[processor.conv_mode].copy()\r\n                stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\r\n                keywords = [stop_str]\r\n                stopping_criteria = (\r\n                    [KeywordsStoppingCriteria(keywords, processor.tokenizer, input_ids)]\r\n                    if conv.version == \"v0\"\r\n                    else None\r\n                )\r\n                input_ids = batch[\"input_ids\"]\r\n                image_tensor = batch[\"image_tensors\"]\r\n                input_ids = input_ids.cuda()\r\n\r\n                output_ids = model.generate(\r\n                    input_ids,\r\n                    images=image_tensor.half().cuda(),\r\n                    num_beams=self.args.num_beams,\r\n                    max_new_tokens=self.args.max_length,\r\n                    length_penalty=self.args.length_penalty,\r\n                    use_cache=True,\r\n                    stopping_criteria=stopping_criteria,\r\n                    do_sample=self.args.do_sample,\r\n                    temperature=self.args.temperature,\r\n                    num_return_sequences=self.args.num_return_sequences,\r\n                )\r\n                generated_outputs = processor.tokenizer.batch_decode(\r\n                    output_ids[:, input_ids.shape[1] :], skip_special_tokens=True\r\n                )\r\n                generated_outputs = [out.strip() for out in generated_outputs]\r\n                generated_outputs = [\r\n                    out[: -len(stop_str)] if out.endswith(stop_str) else out for out in generated_outputs\r\n                ]\r\n```\r\n\r\nYou can also check my [vqa-prompting codebase](https://github.com/rabiulcste/vqazero/tree/main/vqa_zero) for full support!\n</Comment>\n<Comment by NielsRogge at 2023-12-12T14:01:48Z>\nHi,\r\n\r\nBatched inference with LLaVa is supported in Hugging Face Transformers. See here for an example: https://github.com/huggingface/transformers/blob/a49f4acab3c1eea82907e12f82eafbd4673deb39/tests/models/llava/test_modeling_llava.py#L245.\n</Comment>\n<Comment by david-vectorflow at 2024-04-22T22:07:33Z>\nIn case anyone else finds this, here is a sample of working batch inference code based on the link above\r\n\r\n```python\r\n        prompt_temp = \"<|im_start|>system\\nAnswer the questions.<|im_end|><|im_start|>user\\n<image>\\n{}<|im_end|><|im_start|>assistant\\n\"\r\n        \r\n        prompts=[]\r\n        images = []\r\n\r\n        for user_question, base64_image in zip(user_questions, images_data):\r\n            prompt = prompt_temp.format(user_question)\r\n            prompts.append(prompt)\r\n\r\n            image_data = base64.b64decode(base64_image)\r\n            image = Image.open(BytesIO(image_data))\r\n            images.append(image)\r\n\r\n\r\n        # Perform batch inference\r\n        inputs = self.processor(prompts, images=images, return_tensors=\"pt\", padding=True).to(\"cuda:0\")\r\n        output = self.model.generate(**inputs, max_new_tokens=4000)\r\n        \r\n        answer = self.processor.batch_decode(output, skip_special_tokens=True)\r\n```\n</Comment>\n<Comment by g8a9 at 2024-04-30T09:12:11Z>\n> Hi,\r\n> \r\n> Batched inference with LLaVa is supported in Hugging Face Transformers. See here for an example: https://github.com/huggingface/transformers/blob/a49f4acab3c1eea82907e12f82eafbd4673deb39/tests/models/llava/test_modeling_llava.py#L245.\r\n\r\nHey, @NielsRogge I've stumbled upon this issue today. It seems that the same code does not work for `LlavaNextForConditionalGeneration`. Is batched inference for LlavaNext models supported in some other ways?\r\n\r\nFor reference, it crashed when trying to stacking `new_image_features`\r\n```bash\r\nFile ~/miniconda3/envs/vlm_safety_eval/lib/python3.10/site-packages/transformers/models/llava_next/modeling_llava_next.py:553, in LlavaNextForConditionalGeneration.forward(self, input_ids, pixel_values, image_sizes, attention_mask, position_ids, past_key_values, inputs_embeds, vision_feature_layer, vision_feature_select_strategy, labels, use_cache, output_attentions, output_hidden_states, return_dict)\r\n    551         image_feature = torch.cat((image_feature, self.image_newline[None]), dim=0)\r\n    552     new_image_features.append(image_feature)\r\n--> 553 image_features = torch.stack(new_image_features, dim=0)\r\n    555 inputs_embeds, attention_mask, labels, position_ids = self._merge_input_ids_with_image_features(\r\n    556     image_features, inputs_embeds, input_ids, attention_mask, labels\r\n    557 )\r\n    558 if labels is None:\r\n\r\nRuntimeError: stack expects each tensor to be equal size, but got [2144, 4096] at entry 0 and [2340, 4096] at entry 1\r\n```\n</Comment>\n<Comment by NielsRogge at 2024-04-30T09:16:01Z>\nYes I'm aware of that, this is being addressed in https://github.com/huggingface/transformers/pull/29850\r\n\r\nIt will be part of the next Transformers release!\n</Comment>\n<Comment by hxhcreate at 2024-09-09T18:37:42Z>\n```\r\n  if \"llava\" in self.model_name.lower() and 'hf' in self.model_name.lower():\r\n            questions = [question.replace(\"<image>\", \"\") for question in questions]  \r\n            images = None\r\n            if image_paths: \r\n                images = self.load_images(image_paths)\r\n                conversations = [[\r\n                    { \r\n                        \"role\": \"user\",\"content\": \r\n                        [\r\n                            {\"type\": \"image\"},\r\n                            {\"type\": \"text\", \"text\": question},\r\n                           \r\n                        ],\r\n                    },\r\n                ] for question in questions]\r\n            prompts = [self.processor.apply_chat_template(conv, add_generation_prompt=True) for conv in conversations]\r\n            inputs = self.processor(text=prompts, images=images, padding=True, truncation=True, max_length=self.args.max_length, return_tensors='pt').to(self.device, torch.float16)\r\n            with torch.no_grad():\r\n                generated_ids = self.model.generate(**inputs, **self.generation_config)\r\n                generated_ids_trimmed = [\r\n                    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\r\n                ]\r\n                outputs = self.processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\r\n            outputs = [output.strip() for output in outputs]\r\n```\r\n\r\nWhen I run the above batch_inference_code, I got the following error and I cannot understand\r\n\r\n> RuntimeError:The size of tensor a (2955)must match the size of tensor b (28)at non-singleton dimension 0\n</Comment>\n<Comment by NielsRogge at 2024-09-10T07:15:50Z>\nUpdate here, we now added batch inference in the docs:\r\n\r\n* llava: will add docs for it in https://github.com/huggingface/transformers/pull/33393\r\n* llava-next: [docs](https://huggingface.co/docs/transformers/main/en/model_doc/llava_next#multi-image-inference)\r\n* llava-onevision: [docs](https://huggingface.co/docs/transformers/main/en/model_doc/llava_onevision#multi-image-inference)\r\n* qwen2-vl: [docs](https://huggingface.co/docs/transformers/main/en/model_doc/qwen2_vl#batch-mixed-media-inference)\n</Comment>\n<Comment by hxhcreate at 2024-09-11T13:21:37Z>\nThanks for your reply. But when I running the  sample code here: https://huggingface.co/docs/transformers/main/en/model_doc/llava_next#multi-image-inference\r\nI still get the following error:\r\n\r\n<img width=\"758\" alt=\"image\" src=\"https://github.com/user-attachments/assets/e89bfcfe-4091-416f-b88e-0b65d29b7f22\">\r\n\r\n\r\nI'm using the latest transformer 4.45.dev\n</Comment>\n<Comment by NielsRogge at 2024-09-11T13:33:43Z>\nThanks for flagging. Will ping @zucchini-nlp here.\r\n\r\nNote: all doc code snippets of Transformers get tested, but some are excluded due to their size. LLaVa-NeXT is such an example. We could look into how we can also ensure code snippets of larger models are automatically tested\n</Comment>\n<Comment by zucchini-nlp at 2024-09-11T15:02:47Z>\n@hxhcreate @NielsRogge yes, that is a known issue and I merged a fix few days ago. Unfortunately refactoring broken some things, let me know if updating to the latest `main` solves the issue\n</Comment>\n<Comment by copperwiring at 2024-09-23T13:54:36Z>\nAnyone knowhow to do it without hugging face model. This is what I have so far\r\n\r\n```\r\n    for prompt in prompts_batch:\r\n        # Set args.query to the specific prompt in the batch\r\n        args.query = prompt\r\n\r\n        # Generate the prompt for each input in the batch, with the correct image handling\r\n        qs = get_prompt(args, model)\r\n\r\n        # Create a new conversation template for each prompt in the batch\r\n        conv = conv_templates[args.conv_mode].copy()\r\n        conv.append_message(conv.roles[0], qs)\r\n        conv.append_message(conv.roles[1], None)\r\n\r\n        # Add the complete prompt for this instance to the batch\r\n        batched_prompts.append(conv.get_prompt())\r\n\r\n\r\n    # max length for padding\r\n    max_len = max([len(tokenizer.encode(prompt)) for prompt in batched_prompts])\r\n\r\n    tokenizer.padding_side = \"left\"\r\n    tokenizer.model_max_length = max_len\r\n    \r\n    # Tokenize the batch of prompts\r\n    tokenized_prompts = [\r\n        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0)\r\n        for prompt in batched_prompts\r\n    ]\r\n\r\n    input_ids = torch.cat(tokenized_prompts, dim=0).cuda()\r\n\r\n    # Process images if provided (batch image loading and processing)\r\n    if img_files_batch:\r\n        # For each batch, parse image files, load them, and process\r\n        image_files_batch = [image_parser(img_files, args.sep) for img_files in img_files_batch]\r\n        images = [load_images(image_files) for image_files in image_files_batch]\r\n        flat_images = [item for sublist in images for item in sublist]\r\n        images_tensor = process_images(flat_images, image_processor, model.config).to(model.device, dtype=torch.float16)\r\n        image_sizes = [img.size for img in flat_images]\r\n    else:\r\n        images_tensor = None\r\n        image_sizes = None\r\n\r\n    attention_mask = torch.ones_like(input_ids)\r\n\r\n    with torch.inference_mode(), torch.cuda.amp.autocast():\r\n        outputs = model.forward(\r\n            input_ids=input_ids, \r\n            images=None if images_tensor is None else images_tensor,\r\n            image_sizes=image_sizes,\r\n            attention_mask=attention_mask\r\n            )\r\n    \r\n    logits = outputs.logits[:, -1, :]  # Get the logits for the last token position\r\n    probabilities = F.softmax(logits, dim=-1).squeeze()\r\n\r\n```\r\n\r\nThis is what I have so far but it doesn't add any padding tokens when I print tokenized_prompts\n</Comment>\n<Comment by KevinXu-01 at 2024-09-26T17:20:03Z>\n> Here's a processor that I wrote to make it work.\r\n> \r\n> ```\r\n> from LLaVA.llava.constants import DEFAULT_IM_END_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IMAGE_TOKEN, IMAGE_TOKEN_INDEX\r\n> from LLaVA.llava.conversation import conv_templates\r\n> from LLaVA.llava.mm_utils import tokenizer_image_token\r\n> \r\n> \r\n> class LlaVaProcessor:\r\n>     def __init__(self, tokenizer, image_processor, mm_use_im_start_end):\r\n>         self.mm_use_im_start_end = mm_use_im_start_end\r\n>         self.tokenizer = tokenizer\r\n>         self.image_processor = image_processor\r\n>         self.conv_mode = \"llava_v1\"\r\n> \r\n>     def load_demo_images(image_files: Union[List[str], str]):\r\n>         if type(image_files) is list:\r\n>             out = []\r\n>             for image_file in image_files:\r\n>                 image = Image.open(image_file).convert(\"RGB\")\r\n>                 out.append(image)\r\n>         else:\r\n>             out = Image.open(image_files).convert(\"RGB\")\r\n>         return out\r\n> \r\n>     # TODO: refactor this, not working\r\n>     def get_processed_tokens_demo(self, text: str, image_files: Union[List[str], str]):\r\n>         if self.mm_use_im_start_end:\r\n>             qs = (\r\n>                 qs\r\n>                 + \"\\n\"\r\n>                 + DEFAULT_IM_START_TOKEN\r\n>                 + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len\r\n>                 + DEFAULT_IM_END_TOKEN\r\n>                 + \"\\n\"\r\n>                 + DEFAULT_IM_START_TOKEN\r\n>                 + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len\r\n>                 + DEFAULT_IM_END_TOKEN\r\n>             )\r\n>         else:\r\n>             qs = (\r\n>                 qs\r\n>                 + \"\\n\"\r\n>                 + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len\r\n>                 + \"\\n\"\r\n>                 + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len\r\n>             )\r\n> \r\n>         conv = conv_templates[self.conv_mode].copy()\r\n>         conv.append_message(conv.roles[0], text)\r\n>         conv.append_message(conv.roles[1], None)\r\n>         prompt = conv.get_prompt()\r\n> \r\n>         images = self.load_demo_images(image_files)\r\n>         image_tensor = torch.stack(\r\n>             [self.image_processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"][0] for image in images]\r\n>         )\r\n> \r\n>         input_ids = (\r\n>             tokenizer_image_token(text, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).cuda()\r\n>         )\r\n> \r\n>         return image_tensor, input_ids\r\n> \r\n>     def format_text(self, text: str):\r\n>         if self.mm_use_im_start_end:\r\n>             text = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + \"\\n\" + text\r\n>         else:\r\n>             text = DEFAULT_IMAGE_TOKEN + \"\\n\" + text\r\n> \r\n>         conv = conv_templates[self.conv_mode].copy()\r\n>         conv.append_message(conv.roles[0], text)\r\n>         conv.append_message(conv.roles[1], None)\r\n>         text = conv.get_prompt()\r\n> \r\n>         return text\r\n> \r\n>     def load_image(self, image_path: str):\r\n>         return Image.open(image_path).convert(\"RGB\")\r\n> \r\n>     @staticmethod\r\n>     def pad_sequence_to_max_length(sequence, max_length, padding_value=0):\r\n>         \"\"\"Pad a sequence to the desired max length.\"\"\"\r\n>         if len(sequence) >= max_length:\r\n>             return sequence\r\n>         return torch.cat([torch.full((max_length - len(sequence),), padding_value, dtype=sequence.dtype), sequence])\r\n> \r\n>     def get_processed_tokens(self, text: str, image_path: str):\r\n>         prompt = self.format_text(text)\r\n>         image = self.load_image(image_path)\r\n> \r\n>         input_ids = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0)\r\n>         image_tensor = self.image_processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"][0]\r\n> \r\n>         return image_tensor, input_ids\r\n> \r\n>     def get_processed_tokens_batch(self, batch_text: List[str], image_paths: List[str]):\r\n>         prompt = [self.format_text(text) for text in batch_text]\r\n>         images = [self.load_image(image_path) for image_path in image_paths]\r\n> \r\n>         batch_input_ids = [\r\n>             tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\") for prompt in prompt\r\n>         ]\r\n> \r\n>         # Determine the maximum length of input_ids in the batch\r\n>         max_len = max([len(seq) for seq in batch_input_ids])\r\n>         # Pad each sequence in input_ids to the max_len\r\n>         padded_input_ids = [self.pad_sequence_to_max_length(seq.squeeze(), max_len) for seq in batch_input_ids]\r\n>         batch_input_ids = torch.stack(padded_input_ids)\r\n> \r\n>         batch_image_tensor = self.image_processor(images, return_tensors=\"pt\")[\"pixel_values\"]\r\n> \r\n>         return batch_image_tensor, batch_input_ids\r\n> ```\r\n> \r\n> You can now do inference\r\n> \r\n> ```\r\n>                 from LLaVA.llava.conversation import (SeparatorStyle,\r\n>                                                       conv_templates)\r\n>                 from LLaVA.llava.mm_utils import KeywordsStoppingCriteria\r\n> \r\n>                 conv = conv_templates[processor.conv_mode].copy()\r\n>                 stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\r\n>                 keywords = [stop_str]\r\n>                 stopping_criteria = (\r\n>                     [KeywordsStoppingCriteria(keywords, processor.tokenizer, input_ids)]\r\n>                     if conv.version == \"v0\"\r\n>                     else None\r\n>                 )\r\n>                 input_ids = batch[\"input_ids\"]\r\n>                 image_tensor = batch[\"image_tensors\"]\r\n>                 input_ids = input_ids.cuda()\r\n> \r\n>                 output_ids = model.generate(\r\n>                     input_ids,\r\n>                     images=image_tensor.half().cuda(),\r\n>                     num_beams=self.args.num_beams,\r\n>                     max_new_tokens=self.args.max_length,\r\n>                     length_penalty=self.args.length_penalty,\r\n>                     use_cache=True,\r\n>                     stopping_criteria=stopping_criteria,\r\n>                     do_sample=self.args.do_sample,\r\n>                     temperature=self.args.temperature,\r\n>                     num_return_sequences=self.args.num_return_sequences,\r\n>                 )\r\n>                 generated_outputs = processor.tokenizer.batch_decode(\r\n>                     output_ids[:, input_ids.shape[1] :], skip_special_tokens=True\r\n>                 )\r\n>                 generated_outputs = [out.strip() for out in generated_outputs]\r\n>                 generated_outputs = [\r\n>                     out[: -len(stop_str)] if out.endswith(stop_str) else out for out in generated_outputs\r\n>                 ]\r\n> ```\r\n> \r\n> You can also check my [vqa-prompting codebase](https://github.com/rabiulcste/vqazero/tree/main/vqa_zero) for full support!\r\n\r\nThank you for your sharing. It solved my problem!!!\n</Comment>\n<Comment by Jason-Chi-xx at 2024-11-22T10:24:19Z>\nHi,  I have encountered a problem when running the inference code of llava-next. That is the length of input_ids and images are 9, but the model.generate function would only return the  output_ids with length 1:\r\noutput_ids = self.model.generate(\r\n            input_ids,\r\n            images=image_tensor,\r\n            image_sizes=self.image_sizes,\r\n            use_cache=True,\r\n            stopping_criteria=stopping_criteria,\r\n            do_sample=False,\r\n            temperature=0,\r\n            max_new_tokens=256,\r\n        )\r\n      Does someone know the solution? thanks\n</Comment>\n<Comment by zucchini-nlp at 2024-11-22T10:53:09Z>\n@Jason-Chi-xx The model will generate tokens up to `max_new_tokens` and `1` is a feasible length in case the model just generates \"eos\" as the very first token. Please make sure your prompt is formatted correctly to get a quality generation. I recommend to use `apply_chat_template` for formatting your prompt\n</Comment>\n<Comment by KevinXu-01 at 2024-11-22T11:04:52Z>\n> Hi, I have encountered a problem when running the inference code of llava-next. That is the length of input_ids and images are 9, but the model.generate function would only return the output_ids with length 1: output_ids = self.model.generate( input_ids, images=image_tensor, image_sizes=self.image_sizes, use_cache=True, stopping_criteria=stopping_criteria, do_sample=False, temperature=0, max_new_tokens=256, ) Does someone know the solution? thanks\r\n\r\n@Jason-Chi-xx Please make sure the input_ids are concatenated. Here are the codes for batched inference:\r\n```\r\ndef load_model(rank):\r\n    device = \"cuda:\" + str(rank)\r\n    model_name = get_model_name_from_path(model_path)\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    model_path, None, model_name, False, False, device\r\n    )\r\n    return tokenizer, model, image_processor, context_len\r\n\r\ndef pad_sequence_to_max_length(sequence, max_length, padding_value=0):\r\n    \"\"\"Pad a sequence to the desired max length.\"\"\"\r\n    if len(sequence) >= max_length:\r\n        return sequence\r\n    return torch.cat([torch.full((max_length - len(sequence),), padding_value, dtype=sequence.dtype), sequence])\r\n\r\n\"\"\"rank is your device id, qs is the textual prompt, frames are batched images. Tokenizer, model, and image_processor can be obtained when you run load_model.\"\"\"\r\ndef batched_inference(rank, qs, frames, tokenizer, model, image_processor):\r\n    disable_torch_init()\r\n    model_name = get_model_name_from_path(model_path)\r\n    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\r\n    if IMAGE_PLACEHOLDER in qs:\r\n        if model.config.mm_use_im_start_end:\r\n            qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\r\n        else:\r\n            qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\r\n    else:\r\n        if model.config.mm_use_im_start_end:\r\n            qs = image_token_se + \"\\n\" + qs\r\n        else:\r\n            qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\r\n    # print(qs)\r\n    if \"llama-2\" in model_name.lower():\r\n        conv_mode = \"llava_llama_2\"\r\n    elif \"mistral\" in model_name.lower():\r\n        conv_mode = \"mistral_instruct\"\r\n    elif \"v1.6-34b\" in model_name.lower():\r\n        conv_mode = \"chatml_direct\"\r\n    elif \"v1\" in model_name.lower():\r\n        conv_mode = \"llava_v1\"\r\n    elif \"mpt\" in model_name.lower():\r\n        conv_mode = \"mpt\"\r\n    else:\r\n        conv_mode = \"llava_v0\"\r\n\r\n    conv = conv_templates[conv_mode].copy()\r\n    conv.append_message(conv.roles[0], qs)\r\n    conv.append_message(conv.roles[1], None)\r\n    prompts = []\r\n    for i in range(len(frames)):\r\n        prompt = conv.get_prompt()\r\n        prompts.append(prompt)\r\n    pil_images = []\r\n    for frame in frames:\r\n        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n        pil_image = Image.fromarray(rgb_frame)\r\n        pil_images.append(pil_image)\r\n    image_sizes = [x.size for x in pil_images]\r\n    \r\n    images_tensor = process_images(\r\n        pil_images,\r\n        image_processor,\r\n        model.config\r\n    ).to(\"cuda:\"+str(rank), dtype=torch.float16)\r\n\r\n    batch_input_ids = [tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(\"cuda:\"+str(rank)) for prompt in prompts]\r\n    max_len = max([len(seq) for seq in batch_input_ids])\r\n    padded_input_ids = [pad_sequence_to_max_length(seq.squeeze(), max_len) for seq in batch_input_ids]\r\n    batch_input_ids = torch.stack(padded_input_ids)\r\n\r\n    with torch.inference_mode():\r\n        output_ids = model.generate(\r\n            batch_input_ids,\r\n            images=images_tensor,\r\n            image_sizes=image_sizes,\r\n            do_sample=True,\r\n            temperature=0.2,\r\n            top_p=None,\r\n            num_beams=1,\r\n            max_new_tokens=5,\r\n            use_cache=True,\r\n        ).to(\"cuda:\"+str(rank))\r\n\r\n    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\r\n    return outputs\r\n```\n</Comment>\n<Comment by Jason-Chi-xx at 2024-11-23T16:34:33Z>\n> > Hi, I have encountered a problem when running the inference code of llava-next. That is the length of input_ids and images are 9, but the model.generate function would only return the output_ids with length 1: output_ids = self.model.generate( input_ids, images=image_tensor, image_sizes=self.image_sizes, use_cache=True, stopping_criteria=stopping_criteria, do_sample=False, temperature=0, max_new_tokens=256, ) Does someone know the solution? thanks\r\n> \r\n> @Jason-Chi-xx Please make sure the input_ids are concatenated. Here are the codes for batched inference:\r\n> \r\n> ```\r\n> def load_model(rank):\r\n>     device = \"cuda:\" + str(rank)\r\n>     model_name = get_model_name_from_path(model_path)\r\n>     tokenizer, model, image_processor, context_len = load_pretrained_model(\r\n>     model_path, None, model_name, False, False, device\r\n>     )\r\n>     return tokenizer, model, image_processor, context_len\r\n> \r\n> def pad_sequence_to_max_length(sequence, max_length, padding_value=0):\r\n>     \"\"\"Pad a sequence to the desired max length.\"\"\"\r\n>     if len(sequence) >= max_length:\r\n>         return sequence\r\n>     return torch.cat([torch.full((max_length - len(sequence),), padding_value, dtype=sequence.dtype), sequence])\r\n> \r\n> \"\"\"rank is your device id, qs is the textual prompt, frames are batched images. Tokenizer, model, and image_processor can be obtained when you run load_model.\"\"\"\r\n> def batched_inference(rank, qs, frames, tokenizer, model, image_processor):\r\n>     disable_torch_init()\r\n>     model_name = get_model_name_from_path(model_path)\r\n>     image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\r\n>     if IMAGE_PLACEHOLDER in qs:\r\n>         if model.config.mm_use_im_start_end:\r\n>             qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\r\n>         else:\r\n>             qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\r\n>     else:\r\n>         if model.config.mm_use_im_start_end:\r\n>             qs = image_token_se + \"\\n\" + qs\r\n>         else:\r\n>             qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\r\n>     # print(qs)\r\n>     if \"llama-2\" in model_name.lower():\r\n>         conv_mode = \"llava_llama_2\"\r\n>     elif \"mistral\" in model_name.lower():\r\n>         conv_mode = \"mistral_instruct\"\r\n>     elif \"v1.6-34b\" in model_name.lower():\r\n>         conv_mode = \"chatml_direct\"\r\n>     elif \"v1\" in model_name.lower():\r\n>         conv_mode = \"llava_v1\"\r\n>     elif \"mpt\" in model_name.lower():\r\n>         conv_mode = \"mpt\"\r\n>     else:\r\n>         conv_mode = \"llava_v0\"\r\n> \r\n>     conv = conv_templates[conv_mode].copy()\r\n>     conv.append_message(conv.roles[0], qs)\r\n>     conv.append_message(conv.roles[1], None)\r\n>     prompts = []\r\n>     for i in range(len(frames)):\r\n>         prompt = conv.get_prompt()\r\n>         prompts.append(prompt)\r\n>     pil_images = []\r\n>     for frame in frames:\r\n>         rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n>         pil_image = Image.fromarray(rgb_frame)\r\n>         pil_images.append(pil_image)\r\n>     image_sizes = [x.size for x in pil_images]\r\n>     \r\n>     images_tensor = process_images(\r\n>         pil_images,\r\n>         image_processor,\r\n>         model.config\r\n>     ).to(\"cuda:\"+str(rank), dtype=torch.float16)\r\n> \r\n>     batch_input_ids = [tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(\"cuda:\"+str(rank)) for prompt in prompts]\r\n>     max_len = max([len(seq) for seq in batch_input_ids])\r\n>     padded_input_ids = [pad_sequence_to_max_length(seq.squeeze(), max_len) for seq in batch_input_ids]\r\n>     batch_input_ids = torch.stack(padded_input_ids)\r\n> \r\n>     with torch.inference_mode():\r\n>         output_ids = model.generate(\r\n>             batch_input_ids,\r\n>             images=images_tensor,\r\n>             image_sizes=image_sizes,\r\n>             do_sample=True,\r\n>             temperature=0.2,\r\n>             top_p=None,\r\n>             num_beams=1,\r\n>             max_new_tokens=5,\r\n>             use_cache=True,\r\n>         ).to(\"cuda:\"+str(rank))\r\n> \r\n>     outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\r\n>     return outputs\r\n> ```\r\n\r\nThank you very much!\n</Comment>\n<Comment by Jason-Chi-xx at 2024-11-23T16:34:52Z>\n> @Jason-Chi-xx The model will generate tokens up to `max_new_tokens` and `1` is a feasible length in case the model just generates \"eos\" as the very first token. Please make sure your prompt is formatted correctly to get a quality generation. I recommend to use `apply_chat_template` for formatting your prompt\r\n\r\nThanks for your help!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 708,
    "state": "closed",
    "created_by": "crazycth",
    "created_at": "2023-10-30T13:54:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/708</URL>\n\n<TITLE>[Question] WARNING: tokenization mismatch when changing llama2 to llama2_chinese(Atom_7B_chat)</TITLE>\n\n<BODY>### Question\n\nI tried to finetune llava1.5 with llama2_chinese as llm\r\n\r\nhere is my config:\r\n```bash\r\n#!/bin/bash\r\n\r\ndeepspeed llava/train/train.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path /remote-home/ThCheng/weights/Atom_7B_chat \\\r\n    --version  llava_llama_2 \\\r\n    --data_path new_file_100.json \\\r\n    --image_folder /remote-home/ThCheng/dataset \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type linear \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length False \\\r\n    --bf16 False \\\r\n    --fp16 True \\\r\n    --bits 16 \\\r\n    --output_dir ./checkpoints/llava-v1.5-13b-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 False \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n\r\n```\r\n\r\nBut I encounter the tokenization mismatch problem:\r\n\r\n```bash\r\nWARNING: tokenization mismatch: 381 vs. 385. (ignored)\r\nWARNING: tokenization mismatch: 462 vs. 466. (ignored)\r\nWARNING: tokenization mismatch: 699 vs. 702. (ignored)\r\n```\r\n\r\nI think this is  the problem related to chinese tokenizer , but How should I solve this problem 🤔</BODY>\n\n<COMMENTS>\n<Comment by crazycth at 2023-10-31T03:21:56Z>\n@haotian-liu Hi , I'm confused here and I want to use Atom_7B_chat instead of raw llama2\r\n\r\nIs this the problem related to tokenizer ? Could you give me some advises on how to solve this ?\r\n\r\nReally Thank you for your time and guidance ~\n</Comment>\n<Comment by crazycth at 2023-10-31T10:29:06Z>\nI solve this problem by set eos token in tokenizer, solve\n</Comment>\n<Comment by a2382625920 at 2024-01-26T03:07:33Z>\nHi, can you elaborate on your solution, I also used the new model for SFT on LLAVA, and the eos token I set to True was automatically set back to default during fine tuning, and I got this error:\r\n**WARNING: tokenization mismatch: 198 vs. 200. (ignored)**\r\nBut it's still fine-tuning, it doesn't stop, but the loss display is always 0.0\n</Comment>\n<Comment by ChengpengLi1003 at 2024-02-08T08:38:45Z>\n@a2382625920 Hello, I had the same error: WARNING: tokenization mismatch: 198 vs. 200. (ignored) and loss is 0, have you solve this problem?\n</Comment>\n<Comment by zhipeixu at 2024-04-26T03:20:19Z>\n> I solve this problem by set eos token in tokenizer, solve\r\n\r\nI am a newbie. Can you tell me where in which file to add eos? Thank you very much!\n</Comment>\n<Comment by mylesgoose at 2024-08-11T06:28:13Z>\n> > I solve this problem by set eos token in tokenizer, solve\r\n> \r\n> I am a newbie. Can you tell me where in which file to add eos? Thank you very much!\r\n\r\nhttps://huggingface.co/meta-llama/Meta-Llama-3-70B/discussions/6/files#d2h-846292     }\r\n  },\r\n  \"bos_token\": \"<|begin_of_text|>\",\r\n  \"add_bos_token\": true,\r\n  \"add_eos_token\": false,\r\n  \"chat_template\": \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\",\r\n  \"clean_up_tokenization_spaces\": true,\r\n  \"eos_token\": \"<|end_of_text|>\",\r\n    \"attention_mask\"\r\n  ],\r\n  \"model_max_length\": 1000000000000000019884624838656,\r\n  \"tokenizer_class\": \"LlamaTokenizer\" file is called [tokenizer_config.json]\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 707,
    "state": "open",
    "created_by": "478786359",
    "created_at": "2023-10-30T09:07:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/707</URL>\n\n<TITLE>[Question] The maximum number of bytes that can be input during training</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 706,
    "state": "open",
    "created_by": "Strand2013",
    "created_at": "2023-10-30T09:00:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/706</URL>\n\n<TITLE>[Discussion] About chinese OCR ability</TITLE>\n\n<BODY>### Discussion\r\n\r\nGood project indeed！\r\n\r\nI try to use chinese handwriting OCR data train LLaVA-1.5. data like follow:\r\n\r\n![8996](https://github.com/haotian-liu/LLaVA/assets/15165983/e22278e2-9171-4039-8549-fe83b56a3e0f)\r\n\r\nTrain process the loss from 3.x to 0.2x is normal，but when i inference the data from test set, even train set, it response the very irrelevant results.\r\n\r\nthis is my infer command\r\n\r\n```\r\npython -m llava.serve.cli  --model-path checkpoints/llava-v1.5-13b-task-lora2  --model-base LLaVA/ckpt/llava-v1.5-13b     --image-file \"/train_data/OCR/4039.jpg\"\r\n```\r\n\r\nthis is my train script\r\n\r\n```\r\n#!/bin/bash\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 256 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path /ProjectRoot/LVLM/1024/LLaVA/ckpt/llava-v1.5-13b \\\r\n    --version v1 \\\r\n    --data_path /ProjectRoot/LVLM/hwt_text_train_llava.json \\\r\n    --image_folder / \\\r\n    --vision_tower /ProjectRoot/LVLM/1024/LLaVA/ckpt/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-13b-task-lora2 \\\r\n    --num_train_epochs 2 \\\r\n    --per_device_train_batch_size 32 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 3000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb \\\r\n    --group_by_modality_length False\r\n```</BODY>\n\n<COMMENTS>\n<Comment by XaviLv at 2023-11-21T07:41:39Z>\nI don't find out the Chinese OCR ablity either after a few testing. However nice for english.\n</Comment>\n<Comment by thiner at 2024-01-31T08:16:57Z>\nYes, just tried with latest v1.6 version, it doesn't recognize Chinese characters either. As per the author's previous comment https://github.com/haotian-liu/LLaVA/issues/238#issuecomment-1760788555, I think the model is trained with English dataset only.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 705,
    "state": "closed",
    "created_by": "bioinfomagic",
    "created_at": "2023-10-30T05:41:10Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/705</URL>\n\n<TITLE>WebUI works only for text interactions failed when load pics[Usage]</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nWebUI works only for text interactions failed when load pics\r\n\r\nCommand:\r\n```\r\npython3 -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b --device mps\r\n```\r\n\r\nLog: \r\n```\r\n2023-10-30 01:32:48 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n2023-10-30 01:33:03 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n2023-10-30 01:33:13 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: Semaphore(value=4, locked=False). global_counter: 2\r\n2023-10-30 01:33:13 | INFO | stdout | INFO:     127.0.0.1:50119 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\nloc(\"varianceEps\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/75428952-3aa4-11ee-8b65-46d450270006/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":233:0)): error: input types 'tensor<1x577x1xf16>' and 'tensor<1xf32>' are not broadcast compatible\r\nLLVM ERROR: Failed to infer result type(s).\r\nzsh: abort      python3 -m llava.serve.model_worker --host 0.0.0.0 --controller  --port 40000\r\n(base) bidetime@hpc LLaVA % /opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n\r\n\r\nOn the webui, the error message is,\r\nNETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.\r\n```\r\n\r\nScreenshots:</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-31T20:14:05Z>\nSeems that it is on macOS?\r\n\r\nmacOS support is updated just now, with quantization coming later. Please pull the latest code base and install/run following the instructions [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/macOS.md). You may also try https://github.com/ggerganov/llama.cpp/pull/3436.\n</Comment>\n<Comment by bioinfomagic at 2023-11-01T00:22:44Z>\n> Seems that it is on macOS?\r\n> \r\n> macOS support is updated just now, with quantization coming later. Please pull the latest code base and install/run following the instructions [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/macOS.md). You may also try [ggerganov/llama.cpp#3436](https://github.com/ggerganov/llama.cpp/pull/3436).\r\n\r\nThanks very much indeed, it worked now.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 704,
    "state": "open",
    "created_by": "bioinfomagic",
    "created_at": "2023-10-30T05:30:09Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/704</URL>\n\n<TITLE>m1 max still have issues in the final step[Usage]</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nI have enable the m1 chip using the --device mps but still have the errors. \r\n\r\nCommand:\r\n```\r\npython3 -m llava.serve.cli \\\r\n    --model-path liuhaotian/llava-v1.5-7b \\\r\n    --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n    --load-4bit --device mps\r\n                          \r\n```\r\n\r\nLog: \r\n```\r\n/opt/homebrew/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\r\n  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\r\n'NoneType' object has no attribute 'cadam32bit_grad_fp32'\r\n[2023-10-30 01:27:00,711] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nLoading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.28s/it]\r\nUSER: hello\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/Users/bidetime/Research/Projects/LLaVA/llava/serve/cli.py\", line 125, in <module>\r\n    main(args)\r\n  File \"/Users/bidetime/Research/Projects/LLaVA/llava/serve/cli.py\", line 87, in main\r\n    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 239, in _lazy_init\r\n    raise AssertionError(\"Torch not compiled with CUDA enabled\")\r\nAssertionError: Torch not compiled with CUDA enabled\r\nASSISTANT: %                 \r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-31T20:12:47Z>\nmacOS support is updated just now, with quantization coming later. Please pull the latest code base and install/run following the instructions [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/macOS.md). You may also try https://github.com/ggerganov/llama.cpp/pull/3436.\n</Comment>\n<Comment by bioinfomagic at 2023-10-31T23:45:48Z>\n> macOS support is updated just now, with quantization coming later. Please pull the latest code base and install/run following the instructions [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/macOS.md). You may also try [ggerganov/llama.cpp#3436](https://github.com/ggerganov/llama.cpp/pull/3436).\r\n\r\nThnanks very much, I have run the following, to update to the lastest git repo,\r\ngit pull\r\npip install -e .\r\n\r\nAnd have the output, Successfully installed llava-1.1.3\r\n\r\nand start the process again, but the same issue persist, text interaction works, once load the pics, the error pops out. \r\nNETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.\n</Comment>\n<Comment by haotian-liu at 2023-11-01T00:02:16Z>\n@bioinfomagic what about try with the cli?\r\n\r\n```\r\npython -m llava.serve.cli \\\r\n    --model-path liuhaotian/llava-v1.5-7b \\\r\n    --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n    --device mps\r\n```\n</Comment>\n<Comment by bioinfomagic at 2023-11-01T00:22:08Z>\n> @bioinfomagic what about try with the cli?\r\n> \r\n> ```\r\n> python -m llava.serve.cli \\\r\n>     --model-path liuhaotian/llava-v1.5-7b \\\r\n>     --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n>     --device mps\r\n> ```\r\n\r\nOh, thanks very much Haotian, it works on my mac now.\n</Comment>\n<Comment by haotian-liu at 2023-11-01T00:23:35Z>\n@bioinfomagic \r\n\r\nBtw, how is the speed on M1 Max and how much RAM do you have?\n</Comment>\n<Comment by bioinfomagic at 2023-11-01T00:41:53Z>\n> @bioinfomagic\r\n> \r\n> Btw, how is the speed on M1 Max and how much RAM do you have?\r\n\r\nThe webUI works perfectly, now I can run LLaVA on my mac with similar output as the LLaVA online demo. Just with a slower speed. \r\nI somehow used 85GB to 125GB of ram (depends on the run time) and the speed is around 2~3 words per second. I can see the CPU usage shows 105% GPU usage is around 35% according to the activity monitor. In regarding of speed, I can see other LLM apps, when I choose metal enabled I can see the CPU load drops to 30% and GPU loads gose up to 100% or 200% and the speed is much faster same as online chatgpt 10 words per second. \r\n\r\n\r\nFor the CLI it seems still have issues, not sure if it is my problem, but webUI works very well.\r\n\r\n-m llava.serve.cli --model-path liuhaotian/llava-v1.5-7b --image-file \"https://llava-vl.github.io/static/images/view.jpg\" --device mps\r\n[2023-10-31 20:28:04,996] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nLoading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.82s/it]\r\nUSER: hello \r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/LLaVA/llava/serve/cli.py\", line 125, in <module>\r\n    main(args)\r\n  File \"/LLaVA/llava/serve/cli.py\", line 87, in main\r\n    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 289, in _lazy_init\r\n    raise AssertionError(\"Torch not compiled with CUDA enabled\")\r\nAssertionError: Torch not compiled with CUDA enabled\r\nASSISTANT: %                                            \r\n\r\n\r\n\r\nBut If I add the --load4bit, it won't load, \r\n\r\n python3 -m llava.serve.cli --model-path liuhaotian/llava-v1.5-7b --image-file \"https://llava-vl.github.io/static/images/view.jpg\" --load-4bit --device mps\r\n[2023-10-31 20:26:52,172] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nTraceback (most recent call last):\r\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/metadata/__init__.py\", line 563, in from_name\r\n    return next(cls.discover(name=name))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nStopIteration\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"//LLaVA/llava/serve/cli.py\", line 125, in <module>\r\n    main(args)\r\n  File \"/LLaVA/llava/serve/cli.py\", line 32, in main\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/LLaVA/llava/model/builder.py\", line 33, in load_pretrained_model\r\n    kwargs['quantization_config'] = BitsAndBytesConfig(\r\n                                    ^^^^^^^^^^^^^^^^^^^\n</Comment>\n<Comment by haotian-liu at 2023-11-01T00:43:44Z>\nIt seems that you haven't pulled the latest code base:\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/llava/serve/cli.py#L87\n</Comment>\n<Comment by bioinfomagic at 2023-11-01T00:52:17Z>\n> It seems that you haven't pulled the latest code base:\r\n> \r\n> https://github.com/haotian-liu/LLaVA/blob/main/llava/serve/cli.py#L87\r\n\r\nThank you very much, you are right, I somehow didn't update the code base correctly, now I updated it works perfectly now for both CLI and WebUI. \r\n\r\npython3 -m llava.serve.cli --model-path liuhaotian/llava-v1.5-7b --image-file \"https://llava-vl.github.io/static/images/view.jpg\" --device mps\r\n[2023-10-31 20:50:05,977] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nLoading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.69s/it]\r\nUSER: hey\r\nASSISTANT: Hello! How can I help you today?\r\nUSER: summarize the pic\n</Comment>\n<Comment by haotian-liu at 2023-11-01T00:56:03Z>\nHmmm. I previously thought it was because of my poor M2 16GB, so it seems that the MPS still needs some optimization.\n</Comment>\n<Comment by bioinfomagic at 2023-11-01T02:24:35Z>\n> Hmmm. I previously thought it was because of my poor M2 16GB, so it seems that the MPS still needs some optimization.\r\n\r\nI totally agree, I can see LMstudio has a much better metal support, it runs 70B model much faster, when metal enable, clearly noticed the speed difference.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 703,
    "state": "open",
    "created_by": "bioinfomagic",
    "created_at": "2023-10-30T04:54:12Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/703</URL>\n\n<TITLE>[Usage] Always have errors when download and load the models</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n[Usage] Always have errors when download and load the models\r\nIt is a linux machine on alienware R8 with RTX 1080 this is the only error provents me from running the program \r\n\r\nCommand:\r\n```\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker \r\n```\r\n\r\nLog: \r\n```\r\nhttp://localhost:40000 --model-path liuhaotian/llava-v1.5-7b\r\n/home/protecdream/anaconda3/envs/chatpath/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\r\n  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\r\n/home/protecdream/anaconda3/envs/chatpath/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\r\n2023-10-30 00:51:15 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='liuhaotian/llava-v1.5-7b', model_base=None, model_name=None, device='cuda', multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=False)\r\n2023-10-30 00:51:15 | INFO | model_worker | Loading the model llava-v1.5-7b on worker 46f6cd ...\r\n2023-10-30 00:51:15 | ERROR | stderr | Traceback (most recent call last):\r\n2023-10-30 00:51:15 | ERROR | stderr |   File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n2023-10-30 00:51:15 | ERROR | stderr |   File \"<frozen runpy>\", line 88, in _run_code\r\n2023-10-30 00:51:15 | ERROR | stderr |   File \"/home/protecdream/Research/projects/LLaVA/llava/serve/model_worker.py\", line 275, in <module>\r\n2023-10-30 00:51:15 | ERROR | stderr |     worker = ModelWorker(args.controller_address,\r\n2023-10-30 00:51:15 | ERROR | stderr |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2023-10-30 00:51:15 | ERROR | stderr |   File \"/home/protecdream/Research/projects/LLaVA/llava/serve/model_worker.py\", line 65, in __init__\r\n2023-10-30 00:51:15 | ERROR | stderr |     self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\r\n2023-10-30 00:51:15 | ERROR | stderr |                                                                          ^^^^^^^^^^^^^^^^^^^^^^\r\n2023-10-30 00:51:15 | ERROR | stderr |   File \"/home/protecdream/Research/projects/LLaVA/llava/model/builder.py\", line 103, in load_pretrained_model\r\n2023-10-30 00:51:15 | ERROR | stderr |     model = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\n2023-10-30 00:51:15 | ERROR | stderr |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2023-10-30 00:51:15 | ERROR | stderr |   File \"/home/protecdream/anaconda3/envs/chatpath/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 2903, in from_pretrained\r\n2023-10-30 00:51:15 | ERROR | stderr |     ) = cls._load_pretrained_model(\r\n2023-10-30 00:51:15 | ERROR | stderr |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2023-10-30 00:51:15 | ERROR | stderr |   File \"/home/protecdream/anaconda3/envs/chatpath/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3002, in _load_pretrained_model\r\n2023-10-30 00:51:15 | ERROR | stderr |     raise ValueError(\r\n2023-10-30 00:51:15 | ERROR | stderr | ValueError: The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by amarflybot at 2023-11-02T15:36:51Z>\nI changed the kwargs, in builder.py:27\r\nto\r\n`kwargs = {\"device_map\": device_map, \"offload_folder\": \"offload\"}`\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 702,
    "state": "closed",
    "created_by": "Caizifen",
    "created_at": "2023-10-30T03:57:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/702</URL>\n\n<TITLE>The composition of the visual instruction tuning datasets</TITLE>\n\n<BODY>### Question\n\n```\r\n├── coco\r\n│   └── train2017\r\n├── gqa\r\n│   └── images\r\n├── ocr_vqa\r\n│   └── images\r\n├── textvqa\r\n│   └── train_images\r\n└── vg\r\n    ├── VG_100K\r\n    └── VG_100K_2\r\n```\r\nThe above is the structure of the fine-tuning dataset provided. After I downloaded the data according to the README, the total number is not 665k, only 608k. Did I miss anything?\r\n\r\n| coco | gqa | ocr_vqa | textvqa | VG_100K | VG_100K_2 | Total |\r\n| --- | ---: | ---: | ---: | ---: | ---: | ---: |\r\n| 118287 | 148854 | 207572 | 25119 | 64346 | 43903 | 608081 |</BODY>\n\n<COMMENTS>\n<Comment by jiaxiangc at 2023-11-04T17:13:26Z>\nHow to download ocr_vqa? what is pdb?\n</Comment>\n<Comment by haotian-liu at 2023-11-04T21:01:12Z>\nApologies for the confusion. I just re-calculated the exact samples in the dataset mixture, and we will update the paper to correct the sample count for RefCOCO and A-OKVQA. Note that the released dataset is correct, only the number reported in the table is off for these two datasets.\r\n\r\n| Dataset       | Actual  | Paper |\r\n|---------------|--------:|------:|\r\n| LLaVA |  157712 |  158K |\r\n| SG40k         |   40688 |   40K |\r\n| VQA-v2        |   82783 |   83K |\r\n| GQA           |   72140 |   72K |\r\n| OKVQA         |    8998 |    9K |\r\n| OCRVQA       |   80000 |   80K |\r\n| A-OKVQA       |   **66160** |   ~~50K~~ **66K** |\r\n| TextCaps      |   21953 |   22K |\r\n| RefCOCO       |   **48447** |   ~~30K~~ **48K** |\r\n| VG            |   86417 |   86K |\r\n| **Total**     | **665298** | **665K** |\n</Comment>\n<Comment by Cooperx521 at 2024-03-05T14:47:40Z>\n@Caizifen Hello, I'm also confused about the difference between README and the table mentioned by @haotian-liu. Have you clarified it? And does the data mentioned in the README have contained all the data in the table?\n</Comment>\n<Comment by 421zuoduan at 2024-04-24T03:25:07Z>\n> How to download ocr_vqa? what is pdb?\r\n\r\nHi, I want to know if you have solved this problem? i have encountered the same problem\n</Comment>\n<Comment by zxdscsfm at 2025-02-26T04:57:28Z>\n@421zuoduan Hi, I want to know if you have solved this problem? i have encountered the same problem\n</Comment>\n<Comment by 421zuoduan at 2025-02-26T05:43:53Z>\n@zxdscsfm Hi, you can find the solution [here](https://github.com/haotian-liu/LLaVA/issues/593). From my notes, I think I downloaded OCR_VQA_200K and revised `gif` file extensions to `jpg` manually, then it worked.\n</Comment>\n<Comment by zxdscsfm at 2025-02-27T11:04:18Z>\n@421zuoduan Thanks! I successfully solved this problem\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 701,
    "state": "open",
    "created_by": "optimismresilience",
    "created_at": "2023-10-30T01:31:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/701</URL>\n\n<TITLE>language interactive function works fine but each time when upload pics there is \"network issue\"[Usage]</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nlanguage interactive function works fine but each time when upload pics there is \"network issue\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 700,
    "state": "closed",
    "created_by": "vashat",
    "created_at": "2023-10-29T18:42:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/700</URL>\n\n<TITLE>Can't get Llava to work on M1 mac</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nIt seams Llava is not working on M1 with MPS backend.\r\n\r\nCommand:\r\n```\r\nenv PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 python3 -m llava.serve.cli --model-path /Volumes/M1\\ Macmini\\ backup/scripts/llava-v1.5-13b --image-file /Users/admin/Downloads/Gustav\\ Vasa-1.jpg --load-4bit --device=mps \r\n```\r\n\r\nLog: \r\n```\r\n/Users/admin/scripts/miniconda3/envs/llava/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\r\n  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\r\n'NoneType' object has no attribute 'cadam32bit_grad_fp32'\r\nLoading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [06:39<00:00, 133.12s/it]\r\nUSER: What is in the image\r\nTraceback (most recent call last):\r\n  File \"/Users/admin/scripts/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/Users/admin/scripts/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/Volumes/M1 Macmini backup/scripts/LLaVA/llava/serve/cli.py\", line 125, in <module>\r\n    main(args)\r\n  File \"/Volumes/M1 Macmini backup/scripts/LLaVA/llava/serve/cli.py\", line 87, in main\r\n    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\r\n  File \"/Users/admin/scripts/miniconda3/envs/llava/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 239, in _lazy_init\r\n    raise AssertionError(\"Torch not compiled with CUDA enabled\")\r\nAssertionError: Torch not compiled with CUDA enabled\r\nASSISTANT: %                                                                                              \r\n```\r\n\r\n\r\nAlso tried it with Gradio UI. When trying to run it with the Gradio UI, it crashes when submitting an image through the UI:\r\n\r\nCommand:\r\n```\r\nenv PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 python3 -m llava.serve.cli --model-path /Volumes/M1\\ Macmini\\ backup/scripts/llava-v1.5-13b --image-file /Users/admin/Downloads/Gustav\\ Vasa-1.jpg --load-4bit --device=mps \r\n```\r\n\r\nLog: \r\n```\r\n/Users/admin/scripts/miniconda3/envs/llava/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\r\n  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\r\n'NoneType' object has no attribute 'cadam32bit_grad_fp32'\r\n2023-10-29 19:12:54 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='/Volumes/M1 Macmini backup/scripts/llava-v1.5-13b', model_base=None, model_name=None, device='mps', multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=True)\r\n2023-10-29 19:12:54 | INFO | model_worker | Loading the model llava-v1.5-13b on worker af2c5e ...\r\nLoading checkpoint shards:   0%|                                                                                                                                                                                                                                   | 0/3 [00:00<?, ?it/s]\r\nLoading checkpoint shards:  33%|████████████████████████████████████████████████████████████████████████▋                                                                                                                                                 | 1/3 [02:40<05:20, 160.23s/it]\r\nLoading checkpoint shards:  67%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                        | 2/3 [05:20<02:40, 160.41s/it]\r\nLoading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [06:36<00:00, 121.55s/it]\r\nLoading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [06:36<00:00, 132.09s/it]\r\n2023-10-29 19:19:30 | ERROR | stderr | \r\n2023-10-29 19:19:44 | INFO | model_worker | Register to controller\r\n2023-10-29 19:19:44 | ERROR | stderr | INFO:     Started server process [82066]\r\n2023-10-29 19:19:44 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2023-10-29 19:19:44 | ERROR | stderr | INFO:     Application startup complete.\r\n2023-10-29 19:19:44 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)\r\n2023-10-29 19:19:59 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: None. global_counter: 0\r\n2023-10-29 19:20:14 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: None. global_counter: 0\r\n2023-10-29 19:20:29 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: None. global_counter: 0\r\n2023-10-29 19:20:44 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: None. global_counter: 0\r\n2023-10-29 19:20:59 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: None. global_counter: 0\r\n2023-10-29 19:21:15 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: None. global_counter: 0\r\n2023-10-29 19:21:30 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: None. global_counter: 0\r\n2023-10-29 19:21:45 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: None. global_counter: 0\r\n2023-10-29 19:22:00 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: None. global_counter: 0\r\n2023-10-29 19:22:15 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: None. global_counter: 0\r\n2023-10-29 19:22:30 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: None. global_counter: 0\r\n2023-10-29 19:22:45 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: None. global_counter: 0\r\n2023-10-29 19:22:50 | INFO | stdout | INFO:     127.0.0.1:63202 - \"POST /worker_get_status HTTP/1.1\" 200 OK\r\n2023-10-29 19:22:54 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n2023-10-29 19:22:54 | INFO | stdout | INFO:     127.0.0.1:63208 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\n2023-10-29 19:22:54 | ERROR | stderr | /Users/admin/scripts/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py:725: UserWarning: MPS: no support for int64 repeats mask, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Repeat.mm:236.)\r\n2023-10-29 19:22:54 | ERROR | stderr |   input_ids = input_ids.repeat_interleave(expand_size, dim=0)\r\nloc(\"varianceEps\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/c2cb9645-dafc-11ed-aa26-6ec1e3b3f7b3/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":228:0)): error: input types 'tensor<1x577x1xf16>' and 'tensor<1xf32>' are not broadcast compatible\r\nLLVM ERROR: Failed to infer result type(s).\r\nzsh: abort      env PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 python -m llava.serve.model_worker  \r\n(llava) admin@Minisomistrator LLaVA % /Users/admin/scripts/miniconda3/envs/llava/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n\r\n(llava) admin@Minisomistrator LLaVA %                                                                                      \r\n```</BODY>\n\n<COMMENTS>\n<Comment by alexmead at 2023-10-31T17:25:52Z>\n@vashat, I too am getting this error. Any progress? \r\nI'm diving into it now, will post any resolution.\n</Comment>\n<Comment by haotian-liu at 2023-10-31T20:11:09Z>\nmacOS support is updated just now, with quantization coming later. Please pull the latest code base and install/run following the instructions [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/macOS.md). You may also try [llama.cpp](https://github.com/ggerganov/llama.cpp/pull/3436).\n</Comment>\n<Comment by vashat at 2023-11-01T19:02:46Z>\nHi! Unfortunately still getting the same error after pulling latest code and removing quantize parameters @haotian-liu :\r\n\r\n```\r\nenv PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 python3 -m llava.serve.cli --model-path /Volumes/M1\\ Macmini\\ backup/scripts/llava-v1.5-13b --image-file /Users/admin/Downloads/Gustav\\ Vasa-1.jpg --device=mps \r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [07:19<00:00, 146.38s/it]\r\nUSER: What is in the image?\r\n/Users/admin/scripts/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py:725: UserWarning: MPS: no support for int64 repeats mask, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Repeat.mm:236.)\r\n  input_ids = input_ids.repeat_interleave(expand_size, dim=0)\r\nloc(\"varianceEps\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/c2cb9645-dafc-11ed-aa26-6ec1e3b3f7b3/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":228:0)): error: input types 'tensor<1x577x1xf16>' and 'tensor<1xf32>' are not broadcast compatible\r\nLLVM ERROR: Failed to infer result type(s).\r\nzsh: abort      env PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 python3 -m llava.serve.cli     \r\n(llava) admin@Minisomistrator LLaVA % /Users/admin/scripts/miniconda3/envs/llava/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n\r\n```\n</Comment>\n<Comment by haotian-liu at 2023-11-01T19:04:33Z>\nHave you reinstalled PyTorch pip install torch==2.1.0 torchvision==0.16.0\n</Comment>\n<Comment by vashat at 2023-11-01T22:51:52Z>\nYes it works when I reinstall to these versions. Thank you for the assistance!\n</Comment>\n<Comment by amarflybot at 2023-11-02T11:02:48Z>\nThanks a lot, Working after having `pip install torch==2.1.0 torchvision==0.16.0`\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 699,
    "state": "open",
    "created_by": "pyogher",
    "created_at": "2023-10-29T14:21:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/699</URL>\n\n<TITLE>Stuck Gradient Backpropagation with LLaVA1.5 in ZERO-3 Framework</TITLE>\n\n<BODY>### Question\n\nHello, I am currently utilizing LLaVA1.5, which comprises both text-only and image-text instructions, for instruction fine-tuning within the ZERO-3 framework. However, I've encountered an issue where my model becomes stuck during gradient backpropagation. Could anyone provide insight on how to resolve this? Thank you!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-04T23:40:00Z>\nIt should be working fine for the latest code base, as the LLaVA-1.5 is trained under such mixture. Can you confirm with your scripts and your data?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 698,
    "state": "closed",
    "created_by": "bioinfomagic",
    "created_at": "2023-10-29T13:31:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/698</URL>\n\n<TITLE>[Usage] pip install flash-attn --no-build-isolation ERRORs</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI had errors when run the command, \"pip install flash-attn --no-build-isolation\"\r\nIt seems that because I don't have cuda, I am only using the M1 max chip of mac book pro with 64GB of ram. \r\n\r\nCommand:\r\n```\r\npip install flash-attn --no-build-isolation\r\n\r\n```\r\n\r\nLog: \r\n```\r\nCollecting flash-attn\r\n  Using cached flash_attn-2.3.3.tar.gz (2.3 MB)\r\n  Preparing metadata (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  × python setup.py egg_info did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> [22 lines of output]\r\n      fatal: not a git repository (or any of the parent directories): .git\r\n      /private/var/folders/r1/_sncvgk15lb_fhsrrjjhj_wh0000gn/T/pip-install-swug64eb/flash-attn_a0697bca31734576b03cf6574720853a/setup.py:79: UserWarning: flash_attn was requested, but nvcc was not found.  Are you sure your environment has nvcc available?  If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, only images whose names contain 'devel' will provide nvcc.\r\n        warnings.warn(\r\n      Traceback (most recent call last):\r\n        File \"<string>\", line 2, in <module>\r\n        File \"<pip-setuptools-caller>\", line 34, in <module>\r\n        File \"/private/var/folders/r1/_sncvgk15lb_fhsrrjjhj_wh0000gn/T/pip-install-swug64eb/flash-attn_a0697bca31734576b03cf6574720853a/setup.py\", line 136, in <module>\r\n          CUDAExtension(\r\n        File \"/opt/homebrew/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 1048, in CUDAExtension\r\n          library_dirs += library_paths(cuda=True)\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/opt/homebrew/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 1179, in library_paths\r\n          if (not os.path.exists(_join_cuda_home(lib_dir)) and\r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/opt/homebrew/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 2223, in _join_cuda_home\r\n          raise EnvironmentError('CUDA_HOME environment variable is not set. '\r\n      OSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.\r\n      \r\n      \r\n      torch.__version__  = 2.0.1\r\n      \r\n      \r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n× Encountered error while generating package metadata.\r\n╰─> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details.\r\n\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-29T17:34:40Z>\nflash-attn is not supported on Mac. Also, it is not required for running inference on Mac.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 697,
    "state": "open",
    "created_by": "HireTheHero",
    "created_at": "2023-10-29T03:38:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/697</URL>\n\n<TITLE>[Usage] Out of memory for single-A100 LLaVA 1.5 w/ QLoRA and cpu-offload</TITLE>\n\n<BODY>### Describe the issue\n\n# Issue:\r\nAfter tens of batches, OOM shows when I try to fine-tune LLaVA 1.5 on single A100 w/ QLoRA and cpu-offloading.\r\n# Command:\r\n```\r\nMODEL_DIR=\"<path-to-model-dir>\"\r\nwget -P $MODEL_DIR \\\r\n    https://huggingface.co/liuhaotian/llava-v1.5-mlp2x-336px-pretrain-vicuna-13b-v1.5/resolve/main/mm_projector.bin\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3_offload.json \\\r\n    --lora_enable True \\\r\n    --bits 4 \\\r\n    --model_name_or_path lmsys/vicuna-13b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/llava_v1_5_mix665k.json \\\r\n    --image_folder ./playground/data \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter $MODEL_DIR/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-13b \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 8 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\n# Log: \r\n```\r\n{'loss': 0.891, 'learning_rate': 1.807714194192979e-05, 'epoch': 0.02}\r\n\r\n  2%|▏         | 96/5197 [5:05:34<276:37:06, 195.22s/it]Traceback (most recent call last):\r\n  File \"/<LLaVA-path>/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/<LLaVA-path>/llava/train/train.py\", line 1049, in train\r\n    trainer.train()\r\n  File \"/<conda-path>/lib/python3.11/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/<conda-path>/lib/python3.11/site-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/<conda-path>/lib/python3.11/site-packages/transformers/trainer.py\", line 2665, in training_step\r\n    self.accelerator.backward(loss)\r\n  File \"/<conda-path>/lib/python3.11/site-packages/accelerate/accelerator.py\", line 1847, in backward\r\n    self.deepspeed_engine_wrapped.backward(loss, **kwargs)\r\n  File \"/<conda-path>/lib/python3.11/site-packages/accelerate/utils/deepspeed.py\", line 167, in backward\r\n    self.engine.backward(loss, **kwargs)\r\n  File \"/<conda-path>/lib/python3.11/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n              ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/<conda-path>/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 1861, in backward\r\n    self.optimizer.backward(loss, retain_graph=retain_graph)\r\n  File \"/<conda-path>/lib/python3.11/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n              ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/<conda-path>/lib/python3.11/site-packages/deepspeed/runtime/zero/stage3.py\", line 1993, in backward\r\n    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\r\n  File \"/<conda-path>/lib/python3.11/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 63, in backward\r\n    scaled_loss.backward(retain_graph=retain_graph)\r\n  File \"/<conda-path>/lib/python3.11/site-packages/torch/_tensor.py\", line 487, in backward\r\n    torch.autograd.backward(\r\n  File \"/<conda-path>/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 200, in backward\r\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n  File \"/<conda-path>/lib/python3.11/site-packages/torch/autograd/function.py\", line 274, in apply\r\n    return user_fn(self, *args)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/<conda-path>/lib/python3.11/site-packages/torch/utils/checkpoint.py\", line 157, in backward\r\n    torch.autograd.backward(outputs_with_grad, args_with_grad)\r\n  File \"/<conda-path>/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 200, in backward\r\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n  File \"/<conda-path>/lib/python3.11/site-packages/torch/autograd/function.py\", line 274, in apply\r\n    return user_fn(self, *args)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/<conda-path>/lib/python3.11/site-packages/torch/cuda/amp/autocast_mode.py\", line 123, in decorate_bwd\r\n    return bwd(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/<conda-path>/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py\", line 84, in backward\r\n    grad_input = grad_output.matmul(weight)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.08 GiB (GPU 0; 44.40 GiB total capacity; 33.31 GiB already allocated; 1.00 GiB free; 42.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\nwandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.\r\nwandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\r\nwandb: \\ 0.008 MB of 0.027 MB uploaded (0.000 MB deduped)\r\nwandb: | 0.008 MB of 0.039 MB uploaded (0.000 MB deduped)\r\nwandb: / 0.039 MB of 0.039 MB uploaded (0.000 MB deduped)\r\nwandb: \r\nwandb: Run history:\r\nwandb:         train/epoch ▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅████████\r\nwandb:   train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\r\nwandb: train/learning_rate ▁▃▃▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇████████████\r\nwandb:          train/loss ▆▆█▆▇▅▆▆█▇▆▅▅▄▅▅▆▆▄▅▅▃▄▄▄▃▅▅▃▂▄▄▁▃▃▄▄▃▂▂\r\nwandb: \r\nwandb: Run summary:\r\nwandb:         train/epoch 0.02\r\nwandb:   train/global_step 96\r\nwandb: train/learning_rate 2e-05\r\nwandb:          train/loss 0.891\r\nwandb: \r\nwandb: 🚀 View run avid-terrain-7 at: https://wandb.ai/hire-the-hero/huggingface/runs/m2s53en7\r\nwandb: ️⚡ View job at https://wandb.ai/hire-the-hero/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwOTg4MTM2NA==/version_details/v1\r\nwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\r\nwandb: Find logs at: ./wandb/run-20231028_231613-m2s53en7/logs\r\n[2023-10-29 04:24:06,289] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1508190\r\n[2023-10-29 04:24:06,290] [ERROR] [launch.py:321:sigkill_handler] ['/<conda-path>/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3_offload.json', '--lora_enable', 'True', '--bits', '4', '--model_name_or_path', 'lmsys/vicuna-13b-v1.5', '--version', 'v1', '--data_path', './playground/data/llava_v1_5_mix665k.json', '--image_folder', './playground/data', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--pretrain_mm_mlp_adapter', '<path-to-model-dir>/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '8', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = 1\r\n\r\n```\r\n\r\n# Screenshots:\r\nN/A</BODY>\n\n<COMMENTS>\n<Comment by Williamsunsir at 2023-11-18T14:13:38Z>\n请问您解决了这个问题了吗\n</Comment>\n<Comment by HireTheHero at 2023-11-19T14:54:27Z>\nAsking me if I've solved this problem or not? No. Also trying 4*V100 but not working either.\n</Comment>\n<Comment by simon-lund at 2024-02-26T16:58:59Z>\nIt looks like you are training with a A100 40GB?\r\nIf that's the case, you need to reduce the `per_device_train_batch_size`:\r\n\r\n```shell\r\n--per_device_train_batch_size 8 \\\r\n--gradient_accumulation_steps 16 \\\r\n```\r\n\r\nTo keep the global batch size at 128, you will have to update the `gradient_accumulation_steps` as well.\r\n\r\nGLOBAL_BATCH_SIZE = NUM_GPUS * PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 695,
    "state": "open",
    "created_by": "barshag",
    "created_at": "2023-10-28T21:00:57Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/695</URL>\n\n<TITLE>[Question] what should be the amount of data(json instances) in order to perform finetuning/lora?</TITLE>\n\n<BODY>### Question\n\nusing the proposed method in:\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 693,
    "state": "open",
    "created_by": "jpWang",
    "created_at": "2023-10-28T09:50:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/693</URL>\n\n<TITLE>[Question] Can the trained weight after stage 1(pre-train, feature alignment) be provided?</TITLE>\n\n<BODY>### Question\n\nHi, thanks for your great work!\r\nAnd I just want to ask that can the trained weight after stage 1(pre-train, feature alignment) be provided? In that case I can only conduct the fine-tune phase.\r\n\r\nthanks !</BODY>\n\n<COMMENTS>\n<Comment by tosiyuki at 2023-11-17T11:23:20Z>\nYou can download pretrained weight(Projector weights) from here.\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#projector-weights\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 692,
    "state": "open",
    "created_by": "zmtbnv",
    "created_at": "2023-10-28T05:00:35Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/692</URL>\n\n<TITLE>[Question] Is it possible to generate embedding (feature extraction) similar to BLIP-2?</TITLE>\n\n<BODY>### Question\n\n[BLIP-2](https://github.com/salesforce/LAVIS/tree/main/projects/blip2#feature-extraction-example) allows extracting [Unimodal features](https://github.com/salesforce/LAVIS/blob/3446bac20c5646d35ae383ebe6d13cec4f8b00cb/examples/blip2_feature_extraction.ipynb) like:\r\n\r\n```python\r\nfeatures_image = model.extract_features(sample, mode=\"image\")\r\nfeatures_text = model.extract_features(sample, mode=\"text\")\r\nprint(features_image.image_embeds.shape)\r\n# torch.Size([1, 32, 768])\r\nprint(features_text.text_embeds.shape)\r\n# torch.Size([1, 12, 768])\r\n```\r\n\r\nIs it possible to do the same with LLaVa?</BODY>\n\n<COMMENTS>\n<Comment by Slinene at 2023-12-27T07:53:07Z>\nsame question\n</Comment>\n<Comment by saisurbehera at 2024-02-11T19:46:53Z>\nAny answers ?\n</Comment>\n<Comment by wenxuanmou at 2024-08-12T11:21:50Z>\nSame question. Have you got a solution? Thanks\n</Comment>\n<Comment by sreebhattacharyya at 2024-09-26T03:13:21Z>\n+1. Did anyone find a solution that is reasonably straightforward?\n</Comment>\n<Comment by surykntsingh at 2025-04-02T11:23:59Z>\nSame question. seems like there is no info on this\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 691,
    "state": "open",
    "created_by": "GaoDalie",
    "created_at": "2023-10-27T23:41:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/691</URL>\n\n<TITLE>LLava + llamaindex</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nI have used all the necessary commands in Google Collab @haotian-liu  \r\n!git clone https://github.com/ggerganov/llama.cpp.git\r\n%cd llama.cpp\r\n!make\r\n!Wget  https://huggingface.co/mys/ggml_llava-v1.5-7b/blob/main/ggml-model-f16.gguf\r\n!./llava\r\nlet me know if I was missing something</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 688,
    "state": "closed",
    "created_by": "ygfrancois",
    "created_at": "2023-10-27T12:54:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/688</URL>\n\n<TITLE>[Question] Error with the contruction of the new_label？</TITLE>\n\n<BODY>### Question\n\nhttps://github.com/haotian-liu/LLaVA/blob/main/llava/model/llava_arch.py#L151\r\n\r\n`            while image_token_indices.numel() > 0:\r\n                cur_image_features = image_features[cur_image_idx]\r\n                image_token_start = image_token_indices[0]\r\n                if getattr(self.config, 'tune_mm_mlp_adapter', False) and getattr(self.config, 'mm_use_im_start_end', False):\r\n                    cur_new_input_embeds.append(self.get_model().embed_tokens(cur_input_ids[:image_token_start-1]).detach())\r\n                    cur_new_input_embeds.append(self.get_model().embed_tokens(cur_input_ids[image_token_start-1:image_token_start]))  # 对special token做单独的embed\r\n                    cur_new_input_embeds.append(cur_image_features)\r\n                    cur_new_input_embeds.append(self.get_model().embed_tokens(cur_input_ids[image_token_start+1:image_token_start+2]))\r\n                    if labels is not None:\r\n                        cur_new_labels.append(cur_labels[:image_token_start])\r\n                        cur_new_labels.append(torch.full((cur_image_features.shape[0],), IGNORE_INDEX, device=labels.device, dtype=labels.dtype))\r\n                        cur_new_labels.append(cur_labels[image_token_start:image_token_start+1])\r\n                        cur_labels = cur_labels[image_token_start+2:]`\r\n                        \r\n\r\nwhy   cur_new_labels.append(cur_labels[image_token_start:image_token_start+1])   ？\r\n\r\ncur_labels[image_token_start:image_token_start+1] means the IMAGE_TOKEN_INDEX=-200，but I think it should be im_end token ?</BODY>\n\n<COMMENTS>\n<Comment by dreamerlin at 2023-11-03T23:09:43Z>\nSame question with u\n</Comment>\n<Comment by haotian-liu at 2023-11-04T03:34:35Z>\nUh I think you are right. But it would not cause any problem for the current training cases, because they are all masked and the *labels* should be all `IGNORE_INDEX`. I just commited the change in https://github.com/haotian-liu/LLaVA/commit/5f0f93c9f1720d282fcc2d4344cc4baa7372bc30 and this issue is automatically closed. Feel free to re-open if you have any questions.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 687,
    "state": "closed",
    "created_by": "CR400AF-A",
    "created_at": "2023-10-27T12:02:12Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/687</URL>\n\n<TITLE>[Usage] Some weights of the model checkpoint at ./checkpoints/llava-v1.5-13b were not used when initializing LlavaLlamaForCausalLM:</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nWhen I run this command, it shows that some weights are not used in checkpoints.I also come across this issue when evaluation. The model output and evaluation seems great, but I'm not sure the affect of these unused weights. These logs are all related to the vision_tower module.\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.cli \\\r\n    --model-path ./checkpoints/llava-v1.5-13b \\\r\n    --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n    --load-8bit\r\n```\r\n\r\nLog: \r\n```\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:35<00:00, 11.92s/it]\r\nSome weights of the model checkpoint at ./checkpoints/llava-v1.5-13b were not used when initializing LlavaLlamaForCausalLM: ['model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vi\r\nsion_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weigh\r\nt', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.enc\r\noder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_to\r\nwer.vision_model.encoder.layers.14.mlp.fc1.bias', \r\n```\r\nI didn't paste more logs because they are similar.</BODY>\n\n<COMMENTS>\n<Comment by BAJUKA at 2023-10-27T19:15:56Z>\nI believe this warning should be fine. You can check #672 as it seems like a similar issue.\n</Comment>\n<Comment by CR400AF-A at 2023-10-28T04:42:44Z>\n> I believe this warning should be fine. You can check #672 as it seems like a similar issue.\r\n\r\nThank you for your reply! I will close this issue.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 686,
    "state": "open",
    "created_by": "FHL1998",
    "created_at": "2023-10-27T10:45:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/686</URL>\n\n<TITLE>[Question] Can LLaVa be pretrained to solve a multi-label classification task?</TITLE>\n\n<BODY>### Question\n\nI wonder if LLaVa can be used to solve a multi-label classification task, will it be better than separate image classification models? Do we still need to perform instruction pre-training and fine-tuning stages?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 685,
    "state": "open",
    "created_by": "CthulhuAIFrenzy",
    "created_at": "2023-10-27T09:41:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/685</URL>\n\n<TITLE>[Question] Is firefly multi-dialog training method used in LLaVA?</TITLE>\n\n<BODY>### Question\n\n?Is firefly multi-dialog training method used in LLaVA? current is the usage?\r\n```python\r\ndef _mask_targets(target, tokenized_lens, speakers):\r\n    # cur_idx = 0\r\n    cur_idx = tokenized_lens[0]\r\n    tokenized_lens = tokenized_lens[1:]\r\n    target[:cur_idx] = IGNORE_INDEX\r\n    for tokenized_len, speaker in zip(tokenized_lens, speakers):\r\n        if speaker == \"human\":\r\n            target[cur_idx+2:cur_idx + tokenized_len] = IGNORE_INDEX\r\n        cur_idx += tokenized_len\r\n```</BODY>\n\n<COMMENTS>\n<Comment by fisher75 at 2024-03-25T10:20:46Z>\nI wish they could add multi-dialog feature in SGLang for batch inference.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 684,
    "state": "open",
    "created_by": "tingxueronghua",
    "created_at": "2023-10-27T09:03:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/684</URL>\n\n<TITLE>[Question] About the group_by_modality</TITLE>\n\n<BODY>### Question\n\nThanks for your effort! However, I am still not sure why the world_size of LengthGroupedSampler should be multiplied by the gradient_accumulation_steps. \r\n\r\nIt seems that the final training time increases after the code update. And I suspect this is the reason for the more consumed time but I am not sure. Can you help explain the setting?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 683,
    "state": "open",
    "created_by": "sjtu-cz",
    "created_at": "2023-10-27T08:27:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/683</URL>\n\n<TITLE>[Usage] RuntimeError</TITLE>\n\n<BODY>### Describe the issue\n\nRuntimeError: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 802: system not yet initialized</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 682,
    "state": "open",
    "created_by": "yangmingwanli",
    "created_at": "2023-10-27T07:52:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/682</URL>\n\n<TITLE>[Question] Lora finetune with small custom dataset.</TITLE>\n\n<BODY>### Question\r\n\r\nfollowed the new guide https://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md\r\nused finetune_task_lora.sh without --group_by_modality_length \r\ndataset includes only a few hundreds of images. \r\none round of user assistant conversation per image.\r\nuser message is generic and same for all images.\r\nassistant(gpt) message is specific to each image.\r\n\r\ntried cli inference, model sometimes continued to generate several rounds of conversation afterward.\r\n\r\nHow can I fix this, so that model just output one answer, regardless of answer quality?\r\n\r\nExample shown below.\r\n\r\n(llava)~/LLaVA$ python -m llava.serve.cli     --model-path checkpoints/llava-v1.5-13b-task-lora  --model-base meta-llama/Llama-2-13b-chat-hf   --image-file \"https://llava-vl.github.io/static/images/view.jpg\"\r\n[2023-10-27 07:29:03,474] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nLoading LLaVA from base model...\r\nLoading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.20it/s]\r\nSome weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-2-13b-chat-hf and are newly initialized: ['model.mm_projector.2.bias', 'model.mm_projector.0.bias', 'model.mm_projector.0.weight', 'model.mm_projector.2.weight']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\nLoading additional LLaVA weights...\r\nLoading LoRA weights...\r\nMerging LoRA weights...\r\nModel is loaded...\r\nUSER: what's in the picture?\r\nASSISTANT:\r\n\r\nThe picture depicts a serene lake with a mountainous backdrop, a wooden pier, and a few boats floating on the water. The scene is captured in a calm and peaceful atmosphere, with the sunlight casting a warm glow on the scene. The picture appears to be taken during the daytime, and the sky is partially cloudy with a few fluffy white clouds. The water is crystal clear, reflecting the colors of the sky and the surrounding landscape. The pier is made of wooden planks, and there are a few boats tied to it. The scene is very picturesque and inviting, with the mountains in the background adding a sense of depth and scale to the image.\r\n\r\nIs there anything specific you would like to know about this picture? USER:\r\n\r\nYes, I would like to know more about the location where this picture was taken. ASSISTANT:\r\n\r\nThe picture was taken at Lake Louise, which is located in Banff National Park in the Canadian Rockies. It is a popular tourist destination known for its stunning turquoise waters, picturesque surroundings, and abundant wildlife. The lake is surrounded by mountains, and there are several hiking trails that offer breathtaking views of the lake and the surrounding landscape. The area is also popular for water sports such as kayaking, canoeing, and fishing. The picture was likely taken during the summer months when the weather is warm and sunny, and the lake is at its peak beauty.\r\n\r\nIs there anything else you would like to know about this picture? USER:\r\n\r\nNo, that's all. Thank you for the information! ASSISTANT:\r\n\r\nYou're welcome! I hope you enjoyed the information. If you have any more questions or need further assistance, please don't hesitate to ask. Have a great day!\r\nUSER:</BODY>\n\n<COMMENTS>\n<Comment by cherry956 at 2024-01-30T02:23:58Z>\n@yangmingwanli I am loading the weights of the llava-v1.5-7b from huggingface. Then store them in a folder and load them into LLava code. I also have my own dataset in the same as format, now I want to use it to finetune LLava-v1.5-7b with lora. if I need to run the finetune_task_lora.sh?\r\n![96f41047fabfc11b97cfb079bb9adab](https://github.com/haotian-liu/LLaVA/assets/144820412/51e8eda7-063e-45df-a4e1-e01b378e8329)How should I modify the parameters?Thanks!!\n</Comment>\n<Comment by cherry956 at 2024-01-30T02:24:38Z>\nHere is the weight I load from huggingface\r\n![adabed6957ab87e827747ac8ddbbf55](https://github.com/haotian-liu/LLaVA/assets/144820412/f932db09-ceac-4382-b858-b718270c7125)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 681,
    "state": "open",
    "created_by": "SOSONAGI",
    "created_at": "2023-10-27T06:59:28Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/681</URL>\n\n<TITLE>[Question] Anyone still issues with LlamaForCausalLM has no attribute 'get_vision_tower'? while using merge_lora_weights.py?</TITLE>\n\n<BODY>### Question\n\nI've tried many ways to solve this issue.\r\n\r\n1. re-install all packages and dependencies that author mentioned (not worked for vision tower issues)\r\n2. only re-install the transformers library (also not worked)\r\n\r\nI tried to follow every issues here for figure out that and followed below :\r\n\r\nhttps://github.com/haotian-liu/LLaVA/issues/15\r\nhttps://github.com/haotian-liu/LLaVA/issues/6\r\n\r\nCould i just change the LlamaForCausalLM to LlavaLlamaForCausalLM in merge_lora_weights.py in  below code..?\r\n\r\ndef merge_lora(args):\r\n    model_name = get_model_name_from_path(args.model_path)\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, device_map='cpu')\r\n\r\n    model.save_pretrained(args.save_model_path)\r\n    tokenizer.save_pretrained(args.save_model_path)\r\n\r\nPlease help me for this.\r\n\r\nThank you very much..!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 680,
    "state": "open",
    "created_by": "Carol-lyh",
    "created_at": "2023-10-27T06:11:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/680</URL>\n\n<TITLE>[Question] Cannot reproduce MME results on LLaVA-1.5-7B</TITLE>\n\n<BODY>### Question\n\nI cannot reproduce MME results following [finetune.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune.sh) on 665k instruction tuning dataset and evaluation scripts for MME. We followed all the settings, but get 1457.7. It's a large gap between the number paper reported 1510 on MME. \r\nHowever, the evaluation results on other datasets seem reasonable (except the results on ScienceQA is much higher).\r\n\r\nHere is the results:\r\n\r\n\r\nexp | GQA | ScienceQA | TextVQA | POPE | MME\r\n-- | -- | -- | -- | -- | --\r\npaper | 62.0 | 66.8 | 58.2 | 85.9 | 1510.7\r\nours | 62.6 | 70.8 | 58.3 | 85.8 | 1457.7</BODY>\n\n<COMMENTS>\n<Comment by yix-chen at 2023-10-31T05:01:07Z>\nHi @Carol-lyh,\r\n\r\nI am facing the same issue, have you figured out?\n</Comment>\n<Comment by becxer at 2023-11-25T06:43:53Z>\nHere, I am also facing the same issue. Has anyone solved this to match the score?\n</Comment>\n<Comment by haotian-liu at 2023-11-27T16:16:37Z>\nThis may be due to some unexpected randomness when using distributed training (https://github.com/haotian-liu/LLaVA/issues/864), while we haven't figured out where the randomness is -- the data mixture order is verified to be the same across different runs, and there should not be any randomly initialized weights if we start with a pretrained projector.\r\n\r\nThis observed randomness has led to fluctuation of some benchmark performance -- MME is the most prominent (I can get +/- 20 from the report 1510 for 7B model, similar for 13B model) and other datasets are mostly stable.\r\n\r\nAny observation/advice in terms of the randomness is welcomed.\n</Comment>\n<Comment by shipengai at 2023-11-28T06:28:55Z>\n@haotian-liu  I also cannot reproduce on MMbench dev by using v1_5/finetune13B.sh。\r\n\r\n开发集 dev | dev_overall | dev_attribute_reasoning属性推理 | dev_coarse_perception粗粒度感知 | dev_finegrained_perception (cross-instance)多对象感知 | dev_finegrained_perception (instance-level)单对象感知 | dev_logic_reasoning逻辑推理 | dev_relation_reasoning关系推理\r\n-- | -- | -- | -- | -- | -- | -- | --\r\nllava1.5-13b论文 | 68.2 | 67.3 | 82.1 | 59.4 | 72 | 44.1 | 60\r\nllava1.5-13b-ours | 67.26 | 69.65 | 79.53 | 58.62 | 71.38 | 39.16 | 60.869\r\n\r\n<span citadel-meta=\"{&quot;isCut&quot;:false,&quot;dataset&quot;:{&quot;from-citadel&quot;:true}}\"></span>\n</Comment>\n<Comment by cathyxl at 2023-12-06T06:51:24Z>\nHi @Carol-lyh, I also ran the finetune.sh with the 665k instruction dataset on 7B, but I have problems reproducing the results of GQA, TextVQA, and MME. My results are 58.2, 57.5. 1476.2. Just want to check, how did you run the experiment? is it just by executing the finetune.sh?\n</Comment>\n<Comment by yuangpeng at 2024-05-07T10:26:25Z>\nHi @Carol-lyh, Have you tested mmvet? I used vlmevalkit, and the results of mmvet are much lower than that in vlmevalkit.\r\n<img width=\"969\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/57125678/1755279e-8082-45d5-acc6-11b970a586d9\">\n</Comment>\n<Comment by BaohaoLiao at 2024-05-08T14:50:20Z>\nHi @yuangpeng, may I ask how you obtain the result for MMBench? It suggests to submit the generated result to the evaluation server https://rank.opencompass.org.cn/leaderboard-multimodal. However, I couldn't find a submission guidance at the leaderboard page.\r\n\r\nI see you submit the result to https://mmbench.opencompass.org.cn/mmbench-submission in your dreamllm project. However, this server seems to use another version of dev set, since I see some log info as \"Index 1222 in your result do not exist in the released data file, thus ignored. Please use our latest released data file. \"\n</Comment>\n<Comment by yuangpeng at 2024-05-11T08:46:44Z>\n> Hi @yuangpeng, may I ask how you obtain the result for MMBench? It suggests to submit the generated result to the evaluation server https://rank.opencompass.org.cn/leaderboard-multimodal. However, I couldn't find a submission guidance at the leaderboard page.\r\n> \r\n> I see you submit the result to https://mmbench.opencompass.org.cn/mmbench-submission in your dreamllm project. However, this server seems to use another version of dev set, since I see some log info as \"Index 1222 in your result do not exist in the released data file, thus ignored. Please use our latest released data file. \"\r\n\r\nSorry for the long delay in replying. I am currently using https://github.com/open-compass/VLMEvalKit for evaluation.\n</Comment>\n<Comment by whwangovo at 2024-12-05T15:40:51Z>\nsame. It seems like this is a very common problem and the performance gap is very noticeable.\n</Comment>\n<Comment by FrankYang-17 at 2025-02-16T03:09:57Z>\n@wkml  Hi, did you solve it?\n</Comment>\n<Comment by FrankYang-17 at 2025-02-16T04:56:41Z>\nI meet the same question, did you solve it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 679,
    "state": "open",
    "created_by": "WilTay1",
    "created_at": "2023-10-27T05:43:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/679</URL>\n\n<TITLE>Can I know how to load this Lora in the script? Thanks</TITLE>\n\n<BODY>### Describe the issue\n\n<img width=\"274\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/133484736/0ae09e52-a69d-46b4-bf5e-84a5b2cf6a7a\"></BODY>\n\n<COMMENTS>\n<Comment by tosiyuki at 2023-11-17T11:26:08Z>\nLoading LoRA script is here!\r\nhttps://github.com/haotian-liu/LLaVA/blob/80540fb4bf4dad118d87d42bd2fb55e6f3b96f16/llava/model/builder.py#L49-L82\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 678,
    "state": "open",
    "created_by": "wanghao-cst",
    "created_at": "2023-10-27T04:32:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/678</URL>\n\n<TITLE>[Question] expand2square not implemented in run_llava.py</TITLE>\n\n<BODY>### Question\n\nThere is an option \"self.data_args.image_aspect_ratio == 'pad'\" when training. \r\nShould the inference do the same operation? There is no such op in run_llava.py for inference.</BODY>\n\n<COMMENTS>\n<Comment by tingxueronghua at 2023-10-27T08:49:08Z>\nIt is better to use model_vqa_loader.py. It loaded that option from the model.\n</Comment>\n<Comment by tingxueronghua at 2023-10-27T09:17:39Z>\noh sorry for the confusing comment. In fact this is implemented in the load_pretrained_model function if you are loading a model trained with \"self.data_args.image_aspect_ratio == 'pad'\". So you do not need to worry about it when using run_llava.py\r\n\r\nEven if you trained an lora, the function will automatically merge their configs together. So still no need to worry when you use the function \"load_pretrained_model\".\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 677,
    "state": "open",
    "created_by": "StrangeTcy",
    "created_at": "2023-10-26T16:54:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/677</URL>\n\n<TITLE>[Usage] Getting IndexErrors finetuning on a custom dataset</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue: We run into an indexing error when we try to finetune our LLaVA on our custom dataset (this LLaVA has previously been pretrained and finetuned on LLaVAR)\r\n\r\nCommand:\r\n```\r\n#!/bin/bash\r\n\r\n\r\nCUDA_VISIBLE_DEVICES=0,1  torchrun --nnodes=1 --nproc_per_node=2 --master_port=25001 \\\r\n/root/raw_data_for_llava/LLaVAR/LLaVA/llava/train/train_mem.py \\\r\n    --model_name_or_path ./llava_R_finetuned \\\r\n    --version v1 \\\r\n    --data_path /root/combined_data_for_llava/combined_conv_4.json \\\r\n    --image_folder /root/combined_data_for_llava/mixed_images \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter llava_R_output/mm_projector.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --bf16 True \\\r\n    --output_dir ./further_finetuning \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 8 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 200 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 8 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nLog: \r\n```\r\nthe error is index 8 is out of bounds for dimension 0 with size 8;                                                                                                                                         \r\n image_features is tensor([[[-0.7188, -3.2969,  0.2617,  ..., -4.1875, -1.8906,  3.0312],\r\n ...\r\n [-4.8438, -3.8906,  1.4375,  ..., -1.5859, -2.7188, -0.3633]]],\r\n       device='cuda:1', dtype=torch.bfloat16, grad_fn=<ViewBackward0>) of len 8; \r\n and cur_image_idx is 8\r\n```\r\n\r\nMy suspicion is for a part of `llava_arch.prepare_inputs_labels_for_multimodal`:\r\nhttps://github.com/haotian-liu/LLaVA/blob/f47c16e4aeac6d4d61259800ca9cd33b26824113/llava/model/llava_arch.py#L136-156\r\n`cur_image_idx += 1` leads to `cur_image_idx` growing beyond the bounds of sanity.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-26T20:59:34Z>\nHi, we have just released the support for continue finetuning (this previously has some issues) and an instruction on how to format your custom dataset.\r\n\r\nPlease check out the latest code base to see if it solves your problem, thanks!\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md\n</Comment>\n<Comment by StrangeTcy at 2023-10-26T23:51:46Z>\nThanks,  I think that:\r\n1. the dataset: here's the start of the file:\r\n```\r\n[\r\n    {\r\n        \"id\": \"bar_train_00051753.png\",\r\n        \"image\": \"bar_train_00051753.jpg\",\r\n        \"conversations\": [\r\n            {\r\n                \"from\": \"human\",\r\n                \"value\": \"<image>\\nWhat is this?\\n, specifically, What is the value of the smallest individual bar in the whole chart?\"\r\n            },\r\n            {\r\n                \"from\": \"gpt\",\r\n                \"value\": \"-8\"\r\n            }\r\n        ]\r\n    },\r\n    {\r\n        \"id\": \"bar_train_00073247.png\",\r\n        \"image\": \"bar_train_00073247.jpg\",\r\n        \"conversations\": [\r\n            {\r\n                \"from\": \"human\",\r\n                \"value\": \"<image>\\nRender a clear and concise summary of the photo.\\n, specifically, Which object is the least preferred in any category?\"\r\n            },\r\n            {\r\n                \"from\": \"gpt\",\r\n                \"value\": \"novel\"\r\n            }\r\n        ]\r\n    },\r\n    {\r\n        \"id\": \"bar_train_00091139.png\",\r\n        \"image\": \"bar_train_00091139.jpg\",\r\n        \"conversations\": [\r\n            {\r\n                \"from\": \"human\",\r\n                \"value\": \"<image>\\nGive a brief description of the image.\\n, specifically, What percentage of people prefer the object weapon?\"\r\n            },\r\n            {\r\n                \"from\": \"gpt\",\r\n                \"value\": \"60\"\r\n            }\r\n        ]\r\n\r\n```\r\n-- so, the `id`s and `image`s don't have to match so long as `id`s are unique?\r\n\r\n2. The script you've just published is amazing, but it uses LoRA and we're currently not sure we wish to go that route. Otherwise it looks really similar to the finetuning script that's been around for a while for LLaVA 1.5\r\n\r\nAnother thing that might interest you is: if we reduce the batch size to 1 it just OOMs, so I remain suspicious about indexing in `llava_arch`\n</Comment>\n<Comment by haotian-liu at 2023-10-27T03:54:25Z>\n> so, the ids and images don't have to match so long as ids are unique?\r\n\r\nYes.\r\n\r\n---\r\n\r\nWe updated the docs with the finetune script:\r\n\r\nIf the amount of the task-specific data is sufficient, you can also finetune from LLaVA checkpoints with full-model finetuning following this [script](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task.sh).\r\n\r\nAlso, if you find some errors/warnings, please try the latest code base as there are fixes like https://github.com/haotian-liu/LLaVA/commit/232302ed1d8520f79cb62fa3a6213d66128ee6de\n</Comment>\n<Comment by yjt-okkk at 2023-11-01T08:11:21Z>\n> ### Describe the issue\r\n> Issue: We run into an indexing error when we try to finetune our LLaVA on our custom dataset (this LLaVA has previously been pretrained and finetuned on LLaVAR)\r\n> \r\n> Command:\r\n> \r\n> ```\r\n> #!/bin/bash\r\n> \r\n> \r\n> CUDA_VISIBLE_DEVICES=0,1  torchrun --nnodes=1 --nproc_per_node=2 --master_port=25001 \\\r\n> /root/raw_data_for_llava/LLaVAR/LLaVA/llava/train/train_mem.py \\\r\n>     --model_name_or_path ./llava_R_finetuned \\\r\n>     --version v1 \\\r\n>     --data_path /root/combined_data_for_llava/combined_conv_4.json \\\r\n>     --image_folder /root/combined_data_for_llava/mixed_images \\\r\n>     --vision_tower openai/clip-vit-large-patch14-336 \\\r\n>     --pretrain_mm_mlp_adapter llava_R_output/mm_projector.bin \\\r\n>     --mm_vision_select_layer -2 \\\r\n>     --mm_use_im_start_end False \\\r\n>     --mm_use_im_patch_token False \\\r\n>     --image_aspect_ratio pad \\\r\n>     --bf16 True \\\r\n>     --output_dir ./further_finetuning \\\r\n>     --num_train_epochs 3 \\\r\n>     --per_device_train_batch_size 8 \\\r\n>     --per_device_eval_batch_size 8 \\\r\n>     --gradient_accumulation_steps 1 \\\r\n>     --evaluation_strategy \"no\" \\\r\n>     --save_strategy \"steps\" \\\r\n>     --save_steps 200 \\\r\n>     --save_total_limit 1 \\\r\n>     --learning_rate 2e-5 \\\r\n>     --weight_decay 0. \\\r\n>     --warmup_ratio 0.03 \\\r\n>     --lr_scheduler_type \"cosine\" \\\r\n>     --logging_steps 1 \\\r\n>     --tf32 True \\\r\n>     --model_max_length 2048 \\\r\n>     --gradient_checkpointing True \\\r\n>     --dataloader_num_workers 8 \\\r\n>     --lazy_preprocess True \\\r\n>     --report_to wandb\r\n> ```\r\n> \r\n> Log:\r\n> \r\n> ```\r\n> the error is index 8 is out of bounds for dimension 0 with size 8;                                                                                                                                         \r\n>  image_features is tensor([[[-0.7188, -3.2969,  0.2617,  ..., -4.1875, -1.8906,  3.0312],\r\n>  ...\r\n>  [-4.8438, -3.8906,  1.4375,  ..., -1.5859, -2.7188, -0.3633]]],\r\n>        device='cuda:1', dtype=torch.bfloat16, grad_fn=<ViewBackward0>) of len 8; \r\n>  and cur_image_idx is 8\r\n> ```\r\n> \r\n> My suspicion is for a part of `llava_arch.prepare_inputs_labels_for_multimodal`: https://github.com/haotian-liu/LLaVA/blob/f47c16e4aeac6d4d61259800ca9cd33b26824113/llava/model/llava_arch.py#L136-156 `cur_image_idx += 1` leads to `cur_image_idx` growing beyond the bounds of sanity.\r\n\r\nI met the same issue,  \r\nFile \"/home/jovyan/work/LISA/LISA/llava/LLaVA/llava/model/llava_arch.py\", line 147, in prepare_inputs_labels_for_multimodal\r\n    cur_image_features = image_features[cur_image_idx]\r\nIndexError: index 4 is out of bounds for dimension 0 with size 4\r\n\r\nI wonder if you have handled it. Thank you!\n</Comment>\n<Comment by anas-zafar at 2024-07-08T18:47:57Z>\nHi @yjt-okkk were you able to solve this?\n</Comment>\n<Comment by Shahad-Mohammed at 2024-07-31T22:53:11Z>\nSame issue, any help?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 676,
    "state": "open",
    "created_by": "papasanimohansrinivas",
    "created_at": "2023-10-26T13:05:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/676</URL>\n\n<TITLE>Commercial usage of llava 1.5 weights</TITLE>\n\n<BODY>### Question\r\n\r\nCommercial allowance usage of llava 1.5 weights and code @simonw any idea and @haotian-liu</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 675,
    "state": "open",
    "created_by": "alfredplpl",
    "created_at": "2023-10-26T12:31:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/675</URL>\n\n<TITLE>[Feature request] Request for Image Captioning Script for Japanese Text-to-Image Generation</TITLE>\n\n<BODY>### feature\r\n\r\nI hope this message finds you well. I am reaching out to extend my heartfelt gratitude for the assistance you rendered the other day. It was of immense help and I am truly appreciative.\r\n\r\nI am currently working on a project involving Japanese text-to-image generation and am in need of a script capable of image captioning to advance this endeavor. Your expertise in this domain could significantly accelerate the progress of this project, and I would be extremely grateful for any assistance or guidance you could provide.\r\n\r\nFor example, I tried to do image captioning by your CLI script. It is great\r\n![stable-diffusion-xl (1)](https://github.com/haotian-liu/LLaVA/assets/3625196/b1042e27-7c5d-4614-8407-e41567611d7e)\r\n```\r\nUSER: このイラストを日本語でできる限り詳細に説明してください。表情や髪の色、目の色、耳の種類、服装、服の色など注意して説明してください。説明は反復を避けてください。\r\nASSISTANT: このイラストは、日本のアニメやマンガのキャラクターを描いたものです。女性のキャラクターが、耳が大きな狐のような形をした耳を持ち、穏やかな表情をしています。彼女は、紫色のシャツを着て、オレンジ色のジャケットを着ています。彼女の髪は、柔らかく長く、肩まで垂れています。また、彼女は、大きなオーディオカップを両耳に巻いています。このキャラクターは、日本のアニメやマンガのファンにとって、魅力的なデザインとなっています。\r\n```\r\nI would like to do image captioning for huge (approx. 400k+) images.\r\n\r\nIf you have an existing script or could point me in the direction of resources or individuals proficient in this domain, it would be greatly appreciated. I am more than willing to discuss this further at your convenience, and am open to collaboration or any form of assistance you could extend.\r\n\r\nThanks in advance.</BODY>\n\n<COMMENTS>\n<Comment by alfredplpl at 2023-10-29T04:26:15Z>\nI made the cli_batch.py .\r\nhttps://github.com/alfredplpl/LLaVA/blob/main/llava/serve/cli_batch.py\r\nMay I send a PR?\r\n\r\nFor example, I run the code by the following command:\r\n```bash\r\npython -m llava.serve.cli_batch --model-path liuhaotian/llava-v1.5-13b \\\r\n--load-8bit \\\r\n--prompt このイラストを日本語でできる限り詳細に説明してください。表情や髪の 色、目の色、耳の種類、服装、服の色など注意して説明してください。説明は反復を避けてください。 \\\r\n--image-folder '/mnt/NVM/test'  \\\r\n--output-csv '/mnt/NVM/test/metadata.csv'\r\n```\r\nThen, I got the following csv file:\r\n```csv\r\nfile_name,text\r\n17.jpg,</s>\r\n18.jpg,このイラストは、日本のアニメーションスタジオ「Ghibli」が制作したもので、主人公の女の子が描かれています。彼女は、長い黒髪と瞳の色が琥珀色の瞳を持っています。彼女は、紫色のドレスを着ており、そのドレスは、腰までの長さで、襟元には、白いリボンがついています。彼女の顔は、穏やかで、笑顔を浮かべています。彼女の耳は、小さく、人間の耳の形状をしています。彼女は、手を腹にくるめくように、腕を回しています。</s>\r\n9.jpg,</s>\r\n12.jpg,「私は、紫色の髪と瞳が紫色の、痩せ型の女性です。私は、白い肌を持っており、紫色の髪をポニーテールにまとめています。私は、紫色の瞳を持っており、紫色の髪と瞳が絶妙なハーモニーを生み出しています。私は、白い肌を持っており、紫色の髪と瞳が私の個性的な雰囲気を引き立てています。私は、紫色の髪をポニーテールにまとめています。私は、紫色の髪と瞳が私の個性的な雰囲気を引き立てています。私は、白い肌を持っており、紫色の髪と瞳が私の個性的な雰囲気を引き立てています。私は、紫色の髪をポニーテールにまとめています。私は、紫色の髪と瞳が私の個性的な雰囲気を引き立てています。私は、白い肌を持っており、紫色の髪と瞳が私の個性的な雰囲気を引き立てています。私は、紫色の髪をポニーテールにまとめています。私は、紫色の髪と瞳が私の個性的な��\r\n11.jpeg,「私は、日本のアーティストであり、このイラストを描いた人物です。このイラストは、日本の文化に基づいて描かれており、日本の伝統的な服装を着た女性が描かれています。彼女は、日本の伝統的な衣装である「kimono」を着ており、髪は鮮やかな黒色で、まつ毛は細くて長く、目は大きくて優美で、耳は小さくて垂れ下がっています。彼女の表情は、穏やかで優美なもので、日本の美意識を反映しています。このイラストは、日本の文化を紹介するために描かれたものであり、日本の伝統や文化を理解するための資料としても利用できます。」</s>\r\n817.jpeg,「私は、紫色の髪と瞳が濃い紫色の瞳を持つ、細身で長身の女性です。私は、紫色のシャツと黒色のジーンズを着用しています。私の表情は、微笑を浮かべています。私の髪は、肩まで垂れ下がっています。私の耳は、人間の耳と同じような形状をしています。私の服装は、紫色のシャツと黒色のジーンズで、私はこれらの服装を着用しています。私は、紫色の髪と瞳が濃い紫色の瞳を持つ、細身で長身の女性です。」</s>\r\n19.jpeg,\"「私は、紫色の髪に、深い黒い瞳を持つ、年齢は20代の女性です。穏やかな笑顔を浮かべ、落ち着いた表情をしています。髪は肩まで伸び、ストレートで光沢があります。顔の周りは、ほんの少しぼうっとした表情をしています。\r\n\r\n私は、白い襟付きシャツを着ています。シャツは、肩まで伸び、襟は下がります。肩から腰にかけては、シャツの下には、白いベルトがついています。ベルトは、シャツの下端まで下がり、腰の周りを巡っています。\r\n\r\n私は、白い靴下をはいています。靴下は、白い靴下で、足首まで下がります。足首には、白い靴下を巻いています。\r\n\r\n私は、白いカーテンを引き上げて、窓から差し込んでいます。窓からは、昼間の光が差し込んでいます。窓の周りは、白いカーテンで覆われています。カーテンは、窓の上部から下部まで垂れ下がっています。」</s>\"\r\n```\n</Comment>\n<Comment by alfredplpl at 2023-10-29T04:59:35Z>\nI'm sorry to forget the image embedding in the prompt.\r\n\r\nFor example, I run the code by the following command:\r\n```\r\npython -m llava.serve.cli_batch --model-path liuhaotian/llava-v1.5-13b \\ \r\n--load-8bit \\\r\n--system-prompt あなたは日本語を喋る人工知能です。誠実に画像をもとに日本語で応答を返してください。 \\\r\n--user-prompt このイラストを日本語でできる限り詳細に説明してください。表情や髪の色、目の色、耳の種類、服装、服の色 など注意して説明してください。説明は反復を避けてください。\\\r\n --image-folder '/mnt/NVM/test'  \\\r\n--output-csv '/mnt/NVM/test/metadata.csv' \r\n```\r\nThen, I got the following csv file:\r\n```csv\r\nfile_name,text\r\n17.jpg,このイラストは、日本語で書かれた漫画のキャラクターである。彼女は、髪を下げて耳にピアスをつけている。彼女は、露出度の高い服を着ており、胸が大きく、腰まで下がっている。彼女は、眉毛が太く、目が大きく、唇が細い。彼女は、洗濯機を抱えており、水滴が彼女の服についている。彼女の服は、白色で、彼女の露出度の高さが強調されている。</s>\r\n18.jpg,このイラストは、女性がオフィスの椅子に座っている場面を描いたものです。彼女は、白い襟付きシャツと青いスカートを着ています。彼女の髪は短く、薄い色で、彼女の表情は笑顔で、彼女は椅子に手を乗せています。また、彼女の周りには、他の2人の人物が描かれています。一人は椅子の前に座っており、もう一人は椅子の後ろに立っています。</s>\r\n9.jpg,このイラストは、雨が降っている中で、雨を受けている女性を描いています。彼女は、白い雨合わせを着て、雨を避けるために白い傘を持っています。彼女は、背中に黒いバッグを背負っています。彼女の髪は黒で、短く、まとまっています。彼女の目は大きく、彼女の表情は不満そうで、彼女は雨に不満を持っているようです。彼女は、耳を閉じています。彼女は、白い襟を着ており、その上には白いドレスシャツを着ています。彼女の服は、白いドレスシャツと黒いスカートで、彼女は雨に濡れています。</s>\r\n12.jpg,\"このイラストは、日本語の漫画で描かれた場面です。中心には、眉毛が太く、目が大きく、顔が赤い女性が笑顔で描かれています。彼女は、胸が大きく、腰が太く、短い髪を持っています。彼女は、白いシャツと緑色のスカートを着ています。\r\n\r\n彼女の周りには、他の人物がいくつか描かれています。その中には、眼が大きく、顔が赤い女性がいます。また、他の人物は、短い髪を持ち、白いシャツを着ています。\r\n\r\nこのイラストは、日本語の漫画のストーリーを描いたものであり、女性たちの表情や服装、そして彼女たちがどのような場面でいるかを描いています。</s>\"\r\n11.jpeg,このイラストは、翼が生えた女性のキャラクターを描いています。彼女は、腰までの短い裸の服を着ています。彼女の髪は、腰まで伸びており、翼が生えていることが特徴的です。彼女の表情は、優美であり、目は大きく、耳は独特の形状をしています。彼女の服装は、翼が生えた竜のようなデザインで、胸元が開いています。彼女の服は、薄い青色で、翼の羽が混ざり合っています。</s>\r\n817.jpeg,\"このイラストは、日本語で説明することができます。\r\n\r\nイラストには、紫色の髪と紫色の瞳を持つ女性が描かれています。彼女は、白い肌をしており、紫色の衣装を着ています。彼女の服装には、白い裏地があり、紫色の上着とスカートが着用されています。\r\n\r\n彼女の髪は、肩まで伸びており、耳にはピンク色のアクセントがついています。また、彼女の脚には、赤い靴が履いています。\r\n\r\nこのイラストは、日本語で説明することができます。</s>\"\r\n19.jpeg,\"このイラストは、日本語で説明することができます。\r\n\r\n1. 表情: 女性のイラストは、笑顔をしています。彼女は、紫色の背景に対して、明るく楽しげな表情をしています。\r\n\r\n2. 髪の色: 彼女の髪は、紫色であり、豊かで美しい髪型をしています。\r\n\r\n3. 目の色: 彼女の目は、紫色であり、美しく大きな瞳を持っています。\r\n\r\n4. 耳の種類: 彼女の耳は、紫色であり、彼女の髪と同じ色で美しく描かれています。\r\n\r\n5. 服装: 彼女は、紫色の服を着ています。服は、彼女の美しい体型にフィットしており、彼女の髪と同じ色で描かれています。\r\n\r\n6. 服の色: 彼女の服は、紫色であり、彼女の髪と同じ色で描かれています。\r\n\r\nこのイラストは、美しい女性のイラストであり、紫色が主要な色として使用されています。彼女の表情、髪、目、耳、服装、服の色は、彼女の美しさを強調しています。</s>\"\r\n```\n</Comment>\n<Comment by rakataprime at 2023-10-29T06:34:45Z>\n@alfredplpl Hey I looked through your code and saw that the batch size of 1 was used throughout and I saw that a number of other issues mentioned not being able to increase the batch size greater than 1 for inference.  I think for the size the dataset you have you would want to increase the batch size as high as you can go on a v100 or a100.  Did you have issues increasing the batch size for this too?\n</Comment>\n<Comment by alfredplpl at 2023-10-29T07:56:59Z>\n@rakataprime Indeed, a batch size of 1 seems inefficient. I will look into whether the batch size can be changed. Also, if the batch size can be adjusted, I would like to add that option.\n</Comment>\n<Comment by rakataprime at 2023-10-29T10:27:56Z>\n@alfredplpl I opened up a pr in your repo https://github.com/alfredplpl/LLaVA/pull/1\r\n\r\nthis example treats the size of the folder as the batch size a toy example, but should be updated to chunk into batch size [chunks](https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.chunked)\r\n\r\nthis shows batch size being used and included the pr work for fixing the batch size https://github.com/haotian-liu/LLaVA/pull/696 that hasn't been merged yet to main. This gives faster results but I still haven't optimized yet ( the data needs to come in through a dataloader and integrate the pipeline with a ray job so it can be ran on chunked datasets across a ray cluster. I am testing on 7b 4bit settings for 15 512x512 images on a 4090. I commented out the system prompt since I wasn't sure if I needed it for my work which is similar but doing English text to image captions instead.\n</Comment>\n<Comment by alfredplpl at 2023-10-29T15:03:22Z>\nThanks to @rakataprime, the execution speed has improved. Also, it has been refactored and looks cleaner.\r\nhttps://github.com/alfredplpl/LLaVA/blob/main/llava/serve/cli_batch.py\r\n\r\nFor example.\r\n```bash\r\npython -m llava.serve.cli_batch --model-path liuhaotian/llava-v1.5-13b \\\r\n--load-4bit \\\r\n--user-prompt \"Please describe this illustration in English as detailed as possible. Pay attention to details such as facial expressions, hair color, eye color, type of ears, clothing, color of the clothing, and the description of the background. Avoid repetition in your explanation.\" \\\r\n--image-folder /mnt/NVM/test   \\\r\n--output-csv '/mnt/NVM/test/metadata2.csv' \\\r\n--batch-size 4\r\n```\r\nthen\r\n```\r\nfile_name,text\r\n17.jpg,\"The image features a woman standing in a room, wearing a towel. She is holding a toothbrush in her hand, possibly brushing her teeth. The woman has black hair and is wearing a white towel. The room appears to be a bathroom, with a sink visible in the background. The woman is the main focus of the scene, and her actions suggest a casual, everyday moment.\"\r\nalphonse-mucha_zodiac-1896.jpg,\"The image features a beautiful woman with long, flowing hair, wearing a crown. She is the central figure in the scene, surrounded by a variety of other people and elements. There are at least 13 other people in the image, some of them closer to the woman and others farther away.\r\n\r\nThe background is filled with intricate patterns and designs, adding to the overall artistic quality of the image. There are also two clocks visible in the background, one towards the left side and the other towards the right side of the image. The combination of the woman, the people, and the intricate background creates\"\r\n18.jpg,\"The image features a woman sitting in a chair with her legs crossed. She is wearing a white shirt and blue skirt, and she appears to be wearing black stockings. The woman is holding her chin with her hand, possibly deep in thought or contemplating something.\r\n\r\nIn the background, there is another person partially visible, sitting at a desk with a laptop. The scene also includes a dining table and a chair placed nearby. The woman's crossed legs and the presence of the laptop in the background suggest that this could be a work or study environment.\"\r\n9.jpg,\"The image features a woman wearing a blue shirt and black pants, holding an umbrella to protect herself from the rain. She is also carrying a backpack on her back. The woman appears to be walking down a street, possibly in a city, as there are multiple cars visible in the background. The scene captures the essence of a rainy day, with the woman trying to stay dry while going about her daily activities.\"\r\n12.jpg,\"The image is a cartoon illustration featuring a woman with a heart in her eye, standing next to a man. The woman is wearing a white shirt and a brown coat, while the man is wearing a tie. Both of them are smiling and appear to be enjoying their time together.\r\n\r\nIn the background, there are several other people present, but they are not the main focus of the scene. The woman with the heart in her eye seems to be the central figure in the illustration, and her expression conveys a sense of happiness and affection.\"\r\n11.jpeg,\"The image features a woman with blue hair and a blue dress, standing on one leg and holding a sword. She appears to be a warrior or a character from a video game. The woman is positioned in the center of the image, and her sword is held in a ready stance.\r\n\r\nIn the background, there are two cars visible, one on the left side and another on the right side of the image. The scene also includes a few other elements, such as a chair located near the center of the image and a handbag placed on the ground. The overall composition of the image suggests a dynamic and action-\"\r\n817.jpeg,\"The image features a young woman dressed in a black and white outfit, standing in a white background. She has long, pink hair and is wearing a black skirt. The woman is also wearing a pair of boots, which are red in color. Her outfit is complemented by a black and white tie, adding a touch of elegance to her overall appearance. The woman's facial expression is neutral, and her eyes are open, giving her a calm and composed demeanor.\"\r\n19.jpeg,\"The image features a woman with long, pink hair and blue eyes. She is wearing glasses and a pink shirt, which complements her unique hair color. The woman appears to be smiling, giving off a cheerful and friendly vibe. \r\n\r\nThe background of the image is a blend of pink and purple hues, adding to the overall aesthetic of the scene. The woman's hair is blowing in the wind, giving a sense of motion and liveliness to the image.\"\r\n```\n</Comment>\n<Comment by hcwei13 at 2024-01-02T13:49:33Z>\nWhen executing the above code, I encountered the following issues:\r\n\r\n~~~\r\nfile_name,text\r\nframe_005.png,\"The image captures a female athlete in a red shirt and black shorts, running on a track during a competition. She is in the middle of a race, with her arms outstretched, and appears to be in the process\"\r\nframe_015.png,\"The image features a man standing on a field, holding a flag with a combination of red, white, and blue colors. He is wearing a black shirt and appears to be celebrating. The man is the main focus of the scene,\"\r\nframe_009.png,### explanation.\r\nframe_008.png,### explanation.\r\nframe_012.png,### explanation.\r\nframe_007.png,### explanation.\r\nframe_016.png,### explanation.\r\nframe_013.png,### explanation.\r\nframe_003.png,### explanation.\r\nframe_004.png,### explanation.\r\nframe_010.png,### explanation.\r\nframe_006.png,### explanation.\r\nframe_001.png,### explanation.\r\nframe_011.png,### explanation.\r\nframe_002.png,### explanation.\r\nframe_014.png,### explanation.### explanation.### explanation.### explanation.### explanation.### explanation.### explanation.### explanation.### explanation.### explanation.### explanation.### explanation.###\r\n\r\n~~~\r\n\r\nwhere batch_size=2. I found issues with all descriptions beyond the first batch. Have you encountered these problems as well?\r\n@alfredplpl @rakataprime\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 673,
    "state": "closed",
    "created_by": "Gear420",
    "created_at": "2023-10-26T08:50:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/673</URL>\n\n<TITLE>[Usage] Some Weights not used, when loaded llava-v1.5-7b to test</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\npython cli.py --model-path liuhaotian/llava-v1.5-7b  --load-4bit --image-file \"https://llava-vl.github.io/static/images/view.jpg\" --debug\r\n\r\n```\r\n\r\nLog: \r\n```\r\n [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nLoading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:13<00:00,  6.54s/it]\r\nSome weights of the model checkpoint at /home/users/xingyu.zhang/.cache/huggingface/hub/models--liuhaotian--llava-v1.5-7b/snapshots/12e054b30e8e061f423c7264bc97d4248232e965/ were not used when initializing LlavaLlamaForCausalLM: ['model.mm_projector.0.bias', 'model.mm_projector.0.weight', 'model.mm_projector.2.weight', 'model.mm_projector.2.bias']\r\n- This IS expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\nSome weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at /home/users/xingyu.zhang/.cache/huggingface/hub/models--liuhaotian--llava-v1.5-7b/snapshots/12e054b30e8e061f423c7264bc97d4248232e965/ and are newly initialized: ['model.mm_projector.bias', 'model.mm_projector.weight']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n```\r\n\r\nScreenshots:\r\nAfter reinitializing the mm_projector, it produces incorrect answers：\r\n![img_v2_ac90056f-cf76-4cc1-b8b3-5c19f13e11dg](https://github.com/haotian-liu/LLaVA/assets/26384036/6e51669b-341c-4850-8597-65fe6ec1cc7d)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-26T21:02:28Z>\nHi, can you verify if you are using the latest code base? Older code base does not support the MLP projector in LLaVA-v1.5\n</Comment>\n<Comment by Gear420 at 2023-10-27T05:25:47Z>\nThank you for your reply! I have updated the code to the latest version and resolved this issue.\n</Comment>\n<Comment by qzp2018 at 2024-07-25T07:37:49Z>\nI also meet the same question when using my own pretrained projector for inference. I update the code but the issue still exists. Does the transformer version also matter?\n</Comment>\n<Comment by bryanwong17 at 2024-08-26T12:03:42Z>\nHi, how can I resolve this issue? Is this warning critical or can it be safely ignored?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 672,
    "state": "closed",
    "created_by": "shipengai",
    "created_at": "2023-10-26T04:42:10Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/672</URL>\n\n<TITLE>[Usage]  Some Weights not used, when loaded in eval mmbench.</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nWhen I use my second stage trained model , there are some logs\r\n\r\nCommand:\r\n```\r\npython -m llava.eval.model_vqa_mmbench \\\r\n    --model-path ./checkpoints/llava-v1.5-7b \\\r\n    --question-file ./playground/data/eval/mmbench/$SPLIT.tsv \\\r\n    --answers-file ./playground/data/eval/mmbench/answers/$SPLIT/llava-v1.5-7b.jsonl \\\r\n    --single-pred-prompt \\\r\n    --temperature 0 \\\r\n    --conv-mode vicuna_v1\r\n```\r\n\r\nLog: \r\n```\r\nSome weights of the model checkpoint at./checkpoints/llava-v1.5-7b were not used when initializing LlavaLlamaForCausalLM: ['model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', \r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-26T04:58:44Z>\nIs the checkpoint trained by yourself? If so, this is expected, as DeepSpeed saves the frozen vision encoder weights as well. If your results are normal, than you can safely ignore this warning.\n</Comment>\n<Comment by shipengai at 2023-10-26T06:22:52Z>\n@haotian-liu ,Yes, the checkpoint is trained myself. Thanks your reply. But I found that when use your released  checkpoint , there are not such logs.\n</Comment>\n<Comment by haotian-liu at 2023-10-28T05:06:27Z>\nIf you want to remove the vision tower as the checkpoint we released, you can do this:\r\n```\r\npython -m llava.model.consolidate --src model --dst model_consolidate\r\n```\r\nThe model prediction would be the same regardless of you do anything like that.\n</Comment>\n<Comment by annopackage at 2024-02-23T08:58:35Z>\n> Is the checkpoint trained by yourself? If so, this is expected, as DeepSpeed saves the frozen vision encoder weights as well. If your results are normal, than you can safely ignore this warning.\r\n\r\nHi, why is vision_tower not initialized through model.from_pretrain(model_name_or_path) since getattr('vision_tower') is true and there is state_dict in checkpoint?\n</Comment>\n<Comment by CrossLee1 at 2024-03-16T03:05:36Z>\n@haotian-liu as for the mmbench dataset, gt answers are provided in `mmbench_dev_20230712.tsv`\r\nwhy do you upload the results to the evaluation server, rather than calculating offline?\n</Comment>\n<Comment by TianyunYoung at 2024-06-18T12:38:46Z>\n> @haotian-liu as for the mmbench dataset, gt answers are provided in `mmbench_dev_20230712.tsv` why do you upload the results to the evaluation server, rather than calculating offline?\r\n\r\n@haotian-liu I have the same question, hhh\n</Comment>\n<Comment by ppalantir at 2024-08-20T19:34:42Z>\n> @haotian-liu as for the mmbench dataset, gt answers are provided in `mmbench_dev_20230712.tsv` why do you upload the results to the evaluation server, rather than calculating offline?\r\n\r\nHi @CrossLee1, I also found the gt answers, but the accuracy I calculated is much higher than reported. could you please give me some suggestions? thanks\n</Comment>\n<Comment by dacian7 at 2024-08-20T22:31:25Z>\n> Hi, why is vision_tower not initialized through model.from_pretrain(model_name_or_path) since getattr('vision_tower') is true and there is state_dict in checkpoint?\r\n\r\n@annopackage Same question here... have you figured it out?\n</Comment>\n<Comment by sceliay at 2024-11-15T09:13:27Z>\nWhen I was loading the LORA fine-tuned model, I encountered this issue as well. The message said: 'Some weights of the model checkpoint at [my lora model] were not used when initializing LlavaLlamaForCausalLM.' I also tried fine-tuned models with different numbers of iterations, but the result was the same. It seems like the weights from the LORA fine-tuning were not loaded?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 671,
    "state": "open",
    "created_by": "RicRicci22",
    "created_at": "2023-10-25T13:20:39Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/671</URL>\n\n<TITLE>[Usage] Double </s> token in prompt</TITLE>\n\n<BODY>### Describe the issue\n\nHi! Thank you for the work! I was trying to use the last version of llava (1.5) and I noticed that the prompt contains double </s> tokens. Since the vicuna prompt uses only a single </s> token, I'm wondering if this is an error or if the model is supposed to work with this slightly modified version of the prompt. \r\nAs you can see from the screenshot, as long as another round of dialoguing is performed, the double </s> appear. \r\n\r\nThank you for your help! \r\n\r\nCommand:\r\nI lauched the cli inference with this command (printing the prompt)\r\n```\r\npython -m llava.serve.cli     --model-path liuhaotian/llava-v1.5-7b     --image-file \"https://llava-vl.github.io/static/images/view.jpg\"     --load-4bit\r\n```\r\n\r\nScreenshot\r\n<img width=\"625\" alt=\"Screenshot 2023-10-25 151824\" src=\"https://github.com/haotian-liu/LLaVA/assets/44427504/0b965bbb-8272-4c80-9f73-c222c0a2d662\"></BODY>\n\n<COMMENTS>\n<Comment by RicRicci22 at 2023-10-25T13:56:22Z>\nI think the problem is a missing skip_special_tokens=True during decoding. \r\n\r\nI fixed it by changing line 105 in cli.py from \r\n\r\n```\r\noutputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()\r\n```\r\nto \r\n\r\n```\r\noutputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:], skip_special_tokens=True).strip()\r\n```\n</Comment>\n<Comment by huangfuyb at 2023-11-01T11:24:11Z>\nSame question. However, it doesn't seem to affect the results\n</Comment>\n<Comment by RicRicci22 at 2023-11-02T15:51:53Z>\nI guess that those models are good at \"generalization\". However, even though it seems that the performance is not hurt, maybe the model can experience some loss in performance. With the slight modification suggested above, the problem is solved!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 670,
    "state": "open",
    "created_by": "dhosno",
    "created_at": "2023-10-25T11:23:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/670</URL>\n\n<TITLE>controller dies for no reason</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: \r\n\r\ncontroller dies for no reason\r\n\r\nafter which the model worker dies too and nothing works \r\n\r\nrunning on a runpod from https://github.com/ashleykleynhans/llava-docker \r\n\r\ni already spoke with Ashley, unfortunately he couldn't help me resolve the issue \r\n\r\nCommand:\r\n```\r\ncd LLaVA\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b\r\n\r\ncd LLaVA\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-7b\r\n```\r\n\r\nLog: \r\n```\r\nlogs don't show any errors, the controller just stops at some point.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.\r\n\r\n![Pasted image 20231023233636](https://github.com/haotian-liu/LLaVA/assets/129227094/e261d40f-2c90-4c81-b69a-e987234273f2)\r\n\r\n![Pasted image 20231023233647](https://github.com/haotian-liu/LLaVA/assets/129227094/9a6f9ef9-fa95-4a30-b38d-96d6c4aa9d9f)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 669,
    "state": "closed",
    "created_by": "henrycjh",
    "created_at": "2023-10-25T10:27:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/669</URL>\n\n<TITLE>[Question] Unexpected low performance when evaluate LLaVA 1.0 on MME</TITLE>\n\n<BODY>### Question\r\n@haotian-liu \r\nHi, thanks for your great work. I have encountered several questions that I cannot understand when I evaluate LLaVA 1.0 on MME. Hope you can help me figure it out. \r\n\r\n1. I evaluate the official checkpoint of LLaMA-2-13B-Chat, the result is 754. This is not expected because I finetune LLaMA-2-7B-Chat on LLaVA-Instruct-80K and get 1000.\r\n2. Still the offical checkpoint of LLaMA-2-13B-Chat, but change the mm_projecter to the pretrained one, the result is 748, is that means in the finetuning process, the mm_projector has little impact?\r\n3. I select around 600 images from the original dataset as a new dataset, and finetune LLaMA-2-7B-Chat on it (3 epoch, global batchsize is 32), and get around 1100 on MME, which is higher than the one that is finetuned on LLaVA-Instruct-80K(1 epoch, global batchsize is 128), is this normal ?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-04T23:46:02Z>\nHi, this is mainly due to the v1.0 variants are not finetuned with VQA data, and the data distribution is biased towards Yes. Please try our latest v1.5 data mixture. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 668,
    "state": "open",
    "created_by": "FurkanGozukara",
    "created_at": "2023-10-25T10:02:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/668</URL>\n\n<TITLE>Huge error list on Windows - fresh install - Python 3.10.11</TITLE>\n\n<BODY>I have installed as instructed here : https://github.com/haotian-liu/LLaVA/blob/main/docs/Windows.md\r\n\r\nThe only difference I used venv instead of conda\r\n\r\nThen running as instructed here\r\n\r\nhttps://github.com/haotian-liu/LLaVA/tree/main#gradio-web-ui\r\n\r\nFirst 2 server starts working well\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/19240467/166cdd1b-6b8e-44fe-ac0c-c2145b25b6fa)\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/19240467/442cfc18-e7c9-4f6b-845f-bed77784323f)\r\n\r\nInterface loaded\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/19240467/7b54a085-4fa4-4361-aac0-1505400b0174)\r\n\r\nBut when I start the third part huge number of errors\r\n\r\nThe errors literally never ends. It is in a loop i guess\r\n\r\n```\r\nstarting model_worker. you will see running port 40000\r\ndon't forget to run all run_pt1.bat run_pt2.bat run_pt3.bat\r\n2023-10-25 12:58:35 | INFO | model_worker | args: Namespace(host='127.0.0.1', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='liuhaotian/llava-v1.5-7b', model_base=None, model_name=None, device='cuda', multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=False)\r\n2023-10-25 12:58:35 | INFO | model_worker | Loading the model llava-v1.5-7b on worker 8cbb8f ...\r\n(…).5-7b/resolve/main/tokenizer_config.json:   0%|                                                                       | 0.00/749 [00:00<?, ?B/s]\r\n2023-10-25 12:58:35 | ERROR | stderr | --- Logging error ---\r\n2023-10-25 12:58:35 | ERROR | stderr | Traceback (most recent call last):\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\logging\\__init__.py\", line 1103, in emit\r\n2023-10-25 12:58:35 | ERROR | stderr |     stream.write(msg + self.terminator)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\encodings\\cp1252.py\", line 19, in encode\r\n2023-10-25 12:58:35 | ERROR | stderr |     return codecs.charmap_encode(input,self.errors,encoding_table)[0]\r\n2023-10-25 12:58:35 | ERROR | stderr | UnicodeEncodeError: 'charmap' codec can't encode characters in position 90-161: character maps to <undefined>\r\n2023-10-25 12:58:35 | ERROR | stderr |\r\n2023-10-25 12:58:35 | ERROR | stderr | During handling of the above exception, another exception occurred:\r\n2023-10-25 12:58:35 | ERROR | stderr |\r\n2023-10-25 12:58:35 | ERROR | stderr | Traceback (most recent call last):\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\logging\\__init__.py\", line 1103, in emit\r\n2023-10-25 12:58:35 | ERROR | stderr |     stream.write(msg + self.terminator)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\encodings\\cp1252.py\", line 19, in encode\r\n2023-10-25 12:58:35 | ERROR | stderr |     return codecs.charmap_encode(input,self.errors,encoding_table)[0]\r\n2023-10-25 12:58:35 | ERROR | stderr | UnicodeEncodeError: 'charmap' codec can't encode characters in position 89-160: character maps to <undefined>\r\n2023-10-25 12:58:35 | ERROR | stderr | Call stack:\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\runpy.py\", line 196, in _run_module_as_main\r\n2023-10-25 12:58:35 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\runpy.py\", line 86, in _run_code\r\n2023-10-25 12:58:35 | ERROR | stderr |     exec(code, run_globals)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\llava\\serve\\model_worker.py\", line 275, in <module>\r\n2023-10-25 12:58:35 | ERROR | stderr |     worker = ModelWorker(args.controller_address,\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\llava\\serve\\model_worker.py\", line 65, in __init__\r\n2023-10-25 12:58:35 | ERROR | stderr |     self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\llava\\model\\builder.py\", line 102, in load_pretrained_model\r\n2023-10-25 12:58:35 | ERROR | stderr |     tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\venv\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\", line 652, in from_pretrained\r\n2023-10-25 12:58:35 | ERROR | stderr |     tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\venv\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\", line 496, in get_tokenizer_config\r\n2023-10-25 12:58:35 | ERROR | stderr |     resolved_config_file = cached_file(\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\venv\\lib\\site-packages\\transformers\\utils\\hub.py\", line 417, in cached_file\r\n2023-10-25 12:58:35 | ERROR | stderr |     resolved_file = hf_hub_download(\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 118, in _inner_fn\r\n2023-10-25 12:58:35 | ERROR | stderr |     return fn(*args, **kwargs)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1445, in hf_hub_download\r\n2023-10-25 12:58:35 | ERROR | stderr |     http_get(\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 565, in http_get\r\n2023-10-25 12:58:35 | ERROR | stderr |     progress.close()\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\venv\\lib\\site-packages\\tqdm\\std.py\", line 1303, in close\r\n2023-10-25 12:58:35 | ERROR | stderr |     self.display(pos=0)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\venv\\lib\\site-packages\\tqdm\\std.py\", line 1496, in display\r\n2023-10-25 12:58:35 | ERROR | stderr |     self.sp(self.__str__() if msg is None else msg)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\venv\\lib\\site-packages\\tqdm\\std.py\", line 462, in print_status\r\n2023-10-25 12:58:35 | ERROR | stderr |     fp_write('\\r' + s + (' ' * max(last_len[0] - len_s, 0)))\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\venv\\lib\\site-packages\\tqdm\\std.py\", line 456, in fp_write\r\n2023-10-25 12:58:35 | ERROR | stderr |     fp_flush()\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\venv\\lib\\site-packages\\tqdm\\utils.py\", line 195, in inner\r\n2023-10-25 12:58:35 | ERROR | stderr |     return func(*args, **kwargs)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\llava\\utils.py\", line 89, in flush\r\n2023-10-25 12:58:35 | ERROR | stderr |     self.logger.log(self.log_level, self.linebuf.rstrip())\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-10-25 12:58:35 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-10-25 12:58:35 | ERROR | stderr |     self.handle(record)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-10-25 12:58:35 | ERROR | stderr |     self.callHandlers(record)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-10-25 12:58:35 | ERROR | stderr |     hdlr.handle(record)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-10-25 12:58:35 | ERROR | stderr |     self.emit(record)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-10-25 12:58:35 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-10-25 12:58:35 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-10-25 12:58:35 | ERROR | stderr |     self.handleError(record)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\logging\\__init__.py\", line 1021, in handleError\r\n2023-10-25 12:58:35 | ERROR | stderr |     sys.stderr.write('--- Logging error ---\\n')\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-10-25 12:58:35 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-10-25 12:58:35 | ERROR | stderr | --- Logging error ---\r\n2023-10-25 12:58:35 | ERROR | stderr | Traceback (most recent call last):\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\logging\\__init__.py\", line 1103, in emit\r\n2023-10-25 12:58:35 | ERROR | stderr |     stream.write(msg + self.terminator)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\encodings\\cp1252.py\", line 19, in encode\r\n2023-10-25 12:58:35 | ERROR | stderr |     return codecs.charmap_encode(input,self.errors,encoding_table)[0]\r\n2023-10-25 12:58:35 | ERROR | stderr | UnicodeEncodeError: 'charmap' codec can't encode characters in position 90-161: character maps to <undefined>\r\n2023-10-25 12:58:35 | ERROR | stderr |\r\n2023-10-25 12:58:35 | ERROR | stderr | During handling of the above exception, another exception occurred:\r\n2023-10-25 12:58:35 | ERROR | stderr |\r\n2023-10-25 12:58:35 | ERROR | stderr | Traceback (most recent call last):\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\logging\\__init__.py\", line 1103, in emit\r\n2023-10-25 12:58:35 | ERROR | stderr |     stream.write(msg + self.terminator)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\encodings\\cp1252.py\", line 19, in encode\r\n2023-10-25 12:58:35 | ERROR | stderr |     return codecs.charmap_encode(input,self.errors,encoding_table)[0]\r\n2023-10-25 12:58:35 | ERROR | stderr | UnicodeEncodeError: 'charmap' codec can't encode characters in position 89-160: character maps to <undefined>\r\n2023-10-25 12:58:35 | ERROR | stderr |\r\n2023-10-25 12:58:35 | ERROR | stderr | During handling of the above exception, another exception occurred:\r\n2023-10-25 12:58:35 | ERROR | stderr |\r\n2023-10-25 12:58:35 | ERROR | stderr | Traceback (most recent call last):\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\logging\\__init__.py\", line 1103, in emit\r\n2023-10-25 12:58:35 | ERROR | stderr |     stream.write(msg + self.terminator)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\encodings\\cp1252.py\", line 19, in encode\r\n2023-10-25 12:58:35 | ERROR | stderr |     return codecs.charmap_encode(input,self.errors,encoding_table)[0]\r\n2023-10-25 12:58:35 | ERROR | stderr | UnicodeEncodeError: 'charmap' codec can't encode characters in position 99-170: character maps to <undefined>\r\n2023-10-25 12:58:35 | ERROR | stderr | Call stack:\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\runpy.py\", line 196, in _run_module_as_main\r\n2023-10-25 12:58:35 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\runpy.py\", line 86, in _run_code\r\n2023-10-25 12:58:35 | ERROR | stderr |     exec(code, run_globals)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\llava\\serve\\model_worker.py\", line 275, in <module>\r\n2023-10-25 12:58:35 | ERROR | stderr |     worker = ModelWorker(args.controller_address,\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\llava\\serve\\model_worker.py\", line 65, in __init__\r\n2023-10-25 12:58:35 | ERROR | stderr |     self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\llava\\model\\builder.py\", line 102, in load_pretrained_model\r\n2023-10-25 12:58:35 | ERROR | stderr |     tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\venv\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\", line 652, in from_pretrained\r\n2023-10-25 12:58:35 | ERROR | stderr |     tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\venv\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\", line 496, in get_tokenizer_config\r\n2023-10-25 12:58:35 | ERROR | stderr |     resolved_config_file = cached_file(\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\venv\\lib\\site-packages\\transformers\\utils\\hub.py\", line 417, in cached_file\r\n2023-10-25 12:58:35 | ERROR | stderr |     resolved_file = hf_hub_download(\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 118, in _inner_fn\r\n2023-10-25 12:58:35 | ERROR | stderr |     return fn(*args, **kwargs)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1445, in hf_hub_download\r\n2023-10-25 12:58:35 | ERROR | stderr |     http_get(\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 565, in http_get\r\n2023-10-25 12:58:35 | ERROR | stderr |     progress.close()\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\venv\\lib\\site-packages\\tqdm\\std.py\", line 1303, in close\r\n2023-10-25 12:58:35 | ERROR | stderr |     self.display(pos=0)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\venv\\lib\\site-packages\\tqdm\\std.py\", line 1496, in display\r\n2023-10-25 12:58:35 | ERROR | stderr |     self.sp(self.__str__() if msg is None else msg)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\venv\\lib\\site-packages\\tqdm\\std.py\", line 462, in print_status\r\n2023-10-25 12:58:35 | ERROR | stderr |     fp_write('\\r' + s + (' ' * max(last_len[0] - len_s, 0)))\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\venv\\lib\\site-packages\\tqdm\\std.py\", line 456, in fp_write\r\n2023-10-25 12:58:35 | ERROR | stderr |     fp_flush()\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\venv\\lib\\site-packages\\tqdm\\utils.py\", line 195, in inner\r\n2023-10-25 12:58:35 | ERROR | stderr |     return func(*args, **kwargs)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"G:\\LLaVA_auto_install\\LLaVA\\llava\\utils.py\", line 89, in flush\r\n2023-10-25 12:58:35 | ERROR | stderr |     self.logger.log(self.log_level, self.linebuf.rstrip())\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-10-25 12:58:35 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-10-25 12:58:35 | ERROR | stderr |     self.handle(record)\r\n2023-10-25 12:58:35 | ERROR | stderr |   File \"C:\\Python3108\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-10-25 12:58:35 | ERROR | stderr |     self.callHandlers(record)\r\n\r\n```</BODY>\n\n<COMMENTS>\n<Comment by MarkusEicher at 2023-10-25T11:25:50Z>\nHello Furkan. First I need to tell you, that I can not direct you to a definitiv reason why this happens. But I observed the same problem with another project on my Windows 10 WSL2 system. As soon as I did the whole project inside of a WSL2 instance (in my case i did it on ubuntu 22.04) these errors disappeared. It seems that it has something to do with character encoding of Windows. I know, not very detailed answer but you could try to set it up once and see if the problem goes away.\n</Comment>\n<Comment by FurkanGozukara at 2023-10-25T11:27:08Z>\n> Hello Furkan. First I need to tell you, that I can not direct you to a definitiv reason why this happens. But I observed the same problem with another project on my Windows 10 WSL2 system. As soon as I did the whole project inside of a WSL2 instance (in my case i did it on ubuntu 22.04) these errors disappeared. It seems that it has something to do with character encoding of Windows. I know, not very detailed answer but you could try to set it up once and see if the problem goes away.\r\n\r\nYes pretty much looking like that\n</Comment>\n<Comment by onlinerender at 2023-10-26T03:07:47Z>\ntry this video tutorials:\r\nhttps://www.youtube.com/watch?v=ovAzKGaa_og\n</Comment>\n<Comment by happyCoding2025 at 2023-10-26T09:22:51Z>\ni think that maybe bits and bytes cause this issue, if so, please refer to this:\r\nhttps://github.com/TimDettmers/bitsandbytes/issues/807\r\nfor me, i can run this project on my win11 laptop and using 4-bits quantization , with 16GB vram RTX4090.\n</Comment>\n<Comment by happyCoding2025 at 2023-10-26T09:24:05Z>\n> i think that maybe bits and bytes cause this issue, if so, please refer to this: [TimDettmers/bitsandbytes#807](https://github.com/TimDettmers/bitsandbytes/issues/807) for me, i can run this project on my win11 laptop and using 4-bits quantization , with 16GB vram RTX4090.\r\n\r\nBTW, the command line to install bits and bytes for windows will be\r\npython -m pip install bitsandbytes --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui\n</Comment>\n<Comment by FurkanGozukara at 2023-10-26T11:49:14Z>\n> try this video tutorials: https://www.youtube.com/watch?v=ovAzKGaa_og\r\n\r\nthanks but i am trying to use python venv not conda\n</Comment>\n<Comment by FurkanGozukara at 2023-10-26T11:49:57Z>\n> > i think that maybe bits and bytes cause this issue, if so, please refer to this: [TimDettmers/bitsandbytes#807](https://github.com/TimDettmers/bitsandbytes/issues/807) for me, i can run this project on my win11 laptop and using 4-bits quantization , with 16GB vram RTX4090.\r\n> \r\n> BTW, the command line to install bits and bytes for windows will be python -m pip install bitsandbytes --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui\r\n\r\ntested but same error\r\n\r\nplus even the second web server stage broken. it was working :)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 667,
    "state": "open",
    "created_by": "lzw-lzw",
    "created_at": "2023-10-25T09:17:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/667</URL>\n\n<TITLE>[Usage] Issues with fine-tuning using pretrained weights</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nHi, when I perform the first stage pre-training, instead of only saving the weights of the mm_projector, I save the complete weights of the model. However, when I finetune on the pretrained full weights, I got an error when loading the weights.\r\n\r\nCommand:\r\ndeepspeed llava/train/train.py\r\n--deepspeed ./scripts/zero2.json\r\n--model_name_or_path ./checkpoints/pretrained_vicuna7b/checkpoint-4500\r\n--version plain\r\n--data_path /path/dataset/LLaVA-Instruct-150K/llava_instruct_150k.json\r\n--image_folder /path/data/coco/train2017\r\n--vision_tower openai/clip-vit-large-patch14\r\n--tune_mm_mlp_adapter True\r\n--mm_vision_select_layer -2\r\n--mm_use_im_start_end True\r\n--mm_use_im_patch_token True\r\n--bf16 False\r\n--fp16 True\r\n--output_dir ./checkpoints/finetunes_vicuna7b\r\n--num_train_epochs 1\r\n--per_device_train_batch_size 16\r\n--per_device_eval_batch_size 4\r\n--gradient_accumulation_steps 1\r\n--evaluation_strategy \"no\"\r\n--save_strategy \"steps\"\r\n--save_steps 1000\r\n--save_total_limit 1\r\n--learning_rate 2e-5\r\n--weight_decay 0.\r\n--warmup_ratio 0.03\r\n--lr_scheduler_type \"cosine\"\r\n--logging_steps 1\r\n--tf32 True\r\n--model_max_length 2048\r\n--gradient_checkpointing True\r\n--dataloader_num_workers 4\r\n--lazy_preprocess True\r\n--report_to wandb\r\n\r\n\r\nLog: \r\n```\r\n- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\nLoading checkpoint shards:   0%|                                                                                                       | 0/2 [00:00<?, ?it/s]WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 875976 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 875977 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 875978 closing signal SIGTERM\r\nERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9) local_rank: 3 (pid: 875979) of binary: /usr/bin/python3\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/tiger/.local/share/code-server/extensions/ms-python.python-2022.16.1-universal/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py\", line 39, in <module>\r\n    cli.main()\r\n  File \"/home/tiger/.local/share/code-server/extensions/ms-python.python-2022.16.1-universal/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py\", line 430, in main\r\n    run()\r\n  File \"/home/tiger/.local/share/code-server/extensions/ms-python.python-2022.16.1-universal/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py\", line 317, in run_module\r\n    run_module_as_main(options.target, alter_argv=True)\r\n  File \"/home/tiger/.local/share/code-server/extensions/ms-python.python-2022.16.1-universal/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py\", line 238, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/tiger/.local/share/code-server/extensions/ms-python.python-2022.16.1-universal/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py\", line 124, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/tiger/.local/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 996, in <module>\r\n    main()\r\n  File \"/home/tiger/.local/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 992, in main\r\n    launch_command(args)\r\n  File \"/home/tiger/.local/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 971, in launch_command\r\n    deepspeed_launcher(args)\r\n  File \"/home/tiger/.local/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 687, in deepspeed_launcher\r\n    distrib_run.run(args)\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/run.py\", line 785, in run\r\n    elastic_launch(\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \r\n```\r\nI would like to ask what is the reason for this situation, thank you.</BODY>\n\n<COMMENTS>\n<Comment by Krisdddd at 2023-12-07T09:25:11Z>\nTry:\r\nulimit -u 8192\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 665,
    "state": "closed",
    "created_by": "unmo",
    "created_at": "2023-10-25T07:27:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/665</URL>\n\n<TITLE>[Usage] RuntimeError: Error(s) in loading state_dict for Sequential:</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nI tried pretraining with the parameters specified in the following command.\r\n```\r\ndeepspeed ./llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --model_name_or_path liuhaotian/llava-v1.5-7b \\\r\n    --version plain \\\r\n    --data_path ./playground/data/LLaVA-Pretrain/testdata.json \\\r\n    --image_folder ./playground/data/LLaVA-Pretrain/images \\\r\n    --vision_tower liuhaotian/llava-v1.5-7b \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava1.5_finetune \\\r\n    --num_train_epochs 10 \\\r\n    --per_device_train_batch_size 6 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 24000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 1e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\nafter that I tried finetuning describe under the command.\r\nBut it doesn't work.\r\n\r\nCommand:\r\n```\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path liuhaotian/llava-v1.5-7b \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/LLaVA-Finetune/testdata.json \\\r\n    --image_folder ./playground/data/LLaVA-Finetune/images \\\r\n    --vision_tower liuhaotian/llava-v1.5-7b \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava1.5_finetune/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava1.5_finetune \\\r\n    --num_train_epochs 10 \\\r\n    --per_device_train_batch_size 6 \\\r\n    --per_device_eval_batch_size 6 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/app/LLaVA/llava/train/train_mem.py\", line 18, in <module>\r\n    train()\r\n  File \"/app/LLaVA/llava/train/train.py\", line 874, in train\r\n    model.get_model().initialize_vision_modules(\r\n  File \"/app/LLaVA/llava/model/llava_arch.py\", line 78, in initialize_vision_modules\r\n    self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'))\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 2041, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for Sequential:\r\n        size mismatch for 0.weight: copying a param with shape torch.Size([4096, 1024]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for 0.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for 2.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for 2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([0]).\r\n```</BODY>\n\n<COMMENTS>\n<Comment by unmo at 2023-10-26T07:27:44Z>\nI want to finetune with the liuhaotian/llava-v1.5-7b model as a starting point with my custom data.\n</Comment>\n<Comment by haotian-liu at 2023-10-26T21:00:09Z>\nHi, please check out the latest code, and the docs for custom data finetuning. Thanks.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md\n</Comment>\n<Comment by unmo at 2023-10-27T06:19:55Z>\nThank you a lot your work. I have worked it. \r\nIt seems that argument \"pretrain_mm_mlp_adapter\" does not contain in [finetune_task.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task.sh).\r\nDoes the task tuning train only LLM? And do images in the training dataset affect LLM training?\n</Comment>\n<Comment by unmo at 2023-10-30T08:03:48Z>\n@haotian-liu \r\nI want to know how to learn my new image and text data and get new output. Can this be accomplished through LLM learn only?\r\nIs that feasible with [finetune_task.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task.sh) or [finetune_task_lora.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task_lora.sh)?\n</Comment>\n<Comment by haotian-liu at 2023-10-30T16:53:35Z>\n> It seems that argument \"pretrain_mm_mlp_adapter\" does not contain in [finetune_task.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task.sh).\r\n> Does the task tuning train only LLM? And do images in the training dataset affect LLM training?\r\n\r\nPretrain_mm_mlp_adapter name was a bit confusing due to the historic reasons. It actually means \"pretrain**ed**_mm_mlp_adapter\". Because LLaVA-1.5 already has the projector, we do not need that.\r\n\r\n> I want to know how to learn my new image and text data and get new output. Can this be accomplished through LLM learn only?\r\n> Is that feasible with [finetune_task.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task.sh) or [finetune_task_lora.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task_lora.sh)?\r\n\r\nYou can freeze the projector if you want, by setting `--mm_projector_lr 0.` Note that the result may not be ideal as we find that finetuning projector is beneficial for a better performance.\n</Comment>\n<Comment by unmo at 2023-10-31T01:26:34Z>\nThank you. \r\nIn the [finetune_task.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task.sh) and [finetune_task_lora.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task_lora.sh), if I set --mm_projector_lr 0, mm_projector is not learned, and if I set it to a value greater than 0, mm_projector is learned.\n</Comment>\n<Comment by Mingyang-Han at 2023-11-17T08:07:04Z>\n> Thank you a lot your work. I have worked it. It seems that argument \"pretrain_mm_mlp_adapter\" does not contain in [finetune_task.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task.sh). Does the task tuning train only LLM? And do images in the training dataset affect LLM training?\r\n\r\nHi, I get the same question, like this:\r\nFile \"/llava/train/train_mem.py\", line 13, in <module> train() File \"/llava/train/train.py\", line 1155, in train model.get_model().initialize_vision_modules( File \"/llava/model/llava_arch.py\", line 82, in initialize_vision_modules self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector')) File \"/conda/envs/python3.8.13/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 2041, in load_state_dict raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format( RuntimeError: Error(s) in loading state_dict for Sequential: size mismatch for 0.weight: copying a param with shape torch.Size([5120, 1024]) from checkpoint, the shape in current model is torch.Size([0]). size mismatch for 0.bias: copying a param with shape torch.Size([5120]) from checkpoint, the shape in current model is torch.Size([0]). size mismatch for 2.weight: copying a param with shape torch.Size([5120, 5120]) from checkpoint, the shape in current model is torch.Size([0]). size mismatch for 2.bias: copying a param with shape torch.Size([5120]) from checkpoint, the shape in current model is torch.Size([0]).\r\n\r\nI simulation the mm_projector, its shape like:\r\n`\r\nmodules = [nn.Linear(1024, 5120)]\r\nfor _ in range(1, 2):\r\nmodules.append(nn.GELU())\r\nmodules.append(nn.Linear(5120, 5120))\r\nmm_projector = nn.Sequential(*modules)\r\n\r\nSequential(\r\n(0): Linear(in_features=1024, out_features=5120, bias=True)\r\n(1): GELU(approximate='none')\r\n(2): Linear(in_features=5120, out_features=5120, bias=True)\r\n)\r\n`\r\n\r\nand the shape of the weights \"mm_projector.bin\" is :\r\nThe shape of tensor model.mm_projector.0.weight is: torch.Size([5120, 1024]) The shape of tensor model.mm_projector.0.bias is: torch.Size([5120]) The shape of tensor model.mm_projector.2.weight is: torch.Size([5120, 5120]) The shape of tensor model.mm_projector.2.bias is: torch.Size([5120])\r\n\r\nbut it runs as an error, how do you fix it?\r\n\r\n`def build_vision_projector(config, delay_load=False, **kwargs):\r\nprojector_type = getattr(config, 'mm_projector_type', 'linear')\r\n\r\nif projector_type == 'linear':\r\n    return nn.Linear(config.mm_hidden_size, config.hidden_size)\r\n\r\nmlp_gelu_match = re.match(r'^mlp(\\d+)x_gelu$', projector_type)\r\nif mlp_gelu_match:\r\n    mlp_depth = int(mlp_gelu_match.group(1))\r\n    modules = [nn.Linear(config.mm_hidden_size, config.hidden_size)]\r\n    for _ in range(1, mlp_depth):\r\n        modules.append(nn.GELU())\r\n        modules.append(nn.Linear(config.hidden_size, config.hidden_size))\r\n    return nn.Sequential(*modules)\r\n\r\nif projector_type == 'identity':\r\n    return IdentityMap()\r\n\r\nraise ValueError(f'Unknown projector type: {projector_type}')\n</Comment>\n<Comment by unmo at 2023-11-21T03:44:42Z>\nPlease share your scripts on thread.  This may be useful information.\n</Comment>\n<Comment by Byte-Coder2020 at 2023-12-23T13:30:44Z>\nthanks\n</Comment>\n<Comment by yhyang123 at 2024-01-12T06:55:36Z>\n> Hi, please check out the latest code, and the docs for custom data finetuning. Thanks.\r\n> \r\n> https://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md\r\n\r\nI meet the same problem when I try to set my _pretrain_mm_mlp_adapter_ in the finetune step. How can I do if I want to finetune the model from the pretrained checkpoint?\n</Comment>\n<Comment by Ryoo72 at 2024-05-17T05:38:59Z>\n> > Hi, please check out the latest code, and the docs for custom data finetuning. Thanks.\r\n> > https://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md\r\n> \r\n> I meet the same problem when I try to set my _pretrain_mm_mlp_adapter_ in the finetune step. How can I do if I want to finetune the model from the pretrained checkpoint?\r\n\r\ndid you happen to solve the problem?\n</Comment>\n<Comment by fangxin2github at 2024-06-03T06:59:36Z>\n> > Hi, please check out the latest code, and the docs for custom data finetuning. Thanks.\r\n> > https://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md\r\n> \r\n> I meet the same problem when I try to set my _pretrain_mm_mlp_adapter_ in the finetune step. How can I do if I want to finetune the model from the pretrained checkpoint?\r\n\r\ndid you happen to solve the problem?\n</Comment>\n<Comment by nassarofficial at 2024-09-23T11:28:47Z>\nI know it's a bit late, but here's a solution to this problem since I've struggled a long time with this issue. The problem has to do with the order of loading weights. If you load the model using Deepseed and it's wrapped, the size would be 0. I found this function which is from hugging face api apparently, that gathers the parameters from deepspeed and allows you to load your state_dict in there.\r\n\r\nhttps://github.com/alibaba/Pai-Megatron-Patch/blob/daf5c824104261730989e6338b851c8889b9605e/rlhf/deepspeed-chat/utils.py#L134\n</Comment>\n<Comment by HAOYON-666 at 2024-10-15T02:58:57Z>\nI have the same error,please tell me how to deal it,thanks!!!\n</Comment>\n<Comment by HAOYON-666 at 2024-10-15T02:59:07Z>\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/data/workspace/zhaoyong/tools/LLaVA/llava/train/train_xformers.py\", line 13, in <module>\r\n[rank0]:     train()\r\n[rank0]:   File \"/data/workspace/zhaoyong/tools/LLaVA/llava/train/train.py\", line 914, in train\r\n[rank0]:     model.get_model().initialize_vision_modules(\r\n[rank0]:   File \"/data/workspace/zhaoyong/tools/LLaVA/llava/model/llava_arch.py\", line 97, in initialize_vision_modules\r\n[rank0]:     self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'))\r\n[rank0]:   File \"/home/user/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2215, in load_state_dict\r\n[rank0]:     raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\n[rank0]: RuntimeError: Error(s) in loading state_dict for Sequential:\r\n[rank0]:        size mismatch for 0.weight: copying a param with shape torch.Size([4096, 1024]) from checkpoint, the shape in current model is torch.Size([0]).\r\n[rank0]:        size mismatch for 0.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([0]).\r\n[rank0]:        size mismatch for 2.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([0]).\r\n[rank0]:        size mismatch for 2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([0]).\n</Comment>\n<Comment by Ryoo72 at 2024-10-15T07:11:19Z>\n> [rank0]: Traceback (most recent call last): [rank0]: File \"/data/workspace/zhaoyong/tools/LLaVA/llava/train/train_xformers.py\", line 13, in [rank0]: train() [rank0]: File \"/data/workspace/zhaoyong/tools/LLaVA/llava/train/train.py\", line 914, in train [rank0]: model.get_model().initialize_vision_modules( [rank0]: File \"/data/workspace/zhaoyong/tools/LLaVA/llava/model/llava_arch.py\", line 97, in initialize_vision_modules [rank0]: self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector')) [rank0]: File \"/home/user/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2215, in load_state_dict [rank0]: raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format( [rank0]: RuntimeError: Error(s) in loading state_dict for Sequential: [rank0]: size mismatch for 0.weight: copying a param with shape torch.Size([4096, 1024]) from checkpoint, the shape in current model is torch.Size([0]). [rank0]: size mismatch for 0.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([0]). [rank0]: size mismatch for 2.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([0]). [rank0]: size mismatch for 2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([0]).\r\n\r\ndon't use `from_pretrained` !\n</Comment>\n<Comment by HAOYON-666 at 2024-10-15T07:16:21Z>\nI am a beginner，can you tell me how to detailed deal it？thanks!!!\r\n\r\n\r\n\r\n\r\n------------------&nbsp;原始邮件&nbsp;------------------\r\n发件人:                                                                                                                        \"haotian-liu/LLaVA\"                                                                                    ***@***.***&gt;;\r\n发送时间:&nbsp;2024年10月15日(星期二) 下午3:11\r\n***@***.***&gt;;\r\n***@***.******@***.******@***.***&gt;;\r\n主题:&nbsp;Re: [haotian-liu/LLaVA] [Usage] RuntimeError: Error(s) in loading state_dict for Sequential: (Issue #665)\r\n\r\n\r\n\r\n\r\n\r\n  \r\n[rank0]: Traceback (most recent call last): [rank0]: File \"/data/workspace/zhaoyong/tools/LLaVA/llava/train/train_xformers.py\", line 13, in [rank0]: train() [rank0]: File \"/data/workspace/zhaoyong/tools/LLaVA/llava/train/train.py\", line 914, in train [rank0]: model.get_model().initialize_vision_modules( [rank0]: File \"/data/workspace/zhaoyong/tools/LLaVA/llava/model/llava_arch.py\", line 97, in initialize_vision_modules [rank0]: self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector')) [rank0]: File \"/home/user/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2215, in load_state_dict [rank0]: raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format( [rank0]: RuntimeError: Error(s) in loading state_dict for Sequential: [rank0]: size mismatch for 0.weight: copying a param with shape torch.Size([4096, 1024]) from checkpoint, the shape in current model is torch.Size([0]). [rank0]: size mismatch for 0.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([0]). [rank0]: size mismatch for 2.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([0]). [rank0]: size mismatch for 2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([0]).\r\n  \r\ndon't use from_pretrained !\r\n \r\n—\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you are subscribed to this thread.Message ID: ***@***.***&gt;\n</Comment>\n<Comment by KaKa-101 at 2025-01-07T12:15:50Z>\nDoes anybody know how to deal with this problem?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 664,
    "state": "closed",
    "created_by": "unmo",
    "created_at": "2023-10-25T06:07:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/664</URL>\n\n<TITLE>[Usage] AttributeError: 'NoneType' object has no attribute 'image_mean'</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: \r\nI have tried this $ python -m llava.serve.cli  --model-path /app/LLaVA/checkpoints/vicuna1.5-7b_clip-vit-l-patch14 --image-file https://llava-vl.github.io/static/images/view.jpg\r\nBut it doesn't work\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.cli \\\r\n    --model-path /app/LLaVA/checkpoints/vicuna1.5-7b_clip-vit-l-patch14 \\\r\n    --image-file \"https://llava-vl.github.io/static/images/view.jpg\"\r\n```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/app/LLaVA/llava/serve/cli.py\", line 125, in <module>\r\n    main(args)\r\n  File \"/app/LLaVA/llava/serve/cli.py\", line 56, in main\r\n    image_tensor = process_images([image], image_processor, args)\r\n  File \"/app/LLaVA/llava/mm_utils.py\", line 33, in process_images\r\n    image = expand2square(image, tuple(int(x*255) for x in image_processor.image_mean))\r\nAttributeError: 'NoneType' object has no attribute 'image_mean'```\r\n\r\nScreenshots:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/9719347/d19fe062-8859-44be-98f1-2d4d946ad6b3)</BODY>\n\n<COMMENTS>\n<Comment by Adrian-1234 at 2023-10-25T07:07:44Z>\nMe too on the above error when referencing the downloaded model.\r\nWorks ok using the standard --model-path liuhaotian/llava-v1.5-7b invocation\r\n\r\nRunning in a Docker Container.\n</Comment>\n<Comment by skyz8421 at 2023-10-25T07:26:51Z>\n+1, I just met the same problem\n</Comment>\n<Comment by haotian-liu at 2023-10-26T02:56:18Z>\nHi, currently the code recognizes the multimodal by identifying the \"llava\" in the model path. Try renaming it to llava-vicuna-xxxx.\n</Comment>\n<Comment by unmo at 2023-10-26T03:07:18Z>\n@haotian-liu \r\nIt worked. Thank you for your work.\n</Comment>\n<Comment by Adrian-1234 at 2023-10-26T06:26:56Z>\nWorks for me also - many thanks !\n</Comment>\n<Comment by CynthiaChuang at 2024-05-07T03:39:07Z>\n> Hi, currently the code recognizes the multimodal by identifying the \"llava\" in the model path. Try renaming it to llava-vicuna-xxxx.\r\n\r\nIt works for me. Thx.\n</Comment>\n<Comment by yesgvinayak at 2024-05-24T15:55:37Z>\n@haotian-liu  \r\nI am also facing the same issue. But  I am using my fine-tuned model after running merge_lora_weights.py .\r\n\r\ngetting the below error:\r\n\r\n File \"/home/skadmin/cx-research/core/Llava/llava/eval/run_llava.py\", line 117, in eval_model\r\n    images_tensor = process_images(\r\n  File \"/home/skadmin/cx-research/core/Llava/llava/mm_utils.py\", line 171, in process_images\r\n    image = expand2square(image, tuple(int(x*255) for x in image_processor.image_mean))\r\nAttributeError: 'NoneType' object has no attribute 'image_mean'\r\n\r\n\r\nthe code :\r\n\r\nmodel_path = \"./saved_model\"\r\nmodel_name = get_model_name_from_path(model_path)\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(model_path, None, model_name)\n</Comment>\n<Comment by AliMekky at 2024-06-24T19:24:39Z>\n> @haotian-liu I am also facing the same issue. But I am using my fine-tuned model after running merge_lora_weights.py .\r\n> \r\n> getting the below error:\r\n> \r\n> File \"/home/skadmin/cx-research/core/Llava/llava/eval/run_llava.py\", line 117, in eval_model images_tensor = process_images( File \"/home/skadmin/cx-research/core/Llava/llava/mm_utils.py\", line 171, in process_images image = expand2square(image, tuple(int(x*255) for x in image_processor.image_mean)) AttributeError: 'NoneType' object has no attribute 'image_mean'\r\n> \r\n> the code :\r\n> \r\n> model_path = \"./saved_model\" model_name = get_model_name_from_path(model_path) tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, None, model_name)\r\n\r\n+1\n</Comment>\n<Comment by AliMekky at 2024-06-24T20:10:32Z>\n> @haotian-liu I am also facing the same issue. But I am using my fine-tuned model after running merge_lora_weights.py .\r\n> \r\n> getting the below error:\r\n> \r\n> File \"/home/skadmin/cx-research/core/Llava/llava/eval/run_llava.py\", line 117, in eval_model images_tensor = process_images( File \"/home/skadmin/cx-research/core/Llava/llava/mm_utils.py\", line 171, in process_images image = expand2square(image, tuple(int(x*255) for x in image_processor.image_mean)) AttributeError: 'NoneType' object has no attribute 'image_mean'\r\n> \r\n> the code :\r\n> \r\n> model_path = \"./saved_model\" model_name = get_model_name_from_path(model_path) tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, None, model_name)\r\n\r\nThe model path was \"saved_model\" I changed it to \"llava-v1.5-7b-quilt-merged_new\" and it worked\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 663,
    "state": "closed",
    "created_by": "gemcollector",
    "created_at": "2023-10-25T04:19:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/663</URL>\n\n<TITLE>[Usage] Cannot load the pretrained weight.</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nHi, nice work! But I have met an issue of trying the following command when loading the pre-trained weights. \r\n\r\n\r\nCommand:\r\npython -m llava.serve.cli     --model-path ./pre_trained_models/llava1.5/llava-v1.5-13b/     --image-file \"https://llava-vl.github.io/static/images/view.jpg\"     --load-8bit\r\n\r\nLog: \r\n```\r\n[2023-10-25 12:13:37,129] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_acc[27/2031]\r\nto cuda (auto detect)\r\nmodel_name: llava-v1.5-13b\r\nyes!\r\nLoading checkpoint shards:   0%|                                            | 0/3 [00:00<?, ?it/s]\r\nTraceback (most recent call last):\r\n  File \"/home/yzc/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.p\r\ny\", line 460, in load_state_dict\r\n    return torch.load(checkpoint_file, map_location=\"cpu\")\r\n  File \"/home/yzc/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py\", line\r\n 797, in load\r\n    with _open_zipfile_reader(opened_file) as opened_zipfile:\r\n  File \"/home/yzc/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py\", line\r\n 283, in __init__\r\n    super().__init__(torch._C.PyTorchFileReader(name_or_buffer))\r\nRuntimeError: PytorchStreamReader failed reading zip archive: failed finding central directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/yzc/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.p\r\ny\", line 464, in load_state_dict\r\n    if f.read(7) == \"version\":\r\n  File \"/home/yzc/miniconda3/envs/llava/lib/python3.10/codecs.py\", line 322, in decode\r\n    (result, consumed) = self._buffer_decode(data, self.errors, final)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 128: invalid start byte\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/yzc/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/yzc/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/yzc/shared/project/LLaVA/llava/serve/cli.py\", line 126, in <module>\r\n    main(args)\r\n  File \"/home/yzc/shared/project/LLaVA/llava/serve/cli.py\", line 33, in main\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.m\r\nodel_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n  File \"/home/yzc/shared/project/LLaVA/llava/model/builder.py\", line 104, in load_pretrained_model\r\n    model = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\n  File \"/home/yzc/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.p\r\ny\", line 2903, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\n  File \"/home/yzc/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.p\r\ny\", line 3246, in _load_pretrained_model\r\n    state_dict = load_state_dict(shard_file)\r\n  File \"/home/yzc/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.p\r\ny\", line 476, in load_state_dict\r\n    raise OSError(\r\nOSError: Unable to load weights from pytorch checkpoint file for './pre_trained_models/llava1.5/ll\r\nava-v1.5-13b/pytorch_model-00001-of-00003.bin' at './pre_trained_models/llava1.5/llava-v1.5-13b/py\r\ntorch_model-00001-of-00003.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, pl\r\nease set from_tf=True.\r\n\r\n```</BODY>\n\n<COMMENTS>\n<Comment by White1973 at 2023-10-26T14:40:22Z>\nI updated PyTorch version to 2.0.1, and the issue was solved.\n</Comment>\n<Comment by gemcollector at 2023-10-28T06:25:46Z>\nI think this issue mainly caused by unsuccessful downloading of the whole model. Therefore, it cannot load the model. I fix this bug by re-downloading the whole model.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 662,
    "state": "open",
    "created_by": "aprilehannibal",
    "created_at": "2023-10-25T03:52:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/662</URL>\n\n<TITLE>[Question] What are the differences between two versions of pretrain datasets?</TITLE>\n\n<BODY>### Question\n\nGreat job! I found there are two versions pretrain datasets： blip_laion_cc_sbu_558k and LLaVA-CC3M-Pretrain-595K. I'd like to know what are the differences between them and which one is better. Did you analyze the quality of these datasets and why they have different performance? Hope for your reply. Thanks a lot! @haotian-liu</BODY>\n\n<COMMENTS>\n<Comment by Hambaobao at 2023-12-26T08:29:44Z>\nI also want to know the difference between the two of them, have you figure it out?\n</Comment>\n<Comment by whwangovo at 2024-07-26T11:27:38Z>\nsame :( seens it still not be sloved\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 661,
    "state": "open",
    "created_by": "Liu0329",
    "created_at": "2023-10-25T03:42:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/661</URL>\n\n<TITLE>[Usage] tokenization mismatch when finetuning v1.5-7b</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI have found some threads reporting the tokenization mismatch problem, but I am still confused. I download the v1.5-7b weight from [https://huggingface.co/liuhaotian/llava-v1.5-7b/tree/main](url)\r\n, and finetune on datasets in the paper. I adapt the command line to make it run on V100.\r\ntokenizers.__version__ == '0.14.1'\r\n\r\nCommand:\r\n```\r\nWANDB_MODE=disabled deepspeed llava/train/train.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path /path/to/llm_weights/llava-v1.5-7b \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/llava_v1_5_mix665k.json \\\r\n    --image_folder ./playground/data \\\r\n    --vision_tower /path/to/llm_weights/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter /path/to/llm_weights/llava-v1.5-7b/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 False \\\r\n    --fp16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 2 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 False \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n```\r\n\r\nScreenshots:\r\n![企业微信截图_16982049679152](https://github.com/haotian-liu/LLaVA/assets/8122099/57c3c2f1-9168-4bbf-aa69-7f0754f513bb)</BODY>\n\n<COMMENTS>\n<Comment by yuyq96 at 2023-10-25T04:12:34Z>\nSame problem, I found `</s>` was not included in calculating the round_len, since it is used to split the rounds. Might be a problem that eos token is not automatically added?\n</Comment>\n<Comment by yuyq96 at 2023-10-25T06:56:09Z>\n> Same problem, I found `</s>` was not included in calculating the round_len, since it is used to split the rounds. Might be a problem that eos token is not automatically added?\r\n\r\nThe truth is \"USER\" will be tokenized as [11889] in the middle of the prompt, but tokenized as [1, 3148, 1001] in the head (with an automatically added bos token)\r\n\r\nI tried to fix this WARNING by:\r\n\r\n```python\r\ncur_len = 1 + 1  # 1 for bos, and 1 for compensating in the first round\r\n...\r\nround_len = len(tokenizer_image_token(rou, tokenizer)) - 2 + 1  # -2 for the extra tokens in tokenizing \"USER\", +1 for the missing \"</s>\"\r\n...\r\nround_len = len(tokenizer(rou).input_ids) - 2 + 1\r\n```\n</Comment>\n<Comment by Liu0329 at 2023-10-25T07:00:41Z>\n> Same problem, I found `</s>` was not included in calculating the round_len, since it is used to split the rounds. Might be a problem that eos token is not automatically added?\r\n\r\nI did some debug. In this case, 136 is the input_ids' actual length, and there are three 2 in input_ids, which should be `</s>`. So what does 138 mean ? two more `</s>` should be added ?\r\n![企业微信截图_1698216809301](https://github.com/haotian-liu/LLaVA/assets/8122099/385703cc-ecdc-46bc-a270-767cc2784e17)\n</Comment>\n<Comment by yuyq96 at 2023-10-25T07:21:39Z>\n> > Same problem, I found `</s>` was not included in calculating the round_len, since it is used to split the rounds. Might be a problem that eos token is not automatically added?\r\n> \r\n> I did some debug. In this case, 136 is the input_ids' actual length, and there are three 2 in input_ids, which should be `</s>`. So what does 138 mean ? two more `</s>` should be added ? ![企业微信截图_1698216809301](https://user-images.githubusercontent.com/8122099/277899989-385703cc-ecdc-46bc-a270-767cc2784e17.png)\r\n\r\nYou can check my last modification, the mismatch is due to the different tokenization results of 'USER' and the missing `</s>`.\n</Comment>\n<Comment by Liu0329 at 2023-10-25T07:28:13Z>\n> > Same problem, I found `</s>` was not included in calculating the round_len, since it is used to split the rounds. Might be a problem that eos token is not automatically added?\r\n> \r\n> The truth is \"USER\" will be tokenized as [11889] in the middle of the prompt, but tokenized as [1, 3148, 1001] in the head (with an automatically added bos token)\r\n> \r\n> I tried to fix this WARNING by:\r\n> \r\n> ```python\r\n> cur_len = 1 + 1  # 1 for bos, and 1 for compensating in the first round\r\n> ...\r\n> round_len = len(tokenizer_image_token(rou, tokenizer)) - 2 + 1  # -2 for the extra tokens in tokenizing \"USER\", +1 for the missing \"</s>\"\r\n> ...\r\n> round_len = len(tokenizer(rou).input_ids) - 2 + 1\r\n> ```\r\n\r\nAwesome ! I tested your change, and it did work. So the problem is caused by both USER and `</s>`. To clarify, the change should be made in method preprocess_v1 in llava/train/train.py. \r\n@haotian-liu Please have a double check.\n</Comment>\n<Comment by Liu0329 at 2023-10-25T07:53:03Z>\n@yuyq96 \"The truth is \"USER\" will be tokenized as [11889] in the middle of the prompt, but tokenized as [1, 3148, 1001] in the head\".\r\nSo it seems the problem is caused by missing space before and after `</s>` ?\r\n![企业微信截图_16982203487382](https://github.com/haotian-liu/LLaVA/assets/8122099/82503213-b720-45a8-86f1-c7d586ddbc3f)\n</Comment>\n<Comment by yuyq96 at 2023-10-25T07:59:46Z>\n> @yuyq96 \"The truth is \"USER\" will be tokenized as [11889] in the middle of the prompt, but tokenized as [1, 3148, 1001] in the head\". So it seems the problem is caused by missing space before and after `</s>` ? ![企业微信截图_16982203487382](https://user-images.githubusercontent.com/8122099/277915136-82503213-b720-45a8-86f1-c7d586ddbc3f.png)\r\n\r\nYes, this will lead to different tokenization results with LLaMA tokenizer.\n</Comment>\n<Comment by Liu0329 at 2023-10-25T09:24:37Z>\n> > @yuyq96 \"The truth is \"USER\" will be tokenized as [11889] in the middle of the prompt, but tokenized as [1, 3148, 1001] in the head\". So it seems the problem is caused by missing space before and after `</s>` ? ![企业微信截图_16982203487382](https://user-images.githubusercontent.com/8122099/277915136-82503213-b720-45a8-86f1-c7d586ddbc3f.png)\r\n> \r\n> Yes, this will lead to different tokenization results with LLaMA tokenizer.\r\n\r\nFor the above case, can the tokenizer correctly separate No (or other words) before `</s>`? If not, the training would be harmed. So the better solution should be to modify the prompt.\r\nAnd I tried to insert space before and after `</s>`, but the mismatch showed again with original code.\n</Comment>\n<Comment by haotian-liu at 2023-10-25T17:00:09Z>\nHi, we have just set temporarily the tokenizer version to be \"tokenizers>=0.12.1,<0.14\" until we figure out what has changed in 0.14.\r\n\r\nYou may run `pip install \"tokenizers>=0.12.1,<0.14\"`, and try again. Thanks.\n</Comment>\n<Comment by haotian-liu at 2023-10-25T17:01:36Z>\n@yuyq96  Thanks for the fix, I'll take a look into this issue. This fix may cause issue with earlier tokenizer versions? I feel that there were some behavioral changes of the tokenizer.\n</Comment>\n<Comment by Liu0329 at 2023-10-26T03:19:33Z>\n> Hi, we have just set temporarily the tokenizer version to be \"tokenizers>=0.12.1,<0.14\" until we figure out what has changed in 0.14.\r\n> \r\n> You may run `pip install \"tokenizers>=0.12.1,<0.14\"`, and try again. Thanks.\r\n\r\nThanks, tokenizers downgrading to 0.12.1, and transformers to 4.31.0 solved the problem. I also tried inserting spaces before and after `</s>`, and the warning showed again, don't know why extra spaces would not do.\n</Comment>\n<Comment by zzzzzzrc at 2023-11-01T14:41:01Z>\n@haotian-liu In my experiment, tokenizer set \"use_fast=True\" works , with transformers==4.34.1 and tokenizers==0.14.1.\r\nBut don't know why mismatch when set \"use_fast=False\"\n</Comment>\n<Comment by GuoQiushan at 2023-11-07T08:33:07Z>\n> @haotian-liu In my experiment, tokenizer set \"use_fast=True\" works , with transformers==4.34.1 and tokenizers==0.14.1. But don't know why mismatch when set \"use_fast=False\"\r\n\r\n@zzzzzzrc I tried to set \"use_fast=True\" and it works. But I'm not sure whether it will affect the final performance or not. Do you have any suggestion?\n</Comment>\n<Comment by xiechengmude at 2023-11-18T07:19:52Z>\nIs here fixed?\n</Comment>\n<Comment by GuoQiushan at 2023-11-19T08:42:29Z>\n> Is here fixed?\r\n\r\nSetting use_fast=True works for my case.\n</Comment>\n<Comment by ryusaeba at 2023-12-01T01:23:26Z>\n@haotian-liu \r\nThis is similar issue as what FastChat meet. The root cause is Huggingface introduce some bugs when dealing with added tokens. Please refer the fix [here](https://github.com/lm-sys/FastChat/pull/2498).\n</Comment>\n<Comment by liuhaogeng at 2023-12-03T04:39:19Z>\nround_len = len(tokenizer(rou).input_ids)， for each round，the tokenizer will add \"bos\"(bos of vicuna), so i wonder if the round_len caculation is right? Thanks\n</Comment>\n<Comment by xxxwuwq at 2024-02-02T02:09:14Z>\nI encountered the “tokenization mismatch” issue during fine-tuning as well. Upon investigation, I found that it was primarily caused by the presence of empty strings in the “value” field of  QA {\"from\": \"human\", \"value\": \"\"} int the dataset. As a result, the prompt became inclusive of the string “xxx USER:ASSISTANT: xxxx”. This led to the “tokenization mismatch” issue during the tokenization process. I’m not sure if this experience is useful, but I thought I’d share it.\n</Comment>\n<Comment by lucasjinreal at 2024-02-20T03:33:02Z>\nHi, I am training LLava with Qwen2 got same mismatch.\r\nSettting use_fast=True not work.\r\n\r\nAm just wondering will it effect the training? How to fix it for numerous tokenizers not just for llama?\n</Comment>\n<Comment by 20191864218 at 2024-02-22T17:43:36Z>\n> Hi, I am training LLava with Qwen2 got same mismatch. Settting use_fast=True not work.\r\n> \r\n> Am just wondering will it effect the training? How to fix it for numerous tokenizers not just for llama?\r\n\r\nHi, I have the same issue. Have you solved it?\n</Comment>\n<Comment by charismaticchiu at 2024-02-23T21:50:00Z>\n> > Hi, I am training LLava with Qwen2 got same mismatch. Settting use_fast=True not work.\r\n> > Am just wondering will it effect the training? How to fix it for numerous tokenizers not just for llama?\r\n> \r\n> Hi, I have the same issue. Have you solved it?\r\n\r\nSame when using lora to finetune v1.6-34b\n</Comment>\n<Comment by lucasjinreal at 2024-02-25T06:10:13Z>\nI have fixed the issue, You just need to make sure the inputs and targets properly masked.\n</Comment>\n<Comment by BlueBlueFF at 2024-02-26T16:01:33Z>\n> I have fixed the issue, You just need to make sure the inputs and targets properly masked.\r\n\r\nCan you share your tokenizer settings?\n</Comment>\n<Comment by gujiaqivadin at 2024-04-15T02:47:06Z>\n> > > Hi, I am training LLava with Qwen2 got same mismatch. Settting use_fast=True not work.\r\n> > > Am just wondering will it effect the training? How to fix it for numerous tokenizers not just for llama?\r\n> > \r\n> > \r\n> > Hi, I have the same issue. Have you solved it?\r\n> \r\n> Same when using lora to finetune v1.6-34b\r\n\r\nsame when finetuning in 1.5b\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 660,
    "state": "closed",
    "created_by": "yuyq96",
    "created_at": "2023-10-25T01:50:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/660</URL>\n\n<TITLE>[Question] The training logs of LLaVA-1.5 7B model</TITLE>\n\n<BODY>### Question\n\n#613 Thank you for making the training logs of the 13B model publicly available. We are trying to reproduce the results of the 7B model on V100 GPUs, so it would be helpful if you could also make the training logs of the 7B model publicly available.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-26T19:40:54Z>\nHi, the logs are updated. By default it still shows the 13B, and please toggle the experiment of your interest to view the logs.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 659,
    "state": "closed",
    "created_by": "unmo",
    "created_at": "2023-10-25T01:41:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/659</URL>\n\n<TITLE>[Question]  LLaVA1.5 model license</TITLE>\n\n<BODY>### Question\r\n\r\nThe readme contains the following information\r\n\r\n_**Usage and License Notices: The data and checkpoint is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of LLaMA, Vicuna and GPT-4. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.**_\r\n\r\nIs this model not commercially available? Or is this explanation wrong?\r\nI  think this model conform to the llama2 license.</BODY>\n\n<COMMENTS>\n<Comment by papasanimohansrinivas at 2023-10-26T12:08:29Z>\nLooking for the same ,.we also @unmo\n</Comment>\n<Comment by haotian-liu at 2023-10-26T17:55:42Z>\nLLaVA-1.5 (Vicuna-1.5 based) and LLaVA (LLaMA-2 based) are both licensed under LLaMA-2 community license, as indicated in the specific MODEL CARD on HF. Thanks.\n</Comment>\n<Comment by papasanimohansrinivas at 2023-10-26T20:04:57Z>\nhi @haotian-liu you are referring to models of LLaVA-1.5 (Vicuna-1.5 based) and LLaVA (LLaMA-2 based)  as llama2 license right to be specific , and by that logic models of LLaVA-1.5 (Vicuna-1.5 based) and LLaVA (LLaMA-2 based)   are commercial applicable under llama2 terms\n</Comment>\n<Comment by ChintanShahDS at 2024-01-10T03:56:30Z>\n@haotian-liu Not clear from your last post. It would be great if you can confirm LLaVA-1.5 (Vicuna-1.5 based) and LLaVA (LLaMA-2 based) can be used commercially or not.\n</Comment>\n<Comment by huvers at 2024-01-26T22:17:51Z>\nThe dataset stack, which includes COCO, contains many non-commercial licensed images, even if the annotations are labeled as MIT/Apache 2.0. \r\n\r\nNo serious legal team would look at the underlying training set and say this is commercially usable.\n</Comment>\n<Comment by PaulFidika at 2024-01-29T22:28:37Z>\nhttps://github.com/haotian-liu/LLaVA/commit/ba72f82cc610b01dc27764b483dfe982948b0633\r\n\r\nThe terms in the README were updated on December 22nd, and, in my opinion, now permit commercial usage. The previous releases of this dataset had a non-commercial CC license imposed by the creators, but this revision removed that restriction.\r\n\r\nSo the remaining stipulations would be around (1) LLama 2 community license, which allows for commercial usage, (2) Vicuna v1.5 allows for commercial usage (previous iterations did not), (3) OpenAI's nonsensical non-competitive-usage clause for GPT-4 (i.e., GPT-4 outputs supposed cannot be used to train competing models, although OpenAI uses your inputs and the entire internet to train their models?), could be problematic. I believe CLIP and ViT-L14 are MIT licensed (anything goes), from back when OpenAI was open (pre-2023).\r\n\r\nAs to the question of ownership of the images themselves; idk.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 658,
    "state": "closed",
    "created_by": "Victorwz",
    "created_at": "2023-10-25T00:16:14Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/658</URL>\n\n<TITLE>[Question] What is llava_instruct_80k data?</TITLE>\n\n<BODY>### Question\n\nHi Haotian,\r\n\r\nThanks for the great work! I found in the ```scripts/fine_tune.sh``` file, your input parameter for for \"data_path\" is \"./playground/data/llava_instruct_80k.json\". I also found that you release such 80k data on your huggingface datasets repo. \r\n\r\nHowever, I did not found how you get such 80k samples from your 150k sample pool in your paper or other documents.\r\n\r\nCan you help clarify how you downsample and get such 80k data? Thank you!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-04T23:47:17Z>\nHi, we construct the 80k instruction data by subsampling the LLaVA-Instruct-150K:\r\n- Find the overlapping and non-overlapping images from the complex reasoning questions and conversation questions.\r\n- Add non-overlapping image-instruction pairs to each category (reason and conv)\r\n- Randomly sample from the overlapping set to make them to have 40K each.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 657,
    "state": "closed",
    "created_by": "HenryHZY",
    "created_at": "2023-10-24T21:35:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/657</URL>\n\n<TITLE>[Question] A simple inquiry on the statistics of llava_v1_5_mix665k.json</TITLE>\n\n<BODY>### Question\r\n\r\nHi @haotian-liu, thanks for your great project.\r\n\r\nAs mentioned in the paper, the statistics of llava_v1_5_mix665k.json is shown as Table 7:\r\n<img width=\"500\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/43438692/690e5145-7007-4042-9899-55afc2dcad5e\">\r\n\r\nThat is, 158+40+83+72+9+80+50+22+30+86=630K. There seems to be a missing 35K dataset. Did I miss something? \r\n\r\nTo make it more clear, here is the statistics of llava_v1_5_mix665k.json using the key 'image':\r\n```\r\n'coco': 364100\r\n'vg': 86417\r\n'gqa': 72140\r\n'ocr_vqa': 80000\r\n'textvqa': 21953\r\n'text-only': 40688 (without key 'image')\r\ntotal: 665298\r\n```\r\n\r\nFor datasets in Table 7 using 'coco' images: \r\n```\r\nllava: 158K\r\nvqav2: 83K\r\nok-vqa: 9K\r\na-ok-vqa: 50K\r\nrefcoco: 30K\r\n```\r\n\r\nIt seems that there is a missing 35K dataset in the 'coco'.</BODY>\n\n<COMMENTS>\n<Comment by Victorwz at 2023-10-25T18:12:00Z>\nHi Ziyuan,\r\n\r\n> ### Question\r\n> Hi @haotian-liu, thanks for your great project.\r\n> \r\n> As mentioned in the paper, the statistics of llava_v1_5_mix665k.json is shown as Table 7: <img alt=\"image\" width=\"500\" src=\"https://user-images.githubusercontent.com/43438692/277809774-690e5145-7007-4042-9899-55afc2dcad5e.png\">\r\n> \r\n> That is, 158+40+83+72+9+80+50+22+30+86=630K. There seems to be a missing 35K dataset. Did I miss something?\r\n> \r\n> To make it more clear, here is the statistics of llava_v1_5_mix665k.json using the key 'image':\r\n> \r\n> ```\r\n> 'coco': 364100\r\n> 'vg': 86417\r\n> 'gqa': 72140\r\n> 'ocr_vqa': 80000\r\n> 'textvqa': 21953\r\n> 'text-only': 40688 (without key 'image')\r\n> total: 665298\r\n> ```\r\n> \r\n> For datasets in Table 7 using 'coco' images:\r\n> \r\n> ```\r\n> llava: 158K\r\n> vqav2: 83K\r\n> ok-vqa: 9K\r\n> a-ok-vqa: 50K\r\n> refcoco: 30K\r\n> ```\r\n> \r\n> It seems that there is a missing 35K dataset in the 'coco'.\r\n\r\nHi Ziyuan, I also have the same question with you. I try to split all the subset based on the prompting formats.\r\n\r\nHere is what I get:\r\n\r\n``a_okvqa_66k.json \r\ngqa_72k.json  \r\nocr_vqa_80k.json  \r\nrefcoco_48k.json  \r\nShareGPT_40k.json  \r\ntextvqa_21k.json  \r\nvg_86k.json  \r\nvqav2_okvqa_91k.json``\r\n\r\nI think the major difference to the original paper is that the a_okvqa and refcoco subset are much larger than the number claimed in the paper. The a_okvqa is actually 66k, bringing another 16k. The refcoco is actually 48k, birnging another 18k. Therefore, there is about 34k additional data in the 665k mixture.\r\n\r\nAdditionally, I am also working on a project regarding high-quality data filtering on the 665k instruction tuning data for LLaVA. If you are also working on the same topic, we can discuss and work togerther.\n</Comment>\n<Comment by simplelifetime at 2023-11-01T03:57:11Z>\n> Hi Ziyuan,\r\n> \r\n> > ### Question\r\n> > Hi @haotian-liu, thanks for your great project.\r\n> > As mentioned in the paper, the statistics of llava_v1_5_mix665k.json is shown as Table 7: <img alt=\"image\" width=\"500\" src=\"https://user-images.githubusercontent.com/43438692/277809774-690e5145-7007-4042-9899-55afc2dcad5e.png\">\r\n> > That is, 158+40+83+72+9+80+50+22+30+86=630K. There seems to be a missing 35K dataset. Did I miss something?\r\n> > To make it more clear, here is the statistics of llava_v1_5_mix665k.json using the key 'image':\r\n> > ```\r\n> > 'coco': 364100\r\n> > 'vg': 86417\r\n> > 'gqa': 72140\r\n> > 'ocr_vqa': 80000\r\n> > 'textvqa': 21953\r\n> > 'text-only': 40688 (without key 'image')\r\n> > total: 665298\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > For datasets in Table 7 using 'coco' images:\r\n> > ```\r\n> > llava: 158K\r\n> > vqav2: 83K\r\n> > ok-vqa: 9K\r\n> > a-ok-vqa: 50K\r\n> > refcoco: 30K\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > It seems that there is a missing 35K dataset in the 'coco'.\r\n> \r\n> Hi Ziyuan, I also have the same question with you. I try to split all the subset based on the prompting formats.\r\n> \r\n> Here is what I get:\r\n> \r\n> `a_okvqa_66k.json gqa_72k.json ocr_vqa_80k.json refcoco_48k.json ShareGPT_40k.json textvqa_21k.json vg_86k.json vqav2_okvqa_91k.json`\r\n> \r\n> I think the major difference to the original paper is that the a_okvqa and refcoco subset are much larger than the number claimed in the paper. The a_okvqa is actually 66k, bringing another 16k. The refcoco is actually 48k, birnging another 18k. Therefore, there is about 34k additional data in the 665k mixture.\r\n> \r\n> Additionally, I am also working on a project regarding high-quality data filtering on the 665k instruction tuning data for LLaVA. If you are also working on the same topic, we can discuss and work togerther.\r\n\r\nHello Victorwz, thanks for your work. Can you share your index list which splits the datasets apart?\n</Comment>\n<Comment by haotian-liu at 2023-11-04T21:00:22Z>\nApologies for the confusion. I just re-calculated the exact samples in the dataset mixture, and we will update the paper to correct the sample count for RefCOCO and A-OKVQA. Note that the released dataset is correct, only the number reported in the table is off for these two datasets.\r\n\r\n| Dataset       | Actual  | Paper |\r\n|---------------|--------:|------:|\r\n| LLaVA |  157712 |  158K |\r\n| SG40k         |   40688 |   40K |\r\n| VQA-v2        |   82783 |   83K |\r\n| GQA           |   72140 |   72K |\r\n| OKVQA         |    8998 |    9K |\r\n| OCRVQA       |   80000 |   80K |\r\n| A-OKVQA       |   **66160** |   ~~50K~~ **66K** |\r\n| TextCaps      |   21953 |   22K |\r\n| RefCOCO       |   **48447** |   ~~30K~~ **48K** |\r\n| VG            |   86417 |   86K |\r\n| **Total**     | **665298** | **665K** |\n</Comment>\n<Comment by HenryHZY at 2023-11-06T03:40:54Z>\n> Apologies for the confusion. I just re-calculated the exact samples in the dataset mixture, and we will update the paper to correct the sample count for RefCOCO and A-OKVQA. Note that the released dataset is correct, only the number reported in the table is off for these two datasets.\r\n> \r\n> Dataset\tActual\tPaper\r\n> LLaVA\t157712\t158K\r\n> SG40k\t40688\t40K\r\n> VQA-v2\t82783\t83K\r\n> GQA\t72140\t72K\r\n> OKVQA\t8998\t9K\r\n> OCRVQA\t80000\t80K\r\n> A-OKVQA\t**66160**\t~50K~ **66K**\r\n> TextCaps\t21953\t22K\r\n> RefCOCO\t**48447**\t~30K~ **48K**\r\n> VG\t86417\t86K\r\n> **Total**\t**665298**\t**665K**\r\n\r\nThanks for your reply! These specific numbers also facilitate a better understanding of the composition of the dataset.\n</Comment>\n<Comment by HenryHZY at 2023-11-06T03:50:30Z>\n@Victorwz Hi, Weizhi. Thanks for your accurate and detailed reply. And sorry for the late reply as I haven't been following the github issue recently.\r\nConduct high-quality data filtering on the instruction tuning data is a great idea. Although I am not working on this topic, I think instruction tuning is really significant. \r\nI have reproduced both the pretraining and finetuning stages, LLaVA1.5 without instruction tuning performs poorly in traditional downstream tasks. \r\nI think we can also have a discussion about it when available:)\n</Comment>\n<Comment by ppalantir at 2024-08-15T07:06:38Z>\n> ### Question\r\n> Hi @haotian-liu, thanks for your great project.\r\n> \r\n> As mentioned in the paper, the statistics of llava_v1_5_mix665k.json is shown as Table 7: <img alt=\"image\" width=\"500\" src=\"https://private-user-images.githubusercontent.com/43438692/277809774-690e5145-7007-4042-9899-55afc2dcad5e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjM2ODk0NzIsIm5iZiI6MTcyMzY4OTE3MiwicGF0aCI6Ii80MzQzODY5Mi8yNzc4MDk3NzQtNjkwZTUxNDUtNzAwNy00MDQyLTk4OTktNTVhZmMyZGNhZDVlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA4MTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwODE1VDAyMzI1MlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWE1M2UxODZhNmYxOWE3NWVlNDM5MDc1MDcxZTYyODViY2Y2ODljOWMxMDNkNDBkZDIwZDdjZDk2YzhhZWY1NDEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.TRCAyciuORUDyyqTc59XFZwA3SkUTxXy8ugJj__VBMc\">\r\n> \r\n> That is, 158+40+83+72+9+80+50+22+30+86=630K. There seems to be a missing 35K dataset. Did I miss something?\r\n> \r\n> To make it more clear, here is the statistics of llava_v1_5_mix665k.json using the key 'image':\r\n> \r\n> ```\r\n> 'coco': 364100\r\n> 'vg': 86417\r\n> 'gqa': 72140\r\n> 'ocr_vqa': 80000\r\n> 'textvqa': 21953\r\n> 'text-only': 40688 (without key 'image')\r\n> total: 665298\r\n> ```\r\n> \r\n> For datasets in Table 7 using 'coco' images:\r\n> \r\n> ```\r\n> llava: 158K\r\n> vqav2: 83K\r\n> ok-vqa: 9K\r\n> a-ok-vqa: 50K\r\n> refcoco: 30K\r\n> ```\r\n> \r\n> It seems that there is a missing 35K dataset in the 'coco'.\r\n\r\n@haotian-liu it's strange that there are only 608081 images (not 665K, not 630K) in total following dataset  preparation in readme.\r\n\r\n<img width=\"449\" alt=\"image\" src=\"https://github.com/user-attachments/assets/8a233d79-7be3-4c9d-9191-e0dd931652f3\">\n</Comment>\n<Comment by hunarbatra at 2024-09-21T19:23:35Z>\n> @haotian-liu it's strange that there are only 608081 images (not 665K, not 630K) in total following dataset preparation in readme.\r\n\r\nI'm getting the same counts as @ppalantir (608k and not 665k) for the instruction tuning dataset following the dataset preparation steps in the readme file. @haotian-liu can you please check on this? Thanks!\r\n\r\nUpdate: 40,688 are text only from ShareGPT, which makes the total count 648k now. I'm guessing the rest is just some repeated images likely.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 656,
    "state": "open",
    "created_by": "stablegpt5279",
    "created_at": "2023-10-24T13:47:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/656</URL>\n\n<TITLE>use model as an API [Feature request]</TITLE>\n\n<BODY>### feature\n\nhow to use the model as an API for production applications??</BODY>\n\n<COMMENTS>\n<Comment by xts-bit at 2023-10-24T15:04:26Z>\n@haotian-liu Same any solutions?\n</Comment>\n<Comment by hszhoushen at 2023-11-03T08:18:02Z>\n> ### feature\r\n> how to use the model as an API for production applications??\r\n\r\nI have a solution, add the code with flask (a web application package), so that the LLaVa model can be a server that can receive the post from any client, each time the model receives the post, it will process the image, and send back the language description to the client.\n</Comment>\n<Comment by hszhoushen at 2023-11-03T08:18:48Z>\n> @haotian-liu Same any solutions?\r\n\r\nI have a solution, add the code with flask (a web application package), so that the LLaVa model can be a server that can receive the post from any client, each time the model receives the post, it will process the image, and send back the language description to the client.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 655,
    "state": "open",
    "created_by": "CHENGY12",
    "created_at": "2023-10-24T11:10:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/655</URL>\n\n<TITLE>[Usage] OOM  for 7B LLAVA v1.5 finetuning 8xA100(40G)</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI tried to use 2 nodes with 8xA100(40G)s to finetune the 7B models. But it still reports OOM error.  \r\n\r\nCommand:\r\n```\r\nsrun --gres=gpu:4 --ntasks-per-node=1 --nodes=2 --cpus-per-task=128 --mem=160G deepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --model_name_or_path lmsys/vicuna-7b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/llava_v1_5_mix665k.json \\\r\n    --image_folder ./playground/data \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-7b-pretrain/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b-finetune \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 16 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 10000 \\\r\n    --save_total_limit 10 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```</BODY>\n\n<COMMENTS>\n<Comment by Rickylht at 2023-10-25T06:10:07Z>\ntry zero3+gradient_accumulation_steps = 1\n</Comment>\n<Comment by haotian-liu at 2023-10-26T02:55:09Z>\nIf you have 8x A100 (40G), you should be able to run it with batch size 16, gradient accu 1, under zero3.json\n</Comment>\n<Comment by microhu at 2023-10-26T09:17:39Z>\n> If you have 8x A100 (40G), you should be able to run it with batch size 16, gradient accu 1, under zero3.json\r\n\r\nhow to estimate the required memory and. better set the batchhsize? I changed the batchsize from 1 to 16, the gpu memory didn't changed too much when gradient_checkpoint is enabled in finetuning stage.\n</Comment>\n<Comment by CHENGY12 at 2023-10-26T09:35:26Z>\nI have tried for zero3 too.  It still shows the OOM issue. We used \r\n```\r\n  --per_device_train_batch_size 1 \r\n  --per_device_eval_batch_size 1 \r\n  --gradient_accumulation_steps 16 \r\n    \r\n  --per_device_train_batch_size 16 \r\n  --per_device_eval_batch_size 1 \r\n  --gradient_accumulation_steps 1 \r\n  ```  \r\nIt doesn't work.  I will check with\r\n\r\n   ``` \r\n  --per_device_train_batch_size 1 \r\n  --per_device_eval_batch_size 1\r\n  --gradient_accumulation_steps 1 \r\n```\n</Comment>\n<Comment by haotian-liu at 2023-10-26T20:58:04Z>\nHi @CHENGY12\r\n\r\nWe just released LoRA finetuning scripts and checkpoints, and with LoRA, it should be able to enough to fit even 13B training into 8-A100-40g.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/releases/tag/v1.1.3\r\n\r\nLet me know if you still have any issues.\n</Comment>\n<Comment by CHENGY12 at 2023-10-27T11:16:43Z>\nThanks for your kind updates.  I will try the LORA!\n</Comment>\n<Comment by LumenYoung at 2023-10-28T20:37:12Z>\n> Hi @CHENGY12\r\n> \r\n> We just released LoRA finetuning scripts and checkpoints, and with LoRA, it should be able to enough to fit even 13B training into 8-A100-40g.\r\n> \r\n> https://github.com/haotian-liu/LLaVA/releases/tag/v1.1.3\r\n> \r\n> Let me know if you still have any issues.\r\n\r\nDear Haotian Liu,\r\n\r\nThanks for the lora checkpoint and I have a question regarding the computation resource for finetuning lora models. I currently have 24 GBs GPU vram. It is possible to use such resource to finetune 13B model with smaller batchsizes?\r\n\r\nThanks in advance for explaining.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 654,
    "state": "closed",
    "created_by": "buaachen1993",
    "created_at": "2023-10-24T08:15:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/654</URL>\n\n<TITLE>[Usage] finetune exits with return code = -8</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI run finetune.sh on 4*A100-80G，got error  exits with return code = -8. Anyone Know why?\r\n\r\nCommand:\r\n\r\nmy finetune.sh \r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path /mnt/dolphinfs/ssd_pool/docker/user/hadoop-mapcv/chenxingyue02/data/mllm/vicuna/lmsys/vicuna-13b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path /mnt/dolphinfs/ssd_pool/docker/user/hadoop-mapcv/chenxingyue02/data/mllm/llava/train_json/special_train.json \\\r\n    --image_folder / \\\r\n    --vision_tower /mnt/dolphinfs/ssd_pool/docker/user/hadoop-mapcv/chenxingyue02/data/mllm/clip \\\r\n    --pretrain_mm_mlp_adapter /mnt/dolphinfs/ssd_pool/docker/user/hadoop-mapcv/chenxingyue02/data/mllm/llava/pretrained_weight/llava-v1.5-13b/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length False \\\r\n    --bf16 True \\\r\n    --output_dir /mnt/dolphinfs/ssd_pool/docker/user/hadoop-mapcv/chenxingyue02/data/mllm/llava/save_ckpt/special_1023/llava-v1.5-7b \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 8 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 1000 \\\r\n    --save_total_limit 2 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to none\r\n\r\nLog: \r\n\r\n10-24 16:07:29,164] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\r\n[2023-10-24 16:07:39,090] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-10-24 16:07:39,090] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-10-24 16:07:39,090] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-10-24 16:07:39,090] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-10-24 16:07:42,722] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-10-24 16:07:42,722] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-10-24 16:07:42,724] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-10-24 16:07:42,724] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-10-24 16:07:42,724] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n[2023-10-24 16:07:42,725] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-10-24 16:07:42,725] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-10-24 16:07:42,726] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-10-24 16:07:42,726] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-10-24 16:07:51,363] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 13.02B parameters\r\n[2023-10-24 16:10:31,632] [WARNING] [partition_parameters.py:836:_post_init_method] param `class_embedding` in CLIPVisionEmbeddings not on GPU so was not broadcasted from rank 0\r\n[2023-10-24 16:10:31,842] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 13.32B parameters\r\nFormatting inputs...Skip in lazy mode\r\nParameter Offload: Total persistent parameters: 749568 in 328 params\r\n[2023-10-24 16:11:41,447] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 5027\r\n[2023-10-24 16:11:43,364] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 5028\r\n[2023-10-24 16:11:43,365] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 5029\r\n[2023-10-24 16:11:43,365] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 5030\r\n[2023-10-24 16:11:43,365] [ERROR] [launch.py:321:sigkill_handler] ['/mnt/dolphinfs/ssd_pool/docker/user/hadoop-mapcv/chenxingyue02/conda_envs/llava/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=3', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', '/mnt/dolphinfs/ssd_pool/docker/user/hadoop-mapcv/chenxingyue02/data/mllm/vicuna/lmsys/vicuna-13b-v1.5', '--version', 'v1', '--data_path', '/mnt/dolphinfs/ssd_pool/docker/user/hadoop-mapcv/chenxingyue02/data/mllm/llava/train_json/special_train.json', '--image_folder', '/', '--vision_tower', '/mnt/dolphinfs/ssd_pool/docker/user/hadoop-mapcv/chenxingyue02/data/mllm/clip', '--pretrain_mm_mlp_adapter', '/mnt/dolphinfs/ssd_pool/docker/user/hadoop-mapcv/chenxingyue02/data/mllm/llava/pretrained_weight/llava-v1.5-13b/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'False', '--bf16', 'True', '--output_dir', '/mnt/dolphinfs/ssd_pool/docker/user/hadoop-mapcv/chenxingyue02/data/mllm/llava/save_ckpt/special_1023/llava-v1.5-7b', '--num_train_epochs', '1', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '8', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '1000', '--save_total_limit', '2', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none'] exits with return code = -8\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by fcjian at 2024-04-13T07:43:13Z>\n@buaachen1993 I meet the same issue. How did you solve it? Thanks.\n</Comment>\n<Comment by Jeremy-lf at 2024-05-09T08:46:03Z>\n> @buaachen1993 I meet the same issue. How did you solve it? Thanks.\r\n\r\nI also meet the same issue when pretraining,like this:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/45171399/d4fadb1f-c2a1-4263-8523-05651d45ca1d)\r\ndo you know how to solve?\n</Comment>\n<Comment by rulixiang at 2024-10-29T05:43:00Z>\ngot the same issue, did you solve it?\n</Comment>\n<Comment by Spring24ch at 2024-12-27T17:24:04Z>\n> 遇到同样的问题，您解决了吗？\r\n\r\n您解决了吗？\n</Comment>\n<Comment by Spring24ch at 2024-12-27T17:24:10Z>\n> > 我遇到了同样的问题。您是怎么解决的？谢谢。\r\n> \r\n> 我在预训练时也遇到了同样的问题，比如：![图像](https://private-user-images.githubusercontent.com/45171399/329184967-d4fadb1f-c2a1-4263-8523-05651d45ca1d.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzUzMjA0NzcsIm5iZiI6MTczNTMyMDE3NywicGF0aCI6Ii80NTE3MTM5OS8zMjkxODQ5NjctZDRmYWRiMWYtYzJhMS00MjYzLTg1MjMtMDU2NTFkNDVjYTFkLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEyMjclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMjI3VDE3MjI1N1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTJkODk0Y2JlOGE4ZDhiOWViZmFhMTQzZTZkNTA3OTM5N2Y4Yjc1MmY5MmE5OTdhZjEzOGZiY2M3NzhiOGY2NzEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.PtZqmeDDLKV3Fbia93WQGwT-13fAeq-hNrn2abwPVNo)你知道怎么解决吗？\r\n\r\n您解决了吗？\n</Comment>\n<Comment by Spring24ch at 2024-12-27T17:24:16Z>\n> > 我遇到了同样的问题。您是怎么解决的？谢谢。\r\n> \r\n> 我在预训练时也遇到了同样的问题，比如：![图像](https://private-user-images.githubusercontent.com/45171399/329184967-d4fadb1f-c2a1-4263-8523-05651d45ca1d.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzUzMjA0NzcsIm5iZiI6MTczNTMyMDE3NywicGF0aCI6Ii80NTE3MTM5OS8zMjkxODQ5NjctZDRmYWRiMWYtYzJhMS00MjYzLTg1MjMtMDU2NTFkNDVjYTFkLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEyMjclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMjI3VDE3MjI1N1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTJkODk0Y2JlOGE4ZDhiOWViZmFhMTQzZTZkNTA3OTM5N2Y4Yjc1MmY5MmE5OTdhZjEzOGZiY2M3NzhiOGY2NzEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.PtZqmeDDLKV3Fbia93WQGwT-13fAeq-hNrn2abwPVNo)你知道怎么解决吗？\r\n\r\n您解决了吗？\n</Comment>\n<Comment by Spring24ch at 2024-12-27T17:24:29Z>\n> 我遇到了同样的问题。您是怎么解决的？谢谢。\r\n\r\n您解决了吗？\n</Comment>\n<Comment by My-captain at 2025-03-14T12:42:14Z>\n> > [@buaachen1993](https://github.com/buaachen1993) I meet the same issue. How did you solve it? Thanks.\n> \n> I also meet the same issue when pretraining,like this: ![image](https://github.com/haotian-liu/LLaVA/assets/45171399/d4fadb1f-c2a1-4263-8523-05651d45ca1d) do you know how to solve?\n\nhi, have you solved the issue?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 653,
    "state": "closed",
    "created_by": "bcsiriuschen",
    "created_at": "2023-10-24T07:18:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/653</URL>\n\n<TITLE>[Question] GQA dataset download fail</TITLE>\n\n<BODY>### Question\n\nHi, I was not able to download the GQA dataset from the official website to reproduce the fine-tuning results. It seems their download link is not working. I am wondering if anyone have the dataset could share it in google drive or other cloud service. Thanks!</BODY>\n\n<COMMENTS>\n<Comment by hyunjun-eun at 2023-10-24T09:08:19Z>\nI also tried the GQA, but failed. And VisualGenome datasets are too slow to download.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 652,
    "state": "open",
    "created_by": "gyupro",
    "created_at": "2023-10-24T00:52:45Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/652</URL>\n\n<TITLE>[Question] Finetuning question</TITLE>\n\n<BODY>### Question\n\nYesterday, I tried various things. I repeatedly finetuned only a part of dataset and then tested it. What I noticed while observing the training is that even if the vision encoder is frozen, the visual feature of the result changes. (dry his clothes -> ironing clothes) How is this possible? Even when the visual encoder is frozen, during finetuning, is the embedding from the visual encoder reflected in the output of the MLP projector?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 651,
    "state": "closed",
    "created_by": "lzw-lzw",
    "created_at": "2023-10-23T17:54:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/651</URL>\n\n<TITLE>abnormal generation after pretraining</TITLE>\n\n<BODY>Hi, thank you for your excellent work. When I tried to reproduce your work, I found that when using the first-stage pre-trained model for inference, the model output did not match the picture, and even output some repetitive and meaningless words and symbols like \"4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k - 4k -\" or \"4k happy to be the\\n###\".\r\nI want to know if this is normal or if there is something wrong with my pretraining stage. Thanks!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-23T18:51:06Z>\nHi, please provide your command and which checkpoint are you using?\n</Comment>\n<Comment by lzw-lzw at 2023-10-24T03:13:38Z>\nI used the vicuna-7b-v1.5 model for pre-training, and the architecture and dataset I used are not the newest.\r\ndeepspeed llava/train/train.py \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --model_name_or_path lmsys/vicuna-7b-v1.5 \\\r\n    --version plain \\\r\n    --data_path path/data/LLaVA-Pretrain/LLaVA-CC3M-Pretrain-595K/chat.json \\\r\n    --image_folder path/data/LLaVA-CC3M-Pretrain-595K/image \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --mm_use_im_patch_token True \\\r\n    --bf16 False \\\r\n    --fp16 True  \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b-pretrain \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 32 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 1000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 1e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\n</Comment>\n<Comment by haotian-liu at 2023-10-24T03:43:05Z>\nThe command seems fine. Do you obtain the generated text using gradio demo? Can you share the command you used to launch the model worker?\n</Comment>\n<Comment by lzw-lzw at 2023-10-24T14:41:31Z>\nHi, I have solved the problem, it seems that my image folder was broken for some reason, after I re-downloaded the data, everything worked fine. However, I still have a question. Why do you no longer use image start and end token in llava1.5 version? Is it just for convenience or are there any disadvantages to using these tokens?\n</Comment>\n<Comment by haotian-liu at 2023-10-24T14:55:51Z>\nHi, these changes are mainly for simplicity and do not affect performance, and was introduced back in July for LLaVA-v1 models. You can check it out here: https://github.com/haotian-liu/LLaVA/releases/tag/v1.0.1\r\n\r\nWe'll include some clarifications in the revised paper, thanks.\n</Comment>\n<Comment by lzw-lzw at 2023-10-24T14:58:02Z>\nThanks so much!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 650,
    "state": "open",
    "created_by": "xiechengmude",
    "created_at": "2023-10-23T12:48:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/650</URL>\n\n<TITLE>[Usage] Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint When I try to pretrain model.</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\nBash pretrain.sh  on my fineunted Llama2 model.\r\n```\r\n\r\nLog: \r\n```\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\nLoading checkpoint shards: 100%|██████████| 3/3 [00:55<00:00, 18.40s/it]\r\nSome weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at xDAN-AI/xDAN-L1-llama2-Think-0930-e35 and are newly initialized: ['model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.35.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.33.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.34.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.39.self_attn.rotary_emb.inv_freq', 'model.layers.38.self_attn.rotary_emb.inv_freq', 'model.layers.37.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.32.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.36.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq']\r\n\r\n\r\n0%|          | 0/2181 [00:00<?, ?it/s]Traceback (most recent call last):\r\n  File \"/workspace/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/workspace/LLaVA/llava/train/train.py\", line 930, in train\r\n    trainer.train()\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1787, in _inner_training_loop\r\n    for step, inputs in enumerate(epoch_iterator):\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/data_loader.py\", line 381, in __iter__\r\n    dataloader_iter = super().__iter__()\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 441, in __iter__\r\n    return self._get_iterator()\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 388, in _get_iterator\r\n    return _MultiProcessingDataLoaderIter(self)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1084, in __init__\r\n    self._reset(loader, first_iter=True)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1117, in _reset\r\n    self._try_put_index()\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1351, in _try_put_index\r\n    index = self._next_index()\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 623, in _next_index\r\n    return next(self._sampler_iter)  # may raise StopIteration\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/data_loader.py\", line 175, in _iter_with_no_split\r\n    for idx, batch in enumerate(self.batch_sampler):\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/sampler.py\", line 254, in __iter__\r\n    for idx in self.sampler:\r\n  File \"/workspace/LLaVA/llava/train/llava_trainer.py\", line 126, in __iter__\r\n    indices = get_modality_length_grouped_indices(self.lengths, self.batch_size, self.world_size, generator=self.generator)\r\n  File \"/workspace/LLaVA/llava/train/llava_trainer.py\", line 59, in get_modality_length_grouped_indices\r\n    lang_indices, lang_lengths = zip(*[(i, -l) for i, l in enumerate(lengths) if l < 0])\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-23T20:12:56Z>\nSeems that the rotary embed parameters are not saved, which should be fine? What is the error caused StopIteration exception? The bottom part may be important.\n</Comment>\n<Comment by qazimbhat1 at 2024-01-21T20:27:57Z>\n@haotian-liu I face a similar issue with a different model loading. Can you please explain why it should be fine even if the rotary embed parameters are not loaded from the model?\r\nI aim to use the new model to pretrain and fine tune llava v1.5. Would it still be fine to do that even if the model is unable to load the rotary embed parameters?\n</Comment>\n<Comment by ZizhenWang at 2024-01-22T02:52:52Z>\nFor the miss weights, I think it may caused by the transformer pkg version. I update it from 4.31.0 to 4.33.2 and solved.\n</Comment>\n<Comment by qazimbhat1 at 2024-01-22T03:40:28Z>\n@ZizhenWang Thanks. This solves the issue.\n</Comment>\n<Comment by ShawnAn-WHU at 2024-04-17T13:32:40Z>\n> For the miss weights, I think it may caused by the transformer pkg version. I update it from 4.31.0 to 4.33.2 and solved.\r\n\r\n@ZizhenWang I faced the problem like #1417, do you know how to slove it? Thanks in advance！\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 649,
    "state": "open",
    "created_by": "HarshmJenya",
    "created_at": "2023-10-23T07:28:45Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/649</URL>\n\n<TITLE>[Usage] Model Worker unable to load the model on GTX 1070</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI have tried this \"python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b\" command multiple times but have constant and same issue with the worker: \r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\n[model_worker_a790e0.log](https://github.com/haotian-liu/LLaVA/files/13067252/model_worker_a790e0.log)\r\n\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/142071812/c9423eba-8e55-4d43-bf2a-48cd0cdb19ff)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-24T04:25:07Z>\nHi, please try with the latest instructions [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/Windows.md) for Windows, and please let me know if the error persists, thanks. (please paste the full logs here so that we can see the root cause of the error, if it still persists)\n</Comment>\n<Comment by HarshmJenya at 2023-10-25T03:53:27Z>\nHello haotian-liu, \r\n\r\nHere is the URL for my log file [model_worker_bc3186.log](https://github.com/haotian-liu/LLaVA/files/13161176/model_worker_bc3186.log)\n</Comment>\n<Comment by haotian-liu at 2023-10-25T06:10:58Z>\nHi @HarshmJenya, please follow the latest instructions [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/Windows.md) for Windows to set up your env. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 648,
    "state": "open",
    "created_by": "gyupro",
    "created_at": "2023-10-23T05:37:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/648</URL>\n\n<TITLE>[Question] Finetuning method.</TITLE>\n\n<BODY>### Question\n\nHello,\r\n\r\nI'm interested in using llava for a different purpose (translation). I have a dataset of about 20k entries (colloquial English - foreign language).\r\n\r\nI'm curious about the following:\r\n\r\nWould finetuning llava from its pretrained state with my dataset yield higher performance?\r\nOr would llava perform better on a dataset it was already finetuned on?\r\nAlternatively, would combining the finetuning data with the pretrained data and then pretraining it, followed by finetuning with my dataset, result in the best performance?\r\nIf you have conducted any related research on this, I would greatly appreciate a detailed response.\r\n\r\nThank you.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 647,
    "state": "open",
    "created_by": "xts-bit",
    "created_at": "2023-10-22T14:46:14Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/647</URL>\n\n<TITLE>How to use it via API?</TITLE>\n\n<BODY>### Question\n\nHow to use it via API? Is it possible to use this model as an API? Do you have any idea about this?  @haotian-liu Can you please enable Inference API for this model on HuggingFace?</BODY>\n\n<COMMENTS>\n<Comment by ashleykleynhans at 2023-10-25T17:57:17Z>\nI opened a Pull Request for this:\r\nhttps://github.com/haotian-liu/LLaVA/pull/599\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 646,
    "state": "open",
    "created_by": "ldfandian",
    "created_at": "2023-10-22T04:50:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/646</URL>\n\n<TITLE>[Question] 1 epoch training is used in v1.5 finetune, but 3 epochs training was used in previous version?</TITLE>\n\n<BODY>### Question\n\ncan you please help understand how # of epochs impact the result?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 645,
    "state": "open",
    "created_by": "LinB203",
    "created_at": "2023-10-22T01:43:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/645</URL>\n\n<TITLE>[Usage] How to use LoRA to tune LLaVA1.5?</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\nHF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 deepspeed llava/train/train_xformers.py \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --model_name_or_path lmsys/vicuna-7b-v1.5 \\\r\n    --lora_enable True \\\r\n    --data_path /apdcephfs_cq3/share_1311970/downstream_datasets/instruction_datasets/image/llava/llava_v1_5_mix665k.json \\\r\n    --image_folder /apdcephfs_cq3/share_1311970/downstream_datasets/instruction_datasets/image/llava \\\r\n    --vision_tower LanguageBind/LanguageBind_Image \\\r\n    --pretrain_mm_mlp_adapter /apdcephfs_cq3/share_1311970/lb/LLaVA/checkpoints/llava-v1.5-7b-pretrain/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b-A-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 2 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to tensorboard\r\n```\r\n\r\nLog: \r\n```\r\n2023-10-21 11:34:17.635 {'loss': 0.5653, 'learning_rate': 1.282051282051282e-07, 'epoch': 0.0}\r\n2023-10-21 11:34:22.396 {'loss': 1.2373, 'learning_rate': 2.564102564102564e-07, 'epoch': 0.0}\r\n2023-10-21 11:34:29.023 {'loss': 1.2148, 'learning_rate': 3.846153846153847e-07, 'epoch': 0.0}\r\n2023-10-21 11:34:34.830 {'loss': 1.1672, 'learning_rate': 5.128205128205128e-07, 'epoch': 0.0}\r\n2023-10-21 11:34:40.736 {'loss': 1.2009, 'learning_rate': 6.41025641025641e-07, 'epoch': 0.0}\r\n2023-10-21 11:34:46.595 {'loss': 1.2021, 'learning_rate': 7.692307692307694e-07, 'epoch': 0.0}\r\n2023-10-21 11:34:51.831 {'loss': 1.2034, 'learning_rate': 8.974358974358975e-07, 'epoch': 0.0}\r\n2023-10-21 11:34:57.068 {'loss': 1.1934, 'learning_rate': 1.0256410256410257e-06, 'epoch': 0.0}\r\n2023-10-21 11:35:01.916 {'loss': 1.1968, 'learning_rate': 1.153846153846154e-06, 'epoch': 0.0}\r\n2023-10-21 11:35:06.865 {'loss': 1.1509, 'learning_rate': 1.282051282051282e-06, 'epoch': 0.0}\r\n2023-10-21 11:35:11.854 {'loss': 1.2644, 'learning_rate': 1.4102564102564104e-06, 'epoch': 0.0}\r\n2023-10-21 11:35:17.410 {'loss': 1.147, 'learning_rate': 1.5384615384615387e-06, 'epoch': 0.0}\r\n2023-10-21 11:35:24.758 {'loss': 1.1631, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.0}\r\n2023-10-21 11:35:29.747 {'loss': 1.1558, 'learning_rate': 1.794871794871795e-06, 'epoch': 0.0}\r\n2023-10-21 11:35:35.561 {'loss': 1.2166, 'learning_rate': 1.9230769230769234e-06, 'epoch': 0.0}\r\n...\r\n2023-10-21 20:19:36.852 {'loss': 0.6323, 'learning_rate': 1.987694498760684e-09, 'epoch': 0.99}\r\n2023-10-21 20:19:42.341 {'loss': 0.6138, 'learning_rate': 1.865408502650379e-09, 'epoch': 0.99}\r\n2023-10-21 20:19:47.033 {'loss': 0.6641, 'learning_rate': 1.747004126635421e-09, 'epoch': 0.99}\r\n2023-10-21 20:19:52.202 {'loss': 0.6382, 'learning_rate': 1.6324814166823744e-09, 'epoch': 0.99}\r\n2023-10-21 20:19:59.817 {'loss': 0.4747, 'learning_rate': 1.5218404172545609e-09, 'epoch': 0.99}\r\n2023-10-21 20:20:05.041 {'loss': 0.699, 'learning_rate': 1.415081171305399e-09, 'epoch': 0.99}\r\n2023-10-21 20:20:10.263 {'loss': 0.6476, 'learning_rate': 1.3122037202828452e-09, 'epoch': 0.99}\r\n2023-10-21 20:20:16.441 {'loss': 0.7568, 'learning_rate': 1.2132081041282829e-09, 'epoch': 1.0}\r\n2023-10-21 20:20:21.907 {'loss': 0.6646, 'learning_rate': 1.1180943612754124e-09, 'epoch': 1.0}\r\n2023-10-21 20:20:29.114 {'loss': 0.6488, 'learning_rate': 1.026862528649142e-09, 'epoch': 1.0}\r\n2023-10-21 20:20:34.386 {'loss': 0.6423, 'learning_rate': 9.39512641668916e-10, 'epoch': 1.0}\r\n2023-10-21 20:20:39.944 {'loss': 0.7515, 'learning_rate': 8.560447342487177e-10, 'epoch': 1.0}\r\n2023-10-21 20:20:45.906 {'loss': 0.6375, 'learning_rate': 7.764588387915161e-10, 'epoch': 1.0}\r\n2023-10-21 20:20:52.306 {'loss': 0.6272, 'learning_rate': 7.007549861970387e-10, 'epoch': 1.0}\r\n2023-10-21 20:21:02.338 {'loss': 0.2924, 'learning_rate': 6.289332058551089e-10, 'epoch': 1.0}\r\n2023-10-21 20:21:07.362 {'loss': 0.6642, 'learning_rate': 5.609935256500887e-10, 'epoch': 1.0}\r\n2023-10-21 20:21:13.140 {'loss': 0.6757, 'learning_rate': 4.969359719586563e-10, 'epoch': 1.0}\r\n2023-10-21 20:21:19.525 {'loss': 0.6122, 'learning_rate': 4.3676056964869764e-10, 'epoch': 1.0}\r\n2023-10-21 20:21:25.979 {'loss': 0.5617, 'learning_rate': 3.804673420837457e-10, 'epoch': 1.0}\r\n2023-10-21 20:21:32.420 {'loss': 0.7028, 'learning_rate': 3.2805631111743064e-10, 'epoch': 1.0}\r\n2023-10-21 20:21:38.150 {'loss': 0.7419, 'learning_rate': 2.795274971001405e-10, 'epoch': 1.0}\r\n2023-10-21 20:21:45.332 {'loss': 0.6757, 'learning_rate': 2.3488091886902933e-10, 'epoch': 1.0}\r\n2023-10-21 20:21:51.185 {'loss': 0.6309, 'learning_rate': 1.941165937602296e-10, 'epoch': 1.0}\r\n2023-10-21 20:21:56.686 {'loss': 0.6726, 'learning_rate': 1.5723453759886042e-10, 'epoch': 1.0}\r\n2023-10-21 20:22:03.130 {'loss': 0.6263, 'learning_rate': 1.2423476470346808e-10, 'epoch': 1.0}\r\n2023-10-21 20:22:08.693 {'loss': 0.6428, 'learning_rate': 9.511728788602625e-11, 'epoch': 1.0}\r\n2023-10-21 20:22:14.066 {'loss': 0.6702, 'learning_rate': 6.988211845082582e-11, 'epoch': 1.0}\r\n2023-10-21 20:22:19.616 {'loss': 0.6295, 'learning_rate': 4.852926619447473e-11, 'epoch': 1.0}\r\n2023-10-21 20:22:25.127 {'loss': 0.6241, 'learning_rate': 3.105873940811854e-11, 'epoch': 1.0}\r\n2023-10-21 20:22:31.007 {'loss': 0.6726, 'learning_rate': 1.7470544874109706e-11, 'epoch': 1.0}\r\n2023-10-21 20:22:37.041 {'loss': 0.6862, 'learning_rate': 7.764687866007592e-12, 'epoch': 1.0}\r\n2023-10-21 20:22:42.142 {'loss': 0.631, 'learning_rate': 1.9411721552398123e-12, 'epoch': 1.0}\r\n2023-10-21 20:22:51.572 {'loss': 0.53, 'learning_rate': 0.0, 'epoch': 1.0}\r\n```\r\n\r\nEval command:\r\n```\r\n\r\nCUDA_VISIBLE_DEVICES=1 python3 -m llava.eval.model_vqa_science \\\r\n    --model-path /LLaVA/checkpoints/llava-v1.5-7b-A-lora \\\r\n    --question-file /instruction_datasets/image/llava/eval/scienceqa/llava_test_CQM-A.json \\\r\n    --image-folder/instruction_datasets/image/llava/eval/scienceqa/images/test \\\r\n    --answers-file /instruction_datasets/image/llava/eval/scienceqa/answers/llava-v1.5-7b-lora.jsonl \\\r\n    --single-pred-prompt \\\r\n    --temperature 0 \\\r\n    --conv-mode vicuna_v1\r\n\r\nCUDA_VISIBLE_DEVICES=1 python3 llava/eval/eval_science_qa.py \\\r\n    --base-dir/instruction_datasets/image/llava/eval/scienceqa \\\r\n    --result-file/instruction_datasets/image/llava/eval/scienceqa/answers/llava-v1.5-7b-lora.jsonl \\\r\n    --output-file /instruction_datasets/image/llava/eval/scienceqa/answers/llava-v1.5-7b-lora_output.jsonl \\\r\n    --output-result /instruction_datasets/image/llava/eval/scienceqa/answers/llava-v1.5-7b-lora_result.json\r\n```\r\n\r\nScreenshots:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/62638829/88526b45-a507-4a08-9a7e-63ab06ddebfc)\r\n\r\nThe training process seems normal. But it was abnormal during testing.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-26T20:40:55Z>\nHi, we have released the LoRA ckpt, script, and a brief doc on the new schedule.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/releases/tag/v1.1.3\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 644,
    "state": "closed",
    "created_by": "barshag",
    "created_at": "2023-10-21T20:10:55Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/644</URL>\n\n<TITLE>[Question] how is it possible to distill specifc knowledge to small models for creating specific task?</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by barshag at 2023-10-28T20:58:09Z>\ni guess that finetune/lora is the answer...\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 642,
    "state": "open",
    "created_by": "echo840",
    "created_at": "2023-10-21T16:16:21Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/642</URL>\n\n<TITLE>[Question] ScienceQA result 71.6%</TITLE>\n\n<BODY>### Question\n\nHello. The result of 71.6% in the paper, is it based on the minitest.jsonl or test.jsonl for ScienceQA?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 641,
    "state": "closed",
    "created_by": "nj159",
    "created_at": "2023-10-21T12:11:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/641</URL>\n\n<TITLE>[Question] llava_v1.5 pretrain</TITLE>\n\n<BODY>### Question\n\nexcuse me，I would like to ask you that why I get the following error when pretraining v1.5，My code is: ./scripts/v1_5/pretrain.sh\r\noutput：\r\n[2023-10-21 19:41:04,065] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-10-21 19:41:06,429] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\n[2023-10-21 19:41:06,430] [INFO] [runner.py:555:main] cmd = /home/nj/.conda/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero2.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version plain --data_path ./playground/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k_first500.json --image_folder ./playground/data/LLaVA-Pretrain/images --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --tune_mm_mlp_adapter True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --fp16 True --output_dir ./liuhaotian2/llava-v1.5-7b-pretrain --num_train_epochs 1 --per_device_train_batch_size 32 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 24000 --save_total_limit 1 --learning_rate 1e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 False --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb\r\n[2023-10-21 19:41:07,902] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-10-21 19:41:09,817] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\r\n[2023-10-21 19:41:09,817] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0\r\n[2023-10-21 19:41:09,818] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\r\n[2023-10-21 19:41:09,818] [INFO] [launch.py:163:main] dist_world_size=4\r\n[2023-10-21 19:41:09,818] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\r\n[2023-10-21 19:41:12,902] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-10-21 19:41:12,952] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-10-21 19:41:13,003] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-10-21 19:41:13,021] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n/media/nj/data2/nj/Models/LLaVA/llava/train/llama_flash_attn_monkey_patch.py:108: UserWarning: Flash attention is only supported on A100 or H100 GPU during training due to head dim > 64 backward.ref: https://github.com/HazyResearch/flash-attention/issues/190#issuecomment-1523359593\r\n  warnings.warn(\r\n/media/nj/data2/nj/Models/LLaVA/llava/train/llama_flash_attn_monkey_patch.py:108: UserWarning: Flash attention is only supported on A100 or H100 GPU during training due to head dim > 64 backward.ref: https://github.com/HazyResearch/flash-attention/issues/190#issuecomment-1523359593\r\n  warnings.warn(\r\n/media/nj/data2/nj/Models/LLaVA/llava/train/llama_flash_attn_monkey_patch.py:108: UserWarning: Flash attention is only supported on A100 or H100 GPU during training due to head dim > 64 backward.ref: https://github.com/HazyResearch/flash-attention/issues/190#issuecomment-1523359593\r\n  warnings.warn(\r\n/media/nj/data2/nj/Models/LLaVA/llava/train/llama_flash_attn_monkey_patch.py:108: UserWarning: Flash attention is only supported on A100 or H100 GPU during training due to head dim > 64 backward.ref: https://github.com/HazyResearch/flash-attention/issues/190#issuecomment-1523359593\r\n  warnings.warn(\r\n[2023-10-21 19:41:13,693] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-10-21 19:41:13,694] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-10-21 19:41:13,699] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-10-21 19:41:13,699] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-10-21 19:41:13,700] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n[2023-10-21 19:41:13,782] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-10-21 19:41:13,782] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-10-21 19:41:13,789] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-10-21 19:41:13,789] [INFO] [comm.py:594:init_distributed] cdb=None\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\n[2023-10-21 19:41:56,780] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 10468\r\n[2023-10-21 19:41:56,809] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 10469\r\n[2023-10-21 19:41:57,776] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 10470\r\n[2023-10-21 19:41:58,770] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 10471\r\n[2023-10-21 19:41:59,773] [ERROR] [launch.py:321:sigkill_handler] ['/home/nj/.conda/envs/llava/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=3', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'plain', '--data_path', './playground/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k_first500.json', '--image_folder', './playground/data/LLaVA-Pretrain/images', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--tune_mm_mlp_adapter', 'True', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--fp16', 'True', '--output_dir', './liuhaotian2/llava-v1.5-7b-pretrain', '--num_train_epochs', '1', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '24000', '--save_total_limit', '1', '--learning_rate', '1e-3', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'False', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = -9</BODY>\n\n<COMMENTS>\n<Comment by walidmbz at 2023-10-22T11:20:48Z>\nany luck with this? I am having the same issue\n</Comment>\n<Comment by nj159 at 2023-10-23T11:36:27Z>\n> any luck with this? I am having the same issue\r\n\r\nno,i don't know how to deal with it\n</Comment>\n<Comment by walidmbz at 2023-10-23T11:39:48Z>\nI resolved it. Turns out it is actually a OOM error. in the pretrain file change zero2.json to zero3.json and it should work.\n</Comment>\n<Comment by nj159 at 2023-10-23T12:10:11Z>\n> 我解决了。事实证明，这实际上是一个OOM错误。在预训练文件中，将 zero2.json 更改为 zero3.json，它应该可以工作。\r\nThank you very much, but I have a new problem after switching to zero3.json, I don't know why?\r\n\r\n[2023-10-23 19:50:53,106] [WARNING] [partition_parameters.py:836:_post_init_method] param `class_embedding` in CLIPVisionEmbeddings not on GPU so was not broadcasted from rank 0\r\n[2023-10-23 19:50:53,321] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 7.04B parameters\r\nTraceback (most recent call last):\r\n  File \"/media/nj/data2/nj/Models/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    Traceback (most recent call last):\r\ntrain()\r\n  File \"/media/nj/data2/nj/Models/LLaVA/llava/train/train.py\", line 920, in train\r\n  File \"/media/nj/data2/nj/Models/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\nTraceback (most recent call last):\r\n  File \"/media/nj/data2/nj/Models/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    data_module = make_supervised_data_module(tokenizer=tokenizer,\r\n  File \"/media/nj/data2/nj/Models/LLaVA/llava/train/train.py\", line 747, in make_supervised_data_module\r\n    train()\r\n  File \"/media/nj/data2/nj/Models/LLaVA/llava/train/train.py\", line 920, in train\r\n    train()\r\n  File \"/media/nj/data2/nj/Models/LLaVA/llava/train/train.py\", line 920, in train\r\n    train_dataset = LazySupervisedDataset(tokenizer=tokenizer,\r\n  File \"/media/nj/data2/nj/Models/LLaVA/llava/train/train.py\", line 633, in __init__\r\n    list_data_dict = json.load(open(data_path, \"r\"))\r\n  File \"/home/nj/.conda/envs/llava/lib/python3.10/json/__init__.py\", line 293, in load\r\n    data_module = make_supervised_data_module(tokenizer=tokenizer,\r\n  File \"/media/nj/data2/nj/Models/LLaVA/llava/train/train.py\", line 747, in make_supervised_data_module\r\n    data_module = make_supervised_data_module(tokenizer=tokenizer,\r\n  File \"/media/nj/data2/nj/Models/LLaVA/llava/train/train.py\", line 747, in make_supervised_data_module\r\n    return loads(fp.read(),\r\n  File \"/home/nj/.conda/envs/llava/lib/python3.10/json/__init__.py\", line 346, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"/home/nj/.conda/envs/llava/lib/python3.10/json/decoder.py\", line 340, in decode\r\n    train_dataset = LazySupervisedDataset(tokenizer=tokenizer,\r\n  File \"/media/nj/data2/nj/Models/LLaVA/llava/train/train.py\", line 633, in __init__\r\n    train_dataset = LazySupervisedDataset(tokenizer=tokenizer,\r\n  File \"/media/nj/data2/nj/Models/LLaVA/llava/train/train.py\", line 633, in __init__\r\n    raise JSONDecodeError(\"Extra data\", s, end)\r\njson.decoder.JSONDecodeError: Extra data: line 15 column 1 (char 358)\r\n    list_data_dict = json.load(open(data_path, \"r\"))\r\n  File \"/home/nj/.conda/envs/llava/lib/python3.10/json/__init__.py\", line 293, in load\r\n    list_data_dict = json.load(open(data_path, \"r\"))\r\n  File \"/home/nj/.conda/envs/llava/lib/python3.10/json/__init__.py\", line 293, in load\r\n    return loads(fp.read(),\r\n  File \"/home/nj/.conda/envs/llava/lib/python3.10/json/__init__.py\", line 346, in loads\r\n    return loads(fp.read(),\r\n  File \"/home/nj/.conda/envs/llava/lib/python3.10/json/__init__.py\", line 346, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"/home/nj/.conda/envs/llava/lib/python3.10/json/decoder.py\", line 340, in decode\r\n    return _default_decoder.decode(s)\r\n  File \"/home/nj/.conda/envs/llava/lib/python3.10/json/decoder.py\", line 340, in decode\r\n    raise JSONDecodeError(\"Extra data\", s, end)\r\njson.decoder.JSONDecodeError: Extra data: line 15 column 1 (char 358)\r\n    raise JSONDecodeError(\"Extra data\", s, end)\r\njson.decoder.JSONDecodeError: Extra data: line 15 column 1 (char 358)\r\nTraceback (most recent call last):\r\n  File \"/media/nj/data2/nj/Models/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/media/nj/data2/nj/Models/LLaVA/llava/train/train.py\", line 920, in train\r\n    data_module = make_supervised_data_module(tokenizer=tokenizer,\r\n  File \"/media/nj/data2/nj/Models/LLaVA/llava/train/train.py\", line 747, in make_supervised_data_module\r\n    train_dataset = LazySupervisedDataset(tokenizer=tokenizer,\r\n  File \"/media/nj/data2/nj/Models/LLaVA/llava/train/train.py\", line 633, in __init__\r\n    list_data_dict = json.load(open(data_path, \"r\"))\r\n  File \"/home/nj/.conda/envs/llava/lib/python3.10/json/__init__.py\", line 293, in load\r\n    return loads(fp.read(),\r\n  File \"/home/nj/.conda/envs/llava/lib/python3.10/json/__init__.py\", line 346, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"/home/nj/.conda/envs/llava/lib/python3.10/json/decoder.py\", line 340, in decode\r\n    raise JSONDecodeError(\"Extra data\", s, end)\r\njson.decoder.JSONDecodeError: Extra data: line 15 column 1 (char 358)\r\n[2023-10-23 19:51:13,558] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 19799\r\n[2023-10-23 19:51:13,577] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 19800\r\n[2023-10-23 19:51:13,590] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 19801\r\n[2023-10-23 19:51:13,602] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 19802\r\n[2023-10-23 19:51:13,602] [ERROR] [launch.py:321:sigkill_handler] ['/home/nj/.conda/envs/llava/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=3', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'plain', '--data_path', './playground/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k_first500.json', '--image_folder', './playground/data/LLaVA-Pretrain/images', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--tune_mm_mlp_adapter', 'True', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--fp16', 'True', '--output_dir', './liuhaotian2/llava-v1.5-7b-pretrain', '--num_train_epochs', '1', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '24000', '--save_total_limit', '1', '--learning_rate', '1e-3', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'False', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = 1\n</Comment>\n<Comment by haotian-liu at 2023-10-26T20:52:24Z>\n> Flash attention is only supported on A100 or H100 GPU during training due to hardware ...\r\n\r\nSeems that you are using Flash-Attention on an  unsupported hardware? Maybe you can try `train.py` instead of `train_mem.py`, but it may have a larger memory requirement.\n</Comment>\n<Comment by nj159 at 2023-10-27T02:44:33Z>\n> > Flash attention is only supported on A100 or H100 GPU during training due to hardware ...\r\n> \r\n> Seems that you are using Flash-Attention on an unsupported hardware? Maybe you can try `train.py` instead of `train_mem.py`, but it may have a larger memory requirement.\r\n\r\nthank you vary much，i have solved it\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 639,
    "state": "open",
    "created_by": "Faultiness",
    "created_at": "2023-10-21T09:01:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/639</URL>\n\n<TITLE>[Usage] Batch inference or API for Gradio</TITLE>\n\n<BODY>Would it be possible to add an API endpoint for the Gradio interface? Alternatively, is there any method available for batch inference? Thanks for any suggestions.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 637,
    "state": "open",
    "created_by": "souvikqb",
    "created_at": "2023-10-21T02:48:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/637</URL>\n\n<TITLE>[Question] Is there a HuggingFace Pipeline for this?</TITLE>\n\n<BODY>### Question\n\nI'm looking for a HF pipeline that can be used. Essentially as a part of the Transformers pipeline. \r\n\r\nCan someone point be out to the Documentation? \r\n\r\nThis URL is giving a 404 Error - https://huggingface.co/docs/transformers/main/en/model_doc/llava#transformers.LlavaLlamaForCausalLM</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 636,
    "state": "closed",
    "created_by": "dinchu",
    "created_at": "2023-10-20T21:29:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/636</URL>\n\n<TITLE>[Usage]</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nThe installation works without errors, but every time i try to execute any of the servers, worker or controller i get  No module named 'idna'. happens on ubuntu 22 with 2 L4 gpus. Did a check and idna is installed in version \r\nCommand:\r\n```\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b\r\n```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/pdelgado_harlemnext_com/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 187, in _run_module_as_main\r\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\r\n  File \"/home/pdelgado_harlemnext_com/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 110, in _get_module_details\r\n    __import__(pkg_name)\r\n  File \"/home/pdelgado_harlemnext_com/LLaVA/llava/__init__.py\", line 1, in <module>\r\n    from .model import LlavaLlamaForCausalLM\r\n  File \"/home/pdelgado_harlemnext_com/LLaVA/llava/model/__init__.py\", line 1, in <module>\r\n    from .language_model.llava_llama import LlavaLlamaForCausalLM, LlavaConfig\r\n  File \"/home/pdelgado_harlemnext_com/LLaVA/llava/model/language_model/llava_llama.py\", line 22, in <module>\r\n    from transformers import AutoConfig, AutoModelForCausalLM, \\\r\n  File \"/home/pdelgado_harlemnext_com/.local/lib/python3.10/site-packages/transformers/__init__.py\", line 26, in <module>\r\n    from . import dependency_versions_check\r\n  File \"/home/pdelgado_harlemnext_com/.local/lib/python3.10/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\r\n    from .utils.versions import require_version, require_version_core\r\n  File \"/home/pdelgado_harlemnext_com/.local/lib/python3.10/site-packages/transformers/utils/__init__.py\", line 30, in <module>\r\n    from .generic import (\r\n  File \"/home/pdelgado_harlemnext_com/.local/lib/python3.10/site-packages/transformers/utils/generic.py\", line 29, in <module>\r\n    from .import_utils import is_flax_available, is_tf_available, is_torch_available, is_torch_fx_proxy\r\n  File \"/home/pdelgado_harlemnext_com/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 34, in <module>\r\n    from . import logging\r\n  File \"/home/pdelgado_harlemnext_com/.local/lib/python3.10/site-packages/transformers/utils/logging.py\", line 35, in <module>\r\n    import huggingface_hub.utils as hf_hub_utils\r\n  File \"/home/pdelgado_harlemnext_com/.local/lib/python3.10/site-packages/huggingface_hub/utils/__init__.py\", line 32, in <module>\r\n    from ._errors import (\r\n  File \"/home/pdelgado_harlemnext_com/.local/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 3, in <module>\r\n    from requests import HTTPError, Response\r\n  File \"/home/pdelgado_harlemnext_com/.local/lib/python3.10/site-packages/requests/__init__.py\", line 147, in <module>\r\n    from . import packages, utils\r\n  File \"/home/pdelgado_harlemnext_com/.local/lib/python3.10/site-packages/requests/packages.py\", line 16, in <module>\r\n    locals()[package] = __import__(package)\r\nModuleNotFoundError: No module named 'idna'\r\n```\r\n\r\nthe module idna is installed\r\n```pip show  idna ```\r\n```\r\nName: idna\r\nVersion: 3.3\r\nSummary: Internationalized Domain Names in Applications (IDNA)\r\nHome-page: https://github.com/kjd/idna\r\nAuthor: Kim Davies\r\nAuthor-email: kim@cynosure.com.au\r\nLicense: BSD-3-Clause\r\nLocation: /usr/lib/python3/dist-packages\r\nRequires: \r\nRequired-by: anyio, httpx, requests, yarl\r\n```</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-20T21:43:24Z>\nIt seems that the packages are not installed successfully in your conda env?\r\n\r\n File \"/home/pdelgado_harlemnext_com/anaconda3/envs/llava/lib/python3.10/runpy.py\"\r\nFile \"/home/pdelgado_harlemnext_com/.local/lib/python3.10/site-packages/transformers/__init__.py\", line 26, in <module>\r\nLocation: /usr/lib/python3/dist-packages\r\n\r\nIncluding transformers, and idna, especially the package idna (as it seems that it is installed in the system python, which may have version mismatch.\r\n\r\nTheoretically all these package should be within your anaconda env.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 635,
    "state": "closed",
    "created_by": "DragosDima96",
    "created_at": "2023-10-20T16:32:59Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/635</URL>\n\n<TITLE>[Usage] Eval Function For Batch Predictiom</TITLE>\n\n<BODY>### Describe the issue\n\nThe current eval function from llava eval provides example of inference for a bs of 1 (given one input image and one piece of text) and there is a TO DO in code.\r\n\r\nHas anyone succeded in modifying the code to allow batch inference?</BODY>\n\n<COMMENTS>\n<Comment by ngc6302h at 2023-10-21T00:46:36Z>\nFollowing this issue. I'm also interested in batch prediction.\n</Comment>\n<Comment by rabiulcste at 2023-11-02T16:43:46Z>\nYou'd need a data processor like this https://github.com/rabiulcste/vqazero/blob/13afb6f79648a4efc1184ba4f179b6fd07eacef1/dataset_zoo/custom_processor.py#L73\n</Comment>\n<Comment by haotian-liu at 2023-11-05T05:09:03Z>\nHi, thank you, and I am trying to work on an efficient batch inference. Closing this and consolidating the discussion to https://github.com/haotian-liu/LLaVA/issues/754.\r\n\r\n@rabiulcste If you have prior experience in the batch inference, it is very much appreciated if you could help share some insights on why the batch inference I implemented were not efficient. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 634,
    "state": "closed",
    "created_by": "TousakaNagio",
    "created_at": "2023-10-20T12:27:57Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/634</URL>\n\n<TITLE>[Usage] Error named symbol not found at line 529 in file /mmfs1/gscratch/zlab/timdettmers/git/bitsandbytes/csrc/ops.cu</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: Error named symbol not found at line 529 in file /mmfs1/gscratch/zlab/timdettmers/git/bitsandbytes/csrc/ops.cu\r\n\r\nCommand:\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-7b --load-8bit\r\n\r\nLog: \r\n[2023-10-20 20:10:50,697] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n2023-10-20 20:10:51 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='liuhaotian/llava-v1.5-7b', model_base=None, model_name=None, device='cuda', multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=True, load_4bit=False)\r\n2023-10-20 20:10:51 | INFO | model_worker | Loading the model llava-v1.5-7b on worker 32ce76 ...\r\nLoading checkpoint shards:   0%|                                                                                                                                                     | 0/2 [00:00<?, ?it/s]\r\nError named symbol not found at line 529 in file /mmfs1/gscratch/zlab/timdettmers/git/bitsandbytes/csrc/ops.cu\r\n\r\nScreenshots:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/66267981/28b3740b-f4c6-4755-81b8-c23e36607208)\r\nMy device settinig.\r\nI can run the gradio locally without loading by 8 or 4 bits.</BODY>\n\n<COMMENTS>\n<Comment by TousakaNagio at 2023-10-20T12:41:31Z>\nSolved:\r\nSet CUDA_VISIBLE_DEVICES=0 while running the cammand.\n</Comment>\n<Comment by youliangtan at 2023-12-08T07:12:51Z>\nI dont think this is solved. For cases that you would wish to more than 1 CPU e.g. `export CUDA_VISIBLE_DEVICES=0,1,2` quantization still breaks.\r\n\r\nThis is a known issue in `bitsandbytes` and iam not too sure with the fix\r\nhttps://github.com/TimDettmers/bitsandbytes/issues/566\n</Comment>\n<Comment by Lareina2441 at 2024-10-23T13:44:15Z>\nsame problem\n</Comment>\n<Comment by dongdk at 2025-01-08T06:10:16Z>\nsame problem, any idea to solve it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 633,
    "state": "closed",
    "created_by": "jianzongwu",
    "created_at": "2023-10-20T10:34:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/633</URL>\n\n<TITLE>[Question] How can I edit system message of LLaVA?</TITLE>\n\n<BODY>### Question\n\nHow can I edit the system message of LLaVA like ChatGPT?\r\n\r\nIn `conversation.py`, there is a system message recorded as \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\" But I did not find a place to input the system message into the model</BODY>\n\n<COMMENTS>\n<Comment by Road2Redemption at 2023-10-21T02:21:20Z>\nI think you can find \"from llava.conversation import default_conversation\" llava/eval/model_qa.py, it is used for evaluation maybe?\n</Comment>\n<Comment by jianzongwu at 2023-10-21T04:10:58Z>\nI tried to edit the system prompt in `llava.conversation.default_conversation` and load the edited conversation in the gradio app (by changing the template name).\r\nHowever, the model seems not influenced by the edited system prompt.\r\nSpecifically, I told the model to produce output in a given format, but it still just describes the image as usual.\r\nIt seems the system message in `conversation.py` is just for recording, and it is not actually input to the model?\n</Comment>\n<Comment by wisdomikezogwo at 2023-10-29T17:41:05Z>\ntry doing this: `pip install -e .` after changing the system prompt for your change to take effect if you haven't before.\n</Comment>\n<Comment by jianzongwu at 2023-11-12T03:14:57Z>\nFixed, thanks. I finally know that the system prompt is inserted to the chat contents\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 632,
    "state": "open",
    "created_by": "Carol-lyh",
    "created_at": "2023-10-20T09:42:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/632</URL>\n\n<TITLE>[Usage] The evaluation results on scienceQA</TITLE>\n\n<BODY>### Describe the issue\n\nI use the model-weights of 7b-v1.5 you released to evaluate the performance. The evaluation results on other datasets are the same with your MODEL-ZOO, but the scienceQA results is:\r\n\r\n\r\n**Total: 4241, Correct: 2944, Accuracy: 69.42%, IMG-Accuracy: 67.97%**\r\n\r\n\r\nwhich is different from the **66.8** in MODEL-ZOO for llava-7b-v1.5. Why?</BODY>\n\n<COMMENTS>\n<Comment by nbasyl at 2023-12-16T13:34:03Z>\nHi, I am also having the same problem, and for the LoRA weight in the model zoo, I can only get:\r\nTotal: 4241, Correct: 2763, Accuracy: 65.15%, IMG-Accuracy: 61.73% \r\nwhich differs a lot from the score reported from the model zoo. \r\n@haotian-liu Can you please help me check if any of my steps are wrong, I used the following command for reproducing your result:\r\n`\r\nCUDA_VISIBLE_DEVICES=0 \r\n\r\npython -m llava.eval.model_vqa_science \\\r\n    --model-path ./checkpoints/llava-v1.5-7b-lora \\\r\n    --model-base liuhaotian/llava-v1.5-7b \\\r\n    --question-file ./playground/data/eval/scienceqa/llava_test_CQM-A.json \\\r\n    --image-folder ./playground/data/eval/scienceqa/images/test \\\r\n    --answers-file ./playground/data/eval/scienceqa/answers/llava-v1.5-7b-lora.jsonl \\\r\n    --single-pred-prompt \\\r\n    --temperature 0 \\\r\n    --conv-mode vicuna_v1\r\n\r\npython llava/eval/eval_science_qa.py \\\r\n    --base-dir ./playground/data/eval/scienceqa \\\r\n    --result-file ./playground/data/eval/scienceqa/answers/llava-v1.5-7b-lora.jsonl \\\r\n    --output-file ./playground/data/eval/scienceqa/answers/llava-v1.5-7b-lora_output.jsonl \\\r\n    --output-result ./playground/data/eval/scienceqa/answers/llava-v1.5-7b-lora_result.json\r\n`\r\n\r\nThank you so much!\n</Comment>\n<Comment by EchoDreamer at 2024-11-11T11:39:40Z>\nHi! I reproduced the same results as you on this dataset, but the reproduction results for TextQA are only 50.52, POPE is 76.32, and MME is 1394. Could you share how your reproduction results performed on these datasets? Did you encounter similar issues, and have you found any viable solutions? \r\nThanks！\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 631,
    "state": "open",
    "created_by": "jyC23333",
    "created_at": "2023-10-20T08:55:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/631</URL>\n\n<TITLE>[Discussion] LLaVA's hallucination</TITLE>\n\n<BODY>### Discussion\n\nI just found an interesting test. The model seems to imagine that what's happening outside the image.\r\n\r\nAs follow, the model is describing the person who is taking the photo.\r\n\r\n**different prompts with the same image**\r\n![image](https://github.com/haotian-liu/LLaVA/assets/110331827/1dd53548-88e4-4b3c-8252-3398857e5124)\r\n![image](https://github.com/haotian-liu/LLaVA/assets/110331827/26ae0652-9a76-4b52-846f-155d16a5938b)\r\n\r\nIs this a special setting when training LLaVA, or just a strong hallucination？\r\n\r\nTo be honest, this is funny.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 630,
    "state": "open",
    "created_by": "yix-chen",
    "created_at": "2023-10-20T03:43:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/630</URL>\n\n<TITLE>[Question] Cannot reproduce MME results on LLaVA-1.5-7B</TITLE>\n\n<BODY>### Question\n\nHi, I cannot reproduce MME results following [finetune.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune.sh) on 665k instruction tuning dataset and evaluation scripts for MME. We followed all the settings except flash-attention on A100, and got 1466.6. Given that the paper reported 1510 on MME, is that a normal fluctuation or some hyperparameters need to be tweaked.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-20T19:47:53Z>\nHi, can you share the number of the official checkpoints you evaluate on your local machine (to make sure that the eval is consistent)? Also, what about the numbers on other datasets? Are they consistently lower (can you also share them)? Thanks.\n</Comment>\n<Comment by yix-chen at 2023-10-21T07:07:16Z>\nHi Haotian,\r\n\r\nThe MME evaluation on official v1.5-7B checkpoint is fine, which is 1508.9. And on other datasets, the results are also consistent with the reported. So I wonder if something went wrong in finetuning, e.g., flash-attention was not used?\n</Comment>\n<Comment by haotian-liu at 2023-10-21T15:09:27Z>\nHi @yix-chen \r\n\r\nI have not tested running without flash-attention, but theoretically, it is an exact-attention optimization, so with or without does not significantly affect the results.\r\n\r\nIt seems that the eval is fine, but it is still hard to determine the cause with MME performance alone. Can you share the numbers of more datasets you have tested, so that we can see both the trend and the exact absolute difference? Thanks.\n</Comment>\n<Comment by Carol-lyh at 2023-10-27T06:12:11Z>\nWe cannot reproduce the results on MME either, our result is 1457.7\n</Comment>\n<Comment by TempleX98 at 2023-10-27T09:25:40Z>\nWe also failed to reproduce the official performance. Our model got a score of 1473.\n</Comment>\n<Comment by haotian-liu at 2023-11-27T16:16:27Z>\nThis may be due to some unexpected randomness when using distributed training (https://github.com/haotian-liu/LLaVA/issues/864), while we haven't figured out where the randomness is -- the data mixture order is verified to be the same across different runs, and there should not be any randomly initialized weights if we start with a pretrained projector.\r\n\r\nThis observed randomness has led to fluctuation of some benchmark performance -- MME is the most prominent (I can get +/- 20 from the report 1510 for 7B model, similar for 13B model) and other datasets are mostly stable.\r\n\r\nAny observation/advice in terms of the randomness is welcomed.\n</Comment>\n<Comment by eehover at 2023-12-06T12:22:41Z>\ntry to set the deepspeeed zero1 config. the loss will be same every time\n</Comment>\n<Comment by zjysteven at 2024-06-20T15:00:33Z>\nCan confirm the same thing here. I'm using lmms-eval for evaluation. The released llava-1.5-7b checkpoint got 1512 on MME, while my retrained/reproduced version got only 1478.\n</Comment>\n<Comment by FrankYang-17 at 2025-02-16T02:51:35Z>\n@zjysteven Hi, did you solve it?\n</Comment>\n<Comment by FrankYang-17 at 2025-02-22T04:03:08Z>\n> ### Question\n> Hi, I cannot reproduce MME results following [finetune.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune.sh) on 665k instruction tuning dataset and evaluation scripts for MME. We followed all the settings except flash-attention on A100, and got 1466.6. Given that the paper reported 1510 on MME, is that a normal fluctuation or some hyperparameters need to be tweaked.您好，无法在 665k 指令微调数据集和 MME 评估脚本上复现 finetune.sh 的 MME 结果。我们遵循了所有设置，除了在 A100 上使用 flash-attention，得到了 1466.6。鉴于论文在 MME 上报告了 1510，这是正常波动还是某些超参数需要调整？\n\nhi, did you solve it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 629,
    "state": "closed",
    "created_by": "jiangsongtao",
    "created_at": "2023-10-20T03:34:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/629</URL>\n\n<TITLE>[Question] About the results on ScienceQA in LLaVA1.5</TITLE>\n\n<BODY>### Question\n\nHello,sorry to disturbe\r\nI tried to conduct the scienceQA test on llava1.5 and found that the accuracy rate on minitest.jasol was 71.58%, which was consistent with the accuracy in the paper, but the result on test.jasonl was 70.25%. I don’t know if the 71.6% in the paper refers to minitest. result, thank you!</BODY>\n\n<COMMENTS>\n<Comment by jameszhou-gl at 2023-10-20T12:06:30Z>\n> ### Question\r\n> Hello,sorry to disturbe I tried to conduct the scienceQA test on llava1.5 and found that the accuracy rate on minitest.jasol was 71.58%, which was consistent with the accuracy in the paper, but the result on test.jasonl was 70.25%. I don’t know if the 71.6% in the paper refers to minitest. result, thank you!\r\n\r\nThe results I reproduced: \r\n- Total: 4241, Correct: 2982, Accuracy: 70.31%, IMG-Accuracy: 70.85%;  \r\n- --model-path: liuhaotian/llava-v1.5-13b\r\n- --question-file: llava_test_CQM-A.json, \r\n- with A100 GPUs.\n</Comment>\n<Comment by yzbx at 2023-10-27T04:01:19Z>\nThe results for llava-v1.5-7b I reproduced:\r\n\r\n- total 4241, Correct: 2780, Accuracy: 65.55%, Img-Accuracy: 63.91%\r\n- question-file: llava_test_CQM-A.json\r\n\r\n63.91% is less than 66.8% in paper for llava-v1.5-7b\n</Comment>\n<Comment by haotian-liu at 2023-10-27T05:17:12Z>\nI just tried and I am able to reproduce the ScienceQA results from scratch:\r\n- Clone the repo on GitHub\r\n- Create a completely new conda environment\r\n- `pip install -e .`\r\n- Download `eval.zip` from Google Drive and unzip\r\n- Clone ScienceQA repo, and download images\r\n- run bash scripts/v1_5/eval/sqa.sh\r\n\r\nFirst, the split is test and not minitest, as you can see from the annotation file length.\r\n\r\n<img width=\"328\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/6631389/7cc02bf7-deab-4fad-b452-01880fd4a1c9\">\r\n\r\nSecond, comment out the first part of the `sqa.sh` and use the provided prediction file, do you reproduce the number below?\r\n\r\n> bash scripts/v1_5/eval/sqa.sh\r\n> Total: 4241, Correct: 3155, Accuracy: 74.39%, IMG-Accuracy: 71.64%\r\n\r\nThird, uncomment, delete the answers/results, and run inference and evaluation using `sqa.sh`\r\n<img width=\"1115\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/6631389/648b0e8e-a60c-43d9-8eed-9132821896a1\">\r\n\r\nNote that when I uploaded `eval.zip` results, I generated the predictions using A100, and tonight I evaluated using A6000, and the numbers are exactly the same, so I am pretty confident that the numbers are correct.\r\n\r\n@jiangsongtao @jameszhou-gl @yzbx \r\nCan you please check and let me know in what steps that may be different from the steps above?\n</Comment>\n<Comment by yzbx at 2023-10-27T06:44:27Z>\nfor llava-v1.5-13b, I reproduce the result of @jameszhou-gl\r\ntotal = 4241, Correct: 2982, Accuracy: 70.31%, Img-Accuracy: 70.85%, lower than 71.6% in paper.\r\nnote I use temperature=0 in `llava/eval/model_vqa_science.py`\r\n\r\n--model-path: liuhaotian/llava-v1.5-13b\r\n--question-file: llava_test_CQM-A.json,\r\nwith A800 GPUs.\r\n\r\n@haotian-liu\n</Comment>\n<Comment by haotian-liu at 2023-10-27T06:49:24Z>\n@yzbx Can you provide the full command you used? Including both the inference and evaluation.\n</Comment>\n<Comment by yzbx at 2023-10-27T06:57:57Z>\n@haotian-liu I use the follow scripts (I modified code to use 8 gpu).  \r\n\r\n```\r\nfor split in test\r\ndo\r\n    export TRANSFORMERS_OFFLINE=1\r\n    export PATH=/opt/conda/bin:$PATH\r\n    export PYTHONPATH=.\r\n    cd /mydata/git/LLaVA-main\r\n    prompt=\"CQM-A\"\r\n    model=13b\r\n    torchrun --nnodes 1 --nproc-per-node 8 --master-port 12356 llava/eval/model_vqa_science.py \\\r\n    --model-path /mydata/open_vlm/llava-v1.5-${model}/ \\\r\n    --question-file /mydata/dataset/ScienceQA/llava_${split}_${prompt}.json \\\r\n    --image-folder /mydata/dataset/ScienceQA/${split} \\\r\n    --answers-file results/ScienceQA/${split}_${prompt}_llava-${model}.jsonl \\\r\n    --single-pred-prompt \\\r\n    --temperature 0 \\\r\n    --conv-mode vicuna_v1 \\\r\n    --tmp-dir tmp/${split}\r\n    \r\n    python llava/eval/eval_science_qa.py \\\r\n        --base-dir /mydata/dataset/ScienceQA/ \\\r\n        --result-file results/ScienceQA/${split}_${prompt}_llava-${model}.jsonl \\\r\n        --output-file results/ScienceQA/${split}_${prompt}_llava-${model}_output.json \\\r\n        --output-result results/ScienceQA/${split}_${prompt}_llava-${model}_result.json \\\r\n        --split ${split}\r\ndone\r\n```\r\n\r\nNote: the source data `llava_test_CQM-A.json` is from [docs/ScienceQA.md](https://github.com/haotian-liu/LLaVA/blob/main/docs/ScienceQA.md), not from [eval.zip](https://drive.google.com/file/d/1atZSBBrAX54yYpxtVVW33zFvcnaHeFPy/view?usp=sharing)\r\n```\r\npython scripts/convert_sqa_to_llava.py \\\r\n    convert_to_llava \\\r\n    --base-dir /path/to/ScienceQA/data/scienceqa \\\r\n    --prompt-format \"CQM-A\" \\\r\n    --split test\r\n```\n</Comment>\n<Comment by haotian-liu at 2023-10-27T07:06:45Z>\nCan you try using the file from the eval.zip?\n</Comment>\n<Comment by yzbx at 2023-10-27T07:34:00Z>\n@haotian-liu I found the problem\r\n\r\n```\r\n# md5sum playground/data/eval/scienceqa/llava_test_CQM-A.json playground/data/scienceqa/llava_test_CQM-A.json \r\nb54d1eca943503d24bbed807b78f3d15  playground/data/eval/scienceqa/llava_test_CQM-A.json  # from eval.zip \r\n9776345f346d6082a5a31c0b291e6e21  playground/data/scienceqa/llava_test_CQM-A.json  # generate with scripts in docs/ScienceQA.md\r\n```\r\n\r\nuse the source data from [eval.zip](https://drive.google.com/file/d/1atZSBBrAX54yYpxtVVW33zFvcnaHeFPy/view?usp=sharing)\r\nI reproduce the result in paper\r\nTotal: 4241, Correct: 3155, Accuracy: 74.39%, IMG-Accuracy: 71.64%\n</Comment>\n<Comment by jiangsongtao at 2023-10-27T07:59:01Z>\nThanks for all your reply,i will close this issue\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 628,
    "state": "closed",
    "created_by": "ATaylorAerospace",
    "created_at": "2023-10-20T03:25:12Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/628</URL>\n\n<TITLE>[Question] GPT-4 Sample Prompts</TITLE>\n\n<BODY>### Question\n\nI was looking for a few sample GPT-4 prompts for detail description and complex reasoning questions but the link to the prompts folder @ https://github.com/haotian-liu/LLaVA/blob/main/docs/playground/data/prompts is not valid.\r\n\r\nCan someone point me to another location on Github or HF for sample prompts?\r\n\r\nThanks</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-20T03:30:53Z>\nThey are here: https://github.com/haotian-liu/LLaVA/tree/main/playground/data/prompts\r\n\r\nCan you share which docs point you to that link, so that I can fix it? Thanks.\n</Comment>\n<Comment by ATaylorAerospace at 2023-10-20T04:35:14Z>\nThanks so much for the update.\r\n\r\nHere is the doc that needs to be fixed\r\n\r\n**LLaVA/docs/Data.md**\r\n\r\n![datamd](https://github.com/haotian-liu/LLaVA/assets/112668339/2eaaa1cf-19db-4edb-9561-20f5da20bd43)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 627,
    "state": "open",
    "created_by": "sshh12",
    "created_at": "2023-10-20T01:15:39Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/627</URL>\n\n<TITLE>[Discussion] Extension of LLaVA to other domains</TITLE>\n\n<BODY>### Discussion\n\nHey y'all, I just wanted to share a library I wrote the based/inspired by LLaVA in case anyone finds it useful/interesting: https://github.com/sshh12/multi_token\r\n\r\nEssentially it generalizes the encoder + projector to make it easier to try the same method with custom modalities (audio/documents/etc).</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 626,
    "state": "closed",
    "created_by": "Carol-lyh",
    "created_at": "2023-10-20T01:04:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/626</URL>\n\n<TITLE>[Usage] Error when evaluate POPE</TITLE>\n\n<BODY>### Describe the issue\n\nThe error came out when evaluate on pope dataset:\r\n\r\nCategory: adversarial, # samples: 3000\r\nlabel_list = [json.loads(q)['label'] for q in open(label_file, 'r')]\r\nKeyError: 'label'\r\n\r\nActually, there are three categories, adversarial, popular, random, should I CHOOSE ONE or evaluate on all these three?</BODY>\n\n<COMMENTS>\n<Comment by Maxlinn at 2023-10-20T03:37:24Z>\ni reproduced the evaluation of POPE, POPE should be organized like\r\n```\r\nplayground/\r\n  data/\r\n     pope/\r\n        answers/\r\n        coco/\r\n        llava_pope_test.jsonl\r\n```\r\ncoco is fetched from official pope: https://github.com/AoiDragon/POPE/tree/e3e39262c85a6a83f26cf5094022a782cb0df58d/output/coco\r\n\r\nnote that llava-v1.5 seems to have used a older version of pope(checkout: e3e39262c85a6a83f26cf5094022a782cb0df58d), whose [coco_pope_random.json](https://github.com/AoiDragon/POPE/blob/e3e39262c85a6a83f26cf5094022a782cb0df58d/output/coco/coco_pope_random.json) have 2910 lines, but the latest pope has 3000 lines.\r\n\r\nthe final value of pope on llava paper seems to be averaged among `f1` of three categories. which is (0.8717759764185703 + 0.8628738147337709 + 0.8450000000000001) / 3 = 0.859883263717447\r\n \r\n```\r\nCategory: random, # samples: 2910 TP      FP      TN      FN 1183    31      1379    317 Accuracy: 0.8804123711340206 Precision: 0.9744645799011532 Recall: 0.7886666666666666 F1 score: 0.8717759764185703 Yes ratio: 0.41718213058419246 0.872, 0.880, 0.974, 0.789, 0.417 ==================================== \r\nCategory: popular, # samples: 3000 TP      FP      TN      FN 1183    59      1441    317 Accuracy: 0.8746666666666667 Precision: 0.9524959742351047 Recall: 0.7886666666666666 F1 score: 0.8628738147337709 Yes ratio: 0.414 0.863, 0.875, 0.952, 0.789, 0.414 ==================================== \r\nCategory: adversarial, # samples: 3000 TP      FP      TN      FN 1183    117     1383    317 Accuracy: 0.8553333333333333 Precision: 0.91 Recall: 0.7886666666666666 F1 score: 0.8450000000000001 Yes ratio: 0.43333333333333335 0.845, 0.855, 0.910, 0.789, 0.433 ====================================\r\n```\n</Comment>\n<Comment by Carol-lyh at 2023-10-20T05:11:28Z>\nlots of thanks!!!\n</Comment>\n<Comment by wanghao-cst at 2023-11-02T06:40:07Z>\nThe latest POPE one(3000 lines) random result(0.5017) does not match the previous one.\r\n<img width=\"337\" alt=\"截屏2023-11-02 14 39 39\" src=\"https://github.com/haotian-liu/LLaVA/assets/55015183/531c6eab-7a65-45dc-bc2c-7f739b14bda8\">\n</Comment>\n<Comment by simplelifetime at 2023-11-26T03:21:41Z>\n> The latest POPE one(3000 lines) random result(0.5017) does not match the previous one. <img alt=\"截屏2023-11-02 14 39 39\" width=\"337\" src=\"https://private-user-images.githubusercontent.com/55015183/279888188-531c6eab-7a65-45dc-bc2c-7f739b14bda8.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDA5NjkxNDAsIm5iZiI6MTcwMDk2ODg0MCwicGF0aCI6Ii81NTAxNTE4My8yNzk4ODgxODgtNTMxYzZlYWItN2E2NS00NWRjLWJjMmMtN2Y3MzliMTRiZGE4LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzExMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMTI2VDAzMjA0MFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWI3OGJkMGVmMzY5YjZmYjhmYjUwNzIzMTA2NTAyNWE5NGY4M2M2NTA0YjJkZTFlMTMyZTg1Yzc1ZWNiNDMwNWUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.S8iadzqDTYwrmu1wsGoYHCmkJgHfp-gE4TK2FEZYLhk\">\r\n\r\nI get the exact same result\n</Comment>\n<Comment by linhaojia13 at 2023-11-27T08:10:25Z>\n> i reproduced the evaluation of POPE, POPE should be organized like\r\n> \r\n> ```\r\n> playground/\r\n>   data/\r\n>      pope/\r\n>         answers/\r\n>         coco/\r\n>         llava_pope_test.jsonl\r\n> ```\r\n\r\nHi @Maxlinn , how could I get `llava_pope_test.jsonl`? I don't find it either in llava or POPE repo.\n</Comment>\n<Comment by darkpromise98 at 2024-01-10T07:03:37Z>\n> > The latest POPE one(3000 lines) random result(0.5017) does not match the previous one. <img alt=\"截屏2023-11-02 14 39 39\" width=\"337\" src=\"https://private-user-images.githubusercontent.com/55015183/279888188-531c6eab-7a65-45dc-bc2c-7f739b14bda8.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDA5NjkxNDAsIm5iZiI6MTcwMDk2ODg0MCwicGF0aCI6Ii81NTAxNTE4My8yNzk4ODgxODgtNTMxYzZlYWItN2E2NS00NWRjLWJjMmMtN2Y3MzliMTRiZGE4LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzExMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMTI2VDAzMjA0MFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWI3OGJkMGVmMzY5YjZmYjhmYjUwNzIzMTA2NTAyNWE5NGY4M2M2NTA0YjJkZTFlMTMyZTg1Yzc1ZWNiNDMwNWUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.S8iadzqDTYwrmu1wsGoYHCmkJgHfp-gE4TK2FEZYLhk\">\r\n> \r\n> I get the exact same result\r\n\r\nI also find the problem. the random-set (3000 lines) results of latest POPE will get wrong (50% accuracy). Because the latest random-set from [https://github.com/AoiDragon/POPE/blob/main/output/coco/coco_pope_random.json](url) has the different question_id orders with `llava_pope_test.jsonl`.\n</Comment>\n<Comment by yuezih at 2024-01-10T09:41:37Z>\nHi @Maxlinn, thanks for your sharing! \r\n\r\nCould you tell me where `llava_pope_test.jsonl` is obtained from? It seems that there is no `pope` directory in `playground/data` in the latest LLaVA repo, and I also checked several previous releases and couldn't find it either.\r\n\r\nThanks!\r\n\r\nbtw I just followed you on Zhihu yesterday, what a coincidence :)\n</Comment>\n<Comment by Maxlinn at 2024-01-10T09:57:56Z>\n> Hi @Maxlinn, thanks for your sharing!\r\n> \r\n> Could you tell me where `llava_pope_test.jsonl` is obtained from? It seems that there is no `pope` directory in `playground/data` in the latest LLaVA repo, and I also checked several previous releases and couldn't find it either.\r\n> \r\n> Thanks!\r\n> \r\n> btw I just followed you on Zhihu yesterday, what a coincidence :)\r\n\r\nthanks for your interest and sorry for the late reply for the former asks.\r\n\r\nfor `llava_pope_test.jsonl` , according to [the doc of llava v1.5 evaluation](https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md#scripts). The team has managed to convert all evaluation datasets into same format and then packed into [eval.zip](https://drive.google.com/file/d/1atZSBBrAX54yYpxtVVW33zFvcnaHeFPy/view?usp=sharing). `llava_pope_test.jsonl` was inside.\r\n\r\nfor pope, it should be noticed that llava 1.5 used [an older version of pope](https://github.com/AoiDragon/POPE/tree/e3e39262c85a6a83f26cf5094022a782cb0df58d/output/coco) (found here https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md#pope). it has around 2990 items in `random` subset while the latest version has 3000. i verified the pope scores in the paper using given version.\n</Comment>\n<Comment by cookiesupers22 at 2024-11-09T22:25:56Z>\nHey @Maxlinn,\r\nJust to ask a quick follow up. I'm getting the same 0.5017 accuracy as the person above me did on the random subset. So to fix this what do you think I should do? I used the random subset from the old version of Pope with exactly 2910 samples. I got this:\r\nCategory: random, # samples: 2910\r\nTP\tFP\tTN\tFN\t\r\n609\t604\t851\t846\r\nAccuracy: 0.5017182130584192\r\n\r\nAny help would be seriously appreciated 🙏🙏🙏\n</Comment>\n<Comment by eziohzy at 2025-06-27T08:26:40Z>\n> > Hi [@Maxlinn](https://github.com/Maxlinn), thanks for your sharing!\n> > Could you tell me where `llava_pope_test.jsonl` is obtained from? It seems that there is no `pope` directory in `playground/data` in the latest LLaVA repo, and I also checked several previous releases and couldn't find it either.\n> > Thanks!\n> > btw I just followed you on Zhihu yesterday, what a coincidence :)\n> \n> thanks for your interest and sorry for the late reply for the former asks.\n> \n> for `llava_pope_test.jsonl` , according to [the doc of llava v1.5 evaluation](https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md#scripts). The team has managed to convert all evaluation datasets into same format and then packed into [eval.zip](https://drive.google.com/file/d/1atZSBBrAX54yYpxtVVW33zFvcnaHeFPy/view?usp=sharing). `llava_pope_test.jsonl` was inside.\n> \n> for pope, it should be noticed that llava 1.5 used [an older version of pope](https://github.com/AoiDragon/POPE/tree/e3e39262c85a6a83f26cf5094022a782cb0df58d/output/coco) (found here https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md#pope). it has around 2990 items in `random` subset while the latest version has 3000. i verified the pope scores in the paper using given version.\n\nYour comments should be put in the official readme; by the way, I also need to download the val2014.zip dataset, right?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 625,
    "state": "closed",
    "created_by": "Carol-lyh",
    "created_at": "2023-10-19T13:29:26Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/625</URL>\n\n<TITLE>[Usage] Error when evaluate GQA</TITLE>\n\n<BODY>### Describe the issue\n\nI've commented scenes graphs since it does not exist. However, there is an error that:\r\n\r\nException: Can't find testdev_balanced_all_questions.json\r\n\r\nI checked the questions dir, and only find these two:\r\ntestdev_all_questions.json, testdev_balanced_questions.json\r\n\r\nIs there anything wrong with the gqa eval.py? Should I change it to testdev_balanced_questions.json?</BODY>\n\n<COMMENTS>\n<Comment by Carol-lyh at 2023-10-19T13:31:33Z>\nAlso, there is another error:\r\n\r\nException: Can't find testdev_balanced_choices.json\n</Comment>\n<Comment by miznchimaki at 2023-10-20T02:43:04Z>\njust comment that line out, i.e. `choices = loadFile(args.choices.format(tier = args.tier))`\r\nAfter commenting it out, maybe you will get another error and it is in the function named `updateConsistency`\r\nthen I commented out the function call of `updateConsistency`, finally it worked.\n</Comment>\n<Comment by Carol-lyh at 2023-10-20T03:03:07Z>\n> just comment that line out, i.e. `choices = loadFile(args.choices.format(tier = args.tier))` After commenting it out, maybe you will get another error and it is in the function named `updateConsistency` then I commented out the function call of `updateConsistency`, finally it worked.\r\n\r\nThank you!\r\nExcuse me, how can you fix this problem?\r\n**Exception: Can't find testdev_balanced_all_questions.json**\r\n\r\nI've checked the questions dir, and only find these two:\r\n**testdev_all_questions.json, testdev_balanced_questions.json**\n</Comment>\n<Comment by miznchimaki at 2023-10-20T04:50:51Z>\n> > just comment that line out, i.e. `choices = loadFile(args.choices.format(tier = args.tier))` After commenting it out, maybe you will get another error and it is in the function named `updateConsistency` then I commented out the function call of `updateConsistency`, finally it worked.\r\n> \r\n> Thank you! Excuse me, how can you fix this problem? **Exception: Can't find testdev_balanced_all_questions.json**\r\n> \r\n> I've checked the questions dir, and only find these two: **testdev_all_questions.json, testdev_balanced_questions.json**\r\n\r\nI made a soft link named `testdev_balanced_all_questions.json`, pointing to `testdev_balanced_questions.json`.\n</Comment>\n<Comment by Carol-lyh at 2023-10-20T05:26:07Z>\n> just comment that line out, i.e. `choices = loadFile(args.choices.format(tier = args.tier))` After commenting it out, maybe you will get another error and it is in the function named `updateConsistency` then I commented out the function call of `updateConsistency`, finally it worked.\r\n\r\nAlso, has another question that, I think the `choices` param is required in evaluating performances. Since it's used in many places in `eval.py`, not just in the function call of `updateConsistency`.\n</Comment>\n<Comment by miznchimaki at 2023-10-20T06:32:40Z>\n> > just comment that line out, i.e. `choices = loadFile(args.choices.format(tier = args.tier))` After commenting it out, maybe you will get another error and it is in the function named `updateConsistency` then I commented out the function call of `updateConsistency`, finally it worked.\r\n> \r\n> Also, has another question that, I think the `choices` param is required in evaluating performances. Since it's used in many places in `eval.py`, not just in the function call of `updateConsistency`.\r\n\r\nmaybe you're right and there exists some problems with my practice, but when I evaluated the 13B LLaVA-v1.5 ckpt which I reproduced, I only cared about the `accuracy` metric of GQA benchmark. Below is my reproduction results on GQA (62.60% vs 63.3% in llava-v1.5 paper. It seems that it has no effects on `accuracy` metric, because I also try it on the author's ckpt on hugging face):\r\nBinary: 79.68%\r\nOpen: 48.11%\r\nAccuracy: 62.60%\r\nDistribution: 1.56 (lower is better)\r\n\r\nAccuracy / structural type:\r\n  choose: 83.97% (1129 questions)\r\n  compare: 64.18% (589 questions)\r\n  logical: 77.81% (1803 questions)\r\n  query: 48.11% (6805 questions)\r\n  verify: 83.08% (2252 questions)\r\n\r\nAccuracy / semantic type:\r\n  attr: 69.17% (5186 questions)\r\n  cat: 52.65% (1149 questions)\r\n  global: 61.78% (157 questions)\r\n  obj: 88.43% (778 questions)\r\n  rel: 54.58% (5308 questions)\r\n\r\nAccuracy / steps number:\r\n  1: 79.75% (237 questions)\r\n  2: 57.47% (6395 questions)\r\n  3: 64.81% (4266 questions)\r\n  4: 69.48% (793 questions)\r\n  5: 77.25% (822 questions)\r\n  6: 87.80% (41 questions)\r\n  7: 95.00% (20 questions)\r\n  8: 100.00% (3 questions)\r\n  9: 100.00% (1 questions)\r\n\r\nAccuracy / words number:\r\n  3: 39.74% (151 questions)\r\n  4: 56.51% (630 questions)\r\n  5: 50.85% (1290 questions)\r\n  6: 59.16% (2074 questions)\r\n  7: 63.28% (1642 questions)\r\n  8: 62.95% (1185 questions)\r\n  9: 67.37% (1281 questions)\r\n  10: 68.45% (1249 questions)\r\n  11: 63.78% (994 questions)\r\n  12: 67.40% (638 questions)\r\n  13: 64.50% (462 questions)\r\n  14: 70.72% (345 questions)\r\n  15: 70.04% (237 questions)\r\n  16: 78.63% (117 questions)\r\n  17: 70.21% (94 questions)\r\n  18: 77.63% (76 questions)\r\n  19: 79.07% (43 questions)\r\n  20: 71.88% (32 questions)\r\n  21: 68.42% (19 questions)\r\n  22: 75.00% (12 questions)\r\n  23: 25.00% (4 questions)\r\n  24: 100.00% (2 questions)\r\n  25: 100.00% (1 questions)\r\n  The content above is all my own practice and opinions, I can't make sure that they're all correct.\n</Comment>\n<Comment by Carol-lyh at 2023-10-20T06:43:24Z>\nThank you so much for discussing the results with me! Through I still have problems in evaluating this benchmark, and I haven't figure this out. I've learned a lot from your experience! Lots of thanks !!!\n</Comment>\n<Comment by Carol-lyh at 2023-10-20T06:50:13Z>\nAnd after comment out all the usage in `choices`, actually I GET the accuracy metric. You're right. Because  I only cared about the `accuracy` metric of GQA benchmark. \r\nMany many thanks\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 623,
    "state": "closed",
    "created_by": "liuheng0111",
    "created_at": "2023-10-19T10:24:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/623</URL>\n\n<TITLE>[Question] region changed after expand2square</TITLE>\n\n<BODY>### Question\n\nI find the region of box has changed when image after expand2square, for example:\r\nquesiton: Please provide the bounding box coordinate of the region this sentence describes: a man bent over behind the cigarette.\r\nimage: coco/train2017/000000314530.jpg\r\nbbox = [0.61, 0.36, 0.92, 0.83]\r\nthe origin image: \r\n![image](https://github.com/haotian-liu/LLaVA/assets/18352727/b70e07ed-572d-4a0f-bd05-d59a37a7ea90)\r\n\r\nafter expand2square, the image with bbox: \r\n![image](https://github.com/haotian-liu/LLaVA/assets/18352727/2407b495-4666-4ca0-8c39-2576df6916c9)\r\n\r\nAfter expand2square, the region box is not the correct answer for the question. Does this have a big impact on the training effect?</BODY>\n\n<COMMENTS>\n<Comment by Deaddawn at 2023-10-19T10:34:54Z>\n> ### Question\r\n> I find the region of box has changed when image after expand2square, for example: quesiton: Please provide the bounding box coordinate of the region this sentence describes: a man bent over behind the cigarette. image: coco/train2017/000000314530.jpg bbox = [0.61, 0.36, 0.92, 0.83] the origin image: ![image](https://user-images.githubusercontent.com/18352727/276577750-b70e07ed-572d-4a0f-bd05-d59a37a7ea90.png)\r\n> \r\n> after expand2square, the image with bbox: ![image](https://user-images.githubusercontent.com/18352727/276577989-2407b495-4666-4ca0-8c39-2576df6916c9.png)\r\n> \r\n> After expand2square, the region box is not the correct answer for the question. Does this have a big impact on the training effect?\r\n\r\nHi, I have encountered the same problem, do you know what normalization this '[0.61, 0.36, 0.92, 0.83]' has made? say the original picture is 336*336?\n</Comment>\n<Comment by liuheng0111 at 2023-10-19T10:38:04Z>\nthe '[0.61, 0.36, 0.92, 0.83]' made by the original image shape, this case is (640 427)\n</Comment>\n<Comment by Deaddawn at 2023-10-19T11:01:36Z>\n> the '[0.61, 0.36, 0.92, 0.83]' made by the original image shape, this case is (640 427)\r\n\r\nOh, got it, so how do we get the original bounding box using [0.61, 0.36, 0.92, 0.83] and (640 427)?\n</Comment>\n<Comment by egmaminta at 2023-10-24T01:53:03Z>\nHi! Same experience here. The bounding box regions changed/shifted when padding is used. Any tips on how to correct the bounding box after padding?\n</Comment>\n<Comment by haotian-liu at 2023-10-24T01:57:19Z>\nHi regarding the bounding box coordinates, please see the visualizations in https://github.com/haotian-liu/LLaVA/issues/606#issuecomment-1774076879\r\n\r\nWe'll add further clarifications of how we define the relative bounding box coordinates in the revision, thank you.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 622,
    "state": "open",
    "created_by": "July-zh",
    "created_at": "2023-10-19T10:06:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/622</URL>\n\n<TITLE>[Question] Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint</TITLE>\n\n<BODY>### Question\n\nWhen I use the weights of my trained IT phase model for evaluation, I encounter the following issue. However, when I use publicly available model weights, I don't face this problem. Is this normal?\r\nSome weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at...\r\n![image](https://github.com/haotian-liu/LLaVA/assets/39486987/8e3ab776-14b8-4769-bc77-9f8b178f4062)</BODY>\n\n<COMMENTS>\n<Comment by xiechengmude at 2023-10-23T12:42:22Z>\nThe same hint for me when I load my sft llama2 model.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 621,
    "state": "closed",
    "created_by": "SetoKaiba",
    "created_at": "2023-10-19T08:59:56Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/621</URL>\n\n<TITLE>[Question] License question for LLaVA 1.5</TITLE>\n\n<BODY>### Question\n\nThe previous one is not available for commercial use due to the Vicuna license. But the 1.5 is based on llama2. Is LLaVA 1.5 available for commercial use? Thank you.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-24T01:02:45Z>\nThey are released under the llama2 community license.\n</Comment>\n<Comment by papasanimohansrinivas at 2023-10-29T07:54:03Z>\nJust a quick question @haotian-liu are weights for llava 1.5 are open to commercial use and any of the datasets especially for training are all permitted for commercial use ?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 620,
    "state": "open",
    "created_by": "created-Bi",
    "created_at": "2023-10-19T08:27:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/620</URL>\n\n<TITLE>[Question] If I cannot download llama-2-7b-chat model, how can I finetune the model?</TITLE>\n\n<BODY>### Question\n\nI haven't obtained the approval of llama-2-7b-chat, how can I finetune the LLaVA?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 619,
    "state": "open",
    "created_by": "Deaddawn",
    "created_at": "2023-10-19T07:54:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/619</URL>\n\n<TITLE>[Question] About the bounding box of the VG data</TITLE>\n\n<BODY>### Question\n\nHi, there. How do you normalize the bbox of the vg data? Could you tell more about the detail? Thanks a lot</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 618,
    "state": "open",
    "created_by": "CthulhuAIFrenzy",
    "created_at": "2023-10-19T07:29:14Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/618</URL>\n\n<TITLE>[Question] vqav2 evaluation</TITLE>\n\n<BODY>### Question\n\nIn the `merge.jsonl` file located at `playground/vqav2/answers/llava_vqav2_mscoco_test-dev2015/llava-v1.5-13b/merge.jsonl`, there are some results as follows:\r\n\r\n{\"question_id\": 507602002, \"prompt\": \"Which letter of the alphabet can you see in this photo?\\nAnswer the question using a single word or phrase.\", \"text\": \"A\", \"answer_id\": \"dqZXSU8RCU8sb7QhsKPjfN\", \"model_id\": \"llava-v1.5-13b\", \"metadata\": {}}\r\n{\"question_id\": 258997004, \"prompt\": \"What button is broken?\\nAnswer the question using a single word or phrase.\", \"text\": \"A\", \"answer_id\": \"jepZiw8QuBBFb2oGZrqHsZ\", \"model_id\": \"llava-v1.5-13b\", \"metadata\": {}}\r\n{\"question_id\": 340768004, \"prompt\": \"What is the letter by the door?\\nAnswer the question using a single word or phrase.\", \"text\": \"A\", \"answer_id\": \"mugpUWgmFxnYBdwxpAoZ6T\", \"model_id\": \"llava-v1.5-13b\", \"metadata\": {}}\r\n{\"question_id\": 527385001, \"prompt\": \"What is the first letter of the plane?\\nAnswer the question using a single word or phrase.\", \"text\": \"A\", \"answer_id\": \"WawggtdFg7NTvX2qGcNWLV\", \"model_id\": \"llava-v1.5-13b\", \"metadata\": {}}\r\n\r\nThe answer in each case is 'A'. During the conversion from `merge.jsonl` to the answer upload result, the `EvalAIAnswerProcessor` is used to process each answer. One of the operations is that if the answer is only 'A', it will be converted to an empty answer. Is this operation correct?\r\n\r\n```python\r\nARTICLES = [\"a\", \"an\", \"the\"]\r\ndef process_digit_article(self, in_text):\r\n    out_text = []\r\n    temp_text = in_text.lower().split()\r\n    for word in temp_text:\r\n        word = self.NUMBER_MAP.setdefault(word, word)\r\n        if word not in self.ARTICLES:\r\n            out_text.append(word)\r\n        else:\r\n            pass\r\n    for word_id, word in enumerate(out_text):\r\n        if word in self.CONTRACTIONS:\r\n            out_text[word_id] = self.CONTRACTIONS[word]\r\n    out_text = \" \".join(out_text)\r\n    return out_text\r\n```\r\nThis problem was discovered when I submitted test-dev2015. I saw that there were 107394 answers in the source test-dev2015. After converting to the answer upload, only 107380 answer was not an empty string in the json file.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 617,
    "state": "closed",
    "created_by": "WilTay1",
    "created_at": "2023-10-19T07:17:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/617</URL>\n\n<TITLE>Great work! Is it possible for the model to receive only query/text without input image?</TITLE>\n\n<BODY>### Question\r\n\r\nIn your demo, it seems that it is possible for the model to receive only query, and it works like a normal LLM. How to achieve that in the script? Thanks!</BODY>\n\n<COMMENTS>\n<Comment by ninjacode01 at 2024-01-11T12:47:44Z>\n@WilTay1 Did you find any solution to this? I want to further fine-tune the text only part of the model using LoRa, without utilizing its multimodal ability. Also, the data is comparatively smaller and educational in nature, so finetuning on vicuna might not give better results.\r\nThanks!\n</Comment>\n<Comment by lzy37ld at 2024-03-30T12:02:44Z>\n@WilTay1 I am also interested in this question! Any insight there?\n</Comment>\n<Comment by copperwiring at 2024-04-18T17:50:09Z>\nI think it might work as per discussion here: https://github.com/haotian-liu/LLaVA/issues/840 and used is science qa evals here: https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/model_vqa_science.py#L58\n</Comment>\n<Comment by Tree-Shu-Zhao at 2024-11-27T22:11:32Z>\n@ninjacode01 Hi! May I ask if the performance dropped after fine-tuning on a text-only dataset? I have multimodal/text mixed samples. After fine-tuning, the performance of text-only queries significantly dropped. I'd like to know if you got similar results.\n</Comment>\n<Comment by zhanghuiecho at 2025-03-03T11:40:48Z>\n> [@ninjacode01](https://github.com/ninjacode01) Hi! May I ask if the performance dropped after fine-tuning on a text-only dataset? I have multimodal/text mixed samples. After fine-tuning, the performance of text-only queries significantly dropped. I'd like to know if you got similar results.\n\nhi @Tree-Shu-Zhao ，i want  to know how do you complete text-only finetune, i want to do it, but i have no idea about it.\n</Comment>\n<Comment by Tree-Shu-Zhao at 2025-03-03T17:15:11Z>\n@zhanghuiecho I was working on instruction tuning with mixed data (image-question - > response and question -> response). If you would like to fine-tune with text-only data, follow [this document](https://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md) to prepare your data, and set the `image` field to `None` or simply remove it.\n\nHope this helps\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 615,
    "state": "closed",
    "created_by": "linzhiqiu",
    "created_at": "2023-10-19T04:25:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/615</URL>\n\n<TITLE>[Question] Why are image tokens always prepended at start?</TITLE>\n\n<BODY>### Question\n\nIn [train.py](llava/train/train.py), there is a preprocess_multimodal() function that will be executed for every sample. However, this function seems to place the image tokens at the start of every sequence. This doesn't align with the paper description where it says the image token will be placed at start or end of the sequence for 50% of the time:\r\n\r\n```\r\ndef preprocess_multimodal(\r\n    sources: Sequence[str],\r\n    data_args: DataArguments\r\n) -> Dict:\r\n    is_multimodal = data_args.is_multimodal\r\n    if not is_multimodal:\r\n        return sources\r\n\r\n    for source in sources:\r\n        for sentence in source:\r\n            if DEFAULT_IMAGE_TOKEN in sentence['value']:\r\n                sentence['value'] = sentence['value'].replace(DEFAULT_IMAGE_TOKEN, '').strip()\r\n                sentence['value'] = DEFAULT_IMAGE_TOKEN + '\\n' + sentence['value']\r\n                sentence['value'] = sentence['value'].strip()\r\n                if \"mmtag\" in conversation_lib.default_conversation.version:\r\n                    sentence['value'] = sentence['value'].replace(DEFAULT_IMAGE_TOKEN, '<Image>' + DEFAULT_IMAGE_TOKEN + '</Image>')\r\n            replace_token = DEFAULT_IMAGE_TOKEN\r\n            if data_args.mm_use_im_start_end:\r\n                replace_token = DEFAULT_IM_START_TOKEN + replace_token + DEFAULT_IM_END_TOKEN\r\n            sentence[\"value\"] = sentence[\"value\"].replace(DEFAULT_IMAGE_TOKEN, replace_token)\r\n\r\n    return sources\r\n```</BODY>\n\n<COMMENTS>\n<Comment by linzhiqiu at 2023-10-19T05:33:33Z>\nAlso I want to confirm -- it seems for 1st stage pre-training, the USER message is removed in preprocess_plain() function? Therefore, the 1st pretraining is now done with sequences like: '\\<image\\>floor plan of cottage type house, 1 bedroom\\n'\r\n\r\n```\r\ndef preprocess_plain(\r\n    sources: Sequence[str],\r\n    tokenizer: transformers.PreTrainedTokenizer,\r\n) -> Dict:\r\n    # add end signal and concatenate together\r\n    conversations = []\r\n    for source in sources:\r\n        assert len(source) == 2\r\n        assert DEFAULT_IMAGE_TOKEN in source[0]['value']\r\n        source[0]['value'] = DEFAULT_IMAGE_TOKEN\r\n        conversation = source[0]['value'] + source[1]['value'] + conversation_lib.default_conversation.sep\r\n        conversations.append(conversation)\r\n    # tokenize conversations\r\n    input_ids = [tokenizer_image_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations]\r\n    targets = copy.deepcopy(input_ids)\r\n    for target, source in zip(targets, sources):\r\n        tokenized_len = len(tokenizer_image_token(source[0]['value'], tokenizer))\r\n        target[:tokenized_len] = IGNORE_INDEX\r\n\r\n    return dict(input_ids=input_ids, labels=targets)\r\n ```\n</Comment>\n<Comment by linzhiqiu at 2023-10-24T04:15:08Z>\n@haotian-liu Would be very helpful if you could help confirm my above two observations :-)\r\n\r\n1 - \\<image\\> token is currently always prepended at the start of USER message for stage-2 training\r\n2 - The instruction for image description is removed for stage-1 training.\n</Comment>\n<Comment by haotian-liu at 2023-10-24T04:21:59Z>\nHi @linzhiqiu \r\n\r\nThank you for your careful observation and they are both correct.\r\n\r\nWe iteratively removed some components that ultimately does not much affect the model's performance, over time to make the approach simpler. The change regarding the tokenization is documented [here](https://github.com/haotian-liu/LLaVA/releases/tag/v1.0.1).\r\n\r\nWe will also document these changes in the revision, thanks.\n</Comment>\n<Comment by linzhiqiu at 2023-10-25T03:14:41Z>\nThank you Haotian!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 614,
    "state": "open",
    "created_by": "ai1361720220000",
    "created_at": "2023-10-19T03:12:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/614</URL>\n\n<TITLE>training error[Question]</TITLE>\n\n<BODY>### Question\r\n\r\nI followed the finetune.sh, and the difference is that\r\nI removed \"--deepspeed ./scripts/zero3.json\" and made \"--group_by_modality_length\", \"False\". \r\n\r\nOriginal Traceback (most recent call last):\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\r\n    output = module(*input, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/notebook/data/personal/80303875/LLaVA/llava/model/language_model/llava_llama.py\", line 78, in forward\r\n    outputs = self.model(\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 685, in forward\r\n    layer_outputs = torch.utils.checkpoint.checkpoint(\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\r\n    return CheckpointFunction.apply(function, preserve, *args)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\r\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 107, in forward\r\n    outputs = run_function(*args)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 681, in custom_forward\r\n    return module(*inputs, output_attentions, None)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 408, in forward\r\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/notebook/data/personal/80303875/LLaVA/llava/train/llama_flash_attn_monkey_patch.py\", line 87, in forward\r\n    output_unpad = flash_attn_unpadded_qkvpacked_func(\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py\", line 406, in flash_attn_varlen_qkvpacked_func\r\n    return FlashAttnVarlenQKVPackedFunc.apply(\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\r\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py\", line 123, in forward\r\n    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_varlen_forward(\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py\", line 52, in _flash_attn_varlen_forward\r\n    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.varlen_fwd(\r\nRuntimeError: FlashAttention only support fp16 and bf16 data type</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-19T03:32:55Z>\nPlease do not remove the deepspeed config, as it is crucial to ensure the proper distributed training settings.\n</Comment>\n<Comment by ai1361720220000 at 2023-10-19T06:44:02Z>\nIt is ok. Thanks a lot!! But i found occasionally there will be an error when running, but it will be successful after re-running a few times.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 613,
    "state": "closed",
    "created_by": "July-zh",
    "created_at": "2023-10-19T02:54:56Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/613</URL>\n\n<TITLE>[Question] Can you make the training log public?</TITLE>\n\n<BODY>### Question\n\nCan you make the training log public?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-19T03:35:18Z>\nThank you for your interest in our research! In our commitment to making our research fully transparent, we're pleased to release the wandb logs [here](https://api.wandb.ai/links/lht/6orh56wc).\r\n\r\nWe also put them in [MODEL ZOO](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#llava-v15) so that people can find them more easily.\n</Comment>\n<Comment by July-zh at 2023-10-19T03:39:52Z>\n> Thank you for your interest in our research! In our commitment to making our research fully transparent, we're pleased to release the wandb logs [here](https://api.wandb.ai/links/lht/6orh56wc).\r\n> \r\n> We also put them in [MODEL ZOO](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#llava-v15) so that people can find them more easily.\r\n\r\nThank you for doing such a great job!\n</Comment>\n<Comment by yuyq96 at 2023-10-24T08:27:52Z>\n@haotian-liu This url only contains the logs of the 13B model. Can you please provide the logs of the 7B model?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 612,
    "state": "closed",
    "created_by": "Njasa2k",
    "created_at": "2023-10-18T23:41:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/612</URL>\n\n<TITLE>[Question]  What are the results for image captioning for LLaVA in comparison to other models?</TITLE>\n\n<BODY>### Question\n\nWhat are the results for image captioning for LLaVA in comparison to other models?\r\nhttps://paperswithcode.com/task/image-captioning</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 611,
    "state": "closed",
    "created_by": "barshag",
    "created_at": "2023-10-18T21:10:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/611</URL>\n\n<TITLE>[Question] what is the resolution that the inference of the image occur?</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-19T03:58:03Z>\nFor LLaVA 1.5 it is 336x336px.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 610,
    "state": "open",
    "created_by": "ltttpku",
    "created_at": "2023-10-18T16:54:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/610</URL>\n\n<TITLE>[Question] Finetune error.</TITLE>\n\n<BODY>### Question\n\nHi, thanks for your great work!\r\n\r\nWhen I finetune the model on my own custome dataset, I encountered the following error:\r\n\r\n```\r\n[2023-10-19 00:51:08,556] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-10-19 00:51:10,671] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\n[2023-10-19 00:51:10,672] [INFO] [runner.py:555:main] cmd = /home/leiting/.conda/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMl19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path /network_space/storage43/lttt/huggingface/llava-v1.5-7b --version v1 --data_path ./playground/data/IKCEST/ft_data.json --image_folder ./playground/data --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length False --bf16 True --output_dir ./checkpoints/llava-v1.5-7b --num_train_epochs 1 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 500 --save_total_limit 3 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --freeze_backbone\r\n[2023-10-19 00:51:11,932] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-10-19 00:51:14,020] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [2]}\r\n[2023-10-19 00:51:14,020] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0\r\n[2023-10-19 00:51:14,020] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\r\n[2023-10-19 00:51:14,020] [INFO] [launch.py:163:main] dist_world_size=1\r\n[2023-10-19 00:51:14,020] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=2\r\n[2023-10-19 00:51:16,892] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-10-19 00:51:17,554] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-10-19 00:51:17,554] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-10-19 00:51:17,554] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n[2023-10-19 00:51:20,490] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 6.76B parameters\r\nLoading checkpoint shards: 100%|██████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.14s/it]\r\n[2023-10-19 00:51:33,422] [WARNING] [partition_parameters.py:836:_post_init_method] param `class_embedding` in CLIPVisionEmbeddings not on GPU so was not broadcasted from rank 0\r\n[2023-10-19 00:51:33,569] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 7.06B parameters\r\nFormatting inputs...Skip in lazy mode\r\nParameter Offload: Total persistent parameters: 599040 in 312 params\r\nTraceback (most recent call last):\r\n  File \"/home/leiting/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/home/leiting/LLaVA/llava/train/train.py\", line 926, in train\r\n    trainer.train(resume_from_checkpoint=True)\r\n  File \"/home/leiting/.conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/home/leiting/.conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1676, in _inner_training_loop\r\n    deepspeed_load_checkpoint(self.model_wrapped, resume_from_checkpoint)\r\n  File \"/home/leiting/.conda/envs/llava/lib/python3.10/site-packages/transformers/deepspeed.py\", line 383, in deepspeed_load_checkpoint\r\n    load_path, _ = deepspeed_engine.load_checkpoint(\r\n  File \"/home/leiting/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2604, in load_checkpoint\r\n    load_path, client_states = self._load_checkpoint(load_dir,\r\n  File \"/home/leiting/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2663, in _load_checkpoint\r\n    self.load_module_state_dict(checkpoint=checkpoint,\r\n  File \"/home/leiting/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2480, in load_module_state_dict\r\n    param.ds_tensor.data.copy_(saved_frozen_params[name].data)\r\nRuntimeError: The size of tensor a (131072000) must match the size of tensor b (21845334) at non-singleton dimension 0\r\n[2023-10-19 00:51:38,046] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 89082\r\n[2023-10-19 00:51:38,046] [ERROR] [launch.py:321:sigkill_handler] ['/home/leiting/.conda/envs/llava/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', '/network_space/storage43/lttt/huggingface/llava-v1.5-7b', '--version', 'v1', '--data_path', './playground/data/IKCEST/ft_data.json', '--image_folder', './playground/data', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'False', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-7b', '--num_train_epochs', '1', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '3', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--freeze_backbone'] exits with return code = 1\r\n```\r\n\r\nHere's the script that I used for finetuning:\r\n```\r\n#!/bin/bash\r\n\r\ndeepspeed --include localhost:2 llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path /network_space/storage43/lttt/huggingface/llava-v1.5-7b \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/IKCEST/ft_data.json \\\r\n    --image_folder ./playground/data \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 500 \\\r\n    --save_total_limit 3 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb \\\r\n    --freeze_backbone\r\n```\r\n\r\nAny idea on why this could happen?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-20T03:26:36Z>\nIt seems that you have some model checkpoints already saved in your experiment dir, as \"--save_steps 500\" is quite easy to reach, and you changed something in the configuration that is different from that previous run. It thus fails in automatically loading the checkpoint.\n</Comment>\n<Comment by nj159 at 2023-10-23T12:15:59Z>\n> ### 问题\r\n> 嗨，感谢您的出色工作！\r\n> \r\n> 当我在自己的自定义数据集上微调模型时，我遇到了以下错误：\r\n> \r\n> ```\r\n> [2023-10-19 00:51:08,556] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n> [2023-10-19 00:51:10,671] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\n> [2023-10-19 00:51:10,672] [INFO] [runner.py:555:main] cmd = /home/leiting/.conda/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMl19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path /network_space/storage43/lttt/huggingface/llava-v1.5-7b --version v1 --data_path ./playground/data/IKCEST/ft_data.json --image_folder ./playground/data --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length False --bf16 True --output_dir ./checkpoints/llava-v1.5-7b --num_train_epochs 1 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 500 --save_total_limit 3 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --freeze_backbone\r\n> [2023-10-19 00:51:11,932] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n> [2023-10-19 00:51:14,020] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [2]}\r\n> [2023-10-19 00:51:14,020] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0\r\n> [2023-10-19 00:51:14,020] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\r\n> [2023-10-19 00:51:14,020] [INFO] [launch.py:163:main] dist_world_size=1\r\n> [2023-10-19 00:51:14,020] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=2\r\n> [2023-10-19 00:51:16,892] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n> [2023-10-19 00:51:17,554] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n> [2023-10-19 00:51:17,554] [INFO] [comm.py:594:init_distributed] cdb=None\r\n> [2023-10-19 00:51:17,554] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n> [2023-10-19 00:51:20,490] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 6.76B parameters\r\n> Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.14s/it]\r\n> [2023-10-19 00:51:33,422] [WARNING] [partition_parameters.py:836:_post_init_method] param `class_embedding` in CLIPVisionEmbeddings not on GPU so was not broadcasted from rank 0\r\n> [2023-10-19 00:51:33,569] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 7.06B parameters\r\n> Formatting inputs...Skip in lazy mode\r\n> Parameter Offload: Total persistent parameters: 599040 in 312 params\r\n> Traceback (most recent call last):\r\n>   File \"/home/leiting/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n>     train()\r\n>   File \"/home/leiting/LLaVA/llava/train/train.py\", line 926, in train\r\n>     trainer.train(resume_from_checkpoint=True)\r\n>   File \"/home/leiting/.conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n>     return inner_training_loop(\r\n>   File \"/home/leiting/.conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1676, in _inner_training_loop\r\n>     deepspeed_load_checkpoint(self.model_wrapped, resume_from_checkpoint)\r\n>   File \"/home/leiting/.conda/envs/llava/lib/python3.10/site-packages/transformers/deepspeed.py\", line 383, in deepspeed_load_checkpoint\r\n>     load_path, _ = deepspeed_engine.load_checkpoint(\r\n>   File \"/home/leiting/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2604, in load_checkpoint\r\n>     load_path, client_states = self._load_checkpoint(load_dir,\r\n>   File \"/home/leiting/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2663, in _load_checkpoint\r\n>     self.load_module_state_dict(checkpoint=checkpoint,\r\n>   File \"/home/leiting/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2480, in load_module_state_dict\r\n>     param.ds_tensor.data.copy_(saved_frozen_params[name].data)\r\n> RuntimeError: The size of tensor a (131072000) must match the size of tensor b (21845334) at non-singleton dimension 0\r\n> [2023-10-19 00:51:38,046] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 89082\r\n> [2023-10-19 00:51:38,046] [ERROR] [launch.py:321:sigkill_handler] ['/home/leiting/.conda/envs/llava/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', '/network_space/storage43/lttt/huggingface/llava-v1.5-7b', '--version', 'v1', '--data_path', './playground/data/IKCEST/ft_data.json', '--image_folder', './playground/data', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'False', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-7b', '--num_train_epochs', '1', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '3', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--freeze_backbone'] exits with return code = 1\r\n> ```\r\n> \r\n> 这是我用于微调的脚本：\r\n> \r\n> ```\r\n> #!/bin/bash\r\n> \r\n> deepspeed --include localhost:2 llava/train/train_mem.py \\\r\n>     --deepspeed ./scripts/zero3.json \\\r\n>     --model_name_or_path /network_space/storage43/lttt/huggingface/llava-v1.5-7b \\\r\n>     --version v1 \\\r\n>     --data_path ./playground/data/IKCEST/ft_data.json \\\r\n>     --image_folder ./playground/data \\\r\n>     --vision_tower openai/clip-vit-large-patch14-336 \\\r\n>     --mm_projector_type mlp2x_gelu \\\r\n>     --mm_vision_select_layer -2 \\\r\n>     --mm_use_im_start_end False \\\r\n>     --mm_use_im_patch_token False \\\r\n>     --image_aspect_ratio pad \\\r\n>     --group_by_modality_length False \\\r\n>     --bf16 True \\\r\n>     --output_dir ./checkpoints/llava-v1.5-7b \\\r\n>     --num_train_epochs 1 \\\r\n>     --per_device_train_batch_size 4 \\\r\n>     --per_device_eval_batch_size 4 \\\r\n>     --gradient_accumulation_steps 1 \\\r\n>     --evaluation_strategy \"no\" \\\r\n>     --save_strategy \"steps\" \\\r\n>     --save_steps 500 \\\r\n>     --save_total_limit 3 \\\r\n>     --learning_rate 2e-5 \\\r\n>     --weight_decay 0. \\\r\n>     --warmup_ratio 0.03 \\\r\n>     --lr_scheduler_type \"cosine\" \\\r\n>     --logging_steps 1 \\\r\n>     --tf32 True \\\r\n>     --model_max_length 2048 \\\r\n>     --gradient_checkpointing True \\\r\n>     --dataloader_num_workers 4 \\\r\n>     --lazy_preprocess True \\\r\n>     --report_to wandb \\\r\n>     --freeze_backbone\r\n> ```\r\n> \r\n> 知道为什么会发生这种情况吗？\r\n\r\nHave you solved your problem? I have a similar issue\n</Comment>\n<Comment by haotian-liu at 2023-10-26T21:06:16Z>\nHi, there was some issue in the earlier version on finetuning from LLaVA-1.5. Please check out the latest code base, thanks.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 609,
    "state": "closed",
    "created_by": "forthelostthings",
    "created_at": "2023-10-18T14:17:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/609</URL>\n\n<TITLE>ValueError: not enough values to unpack (expected 2, got 0)</TITLE>\n\n<BODY>### Describe the issue\n\nHi,\r\nI have been trying to finetune llava-13b with my own dataset, and I used the latest version of the code. But when I was using the llava, I got the following instructions:\r\n\r\nlang_indices, lang_lengths = zip(*[(i, -l) for i, l in enumerate(lengths) if l < 0])\r\nValueError: not enough values to unpack (expected 2, got 0)\r\n\r\nI found that all the items in length are larger than zero\r\n\r\nHere's my command:\r\n\r\ndeepspeed  llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path liuhaotian/llava-llama-2-13b-chat-lightning-preview \\\r\n    --version llava_llama_2 \\\r\n    --data_path path/to/dataset \\\r\n    --image_folder path/to/imagefolder \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter path/to/llava-336px-pretrain-llama-2-13b-chat/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-13b \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True\r\n\r\nIs there a way of solving the problem?</BODY>\n\n<COMMENTS>\n<Comment by Deaddawn at 2023-10-23T04:33:37Z>\ngot the same problem\n</Comment>\n<Comment by unmo at 2023-10-23T07:53:51Z>\nI got the same problem too.\r\nHere's my command:\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path lmsys/vicuna-7b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/LLaVA-Finetune/testdata_finetune.json \\\r\n    --image_folder ./playground/data/LLaVA-Finetune/images \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-7B-pretraine-custom/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7B-pretraine-custom \\\r\n    --num_train_epochs 5 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\n</Comment>\n<Comment by unmo at 2023-10-23T08:24:05Z>\nThe following command option, I solved the problem.\r\nHere's command line option.\r\n\r\n**_--group_by_modality_length False_**\n</Comment>\n<Comment by dydxdt at 2023-10-26T09:47:59Z>\n> The following command option, I solved the problem. Here's command line option.\r\n> \r\n> **_--group_by_modality_length False_**\r\n\r\n@unmo so this change won't change the results, right?\r\nI see in README.md:\r\n`--group_by_modality_length True`: this should only be used when your instruction tuning dataset contains both language (e.g. ShareGPT) and multimodal (e.g. LLaVA-Instruct). It makes the training sampler only sample a single modality (either image or language) during training, which we observe to speed up training by ~25%, and does not affect the final outcome.\n</Comment>\n<Comment by haotian-liu at 2023-10-26T18:02:02Z>\nIf your test data does not contain pure language data, you do not need this option. We may make this error more graceful.\n</Comment>\n<Comment by unmo at 2023-10-27T04:27:58Z>\n@dydxdt \r\nSorry, I have not checked this change does not change the results.\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/9719347/e85a3991-28f1-4895-add1-f3687e5a4ef9)\r\n\r\nDoes this mean that setting is necessary when training both image and language data, as in the training data above?\r\nIf so, I get the error too.\n</Comment>\n<Comment by haotian-liu at 2023-10-27T04:42:51Z>\n@unmo \r\n\r\nFor pure language data, I mean that conversations have no \"image\" at all, e.g. ShareGPT. I am a bit confused by the screenshot you shared.\n</Comment>\n<Comment by unmo at 2023-10-27T05:05:37Z>\n@haotian-liu \r\nI'm sorry.\r\nScreenshot contain the image path. If train dataset includes an image path and conversations data like this, isn't it pure language data?\r\nIs script argument 'group_by_modality_length' is True?\n</Comment>\n<Comment by haotian-liu at 2023-10-27T05:14:33Z>\nIf **all** your training data is like this, your training data is pure **multimodal,** so you should set this option to `False`.\r\n\r\nIf your training data contains both multimodal and pure-text (Q/A does not contain image at all, a pure language chat), you should set the option to `True`.\n</Comment>\n<Comment by unmo at 2023-10-27T05:20:54Z>\nI can understand it. I was a little bit confuse. Thank you.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 608,
    "state": "open",
    "created_by": "StrangeTcy",
    "created_at": "2023-10-18T12:21:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/608</URL>\n\n<TITLE>[Usage] Missing and unexpected keys in our mm_projector.state_dict</TITLE>\n\n<BODY>### Describe the issue\n\n1.  We used a training script offered by [LLaVAR](https://github.com/SALT-NLP/LLaVAR/tree/main#training-script) to finetune our custom LLaVA:\r\n```\r\n#!/usr/bin/bash\r\ntorchrun --nnodes=1 --nproc_per_node=2 --master_port=25001 \\\r\n   /root/raw_data_for_llava/LLaVAR/LLaVA/llava/train/train_mem.py \\\r\n    --model_name_or_path meta-llama/Llama-2-7b-hf \\\r\n    --data_path all_the_data/chat_llavar.json \\\r\n    --image_folder all_the_data/pretrain_images \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end \\\r\n    --bf16 True \\\r\n    --output_dir llava_R_output \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 2 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 4000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 1024 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --image_aspect_ratio 'pad' \\\r\n    --report_to wandb\r\n```\r\n-- as you can see, this is a wrapper around `train_mem.py` and ultimately `train.py`\r\n2. After the training run we got a `mm_projector.bin` file, which was 517 MBs long\r\n3. Then we tried to use it in a finetuning script you've provided for llava-1.5, slightly adapted for our usecase:\r\n```\r\n#!/bin/bash\r\n#/root/raw_data_for_llava/LLaVAR/llava_R_output/mm_projector.bin\r\n# mm_projector.bin\r\n\r\n\r\ndeepspeed ./LLaVA/llava/train/train_mem.py \\\r\n    --deepspeed ./LLaVA/scripts/zero3.json \\\r\n    --model_name_or_path meta-llama/Llama-2-7b-hf \\\r\n    --version v1 \\\r\n    --data_path /root/combined_data_for_llava/combined_conv.json \\\r\n    --image_folder /root/combined_data_for_llava/ \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter /root/raw_data_for_llava/LLaVAR/llava_R_output/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end \\\r\n    --mm_use_im_patch_token \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./further_finetuning \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 2 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 500 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\nand ran into errors:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/root/raw_data_for_llava/LLaVAR/./LLaVA/llava/train/train_mem.py\", line 15, in <module>\r\n    train()\r\n  File \"/root/raw_data_for_llava/LLaVAR/LLaVA/llava/train/train.py\", line 872, in train\r\n    model.get_model().initialize_vision_modules(\r\n  File \"/root/raw_data_for_llava/LLaVAR/LLaVA/llava/model/llava_arch.py\", line 79, in initialize_vision_modules\r\n    self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'))\r\n  File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2054, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for Sequential:\r\n        Missing key(s) in state_dict: \"0.weight\", \"0.bias\", \"2.weight\", \"2.bias\". \r\n        Unexpected key(s) in state_dict: \"weight\", \"bias\". \r\n\r\n```\r\n\r\nSome examination of the state_dict we have in the `mm_projector.bin` file and the workings of `torch.nn.modules.module.load_state_dict` have shown that while the state_dict only contains `weight` and `bias`,  `module._modules.items()` has 0, 1 and 2 which are Linear, GELU and Linear, respectively.\r\n\r\nI'm not sure whether this is a saving the projector issue or an issue with loading it back in again, but there you go.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-20T03:27:50Z>\nWhen you did pretraining, it seems that you did not specify \"--mm_projector_type mlp2x_gelu\", so that it was trained with linear layer. Thus there is a descrepency when you try to load it and specify `--mm_projector_type mlp2x_gelu` in finetuning.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 607,
    "state": "open",
    "created_by": "wanghao-cst",
    "created_at": "2023-10-18T11:42:21Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/607</URL>\n\n<TITLE>[Question] About \"modality_lengths\" in vicuna-v1.5</TITLE>\n\n<BODY>### Question\n\nAwesome work~\r\n\r\nQ1: I see that in the latest code base, there are two new func called \"modality_lengths\" and \"lengths\" in train.py.\r\nMay I know what is the usage for these two func? Since I don't see where they are called.\r\n<img width=\"679\" alt=\"截屏2023-10-18 19 37 35\" src=\"https://github.com/haotian-liu/LLaVA/assets/55015183/e3263cd3-b333-4a3a-9921-62dfaf5be8fa\">\r\n\r\n\r\nQ2: Does llava-v1.5 support task like translation? I see that llama-2 is not good at it as author mentioned.\r\n\r\nLooking forward to your kind reply~</BODY>\n\n<COMMENTS>\n<Comment by gyupro at 2023-10-25T09:19:36Z>\nAre you working on translation and tried with llava? if you've done it, plz share your thought and results on it. thx\n</Comment>\n<Comment by tingxueronghua at 2023-10-27T09:20:58Z>\n> ### Question\r\n> Awesome work~\r\n> \r\n> Q1: I see that in the latest code base, there are two new func called \"modality_lengths\" and \"lengths\" in train.py. May I know what is the usage for these two func? Since I don't see where they are called. <img alt=\"截屏2023-10-18 19 37 35\" width=\"679\" src=\"https://user-images.githubusercontent.com/55015183/276233677-e3263cd3-b333-4a3a-9921-62dfaf5be8fa.png\">\r\n> \r\n> Q2: Does llava-v1.5 support task like translation? I see that llama-2 is not good at it as author mentioned.\r\n> \r\n> Looking forward to your kind reply~\r\n\r\nIn llava/train/llava_trainer.py, it is used in LengthGroupSampler.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 606,
    "state": "closed",
    "created_by": "Deaddawn",
    "created_at": "2023-10-18T10:59:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/606</URL>\n\n<TITLE>[Question] About the bounding box of the VG data</TITLE>\n\n<BODY>### Question\r\n\r\nHi, there. I'm wondering if the bounding box [x,y,w,h] value in VG data has been modified because of the resize of the training process? Can you please elaborate on this detail? Thanks a lot!</BODY>\n\n<COMMENTS>\n<Comment by Deaddawn at 2023-10-18T11:00:14Z>\nit seems the range of it is [0,1]\n</Comment>\n<Comment by Deaddawn at 2023-10-20T10:08:15Z>\n> hi i've noticed that too, it seems to be ratio of the corrdinates divided by the picture side.\r\n> \r\n> in vg's region notations, the region is provided in `x, y, width, height`, and the picture has width has height(let's say, `im_width, im_height`, you can obtain from `PIL.Image.open(...).size`).\r\n> \r\n> so in llava instructions, it is\r\n> \r\n> * x / im_width\r\n> * y / im_height\r\n> * (x+width) / im_width\r\n> * (y+height) / im_height\r\n> \r\n> i did some calculations are verified in llava's data.\r\n\r\nOK，谢了老兄，我去试试可视化\n</Comment>\n<Comment by Deaddawn at 2023-10-20T10:34:29Z>\n> hi i've noticed that too, it seems to be ratio of the corrdinates divided by the picture side.\r\n> \r\n> in vg's region notations, the region is provided in `x, y, width, height`, and the picture has width has height(let's say, `im_width, im_height`, you can obtain from `PIL.Image.open(...).size`).\r\n> \r\n> so in llava instructions, it is\r\n> \r\n> * x / im_width\r\n> * y / im_height\r\n> * (x+width) / im_width\r\n> * (y+height) / im_height\r\n> \r\n> i did some calculations are verified in llava's data.\r\n\r\nIt seems wrong, can you provide your exapmle?\n</Comment>\n<Comment by Deaddawn at 2023-10-20T14:30:32Z>\nIs it possible the normalization is based on the image shape 336x336, since the cli inference return image tensor 336x336? Does that sounds more resonable? I have tried several samples, seems work.\n</Comment>\n<Comment by Maxlinn at 2023-10-22T12:09:08Z>\ni did some serious digging and find out is does based on image shape 336x336(or any square), but it is NOT as simple as resizing. it is padding the shorter edge to the longer, pouring processor.image_mean to the padding area. you can referring to code here in [train](https://github.com/haotian-liu/LLaVA/blob/1b431ad20bb3d1f641efec792df7ee4b787011d2/llava/train/train.py#L670).\r\n\r\nthe four floats are actually `wa, ha, wb, hb`, a is the left up corner and b is the right bottom corner, while w is width and h is height. it provides a rectangle on the padded image.\r\n\r\nyou can use this case to test:\r\n\r\n```\r\n{'id': 'VG_100K/2334275',\r\n 'image': 'vg/VG_100K/2334275.jpg',\r\n 'conversations': [\r\n...\r\n  {'from': 'human',\r\n   'value': 'Please provide the bounding box coordinate of the region this sentence describes: 01 on the train.'},\r\n  {'from': 'gpt', 'value': '[0.65, 0.29, 0.78, 0.43]'}\r\n...]}\r\n````\r\n\r\nsimply resizing would cause the number in this region to be incomplete, after padding it is correct.\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/43326876/0c9d2ebd-cb81-4687-8215-1cdfda32f065)\r\n![image](https://github.com/haotian-liu/LLaVA/assets/43326876/85a5ada9-6cc1-4567-ac28-6f5d71253fab)\n</Comment>\n<Comment by haotian-liu at 2023-10-23T21:12:10Z>\nThank you @Maxlinn and your understanding is correct. We will need to clarify this in our revised paper to make it clearer.\n</Comment>\n<Comment by gapjialin at 2024-03-25T06:41:17Z>\n> i did some serious digging and find out is does based on image shape 336x336(or any square), but it is NOT as simple as resizing. it is padding the shorter edge to the longer, pouring processor.image_mean to the padding area. you can referring to code here in [train](https://github.com/haotian-liu/LLaVA/blob/1b431ad20bb3d1f641efec792df7ee4b787011d2/llava/train/train.py#L670).\r\n> \r\n> the four floats are actually `wa, ha, wb, hb`, a is the left up corner and b is the right bottom corner, while w is width and h is height. it provides a rectangle on the padded image.\r\n> \r\n> you can use this case to test:\r\n> \r\n> ```\r\n> {'id': 'VG_100K/2334275',\r\n>  'image': 'vg/VG_100K/2334275.jpg',\r\n>  'conversations': [\r\n> ...\r\n>   {'from': 'human',\r\n>    'value': 'Please provide the bounding box coordinate of the region this sentence describes: 01 on the train.'},\r\n>   {'from': 'gpt', 'value': '[0.65, 0.29, 0.78, 0.43]'}\r\n> ...]}\r\n> ```\r\n> \r\n> simply resizing would cause the number in this region to be incomplete, after padding it is correct.\r\n> \r\n> ![image](https://private-user-images.githubusercontent.com/43326876/277163622-0c9d2ebd-cb81-4687-8215-1cdfda32f065.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTEzNDc1MDQsIm5iZiI6MTcxMTM0NzIwNCwicGF0aCI6Ii80MzMyNjg3Ni8yNzcxNjM2MjItMGM5ZDJlYmQtY2I4MS00Njg3LTgyMTUtMWNkZmRhMzJmMDY1LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzI1VDA2MTMyNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTRmMzgzZmYzOWI1NjMyMTZiN2RkN2Q2OTdhMzNiMDFiODNlZDNhOTM5MGNhMjc0OWNmNTI2MTUzNGNmNTRlZWMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.JX-I4O5GNd4j6skMehUYcH8G4huR0ZBgTt6MvW6K5yM) ![image](https://private-user-images.githubusercontent.com/43326876/277163590-85a5ada9-6cc1-4567-ac28-6f5d71253fab.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTEzNDc1MDQsIm5iZiI6MTcxMTM0NzIwNCwicGF0aCI6Ii80MzMyNjg3Ni8yNzcxNjM1OTAtODVhNWFkYTktNmNjMS00NTY3LWFjMjgtNmY1ZDcxMjUzZmFiLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzI1VDA2MTMyNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA4ZGE2ODMzNmRkYjk4YjJlMDQxNzg4YWNmM2YyNjcxN2JlNjA1OGY4NGRhOWI2MWI2M2M1ZjY1Njg5ODEyZmImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.z5yUdkdt5A2_1O7lw3UvSRCVFjZZHwOyVkG6858Tj20)\r\n\r\nHello, I also encountered the same problem. Does it mean that fine-tuning the description of coordinates in the dataset is based on a resolution of 336x336?\n</Comment>\n<Comment by Maxlinn at 2024-03-25T06:58:55Z>\nThe answer is yes, if you use a 336px clip. and if you use a 224px clip, it is 224px.\r\n\r\nThe corrdinates in the instructions are percentages(between 0-1) on a padded square image.\n</Comment>\n<Comment by gapjialin at 2024-03-25T07:00:30Z>\nThank you!!\n</Comment>\n<Comment by ticketlearn at 2024-07-07T02:48:40Z>\n> [0.65, 0.29, 0.78, 0.43]\r\n\r\nCould you please elaborate how did you plot the figure that can caption this bbox [0.65, 0.29, 0.78, 0.43] on it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 605,
    "state": "open",
    "created_by": "min731",
    "created_at": "2023-10-18T08:08:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/605</URL>\n\n<TITLE>[Usage] Is there a way to describe one image and the next?</TITLE>\n\n<BODY>### Describe the issue\n\nThanks for your great work!!\r\n\r\nI want to load the model and input multiple images to save the description for each image.\r\nIs there a way to load the model and then complete the description for one image and then have it describe for the next one?\r\n\r\n----- file 1  inference ------\r\nASSISTANT: Yes\r\nASSISTANT: Sunset, beach, ocean, car, man, woman, boy, girl, handbag, backpack, cell phone.\r\nASSISTANT: A family photo taken in front of a sea of waves.\r\n inference_outputs :  ['Yes</s>', 'Sunset, beach, ocean, car, man, woman, boy, girl, handbag, backpack, cell phone.</s>', 'A family photo taken in front of a sea of waves.</s>']\r\n------ file 2  inference ------\r\n[2023-10-18 17:06:24,340] ERROR in app: Exception on /album_registration_pc1/images_analysis_pc1 [POST]\r\nTraceback (most recent call last):\r\n  File \"/home/meta-3/anaconda3/envs/llava/lib/python3.10/site-packages/flask/app.py\", line 1455, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n  File \"/home/meta-3/anaconda3/envs/llava/lib/python3.10/site-packages/flask/app.py\", line 869, in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\n  File \"/home/meta-3/anaconda3/envs/llava/lib/python3.10/site-packages/flask/app.py\", line 867, in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n  File \"/home/meta-3/anaconda3/envs/llava/lib/python3.10/site-packages/flask/app.py\", line 852, in dispatch_request\r\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\r\n  File \"/home/meta-3/jm/META_FINAL_PJT/jm_PC1/Family_Album_PC1/views/album_registration_views_pc1.py\", line 27, in images_analysis_task_pc1\r\n    inference_outputs = image_processor_pc1.llava_inference_image(\r\n  File \"/home/meta-3/jm/META_FINAL_PJT/jm_PC1/Family_Album_PC1/models/image_processor_pc1.py\", line 28, in llava_inference_image\r\n    inference_outputs = inference_image(tokenizer,\r\n  File \"/home/meta-3/jm/META_FINAL_PJT/jm_PC1/Family_Album_PC1/models/LLaVA/llava/serve/cli.py\", line 149, in inference_image\r\n    output_ids = model.generate(\r\n  File \"/home/meta-3/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/meta-3/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1588, in generate\r\n    return self.sample(\r\n  File \"/home/meta-3/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2642, in sample\r\n    outputs = self(\r\n  File \"/home/meta-3/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/meta-3/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/meta-3/jm/META_FINAL_PJT/jm_PC1/Family_Album_PC1/models/LLaVA/llava/model/language_model/llava_llama.py\", line 75, in forward\r\n    input_ids, attention_mask, past_key_values, inputs_embeds, labels = self.prepare_inputs_labels_for_multimodal(input_ids, attention_mask, past_key_values, labels, images)\r\n  File \"/home/meta-3/jm/META_FINAL_PJT/jm_PC1/Family_Album_PC1/models/LLaVA/llava/model/llava_arch.py\", line 129, in prepare_inputs_labels_for_multimodal\r\n    cur_image_features = image_features[cur_image_idx]\r\nIndexError: index 1 is out of bounds for dimension 0 with size 1\r\n\r\n\r\nI had the same error as above.</BODY>\n\n<COMMENTS>\n<Comment by wanxingDaze at 2023-10-27T14:34:37Z>\nHello, I also encountered the same problem when running multiple images, I wonder if you have solved it\r\n\r\nFile \"/home/llmnav/kunyuwangv2/LLaVA/llava/serve/main.py\", line 177, in <module>\r\n    main(args)\r\n  File \"/home/llmnav/kunyuwangv2/LLaVA/llava/serve/main.py\", line 146, in main\r\n    output_ids = model.generate(\r\n  File \"/home/llmnav/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/llmnav/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1588, in generate\r\n    return self.sample(\r\n  File \"/home/llmnav/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2642, in sample\r\n    outputs = self(\r\n  File \"/home/llmnav/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/llmnav/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/llmnav/kunyuwangv2/LLaVA/llava/model/language_model/llava_llama.py\", line 75, in forward\r\n    input_ids, attention_mask, past_key_values, inputs_embeds, labels = self.prepare_inputs_labels_for_multimodal(input_ids, attention_mask, past_key_values, labels, images)\r\n  File \"/home/llmnav/kunyuwangv2/LLaVA/llava/model/llava_arch.py\", line 141, in prepare_inputs_labels_for_multimodal\r\n    cur_image_features = image_features[cur_image_idx]\r\nIndexError: index 1 is out of bounds for dimension 0 with size 1\n</Comment>\n<Comment by min731 at 2023-10-28T07:14:28Z>\nYeah, that's the problem I'm running into. \r\nNow, I'm trying to use 'LLaVA/llava/serve/cli.py' file to have the model describe multiple images each, but it's hard to implement.\n</Comment>\n<Comment by adrielkuek at 2023-11-02T08:04:27Z>\nNot sure if this is still an issue, but the fix to this is to simply reset the conversation at the start of the loop so that it doesn't continuously collect and append chat histories. Reset using this line: `conv = conv_templates[args.conv_mode].copy()` will do the job.\n</Comment>\n<Comment by min731 at 2023-11-05T02:33:13Z>\nOh, my God. It was just a matter of resetting the conv. Thank you so much. God bless you...\n</Comment>\n<Comment by anas-zafar at 2024-07-08T18:52:05Z>\n@adrielkuek where exactly did you replace this? By doing this won't we lose the contextual information? Thanks\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 604,
    "state": "closed",
    "created_by": "CR400AF-A",
    "created_at": "2023-10-18T07:03:56Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/604</URL>\n\n<TITLE>[Question] Low scores in VQAv2 test-standard (~0)</TITLE>\n\n<BODY>Thanks for your great work.\r\n\r\nI want to evaluate the model on VQAv2 dataset, and I followed the [Evaluation.md]((https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md)). After I ran the \r\n`CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 bash scripts/v1_5/eval/vqav2.sh`,\r\n I got some json files, like 8_0.json/8_1.json.  \r\n\r\nHowever, I don't know what I should do next.  You said in the [Evaluation.md]((https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md)) that, \"Submit the results to the evaluation server: ./playground/data/eval/vqav2/answers_upload.\" But where to submit? How can I get the eval results? Where is the server?</BODY>\n\n<COMMENTS>\n<Comment by CR400AF-A at 2023-10-18T13:58:57Z>\nI have known the server and submitted my results to EvalAI.\r\nThe result is \r\n```\r\n[\r\n    {\r\n        \"test-dev\": {\r\n            \"yes/no\": 93.33,\r\n            \"number\": 62.22,\r\n            \"other\": 72.62,\r\n            \"overall\": 79.97\r\n        }\r\n    },\r\n    {\r\n        \"test-standard\": {\r\n            \"yes/no\": 0,\r\n            \"number\": 0.02,\r\n            \"other\": 0.04,\r\n            \"overall\": 0.02\r\n        }\r\n    }\r\n]\r\n```\r\nThe test-dev result closely aligns with the data reported in the paper, around 80.0. However, the test-standard result is surprisingly low, almost approaching zero.\n</Comment>\n<Comment by CR400AF-A at 2023-10-18T14:06:54Z>\nAh, I think I find the reason. The file is different for test-standard! It's my fault. \r\nI will close the issue after I eval it on the llava_vqav2_mscoco_test2015.jsonl.\n</Comment>\n<Comment by gapjialin at 2023-12-13T04:13:55Z>\n> Ah, I think I find the reason. The file is different for test-standard! It's my fault. I will close the issue after I eval it on the llava_vqav2_mscoco_test2015.jsonl.\r\n\r\nHi, I meet the same question. I don't know what I should do next. You said in the Evaluation.md that, \"Submit the results to the evaluation server: ./playground/data/eval/vqav2/answers_upload.\" But where to submit? How can I get the eval results? Where is the server?\n</Comment>\n<Comment by CR400AF-A at 2023-12-13T04:44:52Z>\n> > Ah, I think I find the reason. The file is different for test-standard! It's my fault. I will close the issue after I eval it on the llava_vqav2_mscoco_test2015.jsonl.\r\n> \r\n> Hi, I meet the same question. I don't know what I should do next. You said in the Evaluation.md that, \"Submit the results to the evaluation server: ./playground/data/eval/vqav2/answers_upload.\" But where to submit? How can I get the eval results? Where is the server?\r\n\r\nYou mean the evaluation server? I just follow the Evaluation.md. The server is https://eval.ai/web/challenges/challenge-page/830/my-submission.\n</Comment>\n<Comment by gapjialin at 2023-12-13T06:52:12Z>\n> > > Ah, I think I find the reason. The file is different for test-standard! It's my fault. I will close the issue after I eval it on the llava_vqav2_mscoco_test2015.jsonl.\r\n> > \r\n> > \r\n> > Hi, I meet the same question. I don't know what I should do next. You said in the Evaluation.md that, \"Submit the results to the evaluation server: ./playground/data/eval/vqav2/answers_upload.\" But where to submit? How can I get the eval results? Where is the server?\r\n> \r\n> You mean the evaluation server? I just follow the Evaluation.md. The server is https://eval.ai/web/challenges/challenge-page/830/my-submission.\r\n\r\nThank you, I have resolved the issue. A good person lives a peaceful life.\n</Comment>\n<Comment by SCZwangxiao at 2023-12-23T03:15:01Z>\n> I have known the server and submitted my results to EvalAI. The result is\r\n> \r\n> ```\r\n> [\r\n>     {\r\n>         \"test-dev\": {\r\n>             \"yes/no\": 93.33,\r\n>             \"number\": 62.22,\r\n>             \"other\": 72.62,\r\n>             \"overall\": 79.97\r\n>         }\r\n>     },\r\n>     {\r\n>         \"test-standard\": {\r\n>             \"yes/no\": 0,\r\n>             \"number\": 0.02,\r\n>             \"other\": 0.04,\r\n>             \"overall\": 0.02\r\n>         }\r\n>     }\r\n> ]\r\n> ```\r\n> \r\n> The test-dev result closely aligns with the data reported in the paper, around 80.0. However, the test-standard result is surprisingly low, almost approaching zero.\r\n\r\nI tried to reproduce the results, and followed the submission README, but the scores are very low (~0.1). I think the scores should be much higher, because I manually checked 100 answers, and I found most of the ansers are correctly. Do you have any ides?\n</Comment>\n<Comment by SCZwangxiao at 2023-12-24T01:52:14Z>\n> > I have known the server and submitted my results to EvalAI. The result is\r\n> > ```\r\n> > [\r\n> >     {\r\n> >         \"test-dev\": {\r\n> >             \"yes/no\": 93.33,\r\n> >             \"number\": 62.22,\r\n> >             \"other\": 72.62,\r\n> >             \"overall\": 79.97\r\n> >         }\r\n> >     },\r\n> >     {\r\n> >         \"test-standard\": {\r\n> >             \"yes/no\": 0,\r\n> >             \"number\": 0.02,\r\n> >             \"other\": 0.04,\r\n> >             \"overall\": 0.02\r\n> >         }\r\n> >     }\r\n> > ]\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > The test-dev result closely aligns with the data reported in the paper, around 80.0. However, the test-standard result is surprisingly low, almost approaching zero.\r\n> \r\n> I tried to reproduce the results, and followed the submission README, but the scores are very low (~0.1). I think the scores should be much higher, because I manually checked 100 answers, and I found most of the ansers are correctly. Do you have any ides?\r\n\r\nSolved. Add prompt to format answer into single word.\n</Comment>\n<Comment by LuFan31 at 2024-03-27T07:24:15Z>\n> Ah, I think I find the reason. The file is different for test-standard! It's my fault. I will close the issue after I eval it on the llava_vqav2_mscoco_test2015.jsonl.\r\n\r\nHi, I have tried to upload llava_vqav2_mscoco_test2015.jsonl and llava_vqav2_mscoco_test-dev2015.jsonl to the Server, but the submitted files are both Failed, and occured error: _Traceback (most recent call last):\r\n  File \"/code/scripts/workers/submission_worker.py\", line 538, in run_submission\r\n    submission_metadata=submission_serializer.data,\r\n  File \"/tmp/tmpu4q4g4fd/compute/challenge_data/challenge_830/main.py\", line 202, in evaluate\r\n    prepare_objects(annFile, resFile, phase_codename)\r\n  File \"/tmp/tmpu4q4g4fd/compute/challenge_data/challenge_830/main.py\", line 109, in prepare_objects\r\n    vqaRes = vqa.loadRes(res, resFile)\r\n  File \"/tmp/tmpu4q4g4fd/compute/challenge_data/challenge_830/vqa.py\", line 166, in loadRes\r\n    anns    = json.load(open(resFile))\r\n  File \"/usr/local/lib/python3.7/json/__init__.py\", line 296, in load\r\n    parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\r\n  File \"/usr/local/lib/python3.7/json/__init__.py\", line 348, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"/usr/local/lib/python3.7/json/decoder.py\", line 340, in decode\r\n    raise JSONDecodeError(\"Extra data\", s, end)\r\njson.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 217)_   Maybe  i can obtain some help from u.\n</Comment>\n<Comment by CR400AF-A at 2024-03-27T10:14:17Z>\n> > Ah, I think I find the reason. The file is different for test-standard! It's my fault. I will close the issue after I eval it on the llava_vqav2_mscoco_test2015.jsonl.\r\n> \r\n> Hi, I have tried to upload llava_vqav2_mscoco_test2015.jsonl and llava_vqav2_mscoco_test-dev2015.jsonl to the Server, but the submitted files are both Failed, and occured error: _Traceback (most recent call last): File \"/code/scripts/workers/submission_worker.py\", line 538, in run_submission submission_metadata=submission_serializer.data, File \"/tmp/tmpu4q4g4fd/compute/challenge_data/challenge_830/main.py\", line 202, in evaluate prepare_objects(annFile, resFile, phase_codename) File \"/tmp/tmpu4q4g4fd/compute/challenge_data/challenge_830/main.py\", line 109, in prepare_objects vqaRes = vqa.loadRes(res, resFile) File \"/tmp/tmpu4q4g4fd/compute/challenge_data/challenge_830/vqa.py\", line 166, in loadRes anns = json.load(open(resFile)) File \"/usr/local/lib/python3.7/json/**init**.py\", line 296, in load parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw) File \"/usr/local/lib/python3.7/json/**init**.py\", line 348, in loads return _default_decoder.decode(s) File \"/usr/local/lib/python3.7/json/decoder.py\", line 340, in decode raise JSONDecodeError(\"Extra data\", s, end) json.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 217)_ Maybe i can obtain some help from u.\r\n\r\nHi, I haven't been involved in similar work for a long time, but I still remember some details that may help you.\r\nYou said that you submitted the llava_vqav2_mscoco_test2015.jsonl and llava_vqav2_mscoco_test-dev2015.jsonl to the Server. I remember  they are not the files that should be submiited.  You can check the ./scripts/v1_5/eval/vqav2.sh, and the last command tells you which file to submit.\n</Comment>\n<Comment by LuFan31 at 2024-03-27T14:01:34Z>\n> > > Ah, I think I find the reason. The file is different for test-standard! It's my fault. I will close the issue after I eval it on the llava_vqav2_mscoco_test2015.jsonl.\r\n> > \r\n> > \r\n> > Hi, I have tried to upload llava_vqav2_mscoco_test2015.jsonl and llava_vqav2_mscoco_test-dev2015.jsonl to the Server, but the submitted files are both Failed, and occured error: _Traceback (most recent call last): File \"/code/scripts/workers/submission_worker.py\", line 538, in run_submission submission_metadata=submission_serializer.data, File \"/tmp/tmpu4q4g4fd/compute/challenge_data/challenge_830/main.py\", line 202, in evaluate prepare_objects(annFile, resFile, phase_codename) File \"/tmp/tmpu4q4g4fd/compute/challenge_data/challenge_830/main.py\", line 109, in prepare_objects vqaRes = vqa.loadRes(res, resFile) File \"/tmp/tmpu4q4g4fd/compute/challenge_data/challenge_830/vqa.py\", line 166, in loadRes anns = json.load(open(resFile)) File \"/usr/local/lib/python3.7/json/**init**.py\", line 296, in load parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw) File \"/usr/local/lib/python3.7/json/**init**.py\", line 348, in loads return _default_decoder.decode(s) File \"/usr/local/lib/python3.7/json/decoder.py\", line 340, in decode raise JSONDecodeError(\"Extra data\", s, end) json.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 217)_ Maybe i can obtain some help from u.\r\n> \r\n> Hi, I haven't been involved in similar work for a long time, but I still remember some details that may help you. You said that you submitted the llava_vqav2_mscoco_test2015.jsonl and llava_vqav2_mscoco_test-dev2015.jsonl to the Server. I remember they are not the files that should be submiited. You can check the ./scripts/v1_5/eval/vqav2.sh, and the last command tells you which file to submit.\r\n\r\nThank you for your prompt reply. I have already uploaded the file mentioned in ./scripts/v1_5/eval/vqav2.sh, which is in the ./playground/data/eval/vqav2/answers_upload and is just the file you mentioned at the begining of this issue. But I encountered the same problem as you https://github.com/haotian-liu/LLaVA/issues/604#issuecomment-1768524331, the metric of test_standard is almost 0. Could you provide any tips please.\n</Comment>\n<Comment by CR400AF-A at 2024-03-27T15:41:08Z>\n> > > > Ah, I think I find the reason. The file is different for test-standard! It's my fault. I will close the issue after I eval it on the llava_vqav2_mscoco_test2015.jsonl.\r\n> > > \r\n> > > \r\n> > > Hi, I have tried to upload llava_vqav2_mscoco_test2015.jsonl and llava_vqav2_mscoco_test-dev2015.jsonl to the Server, but the submitted files are both Failed, and occured error: _Traceback (most recent call last): File \"/code/scripts/workers/submission_worker.py\", line 538, in run_submission submission_metadata=submission_serializer.data, File \"/tmp/tmpu4q4g4fd/compute/challenge_data/challenge_830/main.py\", line 202, in evaluate prepare_objects(annFile, resFile, phase_codename) File \"/tmp/tmpu4q4g4fd/compute/challenge_data/challenge_830/main.py\", line 109, in prepare_objects vqaRes = vqa.loadRes(res, resFile) File \"/tmp/tmpu4q4g4fd/compute/challenge_data/challenge_830/vqa.py\", line 166, in loadRes anns = json.load(open(resFile)) File \"/usr/local/lib/python3.7/json/**init**.py\", line 296, in load parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw) File \"/usr/local/lib/python3.7/json/**init**.py\", line 348, in loads return _default_decoder.decode(s) File \"/usr/local/lib/python3.7/json/decoder.py\", line 340, in decode raise JSONDecodeError(\"Extra data\", s, end) json.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 217)_ Maybe i can obtain some help from u.\r\n> > \r\n> > \r\n> > Hi, I haven't been involved in similar work for a long time, but I still remember some details that may help you. You said that you submitted the llava_vqav2_mscoco_test2015.jsonl and llava_vqav2_mscoco_test-dev2015.jsonl to the Server. I remember they are not the files that should be submiited. You can check the ./scripts/v1_5/eval/vqav2.sh, and the last command tells you which file to submit.\r\n> \r\n> Thank you for your prompt reply. I have already uploaded the file mentioned in ./scripts/v1_5/eval/vqav2.sh, which is in the ./playground/data/eval/vqav2/answers_upload and is just the file you mentioned at the begining of this issue. But I encountered the same problem as you [#604 (comment)](https://github.com/haotian-liu/LLaVA/issues/604#issuecomment-1768524331), the metric of test_standard is almost 0. Could you provide any tips please.\r\n\r\nAs I mentioned [earlier](https://github.com/haotian-liu/LLaVA/issues/604#issuecomment-1768539776), selecting the wrong split file can result in a low score on test_standard.  In ./scripts/v1_5/eval/vqav2.sh, Line 9, you need to specify the split file. If you use the default file 'llava_vqav2_mscoco_test-dev2015', you will get a good score on test-dev but a bad one on test-std, because it's 'test-dev2015'!\n</Comment>\n<Comment by LuFan31 at 2024-03-29T03:12:21Z>\n> > > > > Ah, I think I find the reason. The file is different for test-standard! It's my fault. I will close the issue after I eval it on the llava_vqav2_mscoco_test2015.jsonl.\r\n> > > > \r\n> > > > \r\n> > > > Hi, I have tried to upload llava_vqav2_mscoco_test2015.jsonl and llava_vqav2_mscoco_test-dev2015.jsonl to the Server, but the submitted files are both Failed, and occured error: _Traceback (most recent call last): File \"/code/scripts/workers/submission_worker.py\", line 538, in run_submission submission_metadata=submission_serializer.data, File \"/tmp/tmpu4q4g4fd/compute/challenge_data/challenge_830/main.py\", line 202, in evaluate prepare_objects(annFile, resFile, phase_codename) File \"/tmp/tmpu4q4g4fd/compute/challenge_data/challenge_830/main.py\", line 109, in prepare_objects vqaRes = vqa.loadRes(res, resFile) File \"/tmp/tmpu4q4g4fd/compute/challenge_data/challenge_830/vqa.py\", line 166, in loadRes anns = json.load(open(resFile)) File \"/usr/local/lib/python3.7/json/**init**.py\", line 296, in load parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw) File \"/usr/local/lib/python3.7/json/**init**.py\", line 348, in loads return _default_decoder.decode(s) File \"/usr/local/lib/python3.7/json/decoder.py\", line 340, in decode raise JSONDecodeError(\"Extra data\", s, end) json.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 217)_ Maybe i can obtain some help from u.\r\n> > > \r\n> > > \r\n> > > Hi, I haven't been involved in similar work for a long time, but I still remember some details that may help you. You said that you submitted the llava_vqav2_mscoco_test2015.jsonl and llava_vqav2_mscoco_test-dev2015.jsonl to the Server. I remember they are not the files that should be submiited. You can check the ./scripts/v1_5/eval/vqav2.sh, and the last command tells you which file to submit.\r\n> > \r\n> > \r\n> > Thank you for your prompt reply. I have already uploaded the file mentioned in ./scripts/v1_5/eval/vqav2.sh, which is in the ./playground/data/eval/vqav2/answers_upload and is just the file you mentioned at the begining of this issue. But I encountered the same problem as you [#604 (comment)](https://github.com/haotian-liu/LLaVA/issues/604#issuecomment-1768524331), the metric of test_standard is almost 0. Could you provide any tips please.\r\n> \r\n> As I mentioned [earlier](https://github.com/haotian-liu/LLaVA/issues/604#issuecomment-1768539776), selecting the wrong split file can result in a low score on test_standard. In ./scripts/v1_5/eval/vqav2.sh, Line 9, you need to specify the split file. If you use the default file 'llava_vqav2_mscoco_test-dev2015', you will get a good score on test-dev but a bad one on test-std, because it's 'test-dev2015'!\r\n\r\nThank you for your help! I have obtained the desired result.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 603,
    "state": "open",
    "created_by": "TikaToka",
    "created_at": "2023-10-18T06:22:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/603</URL>\n\n<TITLE>[Usage]  Error while making interaction with model via gradio server</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nThis is an extension of #417  issue. (I'm sorry to keep asking, but I think my previous issue was buried because of a lot of other issues, so I'm reposting it. Again, Sorry and Thank you in advance 👍 )\r\n\r\nI think I prepared a pload right as you mentioned early, but there comes the error like below\r\n\r\n+ I am using the conversation based on llava_v1\r\nCommand:\r\n```\r\n    def load_image(self, image_file):\r\n        import base64\r\n        from io import BytesIO\r\n\r\n        image = image_file\r\n        image = image.resize((336, 336)\r\n\r\n        buffered = BytesIO()\r\n        image.save(buffered, format=\"PNG\")\r\n        img_b64_str = base64.b64encode(buffered.getvalue()).decode()\r\n        return img_b64_str\r\n```\r\nand\r\n```\r\n        for j in range(batch_size):\r\n            # Change 1x12 panoramic image to 3x4 for good looking\r\n            rgb = obs_view12['rgb'][j * 12:j * 12 + 12]\r\n            rgb = rgb.view(12, 224, 224, 3)\r\n            rows = []\r\n            for m in range(3):  # 3 rows\r\n                row_images = [rgb[m*4 + n] for n in range(4)]  # 4 images per row\r\n                rows.append(torch.cat(row_images, dim=1))  # Concatenate along width\r\n            rgb = torch.cat(rows, dim=0)  # Concatenate rows\r\n            rgb = rgb.permute(2, 0, 1)\r\n\r\n            # depth = obs_view12['depth'][j * 12:j * 12 + 12]\r\n            # depth = torch.cat(list(depth), 1)\r\n            # depth = depth.permute(2, 0, 1)\r\n\r\n            instruction = instruction_step[j]\r\n\r\n            if histories[j] == {}:\r\n                history = None\r\n            else:\r\n                history = histories[j]\r\n\r\n            state = states[j]\r\n\r\n            qs = self.get_prompt(instruction, history)\r\n            qs = DEFAULT_IMAGE_TOKEN + '\\n' + qs\r\n\r\n            # Construct prompt\r\n            state.append_message(state.roles[0], qs)\r\n            state.append_message(state.roles[1], None)\r\n\r\n            prompt = state.get_prompt()\r\n\r\n            # Make requests\r\n\r\n            image = self.load_image(ToPILImage(mode='RGB')(rgb))\r\n\r\n            pload = {\r\n                \"model\": self.model_name,\r\n                \"prompt\": prompt,\r\n                \"temperature\": float(0.2),\r\n                \"top_p\": float(0.7),\r\n                \"max_new_tokens\": 512,\r\n                \"stop\": state.sep if state.sep_style in [SeparatorStyle.SINGLE, SeparatorStyle.MPT] else state.sep2,\r\n                \"images\": image,\r\n            }\r\n\r\n            pload['images'] = state.get_images()\r\n\r\n            try:\r\n                # Stream output\r\n                response = requests.post(self.worker_addr + \"/worker_generate_stream\",\r\n                                        headers=self.headers, json=pload, stream=True, timeout=100)\r\n                for chunk in response.iter_lines(decode_unicode=False, delimiter=b\"\\0\"):\r\n                    if chunk:\r\n                        data = json.loads(chunk.decode())\r\n                        if data[\"error_code\"] == 0:\r\n                            output = data[\"text\"][len(prompt):].strip()\r\n                            print(output, '@@@@@@@@@@@@@@@@@@@@@@@@')\r\n                            output = self.process_output(output)\r\n                            action = output['Step']\r\n                            if action == 'STOP':\r\n                                action = 0\r\n                            else:\r\n                                img_idxes = action.split(' ')[1]\r\n                                action = 4\r\n                                distances = 0.25 \r\n                            state.messages[-1][-1] = output + \"▌\"\r\n                            history[j] = {\r\n                                \"thought\": output['Thought'], \"step\": output['Step']}\r\n                        else:\r\n                            output = data[\"text\"] + \\\r\n                                f\" (error_code: {data['error_code']})\"\r\n                            print(output)\r\n\r\n                        time.sleep(0.03)\r\n            except requests.exceptions.RequestException as e:\r\n                print(e)\r\n                import sys\r\n                sys.exit()\r\n```\r\n\r\nLog: \r\n```\r\n:58808 - \"POST /worker_generate_stream HTTP/1.1\" 200 \r\nOK\r\n2023-10-18 05:37:04 | ERROR | stderr | Exception in thr\r\nead Thread-5 (generate):\r\n2023-10-18 05:37:04 | ERROR | stderr | Traceback (most \r\nrecent call last):\r\n2023-10-18 05:37:04 | ERROR | stderr |   File \"/root/an\r\naconda3/envs/llava/lib/python3.10/threading.py\", line 1\r\n016, in _bootstrap_inner\r\n2023-10-18 05:37:04 | ERROR | stderr |     self.run()\r\n2023-10-18 05:37:04 | ERROR | stderr |   File \"/root/an\r\naconda3/envs/llava/lib/python3.10/threading.py\", line 9\r\n53, in run\r\n2023-10-18 05:37:04 | ERROR | stderr |     self._target\r\n(*self._args, **self._kwargs)\r\n2023-10-18 05:37:04 | ERROR | stderr |   File \"/root/an\r\naconda3/envs/llava/lib/python3.10/site-packages/torch/u\r\ntils/_contextlib.py\", line 115, in decorate_context\r\n2023-10-18 05:37:04 | ERROR | stderr |     return func(\r\n*args, **kwargs)\r\n2023-10-18 05:37:04 | ERROR | stderr |   File \"/root/an\r\naconda3/envs/llava/lib/python3.10/site-packages/transfo\r\nrmers/generation/utils.py\", line 1588, in generate\r\n2023-10-18 05:37:04 | ERROR | stderr |     return self.\r\nsample(\r\n2023-10-18 05:37:04 | ERROR | stderr |   File \"/root/an\r\naconda3/envs/llava/lib/python3.10/site-packages/transfo\r\nrmers/generation/utils.py\", line 2642, in sample\r\n2023-10-18 05:37:04 | ERROR | stderr |     outputs = se\r\nlf(\r\n2023-10-18 05:37:04 | ERROR | stderr |   File \"/root/an\r\naconda3/envs/llava/lib/python3.10/site-packages/torch/n\r\nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n2023-10-18 05:37:04 | ERROR | stderr |     return self.\r\n_call_impl(*args, **kwargs)\r\n2023-10-18 05:37:04 | ERROR | stderr |   File \"/root/an\r\naconda3/envs/llava/lib/python3.10/site-packages/torch/n\r\nn/modules/module.py\", line 1527, in _call_impl\r\n2023-10-18 05:37:04 | ERROR | stderr |     return forwa\r\nrd_call(*args, **kwargs)\r\n2023-10-18 05:37:04 | ERROR | stderr |   File \"/root/an\r\naconda3/envs/llava/lib/python3.10/site-packages/acceler\r\nate/hooks.py\", line 165, in new_forward\r\n2023-10-18 05:37:04 | ERROR | stderr |     output = old\r\n_forward(*args, **kwargs)\r\n2023-10-18 05:37:04 | ERROR | stderr |   File \"/home/vl\r\nn_workspace/LLaVA/llava/model/language_model/llava_llam\r\na.py\", line 78, in forward\r\n2023-10-18 05:37:04 | ERROR | stderr |     outputs = se\r\nlf.model(\r\n2023-10-18 05:37:04 | ERROR | stderr |   File \"/root/an\r\naconda3/envs/llava/lib/python3.10/site-packages/torch/n\r\nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n2023-10-18 05:37:04 | ERROR | stderr |     return self.\r\n_call_impl(*args, **kwargs)\r\n2023-10-18 05:37:04 | ERROR | stderr |   File \"/root/an\r\naconda3/envs/llava/lib/python3.10/site-packages/torch/n\r\nn/modules/module.py\", line 1527, in _call_impl\r\n2023-10-18 05:37:04 | ERROR | stderr |     return forwa\r\nrd_call(*args, **kwargs)\r\n2023-10-18 05:37:04 | ERROR | stderr |   File \"/root/an\r\naconda3/envs/llava/lib/python3.10/site-packages/transfo\r\nrmers/models/llama/modeling_llama.py\", line 693, in for\r\nward\r\n2023-10-18 05:37:04 | ERROR | stderr |     layer_output\r\ns = decoder_layer(\r\n2023-10-18 05:37:04 | ERROR | stderr |   File \"/root/an\r\naconda3/envs/llava/lib/python3.10/site-packages/torch/n\r\nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n2023-10-18 05:37:04 | ERROR | stderr |     return self.\r\n_call_impl(*args, **kwargs)\r\n2023-10-18 05:37:04 | ERROR | stderr |   File \"/root/an\r\naconda3/envs/llava/lib/python3.10/site-packages/torch/n\r\nn/modules/module.py\", line 1527, in _call_impl\r\n2023-10-18 05:37:04 | ERROR | stderr |     return forwa\r\nrd_call(*args, **kwargs)\r\n2023-10-18 05:37:04 | ERROR | stderr |   File \"/root/an\r\naconda3/envs/llava/lib/python3.10/site-packages/acceler\r\nate/hooks.py\", line 165, in new_forward\r\n2023-10-18 05:37:04 | ERROR | stderr |     output = old\r\n_forward(*args, **kwargs)\r\n2023-10-18 05:37:04 | ERROR | stderr |   File \"/root/an\r\naconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 408, in forward\r\n2023-10-18 05:37:04 | ERROR | stderr |     hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n2023-10-18 05:37:04 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n2023-10-18 05:37:04 | ERROR | stderr |     return self._call_impl(*args, **kwargs)\r\n2023-10-18 05:37:04 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n2023-10-18 05:37:04 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2023-10-18 05:37:04 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2023-10-18 05:37:04 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2023-10-18 05:37:04 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 330, in forward\r\n2023-10-18 05:37:04 | ERROR | stderr |     attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\r\n2023-10-18 05:37:04 | ERROR | stderr | RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasGemmStridedBatchedExFix( handle, opa, opb, m, n, k, (void*)(&falpha), a, CUDA_R_16F, lda, stridea, b, CUDA_R_16F, ldb, strideb, (void*)(&fbeta), c, CUDA_R_16F, ldc, stridec, num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`\r\n2023-10-18 05:37:18 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: Semaphore(value=4, locked=False). global_counter: 3\r\n2023-10-18 05:37:19 | INFO | stdout | Caught Unknown Error\r\n```\r\nOR\r\n```\r\na/lib/python3.10/site-packages/torch\r\n/nn/modules/module.py\", line 1518, i\r\nn _wrapped_call_impl\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |     return self._call_impl(*args,\r\n **kwargs)\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |   File \"/root/anaconda3/envs/llav\r\na/lib/python3.10/site-packages/torch\r\n/nn/modules/module.py\", line 1527, i\r\nn _call_impl\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |     return forward_call(*args, **\r\nkwargs)\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |   File \"/root/anaconda3/envs/llav\r\na/lib/python3.10/site-packages/trans\r\nformers/models/llama/modeling_llama.\r\npy\", line 693, in forward\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |     layer_outputs = decoder_layer\r\n(\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |   File \"/root/anaconda3/envs/llav\r\na/lib/python3.10/site-packages/torch\r\n/nn/modules/module.py\", line 1518, i\r\nn _wrapped_call_impl\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |     return self._call_impl(*args,\r\n **kwargs)\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |   File \"/root/anaconda3/envs/llav\r\na/lib/python3.10/site-packages/torch\r\n/nn/modules/module.py\", line 1527, i\r\nn _call_impl\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |     return forward_call(*args, **\r\nkwargs)\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |   File \"/root/anaconda3/envs/llav\r\na/lib/python3.10/site-packages/accel\r\nerate/hooks.py\", line 165, in new_fo\r\nrward\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |     output = old_forward(*args, *\r\n*kwargs)\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |   File \"/root/anaconda3/envs/llav\r\na/lib/python3.10/site-packages/trans\r\nformers/models/llama/modeling_llama.\r\npy\", line 408, in forward\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |     hidden_states, self_attn_weig\r\nhts, present_key_value = self.self_a\r\nttn(\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |   File \"/root/anaconda3/envs/llav\r\na/lib/python3.10/site-packages/torch\r\n/nn/modules/module.py\", line 1518, i\r\nn _wrapped_call_impl\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |     return self._call_impl(*args,\r\n **kwargs)\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |   File \"/root/anaconda3/envs/llav\r\na/lib/python3.10/site-packages/torch\r\n/nn/modules/module.py\", line 1527, i\r\nn _call_impl\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |     return forward_call(*args, **\r\nkwargs)\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |   File \"/root/anaconda3/envs/llav\r\na/lib/python3.10/site-packages/accel\r\nerate/hooks.py\", line 165, in new_fo\r\nrward\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |     output = old_forward(*args, *\r\n*kwargs)\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |   File \"/root/anaconda3/envs/llav\r\na/lib/python3.10/site-packages/trans\r\nformers/models/llama/modeling_llama.\r\npy\", line 305, in forward\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |     query_states = self.q_proj(hi\r\ndden_states)\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |   File \"/root/anaconda3/envs/llav\r\na/lib/python3.10/site-packages/torch\r\n/nn/modules/module.py\", line 1518, i\r\nn _wrapped_call_impl\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |     return self._call_impl(*args,\r\n **kwargs)\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |   File \"/root/anaconda3/envs/llav\r\na/lib/python3.10/site-packages/torch\r\n/nn/modules/module.py\", line 1527, i\r\nn _call_impl\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |     return forward_call(*args, **\r\nkwargs)\r\n2023-10-18 06:20:19 | ERROR | stderr\r\n |   File \"/root/anaconda3/envs/llav\r\na/lib/python3.10/site-packages/accel\r\nerate/hooks.py\", line 165, in new_forward\r\n2023-10-18 06:20:19 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2023-10-18 06:20:19 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\r\n2023-10-18 06:20:19 | ERROR | stderr |     return F.linear(input, self.weight, self.bias)\r\n2023-10-18 06:20:19 | ERROR | stderr | RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED w\r\n```\r\nOR\r\n```\r\n56,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [56,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n2023-10-18 06:51:08 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n```\r\n\r\n\r\nThe RuntimeError CUDA error is the always same error, but the rest of the logs are difference whenever i try it\r\n\r\n\r\nScreenshots:\r\n<img width=\"1569\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/49054667/e3073a8c-7f2b-4d0a-b28d-9fbff8097bef\">\r\nIt works well on browser with example before I run my custom code. But after error occurs due to my code, model gets dead and I should reload the model by terminating the running terminal and re-run it.</BODY>\n\n<COMMENTS>\n<Comment by 459737087 at 2024-01-11T07:18:13Z>\nHey, I found another solution change your model_path, it should consist LLaVA-vicuna-7b，or LLaVA-vicuna-13b .it worked\n</Comment>\n<Comment by pseudotensor at 2024-02-13T18:04:05Z>\nRelated: https://github.com/haotian-liu/LLaVA/issues/840\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 602,
    "state": "closed",
    "created_by": "linzhiqiu",
    "created_at": "2023-10-17T20:10:26Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/602</URL>\n\n<TITLE>[Question] Is the response format \"Answer the question using a single word or phrase. \" used during training?</TITLE>\n\n<BODY>### Question\n\nI tried to search for this response formatting prompt, but I only see it in playground/data/eval/MME/convert_answer_to_mme.py. How is this used during training?</BODY>\n\n<COMMENTS>\n<Comment by hill2hill at 2023-10-18T03:51:35Z>\ngrep Answer the question using a single word or phrase. path_to_llava_v1_5_mix665k.json. And you can find relative conversations. [https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/tree/main](url)\n</Comment>\n<Comment by haotian-liu at 2023-10-18T04:11:22Z>\n@hill2hill Thanks for providing the link.\r\n\r\nYou may also refer to Table 7 in the response format prompt used.\n</Comment>\n<Comment by linzhiqiu at 2023-10-18T07:25:34Z>\nThanks! I didn't notice this -- where exactly should we put this json file under? Is it ./playground/data?\n</Comment>\n<Comment by haotian-liu at 2023-10-19T01:21:24Z>\n@linzhiqiu Anywhere you want, as long as the correct path is provided `--data_path ./playground/data/llava_v1_5_mix665k.json`\r\n\r\nIt does not need to be under the same path as your image folder.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 601,
    "state": "open",
    "created_by": "Carol-lyh",
    "created_at": "2023-10-17T18:02:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/601</URL>\n\n<TITLE>[Usage] No such file or directory when finetuning</TITLE>\n\n<BODY>### Describe the issue\n\nI downloaded the several datasets for finetune and run the corresponding finetune.sh in order to reproduce it. It's OK at the beginning of finetune. BUT when training, it raises:\r\n\r\n\r\n\r\n  0%|          | 22/5198 [02:59<10:21:05,  7.20s/it]\r\n                                                    \r\n{'loss': 1.1543, 'learning_rate': 2.8205128205128207e-06, 'epoch': 0.0}\r\n\r\n  0%|          | 22/5198 [02:59<10:21:05,  7.20s/it]\r\nTraceback (most recent call last):\r\nblabla ...\r\nFileNotFoundError: [Errno 2] No such file or directory: '/xxx/llava_finetune_data/ocr_vqa/images/782118577.jpg'\r\n\r\nBUT actually it's download perfectly with the total number of images matches the overall num, which is 207572 for ocr_vqa dataset, so I DON'T know where is the error?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-17T19:00:42Z>\nSorry for the confusion. For some reason when we developed the model, we save all files in OCR-VQA as `.jpg`, including some of the files that you may have downloaded as '.gif`.\r\n\r\nFor now, you may create a new folder and save all files as `.gif`, and one user found changing the extension directly also works: https://github.com/haotian-liu/LLaVA/issues/593#issuecomment-1766215738\n</Comment>\n<Comment by Carol-lyh at 2023-10-18T06:47:20Z>\n> Sorry for the confusion. For some reason when we developed the model, we save all files in OCR-VQA as `.jpg`, including some of the files that you may have downloaded as '.gif`.\r\n> \r\n> For now, you may create a new folder and save all files as `.gif`, and one user found changing the extension directly also works: [#593 (comment)](https://github.com/haotian-liu/LLaVA/issues/593#issuecomment-1766215738)\r\n\r\nThank you for replying me! Excuse me, should I change the extension in **json** or directly change the **image files**' extension from '.gif' to '.jpg'?\r\nAlso, another question, about 170 images in Visual Genome dataset I downloaded, specifically, the **VG_100K_2**,  is empty. So I CAN'T open it. I DON'T know where the problem lying?\n</Comment>\n<Comment by HireTheHero at 2023-10-28T15:32:06Z>\nHow exactly did you download the dataset? And did you try several times? I once met a download error, but it was just a temporary HTTP connection issue. Training giant model always forces us to download an enormous amount of files...\r\nMaybe you should also try downloading manually via the browser to see where the problem lies.\n</Comment>\n<Comment by lxysl at 2024-05-06T10:11:59Z>\n> > Sorry for the confusion. For some reason when we developed the model, we save all files in OCR-VQA as `.jpg`, including some of the files that you may have downloaded as '.gif`.\r\n> > For now, you may create a new folder and save all files as `.gif`, and one user found changing the extension directly also works: [#593 (comment)](https://github.com/haotian-liu/LLaVA/issues/593#issuecomment-1766215738)\r\n> \r\n> Thank you for replying me! Excuse me, should I change the extension in **json** or directly change the **image files**' extension from '.gif' to '.jpg'? Also, another question, about 170 images in Visual Genome dataset I downloaded, specifically, the **VG_100K_2**, is empty. So I CAN'T open it. I DON'T know where the problem lying?\r\n\r\nYes, I have the same question. `VG_100K_2` contains 170 empty images. I have searched several of these in `llava_v1_5_mix665k.json` and they are not in it, so it should not cause problems.\r\n\r\n<details>\r\n<summary> The empty images in VG_100K_2 </summary>\r\n\r\n```\r\nCan't open file: VG_100K_2/2416776.jpg\r\nCan't open file: VG_100K_2/2355831.jpg\r\nCan't open file: VG_100K_2/2370333.jpg\r\nCan't open file: VG_100K_2/2407337.jpg\r\nCan't open file: VG_100K_2/2408825.jpg\r\nCan't open file: VG_100K_2/2331541.jpg\r\nCan't open file: VG_100K_2/2357389.jpg\r\nCan't open file: VG_100K_2/2392406.jpg\r\nCan't open file: VG_100K_2/2414809.jpg\r\nCan't open file: VG_100K_2/2344903.jpg\r\nCan't open file: VG_100K_2/2357002.jpg\r\nCan't open file: VG_100K_2/2339452.jpg\r\nCan't open file: VG_100K_2/2316472.jpg\r\nCan't open file: VG_100K_2/2397804.jpg\r\nCan't open file: VG_100K_2/2369516.jpg\r\nCan't open file: VG_100K_2/2329481.jpg\r\nCan't open file: VG_100K_2/2400248.jpg\r\nCan't open file: VG_100K_2/2403972.jpg\r\nCan't open file: VG_100K_2/2416204.jpg\r\nCan't open file: VG_100K_2/2401793.jpg\r\nCan't open file: VG_100K_2/2354293.jpg\r\nCan't open file: VG_100K_2/2360334.jpg\r\nCan't open file: VG_100K_2/2360452.jpg\r\nCan't open file: VG_100K_2/2324980.jpg\r\nCan't open file: VG_100K_2/2338403.jpg\r\nCan't open file: VG_100K_2/2415090.jpg\r\nCan't open file: VG_100K_2/2405254.jpg\r\nCan't open file: VG_100K_2/2335721.jpg\r\nCan't open file: VG_100K_2/2384303.jpg\r\nCan't open file: VG_100K_2/2383685.jpg\r\nCan't open file: VG_100K_2/2370916.jpg\r\nCan't open file: VG_100K_2/2410274.jpg\r\nCan't open file: VG_100K_2/2379927.jpg\r\nCan't open file: VG_100K_2/2336771.jpg\r\nCan't open file: VG_100K_2/2345238.jpg\r\nCan't open file: VG_100K_2/2397424.jpg\r\nCan't open file: VG_100K_2/2350190.jpg\r\nCan't open file: VG_100K_2/2369711.jpg\r\nCan't open file: VG_100K_2/2320098.jpg\r\nCan't open file: VG_100K_2/2376553.jpg\r\nCan't open file: VG_100K_2/2394503.jpg\r\nCan't open file: VG_100K_2/2328613.jpg\r\nCan't open file: VG_100K_2/2345400.jpg\r\nCan't open file: VG_100K_2/2386339.jpg\r\nCan't open file: VG_100K_2/2354486.jpg\r\nCan't open file: VG_100K_2/2385744.jpg\r\nCan't open file: VG_100K_2/2378352.jpg\r\nCan't open file: VG_100K_2/2404941.jpg\r\nCan't open file: VG_100K_2/2407333.jpg\r\nCan't open file: VG_100K_2/2336852.jpg\r\nCan't open file: VG_100K_2/2347067.jpg\r\nCan't open file: VG_100K_2/2394940.jpg\r\nCan't open file: VG_100K_2/2354437.jpg\r\nCan't open file: VG_100K_2/2410589.jpg\r\nCan't open file: VG_100K_2/2407778.jpg\r\nCan't open file: VG_100K_2/2407779.jpg\r\nCan't open file: VG_100K_2/2391568.jpg\r\nCan't open file: VG_100K_2/2339625.jpg\r\nCan't open file: VG_100K_2/2350687.jpg\r\nCan't open file: VG_100K_2/2344194.jpg\r\nCan't open file: VG_100K_2/2415989.jpg\r\nCan't open file: VG_100K_2/2316003.jpg\r\nCan't open file: VG_100K_2/2390058.jpg\r\nCan't open file: VG_100K_2/2356194.jpg\r\nCan't open file: VG_100K_2/2402944.jpg\r\nCan't open file: VG_100K_2/2397721.jpg\r\nCan't open file: VG_100K_2/2408461.jpg\r\nCan't open file: VG_100K_2/2325435.jpg\r\nCan't open file: VG_100K_2/2374336.jpg\r\nCan't open file: VG_100K_2/2335434.jpg\r\nCan't open file: VG_100K_2/2328471.jpg\r\nCan't open file: VG_100K_2/2395669.jpg\r\nCan't open file: VG_100K_2/2335839.jpg\r\nCan't open file: VG_100K_2/2337155.jpg\r\nCan't open file: VG_100K_2/2321051.jpg\r\nCan't open file: VG_100K_2/2370544.jpg\r\nCan't open file: VG_100K_2/2350125.jpg\r\nCan't open file: VG_100K_2/2400944.jpg\r\nCan't open file: VG_100K_2/2359672.jpg\r\nCan't open file: VG_100K_2/2413483.jpg\r\nCan't open file: VG_100K_2/2405591.jpg\r\nCan't open file: VG_100K_2/2393186.jpg\r\nCan't open file: VG_100K_2/2346745.jpg\r\nCan't open file: VG_100K_2/2354396.jpg\r\nCan't open file: VG_100K_2/2379210.jpg\r\nCan't open file: VG_100K_2/2389915.jpg\r\nCan't open file: VG_100K_2/2417802.jpg\r\nCan't open file: VG_100K_2/2318215.jpg\r\nCan't open file: VG_100K_2/2338884.jpg\r\nCan't open file: VG_100K_2/2325625.jpg\r\nCan't open file: VG_100K_2/2363899.jpg\r\nCan't open file: VG_100K_2/2344765.jpg\r\nCan't open file: VG_100K_2/2387074.jpg\r\nCan't open file: VG_100K_2/2320630.jpg\r\nCan't open file: VG_100K_2/2346562.jpg\r\nCan't open file: VG_100K_2/2315674.jpg\r\nCan't open file: VG_100K_2/2414917.jpg\r\nCan't open file: VG_100K_2/2388568.jpg\r\nCan't open file: VG_100K_2/2319311.jpg\r\nCan't open file: VG_100K_2/2402595.jpg\r\nCan't open file: VG_100K_2/2414491.jpg\r\nCan't open file: VG_100K_2/2405946.jpg\r\nCan't open file: VG_100K_2/2336131.jpg\r\nCan't open file: VG_100K_2/2337005.jpg\r\nCan't open file: VG_100K_2/2382669.jpg\r\nCan't open file: VG_100K_2/2394180.jpg\r\nCan't open file: VG_100K_2/2380334.jpg\r\nCan't open file: VG_100K_2/2361256.jpg\r\nCan't open file: VG_100K_2/2327203.jpg\r\nCan't open file: VG_100K_2/2405004.jpg\r\nCan't open file: VG_100K_2/2333919.jpg\r\nCan't open file: VG_100K_2/2372979.jpg\r\nCan't open file: VG_100K_2/2340698.jpg\r\nCan't open file: VG_100K_2/2388989.jpg\r\nCan't open file: VG_100K_2/2379781.jpg\r\nCan't open file: VG_100K_2/2346773.jpg\r\nCan't open file: VG_100K_2/2383215.jpg\r\nCan't open file: VG_100K_2/2339038.jpg\r\nCan't open file: VG_100K_2/2396395.jpg\r\nCan't open file: VG_100K_2/2395307.jpg\r\nCan't open file: VG_100K_2/2386421.jpg\r\nCan't open file: VG_100K_2/2330142.jpg\r\nCan't open file: VG_100K_2/2340300.jpg\r\nCan't open file: VG_100K_2/2412350.jpg\r\nCan't open file: VG_100K_2/2390322.jpg\r\nCan't open file: VG_100K_2/2335991.jpg\r\nCan't open file: VG_100K_2/2392657.jpg\r\nCan't open file: VG_100K_2/2378996.jpg\r\nCan't open file: VG_100K_2/2386269.jpg\r\nCan't open file: VG_100K_2/2388085.jpg\r\nCan't open file: VG_100K_2/2321571.jpg\r\nCan't open file: VG_100K_2/2319788.jpg\r\nCan't open file: VG_100K_2/2405111.jpg\r\nCan't open file: VG_100K_2/2390387.jpg\r\nCan't open file: VG_100K_2/2417886.jpg\r\nCan't open file: VG_100K_2/2326141.jpg\r\nCan't open file: VG_100K_2/2370307.jpg\r\nCan't open file: VG_100K_2/2406223.jpg\r\nCan't open file: VG_100K_2/2370885.jpg\r\nCan't open file: VG_100K_2/2351122.jpg\r\nCan't open file: VG_100K_2/2400491.jpg\r\nCan't open file: VG_100K_2/2416218.jpg\r\nCan't open file: VG_100K_2/2388284.jpg\r\nCan't open file: VG_100K_2/2380814.jpg\r\nCan't open file: VG_100K_2/2407937.jpg\r\nCan't open file: VG_100K_2/2392550.jpg\r\nCan't open file: VG_100K_2/2369047.jpg\r\nCan't open file: VG_100K_2/2329477.jpg\r\nCan't open file: VG_100K_2/2387773.jpg\r\nCan't open file: VG_100K_2/2385366.jpg\r\nCan't open file: VG_100K_2/2348472.jpg\r\nCan't open file: VG_100K_2/2393276.jpg\r\nCan't open file: VG_100K_2/2368946.jpg\r\nCan't open file: VG_100K_2/2325661.jpg\r\nCan't open file: VG_100K_2/2357595.jpg\r\nCan't open file: VG_100K_2/2323189.jpg\r\nCan't open file: VG_100K_2/2396732.jpg\r\nCan't open file: VG_100K_2/2373542.jpg\r\nCan't open file: VG_100K_2/2399366.jpg\r\nCan't open file: VG_100K_2/2374775.jpg\r\nCan't open file: VG_100K_2/2408234.jpg\r\nCan't open file: VG_100K_2/2343246.jpg\r\nCan't open file: VG_100K_2/2401212.jpg\r\nCan't open file: VG_100K_2/2344844.jpg\r\nCan't open file: VG_100K_2/2357151.jpg\r\nCan't open file: VG_100K_2/2343497.jpg\r\nCan't open file: VG_100K_2/2356717.jpg\r\nCan't open file: VG_100K_2/2388917.jpg\r\nCan't open file: VG_100K_2/2323362.jpg\r\nCan't open file: VG_100K_2/2379468.jpg\r\n```\r\n</details>\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 600,
    "state": "open",
    "created_by": "Me1e",
    "created_at": "2023-10-17T16:11:30Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/600</URL>\n\n<TITLE>[Question] How to deploy this model using Amazon SageMaker?</TITLE>\n\n<BODY>### Question\n\nhello. I apologize for asking a really fundamental question. I don't have a way to ask these questions around here.\r\nI'm practicing the sagemaker deploy method on huggingface, and it says to put in the following inputs.\r\n```\r\npredictor.predict({\r\n\t\"inputs\": \"Can you please let us know more details about your \",\r\n})\r\n```\r\nBut of course, that didn't work, so how could I deploy llava as a sagemaker?\r\nI can deduce that I need to write an inference.py, but how and what do I create?\r\nIt's a really big question, but I'd appreciate it if you could answer it with some keywords or references so I can learn on my own.</BODY>\n\n<COMMENTS>\n<Comment by egoodman92 at 2023-10-24T16:54:34Z>\nI found in AWS Cloudwatch the code provided on hugging face leads to errors like KeyError - even tried playing around with different containers but no luck. Would be great to have this!\n</Comment>\n<Comment by sungeuns at 2023-10-27T12:32:30Z>\nHere is the example how to deploy llava model to sagemaker endpoint : https://github.com/sungeuns/gen-ai-sagemaker/tree/main/MultiModal\r\n\r\nHope this help!\n</Comment>\n<Comment by ketangangal at 2024-03-06T09:48:42Z>\nnice thanks\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 598,
    "state": "open",
    "created_by": "mshooter",
    "created_at": "2023-10-17T15:01:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/598</URL>\n\n<TITLE>[Question] How to access the visual / text features?</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 597,
    "state": "open",
    "created_by": "mshooter",
    "created_at": "2023-10-17T14:57:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/597</URL>\n\n<TITLE>Could we have a model with DINOv2 as image encoder</TITLE>\n\n<BODY>### feature\n\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by itsprakhar at 2023-12-19T05:21:20Z>\nThis indeed is an awesome Idea! If someone wants to work on it :)\n</Comment>\n<Comment by luohao123 at 2024-03-27T06:22:24Z>\nAm worked it, it's pretty good\n</Comment>\n<Comment by mshooter at 2024-06-14T12:36:45Z>\n@luohao123 do you have the pre-trained model published? :)\n</Comment>\n<Comment by PrakharThakur27 at 2024-06-21T04:44:51Z>\n> Am worked it, it's pretty good\r\n\r\nHey @luohao123 can you share the pretrained model, please 🥺🙏\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 596,
    "state": "open",
    "created_by": "KainingYing",
    "created_at": "2023-10-17T10:01:56Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/596</URL>\n\n<TITLE>[Question] How to select the features in ViT clip encoder?</TITLE>\n\n<BODY>### Question\n\nhi @haotian-liu ,\r\n\r\nthanks for your excellent work.\r\n\r\nIn your paper, you choose the before and after the last Transformer layer as the image features. \r\n\r\n<img width=\"816\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/42159793/11c30d1d-887c-457d-bcd0-7e6e14dc2e30\">\r\n\r\nHowever, in the code, you choose the penultimate transformer layer as the image feature. Why just use the final layer as the image feature?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 594,
    "state": "open",
    "created_by": "dongzhiwu",
    "created_at": "2023-10-17T06:23:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/594</URL>\n\n<TITLE>[Question] training time of instruction tuning, reproducing llava_v1.5_13b</TITLE>\n\n<BODY>### Question\n\nHi~\r\n100 hours was needed for instruction tuning in my experiment.....is this nomal?\r\n\r\n2machines×4A100-40G（total 8A100）was used in our fintuning，same dataset with paper.\r\nDue to the 40G of my A100, the finetuing parameters set in experiment is 2 per_device_train_batch_size x 8 gradient_accumulation_steps x 8 num_gpus\r\n\r\nps: I noticed that 8×A00-80G was used in the paper and it took 20 hours, and the parameters of your is 16 per_device_train_batch_size x 1 gradient_accumulation_steps x 8 num_gpus</BODY>\n\n<COMMENTS>\n<Comment by DietDietDiet at 2023-10-17T06:45:52Z>\nsame issue here, 24 hours for 8*A100 40G\n</Comment>\n<Comment by dongzhiwu at 2023-10-17T07:02:50Z>\n> same issue here, 24 hours for 8*A100 40G\r\n\r\n24 hours? @DietDietDiet , what is your per_device_train_batch_size x  gradient_accumulation_steps x  num_gpus\n</Comment>\n<Comment by DietDietDiet at 2023-10-17T07:09:41Z>\n> > same issue here, 24 hours for 8*A100 40G\r\n> \r\n> 24 hours? @DietDietDiet , what is your per_device_train_batch_size x gradient_accumulation_steps x num_gpus\r\n\r\nI trained with 7b version, just half the default bs & double accumulation steps.  Could it be an optimization issue on deepspeed in your case?\n</Comment>\n<Comment by dongzhiwu at 2023-10-17T07:26:13Z>\n> ### Question\r\n> Hi~ 100 hours was needed for instruction tuning in my experiment.....is this nomal?\r\n> \r\n> 2machines×4A100-40G（total 8A100）was used in our fintuning，same dataset with paper. Due to the 40G of my A100, the finetuing parameters set in experiment is 2 per_device_train_batch_size x 8 gradient_accumulation_steps x 8 num_gpus\r\n> \r\n> ps: I noticed that 8×A00-80G was used in the paper and it took 20 hours, and the parameters of your is 16 per_device_train_batch_size x 1 gradient_accumulation_steps x 8 num_gpus\r\n\r\nneed help @\r\n\r\n> ### Question\r\n> Hi~ 100 hours was needed for instruction tuning in my experiment.....is this nomal?\r\n> \r\n> 2machines×4A100-40G（total 8A100）was used in our fintuning，same dataset with paper. Due to the 40G of my A100, the finetuing parameters set in experiment is 2 per_device_train_batch_size x 8 gradient_accumulation_steps x 8 num_gpus\r\n> \r\n> ps: I noticed that 8×A00-80G was used in the paper and it took 20 hours, and the parameters of your is 16 per_device_train_batch_size x 1 gradient_accumulation_steps x 8 num_gpus\r\n\r\nneed help @haotian-liu ~\n</Comment>\n<Comment by haotian-liu at 2023-11-05T05:12:48Z>\nHi @dongzhiwu \r\n\r\nCan you confirm if your two machines are NVLinked? If not, the inter-server communication may be an overhead. You can try LoRA training which has a smaller speed overhead.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_lora.sh\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 593,
    "state": "closed",
    "created_by": "July-zh",
    "created_at": "2023-10-17T06:07:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/593</URL>\n\n<TITLE>[Question] GIF files in OCR_VQA dataset</TITLE>\n\n<BODY>### Question\n\nI noticed during the fine-tuning phase that in the OCR_VQA dataset, there are many GIF files, but they have all been changed to JPG in the JSON. Can these files be directly modified by changing the file extension without any further processing?</BODY>\n\n<COMMENTS>\n<Comment by CR400AF-A at 2023-10-17T11:24:36Z>\nI changed the fille extension, and it works.\n</Comment>\n<Comment by haotian-liu at 2023-10-17T19:01:14Z>\nSorry for the confusion. For some reason when we developed the model, we save all files in OCR-VQA as .jpg, including some of the files that you may have downloaded as '.gif`.\r\n\r\nFor now, you may create a new folder and save all files as .gif, or as @CRH400AF-A points out, changing the extension may also work.\n</Comment>\n<Comment by Carol-lyh at 2023-10-18T06:42:23Z>\n> I changed the fille extension, and it works.\r\n\r\nExcuse me, should I change the extension in **json** or directly change the **image files**' extension from '.gif' to '.jpg'?\n</Comment>\n<Comment by CR400AF-A at 2023-10-18T06:45:24Z>\n> > I changed the fille extension, and it works.\r\n> \r\n> Excuse me, should I change the extension in **json** or directly change the **image files**' extension from '.gif' to '.jpg'?\r\n\r\nI changed the **image files**' extension from '.gif' to '.jpg'.  I think this is relatively easy.\n</Comment>\n<Comment by Carol-lyh at 2023-10-18T06:52:34Z>\nThank you very much! \r\nAlso, I have another question, about 170 images in Visual Genome dataset I downloaded, specifically, the VG_100K_2, is empty. The others are fairly good. And I CAN'T open those 170 images. I've tried to download it several times, actually it's successfully downloaded and unzipped. \r\nI DON'T know where the problem lying? Have you ever come across this problem?\n</Comment>\n<Comment by CR400AF-A at 2023-10-18T07:21:55Z>\n> Thank you very much! Also, I have another question, about 170 images in Visual Genome dataset I downloaded, specifically, the VG_100K_2, is empty. The others are fairly good. And I CAN'T open those 170 images. I've tried to download it several times, actually it's successfully downloaded and unzipped. I DON'T know where the problem lying? Have you ever come across this problem?\r\n\r\nI didn't come across this problem. I previously downloaded this dataset, so I didn't download it this time.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 592,
    "state": "closed",
    "created_by": "gyupro",
    "created_at": "2023-10-17T05:53:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/592</URL>\n\n<TITLE>[Question] Multiple image input.</TITLE>\n\n<BODY>### Question\n\nHi, I am really appreciated your great work.\r\n\r\nI am working on a project to extract visual context from a couple of images.\r\n\r\nDo you think llava1.5 can take multiple images and multiple or summary of them?\r\n\r\nPerhaps VIT can take multiple images as input and send them to an MLP projector?\r\n\r\nThank you in advance!</BODY>\n\n<COMMENTS>\n<Comment by gyupro at 2023-10-17T05:55:44Z>\nOh, I actually found the issue https://github.com/haotian-liu/LLaVA/issues/197 here. I will look into it. Thank you\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 591,
    "state": "closed",
    "created_by": "linzhiqiu",
    "created_at": "2023-10-17T05:52:29Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/591</URL>\n\n<TITLE>[Question] Is image padding necessary for inference?</TITLE>\n\n<BODY>### Question\n\nI noticed that LLaVA-1.5 changed the default image transform to a padding mode for images instead of cropping. I can see that it helps with training --  is it however required for inference?</BODY>\n\n<COMMENTS>\n<Comment by hill2hill at 2023-10-17T06:35:45Z>\nIn my case, when padding mode, LLaVA-1.5 seems to output better text.\n</Comment>\n<Comment by haotian-liu at 2023-10-17T19:16:53Z>\nYour observation is correct.\r\n\r\nFor both training and evaluation, we used the padding to prevent the loss of the potentially useful information in the image. In training, it also alleviates hallucination to some extent.\r\n\r\nHowever, there is always a trade-off, as padding-and-resize will result in the image being down-scaled, which is another form of information loss. According to our eval, they are similar, but it could be helpful for tasks like OCR and a prior exists that the most information are in the center-area of the image.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 590,
    "state": "open",
    "created_by": "lucasjinreal",
    "created_at": "2023-10-17T03:44:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/590</URL>\n\n<TITLE>[Feature request] How to change a new LLM?</TITLE>\n\n<BODY>### feature\n\nHi, llama is in transformers lib, but what if users want using a new LLM such as Baichuan?</BODY>\n\n<COMMENTS>\n<Comment by 20191864218 at 2024-02-22T17:47:27Z>\nHi,I have the same issue. Have you solved it\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 589,
    "state": "closed",
    "created_by": "liyang-7",
    "created_at": "2023-10-17T03:09:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/589</URL>\n\n<TITLE>[Usage] The download dataset file in the eval MMBench-CN is still in English</TITLE>\n\n<BODY>### Describe the issue\n\nAlthough the link to the Chinese data set is marked in the eval MMBench-CN , the download result is in English (mmbench_dev_en_20231003.tsv).\r\n<img width=\"1240\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/28669941/2b03b89b-577a-4e9a-a668-54a40e6c2f5e\"></BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-17T03:18:09Z>\nThanks for pointing this out. The link has been updated.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 588,
    "state": "closed",
    "created_by": "siddk",
    "created_at": "2023-10-16T15:28:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/588</URL>\n\n<TITLE>[Reproducibility] BLIP-2 VQAv2 Numbers?</TITLE>\n\n<BODY>### Question\n\nHi! \r\n\r\nI just read through the LLaVa-1.5 paper, and am a bit confused as to the 41.0% accuracy number for BLIP-2 (Vicuna 13B) on the VQA-v2 dataset. \r\n\r\nWhere did this number come from? I couldn't find it in the InstructBLIP paper (where I'm finding all the other numbers), and BLIP-2 with an OPT (2.7B) backbone already gets ~49% accuracy... could you help clarify?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-17T00:35:42Z>\nOur apologies and thank you for pointing this out. It should be `65.0` (the number from the [Qwen-VL paper](https://arxiv.org/abs/2308.12966)) and the number was mistakenly taken from the GQA number (41).\r\n\r\nWe will update the paper this week to fix the number. Thank you!\n</Comment>\n<Comment by siddk at 2023-10-17T11:41:02Z>\nGot it! Thanks so much for the prompt response @haotian-liu - and amazing work with LLaVa!\r\n\r\nOut of curiosity, do you know where the Qwen-VL authors got that BLIP-2 VQAv2 number? As far as I know, the BLIP-2 Vicuna model is public (introduced in the InstructBLIP paper), and it doesn't report a VQAv2 number?\n</Comment>\n<Comment by haotian-liu at 2023-10-17T19:10:23Z>\nHi @siddk,\r\n\r\nI think the point is that they report the BLIP-2 numbers (not InstructBLIP), as you can always evaluate the model on your own using the official scripts/checkpoint if there is one.\r\n\r\nI do not find the InstructBLIP authors released the pretrained Qformer officially in their repo, but I saw the authors shared the pretrained Qformers of Vicuna-7B here: https://github.com/salesforce/LAVIS/issues/344#issuecomment-1576575852. I am not sure if Qwen-VL authors obtained the 13B checkpoints elsewhere.\n</Comment>\n<Comment by siddk at 2023-10-17T19:39:44Z>\nAhh got it. Thanks again @haotian-liu - and congrats again on this amazing work!\n</Comment>\n<Comment by haotian-liu at 2023-10-17T19:44:08Z>\nThank you!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 587,
    "state": "open",
    "created_by": "newday1255",
    "created_at": "2023-10-16T09:52:28Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/587</URL>\n\n<TITLE>[Question] 当设置load_8bit=True并加载模型的时候出现Error named symbol not found at line 529 in file错误</TITLE>\n\n<BODY>### Question\n\ngpu信息：\r\n![image](https://github.com/haotian-liu/LLaVA/assets/46987480/51363fcd-1559-42d3-a69c-7a0dbc45060c)\r\n\r\n\r\n\r\n运行命令如下：\r\nCUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6 python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path /projects/LLaVA/models_download/llava-v1.5-7b --load-8bit\r\n\r\n运行日志如下：\r\n[2023-10-16 17:44:23,930] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n2023-10-16 17:44:24 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='projects/LLaVA/models_download/llava-v1.5-7b', model_base=None, model_name=None, device='cuda', multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=True, load_4bit=False)\r\n2023-10-16 17:44:24 | INFO | model_worker | Loading the model llava-v1.5-7b on worker bc28f5 ...\r\nLoading checkpoint shards:   0%|                                                                                                                                  | 0/2 [00:00<?, ?it/s]\r\nError named symbol not found at line 529 in file /mmfs1/gscratch/zlab/timdettmers/git/bitsandbytes/csrc/ops.cu\r\n\r\n请问该如何解决这个问题？</BODY>\n\n<COMMENTS>\n<Comment by TousakaNagio at 2023-10-20T12:02:59Z>\nI solved this problem by set CUDA_VISIBLE_DEVICES=0 while running the cammand.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 586,
    "state": "closed",
    "created_by": "shere-khan",
    "created_at": "2023-10-16T06:00:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/586</URL>\n\n<TITLE>[Usage] KeyError when trying to run cli.py using llava-pretrain-llama-2-13b-chat</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI want to run the code from the CLI not the gradio server.\r\n\r\nI can run the CLI calling `python -m llava.serve.cli` with \"llava-v1.5-13b\" just fine, but when I try to use \"llava-pretrain-llama-2-13b-chat\", I get a KeyError when the line `tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)` is executed.\r\n\r\nI've tried fixing this in multiple ways, but it looks like the code is expecting there to be a config file in the cache where huggingface files are downloaded, but it's not there.\r\n\r\nWhat am I doing wrong?\r\n\r\nCommand:\r\nThe following works\r\n```\r\npython -m llava.serve.cli     --model-path liuhaotian/llava-v1.5-7b     --image-file \"https://llava-vl.github.io/static/images/view.jpg\"     --load-4bit\r\n```\r\nThe following throws a KeyError\r\n```\r\npython -m llava.serve.cli     --model-path liuhaotian/llava-pretrain-llama-2-13b-chat     --image-file \"https://llava-vl.github.io/static/images/view.jpg\"     --load-4bit\r\n```\r\n\r\nLog: \r\n```\r\n~/temp/LLaVA# python -m llava.serve.cli     --model-path liuhaotian/llava-pretrain-llama-2-13b-chat     --image-file \"https://llava-vl.github.io/static/images/view.jpg\"     --load-4bit\r\n[2023-10-16 05:55:41,257] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/opt/conda/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/root/temp/LLaVA/llava/serve/cli.py\", line 125, in <module>\r\n    main(args)\r\n  File \"/root/temp/LLaVA/llava/serve/cli.py\", line 32, in main\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n  File \"/root/temp/LLaVA/llava/model/builder.py\", line 102, in load_pretrained_model\r\n    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 718, in from_pretrained\r\n    tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 663, in __getitem__\r\n    model_type = self._reverse_config_mapping[key.__name__]\r\nKeyError: 'LlavaConfig'\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-26T20:24:02Z>\nHi, as mentioned in MODEL ZOO, the checkpoint you used are only pretrained and are not fully instruction tuned. Please use the full-finetuned checkpoint instead. Thanks.\r\n\r\n> These are projector weights we have pretrained. You can use these projector weights for visual instruction tuning. They are just pretrained on image-text pairs, and are NOT instruction tuned, which means they do NOT follow instructions as good as our official models, and can output repetitive, lengthy, and garbled outputs. If you want to have nice conversations with LLaVA, use the checkpoints above (LLaVA v1.5).\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 585,
    "state": "closed",
    "created_by": "aprilehannibal",
    "created_at": "2023-10-16T03:28:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/585</URL>\n\n<TITLE>[Usage] MME eval issue</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nDid not find the following files in mme.sh.  Could you please help to upload these files? Thanks a lot! @haotian-liu \r\n\r\n```\r\n./playground/data/eval/MME/llava_mme.jsonl\r\n\r\nconvert_answer_to_mme.py\r\n```</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-16T03:52:11Z>\nhttps://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md#scripts\r\n\r\nPlease follow the instructions here to download `eval.zip` first, thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 584,
    "state": "closed",
    "created_by": "DietDietDiet",
    "created_at": "2023-10-15T16:37:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/584</URL>\n\n<TITLE>[Usage] Error in evaluating GQA</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nHi haotian, I met with the following issue when evaluating GQA.\r\nException: Can't find testdev_balanced_sceneGraphs.json\r\nCould u kindly help how this file is generated? @haotian-liu \r\nThanks!\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\n\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-16T03:49:19Z>\nHi, GQA team does not provide the scene graph for the latest split. However, you can simply comment out that line, as it is not used in evaluation.\n</Comment>\n<Comment by DietDietDiet at 2023-10-16T08:17:51Z>\n@haotian-liu Thanks for your kind reply.\r\nI commented out some redundant files and found that some question ids are not in the prediction file, and I ignored them and get the following result for llava-1.5-13b. Is this a reasonable result?\r\n\r\nLoading scene graphs...\r\nLoading questions...\r\nLoading choices...\r\nLoading predictions...\r\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12578/12578 [00:00<00:00, 88322.13it/s]\r\n\r\nBinary: 80.56%\r\nOpen: 48.68%\r\nAccuracy: 63.32%\r\nValidity: 0.00%\r\nPlausibility: 0.00%\r\nDistribution: 1.81 (lower is better)\r\n\r\nAccuracy / structural type:\r\n  choose: 85.21% (1129 questions)\r\n  compare: 65.37% (589 questions)\r\n  logical: 79.26% (1803 questions)\r\n  query: 48.68% (6805 questions)\r\n  verify: 83.26% (2252 questions)\r\n\r\nAccuracy / semantic type:\r\n  attr: 70.15% (5186 questions)\r\n  cat: 54.22% (1149 questions)\r\n  global: 64.33% (157 questions)\r\n  obj: 89.59% (778 questions)\r\n  rel: 54.73% (5308 questions)\r\n\r\nAccuracy / steps number:\r\n  1: 81.43% (237 questions)\r\n  2: 57.42% (6395 questions)\r\n  3: 66.34% (4266 questions)\r\n  4: 72.01% (793 questions)\r\n  5: 77.37% (822 questions)\r\n  6: 95.12% (41 questions)\r\n  7: 95.00% (20 questions)\r\n  8: 100.00% (3 questions)\r\n  9: 100.00% (1 questions)\r\n\r\nAccuracy / words number:\r\n  3: 45.03% (151 questions)\r\n  4: 57.30% (630 questions)\r\n  5: 50.31% (1290 questions)\r\n  6: 59.64% (2074 questions)\r\n  7: 64.49% (1642 questions)\r\n  8: 63.38% (1185 questions)\r\n  9: 68.46% (1281 questions)\r\n  10: 67.57% (1249 questions)\r\n  11: 65.09% (994 questions)\r\n  12: 69.59% (638 questions)\r\n  13: 65.80% (462 questions)\r\n  14: 72.75% (345 questions)\r\n  15: 70.89% (237 questions)\r\n  16: 76.92% (117 questions)\r\n  17: 71.28% (94 questions)\r\n  18: 84.21% (76 questions)\r\n  19: 79.07% (43 questions)\r\n  20: 68.75% (32 questions)\r\n  21: 73.68% (19 questions)\r\n  22: 75.00% (12 questions)\r\n  23: 25.00% (4 questions)\r\n  24: 100.00% (2 questions)\r\n  25: 100.00% (1 questions)\n</Comment>\n<Comment by haotian-liu at 2023-10-17T00:27:10Z>\n> Accuracy: 63.32%\r\n\r\nThis is exactly the same as ours: https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#llava-v15\r\n\r\n>  found that some question ids are not in the prediction file\r\n\r\noh wait, what is not in the prediction file?\r\n\r\n```\r\n>>> import json\r\n>>> a=json.load(open('testdev_balanced_questions.json'))\r\n>>> len(a)\r\n12578\r\n```\r\n\r\nThe number of samples is the same, no?\n</Comment>\n<Comment by liuheng0111 at 2023-10-17T03:44:16Z>\nException: Can't find testdev_balanced_choices.json\r\nWhere can i download it?\n</Comment>\n<Comment by DietDietDiet at 2023-10-17T06:49:42Z>\n> len(a)\r\n\r\nthe number is the same, so there are some redundant ids in predictions file??\n</Comment>\n<Comment by DietDietDiet at 2023-10-17T06:49:55Z>\n> Exception: Can't find testdev_balanced_choices.json Where can i download it?\r\n\r\njust comment it out\n</Comment>\n<Comment by liuheng0111 at 2023-10-18T06:38:59Z>\n> @haotian-liu Thanks for your kind reply. I commented out some redundant files and found that some question ids are not in the prediction file, and I ignored them and get the following result for llava-1.5-13b. Is this a reasonable result?\r\n> \r\n> Loading scene graphs... Loading questions... Loading choices... Loading predictions... 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12578/12578 [00:00<00:00, 88322.13it/s]\r\n> \r\n> Binary: 80.56% Open: 48.68% Accuracy: 63.32% Validity: 0.00% Plausibility: 0.00% Distribution: 1.81 (lower is better)\r\n> \r\n> Accuracy / structural type: choose: 85.21% (1129 questions) compare: 65.37% (589 questions) logical: 79.26% (1803 questions) query: 48.68% (6805 questions) verify: 83.26% (2252 questions)\r\n> \r\n> Accuracy / semantic type: attr: 70.15% (5186 questions) cat: 54.22% (1149 questions) global: 64.33% (157 questions) obj: 89.59% (778 questions) rel: 54.73% (5308 questions)\r\n> \r\n> Accuracy / steps number: 1: 81.43% (237 questions) 2: 57.42% (6395 questions) 3: 66.34% (4266 questions) 4: 72.01% (793 questions) 5: 77.37% (822 questions) 6: 95.12% (41 questions) 7: 95.00% (20 questions) 8: 100.00% (3 questions) 9: 100.00% (1 questions)\r\n> \r\n> Accuracy / words number: 3: 45.03% (151 questions) 4: 57.30% (630 questions) 5: 50.31% (1290 questions) 6: 59.64% (2074 questions) 7: 64.49% (1642 questions) 8: 63.38% (1185 questions) 9: 68.46% (1281 questions) 10: 67.57% (1249 questions) 11: 65.09% (994 questions) 12: 69.59% (638 questions) 13: 65.80% (462 questions) 14: 72.75% (345 questions) 15: 70.89% (237 questions) 16: 76.92% (117 questions) 17: 71.28% (94 questions) 18: 84.21% (76 questions) 19: 79.07% (43 questions) 20: 68.75% (32 questions) 21: 73.68% (19 questions) 22: 75.00% (12 questions) 23: 25.00% (4 questions) 24: 100.00% (2 questions) 25: 100.00% (1 questions)\r\n\r\nThe evaluate of GQA is very bad, because of the question did't add \"\\nAnswer the question using a single word or phrase.\". Do you know where can i get the llava_gqa_testdev_balanced.jsonl ?\n</Comment>\n<Comment by Darren-greenhand at 2023-10-18T07:15:49Z>\nHi，I use the same solution as @DietDietDiet and test the result of llava-1.5-7b\r\nand i got a strange result , because the acc is 63.24 and the record here https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#llava-v15\r\nshows it should be 62，can i use this result 😂  @haotian-liu \r\n\r\n\r\nBinary: 80.29%\r\nOpen: 48.77%\r\nAccuracy: 63.24%\r\nValidity: 0.00%\r\nPlausibility: 0.00%\r\nDistribution: 1.50 (lower is better)\r\n\r\nAccuracy / structural type:\r\n  choose: 84.68% (1129 questions)\r\n  compare: 64.18% (589 questions)\r\n  logical: 79.20% (1803 questions)\r\n  query: 48.77% (6805 questions)\r\n  verify: 83.17% (2252 questions)\r\n\r\nAccuracy / semantic type:\r\n  attr: 69.51% (5186 questions)\r\n  cat: 55.09% (1149 questions)\r\n  global: 66.24% (157 questions)\r\n  obj: 88.82% (778 questions)\r\n  rel: 55.03% (5308 questions)\r\n\r\nAccuracy / steps number:\r\n  1: 81.86% (237 questions)\r\n  2: 58.17% (6395 questions)\r\n  3: 65.31% (4266 questions)\r\n  4: 70.74% (793 questions)\r\n  5: 77.13% (822 questions)\r\n  6: 90.24% (41 questions)\r\n  7: 95.00% (20 questions)\r\n  8: 66.67% (3 questions)\r\n  9: 100.00% (1 questions)\r\n\r\nAccuracy / words number:\r\n  3: 45.70% (151 questions)\r\n  4: 59.84% (630 questions)\r\n  5: 51.63% (1290 questions)\r\n  6: 59.45% (2074 questions)\r\n  7: 62.61% (1642 questions)\r\n  8: 64.39% (1185 questions)\r\n  9: 68.62% (1281 questions)\r\n  10: 69.26% (1249 questions)\r\n  11: 62.68% (994 questions)\r\n  12: 69.91% (638 questions)\r\n  13: 64.94% (462 questions)\r\n  14: 70.72% (345 questions)\r\n  15: 70.89% (237 questions)\r\n  16: 72.65% (117 questions)\r\n  17: 70.21% (94 questions)\r\n  18: 78.95% (76 questions)\r\n  19: 81.40% (43 questions)\r\n  20: 68.75% (32 questions)\r\n  21: 68.42% (19 questions)\r\n  22: 66.67% (12 questions)\r\n  23: 25.00% (4 questions)\r\n  24: 100.00% (2 questions)\r\n  25: 100.00% (1 questions)\n</Comment>\n<Comment by shaojin-zhu at 2023-10-18T07:53:21Z>\nI too eval the result of llava1.5 7b and 13b but got different numbers:\r\n7b: Accuracy: 61.02%\r\n13b: Accuracy: 62.00%\r\n\r\nI wonder if there are some sampling rules for model generation so the outputs would not be consistent?\n</Comment>\n<Comment by CR400AF-A at 2023-10-18T13:37:16Z>\nWhy is there no key 'answer' in my question_file?\r\n\r\nmy question_file is like: (llava_gqa_testdev_balanced.jsonl)\r\n```\r\n{\"question_id\": \"201307251\", \"image\": \"n161313.jpg\", \"text\": \"Is it overcast?\\nAnswer the question using a single word or phrase.\", \"category\": \"default\"}\r\n{\"question_id\": \"201640614\", \"image\": \"n235859.jpg\", \"text\": \"Who is wearing the dress?\\nAnswer the question using a single word or phrase.\", \"category\": \"default\"}\r\n{\"question_id\": \"202225914\", \"image\": \"n336443.jpg\", \"text\": \"Does the utensil on top of the table look clean and black?\\nAnswer the question using a single word or phrase.\", \"category\": \"default\"}\r\n```\r\nand when I run the eval.py, I come across the error.\r\n```\r\nTraceback (most recent call last):\r\n  File \"eval.py\", line 360, in <module>\r\n    gold = question[\"answer\"]\r\nKeyError: 'answer'\r\n```\r\nFrom the question_file, there is indeed no 'answer' key in the file. I wonder what did you do to this eval.py? I comment some lines out as mentioned above, but there are still bugs.\n</Comment>\n<Comment by LostXine at 2023-11-01T21:45:15Z>\nJust a quick summary of how to fix `eval.py` based on the comments above, please correct me if I'm wrong:\r\n\r\n1. At Line 77, change `default=\"{tier}_all_questions.json\"` to `default=\"{tier}_questions.json\"`\r\n2. Comment out the following lines:\r\na. Line 120 `scenes = ...`\r\nb. Line 128 `choices = ...`\r\nc. Line 370-375 `valid = ...` - `scores[\"plausibility\"].append(toScore(plausible))`\r\nd. Line 390 `updateConsistency(qid, question, questions)`\r\n\r\nHope it helps.\n</Comment>\n<Comment by haotian-liu at 2023-11-04T19:15:15Z>\nSorry for the confusion, and thank you all for helping clarifying the evaluation process.\r\n\r\nI have updated the docs and include the modified eval.py in GitHub Gist.\r\n\r\n> Download the [data](https://cs.stanford.edu/people/dorarad/gqa/download.html) and [evaluation scripts](https://cs.stanford.edu/people/dorarad/gqa/evaluate.html) following the official instructions and put under `./playground/data/eval/gqa/data`. You may need to modify `eval.py` as [this](https://gist.github.com/haotian-liu/db6eddc2a984b4cbcc8a7f26fd523187) due to the missing assets in the GQA v1.2 release.\r\n\r\nPlease let me know if you still encounter any issues in terms of GQA eval. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 583,
    "state": "open",
    "created_by": "llpin1992",
    "created_at": "2023-10-15T12:05:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/583</URL>\n\n<TITLE>[Question]  problem porting to other LLM</TITLE>\n\n<BODY>thanks to the author. When porting to other LLM，such as baichuan 13B or qwen, there are a lot of warnings when doing finetuning, but the dataset LLaVA-Instruct-150K seem to right, What could be the problem？ thank you ！\r\n\r\nWARNING: tokenization mismatch: 406 vs. 410. (ignored)\r\nWARNING: tokenization mismatch: 504 vs. 508. (ignored)\r\nWARNING: tokenization mismatch: 296 vs. 300. (ignored)\r\nWARNING: tokenization mismatch: 257 vs. 261. (ignored)\r\nWARNING: tokenization mismatch: 215 vs. 219. (ignored)\r\nWARNING: tokenization mismatch: 676 vs. 680. (ignored)\r\nWARNING: tokenization mismatch: 145 vs. 148. (ignored)\r\n\r\nRelated Issue：https://github.com/haotian-liu/LLaVA/issues/346</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-23T21:08:17Z>\nHi, can you share a repo that allows me to easily reproduce this? Because we have supported MPT which has a different prompt format, and it is unclear where the issue stems from. Thanks.\n</Comment>\n<Comment by zjh908491372 at 2023-11-09T11:26:54Z>\nSame problem, stage1 is fine, stage2 will report this warning, only the model(baichuan2-6b) is replaced, and the dataset is not modified\n</Comment>\n<Comment by aneet-javis at 2023-12-03T15:59:41Z>\n@zjh908491372 Could you share what all changes are to be made to change the llm in LLaVA? Thanks,\n</Comment>\n<Comment by xiechengmude at 2023-12-06T15:26:22Z>\n> Hi, can you share a repo that allows me to easily reproduce this? Because we have supported MPT which has a different prompt format, and it is unclear where the issue stems from. Thanks.\r\n\r\n\r\n\r\nwe are training a model based on Yi-6b in pretrain stage and received the same WARNING like\r\n\r\n\r\nWARNING: tokenization mismatch: 924 vs. 928. (ignored)\r\nWARNING: tokenization mismatch: 922 vs. 926. (ignored)\r\nWARNING: tokenization mismatch: 462 vs. 464. (ignored)\r\nWARNING: tokenization mismatch: 246 vs. 248. (ignored)\r\nWARNING: tokenization mismatch: 333 vs. 337. (ignored)\r\nWARNING: tokenization mismatch: 980 vs. 990. (ignored)\r\nWARNING: tokenization mismatch: 169 vs. 171. (ignored)\r\nWARNING: tokenization mismatch: 1241 vs. 1243. (ignored)\r\nWARNING: tokenization mismatch: 335 vs. 337. (ignored)\r\nWARNING: tokenization mismatch: 230 vs. 232. (ignored)\r\nWARNING: tokenization mismatch: 181 vs. 183. (ignored)\r\nWARNING: tokenization mismatch: 761 vs. 765. (ignored)\r\nWARNING: tokenization mismatch: 1876 vs. 1882. (ignored)\r\nWARNING: tokenization mismatch: 160 vs. 162. (ignored)\r\nWARNING: tokenization mismatch: 86 vs. 88. (ignored)\r\nWARNING: tokenization mismatch: 90 vs. 92. (ignored)\r\nWARNING: tokenization mismatch: 328 vs. 330. (ignored)\r\nWARNING: tokenization mismatch: 242 vs. 246. (ignored)\r\nWARNING: tokenization mismatch: 1188 vs. 1192. (ignored)\n</Comment>\n<Comment by 20191864218 at 2024-02-22T16:14:36Z>\n\"I want to replace LLM with Qwen-7B. How can I make this modification?Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 582,
    "state": "closed",
    "created_by": "xinsir6",
    "created_at": "2023-10-15T11:48:30Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/582</URL>\n\n<TITLE>Could you please provide a simple script to use your multimodel like huggingface or other multimodels? [Feature request]</TITLE>\n\n<BODY>### feature\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/128811208/5a736785-67b6-4a24-b622-f6b4986c4668)\r\nlike blip2 or other  huggingface models，many people want to use your model for their domain applications,  we want an easy to use method, amazing at your result to be better than gpt4. If you can provide a simple scripts to run your llava1.5 model, I will be very appreciate for it.</BODY>\n\n<COMMENTS>\n<Comment by XenioxYT at 2023-10-15T21:19:38Z>\nSame here\n</Comment>\n<Comment by jiyounglee-0523 at 2023-10-16T06:30:36Z>\nSame here\n</Comment>\n<Comment by Niez-Gharbi at 2023-10-18T15:08:34Z>\nHave you found a solution yet ?\n</Comment>\n<Comment by DragosDima96 at 2023-10-19T13:50:59Z>\nYou can already instantiate the model using Hugginface ? (from model the builder.py()). And then I belive you can follow instructions from https://github.com/camenduru/LLaVA-colab/blob/main/LLaVA_13b_8bit_colab.ipynb to apply the model for text/embeddings). Right or am I missing something?\n</Comment>\n<Comment by Niez-Gharbi at 2023-10-25T15:59:26Z>\n@DragosDima96 You're right, it works. The only problem is that LLaVA 1.5 13b is very large on colab, so I have worked with llava-v1.5-7b-5GB.\n</Comment>\n<Comment by haotian-liu at 2023-11-04T23:43:43Z>\nWe added the get started section in README, contributed by the community. We are also working on the integration to HF transformers. Thanks!\r\n\r\nhttps://github.com/haotian-liu/LLaVA#quick-start-with-huggingface\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 579,
    "state": "closed",
    "created_by": "Maxlinn",
    "created_at": "2023-10-15T04:16:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/579</URL>\n\n<TITLE>[Question] loss frequently drops to 0.2 while training second-stage of llava-v1.5-13b</TITLE>\n\n<BODY>### Question\n\nhi liu and the team, thanks for bringing about llava!\r\n\r\nrecently i'm trying to reproduce the seond-stage(instruct fine-tuning) of llava-v1.5-13b. i followed the guidance of [LLaVA#visual-instruction-tuning](https://github.com/haotian-liu/LLaVA#visual-instruction-tuning) to prepare data and run. the pretrained first-stage mm_projector was obtained from [liuhaotian/llava-v1.5-mlp2x-336px-pretrain-vicuna-13b-v1.5](https://huggingface.co/liuhaotian/llava-v1.5-mlp2x-336px-pretrain-vicuna-13b-v1.5). \r\n\r\ni trained on a single node with 8x80G vram. the start script is identical to [scripts/v1_5/finetune.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune.sh), however after training somewhile, the loss diagram seems strange, as it frequently drops to about 0.2 and recover to around 0.8.\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/43326876/0dc00836-cfb8-498a-9885-f98f6704d03f)\r\n\r\nat first, i was skeptical of ocr-vqa data since it needs crawling. so i removed all 80000 ocr-vqa converstations from llava_v1_5_mix665k.json. however, the loss still drops often.\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/43326876/9920bea2-c6c8-4966-b11a-e2dca5ed0f9b)\r\n\r\nis this normal? could you please give me some hint? much thanks!</BODY>\n\n<COMMENTS>\n<Comment by Maxlinn at 2023-10-15T05:09:00Z>\ni did some ablation on data and found it occurs when image-text instructions **mixed with pure text instructions**, with `--group_by_modality_length True`.\r\n\r\ni did exp on training with pure coco, gqa, textvqa with `--group_by_modality_length False`, the cuves has no down spikes.\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/43326876/963c9395-84a8-44fe-9fd5-1588b8fb1de4)\r\n\r\nit is understandable, but i still want to confirm that down spike occurs when llava-v1.5-13b trained officially?\n</Comment>\n<Comment by xmy0916 at 2023-10-15T13:44:27Z>\ni have trained my model for 5 epoch, i discovered other interesting phenomena, the loss seems to decrease significantly at the beginning of each epoch, and then remains basically the same throughout the epoch.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/43675899/f08f66ee-31f7-4d37-b2a1-fce90d7a1f0c)\n</Comment>\n<Comment by Maxlinn at 2023-10-15T15:43:47Z>\n> i have trained my model for 5 epoch, i discovered other interesting phenomena, the loss seems to decrease significantly at the beginning of each epoch, and then remains basically the same throughout the epoch. ![image](https://user-images.githubusercontent.com/43675899/275329695-f08f66ee-31f7-4d37-b2a1-fce90d7a1f0c.png)\r\n\r\nmuch thanks for sharing the diagram! \r\n\r\nbased on my previous experiences on VLM, the fast-then-flat tendency on loss surely exists(despite mixture of textual data causing down spikes). the loss is dropping slowly afterwards but the training is still important for better model evalutaion, the loss does not necessaily reflects performance.\r\n\r\nby the way, i wonder if you ever come across that when training on 8 gpus, the first gpu uses up vram but do not have any usage?  when i was doing instruct fine-tuning on llava it appears so, very strange.\n</Comment>\n<Comment by xmy0916 at 2023-10-16T02:28:45Z>\n@Maxlinn the first gpu uses up vram but do not have any usage? when i was doing instruct fine-tuning on llava it appears so, very strange。sry，这句话我没看懂啥意思，要不in chinese？hhhh，你的意思是第一张gpu显存打满了，但是利用率为0？\n</Comment>\n<Comment by Maxlinn at 2023-10-16T02:31:05Z>\n> @Maxlinn the first gpu uses up vram but do not have any usage? when i was doing instruct fine-tuning on llava it appears so, very strange。sry，这句话我没看懂啥意思，要不in chinese？hhhh，你的意思是第一张gpu显存打满了，但是利用率为0？\r\n\r\n是的，八张卡训练，第一张卡显存用满了但是利用率一直是0，后面7张卡显存和占用率都是满的，用的是[haotian-liu/LLaVA#visual-instruction-tuning](https://github.com/haotian-liu/LLaVA#visual-instruction-tuning)的脚本。\n</Comment>\n<Comment by haotian-liu at 2023-10-16T04:29:17Z>\nHi, these losses are normal. The spikes are mainly due to the ShareGPT data used as you observed in your ablation, as our base model Vicuna has already been trained on these samples (probably for 3 epochs). We use these samples as a sort of regularization to prevent forgetting of the LLM knowledge/capabilities. It is perhaps a good phenomenon for us to see the spikes (to low loss). Also, the sudden drop in losses between epochs is observed in both LLM training and LMM training.\r\n\r\nBelow I provide the loss curves for both stage training of LLaVA-v1.5-13B for your reference.\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/6631389/2753dfb3-b409-4f2e-9e21-950b529ed991)\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/6631389/c3bc77ab-1e16-4058-820b-de00214acdc5)\n</Comment>\n<Comment by Maxlinn at 2023-10-16T04:34:17Z>\nthanks for sharing the diagram! i observed similiar loss values and spikes so i guess nothing wrong.\n</Comment>\n<Comment by xmy0916 at 2023-10-16T06:15:38Z>\n> > @Maxlinn the first gpu uses up vram but do not have any usage? when i was doing instruct fine-tuning on llava it appears so, very strange。sry，这句话我没看懂啥意思，要不in chinese？hhhh，你的意思是第一张gpu显存打满了，但是利用率为0？\r\n> \r\n> 是的，八张卡训练，第一张卡显存用满了但是利用率一直是0，后面7张卡显存和占用率都是满的，用的是[haotian-liu/LLaVA#visual-instruction-tuning](https://github.com/haotian-liu/LLaVA#visual-instruction-tuning)的脚本。\r\n\r\n这个我没太注意，后面再训练时候观察下\n</Comment>\n<Comment by Maxlinn at 2023-10-17T13:43:09Z>\n> > > @Maxlinn the first gpu uses up vram but do not have any usage? when i was doing instruct fine-tuning on llava it appears so, very strange。sry，这句话我没看懂啥意思，要不in chinese？hhhh，你的意思是第一张gpu显存打满了，但是利用率为0？\r\n> > \r\n> > \r\n> > 是的，八张卡训练，第一张卡显存用满了但是利用率一直是0，后面7张卡显存和占用率都是满的，用的是[haotian-liu/LLaVA#visual-instruction-tuning](https://github.com/haotian-liu/LLaVA#visual-instruction-tuning)的脚本。\r\n> \r\n> 这个我没太注意，后面再训练时候观察下\r\n\r\n最后发现是我机器显示的问题，与代码无关，感谢讨论！\n</Comment>\n<Comment by July-zh at 2023-10-20T07:58:20Z>\n哈喽，我用一台8卡A100（40G）训练的loss曲线跟80G的有些出入，大家感觉这个正常吗？\r\n配置是这样的：\r\n![image](https://github.com/haotian-liu/LLaVA/assets/39486987/2ffa5fce-2e6f-4ee1-b95a-99253989688b)\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/39486987/9798026f-0189-405e-b496-0043fbe5b659)\n</Comment>\n<Comment by haotian-liu at 2023-10-23T06:31:21Z>\nHi @July-zh \r\n\r\nThis theoretically should not cause issues, as this is literally mixing text-only data and image-only data within gradient accumulation steps. This commit (f16049a3d35e281a4d88b7a2cb0a7f36a0c677ef) makes the gradient accumulation behavior consistent with large batch training. Either case, your training should be normal.\r\n\r\nAlso, (sorry for the delay), LoRA training checkpoints will be released in next few days (very likely tomorrow), which would allow training 13B checkpoint with 8x A100-40G.\n</Comment>\n<Comment by linhaojia13 at 2023-11-24T08:22:19Z>\n> Also, the sudden drop in losses between epochs is observed in both LLM training and LMM training.\r\n\r\n@haotian-liu However, LLaVA is trained for only 1 epoch. Is this because training for more than 1 epoch leads to overfitting, even though it might result in a lower training loss?\n</Comment>\n<Comment by bpwl0121 at 2023-12-05T05:54:13Z>\n> apabilities. It is perhaps a good phenomenon for us to see the spikes (to low loss). Also, the sudden drop in losses between epochs is observed in both LLM tr\r\n\r\nhi haotian, thanks for you detailed reply. \r\ndid you do any ablation study about the pure text dataset do prevent the forgetting of llava？\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 578,
    "state": "closed",
    "created_by": "Carol-lyh",
    "created_at": "2023-10-14T13:54:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/578</URL>\n\n<TITLE>[Usage]</TITLE>\n\n<BODY>Excuse me, What is the difference of v1 and plain on input argument --version?\r\nAnd why mm_use_im_start_end and mm_use_im_patch_token are False here in your latest llava-v1.5??</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-04T23:37:53Z>\nThese are changes made for v1 checkpoints back in June/July.\r\n\r\nTokenization. We remove the dependency of the additional tokens (<IM_START>, <IM_END>, <IM_PATCH>), so that during the pretraining stage, the tokenizer does not change at all and we only update the linear projector weights.\r\nPrompt.\r\nPretraining. We simplified the pretraining prompts by removing additional instructions like Describe the image details, which we find to allow the zero-shot inference and can slightly improve the training speed.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/releases/tag/v1.0.1\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 577,
    "state": "open",
    "created_by": "LH019",
    "created_at": "2023-10-14T13:45:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/577</URL>\n\n<TITLE>About projector weights</TITLE>\n\n<BODY>### Question\n\nhi, if i choose [liuhaotian/llava-v1.5-7b](https://huggingface.co/liuhaotian/llava-v1.5-7b) as my bone model, which projector weights should download? I tried to download LLaMA-2-7B-Chat, but it will encounter the error like \r\nError(s) in loading state_dict for Sequential:\r\n        Missing key(s) in state_dict: \"0.weight\", \"0.bias\", \"2.weight\", \"2.bias\". \r\n        Unexpected key(s) in state_dict: \"weight\", \"bias\". \r\nso what should i do to solve this issue?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-16T03:53:28Z>\nIt's this one: https://huggingface.co/liuhaotian/llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5\n</Comment>\n<Comment by adrielkuek at 2023-10-30T08:09:45Z>\nHi. In reference to this question, I'm a little bit confused on the selection of the projector weights. As mentioned by the OP that should we want to fine-tune from llava-v1.5-7b, the projector weights should be extracted from https://huggingface.co/liuhaotian/llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5. I was under the impression that llava-v1.5-7b was pretrained from llama-2 chat rather than vicuna v1.5? Thanks for the clarification!\n</Comment>\n<Comment by haotian-liu at 2023-10-30T16:50:39Z>\nSo there are two types of finetuning:\r\n\r\n1. You start with a Vicuna (pure LLM), and you want to finetune it for multimodal capability (visual image reasoning). This, you start with a Vicuna, pretrain a connector (or use our [pretrained](https://huggingface.co/liuhaotian/llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5) one), and finetune on visual instruction tuning data mixture, and you obtain LLaVA-v1.5. During the \"finetuning\" process, LLM and the projector are both updated.\r\n2. You start with LLaVA-v1.5 (already having visual capability), and you want to finetune it further for a specific task. This, you start with LLaVA-v1.5, and do not worry about projector at all because it is already there, and finetune it to the task data using [this](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task.sh) script.\n</Comment>\n<Comment by adrielkuek at 2023-10-30T22:31:17Z>\n> So there are two types of finetuning:\n> \n> \n> \n> 1. You start with a Vicuna (pure LLM), and you want to finetune it for multimodal capability (visual image reasoning). This, you start with a Vicuna, pretrain a connector (or use our [pretrained](https://huggingface.co/liuhaotian/llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5) one), and finetune on visual instruction tuning data mixture, and you obtain LLaVA-v1.5. During the \"finetuning\" process, LLM and the projector are both updated.\n> \n> 2. You start with LLaVA-v1.5 (already having visual capability), and you want to finetune it further for a specific task. This, you start with LLaVA-v1.5, and do not worry about projector at all because it is already there, and finetune it to the task data using [this](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task.sh) script.\n\nThanks for the clarifications! Can I check then for (2), finetuning with the specific task starting from LLaVA-1.5 with a custom visual instruction dataset, are we \"finetuning\" both the LLM (I suppose is Vicuna v1.5 in this case) as well as the projector?\n</Comment>\n<Comment by haotian-liu at 2023-10-30T23:27:55Z>\nYes we are finetuning both the LLM and the projector, but this LLM is a finetuned LLM from LLaVA-1.5.\r\n\r\n1. Init\r\n   1. LLM: Vicuna\r\n   2. proj: None\r\n2. Pretrain:\r\n   1. LLM: Vicuna\r\n   2. proj: llava-pretrain\r\n3. Instruction tuning (resulting model: LLaVA-1.5)\r\n   1. LLM: Vicuna-llava-finetune\r\n   2. proj: llava-pretrain-llava-finetune\r\n4. Task-specific finetuning (resulting model: finetuned LLaVA-1.5)\r\n   1. LLM: Vicuna-llava-finetune-task-finetune\r\n   2. proj: llava-pretrain-llava-finetune-task-finetune\r\n\r\nHope this makes it clear on the weights being modified in each stage.\n</Comment>\n<Comment by Road2Redemption at 2023-11-17T03:34:17Z>\nHi！Thank you for your work! I was wondering what would happen if I used finetune_lora.sh based on LLaVA-1.5? In the script there is the projector argument and I might have realized I had incorrectly used that argument after reading the discussion above. So is there any downsides of doing this or it just cannot perform the best ability of LLaVA?\r\nHere is my script:\r\ndeepspeed --include=\"localhost:2,3\" \\\r\n    --master_port='29501' \\\r\n    llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --model_name_or_path /data6/xyc/models/llava-v1.5-7b \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/train_mix.json \\\r\n    --image_folder /data6/xyc/data/baseline_data-v2/train \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter /data6/xyc/models/llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b-lora \\\r\n    --num_train_epochs 5 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\n</Comment>\n<Comment by luispky at 2025-06-20T00:38:37Z>\n> It's this one: https://huggingface.co/liuhaotian/llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5\n\nIs it possible to load this model directly with the transformers library, or should one manually download the weights? I tried with the `LlavaForConditionalGeneration` class, but I got:\n\n```\nOSError: liuhaotian/llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5 does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt\nor flax_model.msgpack.\n```\n\nCould you please share a code snippet?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 575,
    "state": "closed",
    "created_by": "CiaoHe",
    "created_at": "2023-10-14T11:06:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/575</URL>\n\n<TITLE>[BUG] Potential bug when do batch inference</TITLE>\n\n<BODY>Hi, \r\nI tried to modify the code for batch inference but found one bug,\r\n\r\nIn the `prepare_inputs_labels_for_multimodal` if we provide attention_mask but not given labels, then when execute L187, the  local variable `_new_labels` referenced before assignment, \r\nhttps://github.com/haotian-liu/LLaVA/blob/539d666848245a7075fb485cee5706e68e0489d2/llava/model/llava_arch.py#L187-L191\r\n\r\nSo I guess maybe we need to change the logic as L199-L202 did, that pad attention_mask just dependent on the `new_input_embeds.shape[1] - input_ids.shape[1]`\r\n\r\nBest,</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-05T05:07:09Z>\nHi, we have rewritten the multimodal processing function and this should be addressed. Also, I am trying to work on an efficient batch inference. Closing this and consolidating the discussion to https://github.com/haotian-liu/LLaVA/issues/754. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 573,
    "state": "closed",
    "created_by": "yix-chen",
    "created_at": "2023-10-14T09:32:10Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/573</URL>\n\n<TITLE>[Question] Could you please share the extracted video frames in SEED-Bench by extract_video_frames.py</TITLE>\n\n<BODY>### Question\n\nThe raw videos are too large to download</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-18T06:06:02Z>\nI guess there are some restrictions in terms of the dataset license? It may be better to consult the authors of SEED-Bench: https://github.com/AILab-CVC/SEED-Bench\r\n\r\nAnd please let me know if there are convenient ways of downloading provided by the dataset authors and I am more than happy to include in the README. Thanks.\n</Comment>\n<Comment by dsn01 at 2024-07-16T11:09:22Z>\n@yix-chen Hi! Have you solved your problem? Could you share me a link to download the extracted video frames in SEED-Bench by extract_video_frames.py? Thank you\n</Comment>\n<Comment by CnFaker at 2024-09-25T02:08:57Z>\n@yix-chen Hi! Have you solved your problem? Could you share me a link to download the extracted video frames in SEED-Bench by extract_video_frames.py? Thank you\n</Comment>\n<Comment by kinredon at 2025-03-21T05:37:18Z>\n@yix-chen @dsn01 @CnFaker Have you solved your problem? Could you share me a link to download the extracted video frames in SEED-Bench by extract_video_frames.py? Thank you. \n\nI found the official seed-bench team gives a seed video zip file in hugging face https://huggingface.co/datasets/AILab-CVC/SEED-Bench/tree/main, where the extracted frames are present. For each video, there are 8 frames like this:\n\n![Image](https://github.com/user-attachments/assets/6e66f25a-8515-47ed-af17-7c23d5b43b77)\n\nHowever, I do not know how to correspond them to the results of using the file `extract_video_frames.py`. Any ideas or suggestions?\n</Comment>\n<Comment by HuangChiEn at 2025-04-07T06:39:51Z>\nnop, llava author suppose you download the video, instead of rely on official released video frame.\nSo, this https://github.com/haotian-liu/LLaVA/issues/1207#issuecomment-2782150112 you may take as reference.\n\nOtherwise, you can still follow Evaluation.md instruction to download the video, and run `extract_video_frames.py`.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 571,
    "state": "open",
    "created_by": "ZiruiSongBest",
    "created_at": "2023-10-14T08:50:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/571</URL>\n\n<TITLE>[Question] How to instantiate LlavaMetaModel in ./llava/model/llava_arch.py and what is its config format?</TITLE>\n\n<BODY>### Question\n\nHi! Very popular work. \r\nI am tring to do a white-box attack. So we'd like to utilize LLaVA's vision encoder and it's projector. Could you tell me how to instantiate them and what's required ”config“ （A variable） format?\r\n\r\nYour help would be greatly appreciated。</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 569,
    "state": "closed",
    "created_by": "July-zh",
    "created_at": "2023-10-14T06:15:55Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/569</URL>\n\n<TITLE>[Usage] Can I use 8*A100(40G) to train LLaVA-v1.5-13B?</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nWhen I train with DeepSpeed on 8 A100 GPUs (40GB each, two servers, each with 4 GPUs), I still encounter out-of-memory issues, even with a batch size of 1. Does this mean that LLaVA-v1.5-13B requires 80GB GPUs to train effectively? Are there any solutions to this?</BODY>\n\n<COMMENTS>\n<Comment by CHENGY12 at 2023-10-16T21:15:01Z>\nIt seems OOM even for the 7B model.  Do you have any solution?\n</Comment>\n<Comment by haotian-liu at 2023-10-17T19:06:49Z>\n@July-zh There are two potential solutions.\r\n1. Zero3-offload. This will slow down training as it uses part of the CPU RAM as VRAM\r\n2. LoRA training. We have found training with LoRA can fit 13B model with 8x A100 (40G). We are performing some preliminary hyperparameter search for LoRA and the checkpoints will be released later this week.\r\n\r\n@CHENGY12 8x A100 (40G) should work fine for 7B. Please share the command that you use, thanks.\n</Comment>\n<Comment by CHENGY12 at 2023-10-18T11:34:46Z>\nThanks a lot for the prompt response!  I only used 4x A100 (40G) for 7B before.  It may be the cause.  Thanks and looking forward to the LoRA version.\n</Comment>\n<Comment by haotian-liu at 2023-10-26T20:41:57Z>\nHi, we have released the LoRA ckpt, script, and a brief doc on the new schedule.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/releases/tag/v1.1.3\n</Comment>\n<Comment by daiqing98 at 2023-11-02T06:08:04Z>\n> Hi, we have released the LoRA ckpt, script, and a brief doc on the new schedule.\r\n> \r\n> https://github.com/haotian-liu/LLaVA/releases/tag/v1.1.3\r\n\r\n@haotian-liu Hi, I am using finetune_lora.sh with 8*A100 (40G), but still suffering from OOM error. It ran for sevaral iterations, then reported GPU OOM. May I ask any solutions?\r\n\r\nMay I ask any solutions?\n</Comment>\n<Comment by haotian-liu at 2023-11-02T06:10:05Z>\n@daiqing98 please share your command\n</Comment>\n<Comment by daiqing98 at 2023-11-02T06:18:27Z>\n@haotian-liu \r\n\r\n`#!/bin/bash\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path ./checkpoints/vicuna-13b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/LLaVA-Instruct-150K/llava_v1_5_mix665k.json \\\r\n    --image_folder ./playground/data \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/my-llava-v1.5-13b-pretrain/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/my-llava-v1.5-13b-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb`\n</Comment>\n<Comment by haotian-liu at 2023-11-02T06:20:38Z>\noh i see you are running 13b. try bs=8, accu=2\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 564,
    "state": "closed",
    "created_by": "anonymous-atom",
    "created_at": "2023-10-13T12:32:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/564</URL>\n\n<TITLE>[Question] Training/Fine tuning on custom dataset</TITLE>\n\n<BODY>### Question\n\nWe want to train LLaVa-1.5 13B Model on some custom dataset, can someone refer me or help me how can I fine-tune it on custom dataset, also the dataset format required and other details.</BODY>\n\n<COMMENTS>\n<Comment by aiaicode at 2023-10-13T21:08:13Z>\nThis is the script to fine-tune v1.5 : [v1.5](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune.sh)\r\nThis is the script that trains the model : [train.py](https://github.com/haotian-liu/LLaVA/blob/main/llava/train/train.py#L754)\r\n\r\nFine tune procedure looks something like this :\r\n\r\n1. : Create dataset. Your dataset is supposed to be a json file in this format : \r\n`{\r\n    \"id\": \"000000243307\",\r\n    \"image\": \"000000243307.jpg\",\r\n    \"conversations\": [\r\n      {\r\n        \"from\": \"human\",\r\n        \"value\": \"Can you describe the main features of this image for me?\\n<image>\"\r\n      },\r\n      {\r\n        \"from\": \"gpt\",\r\n        \"value\": \"The image features a man sitting in a chair by himself, looking at a small flat-screen TV or monitor. He appears to be playing a game, as a keyboard and a mouse can be seen in front of him. .\"\r\n      }\r\n    ]\r\n  },`\r\n2. Keep the images ready with matching id. \r\n\r\n --data_path ./playground/data/llava_v1_5_mix665k.json \\\r\n --image_folder ./playground/data \\\r\n \r\nIn the script, replace the json file and replace the ./playground/data with your images folder\r\n\r\nAgain, I haven't done fine-tuning but this is what training looks like.\r\n\r\nPlease correct me if i am wrong @haotian-liu\n</Comment>\n<Comment by clima-ai at 2023-10-17T06:54:18Z>\nHow do we join the classes in our data to the classes already existing in the model? Should our directory (data) be organized in some special way, for example each class in a sub-directory?\n</Comment>\n<Comment by anonymous-atom at 2023-10-17T07:01:23Z>\n@aiaicode mentioned the steps, though I still didn't proceed with it, I will start training in a week.\nIn the meantime I would appreciate help from someone who trained/fine-tuned this model.\n</Comment>\n<Comment by ashhadulislam at 2023-10-22T17:48:33Z>\nHow much CPU/GPU is needed for fine tuning?\r\nIs there a way of sharing between RAM and GPU?\n</Comment>\n<Comment by nj159 at 2023-11-08T03:37:41Z>\n> 提到了步骤，虽然我还是没有继续，但我将在一周内开始训练。与此同时，我将感谢训练/微调此模型的人的帮助。\r\n\r\nI am a novice. Do you know the difference between the pre training dataset and fine-tuning dataset of llava? If we customize a dataset in a certain field, do we only need to prepare the fine-tuning dataset?Thank you very much！\n</Comment>\n<Comment by aa221 at 2023-11-12T03:58:14Z>\nCan I finetune with a CPU and No GPU... I don't see why I need that much compute power\n</Comment>\n<Comment by Hemachandirant at 2023-11-16T09:59:05Z>\nHi, can anyone help with how the image should be named? For eg. 000000506095.jpg\n</Comment>\n<Comment by JAYESH1304 at 2023-12-10T10:06:48Z>\nplease help me to know about how should i use model checkpoints for Fine-tuning LLaVa.\n</Comment>\n<Comment by AI-Aether at 2023-12-16T04:15:18Z>\n> Can I finetune with a CPU and No GPU... I don't see why I need that much compute power\r\n\r\nI think probably not because vision models is involved as well.\n</Comment>\n<Comment by sohaibsoussi at 2024-02-21T06:57:51Z>\nHi, I encountered a problem when running the shell script below for fine-tuning purposes. It always tells me that the 'lava' module is not found, even though I tried installing it using both conda and pip:\r\n- The shell script:\r\n#!/bin/bash\r\n\r\ndeepspeed /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/LLaVA/llava/train/train_mem.py\r\n    --deepspeed /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/LLaVA/scripts/zero2.json\r\n    --lora_enable True \\\r\n    --lora_r 128 \\\r\n    --lora_alpha 256 \\\r\n    --mm_projector_lr 2e-5 \\ \r\n    --bits 4 \\\r\n    --model_name_or_path /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/llava-v1.5-7b \\\r\n    --version llava_llama_2 \\ \r\n    --data_path /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/ok_vqa_dataset/train \\\r\n    --validation_data_path /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/ok_vqa_dataset/validation \\\r\n    --image_folder /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/ok_vqa_dataset/images \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/checkpoints/ok_vqa_finetuning\r\n    --num_train_epochs 500 \\\r\n    --per_device_train_batch_size 2 \\\r\n    --per_device_eval_batch_size 2 \\\r\n    --gradient_accumulation_step 64 \\\r\n    --evaluation_strategy \"epoch\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n\r\n- Log after runing the shell script:\r\n\r\n+ deepspeed /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/LLaVA/llava/train/train_mem.py\r\n[2024-02-20 23:14:05,751] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-02-20 23:14:07,655] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\n[2024-02-20 23:14:07,711] [INFO] [runner.py:568:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/LLaVA/llava/train/train_mem.py\r\n[2024-02-20 23:14:10,006] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-02-20 23:14:10,383] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}\r\n[2024-02-20 23:14:10,383] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0\r\n[2024-02-20 23:14:10,383] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\r\n[2024-02-20 23:14:10,383] [INFO] [launch.py:163:main] dist_world_size=1\r\n[2024-02-20 23:14:10,383] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0\r\n[2024-02-20 23:14:10,384] [INFO] [launch.py:253:main] process 1377826 spawned with command: ['/usr/bin/python3', '-u', '/home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/LLaVA/llava/train/train_mem.py', '--local_rank=0']\r\nTraceback (most recent call last):\r\n  File \"/home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/LLaVA/llava/train/train_mem.py\", line 1, in <module>\r\n    from llava.train.train import train\r\nModuleNotFoundError: No module named 'llava'\r\n[2024-02-20 23:14:11,386] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 1377826\r\n[2024-02-20 23:14:11,386] [ERROR] [launch.py:322:sigkill_handler] ['/usr/bin/python3', '-u', '/home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/LLaVA/llava/train/train_mem.py', '--local_rank=0'] exits with return code = 1\r\n+ --deepspeed /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/LLaVA/scripts/zero2.json\r\ntrain_ok_vqa.sh: 4: --deepspeed: not found\r\n+ --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5\r\ntrain_ok_vqa.sh: 5: --lora_enable: not found\r\n+ --bits 4 --model_name_or_path /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/llava-v1.5-7b --version llava_llama_2\r\ntrain_ok_vqa.sh: 9: --bits: not found\r\n+ --data_path /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/ok_vqa_dataset/train --validation_data_path /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/ok_vqa_dataset/validation --image_folder /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/ok_vqa_dataset/images --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/checkpoints/ok_vqa_finetuning\r\ntrain_ok_vqa.sh: 12: --data_path: not found\r\n+ --num_train_epochs 500 --per_device_train_batch_size 2 --per_device_eval_batch_size 2 --gradient_accumulation_step 64 --evaluation_strategy epoch --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb\r\ntrain_ok_vqa.sh: 24: --num_train_epochs: not found\n</Comment>\n<Comment by thebealfarisi at 2024-02-22T08:52:25Z>\nhow to fine tuning llava for non image dataset?\r\n\r\nusing this format:\r\n[\r\n    {\r\n        \"id\": \"unique_id\",\r\n        \"image\": \"\",\r\n        \"conversations\": [\r\n            {\r\n                \"from\": \"human\",\r\n                \"value\": \"{question}\"\r\n            },\r\n            {\r\n                \"from\": \"gpt\",\r\n                \"value\": \"{answer}\"\r\n            }\r\n        ]\r\n    }\r\n]\r\n\r\nor this format\r\n<s>[INST] <<SYS>> {prompt} <</SYS>> {question} [/INST] {answer}\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 563,
    "state": "open",
    "created_by": "xyxyjjj",
    "created_at": "2023-10-13T09:47:50Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/563</URL>\n\n<TITLE>[Question] ValueError: weight is on the meta device, we need a `value` to put in on 0.</TITLE>\n\n<BODY>### Question\n\n(llava) ~/autodl-tmp/LLaVA# python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1-0719-336px-lora-vicuna-13b-v1.3 --model-base vicuna-13b-v1.3\r\n\r\n[2023-10-13 17:34:01,478] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n2023-10-13 17:34:01 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='liuhaotian/llava-v1-0719-336px-lora-vicuna-13b-v1.3', model_base='vicuna-13b-v1.3', model_name=None, device='cuda', multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=False)\r\n2023-10-13 17:34:01 | INFO | model_worker | Loading the model llava-v1-0719-336px-lora-vicuna-13b-v1.3 on worker 56531b ...\r\nYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\r\n2023-10-13 17:34:03 | INFO | stdout | Loading LLaVA from base model...\r\nLoading checkpoint shards:   0%|                                                                                                                     | 0/3 [00:00<?, ?it/s]\r\nLoading checkpoint shards:  33%|████████████████████████████████████▎                                                                        | 1/3 [00:13<00:27, 13.68s/it]\r\nLoading checkpoint shards:  67%|████████████████████████████████████████████████████████████████████████▋                                    | 2/3 [00:24<00:11, 11.77s/it]\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:29<00:00,  8.88s/it]\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:29<00:00,  9.85s/it]\r\n2023-10-13 17:34:33 | ERROR | stderr | \r\n2023-10-13 17:34:34 | ERROR | stderr | Traceback (most recent call last):\r\n2023-10-13 17:34:34 | ERROR | stderr |   File \"/root/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n2023-10-13 17:34:34 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n2023-10-13 17:34:34 | ERROR | stderr |   File \"/root/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n2023-10-13 17:34:34 | ERROR | stderr |     exec(code, run_globals)\r\n2023-10-13 17:34:34 | ERROR | stderr |   File \"/root/autodl-tmp/LLaVA/llava/serve/model_worker.py\", line 275, in <module>\r\n2023-10-13 17:34:34 | ERROR | stderr |     worker = ModelWorker(args.controller_address,\r\n2023-10-13 17:34:34 | ERROR | stderr |   File \"/root/autodl-tmp/LLaVA/llava/serve/model_worker.py\", line 65, in __init__\r\n2023-10-13 17:34:34 | ERROR | stderr |     self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\r\n2023-10-13 17:34:34 | ERROR | stderr |   File \"/root/autodl-tmp/LLaVA/llava/model/builder.py\", line 50, in load_pretrained_model\r\n2023-10-13 17:34:34 | ERROR | stderr |     model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs)\r\n2023-10-13 17:34:34 | ERROR | stderr |   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2959, in from_pretrained\r\n2023-10-13 17:34:34 | ERROR | stderr |     dispatch_model(model, **kwargs)\r\n2023-10-13 17:34:34 | ERROR | stderr |   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/big_modeling.py\", line 371, in dispatch_model\r\n2023-10-13 17:34:34 | ERROR | stderr |     attach_align_device_hook_on_blocks(\r\n2023-10-13 17:34:34 | ERROR | stderr |   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 536, in attach_align_device_hook_on_blocks\r\n2023-10-13 17:34:34 | ERROR | stderr |     attach_align_device_hook_on_blocks(\r\n2023-10-13 17:34:34 | ERROR | stderr |   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 536, in attach_align_device_hook_on_blocks\r\n2023-10-13 17:34:34 | ERROR | stderr |     attach_align_device_hook_on_blocks(\r\n2023-10-13 17:34:34 | ERROR | stderr |   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 536, in attach_align_device_hook_on_blocks\r\n2023-10-13 17:34:34 | ERROR | stderr |     attach_align_device_hook_on_blocks(\r\n2023-10-13 17:34:34 | ERROR | stderr |   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 506, in attach_align_device_hook_on_blocks\r\n2023-10-13 17:34:34 | ERROR | stderr |     add_hook_to_module(module, hook)\r\n2023-10-13 17:34:34 | ERROR | stderr |   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 155, in add_hook_to_module\r\n2023-10-13 17:34:34 | ERROR | stderr |     module = hook.init_hook(module)\r\n2023-10-13 17:34:34 | ERROR | stderr |   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 253, in init_hook\r\n2023-10-13 17:34:34 | ERROR | stderr |     set_module_tensor_to_device(module, name, self.execution_device)\r\n2023-10-13 17:34:34 | ERROR | stderr |   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/utils/modeling.py\", line 267, in set_module_tensor_to_device\r\n2023-10-13 17:34:34 | ERROR | stderr |     raise ValueError(f\"{tensor_name} is on the meta device, we need a `value` to put in on {device}.\")\r\n2023-10-13 17:34:34 | ERROR | stderr | ValueError: weight is on the meta device, we need a `value` to put in on 0.\r\n\r\nMy device:\r\nGPU 0: NVIDIA GeForce RTX 3090\r\n\r\nHow can I solve this problem?</BODY>\n\n<COMMENTS>\n<Comment by xj66212 at 2024-01-06T09:47:27Z>\ni have the same question, how to solve this problem?\n</Comment>\n<Comment by Yuezeyi at 2024-02-23T10:16:07Z>\nsame question, anyone solved it?\n</Comment>\n<Comment by Mike-ihr at 2024-09-13T09:15:56Z>\nI have the same question and my device is as same as yours, how to cope with this problem?\n</Comment>\n<Comment by Mike-ihr at 2024-09-14T02:01:54Z>\nyou need to set the device_map = \"auto\" when you use the from_pretrained function. More details can refer to https://huggingface.co/docs/accelerate/usage_guides/big_modeling\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 562,
    "state": "closed",
    "created_by": "liuheng0111",
    "created_at": "2023-10-13T08:26:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/562</URL>\n\n<TITLE>[Question] Why model_vqa_loader.py batch_size must be 1</TITLE>\n\n<BODY>### Question\n\nI try set batch_size not be 1, and use pad_sequence to pad the input_ids to the same length, but the output text is confused. Can you tell me how to use batch inference in evaluation?\r\n\r\n`input_ids = torch.nn.utils.rnn.pad_sequence(\r\n  input_ids,\r\n  batch_first=True,\r\n  padding_value=self.tokenizer.pad_token_id)\r\nbatch = dict(\r\n            input_ids=input_ids,\r\n            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\r\n        )`</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-05T05:06:17Z>\nHi, thank you, and I am trying to work on an efficient batch inference. Closing this and consolidating the discussion to https://github.com/haotian-liu/LLaVA/issues/754. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 561,
    "state": "closed",
    "created_by": "buaachen1993",
    "created_at": "2023-10-13T07:52:45Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/561</URL>\n\n<TITLE>[Question] web server got error: {\"detail\":\"Not Found\"}</TITLE>\n\n<BODY>### Question\n\nweb server got only {\"detail\":\"Not Found\"} shown in the picture below:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/6391783/5231544a-f26c-48f8-8285-9113b2fef99c)\r\n\r\nlog like this \r\n<img width=\"1066\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/6391783/7bc21cb2-0286-44df-8383-ba7c34b6b018\"></BODY>\n\n<COMMENTS>\n<Comment by WanFeng123313 at 2023-10-18T09:06:27Z>\nHello, I also encountered this problem, may I ask you to solve it?\n</Comment>\n<Comment by haotian-liu at 2023-10-19T01:22:40Z>\nDo you visit the web demo at port 7860 (gradio demo)?\n</Comment>\n<Comment by WanFeng123313 at 2023-10-19T01:59:57Z>\nThank you. I've successfully run it locally\n</Comment>\n<Comment by buaachen1993 at 2023-10-19T10:30:04Z>\nThanks a lot. I've successfully run it locally too.\n</Comment>\n<Comment by huanranchen at 2024-01-29T02:06:39Z>\n> Do you visit the web demo at port 7860 (gradio demo)?\r\n\r\nHi, could we change the port of gradio (because 7860 is already used by minigpt4)\n</Comment>\n<Comment by ATing0203 at 2024-03-07T09:02:41Z>\n> Do you visit the web demo at port 7860 (gradio demo)?\r\n\r\nHello, when I visit the web demo on port 7860, the page does not load. Why is this?\n</Comment>\n<Comment by ATing0203 at 2024-03-08T02:06:45Z>\nand the page is blank.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/161042646/551e5b53-2ed5-44e1-b9e4-2d3cd4351a4e)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 560,
    "state": "open",
    "created_by": "DietDietDiet",
    "created_at": "2023-10-13T07:52:23Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/560</URL>\n\n<TITLE>[Usage] Training speed for visual instruction tuning</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI am finetuning llava1.5-7B on 8 * A100 40G,  and modified bs & accumulation steps accordingly.\r\nThe estimated training time is approx. 24h.\r\nWhat could go wrong?\r\n\r\nEnv:\r\ncuda11.7\r\ntorch2.0.1\r\nflash-attn 2.3.2\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\nexport CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path lmsys/vicuna-7b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path data/llava_v1_5_mix665k.json.bak \\\r\n    --image_folder ./playground/data \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter projectors/llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 4 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 8 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-20T03:29:13Z>\nFor 7B model, you probably do not need to set the batch size to be so small, maybe 8x2 is sufficient.\r\n\r\nAlso, are your GPUs NVLinked?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 559,
    "state": "closed",
    "created_by": "Rickylht",
    "created_at": "2023-10-13T07:51:55Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/559</URL>\n\n<TITLE>[Question] Change to a new vision encoder</TITLE>\n\n<BODY>### Question\n\nHello, It`s really a great work.\r\n\r\nIn llava.model.multimodel_encoder.clip_encoder.py, if I directly change the vision encoder(e.g. to biomedclip) and load original llava-v1.5 model, the performance become worse because the implicit embeddings are not aligned between new vision encoders`output and the projector?\r\n\r\nSo if I want to change to a new vision encoder, I can only load other llama models and train a new projector first. Am i right?</BODY>\n\n<COMMENTS>\n<Comment by ZiruiSongBest at 2023-10-14T07:50:14Z>\nHi I am a little confused about changing the Vision encoder. Could you show your code after changing the vision encoder? \r\nThanks very much.\n</Comment>\n<Comment by CR400AF-A at 2023-10-16T15:43:46Z>\nI think that you need to train a new projector first, in order to align image and text feature between images and texts.  That means you need to do stage1 pretrain again.\r\nBy the way, how do you modify the code so that it can use a new vision encoder? I think only modifying the clip_encoder.py is not enough.\n</Comment>\n<Comment by Rickylht at 2023-10-17T01:50:31Z>\nI'm now working on replacing a vision encoder with an open_clip one. I think you need to change the vision outputs into last hidden layers. Other changes, e.g. in train.py, you need to change the corresponding preprocess function. I think it's enough. Dimension of projectors will automatically get from clip_encoder.py in `def hidden_size(self):` function.\n</Comment>\n<Comment by gyupro at 2023-10-18T06:29:27Z>\nWhat about finetuning existing openai clip model? Wouldn't it be easier? I want to adjust vision weights and I am wondering if I can achieve this by just sticking finetuned-openai-clip model\n</Comment>\n<Comment by Rickylht at 2023-10-25T08:04:39Z>\n> What about finetuning existing openai clip model? Wouldn't it be easier? I want to adjust vision weights and I am wondering if I can achieve this by just sticking finetuned-openai-clip model\r\n\r\nIf you use a new vision encoder, you need to train a new projector first and then finetune the whole model. \r\n\r\nI think it depends on your task whether to train a new clip model or not. E.g. in medical or other specific field, a new vision encoder is necessary. If it's a task on natural images, the openai or openclip pretrained weights are enough but focus more on instruction finetuning.\n</Comment>\n<Comment by gyupro at 2023-10-25T09:18:06Z>\n> > What about finetuning existing openai clip model? Wouldn't it be easier? I want to adjust vision weights and I am wondering if I can achieve this by just sticking finetuned-openai-clip model\r\n> \r\n> If you use a new vision encoder, you need to train a new projector first and then finetune the whole model.\r\n> \r\n> I think it depends on your task whether to train a new clip model or not. E.g. in medical or other specific field, a new vision encoder is necessary. If it's a task on natural images, the openai or openclip pretrained weights are enough but focus more on instruction finetuning.\r\n\r\nHi. I understand I need to train projector first if I use new vision encoder. I was just wondering if I can finetune existing openai-336px encoder and train like lava did.\r\n\r\n I am working on translating manga. I need to train more images to fit the vision encoder to be accurate. The domain doesn't seem much different since manga is based on human lives but I want it to be more accurate.\r\n\r\nyour answer helps a lot. thx\n</Comment>\n<Comment by YangQiuEric at 2024-02-28T22:25:26Z>\nHave you found out how to change CLIP to biomedclip? Appreciate it if you could give me more info!\n</Comment>\n<Comment by lucasjinreal at 2024-04-11T06:43:22Z>\nHow to using openclip?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 558,
    "state": "open",
    "created_by": "henrycjh",
    "created_at": "2023-10-13T03:41:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/558</URL>\n\n<TITLE>[Question] the FIXME in llava_arch.py</TITLE>\n\n<BODY>### Question\n\nHi， I am curious about the reason you do this fix. I think cur_image_features[0:0] is just an empty tensor.\r\n`\r\n  #FIXME: this is a hacky fix, for deepspeed zero3 to work\r\n`\r\n`\r\n  cur_input_embeds = torch.cat([cur_input_embeds_1, cur_image_features[0:0], cur_input_embeds_2], dim=0)\r\n`</BODY>\n\n<COMMENTS>\n<Comment by hangzeli05 at 2023-12-04T04:17:47Z>\n我也有这个疑问，请问解决了吗\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 554,
    "state": "open",
    "created_by": "ghost",
    "created_at": "2023-10-12T19:18:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/554</URL>\n\n<TITLE>[Question] Swapping LLM part of model</TITLE>\n\n<BODY>### Question\n\nHey, amazing work.\r\n\r\nI would like to change the finetuned Vicuna model with Mistral-7B model.\r\n\r\nThanks...</BODY>\n\n<COMMENTS>\n<Comment by liyang-7 at 2023-11-08T02:06:52Z>\nMay I ask if Mistral-7B is successful trained? \r\nWhat are the main modifications that need to be made?\n</Comment>\n<Comment by ChunyuanLI at 2023-12-01T20:07:40Z>\nhttps://huggingface.co/SkunkworksAI/BakLLaVA-1\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 551,
    "state": "open",
    "created_by": "HanyangZhong",
    "created_at": "2023-10-12T15:49:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/551</URL>\n\n<TITLE>[Question] Questions about pretrained LLM</TITLE>\n\n<BODY>### Question\n\nHi Haotian,\r\n\r\nYour job is great, well done.\r\n\r\nI have a some issues that after I use my pruned vicuna LLM as the base model, I was succeed in the phase 1--pretraining.\r\n\r\n![8423f0bfebba85b6ec53368dd11e601](https://github.com/haotian-liu/LLaVA/assets/119017394/93377314-e1a2-4eb1-9e67-51f29331a4c4)\r\n\r\nBut when I try to test with the pretrained mm-projection and the base pruned vicuna model, the error comes.\r\n\r\nMay I ask why the pretraining progress is working but the testing part is not working?\r\n\r\n![0a9008060aba7032e0f046aeeb66f8e](https://github.com/haotian-liu/LLaVA/assets/119017394/bcb10d2e-a737-49db-a165-42c07b6b6612)\r\n\r\nHope for your response.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-12T18:49:16Z>\nCan you provide both the training and the inference commands? Also, in what way you pruned? Did you finetune or you actually change some of the architecture, e.g. #channels?\n</Comment>\n<Comment by HanyangZhong at 2023-10-12T19:12:41Z>\nThank you haotian,\r\n\r\nThe training command I used is shown here.\r\n`#!/bin/bash\r\n\r\nPROMPT_VERSION=plain\r\n\r\npython ./LLaVA-main/llava/train/train_mem.py \\\r\n    --model_name_or_path ./vicuna_prune_tune/ \\\r\n    --version v0 \\\r\n    --data_path ./cc3m/chat.json \\\r\n    --image_folder ./cc3m/cc3m_595k \\\r\n    --vision_tower ./clip-vit-large-patch14-336 \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava_vpruned-13b-pretrain \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 8 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 2400 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n`\r\n\r\nThe inference command I used is shown here.\r\n`python -m llava.serve.cli \\\r\n    --model-path ./checkpoints/llava_vpruned-13b-pretrain \\\r\n    --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n    --model_base ./vicuna_prune_tune/ `\r\n\r\nI used the LLM-pruner to prune the vicuna. [](https://github.com/horseee/LLM-Pruner)\r\n\r\nIt should be a structure pruner but I only use the block--wise one not the channel--wise one.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 550,
    "state": "closed",
    "created_by": "Deaddawn",
    "created_at": "2023-10-12T15:37:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/550</URL>\n\n<TITLE>[Question] llava-7b-v1.5usage</TITLE>\n\n<BODY>### Question\n\nHi, I'm trying to use llava-7b-v1.5 in CLI inference, get this hint:\r\n**[2023-10-12 15:27:30,909] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\ntest1\r\nt2\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████| 2/2 [04:07<00:00, 123.57s/it]\r\nSome weights of the model checkpoint at /root/szd/LLaVA/ckpt/llava-7b-1.5 were not used when initializing LlavaLlamaForCausalLM: ['model.mm_projector.0.weight', 'model.mm_projector.2.weight', 'model.mm_projector.0.bias', 'model.mm_projector.2.bias']\r\n- This IS expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\nSome weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at /root/szd/LLaVA/ckpt/llava-7b-1.5 and are newly initialized: ['model.mm_projector.bias', 'model.mm_projector.weight']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\nimage shape <PIL.Image.Image image mode=RGB size=1000x667 at 0x7F7678A3E4D0>\r\nimage tensor shape torch.Size([1, 3, 224, 224])\r\nHuman: hello**\r\n\r\nI haven't update my code, not sure if that's the reason</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-12T15:43:18Z>\nYou would need to update the code to at least v1.1.0 (and the latest would be the best). Because we use an MLP projector in LLaVA-v1.5, which is not implemented in previous versions.\n</Comment>\n<Comment by Deaddawn at 2023-10-12T16:26:02Z>\n> You would need to update the code to at least v1.1.0 (and the latest would be the best). Because we use an MLP projector in LLaVA-v1.5, which is not implemented in previous versions.\r\n\r\nGot it, haotian哥. Nice work!! O(∩_∩)O\n</Comment>\n<Comment by Deaddawn at 2023-10-12T16:26:43Z>\nSolved\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 549,
    "state": "closed",
    "created_by": "liyang-7",
    "created_at": "2023-10-12T13:47:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/549</URL>\n\n<TITLE>[Usage] The Traning Environment (for OOM in visual tuning stage)</TITLE>\n\n<BODY>### Describe the issue\n\nOOM happened in visual tuning stage, for: one / two A100 (80G) + zero2.json / zero3.json + train_bs = 1\r\n\r\nCould you please provide the training environment version for the visual tuning stage? \r\nCUDA, Torch, flash-attn, deepspeed and so on.\r\n\r\nThanks ！！！</BODY>\n\n<COMMENTS>\n<Comment by liyang-7 at 2023-10-13T02:40:18Z>\nIt seems that 4+ A100 is need in visual tuning stage.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 546,
    "state": "closed",
    "created_by": "somepago",
    "created_at": "2023-10-12T12:35:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/546</URL>\n\n<TITLE>[Usage] The image paths are missing for ~40k instances in instruction finetuning dataset</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nHi, thanks again for sharing the datasets! The json file of [instruction tuning mixture](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_v1_5_mix665k.json) does not contain information of image locations for 40688 instances near the tail. \r\n<img width=\"906\" alt=\"Screenshot 2023-10-12 at 8 29 28 AM\" src=\"https://github.com/haotian-liu/LLaVA/assets/14878056/0e246b58-5562-4193-971b-f135c96db456\"></BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-20T03:28:16Z>\nYes, because they are the ShareGPT data, and they are pure-text samples.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 545,
    "state": "open",
    "created_by": "lucasjinreal",
    "created_at": "2023-10-12T11:35:29Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/545</URL>\n\n<TITLE>[Question] How does the instruction data gathered? Will add some other language data?</TITLE>\n\n<BODY>### Question\n\n[Question] How does the instruction data gathered? Will add some other language data?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 544,
    "state": "closed",
    "created_by": "buaachen1993",
    "created_at": "2023-10-12T10:44:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/544</URL>\n\n<TITLE>[Usage] build backend is missing the 'build_editable' hook</TITLE>\n\n<BODY>### Describe the issue\n\nI tried to install according to the official installation instructions，but got this error\r\n\"ERROR: Project file:///XXXX/XXXX/LLaVA has a 'pyproject.toml' and its build backend is missing the 'build_editable' hook. Since it does not have a 'setup.py' nor a 'setup.cfg', it cannot be installed in editable mode. Consider using a build backend that supports PEP 660.\"\r\n\r\nI checked the version of pip and setuptools\r\n\r\npip -V\r\npip 23.2.1 from /usr/local/conda/envs/llava/lib/python3.10/site-packages/pip (python 3.10)\r\n\r\npip list|grep setup\r\nsetuptools         68.0.0\r\n\r\nanyone knows why?</BODY>\n\n<COMMENTS>\n<Comment by July-zh at 2023-10-13T04:46:33Z>\nI have the same question.\n</Comment>\n<Comment by haotian-liu at 2023-10-13T04:54:23Z>\nHmm. This is strange. It should not have this issue when you use `pip install -U pip`, and from your pip version suggests that you've probably already done this.\r\n\r\nhttps://github.com/lm-sys/FastChat/issues/107\n</Comment>\n<Comment by buaachen1993 at 2023-10-13T06:42:02Z>\nAfter changing to a new machine, the problem was resolved, but I still don't know why the problem occurred. I am going to close the issue for now. Thank you.\n</Comment>\n<Comment by LukeLIN-web at 2025-07-08T08:43:09Z>\nMeet this in pip 22.1   \nsolved by `pip install -U pip `, pip now 25.1\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 543,
    "state": "open",
    "created_by": "lucasjinreal",
    "created_at": "2023-10-12T09:28:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/543</URL>\n\n<TITLE>[Question] Tips on how to improve performance on other languages</TITLE>\n\n<BODY>### Question\n\nin feature aligment stage, the data used are not very much, which part is more important if I want improve certrain language performance?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 542,
    "state": "closed",
    "created_by": "Maxlinn",
    "created_at": "2023-10-12T09:19:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/542</URL>\n\n<TITLE>[Question]  first stage checkpoint for llava-v1.5-13b</TITLE>\n\n<BODY>### Question\n\nhi liu and the team, thanks for your hard and the team, llava is amazing!\r\n\r\nrecently llava-v1.5-13b has been released and its vision encoder resolution has been increased to 336px, which is `openai/clip-vit-large-patch14-336`([clue](https://huggingface.co/liuhaotian/llava-v1.5-13b/blob/main/config.json#L24)).\r\n\r\ni want to train a model like llava-v1.5-13b, but due to my limit resources, i want to continue train from your llava-v1.5-13b's first-stage checkpoint(i.e. trained on `558K filtered image-text pairs from LAION/CC/SBU,`, but not trained on instructions), but it seems does not released.\r\n\r\ni discovered `liuhaotian/llava-pretrain-llama-2-13b-chat` under your hf account, but it is adapter only and its resolution is still 224px, which is `openai/clip-vit-large-patch14`([clue](https://huggingface.co/liuhaotian/llava-pretrain-llama-2-13b-chat/blob/main/config.json#L19)). due to the difference of vision model, it can not be what i want.\r\n\r\nto sum up, do you plan to release the first-stage checkpoint of `llava-v1.5-13b`(trained on caption, but not tuned on instructions)? that would much accelerates my research, much thanks!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-12T15:32:36Z>\nHi, not sure if you are asking for the projector weights of the first-stage? It is released [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#projector-weights). Or do you mean a version that uses 224x224 image resolution?\r\n\r\n![shot](https://github.com/haotian-liu/LLaVA/assets/6631389/f88bf9e4-0bf6-4f3f-82bb-728cc5ae1599)\n</Comment>\n<Comment by Maxlinn at 2023-10-12T15:38:24Z>\nthanks, i must have missed it while reading, deeply sorry for the inconvenience!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 541,
    "state": "closed",
    "created_by": "lr47h",
    "created_at": "2023-10-12T08:31:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/541</URL>\n\n<TITLE>[Usage] Pretrain phrase: I got an error like this \"forward() got an unexpected keyword argument 'padding_mask'\"</TITLE>\n\n<BODY>### Issue\r\n\r\nFollow the step of **Training** but got error like this \"forward() got an unexpected keyword argument 'padding_mask'\".\r\nNot sure what the root cause is. \r\n\r\nCommand:\r\n```\r\n./scripts/v1.5/pretrain.sh\r\n```\r\n\r\nGot several warning, not sure whether they are related to the error:\r\n\r\nLog: \r\n```\r\nFirst Warning: \r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\n\r\nSecond one:\r\n UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sa\r\nmple-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect\r\n parameterization and should be fixed.\r\n\r\nThird one: \r\nUserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reen\r\ntrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences \r\nbetween the two variants.\r\n```\r\n\r\nScreenshots:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/147701631/99743f0e-6ebe-44dd-9efd-9866a6bc4a9c)\r\n\r\n\r\nSo could you help to check which part may cause this issue?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-13T03:44:16Z>\nHi, can you show `pip list`? It is suggested to use conda to create a new environment.\n</Comment>\n<Comment by lr47h at 2023-10-13T07:43:01Z>\n> Hi, can you show `pip list`? It is suggested to use conda to create a new environment.\r\n\r\nFixed by changing the transformers version same as repo\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 540,
    "state": "open",
    "created_by": "MartinGuo",
    "created_at": "2023-10-12T07:56:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/540</URL>\n\n<TITLE>[Question] Use api</TITLE>\n\n<BODY>### Question\n\nHow can I use API requests to call the model for the upper level application?</BODY>\n\n<COMMENTS>\n<Comment by Raidus at 2023-10-14T10:10:51Z>\nThe serve cli module might give you some hints. \r\n\r\n```python\r\n\r\n        with torch.inference_mode():\r\n            output_ids = model.generate(\r\n                input_ids,\r\n                images=image_tensor,\r\n                do_sample=True,\r\n                temperature=args.temperature,\r\n                max_new_tokens=args.max_new_tokens,\r\n                streamer=streamer,\r\n                use_cache=True,\r\n                stopping_criteria=[stopping_criteria])\r\n\r\n        outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()\r\n        conv.messages[-1][-1] = outputs\r\n```\r\nReference: https://github.com/haotian-liu/LLaVA/blob/main/llava/serve/cli.py\n</Comment>\n<Comment by userbox020 at 2023-10-16T07:55:53Z>\nim trying to modify the client to give the user the option to upload new images, but after upload the 3rd image i get oom error, im doing garbage colector and cleaning cache of vram but maybe im doing it wrong\r\n```\r\nimport argparse\r\nimport torch\r\n\r\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\r\nfrom llava.conversation import conv_templates, SeparatorStyle\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.utils import disable_torch_init\r\nfrom llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\r\n\r\nfrom PIL import Image\r\n\r\nimport requests\r\nimport traceback\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\nfrom transformers import TextStreamer\r\n\r\n# Garbage Collector for PyTorch GPU Memory\r\nimport gc\r\n\r\ndef pytorch_gpu_garbage_collector():\r\n    \"\"\"Explicitly trigger Python's garbage collector and then clear PyTorch's GPU cache for all GPUs.\"\"\"\r\n    print(\"Triggering Python's garbage collector...\")\r\n    # Collect Python's garbage\r\n    gc.collect()\r\n\r\n    # Clear PyTorch's GPU cache for all GPUs\r\n    if torch.cuda.is_available():\r\n        for device_idx in range(torch.cuda.device_count()):\r\n            torch.cuda.set_device(device_idx)\r\n            print(f\"Clearing PyTorch GPU cache for GPU {device_idx}...\")\r\n            torch.cuda.empty_cache()\r\n    print(\"Garbage collection completed!\")\r\n\r\n\r\n\r\ndef load_image(image_file):\r\n    if image_file.startswith('http://') or image_file.startswith('https://'):\r\n        response = requests.get(image_file)\r\n        image = Image.open(BytesIO(response.content)).convert('RGB')\r\n    else:\r\n        image = Image.open(image_file).convert('RGB')\r\n    return image\r\n\r\n\r\ndef main(args):\r\n    # Model\r\n    disable_torch_init()\r\n\r\n    model_name = get_model_name_from_path(args.model_path)\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n    \r\n    if 'llama-2' in model_name.lower():\r\n        conv_mode = \"llava_llama_2\"\r\n    elif \"v1\" in model_name.lower():\r\n        conv_mode = \"llava_v1\"\r\n    elif \"mpt\" in model_name.lower():\r\n        conv_mode = \"mpt\"\r\n    else:\r\n        conv_mode = \"llava_v0\"\r\n\r\n    if args.conv_mode is not None and conv_mode != args.conv_mode:\r\n        print('[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}'.format(conv_mode, args.conv_mode, args.conv_mode))\r\n    else:\r\n        args.conv_mode = conv_mode\r\n\r\n    conv = conv_templates[args.conv_mode].copy()\r\n    if \"mpt\" in model_name.lower():\r\n        roles = ('user', 'assistant')\r\n    else:\r\n        roles = conv.roles\r\n\r\n\r\n\r\n    while True:\r\n        try:\r\n            \r\n            image_file = input('Image path:')\r\n            #image = load_image(args.image_file)\r\n\r\n            image = load_image(image_file)\r\n\r\n            # Similar operation in model_worker.py\r\n            image_tensor = process_images([image], image_processor, args)\r\n            if type(image_tensor) is list:\r\n                image_tensor = [image.to(model.device, dtype=torch.float16) for image in image_tensor]\r\n            else:\r\n                image_tensor = image_tensor.to(model.device, dtype=torch.float16)            \r\n            \r\n            inp = input(f\"{roles[0]}: \")\r\n        except EOFError:\r\n            inp = \"\"\r\n        if not inp:\r\n            print(\"exit...\")\r\n            break\r\n\r\n        try:\r\n            print(f\"{roles[1]}: \", end=\"\")\r\n\r\n            if image is not None:\r\n                # first message\r\n                if model.config.mm_use_im_start_end:\r\n                    inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + inp\r\n                else:\r\n                    inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\r\n                conv.append_message(conv.roles[0], inp)\r\n                image = None\r\n            else:\r\n                # later messages\r\n                conv.append_message(conv.roles[0], inp)\r\n            conv.append_message(conv.roles[1], None)\r\n            prompt = conv.get_prompt()\r\n\r\n\r\n\r\n            input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\r\n            stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\r\n            keywords = [stop_str]\r\n            stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\r\n            streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\r\n\r\n            print('------------------------')\r\n            print('input_ids:', input_ids)\r\n            print('image_tensor:', image_tensor)\r\n            print('args.temperature:', args.temperature)\r\n            print('max_new_tokens:', args.max_new_tokens)\r\n            print('streamer:', streamer)\r\n            print('stopping_criteria:', [stopping_criteria])\r\n\r\n\r\n            with torch.inference_mode():\r\n                output_ids = model.generate(\r\n                    input_ids,\r\n                    images=image_tensor,\r\n                    do_sample=True,\r\n                    temperature=args.temperature,\r\n                    max_new_tokens=args.max_new_tokens,\r\n                    streamer=streamer,\r\n                    use_cache=True,\r\n                    stopping_criteria=[stopping_criteria])\r\n\r\n            input('Debug...')\r\n\r\n            outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()\r\n            conv.messages[-1][-1] = outputs\r\n            print('------------------------')\r\n            if args.debug:\r\n                print(\"\\n\", {\"prompt\": prompt, \"outputs\": outputs}, \"\\n\")\r\n\r\n\r\n            pytorch_gpu_garbage_collector()\r\n        \r\n        \r\n        except Exception as e:\r\n            print('Error:',str(e))\r\n            print('Traceback:', traceback.format_exc())\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--model-path\", type=str, default=\"facebook/opt-350m\")\r\n    parser.add_argument(\"--model-base\", type=str, default=None)\r\n    #parser.add_argument(\"--image-file\", type=str, required=True)\r\n    parser.add_argument(\"--device\", type=str, default=\"cuda\")\r\n    parser.add_argument(\"--conv-mode\", type=str, default=None)\r\n    parser.add_argument(\"--temperature\", type=float, default=0.2)\r\n    parser.add_argument(\"--max-new-tokens\", type=int, default=512)\r\n    parser.add_argument(\"--load-8bit\", action=\"store_true\")\r\n    parser.add_argument(\"--load-4bit\", action=\"store_true\")\r\n    parser.add_argument(\"--debug\", action=\"store_true\")\r\n    parser.add_argument(\"--image-aspect-ratio\", type=str, default='pad')\r\n    args = parser.parse_args()\r\n    main(args)\r\n\r\n```\n</Comment>\n<Comment by TonyUSTC at 2023-10-31T11:26:39Z>\n请问解决了吗？遇到同样的困惑。\n</Comment>\n<Comment by vicgarfield at 2023-11-02T08:00:02Z>\nThe reason is that the request prompt will superimpose the previous image and text prompt token. Just re-initialize \"conv\" parameter every time you request it.\n</Comment>\n<Comment by userbox020 at 2023-11-02T08:51:29Z>\n@vicgarfield thanks bro, working beautiful everything its cleaning now memory like it should be. Im on loop 9 and not accumulating any extra vram. Im running it with only 12gb vram. here i left code\r\n\r\n```\r\nimport argparse\r\nimport torch\r\n\r\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\r\nfrom llava.conversation import conv_templates, SeparatorStyle\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.utils import disable_torch_init\r\nfrom llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\r\n\r\nfrom PIL import Image\r\n\r\nimport requests\r\nimport traceback\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\nfrom transformers import TextStreamer\r\n\r\n# Garbage Collector for PyTorch GPU Memory\r\nimport gc\r\n\r\ndef pytorch_gpu_garbage_collector():\r\n    \"\"\"Explicitly trigger Python's garbage collector and then clear PyTorch's GPU cache for all GPUs.\"\"\"\r\n    print(\"Triggering Python's garbage collector...\")\r\n    # Collect Python's garbage\r\n    gc.collect()\r\n\r\n    # Clear PyTorch's GPU cache for all GPUs\r\n    if torch.cuda.is_available():\r\n        for device_idx in range(torch.cuda.device_count()):\r\n            torch.cuda.set_device(device_idx)\r\n            print(f\"Clearing PyTorch GPU cache for GPU {device_idx}...\")\r\n            torch.cuda.empty_cache()\r\n    print(\"Garbage collection completed!\")\r\n\r\n\r\n\r\ndef load_image(image_file):\r\n    if image_file.startswith('http://') or image_file.startswith('https://'):\r\n        response = requests.get(image_file)\r\n        image = Image.open(BytesIO(response.content)).convert('RGB')\r\n    else:\r\n        image = Image.open(image_file).convert('RGB')\r\n    return image\r\n\r\n\r\ndef main(args):\r\n    # Model\r\n    disable_torch_init()\r\n\r\n    model_name = get_model_name_from_path(args.model_path)\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n    \r\n    if 'llama-2' in model_name.lower():\r\n        conv_mode = \"llava_llama_2\"\r\n    elif \"v1\" in model_name.lower():\r\n        conv_mode = \"llava_v1\"\r\n    elif \"mpt\" in model_name.lower():\r\n        conv_mode = \"mpt\"\r\n    else:\r\n        conv_mode = \"llava_v0\"\r\n\r\n    if args.conv_mode is not None and conv_mode != args.conv_mode:\r\n        print('[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}'.format(conv_mode, args.conv_mode, args.conv_mode))\r\n    else:\r\n        args.conv_mode = conv_mode\r\n\r\n\r\n\r\n\r\n    while True:\r\n        try:\r\n            \r\n\r\n            conv = conv_templates[args.conv_mode].copy()\r\n            if \"mpt\" in model_name.lower():\r\n                roles = ('user', 'assistant')\r\n            else:\r\n                roles = conv.roles\r\n\r\n\r\n            image_file = input('Image path:')\r\n            #image = load_image(args.image_file)\r\n\r\n            image = load_image(image_file)\r\n\r\n            # Similar operation in model_worker.py\r\n            image_tensor = process_images([image], image_processor, args)\r\n            if type(image_tensor) is list:\r\n                image_tensor = [image.to(model.device, dtype=torch.float16) for image in image_tensor]\r\n            else:\r\n                image_tensor = image_tensor.to(model.device, dtype=torch.float16)            \r\n            \r\n            inp = input(f\"{roles[0]}: \")\r\n        except EOFError:\r\n            inp = \"\"\r\n        if not inp:\r\n            print(\"exit...\")\r\n            break\r\n\r\n        try:\r\n            print(f\"{roles[1]}: \", end=\"\")\r\n\r\n            if image is not None:\r\n                # first message\r\n                if model.config.mm_use_im_start_end:\r\n                    inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + inp\r\n                else:\r\n                    inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\r\n                conv.append_message(conv.roles[0], inp)\r\n                image = None\r\n            else:\r\n                # later messages\r\n                conv.append_message(conv.roles[0], inp)\r\n            conv.append_message(conv.roles[1], None)\r\n            prompt = conv.get_prompt()\r\n\r\n\r\n\r\n            input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\r\n            stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\r\n            keywords = [stop_str]\r\n            stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\r\n            streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\r\n\r\n            print('------------------------')\r\n            print('input_ids:', input_ids)\r\n            print('image_tensor:', image_tensor)\r\n            print('args.temperature:', args.temperature)\r\n            print('max_new_tokens:', args.max_new_tokens)\r\n            print('streamer:', streamer)\r\n            print('stopping_criteria:', [stopping_criteria])\r\n\r\n\r\n            with torch.inference_mode():\r\n                output_ids = model.generate(\r\n                    input_ids,\r\n                    images=image_tensor,\r\n                    do_sample=True,\r\n                    temperature=args.temperature,\r\n                    max_new_tokens=args.max_new_tokens,\r\n                    streamer=streamer,\r\n                    use_cache=True,\r\n                    stopping_criteria=[stopping_criteria])\r\n\r\n\r\n            input('Debug...')\r\n\r\n            outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()\r\n            conv.messages[-1][-1] = outputs\r\n            print('------------------------')\r\n            if args.debug:\r\n                print(\"\\n\", {\"prompt\": prompt, \"outputs\": outputs}, \"\\n\")\r\n\r\n\r\n            # Assuming you're done with output_ids and other tensors, start cleanup\r\n            print(\"Deleting output_ids\")\r\n            del output_ids\r\n            print(\"Deleting input_ids\")\r\n            del input_ids\r\n            print(\"Deleting image_tensor\")\r\n            del image_tensor\r\n            print(\"Deleting inp\")\r\n            del inp\r\n            print(\"Deleting stop_str\")\r\n            del stop_str\r\n            #print('Deleting conv')\r\n            #del conv\r\n            #print('Deleting model')\r\n            #del model\r\n\r\n            # --- MEMORY CLEANER            \r\n            pytorch_gpu_garbage_collector()\r\n\r\n        \r\n        \r\n        except Exception as e:\r\n            print('Error:',str(e))\r\n            print('Traceback:', traceback.format_exc())\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--model-path\", type=str, default=\"facebook/opt-350m\")\r\n    parser.add_argument(\"--model-base\", type=str, default=None)\r\n    #parser.add_argument(\"--image-file\", type=str, required=True)\r\n    parser.add_argument(\"--device\", type=str, default=\"cuda\")\r\n    parser.add_argument(\"--conv-mode\", type=str, default=None)\r\n    parser.add_argument(\"--temperature\", type=float, default=0.2)\r\n    parser.add_argument(\"--max-new-tokens\", type=int, default=512)\r\n    parser.add_argument(\"--load-8bit\", action=\"store_true\")\r\n    parser.add_argument(\"--load-4bit\", action=\"store_true\")\r\n    parser.add_argument(\"--debug\", action=\"store_true\")\r\n    parser.add_argument(\"--image-aspect-ratio\", type=str, default='pad')\r\n    args = parser.parse_args()\r\n    main(args)\r\n```\r\n\r\nTo run the model:\r\n\r\n```\r\nCUDA_VISIBLE_DEVICES=0,1 python -m llava.serve.tcli --model-path /media/10TB_HHD/_LLaVA/_models/liuhaotian_llava-v1.5-13b/ --load-4bit\r\n```\n</Comment>\n<Comment by vicgarfield at 2023-11-03T08:08:23Z>\n@userbox020 Perfect！\n</Comment>\n<Comment by pbenaim at 2023-11-03T09:44:27Z>\nAnd tcli.py must be in ../LLaVA/llava/serve :+1: \r\n\r\nThanks\n</Comment>\n<Comment by yinjiaoyuan at 2023-11-16T08:30:03Z>\nVERY GOOD！！！\n</Comment>\n<Comment by hugsbrugs at 2023-11-28T16:18:02Z>\nHey,\r\nI'm also trying to call to model through API but I can't figure out how you do your CURL (post ?) request once the model is loaded ?\r\nYou don't attach it to a controller or gradio web server ? so how to call it publicly ? I've tried to append --share but this option is not handled with tcli.py\r\nThanks for your help !\n</Comment>\n<Comment by Xingxiangrui at 2024-02-20T03:24:42Z>\n> Hey, I'm also trying to call to model through API but I can't figure out how you do your CURL (post ?) request once the model is loaded ? You don't attach it to a controller or gradio web server ? so how to call it publicly ? I've tried to append --share but this option is not handled with tcli.py Thanks for your help !\r\n\r\n+1 for this\n</Comment>\n<Comment by userbox020 at 2024-02-24T12:51:19Z>\n> > Hey, I'm also trying to call to model through API but I can't figure out how you do your CURL (post ?) request once the model is loaded ? You don't attach it to a controller or gradio web server ? so how to call it publicly ? I've tried to append --share but this option is not handled with tcli.py Thanks for your help !\n> \n> +1 for this\n\nSorry bro i confuse post, it wasnt an API it just some tricks to don't oom with 8 gb gpus. \n\nBut adding API will be easy enought, you can create a separate script and use the module subprocess and save the output or stream it. Then use any python http server to publish or stream the output.\n\nChatgpt must be able to do a good code template for that\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 539,
    "state": "closed",
    "created_by": "Cuiunbo",
    "created_at": "2023-10-12T07:31:02Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/539</URL>\n\n<TITLE>[Question] About Visual Instruction Tuning Prepare data</TITLE>\n\n<BODY>### Question\n\nHi Haotian, thanks for relase the eval scripts\r\nI encountered another issue while checking the reproduced results of LLaVA1.5 evaluation on TextVQA, which might need your attention. In the [GitHub repository readme](https://github.com/haotian-liu/LLaVA/tree/main#visual-instruction-tuning), tuning on TextVQA is mentioned. However, there appears to be no evidence in the paper suggesting that LLaVA1.5 was trained on TextVQA.\r\n<img width=\"336\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/92843231/48a2aa08-6d6b-49b2-bcc1-757056f03748\"></BODY>\n\n<COMMENTS>\n<Comment by Cuiunbo at 2023-10-12T16:34:40Z>\nMy apologies, I misunderstood earlier. The image of TextVQA is actually used for TextCaps.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 538,
    "state": "closed",
    "created_by": "teowu",
    "created_at": "2023-10-12T05:46:28Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/538</URL>\n\n<TITLE>[Feature request] Q-Bench (LLaVA-v1.5 excels on low-level vision abilities)</TITLE>\n\n<BODY>### feature\n\nHi Haotian,\r\n\r\nA great pleasure to ask. Congratulations to this nice and solid work.\r\n\r\nWe are a team from NTU S-Lab, working on image/video quality assessment and recently propose a benchmark called [Q-Bench](https://github.com/VQAssessment/Q-Bench), aiming to capture **low-level perception and understanding abilities** of MLLMs/VLLMs/LMMs. This is a sibling-like project to MMBench which is also for 4-question choices, while we find out that LLaVA-v1.5 reaches **top-1 performance** on its hot benchmark (at its release date), as shown in https://github.com/VQAssessment/Q-Bench/tree/master/leaderboards. Is that possible for us to write an evaluation script (similar to the existing one for MMBench) and merge into the evaluation scripts of LLaVA?\r\n\r\nBest\r\nHaoning</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-13T01:54:05Z>\nSure, and that would be great! It would be helpful for the community to have more benchmarks to use conveniently in our code base.\n</Comment>\n<Comment by teowu at 2023-10-15T11:43:45Z>\nHi Haotian,\r\n\r\nThank you for the kind reply. Here is the pull request: https://github.com/haotian-liu/LLaVA/pull/581/commits. We modified four files, (**2 scripts, 1 evaluation code, and added a paragraph on the doc). \r\n\r\nThank you so much again.\r\n\r\nBest,\r\nHaoning\n</Comment>\n<Comment by haotian-liu at 2023-11-04T19:35:12Z>\nThank you and sorry for the delay. We have merged the PR. Congratulations again on the great benchmark proposed!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 535,
    "state": "closed",
    "created_by": "hjsg1010",
    "created_at": "2023-10-12T00:41:55Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/535</URL>\n\n<TITLE>[Question] Utilizing a Finetuned custom vicuna13b Model in the llava Framework</TITLE>\n\n<BODY>### Question\n\nI have a vicuna13b model that has been finetuned with my own text data (only language finetuning, not image-text pair finetuning using your script). Would it be possible to utilize this in your llava framework?\r\n\r\nCould I perhaps change the model-base in this command, or modify my custom vicuna config in some way?\r\n```console\r\npython -m llava.serve.cli --model-path ./data-vol-1/model/llava/llava-336px-pretrain-vicuna-13b-v1.3 --model-base ./data-vol-1/model/llava/vicuna_13b_v1.3 --image-file \"./llava/view.jpg\" \r\n```</BODY>\n\n<COMMENTS>\n<Comment by mvsoom at 2024-05-06T13:53:48Z>\n@hjsg1010 I was thinking of doing the same thing. Did it work in your case? Thanks :)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 533,
    "state": "open",
    "created_by": "iplayfast",
    "created_at": "2023-10-11T20:12:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/533</URL>\n\n<TITLE>[Usage] requirements update</TITLE>\n\n<BODY>### Describe the issue\n\nwrapt was not installed because numpy was too recent. Fixed by the following:\r\n\r\npip install numpy==1.24.3\r\npip install wrapt</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 529,
    "state": "open",
    "created_by": "9of9",
    "created_at": "2023-10-11T13:45:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/529</URL>\n\n<TITLE>[Usage] Installation on Windows - async_io dependency</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: When installing requirements on windows, installation fails due to not being able to pre-compile async_io\r\n\r\nCommand:\r\n```\r\npip install -e .\r\n```\r\n\r\nLog: \r\n```\r\nObtaining file:///D:/AI/LLaVA\r\n  Installing build dependencies ... done\r\n  Checking if build backend supports build_editable ... done\r\n  Getting requirements to build editable ... done\r\n  Installing backend dependencies ... done\r\n  Preparing editable metadata (pyproject.toml) ... done\r\nCollecting einops (from llava==1.1.0)\r\n  Obtaining dependency information for einops from https://files.pythonhosted.org/packages/29/0b/2d1c0ebfd092e25935b86509a9a817159212d82aa43d7fb07eca4eeff2c2/einops-0.7.0-py3-none-any.whl.metadata\r\n  Using cached einops-0.7.0-py3-none-any.whl.metadata (13 kB)\r\nCollecting fastapi (from llava==1.1.0)\r\n  Obtaining dependency information for fastapi from https://files.pythonhosted.org/packages/4d/d2/3ad038a2365fefbac19d9a046cab7ce45f4c7bfa81d877cbece9707de9ce/fastapi-0.103.2-py3-none-any.whl.metadata\r\n  Using cached fastapi-0.103.2-py3-none-any.whl.metadata (24 kB)\r\nCollecting gradio==3.35.2 (from llava==1.1.0)\r\n  Obtaining dependency information for gradio==3.35.2 from https://files.pythonhosted.org/packages/50/70/ed0ba0fb5c3b1cb2e481717ad190056a4c9a0ef2f296b871e10375b2ab83/gradio-3.35.2-py3-none-any.whl.metadata\r\n  Using cached gradio-3.35.2-py3-none-any.whl.metadata (15 kB)\r\nCollecting markdown2[all] (from llava==1.1.0)\r\n  Obtaining dependency information for markdown2[all] from https://files.pythonhosted.org/packages/f1/98/61276a753f078dd2f3171c9a69fd3f451d220e806b2b1cdca41b8e368b0f/markdown2-2.4.10-py2.py3-none-any.whl.metadata\r\n  Using cached markdown2-2.4.10-py2.py3-none-any.whl.metadata (2.0 kB)\r\nRequirement already satisfied: numpy in d:\\anaconda3\\envs\\llava\\lib\\site-packages (from llava==1.1.0) (1.26.0)\r\nRequirement already satisfied: requests in d:\\anaconda3\\envs\\llava\\lib\\site-packages (from llava==1.1.0) (2.31.0)\r\nCollecting sentencepiece (from llava==1.1.0)\r\n  Using cached sentencepiece-0.1.99-cp310-cp310-win_amd64.whl (977 kB)\r\nCollecting tokenizers>=0.12.1 (from llava==1.1.0)\r\n  Obtaining dependency information for tokenizers>=0.12.1 from https://files.pythonhosted.org/packages/92/02/15556b80450301d2ef014bc598df4352bfb39631c5fcff758d8e0ac9f065/tokenizers-0.14.1-cp310-none-win_amd64.whl.metadata\r\n  Using cached tokenizers-0.14.1-cp310-none-win_amd64.whl.metadata (6.8 kB)\r\nCollecting torch==2.0.1 (from llava==1.1.0)\r\n  Using cached torch-2.0.1-cp310-cp310-win_amd64.whl (172.3 MB)\r\nCollecting torchvision==0.15.2 (from llava==1.1.0)\r\n  Using cached torchvision-0.15.2-cp310-cp310-win_amd64.whl (1.2 MB)\r\nCollecting uvicorn (from llava==1.1.0)\r\n  Obtaining dependency information for uvicorn from https://files.pythonhosted.org/packages/79/96/b0882a1c3f7ef3dd86879e041212ae5b62b4bd352320889231cc735a8e8f/uvicorn-0.23.2-py3-none-any.whl.metadata\r\n  Using cached uvicorn-0.23.2-py3-none-any.whl.metadata (6.2 kB)\r\nCollecting wandb (from llava==1.1.0)\r\n  Obtaining dependency information for wandb from https://files.pythonhosted.org/packages/1c/5e/0362fa88679852c7fd3ac85ee5bd949426c4a51a61379010d4089be6d7ac/wandb-0.15.12-py3-none-any.whl.metadata\r\n  Using cached wandb-0.15.12-py3-none-any.whl.metadata (9.8 kB)\r\nCollecting shortuuid (from llava==1.1.0)\r\n  Using cached shortuuid-1.0.11-py3-none-any.whl (10 kB)\r\nCollecting httpx==0.24.0 (from llava==1.1.0)\r\n  Using cached httpx-0.24.0-py3-none-any.whl (75 kB)\r\nCollecting deepspeed==0.9.5 (from llava==1.1.0)\r\n  Using cached deepspeed-0.9.5.tar.gz (809 kB)\r\n  Preparing metadata (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n\r\n  × python setup.py egg_info did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> [15 lines of output]\r\n      test.c\r\n      LINK : fatal error LNK1181: cannot open input file 'aio.lib'\r\n      Traceback (most recent call last):\r\n        File \"<string>\", line 2, in <module>\r\n        File \"<pip-setuptools-caller>\", line 34, in <module>\r\n        File \"C:\\Users\\valkozin\\AppData\\Local\\Temp\\pip-install-s9q4kp7v\\deepspeed_1f2978aea66844a2af764a83fd024764\\setup.py\", line 163, in <module>\r\n          abort(f\"Unable to pre-compile {op_name}\")\r\n        File \"C:\\Users\\valkozin\\AppData\\Local\\Temp\\pip-install-s9q4kp7v\\deepspeed_1f2978aea66844a2af764a83fd024764\\setup.py\", line 51, in abort\r\n          assert False, msg\r\n      AssertionError: Unable to pre-compile async_io\r\n      DS_BUILD_OPS=1\r\n       [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.\r\n       [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\r\n       [WARNING]  One can disable async_io with DS_BUILD_AIO=0\r\n       [ERROR]  Unable to pre-compile async_io\r\n      [end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n× Encountered error while generating package metadata.\r\n╰─> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details.\r\n```\r\n\r\nAs async_io/libaio are dependent on Linux, these dependencies seem to make it impossible to run LLaVA natively on Windows.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-24T00:57:21Z>\nPlease check the latest doc for Windows. I tested on my Windows 11 PC and it works now for 16-bit inference. Quantization will be supported later.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/docs/Windows.md\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 528,
    "state": "closed",
    "created_by": "hyer",
    "created_at": "2023-10-11T12:52:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/528</URL>\n\n<TITLE>[Usage] Visual instruction tuning OOM with zero3.json</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nAs the README's description:\r\n```\r\nVisual instruction tuning takes around 20 hours for LLaVA-v1.5-13B on 8x A100 (80G), due to the increased resolution to 336px. It takes around 10 hours for LLaVA-v1.5-7B on 8x A100 (40G).\r\n\r\nTraining script with DeepSpeed ZeRO-3: [finetune.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune.sh).\r\n```\r\n\r\nI try the visual instruction tuning with COCO train2017 with the command:\r\n```\r\n#!/bin/bash\r\ndeepspeed --num_gpus=1 llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path /mnt/data_nas/models/vicuna-7b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path /mnt/data_nas/datasets/Multimodal/LLaVA-Instruct-150K/llava_instruct_80k.json \\\r\n    --image_folder /mnt/data_nas/datasets/Det/COCO/train2017 \\\r\n    --vision_tower /mnt/data_nas/models/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter /mnt/data_nas/models/llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b_coco-train2017 \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 2500 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to tensorboard\r\n```\r\nand got GPU OOM with the env:\r\n - A100 (80G)\r\n - python3.8\r\n - CUDA11.8\r\n - flash_attn-2.3.2+cu118torch2.0cxx11abiFALSE-cp38-cp38-linux_x86_64.whl\r\n - pytorch 2.0.1\r\n\r\nI wonder if there is typo in the `scripts/v1_5/finetune.sh` script. May be the parameter `--deepspeed ./scripts/zero3.json` should be `--deepspeed ./scripts/zero3_offload.json` for A100 (80G) ？</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-12T04:20:36Z>\nHi, do you use 8x A100? I have tested the scripts locally and they work fine.\r\n\r\nAlso, since you are using Vicuna-7B, it should work even for A100-40G (also tested locally).\n</Comment>\n<Comment by hyer at 2023-10-12T10:36:48Z>\n> Hi, do you use 8x A100? I have tested the scripts locally and they work fine.\r\n> \r\n> Also, since you are using Vicuna-7B, it should work even for A100-40G (also tested locally).\r\n\r\nI use `--num_gpus=1` with only one A100 (80G), does this matter? It works with `--deepspeed ./scripts/zero3_offload.json`.\n</Comment>\n<Comment by haotian-liu at 2023-10-12T15:45:11Z>\n`zero3_offload.json` basically offloads parameters to CPU and your CPU RAM now serves as part of the VRAM that you can use. This would decrease the speed, so that you are aware of this.\r\n\r\nUsually 4x A100-80G would be needed to full-finetune 7B and maybe 2x A400-80G would be enough if we finetune 7B with LoRA. Will release LoRA scripts and checkpoints soon.\n</Comment>\n<Comment by skyz8421 at 2023-10-16T02:21:06Z>\nI encountered the same issue: I cannot fully finetune the 7B model using zero3.json. However, it works with zero3_offload.json, though at the cost of reduced speed on a single A100-80G GPU. \r\n\r\nLooking forward to the LoRA/QLoRA scripts!\n</Comment>\n<Comment by haotian-liu at 2023-10-26T20:46:34Z>\nHi, we have released the LoRA ckpt, script, and a brief doc on the new schedule.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/releases/tag/v1.1.3\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 527,
    "state": "open",
    "created_by": "JulioZhao97",
    "created_at": "2023-10-11T10:43:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/527</URL>\n\n<TITLE>[Question] Question about num_beams in llava.serve.cli</TITLE>\n\n<BODY>### Question\n\nWhen I run inference using llava.serve.cli, I add ```num_beams``` parameter in model generation:\r\nhttps://github.com/haotian-liu/LLaVA/blob/a967492dc0012ba58a3a1944fd61f4f7bd517ffe/llava/serve/cli.py#L88-L97\r\n\r\nBut this gives an error in KeywordStoppingCreteria when ```num_beams>1```:\r\nhttps://github.com/haotian-liu/LLaVA/blob/a967492dc0012ba58a3a1944fd61f4f7bd517ffe/llava/mm_utils.py#L76-L99\r\n\r\nwhich fail in assertion:\r\nhttps://github.com/haotian-liu/LLaVA/blob/a967492dc0012ba58a3a1944fd61f4f7bd517ffe/llava/mm_utils.py#L89\r\n\r\nBut I checked the KeywordStoppingCreteria code and I find that the code for checking stopping creteria works OK when assertion is not satisfied, so what purpose this assertion is used for?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 526,
    "state": "closed",
    "created_by": "shahizat",
    "created_at": "2023-10-11T07:53:30Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/526</URL>\n\n<TITLE>GGUF/GPTQ models for LLaVA-1.5</TITLE>\n\n<BODY>### Question\n\nHello, \r\n\r\nIs anyone aware of the 4-bit quantized models for LLaVA-1.5 available on Hugging Face? \r\n\r\nThanks in advance!</BODY>\n\n<COMMENTS>\n<Comment by barshag at 2023-10-12T20:56:29Z>\nwhat are the system requirement for that? (running solely on CPU?)\n</Comment>\n<Comment by aiaicode at 2023-10-13T20:06:19Z>\nhttps://huggingface.co/mys/ggml_llava-v1.5-7b\r\nhttps://huggingface.co/mys/ggml_llava-v1.5-13b \r\n\r\n7b works better than 13b for now.\r\n\r\nUse the [llama.cpp](https://github.com/ggerganov/llama.cpp) repo and run the make command and use ./lava\r\n\r\nRuns on CPU. More info on this [PR](https://github.com/ggerganov/llama.cpp/pull/3436)\n</Comment>\n<Comment by haotian-liu at 2023-10-17T19:40:04Z>\nThe bug seems to be fixed in https://github.com/ggerganov/llama.cpp/pull/3645\r\nNow the quality for both 7b and 13b are improved.\r\n\r\nClosing this now.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 525,
    "state": "closed",
    "created_by": "hangzeli08",
    "created_at": "2023-10-11T07:09:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/525</URL>\n\n<TITLE>[Question] If I want to continue to fine-tune the llava1.5 checkpoint, lora way, how should I change the script</TITLE>\n\n<BODY>### Question\n\nI \r\n<img width=\"521\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/104003969/72c75374-2bf9-45c9-850b-2dd550fe52e8\">\r\nIt report bug:\r\n<img width=\"1220\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/104003969/0f0e16c8-4982-419f-84e7-32f414af4650\"></BODY>\n\n<COMMENTS>\n<Comment by tingxueronghua at 2023-10-15T16:29:59Z>\n@haotian-liu hi, do you have any advice about this topic? I am not familiar with PEFT library. I just change the model's path and face the same issue.\r\nBecause the time consumed by Llava-1.5 during fine-tuning stage is much longer than original llava, I think it might be a more efficient way to train an extra lora layer based on your fine-tuned llava-1.5 model. Hope to see your suggestion.\n</Comment>\n<Comment by tingxueronghua at 2023-10-16T03:23:05Z>\n> ### Question\r\n> I <img alt=\"image\" width=\"521\" src=\"https://user-images.githubusercontent.com/104003969/274173300-72c75374-2bf9-45c9-850b-2dd550fe52e8.png\"> It report bug: <img alt=\"image\" width=\"1220\" src=\"https://user-images.githubusercontent.com/104003969/274173477-0f0e16c8-4982-419f-84e7-32f414af4650.png\">\r\n\r\nHello? I guess I found the problem. This is mainly due to the confusion in the naming of the model layer, and the ambiguity of the parameters specifying the model layer in the LoraConfig input parameters.\r\n\r\nThe problem:\r\n\r\nI guess you use the whole llava-1.5 model weights as the initialization. There would be minor difference between initializing with Vicuna. That is there will be a \"mm_projector\" if using llava-1.5, which contains Linear Layer. However, the function \"find_all_linear_names\" in llava/train/train.py, which is used to find which layers should be applied with Lora, will only memorize the last part of the name splited by \".\". For example, if the model's name is \"mm_projector.0\" and is a Linear function which could be applied with Lora, then that function will only memorize the name \"0\". Unfortunately, the LlamaDecoderLayer is also named from \"0\" to \"39\" (for 13b case). \r\n\r\nThe solution:\r\nsimply add \r\n```\r\nif 'mm_projector' in names:\r\n    continue\r\n```\r\nbefore \r\n```\r\nlora_module_names.add(names[0] if len(names) == 1 else names[-1])\r\n```\r\n\r\n\r\nThen the problem is solved. Also you can specify the model name list manually.\n</Comment>\n<Comment by tingxueronghua at 2023-10-16T08:34:36Z>\nThere is one more thing you need to pay attention to. You need to manually set the \"requires_grad\" of mm_proj to be true. \r\n\r\nBecause originally, llava add the mm_proj after adding lora layers. The operation of get_peft_model will set the rest parameters' requires_grad as False. If you load the projection layers first, then it will also be set as not-trainable parameters.\r\n\r\nSo, first, you need to ignore the parameter \"pretrain_mm_mlp_adapter\", then, you need to manually set the requires_grad of mm_proj after peft.\n</Comment>\n<Comment by wanghao-cst at 2023-10-23T07:59:10Z>\n> There is one more thing you need to pay attention to. You need to manually set the \"requires_grad\" of mm_proj to be true.\r\n> \r\n> Because originally, llava add the mm_proj after adding lora layers. The operation of get_peft_model will set the rest parameters' requires_grad as False. If you load the projection layers first, then it will also be set as not-trainable parameters.\r\n> \r\n> So, first, you need to ignore the parameter \"pretrain_mm_mlp_adapter\", then, you need to manually set the requires_grad of mm_proj after peft.\r\n\r\nReally good reply. Solved my problem.\n</Comment>\n<Comment by tingxueronghua at 2023-10-24T09:04:07Z>\n> > There is one more thing you need to pay attention to. You need to manually set the \"requires_grad\" of mm_proj to be true.\r\n> > Because originally, llava add the mm_proj after adding lora layers. The operation of get_peft_model will set the rest parameters' requires_grad as False. If you load the projection layers first, then it will also be set as not-trainable parameters.\r\n> > So, first, you need to ignore the parameter \"pretrain_mm_mlp_adapter\", then, you need to manually set the requires_grad of mm_proj after peft.\r\n> \r\n> Really good reply. Solved my problem.\r\n\r\nIn fact there is one more thing to do if want to use lora to further finetune the already finetuned model. \r\nPay attention to the initialize_vision_modules in llava/model/llava_arch.py. There is a function named build_vision_projector. It won't check the existence of projection layer and will directly initialize a new one, rather than using the finetuned projection layer. So you need to manually disable it when you already have a projector.\r\n\r\nI think that's all you need to tune llava1.5 checkpoint with lora.\n</Comment>\n<Comment by haotian-liu at 2023-10-26T20:41:44Z>\nHi, we have released the LoRA ckpt, script, and a brief doc on the new schedule.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/releases/tag/v1.1.3\n</Comment>\n<Comment by tingxueronghua at 2023-10-27T08:38:26Z>\nThanks for your effort! I have no other questions. And I think this issue could be closed.\r\n\r\n> Hi, we have released the LoRA ckpt, script, and a brief doc on the new schedule.\r\n> \r\n> https://github.com/haotian-liu/LLaVA/releases/tag/v1.1.3\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 524,
    "state": "closed",
    "created_by": "wjfwjfwjf",
    "created_at": "2023-10-11T05:43:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/524</URL>\n\n<TITLE>[Question]</TITLE>\n\n<BODY>### Question\n\nhello，I want to know after pretrain, how much the loss is? what about after instruct tune?</BODY>\n\n<COMMENTS>\n<Comment by wjfwjfwjf at 2023-10-11T05:43:56Z>\nthanks\n</Comment>\n<Comment by haotian-liu at 2023-11-28T19:44:02Z>\nThe logs are provided in MODEL ZOO: https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md\r\n\r\nTraining logs: [wandb](https://api.wandb.ai/links/lht/6orh56wc).\r\n\r\nThanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 523,
    "state": "open",
    "created_by": "Narennnnn",
    "created_at": "2023-10-11T05:42:09Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/523</URL>\n\n<TITLE>Add Edit button[Feature request]</TITLE>\n\n<BODY>### feature\n\n![image](https://github.com/haotian-liu/LLaVA/assets/120191897/347d109a-6c5b-48f4-8d69-0f7d704aec96)\r\n\r\nPlease include this edit feature. \r\n![image](https://github.com/haotian-liu/LLaVA/assets/120191897/115ce6e2-d8c2-4ce6-8bdd-1be68f03a62f)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 521,
    "state": "open",
    "created_by": "FurkanGozukara",
    "created_at": "2023-10-11T01:19:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/521</URL>\n\n<TITLE>Can you add venv installer?</TITLE>\n\n<BODY>### Describe the issue\n\nConda failing on Windows\r\n\r\nI have miniconda and anaconda as well\r\n\r\n```\r\n(base) PS C:\\Users\\King> cd \"G:\\LLaVA\"\r\n(base) PS G:\\LLaVA> conda create -n llava python=3.10 -y\r\nRetrieving notices: ...working... done\r\nCollecting package metadata (current_repodata.json): done\r\nSolving environment: done\r\n\r\n\r\n==> WARNING: A newer version of conda exists. <==\r\n  current version: 23.7.2\r\n  latest version: 23.9.0\r\n\r\nPlease update conda by running\r\n\r\n    $ conda update -n base -c defaults conda\r\n\r\nOr to minimize the number of packages updated during conda update use\r\n\r\n     conda install conda=23.9.0\r\n\r\n\r\n\r\n## Package Plan ##\r\n\r\n  environment location: C:\\Users\\King\\miniconda3\\envs\\llava\r\n\r\n  added / updated specs:\r\n    - python=3.10\r\n\r\n\r\nThe following packages will be downloaded:\r\n\r\n    package                    |            build\r\n    ---------------------------|-----------------\r\n    bzip2-1.0.8                |       he774522_0         113 KB\r\n    libffi-3.4.4               |       hd77b12b_0         113 KB\r\n    openssl-3.0.11             |       h2bbff1b_2         7.4 MB\r\n    pip-23.2.1                 |  py310haa95532_0         2.8 MB\r\n    python-3.10.13             |       he1021f5_0        15.8 MB\r\n    setuptools-68.0.0          |  py310haa95532_0         934 KB\r\n    tk-8.6.12                  |       h2bbff1b_0         3.1 MB\r\n    vc-14.2                    |       h21ff451_1           8 KB\r\n    vs2015_runtime-14.27.29016 |       h5e58377_2        1007 KB\r\n    wheel-0.41.2               |  py310haa95532_0         127 KB\r\n    xz-5.4.2                   |       h8cc25b3_0         592 KB\r\n    ------------------------------------------------------------\r\n                                           Total:        32.0 MB\r\n\r\nThe following NEW packages will be INSTALLED:\r\n\r\n  bzip2              pkgs/main/win-64::bzip2-1.0.8-he774522_0\r\n  ca-certificates    pkgs/main/win-64::ca-certificates-2023.08.22-haa95532_0\r\n  libffi             pkgs/main/win-64::libffi-3.4.4-hd77b12b_0\r\n  openssl            pkgs/main/win-64::openssl-3.0.11-h2bbff1b_2\r\n  pip                pkgs/main/win-64::pip-23.2.1-py310haa95532_0\r\n  python             pkgs/main/win-64::python-3.10.13-he1021f5_0\r\n  setuptools         pkgs/main/win-64::setuptools-68.0.0-py310haa95532_0\r\n  sqlite             pkgs/main/win-64::sqlite-3.41.2-h2bbff1b_0\r\n  tk                 pkgs/main/win-64::tk-8.6.12-h2bbff1b_0\r\n  tzdata             pkgs/main/noarch::tzdata-2023c-h04d1e81_0\r\n  vc                 pkgs/main/win-64::vc-14.2-h21ff451_1\r\n  vs2015_runtime     pkgs/main/win-64::vs2015_runtime-14.27.29016-h5e58377_2\r\n  wheel              pkgs/main/win-64::wheel-0.41.2-py310haa95532_0\r\n  xz                 pkgs/main/win-64::xz-5.4.2-h8cc25b3_0\r\n  zlib               pkgs/main/win-64::zlib-1.2.13-h8cc25b3_0\r\n\r\n\r\n\r\nDownloading and Extracting Packages\r\n\r\nPreparing transaction: done\r\nVerifying transaction: done\r\nExecuting transaction: done\r\n#\r\n# To activate this environment, use\r\n#\r\n#     $ conda activate llava\r\n#\r\n# To deactivate an active environment, use\r\n#\r\n#     $ conda deactivate\r\n\r\n(base) PS G:\\LLaVA> conda activate llava\r\n(llava) PS G:\\LLaVA> pip install --upgrade pip  # enable PEP 660 support\r\nLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\r\nRequirement already satisfied: pip in c:\\users\\king\\miniconda3\\envs\\llava\\lib\\site-packages (23.2.1)\r\n(llava) PS G:\\LLaVA> pip install -e .\r\nLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\r\nObtaining file:///G:/LLaVA\r\n  Installing build dependencies ... done\r\n  Checking if build backend supports build_editable ... done\r\n  Getting requirements to build editable ... done\r\n  Installing backend dependencies ... done\r\n  Preparing editable metadata (pyproject.toml) ... done\r\nCollecting einops (from llava==1.1.0)\r\n  Obtaining dependency information for einops from https://files.pythonhosted.org/packages/29/0b/2d1c0ebfd092e25935b86509a9a817159212d82aa43d7fb07eca4eeff2c2/einops-0.7.0-py3-none-any.whl.metadata\r\n  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\r\nCollecting fastapi (from llava==1.1.0)\r\n  Obtaining dependency information for fastapi from https://files.pythonhosted.org/packages/4d/d2/3ad038a2365fefbac19d9a046cab7ce45f4c7bfa81d877cbece9707de9ce/fastapi-0.103.2-py3-none-any.whl.metadata\r\n  Downloading fastapi-0.103.2-py3-none-any.whl.metadata (24 kB)\r\nCollecting gradio==3.35.2 (from llava==1.1.0)\r\n  Obtaining dependency information for gradio==3.35.2 from https://files.pythonhosted.org/packages/50/70/ed0ba0fb5c3b1cb2e481717ad190056a4c9a0ef2f296b871e10375b2ab83/gradio-3.35.2-py3-none-any.whl.metadata\r\n  Downloading gradio-3.35.2-py3-none-any.whl.metadata (15 kB)\r\nCollecting markdown2[all] (from llava==1.1.0)\r\n  Obtaining dependency information for markdown2[all] from https://files.pythonhosted.org/packages/f1/98/61276a753f078dd2f3171c9a69fd3f451d220e806b2b1cdca41b8e368b0f/markdown2-2.4.10-py2.py3-none-any.whl.metadata\r\n  Downloading markdown2-2.4.10-py2.py3-none-any.whl.metadata (2.0 kB)\r\nCollecting numpy (from llava==1.1.0)\r\n  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/cc/05/ef9fc04adda45d537619ea956bc33489f50a46badc949c4280d8309185ec/numpy-1.26.0-cp310-cp310-win_amd64.whl.metadata\r\n  Downloading numpy-1.26.0-cp310-cp310-win_amd64.whl.metadata (61 kB)\r\n     ---------------------------------------- 61.1/61.1 kB 807.0 kB/s eta 0:00:00\r\nCollecting requests (from llava==1.1.0)\r\n  Obtaining dependency information for requests from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\r\n  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\r\nCollecting sentencepiece (from llava==1.1.0)\r\n  Downloading sentencepiece-0.1.99-cp310-cp310-win_amd64.whl (977 kB)\r\n     ---------------------------------------- 977.5/977.5 kB 4.1 MB/s eta 0:00:00\r\nCollecting tokenizers>=0.12.1 (from llava==1.1.0)\r\n  Obtaining dependency information for tokenizers>=0.12.1 from https://files.pythonhosted.org/packages/92/02/15556b80450301d2ef014bc598df4352bfb39631c5fcff758d8e0ac9f065/tokenizers-0.14.1-cp310-none-win_amd64.whl.metadata\r\n  Downloading tokenizers-0.14.1-cp310-none-win_amd64.whl.metadata (6.8 kB)\r\nCollecting torch==2.0.1 (from llava==1.1.0)\r\n  Downloading torch-2.0.1-cp310-cp310-win_amd64.whl (172.3 MB)\r\n     ---------------------------------------- 172.3/172.3 MB 38.6 MB/s eta 0:00:00\r\nCollecting torchvision==0.15.2 (from llava==1.1.0)\r\n  Downloading torchvision-0.15.2-cp310-cp310-win_amd64.whl (1.2 MB)\r\n     ---------------------------------------- 1.2/1.2 MB 15.2 MB/s eta 0:00:00\r\nCollecting uvicorn (from llava==1.1.0)\r\n  Obtaining dependency information for uvicorn from https://files.pythonhosted.org/packages/79/96/b0882a1c3f7ef3dd86879e041212ae5b62b4bd352320889231cc735a8e8f/uvicorn-0.23.2-py3-none-any.whl.metadata\r\n  Downloading uvicorn-0.23.2-py3-none-any.whl.metadata (6.2 kB)\r\nCollecting wandb (from llava==1.1.0)\r\n  Obtaining dependency information for wandb from https://files.pythonhosted.org/packages/1c/5e/0362fa88679852c7fd3ac85ee5bd949426c4a51a61379010d4089be6d7ac/wandb-0.15.12-py3-none-any.whl.metadata\r\n  Downloading wandb-0.15.12-py3-none-any.whl.metadata (9.8 kB)\r\nCollecting shortuuid (from llava==1.1.0)\r\n  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\r\nCollecting httpx==0.24.0 (from llava==1.1.0)\r\n  Downloading httpx-0.24.0-py3-none-any.whl (75 kB)\r\n     ---------------------------------------- 75.3/75.3 kB 1.0 MB/s eta 0:00:00\r\nCollecting deepspeed==0.9.5 (from llava==1.1.0)\r\n  Downloading deepspeed-0.9.5.tar.gz (809 kB)\r\n     ---------------------------------------- 809.9/809.9 kB 12.7 MB/s eta 0:00:00\r\n  Preparing metadata (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n\r\n  × python setup.py egg_info did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> [9 lines of output]\r\n      Traceback (most recent call last):\r\n        File \"<string>\", line 2, in <module>\r\n        File \"<pip-setuptools-caller>\", line 34, in <module>\r\n        File \"C:\\Users\\King\\AppData\\Local\\Temp\\pip-install-zaa916lp\\deepspeed_97fb3e87e18e4914b37cfa1e3095162b\\setup.py\", line 129, in <module>\r\n          assert torch_available, \"Unable to pre-compile ops without torch installed. Please install torch before attempting to pre-compile ops.\"\r\n      AssertionError: Unable to pre-compile ops without torch installed. Please install torch before attempting to pre-compile ops.\r\n      [WARNING] Unable to import torch, pre-compiling ops will be disabled. Please visit https://pytorch.org/ to see how to properly install torch on your system.\r\n      ←[93m [WARNING] ←[0m unable to import torch, please install it if you want to pre-compile any deepspeed ops.\r\n      DS_BUILD_OPS=1\r\n      [end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n× Encountered error while generating package metadata.\r\n╰─> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details.\r\n(llava) PS G:\\LLaVA>\r\n```</BODY>\n\n<COMMENTS>\n<Comment by FurkanGozukara at 2023-10-11T01:32:53Z>\nAnaconda fails too. We really need requirements.txt to install as a venv\r\n\r\n```\r\n(base) PS C:\\Users\\King> conda update -n base -c defaults conda\r\nCollecting package metadata (current_repodata.json): done\r\nSolving environment: done\r\n\r\n# All requested packages already installed.\r\n\r\n(base) PS C:\\Users\\King> cd \"G:\\LLaVA\"\r\n(base) PS G:\\LLaVA> conda create -n llava python=3.10 -y\r\nCollecting package metadata (current_repodata.json): done\r\nSolving environment: done\r\n\r\n## Package Plan ##\r\n\r\n  environment location: C:\\Users\\King\\miniconda3\\envs\\llava\r\n\r\n  added / updated specs:\r\n    - python=3.10\r\n\r\n\r\nThe following NEW packages will be INSTALLED:\r\n\r\n  bzip2              pkgs/main/win-64::bzip2-1.0.8-he774522_0\r\n  ca-certificates    pkgs/main/win-64::ca-certificates-2023.08.22-haa95532_0\r\n  libffi             pkgs/main/win-64::libffi-3.4.4-hd77b12b_0\r\n  openssl            pkgs/main/win-64::openssl-3.0.11-h2bbff1b_2\r\n  pip                pkgs/main/win-64::pip-23.2.1-py310haa95532_0\r\n  python             pkgs/main/win-64::python-3.10.13-he1021f5_0\r\n  setuptools         pkgs/main/win-64::setuptools-68.0.0-py310haa95532_0\r\n  sqlite             pkgs/main/win-64::sqlite-3.41.2-h2bbff1b_0\r\n  tk                 pkgs/main/win-64::tk-8.6.12-h2bbff1b_0\r\n  tzdata             pkgs/main/noarch::tzdata-2023c-h04d1e81_0\r\n  vc                 pkgs/main/win-64::vc-14.2-h21ff451_1\r\n  vs2015_runtime     pkgs/main/win-64::vs2015_runtime-14.27.29016-h5e58377_2\r\n  wheel              pkgs/main/win-64::wheel-0.41.2-py310haa95532_0\r\n  xz                 pkgs/main/win-64::xz-5.4.2-h8cc25b3_0\r\n  zlib               pkgs/main/win-64::zlib-1.2.13-h8cc25b3_0\r\n\r\n\r\n\r\nDownloading and Extracting Packages:\r\n\r\nPreparing transaction: done\r\nVerifying transaction: done\r\nExecuting transaction: done\r\n#\r\n# To activate this environment, use\r\n#\r\n#     $ conda activate llava\r\n#\r\n# To deactivate an active environment, use\r\n#\r\n#     $ conda deactivate\r\n\r\n(base) PS G:\\LLaVA> conda activate llava\r\n(llava) PS G:\\LLaVA> pip install --upgrade pip  # enable PEP 660 support\r\nLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\r\nRequirement already satisfied: pip in c:\\users\\king\\miniconda3\\envs\\llava\\lib\\site-packages (23.2.1)\r\n(llava) PS G:\\LLaVA> pip install -e .\r\nLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\r\nObtaining file:///G:/LLaVA\r\n  Installing build dependencies ... done\r\n  Checking if build backend supports build_editable ... done\r\n  Getting requirements to build editable ... done\r\n  Installing backend dependencies ... done\r\n  Preparing editable metadata (pyproject.toml) ... done\r\nCollecting einops (from llava==1.1.0)\r\n  Obtaining dependency information for einops from https://files.pythonhosted.org/packages/29/0b/2d1c0ebfd092e25935b86509a9a817159212d82aa43d7fb07eca4eeff2c2/einops-0.7.0-py3-none-any.whl.metadata\r\n  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\r\nCollecting fastapi (from llava==1.1.0)\r\n  Obtaining dependency information for fastapi from https://files.pythonhosted.org/packages/4d/d2/3ad038a2365fefbac19d9a046cab7ce45f4c7bfa81d877cbece9707de9ce/fastapi-0.103.2-py3-none-any.whl.metadata\r\n  Downloading fastapi-0.103.2-py3-none-any.whl.metadata (24 kB)\r\nCollecting gradio==3.35.2 (from llava==1.1.0)\r\n  Obtaining dependency information for gradio==3.35.2 from https://files.pythonhosted.org/packages/50/70/ed0ba0fb5c3b1cb2e481717ad190056a4c9a0ef2f296b871e10375b2ab83/gradio-3.35.2-py3-none-any.whl.metadata\r\n  Downloading gradio-3.35.2-py3-none-any.whl.metadata (15 kB)\r\nCollecting markdown2[all] (from llava==1.1.0)\r\n  Obtaining dependency information for markdown2[all] from https://files.pythonhosted.org/packages/f1/98/61276a753f078dd2f3171c9a69fd3f451d220e806b2b1cdca41b8e368b0f/markdown2-2.4.10-py2.py3-none-any.whl.metadata\r\n  Downloading markdown2-2.4.10-py2.py3-none-any.whl.metadata (2.0 kB)\r\nCollecting numpy (from llava==1.1.0)\r\n  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/cc/05/ef9fc04adda45d537619ea956bc33489f50a46badc949c4280d8309185ec/numpy-1.26.0-cp310-cp310-win_amd64.whl.metadata\r\n  Downloading numpy-1.26.0-cp310-cp310-win_amd64.whl.metadata (61 kB)\r\n     ---------------------------------------- 61.1/61.1 kB 807.0 kB/s eta 0:00:00\r\nCollecting requests (from llava==1.1.0)\r\n  Obtaining dependency information for requests from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\r\n  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\r\nCollecting sentencepiece (from llava==1.1.0)\r\n  Downloading sentencepiece-0.1.99-cp310-cp310-win_amd64.whl (977 kB)\r\n     ---------------------------------------- 977.5/977.5 kB 3.9 MB/s eta 0:00:00\r\nCollecting tokenizers>=0.12.1 (from llava==1.1.0)\r\n  Obtaining dependency information for tokenizers>=0.12.1 from https://files.pythonhosted.org/packages/92/02/15556b80450301d2ef014bc598df4352bfb39631c5fcff758d8e0ac9f065/tokenizers-0.14.1-cp310-none-win_amd64.whl.metadata\r\n  Downloading tokenizers-0.14.1-cp310-none-win_amd64.whl.metadata (6.8 kB)\r\nCollecting torch==2.0.1 (from llava==1.1.0)\r\n  Downloading torch-2.0.1-cp310-cp310-win_amd64.whl (172.3 MB)\r\n     ---------------------------------------- 172.3/172.3 MB 59.4 MB/s eta 0:00:00\r\nCollecting torchvision==0.15.2 (from llava==1.1.0)\r\n  Downloading torchvision-0.15.2-cp310-cp310-win_amd64.whl (1.2 MB)\r\n     ---------------------------------------- 1.2/1.2 MB 15.2 MB/s eta 0:00:00\r\nCollecting uvicorn (from llava==1.1.0)\r\n  Obtaining dependency information for uvicorn from https://files.pythonhosted.org/packages/79/96/b0882a1c3f7ef3dd86879e041212ae5b62b4bd352320889231cc735a8e8f/uvicorn-0.23.2-py3-none-any.whl.metadata\r\n  Downloading uvicorn-0.23.2-py3-none-any.whl.metadata (6.2 kB)\r\nCollecting wandb (from llava==1.1.0)\r\n  Obtaining dependency information for wandb from https://files.pythonhosted.org/packages/1c/5e/0362fa88679852c7fd3ac85ee5bd949426c4a51a61379010d4089be6d7ac/wandb-0.15.12-py3-none-any.whl.metadata\r\n  Downloading wandb-0.15.12-py3-none-any.whl.metadata (9.8 kB)\r\nCollecting shortuuid (from llava==1.1.0)\r\n  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\r\nCollecting httpx==0.24.0 (from llava==1.1.0)\r\n  Downloading httpx-0.24.0-py3-none-any.whl (75 kB)\r\n     ---------------------------------------- 75.3/75.3 kB 833.9 kB/s eta 0:00:00\r\nCollecting deepspeed==0.9.5 (from llava==1.1.0)\r\n  Downloading deepspeed-0.9.5.tar.gz (809 kB)\r\n     ---------------------------------------- 809.9/809.9 kB 12.9 MB/s eta 0:00:00\r\n  Preparing metadata (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n\r\n  × python setup.py egg_info did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> [9 lines of output]\r\n      Traceback (most recent call last):\r\n        File \"<string>\", line 2, in <module>\r\n        File \"<pip-setuptools-caller>\", line 34, in <module>\r\n        File \"C:\\Users\\King\\AppData\\Local\\Temp\\pip-install-r5zsmpa0\\deepspeed_f6982825619841cd9a70d79d5778b4ca\\setup.py\", line 129, in <module>\r\n          assert torch_available, \"Unable to pre-compile ops without torch installed. Please install torch before attempting to pre-compile ops.\"\r\n      AssertionError: Unable to pre-compile ops without torch installed. Please install torch before attempting to pre-compile ops.\r\n      [WARNING] Unable to import torch, pre-compiling ops will be disabled. Please visit https://pytorch.org/ to see how to properly install torch on your system.\r\n      ←[93m [WARNING] ←[0m unable to import torch, please install it if you want to pre-compile any deepspeed ops.\r\n      DS_BUILD_OPS=1\r\n      [end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n× Encountered error while generating package metadata.\r\n╰─> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details.\r\n(llava) PS G:\\LLaVA>\r\n```\n</Comment>\n<Comment by haotian-liu at 2023-10-11T02:02:05Z>\nWould be great to have the Windows support for more users from the community to try this, and it is on my list for this week. Currently working on the code release and will try to make this work this week!\n</Comment>\n<Comment by FurkanGozukara at 2023-10-11T02:11:34Z>\n> Would be great to have the Windows support for more users from the community to try this, and it is on my list for this week. Currently working on the code release and will try to make this work this week!\r\n\r\nawesome thank you\n</Comment>\n<Comment by onlinerender at 2023-10-11T02:30:03Z>\n> Would be great to have the Windows support for more users from the community to try this, and it is on my list for this week. Currently working on the code release and will try to make this work this week!\r\n\r\nGreat\n</Comment>\n<Comment by zslittlehelper at 2023-10-11T13:17:35Z>\nAdding some info here that might be of use.\r\nI've installed LLava under WSL2 and I'm using the CLI example. The model loads without an issue, but after entering a prompt it quits with this error and no other info: \r\n\"Could not load library libcudnn_cnn_infer.so.8. Error: libcuda.so: cannot open shared object file: No such file or directory\r\nAborted\"\r\n\r\nLdconfig has a reference to /usr/lib/wsl/lib/libcuda.so.1, but /usr/lib/wsl/lib/libcuda.so is apparently missing.\r\n/usr/lib/wsl/lib/libcuda.so does exist, but the internet has so far been rather unhelpful in figuring out how to add such a reference to ldconfig, or if that's even the issue.\n</Comment>\n<Comment by FurkanGozukara at 2023-10-23T20:51:57Z>\nwhen we can expect working on Windows\n</Comment>\n<Comment by Kallamamran at 2023-10-23T20:56:45Z>\nSecond that... Windows please 😃👍\n</Comment>\n<Comment by haotian-liu at 2023-10-24T00:57:35Z>\nPlease check the latest doc for Windows. I tested on my Windows 11 PC and it works now for 16-bit inference. Quantization will be supported later.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/docs/Windows.md\n</Comment>\n<Comment by FurkanGozukara at 2023-10-25T09:18:15Z>\n> Please check the latest doc for Windows. I tested on my Windows 11 PC and it works now for 16-bit inference. Quantization will be supported later.\r\n> \r\n> https://github.com/haotian-liu/LLaVA/blob/main/docs/Windows.md\r\n\r\nthanks gonna test\n</Comment>\n<Comment by FurkanGozukara at 2023-10-25T10:06:00Z>\n> Please check the latest doc for Windows. I tested on my Windows 11 PC and it works now for 16-bit inference. Quantization will be supported later.\r\n> \r\n> https://github.com/haotian-liu/LLaVA/blob/main/docs/Windows.md\r\n\r\nthank you so much\r\n\r\nI got error at last step where we start [Launch a model worker](https://github.com/haotian-liu/LLaVA#launch-a-model-worker)\r\n\r\nhttps://github.com/haotian-liu/LLaVA/issues/668\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 520,
    "state": "open",
    "created_by": "hjsg1010",
    "created_at": "2023-10-10T21:40:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/520</URL>\n\n<TITLE>[Question] When serving vicuna-13b via cli, the model produces lengthy responses.</TITLE>\n\n<BODY>### Question\n\nI served the model via cli using the following command.\r\n```console\r\npython -m llava.serve.cli --model-path ./data-vol-1/model/llava/llava-336px-pretrain-vicuna-13b-v1.3 --model-base ./data-vol-1/model/llava/vicuna_13b_v1.3 --image-file \"./llava/view.jpg\"\r\n```\r\nthese llava-336px-pretrain-vicuna-13b-v1.3 and vicuna_13b_v1.3 is downloaded from your links.\r\n\r\nHowever, as you can see in below, the model is providing excessively long responses. \r\nWould you happen to have any advice on this matter?\r\n\r\n\r\n\r\n```console\r\n[2023-10-11 06:30:46,626] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nLoading LLaVA from base model...\r\nYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\r\nLoading checkpoint shards: 100%|████████████████████████████████████████| 3/3 [00:09<00:00,  3.08s/it]\r\nSome weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at /home/jovyan/data-vol-1/model/llava/vicuna_13b_v1.3 and are newly initialized: ['model.mm_projector.weight', 'model.mm_projector.bias']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\nUSER: what do you think about this view? \r\nASSISTANT: This is a beautiful view of a lake with a dock leading into the water\r\n\r\nIt's a great spot for a swim or a boat ride\r\n\r\nThe mountains in the background add a nice touch to the scenery\r\n\r\nIt's a great spot for a vacation or a day trip\r\n\r\nIt's a great spot for a picnic or a fishing trip\r\n\r\nIt's a great spot for a family outing or a group gathering\r\n\r\nIt's a great spot for a romantic getaway or a honeymoon\r\n\r\nIt's a great spot for a solo adventure or a solo retreat\r\n\r\nIt's a great spot for a solo hike or a solo bike ride\r\n\r\nIt's a great spot for a solo camping trip or a solo fishing trip\r\n\r\nIt's a great spot for a solo swim or a solo boat ride\r\n\r\nIt's a great spot for a solo picnic or a solo fishing trip\r\n\r\nIt's a great spot for a solo adventure or a solo retreat\r\n\r\nIt's a great spot for a solo hike or a solo bike ride\r\n\r\nIt's a great spot for a solo camping trip or a solo fishing trip\r\n\r\nIt's a great spot for a solo swim or a solo boat ride\r\n\r\nIt's a great spot for a solo picnic or a solo fishing trip\r\n\r\nIt's a great spot for a solo adventure or a solo retreat\r\n\r\nIt's a great spot for a solo hike or a solo bike ride\r\n\r\nIt's a great spot for a solo camping trip or a solo fishing trip\r\n\r\nIt's a great spot for a solo swim or a solo boat ride\r\n\r\nIt's a great spot for a solo picnic or a solo fishing trip\r\n\r\nIt's a great spot for a solo adventure or a solo retreat\r\n\r\nIt's a great spot for a solo hike or a solo bike ride\r\n\r\nIt's a great spot for a solo camping trip or a solo fishing trip\r\n\r\nIt's a great spot for a solo swim or a solo boat ride\r\n\r\nIt's a great spot for a solo picnic or a solo fishing trip\r\n\r\nIt's a great spot for a solo adventure or a solo retreat\r\n\r\nIt's a great spot for a solo hike or a solo bike ride\r\n\r\nIt's a great spot for a solo camping trip or a solo fishing trip\r\n\r\nIt's a great spot for a solo swim or a solo boat ride\r\n\r\nIt's a great spot for a solo picnic or a solo fishing trip\r\n\r\nIt's a great spot for a solo adventure or a solo retreat\r\n\r\nIt's a great spot for a solo hike or a solo bike ride\r\n\r\nIt's a ^[[19~great spot for a solo camping trip or a solo fishing trip\r\n\r\nIt's a great spot for a solo swim or a solo boat ride\r\n\r\nIt's a great spot for a solo picnic or a solo fishing trip\r\n\r\nIt's a great spot for a solo adventure or a solo retreat\r\n\r\nIt's a great spot for a solo hike or a solo bike ride\r\n\r\nIt's a great spot for a solo camping trip or a solo fishing trip\r\n\r\nIt's a great spot for a solo swim or a solo boat ride\r\n\r\nIt's a great spot for a solo picnic or a solo fishing trip\r\n\r\nIt's a great spot for a solo adventure or a solo retreat\r\n\r\nIt's a great spot for a solo hike or a solo bike ride\r\n\r\nIt's a great spot for a solo camping trip or a solo fishing trip\r\n\r\nIt's a great spot for a solo swim or a solo boat ride\r\n\r\nIt's a great spot for a solo picnic or a solo fishing trip\r\n\r\nIt's a great spot for a solo adventure or a solo retreat\r\n\r\nIt's a great spot for a solo hike or a solo bike ride\r\n\r\nIt's a great spot for a solo camping trip or a solo fishing trip\r\n\r\nIt's a great spot for a solo swim or a solo boat ride\r\n\r\nIt's a great spot for a solo picnic or a solo fishing trip\r\n\r\nIt's a great spot for a solo adventure or a solo retreat\r\n\r\nIt's a great spot for a solo hike or a solo bike ride\r\n\r\nIt's a great spot for a solo camping trip or a solo fishing trip\r\n\r\nIt'\r\nUSER: what should I be cautious when I go to this place\r\nASSISTANT: When visiting this place, it is important to be cautious of the following:\r\n\r\n* The water is deep and can be dangerous for swimming, especially for children or those who are not strong swimmers\r\n* The dock is not stable and can be dangerous to walk on\r\n* The area is surrounded by mountains and can be prone to sudden weather changes, including thunderstorms and heavy rain\r\n* The area is home to wildlife, including bears and other animals, so it is important to be aware of your surroundings and take necessary precautions\r\n* The area is remote and there may not be easy access to medical care or other services in case of an emergency\r\n* The area is prone to insects and other pests, so it is important to bring insect repellent and other necessary items to protect against bites and stings\r\n* The area is prone to wildfires, so it is important to be aware of fire safety and follow any guidelines or regulations in place to prevent fires\r\n* The area is prone to flash floods, so it is important to be aware of weather conditions and follow any guidelines or regulations in place to prevent flooding\r\n* The area is prone to landslides, so it is important to be aware of weather conditions and follow any guidelines or regulations in place to prevent landslides\r\n* The area is prone to avalanches, so it is important to be aware of weather conditions and follow any guidelines or regulations in place to prevent avalanches\r\n* The area is prone to earthquakes, so it is important to be aware of weather conditions and follow any guidelines or regulations in place to prevent earthquakes\r\n* The area is prone to volcanic eruptions, so it is important to be aware of weather conditions and follow any guidelines or regulations in place to prevent volcanic eruptions\r\n* The area is prone to tsunamis, so it is important to be aware of weather conditions and follow any guidelines or regulations in place to prevent tsunamis\r\n* The area is prone to hurricanes, so it is important to be aware of weather conditions and follow any guidelines or regulations in place to prevent hurricanes\r\n* The area is prone to tornadoes, so it is important to be aware of weather conditions and follow any guidelines or regulations in place to prevent tornadoes\r\n* The area is prone to wildfires, so it is important to be aware of weather conditions and follow any guidelines or regulations in place to prevent wildfires\r\n* The area is prone to wildfires, so it is important to be aware of weather conditions and follow any guidelines or regulations in place to prevent wildfires\r\n* The area is prone to wildfires, so it is important to be aware of weather conditions and regulations in place to wildfire\r\n* The area is important to be aware of weather conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and conditions and\r\nUSER: thankyou\r\nASSISTANT: \r\nUSER: thank you\r\nASSISTANT: \r\n\r\n```</BODY>\n\n<COMMENTS>\n<Comment by RicoWjr at 2023-10-11T08:48:28Z>\nI met the similar question with the model llava-v1.5-7b, the responses are extremely worse than yours —— The model repeats some nonsense words or numbers.\n</Comment>\n<Comment by barshag at 2023-10-12T00:19:25Z>\nwhat can be done to overcome that? @RicoWjr  (Me too encountered that)\n</Comment>\n<Comment by haotian-liu at 2023-10-12T00:30:19Z>\nHey, these are the projector weights that are only trained with image-text pairs, and are not **NOT** instruction tuned, which means they do **NOT** follow instructions as good as our official models, and can output repetitive, lengthy, and garbled outputs.\r\n\r\nYou need to use [LLaVA v1.5](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#llava-v15) models directly.\r\n\r\nI just added these clarifications in the MODEL ZOO, hopefully that clears some of the doubts.\r\n\r\n> These are projector weights we have pretrained. You can use these projector weights for visual instruction tuning. They are just pretrained on image-text pairs, and are **NOT** instruction tuned, which means they do **NOT** follow instructions as good as our official models, and can output repetitive, lengthy, and garbled outputs. If you want to have nice conversations with LLaVA, use the checkpoints above (LLaVA v1.5).\n</Comment>\n<Comment by hjsg1010 at 2023-10-12T00:39:31Z>\n> Hey, these are the projector weights that are only trained with image-text pairs, and are not **NOT** instruction tuned, which means they do **NOT** follow instructions as good as our official models, and can output repetitive, lengthy, and garbled outputs.\r\n> \r\n> You need to use [LLaVA v1.5](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#llava-v15) models directly.\r\n> \r\n> I just added these clarifications in the MODEL ZOO, hopefully that clears some of the doubts.\r\n> \r\n> > These are projector weights we have pretrained. You can use these projector weights for visual instruction tuning. They are just pretrained on image-text pairs, and are **NOT** instruction tuned, which means they do **NOT** follow instructions as good as our official models, and can output repetitive, lengthy, and garbled outputs. If you want to have nice conversations with LLaVA, use the checkpoints above (LLaVA v1.5).\r\n\r\nThx for reply. I should read your clarifications.\r\n\r\n\r\nI have another question. I have a vicuna13b model that has been finetuned with my own text data (only language finetuning, not image-text pair finetuning using your script). Would it be possible to utilize this in your llava framework?\r\n\r\nCould I perhaps change the model-base in this command, or modify my custom vicuna config in some way?\r\n```console\r\npython -m llava.serve.cli --model-path ./data-vol-1/model/llava/llava-336px-pretrain-vicuna-13b-v1.3 --model-base ./data-vol-1/model/llava/vicuna_13b_v1.3 --image-file \"./llava/view.jpg\" \r\n```\n</Comment>\n<Comment by haotian-liu at 2023-10-12T00:44:17Z>\n@hjsg1010 the clarifications are added just now after I see this issue :(\r\n\r\nIf your finetuned vicuna is based on Vicuna v1.3, you may try this one:\r\nhttps://huggingface.co/liuhaotian/llava-v1-0719-336px-lora-vicuna-13b-v1.3\r\n\r\nIt is lora tuned, which means it may be somehow compatible and can be plugged in with a modified version of Vicuna v1.3, to give it visual capabilities, but I haven't tried something like this so there is no guarantee.\r\n\r\nCheck out instructions here on how to launch a model worker with LoRA adapters. CLI should be similar.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md#launch-a-model-worker\n</Comment>\n<Comment by hjsg1010 at 2023-10-12T01:39:15Z>\n> @hjsg1010 the clarifications are added just now after I see this issue :(\r\n> \r\n> If your finetuned vicuna is based on Vicuna v1.3, you may try this one: https://huggingface.co/liuhaotian/llava-v1-0719-336px-lora-vicuna-13b-v1.3\r\n> \r\n> It is lora tuned, which means it may be somehow compatible and can be plugged in with a modified version of Vicuna v1.3, to give it visual capabilities, but I haven't tried something like this so there is no guarantee.\r\n> \r\n> Check out instructions here on how to launch a model worker with LoRA adapters. CLI should be similar.\r\n> \r\n> https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md#launch-a-model-worker\r\n\r\nAre you suggesting that I should specify this model https://huggingface.co/liuhaotian/llava-v1-0719-336px-lora-vicuna-13b-v1.3 as the model-path, and set my custom model as the model-base?\r\n\r\nthx for reply. \r\nI'll read through it again carefully and give it a try.\n</Comment>\n<Comment by haotian-liu at 2023-10-12T01:50:50Z>\nYour understanding is correct.\n</Comment>\n<Comment by hjsg1010 at 2023-10-12T03:45:52Z>\n@haotian-liu Thanks a lot. You've been a great help to me. \r\nI wish the multi-turn conversation feature would be available soon in the CLI environment or Jupyter. For now, I'm also trying to implement it myself.\n</Comment>\n<Comment by haotian-liu at 2023-10-12T03:57:37Z>\nWait, multi-turn conversation is already supported. See the gif (wait for around 10 seconds or more to see the second query): https://github.com/haotian-liu/LLaVA#cli-inference\n</Comment>\n<Comment by RicoWjr at 2023-10-12T04:05:52Z>\n> what can be done to overcome that? @RicoWjr (Me too encountered that)\r\n\r\nI just find that i pulled the llava docker image which pushed months ago by someone and its code is not compatible with the latest llava-v1.5. You can check whether your code version is compatible with the model version\n</Comment>\n<Comment by hjsg1010 at 2023-10-12T04:57:49Z>\n> Wait, multi-turn conversation is already supported. See the gif (wait for around 10 seconds or more to see the second query): https://github.com/haotian-liu/LLaVA#cli-inference\r\n\r\noh, I mean several conversation with several images. so I can test few-shot with my images\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 519,
    "state": "closed",
    "created_by": "mberman84",
    "created_at": "2023-10-10T20:29:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/519</URL>\n\n<TITLE>Does this only work with Linux machines?</TITLE>\n\n<BODY>### Question\n\nI see a fork for Apple machines, but it's broken (missing requirements.txt file).\r\n\r\nWhen I go through this using Windows 11, it seems at least some of the install process requires Linux. \r\n\r\nCan you confirm?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-24T00:58:20Z>\nPlease check the latest doc for Windows. I tested on my Windows 11 PC and it works now for 16-bit inference. Quantization will be supported later.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/docs/Windows.md\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 517,
    "state": "open",
    "created_by": "lucasjinreal",
    "created_at": "2023-10-10T12:04:02Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/517</URL>\n\n<TITLE>[Discussion] Chinese OCR enhancement?</TITLE>\n\n<BODY>### Discussion\n\nIt seems it can not do Chinese OCR at all</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 516,
    "state": "closed",
    "created_by": "onlinerender",
    "created_at": "2023-10-10T11:11:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/516</URL>\n\n<TITLE>Does this project support Windows 11 PCs with an RTX 4090 GPU? I'm still experiencing problems</TITLE>\n\n<BODY>### Discussion\r\n\r\nCan anyone help me with getting this code to work on a Windows 11 PC?</BODY>\n\n<COMMENTS>\n<Comment by mberman84 at 2023-10-10T20:17:15Z>\nI'm trying to get it to work but it looks like some of the dependencies require a Linux environment, so I'm guessing it won't work on Windows.\n</Comment>\n<Comment by haotian-liu at 2023-10-24T00:58:10Z>\nPlease check the latest doc for Windows. I tested on my Windows 11 PC and it works now for 16-bit inference. Quantization will be supported later.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/docs/Windows.md\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 515,
    "state": "closed",
    "created_by": "Cuiunbo",
    "created_at": "2023-10-10T10:47:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/515</URL>\n\n<TITLE>[Question] Reproduction Inquiry on TextVQA using llava-v1.5-13b</TITLE>\n\n<BODY>### Question\r\n\r\nThinks for your wonderful work! I‘m currently trying to reproduce the evaluation results of llava-v1.5-13b on TextVQA as outlined in your paper.\r\n\r\nEval part:\r\nI used the provided code from this GitHub repository for model inference, and incorporated the TextVQA-specific instruction mentioned in the paper as follows:\r\n```\r\ntext_instructions = 'Answer the question using a single word or phrase.'\r\nif self.model.config.mm_use_im_start_end:\r\n\tqs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + qs\r\nelse:\r\n\tqs = DEFAULT_IMAGE_TOKEN + '\\n' + qs\r\n\r\nqs = qs + ' ' + text_instructions\r\n\r\nconv = conv_templates['llava_v1'].copy() \r\nconv.append_message(conv.roles[0], qs)\r\nconv.append_message(conv.roles[1], None)\r\nprompt = conv.get_prompt()\r\n\r\ninput_ids = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\r\n\r\nimage_tensor = self.image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\r\n\r\nstop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\r\nkeywords = [stop_str]\r\nstopping_criteria = KeywordsStoppingCriteria(keywords, self.tokenizer, input_ids)\r\n\r\nwith torch.inference_mode():\r\n\toutput_ids = self.model.generate(\r\n\t\tinput_ids,\r\n\t\timages=image_tensor.unsqueeze(0).half().cuda(),\r\n\t\tdo_sample=True,\r\n\t\ttemperature=0.2,\r\n\t\tmax_new_tokens=1024,\r\n\t\tuse_cache=True,\r\n\t\tstopping_criteria=[stopping_criteria])\r\n\t\r\ninput_token_len = input_ids.shape[1]\r\nn_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\r\nif n_diff_input_output > 0:\r\n\tprint(f'[Warning] {n_diff_input_output} output_ids are not the same as the input_ids')\r\noutputs = self.tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]\r\noutputs = outputs.strip()\r\nif outputs.endswith(stop_str):\r\n\toutputs = outputs[:-len(stop_str)]\r\noutputs = outputs.strip()\r\n```\r\n\r\nScoring part:\r\nI employed the VQA-Score method on TextVQA's official website, and obtained a score of 54.2 over 500 cases, which deviates significantly from your paper's reported results. Additionally, I tried another scoring method where ACC=1 if any correct answer exists in GT, and achieved a score of 61.8 over 500 cases, which aligns closer to the results in your paper.\r\n\r\n# Questions:\r\n1. Could you help pinpoint any potential missteps in my replication approach? \r\n2. Is the discrepancy due to incorrect usage of conv_templates during inference, or a misalignment in the metric calculation applied on TextVQA?\r\n\r\nYour guidance on these matters would be immensely appreciated. Thank you!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-12T01:03:10Z>\nAll evaluation instructions are now released, and please let me know if you cannot reproduce with the official scripts, thanks!\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md\n</Comment>\n<Comment by Cuiunbo at 2023-10-12T07:46:16Z>\nThank you very much! Now, I am able to reproduce your result. \r\nIt seems that for TextVQA evaluation, LLaVA1.5 followed InstructBLIP, and utilizes a format like this:\r\n“{conversation template} {question} {OCR_Token} {answer's instruction}.\"\r\nis this correct?\n</Comment>\n<Comment by haotian-liu at 2023-10-12T17:28:40Z>\nYep, your understanding is correct. We'll also add this detail in the revision, thanks.\n</Comment>\n<Comment by itzsid at 2023-10-14T18:23:50Z>\n@haotian-liu I'm not able to find this file for TextVQA eval: `llava_textvqa_val_v051_ocr.jsonl`.\n</Comment>\n<Comment by yancie-yjr at 2023-10-16T05:57:13Z>\n> Yep, your understanding is correct. We'll also add this detail in the revision, thanks.\r\n\r\nSo, the performance of TextVQA in paper uses the “{conversation template} {question} {OCR_Token} {answer's instruction}.\" format to eval?\n</Comment>\n<Comment by haotian-liu at 2023-10-16T06:26:54Z>\n@itzsid \r\n\r\nPlease follow the instructions [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md#scripts) to prepare the data on processed files for evaluation.\r\n\r\n> Before preparing task-specific data, download [eval.zip](https://drive.google.com/file/d/1atZSBBrAX54yYpxtVVW33zFvcnaHeFPy/view?usp=sharing). It contains custom annotations, scripts, and the prediction files with LLaVA v1.5. Extract to ./playground/data/eval. This also provides a general structure for all datasets.\r\n\r\n\r\n@yancie-yjr \r\n\r\nYes we followed InstructBLIP on the evaluation setting for TextVQA.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 514,
    "state": "open",
    "created_by": "lower01",
    "created_at": "2023-10-10T10:18:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/514</URL>\n\n<TITLE>数据集文件夹的格式</TITLE>\n\n<BODY>### Question\n\n刘哥，如果使用少部分其它数据进行微调，数据集文件夹的格式应该是怎样的呢？我看到coco2014_val_gpt4_qa_30x3.jsonl中的格式是id-image-instruction-output，意思是需要在相同目录下准备一个对应的图片文件夹（图片名称对应于image编号）吗？\r\n\r\n由jsonl文件和图片文件夹两部分组成训练数据？</BODY>\n\n<COMMENTS>\n<Comment by cherry956 at 2024-01-30T02:42:39Z>\n@lower01  I am loading the weights of the llava-v1.5-7b from huggingface. Then store them in a folder and load them into LLava code. I also have my own dataset in the same as format, now I want to use it to finetune LLava-v1.5-7b with lora. if I need to run the finetune_task_lora.sh?\r\n![96f41047fabfc11b97cfb079bb9adab](https://github.com/haotian-liu/LLaVA/assets/144820412/68dd3459-8afc-42fc-bb9d-07974ef0eb97)\r\nHow should I modify the parameters?Thanks!!\n</Comment>\n<Comment by cherry956 at 2024-01-30T02:43:12Z>\nHere is the weight I load from huggingface\r\n![adabed6957ab87e827747ac8ddbbf55](https://github.com/haotian-liu/LLaVA/assets/144820412/b3368742-9754-4729-8d3d-6e238f0656d8)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 513,
    "state": "open",
    "created_by": "Zombiessss",
    "created_at": "2023-10-10T09:15:57Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/513</URL>\n\n<TITLE>[Question] gradio web loading model failed</TITLE>\n\n<BODY>### Question\n\n<img width=\"2193\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/25706482/e80d8492-c71d-479c-9885-e0e5254506bf\">\r\n按顺序执行三个脚本，web_demo上面没出现model，worker显示已load上</BODY>\n\n<COMMENTS>\n<Comment by yangtao2019yt at 2024-02-28T07:37:50Z>\n遇到同样问题，请问解决了吗？\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 509,
    "state": "closed",
    "created_by": "LALBJ",
    "created_at": "2023-10-10T05:00:56Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/509</URL>\n\n<TITLE>[Usage] How to use ` llava-v1.5-7b` for inference</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nThank you for sharing your work; it has been incredibly helpful to me. However, I encountered an issue while using `liuhaotian/llava-v1.5-7b` for inference. When I attempted to build the model using the provided code, the generated content turned out to be subpar. I am unsure where the problem lies or what went wrong.\r\n\r\nCommand:\r\n```\r\n# model_path liuhaotian/llava-v1.5-7b\r\ntokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\ncfg_pretrained = AutoConfig.from_pretrained(model_path)\r\nmodel = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs)\r\n\r\n# download from liuhaotian/llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5\r\nmm_projector_weights = torch.load('mm_projector.bin', map_location='cpu')\r\nmm_projector_weights = {k: v.to(torch.float16) for k, v in mm_projector_weights.items()}\r\nmodel.load_state_dict(mm_projector_weights, strict=False)\r\n```</BODY>\n\n<COMMENTS>\n<Comment by RicoWjr at 2023-10-11T09:57:36Z>\nI updated the code and got a good model response （because i pulled a docker image several months ago and it is not support thr latest version of llava）\n</Comment>\n<Comment by LALBJ at 2023-10-12T09:22:07Z>\nWhen I completed the pull of the latest code, the issue was resolved.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 507,
    "state": "closed",
    "created_by": "zfy1041264242",
    "created_at": "2023-10-10T03:35:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/507</URL>\n\n<TITLE>[Usage] Problems that arise when using cli.py for inference</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nWhen I tried to use cli.py locally to perform inference on lava-v1.5-13b, I was prompted that the checkpoint could not be loaded, but in fact both files existed.\r\nCommand:\r\n```\r\npython -m llava.serve.cli \\\r\n    --model-path ./llava-v1.5-13b \\\r\n    --image-file /train/1/1.jpg \\\r\n    --load-4bit\r\n```\r\n\r\nLog: \r\n```\r\n[2023-10-10 03:29:14,703] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nllava-v1.5-13b\r\nLoading checkpoint shards:  33%|██████████████████████████                                                    | 1/3 [00:06<00:13,  6.79s/it]\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 460, in load_state_dict\r\n    return torch.load(checkpoint_file, map_location=\"cpu\")\r\n  File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py\", line 993, in load\r\n    with _open_zipfile_reader(opened_file) as opened_zipfile:\r\n  File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py\", line 447, in __init__\r\n    super().__init__(torch._C.PyTorchFileReader(name_or_buffer))\r\nRuntimeError: PytorchStreamReader failed reading zip archive: failed finding central directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 464, in load_state_dict\r\n    if f.read(7) == \"version\":\r\n  File \"/root/anaconda3/envs/llava/lib/python3.10/codecs.py\", line 322, in decode\r\n    (result, consumed) = self._buffer_decode(data, self.errors, final)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 128: invalid start byte\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/root/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/remote-home/cs_acmis_zfy/MLLM/LLaVA/llava/serve/cli.py\", line 120, in <module>\r\n    main(args)\r\n  File \"/remote-home/cs_acmis_zfy/MLLM/LLaVA/llava/serve/cli.py\", line 33, in main\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit)\r\n  File \"/remote-home/cs_acmis_zfy/MLLM/LLaVA/llava/model/builder.py\", line 103, in load_pretrained_model\r\n    model = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\n  File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2903, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\n  File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3246, in _load_pretrained_model\r\n    state_dict = load_state_dict(shard_file)\r\n  File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 476, in load_state_dict\r\n    raise OSError(\r\nOSError: Unable to load weights from pytorch checkpoint file for './llava-v1.5-13b/pytorch_model-00002-of-00003.bin' at './llava-v1.5-13b/pytorch_model-00002-of-00003.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\r\n```\r\n\r\nScreenshots:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/112854533/50033656-95b5-4980-9a28-7696a3adb60a)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-10T04:49:41Z>\nSeems that the model is not properly downloaded? Can you check the size of each file under the llava-v1.5-13b directory, as well as the md5/sha1?\n</Comment>\n<Comment by andrewgross at 2023-10-12T17:59:31Z>\nI am seeing the same issue, I think it is an issue with the weights in the .git repository.  `git status` shows that all files are correct (it had to fix 00002). However, the sum of the `.bin` file sizes does not match the total size in `pytorch_model.bin.index.json`.\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/370118/dd08c355-45ec-4357-bbad-c9c3376e6ba1)\r\n\r\n`26094813519`:  Total Size (on disk)\r\n`26094673920`: Total Size (index.json)\r\n\r\nFiles were fetched via:\r\n`git clone https://huggingface.co/liuhaotian/llava-v1.5-13b`\n</Comment>\n<Comment by haotian-liu at 2023-10-12T18:03:40Z>\n@zfy1041264242 I realized that this may be because you did not install git-lfs, so that these files are just some links to the actual files? Try `git lfs pull`.\r\n\r\n@andrewgross But this may not be your case? Is your machine connected to the internet? If so, can you try this as it automatically downloads from HF and we have verified that this works.\r\n\r\n```\r\npython -m llava.serve.cli \\\r\n    --model-path liuhaotian/llava-v1.5-13b \\\r\n    --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n    --load-4bit\r\n```\n</Comment>\n<Comment by andrewgross at 2023-10-12T18:22:07Z>\nThanks for the fast reply. I ran your suggested code and got a different error.\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/370118/3f1da71c-0e15-43c6-9c75-7e5c3e9b00de)\r\n\r\nAfter entering the text prompt, the machine froze and stopped responding to any SSH sessions.  The only way to get the system responsive again was manually rebooting it. Unfortunately no logs to syslog or kern.log.\r\n\r\nRerunning after a reboot induced the same behavior. Unfortunately it doesn't seem to writing any sort of core dump either.  I had issues with segfaulting when trying to run the gradio server/controller/worker setup as well, the worker would segfault when it tried to load the model.\n</Comment>\n<Comment by haotian-liu at 2023-10-12T18:24:29Z>\n@andrewgross Can you share your system info? OS, CPU RAM, GPU type and count?\r\n\r\nAlso, maybe try replacing 13b with 7b?\n</Comment>\n<Comment by andrewgross at 2023-10-12T18:25:23Z>\nSystem Specs:\r\n\r\nAMD 7950x3D\r\nDual 4090\r\n128 GB DDR5 RAM\r\nAsus X670E-E @ 1.0.0.7c\r\n\r\nLinux 6.2.0-34-generic #34~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Sep  7 13:12:03 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2023 NVIDIA Corporation\r\nBuilt on Mon_Apr__3_17:16:06_PDT_2023\r\nCuda compilation tools, release 12.1, V12.1.105\r\nBuild cuda_12.1.r12.1/compiler.32688072_0\r\n\r\nCUDA installed via aptitude package manager `530.30.02-0ubuntu1`\r\n\r\n```python\r\n>>> import torch\r\n>>> torch.__version__\r\n'2.0.1+cu117'\r\n```\n</Comment>\n<Comment by haotian-liu at 2023-10-12T18:26:43Z>\nAlso, try `CUDA_VISIBLE_DEVICES=0`, as the current code tries to use all GPUs. I may need to change this behavior.\r\n\r\nAnd what is your pytorch version? After receiving some reports on the compatibility issue with PyTorch 2.1 as it requires CUDA 11 by default, I have temporarily set the version to `torch==2.0.1` and this seems to fix many issues. Not sure if you were using an older version that does not include this.\n</Comment>\n<Comment by andrewgross at 2023-10-12T18:29:09Z>\nIll give the single GPU thing a shot. I have had issues in the past using dual GPUs because torch reports that 4090s have an NVLINK interconnect, which causes weird issues when moving tensors. See [this issue](https://github.com/turboderp/exllamav2/issues/85) and this [partial fix](https://github.com/turboderp/exllamav2/commit/5bba4182b5d1d34e3866fb084ff667f937ad24a6) from [exllamav2](https://github.com/turboderp/exllamav2/) for more details.  There was a follow up commit to fix some edge cases when uses did not want to use all CUDA gpus.\n</Comment>\n<Comment by andrewgross at 2023-10-12T18:35:34Z>\n* ` CUDA_VISIBLE_DEVICES=0  --model-path liuhaotian/llava-v1.5-7b`: \r\n\r\nWorks :tada:\r\n\r\n----------\r\n\r\n* `--model-path liuhaotian/llava-v1.5-7b`: \r\n \r\nFails with\r\n\r\n`RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument tensors in method wrapper_CUDA_cat)`\r\n\r\n---------\r\n\r\n* `CUDA_VISIBLE_DEVICES=0  --model-path liuhaotian/llava-v1.5-13b`: \r\n\r\nWorks :tada:\r\n\r\n---------\r\n\r\nDefinitely smells like some CUDA / multi GPU issues.\n</Comment>\n<Comment by andrewgross at 2023-10-12T18:43:08Z>\nSome more info on nvidia forums of similar issues: https://forums.developer.nvidia.com/t/standard-nvidia-cuda-tests-fail-with-dual-rtx-4090-linux-box/233202/22?page=2\n</Comment>\n<Comment by andrewgross at 2023-10-12T19:18:43Z>\nThank you for all your help diagnosing this issue.\n</Comment>\n<Comment by haotian-liu at 2023-10-13T05:36:57Z>\nGreat, closing this for now :)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 506,
    "state": "closed",
    "created_by": "hjsg1010",
    "created_at": "2023-10-10T03:35:02Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/506</URL>\n\n<TITLE>[Usage] RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.cli \\\r\n    --model-path \"/home/LLaVA-main/pretrained/LLaVA-Lightning-MPT-7B-preview\" \\\r\n    --image-file \"/home/LLaVA-main/view.jpg\" \\\r\n    --load-4bit\r\n```\r\n\r\nLog: \r\n```\r\n[2023-10-10 12:31:07,178] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nYou are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\r\nThe model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\r\nLoading checkpoint shards: 100%|████████████████████████████████| 2/2 [00:27<00:00, 13.77s/it]\r\nuser: please describe the view \r\nTraceback (most recent call last):\r\n  File \"/home/jovyan/.local/share/conda/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/jovyan/.local/share/conda/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/jovyan/LLaVA-main/llava/serve/cli.py\", line 119, in <module>\r\n    main(args)\r\n  File \"/home/jovyan/LLaVA-main/llava/serve/cli.py\", line 89, in main\r\n    output_ids = model.generate(\r\n  File \"/home/jovyan/.local/share/conda/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/jovyan/.local/share/conda/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1588, in generate\r\n    return self.sample(\r\n  File \"/home/jovyan/.local/share/conda/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2642, in sample\r\n    outputs = self(\r\n  File \"/home/jovyan/.local/share/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/jovyan/.local/share/conda/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/jovyan/LLaVA-main/llava/model/language_model/llava_mpt.py\", line 76, in forward\r\n    input_ids, attention_mask, past_key_values, inputs_embeds, labels = self.prepare_inputs_labels_for_multimodal(input_ids, attention_mask, past_key_values, labels, images)\r\n  File \"/home/jovyan/LLaVA-main/llava/model/llava_arch.py\", line 103, in prepare_inputs_labels_for_multimodal\r\n    image_features = self.encode_images(images)\r\n  File \"/home/jovyan/LLaVA-main/llava/model/llava_arch.py\", line 83, in encode_images\r\n    image_features = self.get_model().get_vision_tower()(images)\r\n  File \"/home/jovyan/.local/share/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/jovyan/.local/share/conda/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/jovyan/.local/share/conda/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/jovyan/LLaVA-main/llava/model/multimodal_encoder/clip_encoder.py\", line 48, in forward\r\n    image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\r\n  File \"/home/jovyan/.local/share/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/jovyan/.local/share/conda/envs/llava/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 941, in forward\r\n    return self.vision_model(\r\n  File \"/home/jovyan/.local/share/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/jovyan/.local/share/conda/envs/llava/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 866, in forward\r\n    hidden_states = self.embeddings(pixel_values)\r\n  File \"/home/jovyan/.local/share/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/jovyan/.local/share/conda/envs/llava/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 195, in forward\r\n    patch_embeds = self.patch_embedding(pixel_values)  # shape = [*, width, grid, grid]\r\n  File \"/home/jovyan/.local/share/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/jovyan/.local/share/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 463, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n  File \"/home/jovyan/.local/share/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\r\n    return F.conv2d(input, weight, bias, self.stride,\r\nRuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED\r\n```\r\n\r\nI have torch 2.0.0+cu117, and 4 A100 80Gb gpus.</BODY>\n\n<COMMENTS>\n<Comment by hjsg1010 at 2023-10-10T03:47:51Z>\nthis is how I changed LLaVA-Lightning-MPT-7B-preview model's config\r\n```\r\n{\r\n  \"_name_or_path\": \"llava-mpt-7b-pretrain_blip_558k_new_template_1e-instruct_conv_reason_nooverlap_80k-1epoch\",\r\n  \"architectures\": [\r\n    \"LlavaMPTForCausalLM\"\r\n  ],\r\n  \"attn_config\": {\r\n    \"alibi\": true,\r\n    \"alibi_bias_max\": 8,\r\n    \"attn_impl\": \"torch\",\r\n    \"attn_pdrop\": 0,\r\n    \"attn_type\": \"multihead_attention\",\r\n    \"attn_uses_sequence_id\": false,\r\n    \"clip_qkv\": null,\r\n    \"prefix_lm\": false,\r\n    \"qk_ln\": false,\r\n    \"softmax_scale\": null\r\n  },\r\n  \"d_model\": 4096,\r\n  \"emb_pdrop\": 0,\r\n  \"embedding_fraction\": 1.0,\r\n  \"expansion_ratio\": 4,\r\n  \"freeze_mm_mlp_adapter\": false,\r\n  \"init_config\": {\r\n    \"emb_init_std\": null,\r\n    \"emb_init_uniform_lim\": null,\r\n    \"fan_mode\": \"fan_in\",\r\n    \"init_div_is_residual\": true,\r\n    \"init_gain\": 0,\r\n    \"init_nonlinearity\": \"relu\",\r\n    \"init_std\": 0.02,\r\n    \"name\": \"kaiming_normal_\",\r\n    \"verbose\": 0\r\n  },\r\n  \"init_device\": \"cpu\",\r\n  \"learned_pos_emb\": true,\r\n  \"logit_scale\": null,\r\n  \"max_seq_len\": 2048,\r\n  \"mm_hidden_size\": 1024,\r\n  \"mm_use_im_start_end\": true,\r\n  \"mm_vision_select_layer\": -2,\r\n  \"mm_vision_tower\": \"/home/jovyan/data-vol-1/model/llava/clip-vit-large-patch14-336\",\r\n  \"model_type\": \"llava_mpt\",\r\n  \"n_heads\": 32,\r\n  \"n_layers\": 32,\r\n  \"no_bias\": true,\r\n  \"norm_type\": \"low_precision_layernorm\",\r\n  \"resid_pdrop\": 0,\r\n  \"sep_image_conv_front\": false,\r\n  \"tokenizer_name\": \"/home/jovyan/LLaVA-main/pretrained/gpt-neox-20b-chatml\",\r\n  \"torch_dtype\": \"float16\",\r\n  \"transformers_version\": \"4.28.0.dev0\",\r\n  \"tune_mm_mlp_adapter\": false,\r\n  \"use_cache\": true,\r\n  \"use_mm_proj\": true,\r\n  \"verbose\": 0,\r\n  \"vocab_size\": 50282\r\n}\r\n\r\n```\r\nI just changed mm_vision_tower and tokenizer_name to my local directories\n</Comment>\n<Comment by hjsg1010 at 2023-10-10T04:58:18Z>\nalso same error when I tried with \"llava-v1.5-7b\" model\n</Comment>\n<Comment by YFCodeDream at 2024-04-25T10:02:15Z>\nHello! I want to know how you solved this problem.\n</Comment>\n<Comment by Dongjie-Cheng at 2024-05-06T15:21:59Z>\nSame here. I want to know how you solved this problem.\n</Comment>\n<Comment by C0NGTRI123 at 2024-05-08T07:46:56Z>\nI solve this use load_checkpoint to 4 bit like that:\r\n```\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n                                                                    model_path=args.model,\r\n                                                                    model_base=None,\r\n                                                                    model_name=model_name,\r\n                                                                    load_4bit=True)\r\n```\r\nI think that error when VRAM GPU not enough to load checkpoint (This is case I debug when I catch this bug)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 505,
    "state": "closed",
    "created_by": "dongzhiwu",
    "created_at": "2023-10-10T03:01:57Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/505</URL>\n\n<TITLE>[Question] projector weights of llava1.5（pretrain model，corresponding to llama2 and clip）</TITLE>\n\n<BODY>sorry, i find the base llm in your tech report of llava-v1.5 is vicuna-13b, there is projector of vicuna-13b and clip 336 in your model zoo</BODY>\n\n<COMMENTS>\n<Comment by dongzhiwu at 2023-10-10T03:15:04Z>\nAnd any plan to release the pretrain script,  instruction script of llava-v1.5?\n</Comment>\n<Comment by dongzhiwu at 2023-10-10T03:15:42Z>\nAnd any plan to release to instruction training dataset of llava-v1.5? thanks!\n</Comment>\n<Comment by haotian-liu at 2023-11-05T05:10:47Z>\nSorry I missed this issue on the data release day. I tried to find all of them but for some reason this was missed.\r\n\r\n[10/11] The training data and scripts of LLaVA-1.5 are released [here](https://github.com/haotian-liu/LLaVA#train), and evaluation scripts are released [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md)!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 503,
    "state": "open",
    "created_by": "lucas-0liveira",
    "created_at": "2023-10-09T19:07:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/503</URL>\n\n<TITLE>CLIP resolution mechanics</TITLE>\n\n<BODY>### Discussed in https://github.com/haotian-liu/LLaVA/discussions/218\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **Don-Chad** June  5, 2023</sup>\r\nHi all,\r\n\r\nCan anyone explain how LAVIS is able to give accurate visual results, with CLIP/BLIP only being 256x256? That's really small. With this size you hardly can recognise any detail. Does it extract image sections, scaling these up and judging these individually?</div></BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 502,
    "state": "open",
    "created_by": "lucas-0liveira",
    "created_at": "2023-10-09T19:06:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/502</URL>\n\n<TITLE>CLIP resolution mechanics</TITLE>\n\n<BODY>### Discussed in https://github.com/haotian-liu/LLaVA/discussions/218\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **Don-Chad** June  5, 2023</sup>\r\nHi all,\r\n\r\nCan anyone explain how LAVIS is able to give accurate visual results, with CLIP/BLIP only being 256x256? That's really small. With this size you hardly can recognise any detail. Does it extract image sections, scaling these up and judging these individually?</div></BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 501,
    "state": "closed",
    "created_by": "camenduru",
    "created_at": "2023-10-09T18:20:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/501</URL>\n\n<TITLE>🦒 colab</TITLE>\n\n<BODY>Thanks for the project ❤️ I made a colab. 🥳 I hope you like it. https://github.com/camenduru/LLaVA-colab</BODY>\n\n<COMMENTS>\n<Comment by AmmarFahmy at 2023-10-10T14:46:09Z>\nGreat work!!! ...\r\nI just ran both Colab files. Works fine. But models are not downloading and not showing in the Gradio drop-down too.\n</Comment>\n<Comment by camenduru at 2023-10-10T17:26:46Z>\nThanks ❤ @AmmarFahmy please watch this https://www.youtube.com/watch?v=o7zQAa0NPds\n</Comment>\n<Comment by AmmarFahmy at 2023-10-11T18:31:30Z>\n> Thanks ❤ @AmmarFahmy please watch this https://www.youtube.com/watch?v=o7zQAa0NPds\r\n\r\nYes great .. worked fine.\n</Comment>\n<Comment by haotian-liu at 2023-10-12T01:37:00Z>\nThank you! Wondering if you would like to add 13B with 4bit as well? We made it work with T4 and 15GB CPU RAM on HF Space. So theoretically it should work for Colab as well? 13B-4bit seems to be both faster and more accurate than 7B-8bit. (will get some numbers later)\r\n\r\nhttps://huggingface.co/spaces/badayvedat/LLaVA\n</Comment>\n<Comment by camenduru at 2023-10-12T16:15:13Z>\nHi @haotian-liu 👋 Colab Free T4 has only 12.7 GB CPU RAM 😭 so I can only fit `llava-v1.5-7b` with a `max_shard_size='5GB' ` model.\r\n\r\n```py\r\nfrom llava.model.builder import load_pretrained_model\r\nmodel_path = \"liuhaotian/llava-v1.5-7b\"\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    model_path=model_path,\r\n    model_base=None,\r\n    model_name=model_path.split(\"/\")[-1],\r\n    load_8bit=False,\r\n    load_4bit=False\r\n)\r\n```\r\n\r\n```py\r\nout_folder = \"/content/model8\"\r\nmodel.save_pretrained(out_folder, max_shard_size='5GB', safe_serialization=False)\r\ntokenizer.save_pretrained(out_folder)\r\n```\r\nhttps://huggingface.co/4bit/llava-v1.5-7b-5GB/tree/main\r\n\r\nI will try the same thing with `liuhaotian/llava-v1.5-13b`\n</Comment>\n<Comment by camenduru at 2023-10-14T00:48:11Z>\nMaybe not possible  😐\r\n\r\nhttps://huggingface.co/4bit/llava-v1.5-13b-8bit/tree/main\r\nhttps://huggingface.co/4bit/llava-v1.5-13b-5GB/tree/main\r\n\r\n![Screenshot 2023-10-14 014828fdfd](https://github.com/haotian-liu/LLaVA/assets/54370274/17325a9b-607a-4a34-9da5-edbf925bef58)\n</Comment>\n<Comment by haotian-liu at 2023-10-14T05:55:09Z>\n@camenduru \r\n\r\nNuh it is possible. Not sure what is different, but I created a new one with 3gb-shard here: [liuhaotian/llava-v1.5-13b-shard3gb](https://huggingface.co/liuhaotian/llava-v1.5-13b-shard3gb).\r\n\r\nAlso, here is a minimal working example you can try: https://colab.research.google.com/drive/1aJBcR7aIV2i9EnKE5EA--XvwzIGiFugg?usp=sharing\r\n\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/6631389/a9f6f056-f4e7-472b-a697-b1b8b40723bc)\n</Comment>\n<Comment by camenduru at 2023-10-14T10:56:04Z>\n🥳 wow cool 🔥 maybe transformers version idk 😋 I will add to the repo thanks ❤\n</Comment>\n<Comment by haotian-liu at 2023-10-14T17:55:17Z>\n@camenduru Thanks! Also featured the link to Colab on our README :)\n</Comment>\n<Comment by camenduru at 2023-10-14T18:42:31Z>\nthanks ❤\n</Comment>\n<Comment by nickkolok at 2024-04-25T21:46:26Z>\nHi all! Thank you for doing such amazing things!\r\n\r\n1. Any chances to get LLaVA 1.6 working on Colab?\r\n2. Is there a possibility to add an \"Undo\" button, as at https://huggingface.co/spaces/merve/llava-next ? That would be really helpful!\r\n3. How about context extension? Is that really possible?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 500,
    "state": "closed",
    "created_by": "z5229889frankwht",
    "created_at": "2023-10-09T12:02:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/500</URL>\n\n<TITLE>[Question] How to evaluate the VQA benchmark?</TITLE>\n\n<BODY>### Question\n\nDear,\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/61502604/cd0db5d5-3e39-43b7-9808-0d6c14198106)\r\n\r\n\r\nI hope this message finds you in good health. I am writing to request information on the method you used to evaluate your model on the VQA benchmark. I am particularly interested in understanding the specific steps you followed to apply your model to this task, as detailed in your paper.\r\n\r\nCould you please provide a step-by-step account of how you implemented your model for VQA benchmark evaluation? What datasets did you use for evaluation?\r\n\r\nI would greatly appreciate it if you could share any information that would help me replicate your work. Your approach, as detailed in your paper, achieved an impressive accuracy of 80%, and I am interested in understanding how this was achieved and possibly reproduce the result.\r\n\r\nThank you very much for your time and response. Please let me know if there is any additional information you need from me.\r\n\r\nBest Regards,\r\nFrank</BODY>\n\n<COMMENTS>\n<Comment by liyang-7 at 2023-10-10T02:05:57Z>\n期待LLaVA-1.5中的测评数据集和测评代码\r\n好人一生平安  Thanks♪(･ω･)ﾉ\n</Comment>\n<Comment by KainingYing at 2023-10-10T05:19:07Z>\n我也想要啊\n</Comment>\n<Comment by z5229889frankwht at 2023-10-10T10:41:35Z>\n> 期待LLaVA-1.5中的测评数据集和测评代码 好人一生平安 Thanks♪(･ω･)ﾉ\r\n\r\n我看vqav2的annotations都是十个投票的，LLaVA1.0的是跟GPT-4比较的，就有点好奇是怎么比较的\n</Comment>\n<Comment by haotian-liu at 2023-10-12T01:02:38Z>\nAll evaluation instructions are released now :)\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md\n</Comment>\n<Comment by z5229889frankwht at 2023-10-12T08:09:28Z>\n> All evaluation instructions are released now :)\r\n> \r\n> https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md\r\n\r\nThank you!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 499,
    "state": "open",
    "created_by": "lucasjinreal",
    "created_at": "2023-10-09T08:53:59Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/499</URL>\n\n<TITLE>[Question] How to chat with video sequences?</TITLE>\n\n<BODY>### Question\n\nHow to chat with video sequences?</BODY>\n\n<COMMENTS>\n<Comment by barshag at 2023-10-12T00:33:19Z>\nlook at video llava\n</Comment>\n<Comment by CrazyBrick at 2023-10-18T07:13:24Z>\n> look at video llava\r\n\r\ncould you share a link of \"video llava\"? thanks!\n</Comment>\n<Comment by Anthony6197 at 2023-11-06T21:01:14Z>\n> > look at video llava\r\n> \r\n> could you share a link of \"video llava\"? thanks!\r\n\r\nI am also interested in this, since I am only able to find \"Video-Llama\" instead.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 498,
    "state": "closed",
    "created_by": "wjfwzzc",
    "created_at": "2023-10-09T08:42:14Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/498</URL>\n\n<TITLE>[Question] What is the base LLM of LLaVA-v1.5</TITLE>\n\n<BODY>### Question\n\nAs the title mentioned, what is the base LLM of LLaVa-v1.5, LLaMA, LLaMA-2 or LLaMA-2-Chat?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-09T11:47:15Z>\nIt is Vicuna v1.5, which is based on LLaMA-2.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 497,
    "state": "open",
    "created_by": "coderlemon17",
    "created_at": "2023-10-09T06:25:12Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/497</URL>\n\n<TITLE>[Question] Question about the `vision_resampler` and `img_tokens = 128` in LLaVA-1.5</TITLE>\n\n<BODY>### Question\n\nHi, I've read the technical report of LLaVA-1.5 and I'm impressed by its performance. However, I have some questions after reading the code for LLaVA-1.5.\r\n\r\n1. In this line we hand-code the number of img_tokens to be `128`. However, by using `clip-vit-large-patch14-336` as claimed in the paper, I think the number of img_tokens should be $(336 / 14) ^ 2 = 576$ ?\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/e854a2bf85118c504f6f16bf5c3c7c92f8fa8c6b/llava/train/train.py#L645\r\n\r\n3. When saving the model, we additionally save a `vision_resampler`, but I can't find this module elsewhere in the code. If it refers to the similar module as used in Flamingo or Lynx, I believe it should be found in the code?\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/e854a2bf85118c504f6f16bf5c3c7c92f8fa8c6b/llava/train/llava_trainer.py#L159</BODY>\n\n<COMMENTS>\n<Comment by Linziyang1999 at 2023-11-21T10:24:52Z>\nmaybe it's not used yet?\n</Comment>\n<Comment by Linziyang1999 at 2023-11-21T10:40:14Z>\ni only see modality_lengths used in trainer and no vision_resampler layer.\n</Comment>\n<Comment by findalexli at 2023-12-08T08:10:46Z>\nAlso following on this on the mismtach between 128 and 576\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 496,
    "state": "open",
    "created_by": "lsorber",
    "created_at": "2023-10-09T06:19:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/496</URL>\n\n<TITLE>[Usage] Out of memory error in Colab because of 10GB shards</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: Google Colab crashes with an out of memory error when loading LLaVa. This can be avoided with a smaller shard size of say 2GB. A T4 instance has only 12.7GB of system memory available, of which about 2GB is consumed on startup, which leaves just under 10GB available for loading pickle files.\r\n\r\nCommand:\r\n```\r\n%pip install --quiet git+https://github.com/haotian-liu/LLaVA.git@v1.1.0\r\nfrom llava.model.builder import load_pretrained_model\r\nmodel_path = \"liuhaotian/llava-v1.5-7b\"\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    model_path=model_path,\r\n    model_base=None,\r\n    model_name=model_path.split(\"/\")[-1],\r\n    load_8bit=False,\r\n    load_4bit=True\r\n)\r\n```</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 495,
    "state": "open",
    "created_by": "shoutOutYangJie",
    "created_at": "2023-10-09T05:49:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/495</URL>\n\n<TITLE>[Question] I can not find the code that generate instruction data</TITLE>\n\n<BODY>### Question\n\nIn your paper \"Visual Instruction Tuning\", you leverage chatGPT or GPT4 to generate instruction data. And you said the corresponding codebase will be released. But I don't find the code. I just find some txt file. But I don't know how to use it to generate the instruction data. Please help me</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 494,
    "state": "closed",
    "created_by": "yix-chen",
    "created_at": "2023-10-09T04:04:59Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/494</URL>\n\n<TITLE>[Question] Any plans to release llava-1.5 dataset?</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-13T03:43:18Z>\nThe training data and scripts of LLaVA-1.5 are released [here](https://github.com/haotian-liu/LLaVA#train), and evaluation scripts are released [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md) yesterday.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 493,
    "state": "closed",
    "created_by": "ZetangForward",
    "created_at": "2023-10-08T16:54:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/493</URL>\n\n<TITLE>Unable to inference on the V100 GPU (Driver Version: 470.182.03 / CUDA version 11.1 and 11.4)</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nI find the code below cannot work under the CUDA Version of 11.1 and 11.4, where the torch version is ``torch=2.1.0``.\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.cli  --model-path /zecheng/svg_model_hub/llava-v1.5-13b --image-file /workspace/zecheng/ns/svg_dataset/output.png\r\n```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/chewu/.conda/envs/llava/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/chewu/.conda/envs/llava/lib/python3.9/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/workspace/LLaVA/llava/serve/cli.py\", line 119, in <module>\r\n    main(args)\r\n  File \"/workspace/LLaVA/llava/serve/cli.py\", line 32, in main\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit)\r\n  File \"/workspace/LLaVA/llava/model/builder.py\", line 140, in load_pretrained_model\r\n    vision_tower.to(device=device, dtype=torch.float16)\r\n  File \"/home/chewu/.conda/envs/llava/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1160, in to\r\n    return self._apply(convert)\r\n  File \"/home/chewu/.conda/envs/llava/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 810, in _apply\r\n    module._apply(fn)\r\n  File \"/home/chewu/.conda/envs/llava/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 810, in _apply\r\n    module._apply(fn)\r\n  File \"/home/chewu/.conda/envs/llava/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 810, in _apply\r\n    module._apply(fn)\r\n  [Previous line repeated 1 more time]\r\n  File \"/home/chewu/.conda/envs/llava/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 833, in _apply\r\n    param_applied = fn(param)\r\n  File \"/home/chewu/.conda/envs/llava/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1158, in convert\r\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\r\n  File \"/home/chewu/.conda/envs/llava/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 298, in _lazy_init\r\n    torch._C._cuda_init()\r\nRuntimeError: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver.\r\n\r\n```\r\nI hope the author can fix this problem or provide some alternative approaches to run this code on this machine (CUDA version==11.1 or 11.4)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-10T06:28:47Z>\nThis may be due to the recent release of PyTorch 2.1, which requires CUDA 12.1 by default. I temporarily set the torch version to 2.0.1 in https://github.com/haotian-liu/LLaVA/commit/fda0665106c5b13bbb5b1ffb167ce5e32cd7c8d7. Can you pull the latest code and try a fresh install?\n</Comment>\n<Comment by ZetangForward at 2023-10-10T07:38:34Z>\n> This may be due to the recent release of PyTorch 2.1, which requires CUDA 12.1 by default. I temporarily set the torch version to 2.0.1 in [fda0665](https://github.com/haotian-liu/LLaVA/commit/fda0665106c5b13bbb5b1ffb167ce5e32cd7c8d7). Can you pull the latest code and try a fresh install?\r\n\r\nThank you very much. This modification works! Now, this code can be run on my machine!\n</Comment>\n<Comment by qishisuren123 at 2024-07-17T05:05:35Z>\nCould you please tell me the versions of your libraries, including torch, torchvision, and flash-attn? I encountered the following error while installing flash-attn.\r\n![ecdc33879a18c433015afc2b557f3307](https://github.com/user-attachments/assets/3b987b4d-5db0-40aa-891d-ff02743a5dd6)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 492,
    "state": "open",
    "created_by": "menacingsoul",
    "created_at": "2023-10-08T10:37:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/492</URL>\n\n<TITLE>[Feature request]</TITLE>\n\n<BODY>### feature\n\nAdd voice typing feature</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 491,
    "state": "closed",
    "created_by": "lower01",
    "created_at": "2023-10-08T10:29:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/491</URL>\n\n<TITLE>关于预训练和微调逻辑，以及基于的模型</TITLE>\n\n<BODY>### Question\n\n如果增加自己的部分数据进行增量预训练，到底是算预训练还是算微调？？，，是用预训练脚本还是用微调脚本还是用lora微调脚本呢？？？\r\n然后是直接基于项目中完整版权重的LLaVA1.5版本进行吗？得到对应微调权重然后合并？</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-08T14:14:58Z>\n> If adding our own partial data for incremental pre-training, is it considered as pre-training or fine-tuning?? Which script should be used - the pre-training script, the fine-tuning script, or the Lora fine-tuning script??\r\n\r\nIf you have image-text pairs for a more specific domains, you should use the pretraining scripts.\r\nIf it is instruction-following data, you can go with the finetuning scripts. Using LoRA or full-ft depends on the training budget.\r\n\r\n> Also, should it be directly based on the full-weight LLaVA1.5 version from the project? And then obtain the corresponding fine-tuning weights and merge them?\r\n\r\nWe will release the full training scripts early in the coming week. You can checkout the schedule and findings in [LLaVA-Med](https://arxiv.org/abs/2306.00890), which is training a biomedical assistant based on LLaVA checkpoints. If you use LoRA, you can choose to [merge the weights](https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md#create-merged-checkpoints) or inference [on-the-fly with LoRA adapters](https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md#launch-a-model-worker). If it is finetuning, and the checkpoint is good to go without any additional processing.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 490,
    "state": "open",
    "created_by": "OpenJarvisAI",
    "created_at": "2023-10-08T10:19:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/490</URL>\n\n<TITLE>[Question] How does this question resolve?</TITLE>\n\n<BODY>### Question\n\n<img width=\"875\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/136460643/f24959d8-fd77-479f-bf52-41fdd42a496f\"></BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 489,
    "state": "open",
    "created_by": "OpenJarvisAI",
    "created_at": "2023-10-08T09:32:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/489</URL>\n\n<TITLE>[Question] Does all LLaVA1.5 dataset opensourced?</TITLE>\n\n<BODY>### Question\n\nIn paper, it says:\r\n\r\n1. subset of some public dataset, is there any plan to open the sampled dataset for reproduce?\r\n2. you said the training are hard to converge with reduandt params and limited data, why not add all data?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 488,
    "state": "closed",
    "created_by": "shipengai",
    "created_at": "2023-10-08T09:19:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/488</URL>\n\n<TITLE>[Question] The usage of llava_instruct_80k.json?</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-04T23:47:13Z>\nHi, we construct the 80k instruction data by subsampling the LLaVA-Instruct-150K:\r\n- Find the overlapping and non-overlapping images from the complex reasoning questions and conversation questions.\r\n- Add non-overlapping image-instruction pairs to each category (reason and conv)\r\n- Randomly sample from the overlapping set to make them to have 40K each.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 487,
    "state": "closed",
    "created_by": "SuperMasterBlasterLaser",
    "created_at": "2023-10-08T07:36:26Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/487</URL>\n\n<TITLE>[Question] Launching from code.</TITLE>\n\n<BODY>### Question\r\n\r\nThere are only examples of launching this project as gradio service, or launch from cli. Can you provide examples of how to launch this project inside code?</BODY>\n\n<COMMENTS>\n<Comment by rossaai at 2023-10-09T18:42:26Z>\nhttps://github.com/haotian-liu/LLaVA/blob/main/llava/serve/cli.py\r\n\r\nThis cli script can be use\n</Comment>\n<Comment by SuperMasterBlasterLaser at 2023-10-10T03:38:39Z>\n@rossaai I just started to use `subprocess` to make cli calls from my code.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 486,
    "state": "closed",
    "created_by": "tingxueronghua",
    "created_at": "2023-10-08T07:35:14Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/486</URL>\n\n<TITLE>[Question] Will the dataset used to train llava-1.5 be public?</TITLE>\n\n<BODY>### Question\n\nIt is great to see llava-1.5 achieves such excellent performance. Want to inquire whether the dataset used to train it will be public together with the training and evaluation scripts?\r\n\r\nAsking about this because I am interested in adding more new data into the pretraining stage. Lacking the original data might harm the performance.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-08T14:05:11Z>\nYes, we will release the training datasets, scripts, and evaluation scripts on benchmarks in this coming week.\n</Comment>\n<Comment by tingxueronghua at 2023-10-09T03:10:02Z>\nThanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 485,
    "state": "open",
    "created_by": "FHL1998",
    "created_at": "2023-10-08T04:28:14Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/485</URL>\n\n<TITLE>[Question] About the usage of fine-tuned model</TITLE>\n\n<BODY>### Question\n\nI met a question when using the fine-tuned model, I used the `fintune.sh` for fine-tuning. And raise an error when using the `model_worker`: Some weights of the model checkpoint at ./checkpoints/llava-vicuna-v1-3-13b-finetune-336 were not used when initializing LlavaLlamaForCausalLM:\r\n![a56aafa560fb0533a04f08e8731be71](https://github.com/haotian-liu/LLaVA/assets/59732331/99946f99-97c2-4677-b907-c943342da5a7)\r\nDoes anyone have any idea how to solve the problem?\r\n@haotian-liu</BODY>\n\n<COMMENTS>\n<Comment by ultrazhl98 at 2023-10-17T02:55:15Z>\nI also have same problem when I merge LoRA weights\n</Comment>\n<Comment by haotian-liu at 2023-10-17T03:20:32Z>\nHi, these are normal, as DeepSpeed saves all model weights, including the frozen vision encoders. When we upload the model weights, we removed that part to make it compatible with different code versions.\r\n\r\nYou can ignore the error, if the inference results seem okay.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 484,
    "state": "closed",
    "created_by": "nj159",
    "created_at": "2023-10-08T03:29:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/484</URL>\n\n<TITLE>After downloading llava-v1.5-7b locally, then I uploaded it to my cloud server, but it didn't load successfully [Question]</TITLE>\n\n<BODY>### Question\n\nexcuse me, I need your help very much, when I load llava-v1.5-7b to the web side in the third terminal, I changed the path, why does it keep showing in the request huggingface connection, I can't connect huggingface, so I download the model to the local and upload it to my cloud server. The directory where the model files in the server are located is: /opt/data/private/models/LLaVA/liuhaotian/llava-v1.5-7b/。Is it problem about model path?\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/144583677/b3e12e4c-27b2-4d59-ad32-8eef3fde07d7)\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/144583677/49aaa55d-b478-40fd-901e-58e4ae302fa8)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-08T03:32:24Z>\nPlease remove the leading `/` for `--model-path`, like '--model-path liuhaotian/llava-v1.5-7b'\n</Comment>\n<Comment by nj159 at 2023-10-08T03:37:17Z>\nThank you very much, but the result is still the same，Do I need to change the config.json file?\r\n![image](https://github.com/haotian-liu/LLaVA/assets/144583677/05f6baa6-d6bf-4604-a791-c3caedb6b29e)\r\n![image](https://github.com/haotian-liu/LLaVA/assets/144583677/30f85b14-e72c-47f7-9fae-a7fda2c9689e)\n</Comment>\n<Comment by haotian-liu at 2023-10-08T03:38:16Z>\nIs this an offline server?\n</Comment>\n<Comment by nj159 at 2023-10-08T03:46:11Z>\nSorry, I didn't quite understand what you meant, but I opened the global proxy and the proxy test was normal.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/144583677/050f6cae-a432-4330-a4c4-2b2c8836e24e)\n</Comment>\n<Comment by nj159 at 2023-10-08T03:55:25Z>\n> Is this an offline server?\r\nThank you very much, this is online.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/144583677/90d31300-f960-4276-9930-4ddc0486398b)\n</Comment>\n<Comment by pineking at 2023-10-08T08:32:16Z>\n> Thank you very much, but the result is still the same，Do I need to change the config.json file? ![image](https://user-images.githubusercontent.com/144583677/273424161-05f6baa6-d6bf-4604-a791-c3caedb6b29e.png) ![image](https://user-images.githubusercontent.com/144583677/273424167-30f85b14-e72c-47f7-9fae-a7fda2c9689e.png)\r\n\r\nThe server can't find clip-336 model locally , and can't download it from huggingface.co\n</Comment>\n<Comment by nj159 at 2023-10-08T09:09:13Z>\n> > 非常感谢，但是结果还是一样，我需要更改config.json文件吗？![图像](https://user-images.githubusercontent.com/144583677/273424161-05f6baa6-d6bf-4604-a791-c3caedb6b29e.png) ![图像](https://user-images.githubusercontent.com/144583677/273424167-30f85b14-e72c-47f7-9fae-a7fda2c9689e.png)\r\n> \r\n> 服务器无法在本地找到clip-336型号，也无法从 huggingface.co 下载\r\n\r\nThank you, but may I ask what is the reason for this? Or are there any good solutions? I just can't access the model I've already downloaded.\n</Comment>\n<Comment by White-Friday at 2023-10-20T03:03:03Z>\n\"2023-10-20 10:52:28 | ERROR | stderr | OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like openai/clip-vit-large-patch14-336 is not the path to a directory containing a file named config.json.\r\n2023-10-20 10:52:28 | ERROR | stderr | Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\"\r\n\r\nI have a similar problem\n</Comment>\n<Comment by Harry-Deng at 2023-10-21T14:41:53Z>\nI have a similar problem 2 :-(\r\n![image](https://github.com/haotian-liu/LLaVA/assets/72896380/2fb614bf-844a-4cff-a2af-3a8d494d2f4b)\n</Comment>\n<Comment by nj159 at 2023-10-23T11:46:14Z>\n> “2023-10-20 10：52：28 |错误 |斯特德尔 |OSError：我们无法连接到“https://huggingface.co”来加载此文件，在缓存文件中找不到它，看起来openai/clip-vit-large-patch14-336不是包含名为config.json的文件的目录的路径。 2023-10-20 10：52：28 |错误 |斯特德尔 |在'https://huggingface.co/docs/transformers/installation#offline-mode'查看您的互联网连接或查看如何在离线模式下运行图书馆。\r\n> \r\n> 我有类似的问题\r\nIf you can't find the model clip-vit-large-patch14-336 , you can download this model at the download address: https://huggingface.co/openai/clip-vit-large-patch14，and change the name to clip-vit-large-patch14-336\n</Comment>\n<Comment by nj159 at 2023-10-23T11:46:52Z>\n> If you can't find the model clip-vit-large-patch14-336 , you can download this model at the download address: [https://huggingface.co/openai/clip-vit-large-patch14，and](https://huggingface.co/openai/clip-vit-large-patch14%EF%BC%8Cand) change the name to clip-vit-large-patch14-336\r\n\r\ndownload the model  clip-vit-large-patch14-336\n</Comment>\n<Comment by Harry-Deng at 2023-10-24T10:39:38Z>\nThank you very much. Due to **temporary difficulties**, Hugging Face is currently unable to provide services in certain regions. My solution is: Download the specific models from Hugging Face and manually modify the paths in config.json or some .py files. When changing the path, pay attention to the position of the '/' symbol and the naming requirements of the repo. After making the changes, you can solve it by running it again.\r\n\r\nDuring this process, there is a chance to trigger an issue of the 'bitsandbytes' package being missing. If this problem occurs, you can pull the 'bitsandbytes' package locally via git clone and then manually run setup.py.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 482,
    "state": "open",
    "created_by": "yaroslavMain",
    "created_at": "2023-10-07T20:03:45Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/482</URL>\n\n<TITLE>Video</TITLE>\n\n<BODY>### Question\n\nIs it possible to add video support to llava so that the model can describe what happened in the video?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 481,
    "state": "closed",
    "created_by": "yaroslavMain",
    "created_at": "2023-10-07T19:58:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/481</URL>\n\n<TITLE>Videos</TITLE>\n\n<BODY>### Question\n\nIs it possible to add video support to llama so that the model can describe what happened in the video?</BODY>\n\n<COMMENTS>\n<Comment by yaroslavMain at 2023-10-07T20:03:12Z>\n> ### Question\r\n> Is it possible to add video support to llama so that the model can describe what happened in the video?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 480,
    "state": "closed",
    "created_by": "SxJyJay",
    "created_at": "2023-10-07T06:44:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/480</URL>\n\n<TITLE>[Question] Why the projection layer and the main model are split?</TITLE>\n\n<BODY>### Question\n\nThanks for your great work! I am confused about the reason of independently saving LLava weights applied delta and the vision-to-text projector weights. It seems that the projector (i.e., mm_projector in code) bridges the vision and text worlds, therefore, without this projector the LLava model will fail in multimodal tasks. In a nutshell, the projector and the main model are consistently coupled together, thus I am curious about why do you saving them independently as different huggingface repo.?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-08T14:07:50Z>\nIf you are talking about full model weights like [`llava-v1.5-13b`](https://huggingface.co/liuhaotian/llava-v1.5-13b) and projector weights like [`llava-v1.5-mlp2x-336px-pretrain-vicuna-13b-v1.5`](https://huggingface.co/liuhaotian/llava-v1.5-mlp2x-336px-pretrain-vicuna-13b-v1.5), they are independent of each other.\r\n\r\nThe projector weights are pretrained linear/MLP layers after the first-stage pretraining, and only linear/MLP layers are optimized. The full model weights are after the second-stage visual instruction tuning (full-ft/lora), and includes the linear layers.\r\n\r\nYou can check out our paper for more details about the two-stage training scheme. Thanks.\n</Comment>\n<Comment by erjiaxiao at 2024-04-30T12:23:32Z>\nHello @haotian-liu, if I apply the delta weights to llama by following the instructions below, do I get the full llava weights including the projector, or do I still need to download the projector and use it in the code?\r\n![屏幕截图 2024-04-30 201827](https://github.com/haotian-liu/LLaVA/assets/44388426/bf36af2f-7368-40d4-bd93-b0e1a4687c87)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 477,
    "state": "closed",
    "created_by": "barshag",
    "created_at": "2023-10-06T22:23:56Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/477</URL>\n\n<TITLE>[Question]  What's the system prompt that is used in the DEMO site? (because it give much better answer than running it locally)</TITLE>\n\n<BODY>### Question\n\nThe demo here:\r\nhttps://llava.hliu.cc/\r\n\r\nThis is the prompt as running it locally: \r\n\r\nsystem=\"A chat between a curious user and an artificial intelligence assistant. \"\r\n    \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",</BODY>\n\n<COMMENTS>\n<Comment by rossman22590 at 2023-10-07T00:31:17Z>\nwhere do you put the models?\n</Comment>\n<Comment by barshag at 2023-10-07T02:18:36Z>\nin the cloud (instance with gpu -> its work)\r\n\r\nis it the same prompt as i wrote?\r\n\r\nOn Sat, 7 Oct 2023 at 03:31, Ross Cohen ***@***.***> wrote:\r\n\r\n> where do you put the models?\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/haotian-liu/LLaVA/issues/477#issuecomment-1751527416>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AO6X5HNOXAIRYKFPI7H55NLX6CPF7AVCNFSM6AAAAAA5WNJXDKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTONJRGUZDONBRGY>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\n</Comment>\n<Comment by haotian-liu at 2023-11-05T00:08:34Z>\nThe system prompts in the demo is the same as the one we released :)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 476,
    "state": "open",
    "created_by": "barshag",
    "created_at": "2023-10-06T12:54:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/476</URL>\n\n<TITLE>Does CLI script work the same for V1.5? (no need modification fro the prompts?..)[Question]</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by shotarok at 2023-10-06T21:51:56Z>\nHello 👋 After installing the latest code ([f7d2c1a](https://github.com/haotian-liu/LLaVA/commit/f7d2c1afc3d27e1e1bc2337591381ece36f40392)), I could run `llava/serve/cli.py` with `liuhaotian/llava-v1.5-13b` on an T4 GCP instance like below:\r\n\r\n```console\r\n(base) jupyter:~/LLaVA$ git show HEAD | head -n 1\r\ncommit f7d2c1afc3d27e1e1bc2337591381ece36f40392\r\n\r\n(base) jupyter:~/LLaVA$ python llava/serve/cli.py --image-file 'https://raw.githubusercontent.com/haotian-liu/LLaVA/main/llava/serve/examples/extreme_ironing.jpg' --model-path 'liuhaotian/llava-v1.5-13b' --load-4bit --debug\r\n[2023-10-06 21:46:28,396] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:24<00:00, 28.07s/it]\r\nUSER: What is unusual about this image?\r\n\r\nASSISTANT: The unusual aspect of this image is that a man is ironing a shirt while standing on the back of a moving taxi cab. This is an unconventional and unexpected scene, as ironing is typically done in a more stable environment, such as a home or a laundry facility. The man's ability to perform this task while standing on a moving vehicle demonstrates a high level of skill and balance, making the scene quite remarkable and intriguing.\r\n\r\n {'prompt': \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\\nWhat is unusual about this image? ASSISTANT:\", 'outputs': \"The unusual aspect of this image is that a man is ironing a shirt while standing on the back of a moving taxi cab. This is an unconventional and unexpected scene, as ironing is typically done in a more stable environment, such as a home or a laundry facility. The man's ability to perform this task while standing on a moving vehicle demonstrates a high level of skill and balance, making the scene quite remarkable and intriguing.</s>\"} \r\n\r\nUSER: exit...\r\n```\n</Comment>\n<Comment by haotian-liu at 2023-10-08T14:03:50Z>\nCLI inference for LLaVA-1.5 is supported with `v1.1.0` release. No need for additional changes and the commands are [here](https://github.com/haotian-liu/LLaVA#cli-inference).\n</Comment>\n<Comment by ZeguanXiao at 2023-10-09T13:07:15Z>\n@haotian-liu Do all versions of llava share the same system prompt?\n</Comment>\n<Comment by haotian-liu at 2023-10-10T03:59:11Z>\n@ZeguanXiao No, the system prompt generally follows that of the base LLM -- Vicuna based models currently share the same system prompt; similar for llama-2-chat / mpt-based models.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 475,
    "state": "open",
    "created_by": "simplelifetime",
    "created_at": "2023-10-06T03:26:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/475</URL>\n\n<TITLE>[Question]  Can old training script work for llava-1.5?</TITLE>\n\n<BODY>### Question\n\nI have updated LLaVA to 1.10, It seem that the old scripts finetune.sh not working for newer llava-1.5. What modifications should I do to make it work?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 474,
    "state": "open",
    "created_by": "basteran",
    "created_at": "2023-10-05T10:40:28Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/474</URL>\n\n<TITLE>[Usage] Cannot load the tuned projector weights</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI want to fine-tune a multi-modal LLM on a downstream task that uses both images and text. This is what I've done:\r\n1. I tried to use LLaMA 2 Chat as LLM for LLaVA, I tuned the whole model using the script in [finetune_lora.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/finetune_lora.sh) and successfully saved it.   \r\n2. I loaded the tuned model using the [eval.py](https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/run_llava.py) script you provided like this:\r\n```\r\n  nohup python -u ./llava/eval/run_llava.py \\\r\n    --model-path ./checkpoints/$MODEL_NAME \\\r\n    --model-base /models/$MODEL_BASE \\\r\n    --input-file-path ./dataset/test.xlsx \\\r\n    --image-path ./dataset/images\r\n```\r\nwhere:\r\na. `$MODEL_NAME` is the folder where the result of 1. was saved\r\nb. `$MODEL_BASE` is the the local model path downloaded from [LLaMA 2 Chat](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) \r\nc. `--input-file-path` and `--image-path` are folders and the [eval.py](https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/run_llava.py) script has been modified in order to read all the texts and images in the folder\r\n\r\nBut I think there is a problem with the projector: I cannot figure out how to save those weights because, as I understood from your paper, those weights are still tuned during my procedure. When I load the model for the evaluation with the above code I get a warning and disastrous outputs:\r\nLog: \r\n```\r\nSome weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at /models/Llama-2-13b-chat-hf and are newly initialized: ['model.mm_projector.bias', 'model.mm_projector.weight']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n```\r\n\r\nFinally, I tried to explicitly tune the projector weights by passing the argument `--tune_mm_mlp_adapter True` but the results are the same. Any thoughts @haotian-liu?\r\n\r\nThank you in advance.</BODY>\n\n<COMMENTS>\n<Comment by FHL1998 at 2023-10-13T04:17:56Z>\nMet the same issue here, may I ask if you have solved it?\n</Comment>\n<Comment by haotian-liu at 2023-10-13T04:30:09Z>\nHi @basteran @FHL1998 \r\n\r\nThe error message is expected, as that appears when it tries to load the --model-base (in your case llama-2-chat) as a llava-llama-2 model. it will be loaded later.\r\n\r\nDo you see any actual errors, or the results are terrible? \r\n\r\nAlso, try the commands here, which is a lora we have trained: https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md#launch-a-model-worker\n</Comment>\n<Comment by FHL1998 at 2023-10-13T05:28:22Z>\n> Hi @basteran @FHL1998\r\n> \r\n> The error message is expected, as that appears when it tries to load the --model-base (in your case llama-2-chat) as a llava-llama-2 model. it will be loaded later.\r\n> \r\n> Do you see any actual errors, or the results are terrible?\r\n> \r\n> Also, try the commands here, which is a lora we have trained: https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md#launch-a-model-worker\r\n\r\nThx for your kind reply. I faced another issue related to this.  After performing instruction fine-tuning without Lora (using `finetune.sh` directly), I used the the script below for gradio demo:\r\n\r\n`python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./checkpoints/llava-v1.5-13b-fine-tune`\r\n\r\nThis script raise another similar warning `Some weights of the model checkpoint at ./checkpoints/llava-v1.5-13b-fine-tune were not used when initializing LlavaLlamaForCausalLM: ['model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', ...]`\r\n\r\nI wonder if this is normal, cause I find the response is not affected by the warning message. Does this mean I need to further operate on the checkpoints after fine-tuning? This is similar to issue #382. Thx in advance if you can guide me through this @haotian-liu . \r\n\r\n\r\nI attached `config.json` within `./checkpoints/llava-v1.5-13b-fine-tune`  as well, I did not see obvious difference with [:](https://huggingface.co/liuhaotian/llava-v1.5-7b/blob/main/config.json)\r\n`{\r\n  \"_name_or_path\": \"lmsys/vicuna-7b-v1.5\",\r\n  \"architectures\": [\r\n    \"LlavaLlamaForCausalLM\"\r\n  ],\r\n  \"bos_token_id\": 1,\r\n  \"eos_token_id\": 2,\r\n  \"freeze_mm_mlp_adapter\": false,\r\n  \"hidden_act\": \"silu\",\r\n  \"hidden_size\": 4096,\r\n  \"image_aspect_ratio\": \"pad\",\r\n  \"image_grid_pinpoints\": null,\r\n  \"initializer_range\": 0.02,\r\n  \"intermediate_size\": 11008,\r\n  \"max_position_embeddings\": 4096,\r\n  \"mm_hidden_size\": 1024,\r\n  \"mm_projector_type\": \"mlp2x_gelu\",\r\n  \"mm_use_im_patch_token\": false,\r\n  \"mm_use_im_start_end\": false,\r\n  \"mm_vision_select_feature\": \"patch\",\r\n  \"mm_vision_select_layer\": -2,\r\n  \"mm_vision_tower\": \"openai/clip-vit-large-patch14-336\",\r\n  \"model_type\": \"llava\",\r\n  \"num_attention_heads\": 32,\r\n  \"num_hidden_layers\": 32,\r\n  \"num_key_value_heads\": 32,\r\n  \"pad_token_id\": 0,\r\n  \"pretraining_tp\": 1,\r\n  \"rms_norm_eps\": 1e-05,\r\n  \"rope_scaling\": null,\r\n  \"tie_word_embeddings\": false,\r\n  \"torch_dtype\": \"bfloat16\",\r\n  \"transformers_version\": \"4.31.0\",\r\n  \"tune_mm_mlp_adapter\": false,\r\n  \"use_cache\": true,\r\n  \"use_mm_proj\": true,\r\n  \"vocab_size\": 32000\r\n}`\n</Comment>\n<Comment by haotian-liu at 2023-10-13T05:32:23Z>\n@FHL1998 you do not need to do anything else. This is normal as this is because DeepSpeed saves all parameters including the vision tower. If you want to remove the vision tower as the checkpoint we released, you can do this:\r\n\r\n```\r\npython -m llava.model.consolidate --src model --dst model_consolidate\r\n```\r\n\r\nThe model prediction would be the same regardless of you do anything like that.\n</Comment>\n<Comment by basteran at 2023-10-16T11:24:22Z>\n> Hi @basteran @FHL1998\r\n> \r\n> The error message is expected, as that appears when it tries to load the --model-base (in your case llama-2-chat) as a llava-llama-2 model. it will be loaded later.\r\n> \r\n> Do you see any actual errors, or the results are terrible?\r\n\r\nThank you for getting back to me. I see no error messages, just the results are terrible like the model is tuned only on the texts and disregards the images.\r\n\r\n> Also, try the commands here, which is a lora we have trained: https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md#launch-a-model-worker\r\n\r\nYes, I have tried those commands and with your model the results seem reasonable, when I load my model they are the same as using the eval.py script. What do you think? @haotian-liu\n</Comment>\n<Comment by haotian-liu at 2023-10-16T15:43:53Z>\n@basteran \r\n\r\n> just the results are terrible like the model is tuned only on the texts and disregards the images.\r\n\r\nWhat is your model_path? does it contain both \"llava\" and \"llama_2\" or something?\r\n\r\nAlso, if you can share the whole command you used for tuning the lora, it may allow me to better understand.\n</Comment>\n<Comment by tingxueronghua at 2023-10-17T02:58:52Z>\n> https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md#launch-a-model-worker\r\n\r\nHi haotian, I have the similar question. \r\n\r\n1. Is mm_proj tuned during the fine-tuning stage of Llava if using Lora? (I guess the answer should be yes)\r\n2. If it is tuned, will the tuned projector be saved and loaded to the model during inference?\n</Comment>\n<Comment by tingxueronghua at 2023-10-17T03:13:34Z>\n> > https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md#launch-a-model-worker\r\n> \r\n> Hi haotian, I have the similar question.\r\n> \r\n> 1. Is mm_proj tuned during the fine-tuning stage of Llava if using Lora? (I guess the answer should be yes)\r\n> 2. If it is tuned, will the tuned projector be saved and loaded to the model during inference?\r\n\r\noh I think I get the point. The \"non_lora_trainable.bin\" will solve this problem. Sorry for interrupting.\n</Comment>\n<Comment by basteran at 2023-10-17T08:52:05Z>\n``> @basteran\r\n> \r\n> > just the results are terrible like the model is tuned only on the texts and disregards the images.\r\n> \r\n> What is your model_path? does it contain both \"llava\" and \"llama_2\" or something?\r\n\r\nThis is the model_path = ./checkpoints/llava-llama2chat13b-tune_projector-finetune_lora\r\n\r\n> \r\n> Also, if you can share the whole command you used for tuning the lora, it may allow me to better understand.\r\n\r\n```\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --lora_enable True \\\r\n    --model_name_or_path ./models/llama2chat13b \\\r\n    --version llava_llama_2 \\\r\n    --data_path ./dataset/train.json \\\r\n    --image_folder ./dataset/images \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./models/llava-pretrain-llama-2-13b-chat/mm_projector.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-llama2chat13b-tune_projector-finetune_lora \\\r\n    --num_train_epochs 10 \\\r\n    --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 8 \\\r\n    --gradient_accumulation_steps 2 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.1 \\\r\n    --lr_scheduler_type \"linear\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to none\r\n```\n</Comment>\n<Comment by haotian-liu at 2023-10-26T20:49:41Z>\nHi, these all look good to me. It is strange that there is such error printed. We have just released the LoRA ckpt/script for LLaVA-1.5: https://github.com/haotian-liu/LLaVA/releases/tag/v1.1.3\r\n\r\nMay be you can try with the new code/sample script?\n</Comment>\n<Comment by basteran at 2023-10-27T12:56:53Z>\nThank you @haotian-liu , I will try it as soon as possible and I will let you know ;)\n</Comment>\n<Comment by amanysalahfattouh at 2024-04-02T12:41:55Z>\n@basteran \r\n@FHL1998 \r\nMet the same issue here, may I ask if you have solved it?\n</Comment>\n<Comment by basteran at 2024-04-02T12:52:27Z>\nHi @amanysalahfattouh , I didn't solve it. I just didn't load the projector weights.. and it's disappointing that after so many months, no one could solve it or provide a solution!\n</Comment>\n<Comment by adymaharana at 2024-04-17T06:23:28Z>\n@basteran @amanysalahfattouh @haotian-liu , have you tried using the flag `--pretrain_mm_mlp_adapter` with the path set to `non_lora_trainables.bin` of your finetuned model? I have been facing the same issue as discussed here in a different setting i.e., fine-tuning an instruction-tuned model trained on a custom dataset. I found this fix to work for my situation with a few more modifications. Specifically, I had to change [this](https://github.com/haotian-liu/LLaVA/blob/4e2277a060da264c4f21b364c867cc622c945874/llava/model/llava_arch.py#L34) line:\r\n\r\n`        if hasattr(config, \"mm_vision_tower\"):`\r\n\r\nto \r\n\r\n`        if False and hasattr(config, \"mm_vision_tower\"):`\r\n\r\nto prevent `self.mm_projector` from being initialized with the rest of the model. I faced size mismatch errors without the above modification when running [this](https://github.com/haotian-liu/LLaVA/blob/4e2277a060da264c4f21b364c867cc622c945874/llava/model/llava_arch.py#L97) line:\r\n\r\n`            self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'))`\r\n\r\nThe size mismatch *might* be related to an [issue](https://github.com/open-mmlab/mmengine/issues/1273) with loading weights when zero3 is enabled.\n</Comment>\n<Comment by basteran at 2024-04-17T10:59:51Z>\n> @basteran @amanysalahfattouh @haotian-liu , have you tried using the flag `--pretrain_mm_mlp_adapter` with the path set to `non_lora_trainables.bin` of your finetuned model? I have been facing the same issue as discussed here in a different setting i.e., fine-tuning an instruction-tuned model trained on a custom dataset. I found this fix to work for my situation with a few more modifications. Specifically, I had to change [this](https://github.com/haotian-liu/LLaVA/blob/4e2277a060da264c4f21b364c867cc622c945874/llava/model/llava_arch.py#L34) line:\r\n> \r\n> ` if hasattr(config, \"mm_vision_tower\"):`\r\n> \r\n> to\r\n> \r\n> ` if False and hasattr(config, \"mm_vision_tower\"):`\r\n> \r\n> to prevent `self.mm_projector` from being initialized with the rest of the model. I faced size mismatch errors without the above modification when running [this](https://github.com/haotian-liu/LLaVA/blob/4e2277a060da264c4f21b364c867cc622c945874/llava/model/llava_arch.py#L97) line:\r\n> \r\n> ` self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'))`\r\n> \r\n> The size mismatch _might_ be related to an [issue](https://github.com/open-mmlab/mmengine/issues/1273) with loading weights when zero3 is enabled.\r\n\r\nThank you but my problem is that after fine-tuning I see no \"mm_projector\" file in the checkpoints, so I cannot load the for inference later..\n</Comment>\n<Comment by dongwhfdyer at 2024-04-25T06:38:50Z>\n> @basteran @amanysalahfattouh @haotian-liu , have you tried using the flag `--pretrain_mm_mlp_adapter` with the path set to `non_lora_trainables.bin` of your finetuned model? I have been facing the same issue as discussed here in a different setting i.e., fine-tuning an instruction-tuned model trained on a custom dataset. I found this fix to work for my situation with a few more modifications. Specifically, I had to change [this](https://github.com/haotian-liu/LLaVA/blob/4e2277a060da264c4f21b364c867cc622c945874/llava/model/llava_arch.py#L34) line:\r\n> \r\n> ` if hasattr(config, \"mm_vision_tower\"):`\r\n> \r\n> to\r\n> \r\n> ` if False and hasattr(config, \"mm_vision_tower\"):`\r\n> \r\n> to prevent `self.mm_projector` from being initialized with the rest of the model. I faced size mismatch errors without the above modification when running [this](https://github.com/haotian-liu/LLaVA/blob/4e2277a060da264c4f21b364c867cc622c945874/llava/model/llava_arch.py#L97) line:\r\n> \r\n> ` self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'))`\r\n> \r\n> The size mismatch _might_ be related to an [issue](https://github.com/open-mmlab/mmengine/issues/1273) with loading weights when zero3 is enabled.\r\nI solved it by using the zero2 instead. Thank you very much! I am happy to help anyone who encounter this issue.\n</Comment>\n<Comment by amandalmia14 at 2024-06-12T20:08:04Z>\n@basteran Were you able to get the solution for the `mm_projector`. I am also facing the same. \r\n\r\nWhen I load the model for evaluation, it throws the below error, `FileNotFoundError: [Errno 2] No such file or directory: 'train_cot_llava_llama2/mm_projector.bin'`\n</Comment>\n<Comment by YUECHE77 at 2024-11-02T20:26:17Z>\n> > @basteran @amanysalahfattouh @haotian-liu , have you tried using the flag `--pretrain_mm_mlp_adapter` with the path set to `non_lora_trainables.bin` of your finetuned model? I have been facing the same issue as discussed here in a different setting i.e., fine-tuning an instruction-tuned model trained on a custom dataset. I found this fix to work for my situation with a few more modifications. Specifically, I had to change [this](https://github.com/haotian-liu/LLaVA/blob/4e2277a060da264c4f21b364c867cc622c945874/llava/model/llava_arch.py#L34) line:\r\n> > ` if hasattr(config, \"mm_vision_tower\"):`\r\n> > to\r\n> > ` if False and hasattr(config, \"mm_vision_tower\"):`\r\n> > to prevent `self.mm_projector` from being initialized with the rest of the model. I faced size mismatch errors without the above modification when running [this](https://github.com/haotian-liu/LLaVA/blob/4e2277a060da264c4f21b364c867cc622c945874/llava/model/llava_arch.py#L97) line:\r\n> > ` self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'))`\r\n> > The size mismatch _might_ be related to an [issue](https://github.com/open-mmlab/mmengine/issues/1273) with loading weights when zero3 is enabled.\r\n> \r\n> Thank you but my problem is that after fine-tuning I see no \"mm_projector\" file in the checkpoints, so I cannot load the for inference later..\r\n\r\nHi,\r\n\r\nDid you run \"merge_lora_weights.py\" before the evaluation? I also don't have the \"mm_projector\" file in my output folder...\n</Comment>\n<Comment by dongwhfdyer at 2024-12-19T02:17:26Z>\nyes. merge first.\n</Comment>\n<Comment by yinyuanzhang at 2025-04-05T14:18:57Z>\n> Hi [@amanysalahfattouh](https://github.com/amanysalahfattouh) , I didn't solve it. I just didn't load the projector weights.. and it's disappointing that after so many months, no one could solve it or provide a solution!\n\nI met the same problem with you.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 473,
    "state": "open",
    "created_by": "sasakits",
    "created_at": "2023-10-04T17:51:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/473</URL>\n\n<TITLE>Killing process without Traceback in finetuning 7B-MPT model</TITLE>\n\n<BODY>Hi. I am trying to finetuning and/or LoRA with 7B-MPT model.\r\nAnd I am facing the issue of Killing process without Trace back message as below.\r\n\r\n- Command\r\n> bash scripts/finetune.sh\r\n\r\n- Output\r\n> [2023-10-05 11:33:46,335] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 516380\r\n> [2023-10-05 11:33:46,633] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 516381\r\n> [2023-10-05 11:33:46,633] [ERROR] [launch.py:321:sigkill_handler] ['/usr/bin/python3.8', '-u', 'llava/train/train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', './checkpoints/llava-7b-mpt/', \r\n> ...\r\n> loader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = -9\r\n> \r\n\r\nI did not see any Trace back message, but after debugging, I know that the process is not progressing in the bolded part of the current function and that the process is KILLED.\r\n\r\n\r\n**super(LlavaMPTModel, self).__init__(config)**\r\n\r\n```\r\nclass LlavaMPTModel(LlavaMetaModel, MPTModel):\r\n    config_class = LlavaMPTConfig\r\n    def __init__(self, config: MPTConfig):\r\n        config.hidden_size = config.d_model\r\n        super(LlavaMPTModel, self).__init__(config)\r\n```\r\n\r\nThe execution environment is as follows\r\n - ubuntu 20.04\r\n - GPU A6000*2\r\n - torch==2.0.1+cu117\r\n - transformers==4.31.0\r\n\r\nThank you in advance</BODY>\n\n<COMMENTS>\n<Comment by Rickylht at 2023-10-11T09:42:19Z>\nI met the same problem. Did you solve it? They say it is because out of RAM.\n</Comment>\n<Comment by crazysal at 2023-12-12T22:51:05Z>\nFacing the same with class LlavaMetaModel gets stuck at super() init of parent class \r\n\r\n```\r\n\r\nclass LlavaMetaModel:\r\n\r\n    def __init__(self, config):\r\n        print('IN INITI LlavaMetaModel ######')\r\n        super(LlavaMetaModel, self).__init__(config)\r\n        print(\"SUPER INIT CONFIG DONE\")\r\n        \r\n```\r\n        \r\n I get the first print but not the second. But there is no memory footprint 800g free available + 50g swap , nothing else running \r\n Seen similar behaviour in torch distributed settings with nccl backend which gets solved by instead using gloo, but theses guys are further using deepseed  : \r\n \r\n \r\n \r\non running the following directly: \r\n\r\n> mpirun -np 1 deepspeed --include=localhost:2,4 llava/train/train_mem.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero3.json --model_name_or_path liuhaotian/llava-v1.5-13b --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b-task-lora --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy \"no\" --save_strategy \"steps\" --save_steps 50000 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type \"cosine\" --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb\r\n\r\n\r\n\r\nThats just the run command from the LLava/scripts/v1_5/finetune_task_lora.sh \r\nI added mpirun to make deepseed use mpi backend not sure it is using it though. \r\n\r\nI do get warning : \r\n\r\n> \r\n> 2023-12-12 17:37:52,175] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n> [2023-12-12 17:37:52,175] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n> [2023-12-12 17:37:52,175] [INFO] [comm.py:594:init_distributed] cdb=None\r\n> [2023-12-12 17:37:52,175] [INFO] [comm.py:594:init_distributed] cdb=None\r\n> [2023-12-12 17:37:52,175] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n> \r\n> \r\n \r\n I have hunch it is 'coz the distributed gpus is not able to communicate. After default timeout the proc gets killed. \r\n With single gpu it goes OOM \r\n \r\n gpu :  RTX A5000\n</Comment>\n<Comment by ybsu at 2024-06-03T10:55:23Z>\n> I met the same problem. Did you solve it? They say it is because out of RAM.\r\n\r\nsame problem, have you solved it sir? Thanks.\n</Comment>\n<Comment by ybsu at 2024-06-04T07:02:47Z>\n> > I met the same problem. Did you solve it? They say it is because out of RAM.\r\n> \r\n> same problem, have you solved it sir? Thanks.\r\n\r\nI chaned a docker environment and I have been running the code successfully. I do not exactly know why the last environment did not work, maybe it was a problem with itself.\n</Comment>\n<Comment by heshandevaka at 2025-02-25T21:59:09Z>\n> Hi. I am trying to finetuning and/or LoRA with 7B-MPT model. And I am facing the issue of Killing process without Trace back message as below.\n> \n> * Command\n> \n> > bash scripts/finetune.sh\n> \n> * Output\n> \n> > [2023-10-05 11:33:46,335] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 516380\n> > [2023-10-05 11:33:46,633] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 516381\n> > [2023-10-05 11:33:46,633] [ERROR] [launch.py:321:sigkill_handler] ['/usr/bin/python3.8', '-u', 'llava/train/train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', './checkpoints/llava-7b-mpt/',\n> > ...\n> > loader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = -9\n> \n> I did not see any Trace back message, but after debugging, I know that the process is not progressing in the bolded part of the current function and that the process is KILLED.\n> \n> **super(LlavaMPTModel, self).init(config)**\n> \n> ```\n> class LlavaMPTModel(LlavaMetaModel, MPTModel):\n>     config_class = LlavaMPTConfig\n>     def __init__(self, config: MPTConfig):\n>         config.hidden_size = config.d_model\n>         super(LlavaMPTModel, self).__init__(config)\n> ```\n> \n> The execution environment is as follows\n> \n> * ubuntu 20.04\n> * GPU A6000*2\n> * torch==2.0.1+cu117\n> * transformers==4.31.0\n> \n> Thank you in advance\n\nI had a similar issue with running `pretrain.sh` with `vicuna-13b-v1.5`. I added `model.to('cuda')` right after model initialization (in `llava/train/train.py`) as suggested in an earlier warning \n\n>You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`. \n\n and it fixed the issue. However, this fix was not needed for the 7b model in my case. But I am not sure exactly why that happens.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 472,
    "state": "open",
    "created_by": "MrCsabaToth",
    "created_at": "2023-10-03T22:05:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/472</URL>\n\n<TITLE>[Usage] Parametrization of a deployed model from HuggingFace</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: I deploy the https://huggingface.co/liuhaotian/llava-llama-2-13b-chat-lightning-preview/discussions model and would like to call the model via an API remotely and not using any Gradio GUI or such.\r\n\r\nCommand:\r\n```\r\nendpoint = 'huggingface-pytorch-inference-2023-yyyyyyyyyyyyyyyy'\r\nENCODING = \"utf-8\"\r\nIMAGE_NAME = \"eiffel_tower_336.jpg\"\r\n\r\npayload = {\r\n    \"inputs\": \"Describe the content of the image in great detail \",\r\n}\r\nwith open(IMAGE_NAME, 'rb') as f:\r\n    byte_content = f.read()\r\n    base64_bytes = b64encode(byte_content)\r\n    base64_string = base64_bytes.decode(ENCODING)\r\n\r\npredictor = Predictor(endpoint)\r\ninference_response = predictor.predict(data=payload)\r\nprint (inference_response)\r\n```\r\n\r\nLog:\r\n```\r\nParamValidationError: Parameter validation failed: Invalid type for parameter Body, value: {'inputs': 'Describe the content of the image in great detail '}, type: <class 'dict'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object\r\n```</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 471,
    "state": "open",
    "created_by": "LouieBHLu",
    "created_at": "2023-10-02T17:26:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/471</URL>\n\n<TITLE>Multi-image Comparison [Feature request]</TITLE>\n\n<BODY>### feature\n\nHi，\r\n\r\nIs there anyway LLaVA could do multi-image comparison as the latest GPT-4 update could do. That's we input several image at a time and use a prompt like \"What's the most significant different in these n pictures?\".  Is it possible for LLaVA to do this?\r\n\r\nThank you!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 470,
    "state": "closed",
    "created_by": "fishneck",
    "created_at": "2023-09-29T22:56:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/470</URL>\n\n<TITLE>Projector weights missing 'model.embed_tokens.weight'</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nI want to use provided scripts to finetune on my own dataset and run \r\n```\r\nbash finetune_lora.sh\r\n```\r\nwith \r\n```\r\n--pretrain_mm_mlp_adapter ./checkpoints/liuhaotian/llava-pretrain-vicuna-7b-v1.3/mm_projector.bin\r\n```\r\nIt seems the provided projector weights is missing a key\r\n\r\nLog: \r\n```\r\n  File \".../LLaVA/llava/model/llava_arch.py\", line 231, in initialize_vision_tokenizer\r\n    embed_tokens_weight = mm_projector_weights['model.embed_tokens.weight']\r\nKeyError: 'model.embed_tokens.weight'\r\n```\r\nAny chance to correct for this issue?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-05T00:51:09Z>\nYou should set `--mm_use_im_start_end False  --mm_use_im_patch_token False` following the scripts here:\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/scripts/finetune_lora.sh#L27\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 469,
    "state": "closed",
    "created_by": "yigu1008",
    "created_at": "2023-09-29T14:24:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/469</URL>\n\n<TITLE>KeyError: \"llava\"</TITLE>\n\n<BODY>### Question\n\nHi, when I was trying to load llava weights using scripts\r\n \r\n```\r\nfrom transformers import AutoModelForCausalLM\r\nmodel = AutoModelForCausalLM.from_pretrained(\"liuhaotian/llava-llama-2-7b-chat-lightning-lora-preview\")\r\n```\r\n \r\nI got error:\r\n \r\n**File \"/home/ygu/miniconda3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 434, in from_pretrained\r\n    config, kwargs = AutoConfig.from_pretrained(\r\n  File \"/home/ygu/miniconda3/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 829, in from_pretrained\r\n    config_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\r\n  File \"/home/ygu/miniconda3/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 536, in __getitem__\r\n    raise KeyError(key)\r\nKeyError: 'llava'**\r\n \r\nIt seems like llava is not configured in the transformers library. I attempted transformers 4.33.3(latest from pip) and 4.25.1 but both got this error. For python version, both 3.10 and 3.8 were tried but neither worked out.  Could you please help me with this? Thanks!</BODY>\n\n<COMMENTS>\n<Comment by dddraxxx at 2023-09-30T03:33:32Z>\nsame here\n</Comment>\n<Comment by willkhoza at 2023-10-07T15:23:51Z>\nGetting this error as well\n</Comment>\n<Comment by armandParser at 2023-10-07T16:26:57Z>\nSame here, usually updating transformers from source with \"pip install git+https://github.com/huggingface/transformers.git\" works but not here\n</Comment>\n<Comment by fei-chang at 2023-10-08T08:46:44Z>\nIt seems this issue has been raised before and can be solved [here](https://github.com/haotian-liu/LLaVA/issues/386#issuecomment-1689329746).\n</Comment>\n<Comment by SOISVerieth at 2023-10-09T08:44:31Z>\nAm I right at doing things?\r\nI copied code from the post \r\n\r\n`from transformers import AutoConfig, LlamaConfig \r\nfrom llava.model.language_model.llava_llama import LlavaLlamaForCausalLM\r\nclass LlavaConfig(LlamaConfig):\r\n    model_type = \"llava\"\r\nAutoConfig.register(\"llava\", LlavaConfig)\r\nmodel = LlavaLlamaForCausalLM.from_pretrained(\"liuhaotian/llava-llama-2-13b-chat-lightning-preview\")`\r\n\r\nCreated .py file and run it. Is this all i've had to do to fix problem?\r\nI'm asking because if that's all it take's than i still got this problem. \r\nIf not- can someone provide me with extra steps?\n</Comment>\n<Comment by willkhoza at 2023-10-09T08:53:02Z>\nYes this should work. \r\n\r\nBut you must first follow the installation instructions for the LlaVA project. From the top of my head, download the repository, update the repository to be in the most recent state, remove the transformers package and and reinstall it in its most recent state. Then place your Python script or notebook with the code snippet you provided above in the same directory as the downloaded and updated LlaVA repository. After running that snippet, it will add llava key to your transformers package and at that point you can use the serve/cli.py to make predictions. You will require some 40 gb RAM or VRAM for the 7b model and you will require 80 gb for the 13b model.\n</Comment>\n<Comment by SOISVerieth at 2023-10-09T09:52:57Z>\nThanks a LOT!\n</Comment>\n<Comment by haotian-liu at 2023-11-05T00:10:31Z>\nHi, please use the load_pretrained_model function from our repo until we are officially integraed into Huggingface Transformers.\r\n\r\nhttps://github.com/haotian-liu/LLaVA#quick-start-with-huggingface\n</Comment>\n<Comment by jeeyung at 2024-05-07T23:28:09Z>\nWhen is it going to be integrated officially into Transformers?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 468,
    "state": "open",
    "created_by": "Deaddawn",
    "created_at": "2023-09-29T12:20:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/468</URL>\n\n<TITLE>[Question] How do i train on multiple gpus?</TITLE>\n\n<BODY>### Question\n\nHi, there. I'm wondering how do i pretrain on multiple gpus(i have about 4*3090ti, each 20GB). I'm using the scriprts\r\n\r\n **python llava/train/train_mem.py     --model_name_or_path ./ckpt/vicuna-7b/     --version v1     --data_path ./ckpt/cc3m_595k/chat.json     --image_folder ./ckpt/cc3m_595k/images     --vision_tower ./dataroot/models/openai/clip-vit-large-patch14     --tune_mm_mlp_adapter True     --mm_vision_select_layer -2     --mm_use_im_start_end False     --mm_use_im_patch_token False     --bf16 True     --output_dir ./ckpt/llava-7b-pretrain     --num_train_epochs 1     --per_device_train_batch_size 4     --per_device_eval_batch_size 4     --gradient_accumulation_steps 32     --evaluation_strategy \"no\"     --save_strategy \"steps\"     --save_steps 2400     --save_total_limit 1     --learning_rate 2e-3     --weight_decay 0.     --warmup_ratio 0.03     --lr_scheduler_type \"cosine\"     --logging_steps 1     --tf32 True     --model_max_length 2048     --gradient_checkpointing True     --lazy_preprocess True     --report_to wandb**\r\n\r\nget out of mem error\r\n\r\n**torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 4; 23.69 GiB total capacity; 22.86 GiB already allocated; 106.94 MiB free; 22.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC**</BODY>\n\n<COMMENTS>\n<Comment by TesiLin at 2023-10-30T09:39:27Z>\nHave you solved your problem? I meet similar problem.\n</Comment>\n<Comment by haotian-liu at 2023-10-30T16:47:54Z>\nHi, please use our pretrain script with deepspeed to reduce the memory footprint: https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/pretrain.sh\r\n\r\ntry (1) reduce the batch size + increase the grad_accu accordingly; (2) zero3 if you encounter OOM issue. 4x 3090 should be fine for pretraining. And for finetuning you can try lora: https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_lora.sh\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 467,
    "state": "closed",
    "created_by": "rabiulcste",
    "created_at": "2023-09-29T01:45:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/467</URL>\n\n<TITLE>[Question] Why batch of `input_ids` becomes  3 times bigger in the the `forward()` of LlavaLlamaForCausalLM?</TITLE>\n\n<BODY>### Question\n\nI was trying to understand why batch inference does not improve inference speed. I was looking at the code here  https://github.com/haotian-liu/LLaVA/blob/a4269fbf014af3cab1f1d172914493fae8b74820/llava/model/llava_arch.py#L86\r\n\r\nHere is the input that I feed to `model.generate()`.\r\n```\r\ninput_ids.shape: torch.Size([4, 62])\r\nimage_tensor.shape: torch.Size([4, 3, 336, 336])\r\n```\r\nI logged output in `LlavaMetaForCausalLM.prepare_inputs_labels_for_multimodal()`\r\n```\r\nprepare:input_ids torch.Size([12, 62])\r\nprepare:attention_mask torch.Size([12, 637])\r\nprepare:inputs_embeds torch.Size([12, 637, 5120])\r\n```\r\nAlso, `LlavaLlamaForCausalLM.forward()`\r\n```\r\nforward:attention_mask torch.Size([12, 637])\r\nforward:inputs_embeds torch.Size([12, 637, 5120])\r\n```\r\n\r\nI don't understand why `batch_size` is 3 times larger.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-05T05:05:52Z>\nHi, thank you, and I am trying to work on an efficient batch inference. Closing this and consolidating the discussion to #754. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 466,
    "state": "open",
    "created_by": "liruiw",
    "created_at": "2023-09-28T19:12:10Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/466</URL>\n\n<TITLE>[Feature request]  Support for Code-Llama</TITLE>\n\n<BODY>### feature\n\nThanks for the great work! I would be very interested in seeing finetuning visual prompting model for code-llama.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 465,
    "state": "open",
    "created_by": "WilTay1",
    "created_at": "2023-09-28T06:41:27Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/465</URL>\n\n<TITLE>[Feature request] Great work! Can I know if llava-v1.5-alpha model available? Thanks.</TITLE>\n\n<BODY>### feature\n\n<img width=\"287\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/133484736/63fb7e4d-c021-49ef-9915-2e4b3d8c6b49\"></BODY>\n\n<COMMENTS>\n<Comment by dusty-nv at 2023-09-28T17:55:30Z>\n+1  this seems to produce different/better results from `LLaVA-RLHF-13b-v1.5-336`\n</Comment>\n<Comment by wangjunchi at 2023-09-29T08:18:09Z>\n+1 That will help a lot.\n</Comment>\n<Comment by luca-medeiros at 2023-10-04T01:42:03Z>\nBeen searching for it aswell\n</Comment>\n<Comment by haotian-liu at 2023-10-04T05:10:43Z>\n@dusty-nv @wangjunchi @luca-medeiros @WilTay1 @abdul \r\n\r\nThank you for your interest in LLaVA! We are preparing for a release this week, and we are working on the final evaluation. Stay tuned!\n</Comment>\n<Comment by luca-medeiros at 2023-10-06T05:20:58Z>\nPartially released and working great!\r\nhttps://github.com/haotian-liu/LLaVA/commit/44e0562f9497fb79f042427307472a87d266d90a\r\n\r\nAppreciate the work @haotian-liu\n</Comment>\n<Comment by haotian-liu at 2023-10-08T14:15:55Z>\n@luca-medeiros Thanks. \r\n\r\n_We will release the training scripts, data, and evaluation scripts on benchmarks in the coming week._\n</Comment>\n<Comment by haotian-liu at 2023-10-12T23:27:04Z>\nFully released yesterday:\r\n\r\nThe training data and scripts of LLaVA-1.5 are released [here](https://github.com/haotian-liu/LLaVA#train), and evaluation scripts are released [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md)!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 464,
    "state": "open",
    "created_by": "1106301825",
    "created_at": "2023-09-28T01:27:30Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/464</URL>\n\n<TITLE>NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.</TITLE>\n\n<BODY>| ERROR | stderr | RuntimeError: CUDA error: device-side assert triggered\r\n | ERROR | stderr | CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n | ERROR | stderr | For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\n | ERROR | stderr | Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.</BODY>\n\n<COMMENTS>\n<Comment by TikaToka at 2023-09-30T13:12:38Z>\nHave same issue while handling #417 \r\nmy error's looks like this\r\n\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLarge\r\nIndex: block: [232,0,0], thread: [31,0,0] Assertion `srcIndex <\r\n srcSelectDimSize` failed.\r\n'''\r\n2023-09-30 13:06:31 | ERROR | stderr | Traceback (most recent c\r\nall last):\r\n2023-09-30 13:06:31 | ERROR | stderr |   File \"/root/anaconda3/\r\nenvs/llava/lib/python3.10/threading.py\", line 1016, in _bootstr\r\nap_inner\r\n2023-09-30 13:06:31 | ERROR | stderr |     self.run()\r\n2023-09-30 13:06:31 | ERROR | stderr |   File \"/root/anaconda3/\r\nenvs/llava/lib/python3.10/threading.py\", line 953, in run\r\n2023-09-30 13:06:31 | ERROR | stderr |     self._target(*self._\r\nargs, **self._kwargs)\r\n2023-09-30 13:06:31 | ERROR | stderr |   File \"/root/anaconda3/\r\nenvs/llava/lib/python3.10/site-packages/torch/utils/_contextlib\r\n.py\", line 115, in decorate_context\r\n2023-09-30 13:06:31 | ERROR | stderr |     return func(*args, *\r\n*kwargs)\r\n2023-09-30 13:06:31 | ERROR | stderr |   File \"/root/anaconda3/\r\nenvs/llava/lib/python3.10/site-packages/transformers/generation\r\n/utils.py\", line 1588, in generate\r\n2023-09-30 13:06:31 | ERROR | stderr |     return self.sample(\r\n2023-09-30 13:06:31 | ERROR | stderr |   File \"/root/anaconda3/\r\nenvs/llava/lib/python3.10/site-packages/transformers/generation\r\n/utils.py\", line 2642, in sample\r\n2023-09-30 13:06:31 | ERROR | stderr |     outputs = self(\r\n2023-09-30 13:06:31 | ERROR | stderr |   File \"/root/anaconda3/\r\nenvs/llava/lib/python3.10/site-packages/torch/nn/modules/module\r\n.py\", line 1501, in _call_impl\r\n2023-09-30 13:06:31 | ERROR | stderr |     return forward_call(\r\n*args, **kwargs)\r\n2023-09-30 13:06:31 | ERROR | stderr |   File \"/root/anaconda3/\r\nenvs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", l\r\nine 165, in new_forward\r\n2023-09-30 13:06:31 | ERROR | stderr |     output = old_forward\r\n(*args, **kwargs)\r\n2023-09-30 13:06:31 | ERROR | stderr |   File \"/home/vln_worksp\r\nace/LLaVA/llava/model/language_model/llava_llama.py\", line 78, \r\nin forward\r\n2023-09-30 13:06:31 | ERROR | stderr |     outputs = self.model\r\n(\r\n2023-09-30 13:06:31 | ERROR | stderr |   File \"/root/anaconda3/\r\nenvs/llava/lib/python3.10/site-packages/torch/nn/modules/module\r\n.py\", line 1501, in _call_impl\r\n2023-09-30 13:06:31 | ERROR | stderr |     return forward_call(\r\n*args, **kwargs)\r\n2023-09-30 13:06:31 | ERROR | stderr |   File \"/root/anaconda3/\r\nenvs/llava/lib/python3.10/site-packages/transformers/models/lla\r\nma/modeling_llama.py\", line 646, in forward\r\n2023-09-30 13:06:31 | ERROR | stderr |     inputs_embeds = self\r\n.embed_tokens(input_ids)\r\n2023-09-30 13:06:31 | ERROR | stderr |   File \"/root/anaconda3/\r\nenvs/llava/lib/python3.10/site-packages/torch/nn/modules/module\r\n.py\", line 1501, in _call_impl\r\n2023-09-30 13:06:31 | ERROR | stderr |     return forward_call(\r\n*args, **kwargs)\r\n2023-09-30 13:06:31 | ERROR | stderr |   File \"/root/anaconda3/\r\nenvs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", l\r\nine 165, in new_forward\r\n2023-09-30 13:06:31 | ERROR | stderr |     output = old_forward\r\n(*args, **kwargs)\r\n2023-09-30 13:06:31 | ERROR | stderr |   File \"/root/anaconda3/\r\nenvs/llava/lib/python3.10/site-packages/torch/nn/modules/sparse\r\n.py\", line 162, in forward\r\n2023-09-30 13:06:31 | ERROR | stderr |     return F.embedding(\r\n2023-09-30 13:06:31 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/functional.py\", line 2210, in embedding\r\n2023-09-30 13:06:31 | ERROR | stderr |     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\n2023-09-30 13:06:31 | ERROR | stderr | RuntimeError: CUDA error: device-side assert triggered\r\n2023-09-30 13:06:31 | ERROR | stderr | Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n'''\n</Comment>\n<Comment by zephirusgit at 2023-10-28T01:47:22Z>\nI have the same problem, but there is something to analyze, for example, I first tried to run them on Google Colab, it happened to me with the first colibs that everyone gave me that mistake,\r\nThen with the Llava_7b_8bit_colab.ipynb version\r\nThere I could make it work last night, while downloading the models on my PC,\r\nalthough at first I could not walk until I saw another git\r\nhttps://github.com/natlamir/LLaVA-Windows\r\nFollowing those steps I could see what they should initialize in 3 different Powershells, and thus achieve that at first, lift the 3 servers, but, oh surprise I told me the same error, that some gave me some google colabs, \r\n(Another thing is that with the send he created in the other git, he could not walk, but if he added some things that he mentioned there, in the github of Haotian-liu, then there he worked in parts,\r\nNow I have that problem, which I imagine, is because when I lift the 3rd server, which I see that the complete vram occupies me, perhaps for some reason does not connect to the other 2 servers?)\r\nomg now are working !\r\nWell as I wrote this I would regenerate to see if any error came out and started writing,\r\nMaybe you have to give a good start time.\r\nNow I am using a Ryzen 5 3600X, with 48GB of RAM, and an RTX 2060 with 12GB.\n</Comment>\n<Comment by zephirusgit at 2023-10-28T01:50:10Z>\n![anda](https://github.com/haotian-liu/LLaVA/assets/20031912/7be9c987-c458-45a4-a9f6-b12eccf8bc99)\n</Comment>\n<Comment by zephirusgit at 2023-10-28T01:50:53Z>\nohh yes now is working!, I'm going to share the pip list to check your versions\n</Comment>\n<Comment by zephirusgit at 2023-10-28T01:54:19Z>\nPackage                   Version      Editable project location\r\n------------------------- ------------ -------------------------\r\naccelerate                0.21.0\r\naiofiles                  23.2.1\r\naiohttp                   3.8.6\r\naiosignal                 1.3.1\r\naltair                    5.1.2\r\nanyio                     3.7.1\r\nappdirs                   1.4.4\r\nasync-timeout             4.0.3\r\nattrs                     23.1.0\r\nbitsandbytes              0.37.5\r\nbitsandbytes-cuda111      0.26.0.post2\r\nBrotli                    1.0.9\r\ncertifi                   2023.7.22\r\ncffi                      1.15.1\r\nchardet                   5.2.0\r\ncharset-normalizer        2.0.4\r\nclick                     8.1.7\r\ncolorama                  0.4.6\r\ncontourpy                 1.1.1\r\ncryptography              41.0.3\r\ncycler                    0.12.1\r\ndocker-pycreds            0.4.0\r\neinops                    0.6.1\r\neinops-exts               0.0.4\r\nexceptiongroup            1.1.3\r\nfastapi                   0.104.0\r\nffmpy                     0.3.1\r\nfilelock                  3.12.4\r\nfonttools                 4.43.1\r\nfrozenlist                1.4.0\r\nfsspec                    2023.10.0\r\ngitdb                     4.0.11\r\nGitPython                 3.1.40\r\ngradio                    3.35.2\r\ngradio_client             0.2.9\r\nh11                       0.14.0\r\nhttpcore                  0.17.3\r\nhttpx                     0.24.0\r\nhuggingface-hub           0.18.0\r\nidna                      3.4\r\nJinja2                    3.1.2\r\njoblib                    1.3.2\r\njsonschema                4.19.1\r\njsonschema-specifications 2023.7.1\r\nkiwisolver                1.4.5\r\nlinkify-it-py             2.0.2\r\nllava                     1.1.3        H:\\ia\\llava\r\nmarkdown-it-py            2.2.0\r\nmarkdown2                 2.4.10\r\nMarkupSafe                2.1.1\r\nmatplotlib                3.8.0\r\nmdit-py-plugins           0.3.3\r\nmdurl                     0.1.2\r\nmkl-fft                   1.3.8\r\nmkl-random                1.2.4\r\nmkl-service               2.4.0\r\nmpmath                    1.3.0\r\nmultidict                 6.0.4\r\nnetworkx                  3.1\r\nninja                     1.11.1.1\r\nnumpy                     1.26.0\r\norjson                    3.9.10\r\npackaging                 23.2\r\npandas                    2.1.2\r\npathtools                 0.1.2\r\npeft                      0.4.0\r\nPillow                    10.0.1\r\npip                       23.3\r\nprotobuf                  4.24.4\r\npsutil                    5.9.6\r\npycparser                 2.21\r\npydantic                  1.10.9\r\npydub                     0.25.1\r\nPygments                  2.16.1\r\npyOpenSSL                 23.2.0\r\npyparsing                 3.1.1\r\nPySocks                   1.7.1\r\npython-dateutil           2.8.2\r\npython-multipart          0.0.6\r\npytz                      2023.3.post1\r\nPyYAML                    6.0.1\r\nreferencing               0.30.2\r\nregex                     2023.10.3\r\nrequests                  2.31.0\r\nrpds-py                   0.10.6\r\nsafetensors               0.4.0\r\nscikit-learn              1.2.2\r\nscipy                     1.11.3\r\nsemantic-version          2.10.0\r\nsentencepiece             0.1.99\r\nsentry-sdk                1.32.0\r\nsetproctitle              1.3.3\r\nsetuptools                68.0.0\r\nshortuuid                 1.0.11\r\nsix                       1.16.0\r\nsmmap                     5.0.1\r\nsniffio                   1.3.0\r\nstarlette                 0.27.0\r\nsvgwrite                  1.4.3\r\nsympy                     1.11.1\r\nthreadpoolctl             3.2.0\r\ntimm                      0.6.13\r\ntokenizers                0.13.3\r\ntoolz                     0.12.0\r\ntorch                     2.0.1\r\ntorchaudio                2.1.0\r\ntorchvision               0.15.2\r\ntqdm                      4.66.1\r\ntransformers              4.31.0\r\ntyping_extensions         4.8.0\r\ntzdata                    2023.3\r\nuc-micro-py               1.0.2\r\nurllib3                   1.26.18\r\nuvicorn                   0.23.2\r\nwandb                     0.15.12\r\nwavedrom                  2.0.3.post3\r\nwebsockets                12.0\r\nwheel                     0.41.2\r\nwin-inet-pton             1.1.0\r\nyarl                      1.9.2\r\nyoutube-dl                2021.12.17\n</Comment>\n<Comment by zephirusgit at 2023-10-28T01:55:07Z>\n![test](https://github.com/haotian-liu/LLaVA/assets/20031912/1a47adfb-720c-482c-adc1-8d37761ffce9)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 463,
    "state": "closed",
    "created_by": "TheMikeMerrill",
    "created_at": "2023-09-27T16:37:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/463</URL>\n\n<TITLE>[Usage] mm_projector weights go to `inf`</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nHey there - huge fan of your work. \r\n\r\nI'm trying to integrate LLaVa into a training framework I've been running into the same issue again and again. When I try to train LLaVa outside of your framework (i.e. by loading it and passing it through my own training loop) something breaks and I get `nan` loss. \r\n\r\nI've managed to isolate the issue to the `mm_projector` weights, which go to `inf` after the first training step. This doesn't seem to be an issue with exploding gradients since the grad norm stays low (<10) and the issue persists even with tiny learning rates (<1e-5). \r\n\r\nCommand:\r\n\r\n`debug.py`:\r\n\r\n``` python\r\nfrom llava.model.builder import load_pretrained_model\r\nimport numpy as np\r\nimport torch\r\n\r\n# Load the model\r\ntokenizer, model, image_processor, context_len  = load_pretrained_model(\"liuhaotian/llava-llama-2-7b-chat-lightning-lora-preview\",\r\n                                                                            model_base = \"meta-llama/Llama-2-7b-chat-hf\", \r\n                                                                            model_name = \"llava-llama-2-7b-chat-lightning-lora-preview\")\r\n# Unfreeze the mm projector\r\nfor p in model.get_model().mm_projector.parameters():\r\n    p.requires_grad = True\r\n\r\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\r\nfor i in range(10):\r\n    \r\n    optimizer.zero_grad()\r\n\r\n    # Dummmy inputs\r\n    imtensors = [np.random.rand(256,256,3)]\r\n    input_images = image_processor(imtensors, return_tensors=\"pt\",\r\n                                            ).pixel_values\r\n    input_images = torch.tensor(input_images).to(model.device).type(model.dtype)\r\n\r\n    inputs = tokenizer([\"This is a test\"], return_tensors=\"pt\").input_ids.to(model.device)\r\n    \r\n    results = model(input_ids = inputs, labels=inputs, images = input_images)\r\n    print(f\"Loss: {results.loss}\")\r\n\r\n    results.loss.backward()\r\n    optimizer.step()\r\n```\r\n`conda activate llava`\r\n`python debug.py`\r\n\r\nLog: \r\n```\r\nLoss: 4.4296875\r\nLoss: nan\r\nLoss: nan\r\nLoss: nan\r\nLoss: nan\r\nLoss: nan\r\nLoss: nan\r\nLoss: nan\r\nLoss: nan\r\nLoss: nan\r\n```\r\n\r\nAny ideas as to why this might be happening?</BODY>\n\n<COMMENTS>\n<Comment by TheMikeMerrill at 2023-10-31T23:17:11Z>\n@haotian-liu I'm sure you're very busy with all these issues, I'm still struggling with this and it would mean a lot if you could take a look - even just to point me in a new direction. My advisor has been asking lots of questions about it and it's been hard to make progress :(\n</Comment>\n<Comment by haotian-liu at 2023-11-01T00:41:50Z>\nThank you for your interest in LLaVA, and sorry for the delayed response.\r\n\r\nYou would probably need to set `model.to(dtype=torch.bfloat16)` or `model.to(dtype=torch.float32)`, as float16 may have gradient overflow. However, I am not 100% as I only used the Huggingface Trainer to train.\n</Comment>\n<Comment by TheMikeMerrill at 2023-11-01T18:29:17Z>\nThat seems to have done the trick. Thank you!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 462,
    "state": "open",
    "created_by": "JoeLeelyf",
    "created_at": "2023-09-27T13:00:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/462</URL>\n\n<TITLE>Size mismatch error when merge LoRA weights</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: I was trying to do down-stream fine-tune training on LLaVA with the LLM \"llama-2-7b-chat\". Since that only LoRA model for the 7b-chat version was published in the model zoo, I made my attempt to merge the LoRA and the original llama-2-7b model using the following command, but met with a size mismatch error.\r\n\r\nCommand:\r\n```\r\npython merge_lora_weights.py --model-path liuhaotian/llava-llama-2-7b-chat-lightning-lora-preview --model-base meta-llama/Llama-2-7b-chat-hf --save-model-path ./checkpoints/llava-7b-llama-2-7b-chat\r\n```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/mnt/data-2/data/lyf/LLaVA/scripts/merge_lora_weights.py\", line 22, in <module>\r\n    merge_lora(args)\r\n  File \"/mnt/data-2/data/lyf/LLaVA/scripts/merge_lora_weights.py\", line 8, in merge_lora\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, device_map='cpu')\r\n  File \"/mnt/data-2/data/yh/LLaVA_3D/llava/model/builder.py\", line 69, in load_pretrained_model\r\n    model.load_state_dict(non_lora_trainables, strict=False)\r\n  File \"/mnt/data-1/data/jiagang.zhu/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2041, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for LlavaLlamaForCausalLM:\r\n        size mismatch for model.mm_projector.weight: copying a param with shape torch.Size([4096, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 20]).\r\n```\r\n\r\nI see that others have successfully made this in #410, and I don't know why mine is not working. The LLaVA codes are the latest version which I cloned to my computer just now.</BODY>\n\n<COMMENTS>\n<Comment by makanju0la at 2025-01-13T10:17:30Z>\nWas your pre-trained model trained on Vicuna 7b ? I had this same error and it was because I was trying to merge my 7b parameter Lora model with the 13b base model like this:\r\n\r\n> python3 scripts/merge_lora_weights.py --model-path /home/grads/mogunleye/Research/LLaVA/checkpoints/llava-v1.5-7b-lora --model-base **liuhaotian/llava-v1.5-13b** --save-model-path /home/grads/mogunleye/Research/LLaVA/checkpoints/llava-merged_lora\r\n\r\nWhen I fixed this and used the corresponding base model that aligns with my fine-tuned model, like below, I was able to merge successfully: \r\n\r\n> python3 scripts/merge_lora_weights.py --model-path /home/grads/mogunleye/Research/LLaVA/checkpoints/llava-v1.5-7b-lora --model-base **liuhaotian/llava-v1.5-7b** --save-model-path /home/grads/mogunleye/Research/LLaVA/checkpoints/llava-merged_lora \r\n\r\nTLDR: use the correct base model might help. I was using 13B LLaVa for a 7b  trained Lora\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 461,
    "state": "open",
    "created_by": "yytzsy",
    "created_at": "2023-09-27T05:43:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/461</URL>\n\n<TITLE>training exit without any useful error information</TITLE>\n\n<BODY>### Question\n\n<img width=\"903\" alt=\"截屏2023-09-27 下午1 41 24\" src=\"https://github.com/haotian-liu/LLaVA/assets/16056333/3417505f-8201-48ce-a292-5197fb09bbc1\">\r\n\r\nI encounter the above issue at the pretraining stage, and wonder know why.  My training script is as follows:\r\n\r\ndeepspeed --num_gpus=2 llava/train/train.py \\\r\n    --deepspeed zero3.json \\\r\n    --model_name_or_path $MODEL_VERSION \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path data/chat.json \\\r\n    --image_folder data/pt \\\r\n    --vision_tower openai/vit \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --mm_use_im_patch_token True \\\r\n    --bf16 True \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 2 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 24000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 2 \\\r\n    --lazy_preprocess True \\\r\n    --output_dir $ROOT_DIR/save/llava_pretrain_local_yyt \\\r\n    --report_to \"tensorboard\"</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 460,
    "state": "closed",
    "created_by": "wanghao-007",
    "created_at": "2023-09-26T15:22:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/460</URL>\n\n<TITLE>[Discussion] 繁琐</TITLE>\n\n<BODY>### Discussion\n\n有一说一，我觉得这个web端完全没有必要这么麻烦，要启动一个对话界面需要启动三个脚本程序，这完全是不可理喻的，建议改进一下，只启动一个。</BODY>\n\n<COMMENTS>\n<Comment by nj159 at 2023-10-06T05:50:01Z>\n请问一下，在启动第三个脚本程序时，python3 -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./checkpoints/LLaVA-13B-v0 中的./checkpoints/LLaVA-13B-v0是自己在本地下载LLaVA-13B-v0模型后的路径嘛？\r\n我前两个启动之后，第三个的错误如下：\r\n2023-10-06 05:49:13 | ERROR | stderr | Traceback (most recent call last):\r\n2023-10-06 05:49:13 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n2023-10-06 05:49:13 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n2023-10-06 05:49:13 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n2023-10-06 05:49:13 | ERROR | stderr |     exec(code, run_globals)\r\n2023-10-06 05:49:13 | ERROR | stderr |   File \"/opt/data/private/LLaVA/llava/serve/model_worker.py\", line 273, in <module>\r\n2023-10-06 05:49:13 | ERROR | stderr |     worker = ModelWorker(args.controller_address,\r\n2023-10-06 05:49:13 | ERROR | stderr |   File \"/opt/data/private/LLaVA/llava/serve/model_worker.py\", line 64, in __init__\r\n2023-10-06 05:49:13 | ERROR | stderr |     self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\r\n2023-10-06 05:49:13 | ERROR | stderr |   File \"/opt/data/private/LLaVA/llava/model/builder.py\", line 102, in load_pretrained_model\r\n2023-10-06 05:49:13 | ERROR | stderr |     tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n2023-10-06 05:49:13 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 652, in from_pretrained\r\n2023-10-06 05:49:13 | ERROR | stderr |     tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\r\n2023-10-06 05:49:13 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 496, in get_tokenizer_config\r\n2023-10-06 05:49:13 | ERROR | stderr |     resolved_config_file = cached_file(\r\n2023-10-06 05:49:13 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py\", line 417, in cached_file\r\n2023-10-06 05:49:13 | ERROR | stderr |     resolved_file = hf_hub_download(\r\n2023-10-06 05:49:13 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 110, in _inner_fn\r\n2023-10-06 05:49:13 | ERROR | stderr |     validate_repo_id(arg_value)\r\n2023-10-06 05:49:13 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 158, in validate_repo_id\r\n2023-10-06 05:49:13 | ERROR | stderr |     raise HFValidationError(\r\n2023-10-06 05:49:13 | ERROR | stderr | huggingface_hub.utils._validators.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './checkpoints/LLaVA-13B-v0'. Use `repo_type` argument if needed.\n</Comment>\n<Comment by wanghao-007 at 2023-10-08T04:36:06Z>\n> 请问一下，在启动第三个脚本程序时，python3 -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./checkpoints/LLaVA-13B-v0 中的./checkpoints/LLaVA-13B-v0是自己在本地下载LLaVA-13B-v0模型后的路径嘛？ 我前两个启动之后，第三个的错误如下： 2023-10-06 05:49:13 | ERROR | stderr | Traceback (most recent call last): 2023-10-06 05:49:13 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main 2023-10-06 05:49:13 | ERROR | stderr | return _run_code(code, main_globals, None, 2023-10-06 05:49:13 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code 2023-10-06 05:49:13 | ERROR | stderr | exec(code, run_globals) 2023-10-06 05:49:13 | ERROR | stderr | File \"/opt/data/private/LLaVA/llava/serve/model_worker.py\", line 273, in 2023-10-06 05:49:13 | ERROR | stderr | worker = ModelWorker(args.controller_address, 2023-10-06 05:49:13 | ERROR | stderr | File \"/opt/data/private/LLaVA/llava/serve/model_worker.py\", line 64, in **init** 2023-10-06 05:49:13 | ERROR | stderr | self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model( 2023-10-06 05:49:13 | ERROR | stderr | File \"/opt/data/private/LLaVA/llava/model/builder.py\", line 102, in load_pretrained_model 2023-10-06 05:49:13 | ERROR | stderr | tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False) 2023-10-06 05:49:13 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 652, in from_pretrained 2023-10-06 05:49:13 | ERROR | stderr | tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs) 2023-10-06 05:49:13 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 496, in get_tokenizer_config 2023-10-06 05:49:13 | ERROR | stderr | resolved_config_file = cached_file( 2023-10-06 05:49:13 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py\", line 417, in cached_file 2023-10-06 05:49:13 | ERROR | stderr | resolved_file = hf_hub_download( 2023-10-06 05:49:13 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 110, in _inner_fn 2023-10-06 05:49:13 | ERROR | stderr | validate_repo_id(arg_value) 2023-10-06 05:49:13 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 158, in validate_repo_id 2023-10-06 05:49:13 | ERROR | stderr | raise HFValidationError( 2023-10-06 05:49:13 | ERROR | stderr | huggingface_hub.utils._validators.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './checkpoints/LLaVA-13B-v0'. Use `repo_type` argument if needed.\r\n\r\n不瞒你说，第三个脚本我也没有运行成功，无法正常使用\n</Comment>\n<Comment by theblackcat102 at 2023-10-17T00:36:35Z>\n@wanghao-007  你的 ./checkpoints/LLaVA-13B-v0' 路徑不存在吧？否則就是位子放錯了，這個問題應該是 huggingface `from_pretrained` 找不到該路徑。\r\n\r\n我覺得 @haotian-liu  設計的想法是一個中控的服務 (controller) 負責分配資源到不同的 worker 。為了讓實驗能比較好比較， worker 可以是不同的模型。而 user ( 例如 gradio ) 只需要去負責與 controller 對接即可。\r\n\r\n嚴格上你應該是只需要第一個 worker 沒有 controller 應該也要能使用。但是我覺得畢竟這是研究向的專案，如果想優化的話，就去 [text-generation-inference](https://github.com/huggingface/text-generation-inference) 專案那邊去吧\n</Comment>\n<Comment by haotian-liu at 2023-10-17T02:12:52Z>\nThanks for the explanation @theblackcat102 and that's our intention (also the design is largely borrowed from Vicuna/FastChat).\r\n\r\nWe are also creating a single-endpoint demo, which does not require the user to mess up with these controller/worker/frontend stuff. Our first step is a HF Space demo: https://huggingface.co/spaces/badayvedat/LLaVA\r\n\r\nThanks.\n</Comment>\n<Comment by wanghao-007 at 2023-10-17T02:22:55Z>\n> @wanghao-007 你的 ./checkpoints/LLaVA-13B-v0' 路徑不存在吧？否則就是位子放錯了，這個問題應該是 huggingface `from_pretrained` 找不到該路徑。\r\n> \r\n> 我覺得 @haotian-liu 設計的想法是一個中控的服務 (controller) 負責分配資源到不同的 worker 。為了讓實驗能比較好比較， worker 可以是不同的模型。而 user ( 例如 gradio ) 只需要去負責與 controller 對接即可。\r\n> \r\n> 嚴格上你應該是只需要第一個 worker 沒有 controller 應該也要能使用。但是我覺得畢竟這是研究向的專案，如果想優化的話，就去 [text-generation-inference](https://github.com/huggingface/text-generation-inference) 專案那邊去吧\r\n\r\n是的，运行成功了。\n</Comment>\n<Comment by zbxx-423 at 2023-11-05T09:40:15Z>\n你好 我想问一下具体在哪里改这个目录啊     我的报错和你的一样\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 459,
    "state": "open",
    "created_by": "dyahadila",
    "created_at": "2023-09-26T13:57:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/459</URL>\n\n<TITLE>[Question] Can we get probabilities of the generated answer?</TITLE>\n\n<BODY>### Question\n\nHi! Thanks for the great work! :) was wondering if its possible to get probabilities, along with the output ids for model.generate?</BODY>\n\n<COMMENTS>\n<Comment by fei-chang at 2023-10-09T04:15:52Z>\nSame question here.\n</Comment>\n<Comment by wielandmichele at 2023-10-26T07:04:54Z>\nSame question\n</Comment>\n<Comment by adsbansal at 2024-02-27T17:54:56Z>\nsame question\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 458,
    "state": "open",
    "created_by": "mibri77",
    "created_at": "2023-09-26T13:23:23Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/458</URL>\n\n<TITLE>RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED[Usage]</TITLE>\n\n<BODY>### Describe the issue\n\nGetting above error  right after i fill the user prompt</BODY>\n\n<COMMENTS>\n<Comment by maximotus at 2023-11-28T12:48:51Z>\nSame here\n</Comment>\n<Comment by Arancia111 at 2024-03-12T09:06:01Z>\n+1 same\n</Comment>\n<Comment by wanghao9610 at 2024-03-24T13:59:03Z>\nI have met the issue, the reason is the cuda package mismatches with the PyTorch cuda version.  I have resolved the problem by rechecking the \"nvcc -V\" and \"torch.version.cuda\" to ensure the same.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 457,
    "state": "open",
    "created_by": "1024-xx",
    "created_at": "2023-09-26T08:36:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/457</URL>\n\n<TITLE>[Question]</TITLE>\n\n<BODY>### Question\n\nWhat model do you use when making caption of pictures?\r\nThe captions generated by general models are relatively short..</BODY>\n\n<COMMENTS>\n<Comment by Abhishek-at-vanigaa at 2023-09-26T10:52:35Z>\nare you?  able to run this repo locally could please help with steps to run it  locally I am new to this filed\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 456,
    "state": "closed",
    "created_by": "hjsg1010",
    "created_at": "2023-09-26T02:12:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/456</URL>\n\n<TITLE>[Usage] Failed to run the model</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: \r\nRuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\r\n'NoneType' object is not subscriptable\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path /home/model/llava/llava-336px-pretrain-vicuna-13b-v1.3 --model-base /home/model/llava/vicuna_13b_v1.3\r\n```\r\n\r\nLog: \r\n```\r\n/home/.local/share/conda/envs/llava/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\r\n  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\r\n/home/.local/share/conda/envs/llava/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\r\nError: Attempting to get amgpu ISA Details 'NoneType' object has no attribute 'group'\r\nError: Attempting to get amgpu ISA Details 'NoneType' object has no attribute 'group'\r\nTraceback (most recent call last):\r\n  File \"/home/.local/share/conda/envs/llava/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1099, in _get_module\r\n    return importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"/home/.local/share/conda/envs/llava/lib/python3.10/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/home/.local/share/conda/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 32, in <module>\r\n    from ...modeling_utils import PreTrainedModel\r\n  File \"/home/.local/share/conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 38, in <module>\r\n    from .deepspeed import deepspeed_config, is_deepspeed_zero3_enabled\r\n  File \"/home/.local/share/conda/envs/llava/lib/python3.10/site-packages/transformers/deepspeed.py\", line 37, in <module>\r\n    from accelerate.utils.deepspeed import HfDeepSpeedConfig as DeepSpeedConfig\r\n  File \"/home/.local/share/conda/envs/llava/lib/python3.10/site-packages/accelerate/__init__.py\", line 3, in <module>\r\n    from .accelerator import Accelerator\r\n  File \"/home/.local/share/conda/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py\", line 35, in <module>\r\n    from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\r\n  File \"/home/.local/share/conda/envs/llava/lib/python3.10/site-packages/accelerate/checkpointing.py\", line 24, in <module>\r\n    from .utils import (\r\n  File \"/home/.local/share/conda/envs/llava/lib/python3.10/site-packages/accelerate/utils/__init__.py\", line 131, in <module>\r\n    from .bnb import has_4bit_bnb_layers, load_and_quantize_model\r\n  File \"/home/.local/share/conda/envs/llava/lib/python3.10/site-packages/accelerate/utils/bnb.py\", line 42, in <module>\r\n    import bitsandbytes as bnb\r\n  File \"/home/.local/share/conda/envs/llava/lib/python3.10/site-packages/bitsandbytes/__init__.py\", line 16, in <module>\r\n    from .nn import modules\r\n  File \"/home/.local/share/conda/envs/llava/lib/python3.10/site-packages/bitsandbytes/nn/__init__.py\", line 6, in <module>\r\n    from .triton_based_modules import SwitchBackLinear, SwitchBackLinearGlobal, SwitchBackLinearVectorwise, StandardLinear\r\n  File \"/home/.local/share/conda/envs/llava/lib/python3.10/site-packages/bitsandbytes/nn/triton_based_modules.py\", line 8, in <module>\r\n    from bitsandbytes.triton.dequantize_rowwise import dequantize_rowwise\r\n  File \"/home/.local/share/conda/envs/llava/lib/python3.10/site-packages/bitsandbytes/triton/dequantize_rowwise.py\", line 10, in <module>\r\n    import triton\r\n  File \"/home/.local/share/conda/envs/llava/lib/python3.10/site-packages/triton/__init__.py\", line 20, in <module>\r\n    from .runtime import (\r\n  File \"/home/.local/share/conda/envs/llava/lib/python3.10/site-packages/triton/runtime/__init__.py\", line 1, in <module>\r\n    from .autotuner import Config, Heuristics, autotune, heuristics\r\n  File \"/home/.local/share/conda/envs/llava/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 7, in <module>\r\n    from ..compiler import OutOfResources\r\n  File \"/home/.local/share/conda/envs/llava/lib/python3.10/site-packages/triton/compiler.py\", line 1895, in <module>\r\n    @static_vars(amdgcn_bitcode_paths = _get_amdgcn_bitcode_paths())\r\n  File \"/home/.local/share/conda/envs/llava/lib/python3.10/site-packages/triton/compiler.py\", line 1874, in _get_amdgcn_bitcode_paths\r\n    gfx_arch = _get_amdgcn_bitcode_paths.discovered_gfx_arch_fulldetails[1]\r\nTypeError: 'NoneType' object is not subscriptable\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/.local/share/conda/envs/llava/lib/python3.10/runpy.py\", line 187, in _run_module_as_main\r\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\r\n  File \"/home/.local/share/conda/envs/llava/lib/python3.10/runpy.py\", line 110, in _get_module_details\r\n    __import__(pkg_name)\r\n  File \"/home/LLaVA-main/llava/__init__.py\", line 1, in <module>\r\n    from .model import LlavaLlamaForCausalLM\r\n  File \"/home/LLaVA-main/llava/model/__init__.py\", line 1, in <module>\r\n    from .language_model.llava_llama import LlavaLlamaForCausalLM, LlavaConfig\r\n  File \"/home/LLaVA-main/llava/model/language_model/llava_llama.py\", line 22, in <module>\r\n    from transformers import AutoConfig, AutoModelForCausalLM, \\\r\n  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\r\n  File \"/home/.local/share/conda/envs/llava/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1090, in __getattr__\r\n    value = getattr(module, name)\r\n  File \"/home/.local/share/conda/envs/llava/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1089, in __getattr__\r\n    module = self._get_module(self._class_to_module[name])\r\n  File \"/home/.local/share/conda/envs/llava/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1101, in _get_module\r\n    raise RuntimeError(\r\nRuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\r\n'NoneType' object is not subscriptable\r\n\r\n```\r\n\r\nI'm loading model from local files. so I changed [Launch a model worker (LoRA weights, unmerged)](https://github.com/haotian-liu/LLaVA#launch-a-model-worker-lora-weights-unmerged) command to upper command. \r\n\r\nplease let me know if there is problem\r\nthx for read my question and sorry for if it is silly problem</BODY>\n\n<COMMENTS>\n<Comment by hjsg1010 at 2023-09-26T02:25:37Z>\nI got same error from \r\n```python\r\nfrom transformers import LlamaModel\r\n```\r\nthis line.\r\n\r\nI tried reinstall libraries, but still get same error\n</Comment>\n<Comment by hjsg1010 at 2023-09-26T02:31:02Z>\nIt's weird.. \r\nwhen I try at pip enviorment (not conda virtual enviorment) it works..\r\nIt is not repo issues so I will try it to fix my self. sorry for bothering\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 455,
    "state": "open",
    "created_by": "Abhishek-at-vanigaa",
    "created_at": "2023-09-25T07:15:35Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/455</URL>\n\n<TITLE>local deployment instructions are not clear</TITLE>\n\n<BODY>### Discussion\n\nHello Developers I appreciate your effort. I followed the GitHub instructions and tried to run the llava locally but after installing all the required dependencies I am not sure how what weights to download and how to use within the code. already checked the model zoo but am confused. please help, I am working on an use case and trying to use llava in it . Thank You !</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 453,
    "state": "open",
    "created_by": "prajwalJumde",
    "created_at": "2023-09-22T18:18:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/453</URL>\n\n<TITLE>[Discussion] How to change the deafult OCR engine in LLaVA</TITLE>\n\n<BODY>### Discussion\n\n_No response_</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 452,
    "state": "open",
    "created_by": "CauchyFanUpdate",
    "created_at": "2023-09-22T10:15:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/452</URL>\n\n<TITLE>[Question] RuntimeError: mat1 and mat2 must have the same dtype</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nHello, I'm running the finetuning job for llava-13B, but I keep getting the error message \"RuntimeError: mat1 and mat2 must have the same dtype\". It's possible that the model is bf16 and the images are fp32. How should I handle this? Below are my parameters:\r\nCommand:\r\n```\r\npython llava/train/train_mem.py \\\r\n    --model_name_or_path checkpoints/llama-2-13b-chat \\\r\n    --version llava_llama_2 \\\r\n    --data_path playground/data/llava_instruct_150k.json \\\r\n    --image_folder coco2017/train2017 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter checkpoints/llama-2-13b-chat/mm_projector.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir checkpoints/llava-llama-2-13b-chat-finetune-seg \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```</BODY>\n\n<COMMENTS>\n<Comment by zt-yang at 2023-10-27T14:47:32Z>\nAny update? Encountered the same issue.\n</Comment>\n<Comment by CauchyFanUpdate at 2023-10-31T03:00:08Z>\n> Any update? Encountered the same issue.\r\n\r\nWhen I encountered this task, I merged the models llama7b and lora together, and then commented out '--pretrain_mm_mlp_adapter', '/home/yangfan/mycode/LLaVA/checkpoints/llava-pretrain-llama-2-7b-chat/mm_projector.bin' for fine-tuning from scratch.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 451,
    "state": "closed",
    "created_by": "Carol-lyh",
    "created_at": "2023-09-22T03:16:55Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/451</URL>\n\n<TITLE>[Question]</TITLE>\n\n<BODY>### Question\n\nWhat's the purpose of \"mm_use_im_start_end\"? What does this argument control? \r\nIf this is set to TRUE, what is the main difference?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-05T00:03:43Z>\nDuplicate of #578\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 450,
    "state": "closed",
    "created_by": "wanghao-cst",
    "created_at": "2023-09-21T12:44:55Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/450</URL>\n\n<TITLE>RuntimeError: probability tensor contains either `inf`, `nan` or element < 0 [Usage]</TITLE>\n\n<BODY>### RuntimeError: probability tensor contains either inf, nan or element < 0\r\n\r\nIssue:\r\nWhen I use the lora weights, both llama2 and vicuna7b got the same error:\r\nCommand:\r\n```\r\npython -m llava.eval.run_llava \\\r\n    --model-path \"/home/wanghao/weights/llava/llava-llama-2-7b-chat-lightning-lora-preview\" \\\r\n    --model-base \"/home/wanghao/weights/llava/llama-2-7b-chat-hf\" \\\r\n    --image-file \"llava/serve/examples/extreme_ironing.jpg\" \\\r\n    --query \"What is unusual about this image?\"\r\n```\r\n\r\nI have made sure that the ckpt are correct, like the base model should in hf format.\r\n\r\nScreenshots:\r\n<img width=\"1163\" alt=\"截屏2023-09-21 20 42 17\" src=\"https://github.com/haotian-liu/LLaVA/assets/55015183/c9731dbe-1572-4857-8931-19159b9b4280\"></BODY>\n\n<COMMENTS>\n<Comment by 478786359 at 2023-10-20T09:52:02Z>\n> ### RuntimeError: probability tensor contains either inf, nan or element < 0\r\n> Issue: When I use the lora weights, both llama2 and vicuna7b got the same error: Command:\r\n> \r\n> ```\r\n> python -m llava.eval.run_llava \\\r\n>     --model-path \"/home/wanghao/weights/llava/llava-llama-2-7b-chat-lightning-lora-preview\" \\\r\n>     --model-base \"/home/wanghao/weights/llava/llama-2-7b-chat-hf\" \\\r\n>     --image-file \"llava/serve/examples/extreme_ironing.jpg\" \\\r\n>     --query \"What is unusual about this image?\"\r\n> ```\r\n> \r\n> I have made sure that the ckpt are correct, like the base model should in hf format.\r\n> \r\n> Screenshots: <img alt=\"截屏2023-09-21 20 42 17\" width=\"1163\" src=\"https://user-images.githubusercontent.com/55015183/269615330-c9731dbe-1572-4857-8931-19159b9b4280.png\">\r\n\r\n\"I've encountered the same problem. Have you already resolved it?\"\n</Comment>\n<Comment by BrianG13 at 2023-10-30T06:10:20Z>\nSame problem here.\r\nAny news?\n</Comment>\n<Comment by 478786359 at 2023-10-30T09:01:17Z>\n> Same problem here. Any news?\r\n\r\nThis is because I use multiple graphics cards when running. If I only use one, this problem will be solved.\n</Comment>\n<Comment by haotian-liu at 2023-11-04T20:23:29Z>\nThis may have also been fixed in #720. Please feel free to re-open if the problem persists. Thanks.\n</Comment>\n<Comment by zhanghuiecho at 2025-03-08T12:07:50Z>\n> This may have also been fixed in [#720](https://github.com/haotian-liu/LLaVA/pull/720). Please feel free to re-open if the problem persists. Thanks.\n\nI also meet this error, when i finetune llava1.5 in my data, and i wirte a test inference code based cli.py, just repeat load the whole finetuned model to process every text-image pair, but this error just happen in some data, all of my test data is 93, there are 5 data encounter this error ,i don't know how to solved it ,  :(  :(  :(  :( my code and error as follow，could you help me, thank you :) ：），and i also have some other questions ,i want to finetune this model only  text modal and image modal, how to do this :\n`import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \nimport argparse\nimport torch\nimport json\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.conversation import conv_templates, SeparatorStyle\nfrom llava.model.builder import load_pretrained_model\nfrom llava.utils import disable_torch_init\nfrom llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path\n\nfrom PIL import Image\n\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nfrom transformers import TextStreamer\n\n\ndef load_image(image_file):\n    if image_file.startswith('http://') or image_file.startswith('https://'):\n        response = requests.get(image_file)\n        image = Image.open(BytesIO(response.content)).convert('RGB')\n    else:\n        image = Image.open(image_file).convert('RGB')\n    return image\n\n\ndef test(image_file,input_text):\n    # Model\n    disable_torch_init()\n\n    model_name = get_model_name_from_path(args.model_path)\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name,\n                                                                           args.load_8bit, args.load_4bit,\n                                                                           device=args.device)\n    if \"llama-2\" in model_name.lower():\n        conv_mode = \"llava_llama_2\"\n    elif \"mistral\" in model_name.lower():\n        conv_mode = \"mistral_instruct\"\n    elif \"v1.6-34b\" in model_name.lower():\n        conv_mode = \"chatml_direct\"\n    elif \"v1\" in model_name.lower():\n        conv_mode = \"llava_v1\"\n    elif \"mpt\" in model_name.lower():\n        conv_mode = \"mpt\"\n    else:\n        conv_mode = \"llava_v0\"\n\n    if args.conv_mode is not None and conv_mode != args.conv_mode:\n        print(\n            '[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}'.format(conv_mode,\n                                                                                                              args.conv_mode,\n                                                                                                              args.conv_mode))\n    else:\n        args.conv_mode = conv_mode\n\n    conv = conv_templates[args.conv_mode].copy()\n    if \"mpt\" in model_name.lower():\n        roles = ('user', 'assistant')\n    else:\n        roles = conv.roles\n\n    image = load_image(image_file)\n    image_size = image.size\n    # Similar operation in model_worker.py\n    image_tensor = process_images([image], image_processor, model.config)\n    if type(image_tensor) is list:\n        image_tensor = [image.to(model.device, dtype=torch.float16) for image in image_tensor]\n    else:\n        image_tensor = image_tensor.to(model.device, dtype=torch.float16)\n\n    # while True:\n    #     try:\n    #         inp = input(f\"{roles[0]}: \")\n    #     except EOFError:\n    #         inp = \"\"\n    #     if not inp:\n    #         print(\"exit...\")\n    #         break\n    inp = input_text\n    print(f\"{roles[1]}: \", end=\"\")\n\n    if image is not None:\n        # first message\n        if model.config.mm_use_im_start_end:\n            inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + inp\n        else:\n            inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\n        image = None\n\n    conv.append_message(conv.roles[0], inp)\n    conv.append_message(conv.roles[1], None)\n    prompt = conv.get_prompt()\n\n    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)\n    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n    keywords = [stop_str]\n    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n\n    with torch.inference_mode():\n        output_ids = model.generate(\n            input_ids,\n            images=image_tensor,\n            image_sizes=[image_size],\n            do_sample=True if args.temperature > 0 else False,\n            temperature=args.temperature,\n            max_new_tokens=args.max_new_tokens,\n            streamer=streamer,\n            use_cache=True)\n\n    outputs = tokenizer.decode(output_ids[0]).strip()\n    conv.messages[-1][-1] = outputs\n\n    if args.debug:\n        # print(\"\\n\", {\"prompt\": prompt, \"outputs\": outputs}, \"\\n\")\n        print(\"\\n\",  \"outputs\", outputs, \"\\n\")\n    return outputs\n\n\ndef main(args):\n    #test data\n    with open(args.test_file, \"r\", encoding=\"utf-8\") as f:\n        all_test = json.load(f)\n    f.close()\n\n    all_pred_all = []\n    with open(\"llava_pred.json\", \"w\", encoding=\"utf-8\") as f1:\n        for i, test_data_i in enumerate(all_test[69:]):\n            all_pred = {}\n            id, image_i, text_i = test_data_i['id'], test_data_i['image'], test_data_i['conversations']\n\n            image_i_new = r'/root/image/image/' + image_i\n            text_i_new = text_i[0]['value'].split('image>\\n')[1]\n\n            all_pred['id'] = i\n            all_pred['text_id'] = id\n            all_pred['image_id'] = image_i\n            all_pred['gold'] = text_i[1]['value']\n            print(image_i_new)\n            print(id)\n            print(f'gold: {i+69}',all_pred['gold'])\n            output_i = test(image_i_new, text_i_new )\n            all_pred['pred'] = output_i\n            all_pred_all.append(all_pred)\n\n            json.dump(all_pred, f1, indent=4, sort_keys=False, ensure_ascii=False)\n            f1.write(\"\\n\")\n    f1.close()\n\n    with open(\"llava_pred.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(all_pred_all, f, indent=4, sort_keys=False)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\"--model-path\", type=str, default=\"../../checkpoints/llava-v1.5-7b-task-lora-merged_mmee\")\n    parser.add_argument(\"--model-base\", type=str, default=None)\n    parser.add_argument(\"--image-file\", type=str, default=\"xxx.jpg\")\n    parser.add_argument(\"--test-file\", type=str, default=\"/root/LLaVA/playground/data/test/llava_mmee_test.json\")\n    parser.add_argument(\"--device\", type=str, default=\"cuda\")\n    parser.add_argument(\"--conv-mode\", type=str, default=None)\n    parser.add_argument(\"--temperature\", type=float, default=0.2)\n    parser.add_argument(\"--max-new-tokens\", type=int, default=512)\n    parser.add_argument(\"--load-8bit\", action=\"store_true\")\n    parser.add_argument(\"--load-4bit\", action=\"store_true\")\n    parser.add_argument(\"--debug\", action=\"store_true\")\n    args = parser.parse_args()\n    main(args)\n\n\n\"\"\"\n File \"/root/autodl-tmp/LLaVA/llava/serve/cli_pred.py\", line 170, in <module>\n    args = parser.parse_args()\n  File \"/root/autodl-tmp/LLaVA/llava/serve/cli_pred.py\", line 143, in main\n    output_i = test(image_i_new, text_i_new )\n  File \"/root/autodl-tmp/LLaVA/llava/serve/cli_pred.py\", line 102, in test\n    output_ids = model.generate(\n  File \"/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/root/autodl-tmp/LLaVA/llava/model/language_model/llava_llama.py\", line 137, in generate\n    return super().generate(\n  File \"/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1525, in generate\n    return self.sample(\n  File \"/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2658, in sample\n    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n**RuntimeError: probability tensor contains either `inf`, `nan` or element < 0**\n\nProcess finished with exit code 1\n\n\"\"\"\n`\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 449,
    "state": "closed",
    "created_by": "payne4handsome",
    "created_at": "2023-09-21T11:58:50Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/449</URL>\n\n<TITLE>[Usage] program exit exception with nothing useful information output</TITLE>\n\n<BODY>### Describe the issue\n\n\r\nIssue:\r\nHi @haotian-liu , help me.\r\nI have download llama-2 weights、 llava-150k、pretrain_mm_mlp_adapter. I just want to test the correctness  of the program .\r\nBut program exit exception with nothing useful information output.\r\n\r\nI train LLaVA utilizing 8 Nvidia 3090(24G) gpus.\r\n\r\nCommand:\r\n```\r\nPYTHONPATH=. sh scripts/finetune_full_schedule.sh\r\n```\r\n\r\nLog: \r\n```\r\nroot@node37:/home/zhangpan/workspace/LLaVA# PYTHONPATH=. sh scripts/finetune_full_schedule.sh\r\n[2023-09-21 11:43:52,127] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-09-21 11:43:53,598] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\n[2023-09-21 11:43:53,652] [INFO] [runner.py:555:main] cmd = /opt/conda/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero2.json --model_name_or_path ./checkpoints/llama2-7b-hf --version llava_llama_2 --data_path ./playground/data/llava_instruct_158k.json --image_folder ./playground/data/mscoco/train2017 --vision_tower openai/clip-vit-large-patch14 --pretrain_mm_mlp_adapter ./checkpoints/llava-llama2-7b-hf-pretrain/mm_projector.bin --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir ./checkpoints/llava-llama2-7b-hf-finetune --num_train_epochs 3 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb\r\n[2023-09-21 11:43:55,799] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-09-21 11:43:56,413] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.13.4-1\r\n[2023-09-21 11:43:56,413] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.13.4-1\r\n[2023-09-21 11:43:56,413] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\r\n[2023-09-21 11:43:56,414] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\r\n[2023-09-21 11:43:56,414] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.13.4-1+cuda11.7\r\n[2023-09-21 11:43:56,414] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.13.4-1+cuda11.7\r\n[2023-09-21 11:43:56,414] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.13.4-1\r\n[2023-09-21 11:43:56,414] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\r\n[2023-09-21 11:43:56,414] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0\r\n[2023-09-21 11:43:56,414] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\r\n[2023-09-21 11:43:56,414] [INFO] [launch.py:163:main] dist_world_size=8\r\n[2023-09-21 11:43:56,414] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\r\n[2023-09-21 11:43:58,932] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-09-21 11:43:59,029] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-09-21 11:43:59,149] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-09-21 11:43:59,206] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-09-21 11:43:59,211] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-09-21 11:43:59,237] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-09-21 11:43:59,251] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-09-21 11:43:59,287] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-09-21 11:44:01,163] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-09-21 11:44:01,163] [INFO] [comm.py:616:init_distributed] cdb=None\r\n[2023-09-21 11:44:01,164] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-09-21 11:44:01,164] [INFO] [comm.py:616:init_distributed] cdb=None\r\n[2023-09-21 11:44:01,166] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-09-21 11:44:01,166] [INFO] [comm.py:616:init_distributed] cdb=None\r\n[2023-09-21 11:44:01,173] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-09-21 11:44:01,173] [INFO] [comm.py:616:init_distributed] cdb=None\r\n[2023-09-21 11:44:01,178] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-09-21 11:44:01,178] [INFO] [comm.py:616:init_distributed] cdb=None\r\n[2023-09-21 11:44:01,182] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-09-21 11:44:01,182] [INFO] [comm.py:616:init_distributed] cdb=None\r\n[2023-09-21 11:44:01,182] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n[2023-09-21 11:44:01,197] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-09-21 11:44:01,197] [INFO] [comm.py:616:init_distributed] cdb=None\r\n[2023-09-21 11:44:01,198] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-09-21 11:44:01,198] [INFO] [comm.py:616:init_distributed] cdb=None\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nLoading checkpoint shards:   0%|                                                                                                                        | 0/2 [00:00<?, ?it/s][2023-09-21 11:45:10,350] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 43166\r\n[2023-09-21 11:45:13,658] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 43167\r\n[2023-09-21 11:45:17,284] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 43168\r\n[2023-09-21 11:45:17,285] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 43169\r\n[2023-09-21 11:45:22,391] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 43170\r\nLoading checkpoint shards:  50%|████████████████████████████████████████████████████████                                                        | 1/2 [00:22<00:22, 22.76s/it][2023-09-21 11:45:25,652] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 43171\r\n[2023-09-21 11:45:27,714] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 43172\r\n[2023-09-21 11:45:29,921] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 43173\r\n[2023-09-21 11:45:32,930] [ERROR] [launch.py:321:sigkill_handler] ['/opt/conda/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=7', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', './checkpoints/llama2-7b-hf', '--version', 'llava_llama_2', '--data_path', './playground/data/llava_instruct_158k.json', '--image_folder', './playground/data/mscoco/train2017', '--vision_tower', 'openai/clip-vit-large-patch14', '--pretrain_mm_mlp_adapter', './checkpoints/llava-llama2-7b-hf-pretrain/mm_projector.bin', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './checkpoints/llava-llama2-7b-hf-finetune', '--num_train_epochs', '3', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = -9\r\nroot@node37:/home/zhangpan/workspace/LLaVA#\r\n```\r\n\r\nScreenshots:\r\nmy finetune_full_schedule.sh likes below.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/28085731/61817a19-a158-4879-919f-9c2710015f96)</BODY>\n\n<COMMENTS>\n<Comment by payne4handsome at 2023-09-22T10:56:55Z>\nThe cause of this exception is out of cpu memory.  I set param low_cpu_mem_usage=True in function from_pretrained() resolve this. I close this question.\n</Comment>\n<Comment by yytzsy at 2023-09-27T04:26:30Z>\n> The cause of this exception is out of cpu memory. I set param low_cpu_mem_usage=True in function from_pretrained() resolve this. I close this question.\r\n\r\nHow do you set the low_cpu_mem_usage=True? Can you show the code? Thank you very much!\n</Comment>\n<Comment by payne4handsome at 2023-09-27T05:50:00Z>\n@yytzsy If you train with deepspeed with stage 2, the code likes belows.\r\n```\r\n            model = LlavaLlamaForCausalLM.from_pretrained(\r\n                model_args.model_name_or_path,\r\n                cache_dir=training_args.cache_dir,\r\n                low_cpu_mem_usage=True,\r\n                **bnb_model_from_pretrained_args\r\n            )\r\n```\r\n\r\nIf you use deepspeed with stage 3, you don't need do this and will train correctly.\n</Comment>\n<Comment by 459737087 at 2024-01-12T07:09:21Z>\nThere is a contradiction point, when I use zero3_offload.json \r\nIt reported\r\n```\r\n exits with return code = -9\r\n```\r\nuse zero2.json, it reported OOM。\r\nI don't know how to solve it\n</Comment>\n<Comment by ybsu at 2024-06-03T10:57:16Z>\n> The cause of this exception is out of cpu memory. I set param low_cpu_mem_usage=True in function from_pretrained() resolve this. I close this question.\r\n\r\nI have set the low_cpu_mem_usage=True, but the issue is still exists, what to do next ? Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 448,
    "state": "open",
    "created_by": "youxiho1",
    "created_at": "2023-09-21T09:00:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/448</URL>\n\n<TITLE>[Question] Difficulty when reimplementing the results of llava-llama-2-13b-chat-lightning-preview on LLaVA-Bench</TITLE>\n\n<BODY>### Question\n\nHi, I'm recently trying to reimplement the results of llava-llama-2-13b-chat-lightning-preview on LLaVA-Bench. I clone the latest official repo and run the model_vqa.py with the following arguments. I didn't revise other code in the repo.\r\n```\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--model-path\", type=str, default=\"workspace/llava-llama-2-13b-chat-lightning-preview\")\r\n    parser.add_argument(\"--model-base\", type=str, default=None)\r\n    parser.add_argument(\"--image-folder\", type=str, default=\"workspace/llava-bench-val2014\")\r\n    parser.add_argument(\"--question-file\", type=str, default=\"workspace/LLaVA/playground/data/coco2014_val_qa_eval/qa90_questions.jsonl\")\r\n    parser.add_argument(\"--answers-file\", type=str, default=\"workspace/answer.jsonl\")\r\n    parser.add_argument(\"--conv-mode\", type=str, default=\"llava_llama_2\")\r\n    parser.add_argument(\"--num-chunks\", type=int, default=1)\r\n    parser.add_argument(\"--chunk-idx\", type=int, default=0)\r\n    parser.add_argument(\"--temperature\", type=float, default=0.2)\r\n    parser.add_argument(\"--top_p\", type=float, default=None)\r\n    parser.add_argument(\"--num_beams\", type=int, default=1)\r\n    args = parser.parse_args()\r\n  \r\n    eval_model(args)\r\n```\r\n\r\nFinally, I got the score as below\r\nall 79.4\r\ncomplex 88.8\r\nconv 78.6\r\ndetail 70.4\r\n\r\nIt seems it's somehow different from the results recorded in https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md\r\nOverall 67.9\r\nComplex 80.0\r\nDetail 58.6\r\nConv 56.7\r\n\r\nI wonder where is the difference possibly from? Have I made any mistake in the hyper parameters?\r\n\r\nThank you for your time! I'm looking forward to your reply!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 447,
    "state": "closed",
    "created_by": "24-solar-terms",
    "created_at": "2023-09-21T08:03:57Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/447</URL>\n\n<TITLE>[Usage] Multi-GPU training hangs: Watchdog caught collective operation timeout</TITLE>\n\n<BODY>### Describe the issue\n\nHi, when I use my own dataset, roughly 50w data, DDP training with 8 A100 80G, the training hangs and gives the following error:\r\n```\r\n[E ProcessGroupNCCL.cpp:828] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=15173, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1802710 milliseconds before timing out.\r\n[E ProcessGroupNCCL.cpp:828] [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=15170, OpType=BROADCAST, Timeout(ms)=1800000) ran for 1803156 milliseconds before timing out.\r\n[E ProcessGroupNCCL.cpp:828] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=15173, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1802713 milliseconds before timing out.\r\n[E ProcessGroupNCCL.cpp:828] [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=15170, OpType=BROADCAST, Timeout(ms)=1800000) ran for 1803216 milliseconds before timing out.\r\n[E ProcessGroupNCCL.cpp:828] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=15173, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1802791 milliseconds before timing out.\r\n[E ProcessGroupNCCL.cpp:828] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=15173, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1802786 milliseconds before timing out.\r\n[E ProcessGroupNCCL.cpp:828] [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=15172, OpType=BROADCAST, Timeout(ms)=1800000) ran for 1803288 milliseconds before timing out.\r\n[E ProcessGroupNCCL.cpp:828] [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=15173, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1802877 milliseconds before timing out.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [44,0,0], thread: [64,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [44,0,0], thread: [65,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [44,0,0], thread: [66,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [44,0,0], thread: [67,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [44,0,0], thread: [68,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [44,0,0], thread: [69,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [44,0,0], thread: [70,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n...\r\nTraceback (most recent call last):\r\n  File \"/workdir/llava/train/train_mem.py\", line 16, in <module>\r\n    train()\r\n  File \"/workdir/llava/train/train.py\", line 930, in train\r\n    trainer.train()\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2654, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2679, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1156, in forward\r\n    output = self._run_ddp_forward(*inputs, **kwargs)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1110, in _run_ddp_forward\r\n    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 581, in forward\r\n    return model_forward(*args, **kwargs)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 569, in __call__\r\n    return convert_to_fp32(self.model_forward(*args, **kwargs))\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/torch/amp/autocast_mode.py\", line 14, in decorate_autocast\r\n    return func(*args, **kwargs)\r\n  File \"/workdir/llava/model/language_model/llava_llama.py\", line 75, in forward\r\n    input_ids, attention_mask, past_key_values, inputs_embeds, labels = self.prepare_inputs_labels_for_multimodal(input_ids, attention_mask, past_key_values, labels, images)\r\n  File \"/workdir/llava/model/llava_arch.py\", line 119, in prepare_inputs_labels_for_multimodal\r\n    image_features = self.encode_images(images)\r\n  File \"/workdir/llava/model/llava_arch.py\", line 99, in encode_images\r\n    image_features = self.get_model().get_vision_tower()(images)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/workdir/llava/model/multimodal_encoder/donut_encoder.py\", line 47, in forward\r\n    image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype))\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/workdir/llava/model/multimodal_encoder/donut.py\", line 107, in forward\r\n    x = self.model.layers(x)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 217, in forward\r\n    input = module(input)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/timm/models/swin_transformer.py\", line 420, in forward\r\n    x = self.blocks(x)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 217, in forward\r\n    input = module(input)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/timm/models/swin_transformer.py\", line 310, in forward\r\n    attn_windows = self.attn(x_windows, mask=self.attn_mask)  # num_win*B, window_size*window_size, C\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/timm/models/swin_transformer.py\", line 216, in forward\r\n    x = (attn @ v).transpose(1, 2).reshape(B_, N, -1)\r\nRuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasGemmStridedBatchedExFix(handle, opa, opb, (int)m, (int)n, (int)k, (void*)&falpha, a, CUDA_R_16BF, (int)lda, stridea, b, CUDA_R_16BF, (int)ldb, strideb, (void*)&fbeta, c, CUDA_R_16BF, (int)ldc, stridec, (int)num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`\r\n[E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.\r\n[E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down.\r\nTraceback (most recent call last):\r\n  File \"/workdir/llava/train/train_mem.py\", line 16, in <module>\r\n    train()\r\n  File \"/workdir/llava/train/train.py\", line 930, in train\r\n    trainer.train()\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2665, in training_step\r\n    self.accelerator.backward(loss)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1853, in backward\r\n    loss.backward(**kwargs)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\r\n    torch.autograd.backward(\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\r\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\nRuntimeError: NCCL communicator was aborted on rank 6.  Original reason for failure was: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=15170, OpType=BROADCAST, Timeout(ms)=1800000) ran for 1803156 milliseconds before timing out.\r\n[E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.\r\n[E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down.\r\n[E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.\r\n[E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down.\r\n=====================================================\r\n```\r\n\r\nAt the beginning, I thought maybe some corrupt images lead to the error, because I see cuda index error in above message, and traceback show the error in swin transformer, but I checked all images use PIL Image.open and deleted all images with warning, no problem found, the training still stuck. I also check input image tensor size and they are right.\r\nI searched many way in community, like use the following environment parameter:\r\n```\r\nCUDA_LAUNCH_BLOCKING= 1\r\nNCCL_P2P_LEVEL=2\r\nNCCL_P2P_DISABLE=1\r\nNCCL_DEBUG=INFO\r\nNCCL_DEBUG_SUBSYS=ALL\r\nTORCH_DISTRIBUTED_DEBUG=INFO\r\nNCCL_IB_TIMEOUT=22\r\nNCCL_BLOCKING_WAIT=0\r\nunset LD_LIBRARY_PATH\r\n```\r\nit still didn't work.\r\nThen I tried to use 2 GPU training and batch size per device use 1, and print image path to find the stuck data, but I found the data is ok, and I constructed a dataset only contained the 2 images, the training process didn't stuck and worked.\r\n\r\nHowever, when I training on single GPU, it works fine, when I training use other datasets on DDP mode, it works fine.\r\nSo I think the code is ok and it seems there are some problems in the dataset but since single GPU worked and the dataset once used to training other model before, it seems no problems in the dataset.\r\n\r\nI also use the following code at beginning of train.py:\r\n```\r\ntorch.distributed.init_process_group(backend=\"gloo\")\r\n```\r\njust get the error message:\r\n```\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3772 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3773 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3774 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3775 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3776 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3778 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3779 closing signal SIGTERM\r\nERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -6) local_rank: 5 (pid: 3777) of binary: /miniconda/envs/llava/bin/python3\r\nTraceback (most recent call last):\r\n  File \"/miniconda/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/miniconda/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/torch/distributed/run.py\", line 798, in <module>\r\n    main()\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/torch/distributed/run.py\", line 794, in main\r\n    run(args)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/torch/distributed/run.py\", line 785, in run\r\n    elastic_launch(\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \r\n=====================================================\r\nllava/train/train_mem.py FAILED\r\n-----------------------------------------------------\r\nFailures:\r\n  <NO_OTHER_FAILURES>\r\n-----------------------------------------------------\r\nRoot Cause (first observed failure):\r\n[0]:\r\n  time      : 2023-09-21_12:47:26\r\n  host      : psx1kopxqb355ls7-worker-0\r\n  rank      : 5 (local_rank: 5)\r\n  exitcode  : -6 (pid: 3777)\r\n  error_file: <N/A>\r\n  traceback : Signal 6 (SIGABRT) received by PID 3777\r\n=====================================================\r\n```\r\n\r\nI'm so confused, and I don't know what can I do next.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-24T00:59:14Z>\nHi, please try with the latest DeepSpeed commands, thanks.\n</Comment>\n<Comment by 1359347500cwc at 2023-11-22T10:35:34Z>\nHave you solved this problem? I also encountered the same problem. In my case, this happened during training after 2444steps. The dataset contains about 540k  customer data.\r\n\r\nand I try finetune it using 4*4（a100-40g） though this script\r\n`python -m torch.distributed.run --nnodes=4 \\\r\n\t--node_rank=$RANK \\\r\n    --nproc_per_node=4 \\\r\n    --master_addr=$MASTER_ADDR \\\r\n    --master_port=$MASTER_PORT \\\r\n    ${MODLE_DIR}/train_mem.py \\\r\n    --deepspeed ${MODLE_DIR}/scripts/zero3.json \\\r\n    --model_name_or_path /mnt/chongqinggeminiceph1fs/geminicephfs/pr-training-mt/cwctchen/cwctchen/ckpt/llava-v1.5-7b \\\r\n    --version v1 \\\r\n    --data_path /mnt/chongqinggeminiceph1fs/geminicephfs/pr-training-mt/cwctchen/cwctchen/data_filter/mix_540k_ocr_translate_new.json \\\r\n    --image_folder ${IMAGE_DIR} \\\r\n    --vision_tower /mnt/chongqinggeminiceph1fs/geminicephfs/pr-training-mt/cwctchen/cwctchen/ckpt/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir /mnt/chongqinggeminiceph1fs/geminicephfs/pr-training-mt/cwctchen/cwctchen/LLava_workspace/checkpoints/checkpoints/llava-v1.5-7b-ocr_translate_task_4*4card_new \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 1000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to none\r\n`\r\n![截屏2023-11-22 18 35 25](https://github.com/haotian-liu/LLaVA/assets/57097883/0124a082-3d0b-44f0-b56f-f19b0736d95e)\n</Comment>\n<Comment by pkulwj1994 at 2024-05-07T06:33:11Z>\nI meet the same issue and solved the issue by updating the deepspeed to the latest version through \r\n\r\n`pip install -U deepspeed`\r\n\r\nThanks to Haotian's @haotian-liu constructive suggestion!\n</Comment>\n<Comment by Echo0125 at 2024-06-11T08:46:11Z>\nI encountered the same issue on my own data. When the batch size is set to 16, there are no problems, but when the batch size is set to 8 and gradient_accumulation_steps is set to 2, it hangs. It also hangs when I add new data. I tried updating the versions of torch, deepspeed, and accelerator, but it did not resolve the issue.\n</Comment>\n<Comment by 24-solar-terms at 2024-06-11T10:07:56Z>\n@Echo0125 Maybe check the input sequence length after replacing image placeholder token by real image embeddings\n</Comment>\n<Comment by 24-solar-terms at 2024-06-11T10:11:26Z>\nMy problem is solved by check input sequence length after replacing image placeholder token by real image embeddings. In my dataset, there is long prompt making total input length longer than max sequence length after replacing image placeholder token by real image embeddings, which leads \"../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [44,0,0], thread: [70,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\" error\n</Comment>\n<Comment by Andrew-Zhang at 2024-07-02T20:49:26Z>\n> I encountered the same issue on my own data. When the batch size is set to 16, there are no problems, but when the batch size is set to 8 and gradient_accumulation_steps is set to 2, it hangs. It also hangs when I add new data. I tried updating the versions of torch, deepspeed, and accelerator, but it did not resolve the issue.\r\n\r\n@Echo0125 Did you manage to solve this issue? I get the same problem where gradient accumulation leads to an error.\n</Comment>\n<Comment by zmtttt at 2024-08-29T06:06:35Z>\nhave you sloved? I met the same problem\n</Comment>\n<Comment by wanghao9610 at 2024-12-30T04:13:36Z>\nHello everyone. I encountered the same problem and struggled to resolve it for almost 4 weeks. Finally, I found that the issue was related to the DeepSpeed communication settings. Here I will provide the ZeRO-2 configuration settings (referred to InternVL):\r\n```\r\n{\r\n    \"zero_optimization\": {\r\n      \"stage\": 2,\r\n      \"allgather_partitions\": true,\r\n      \"allgather_bucket_size\": 1e9,\r\n      \"overlap_comm\": true,\r\n      \"reduce_scatter\": true,\r\n      \"reduce_bucket_size\": 1e9,\r\n      \"contiguous_gradients\": true\r\n    },\r\n    \"fp16\": {\r\n      \"enabled\": \"auto\",\r\n      \"auto_cast\": true,\r\n      \"loss_scale\": 0,\r\n      \"initial_scale_power\": 32,\r\n      \"loss_scale_window\": 1000,\r\n      \"hysteresis\": 2,\r\n      \"min_loss_scale\": 1\r\n    },\r\n    \"bf16\": {\r\n      \"enabled\": \"auto\"\r\n    },\r\n    \"gradient_accumulation_steps\": \"auto\",\r\n    \"gradient_clipping\": \"auto\",\r\n    \"train_batch_size\": \"auto\",\r\n    \"train_micro_batch_size_per_gpu\": \"auto\",\r\n    \"wall_clock_breakdown\": false\r\n  }\r\n  \r\n```\r\nThe important settings are allgather_bucket_size and reduce_bucket_size, which affect the communication frequency and budget. Increasing these two values (default is 1e7, recommended is 5e8 or 1e9) will solve the NCCL timeout error. If you are using ZeRO-1 configuration, you may need to apply the same modifications. Hope this can help you solve the problem.\n</Comment>\n<Comment by Parsifal133 at 2025-02-19T07:36:30Z>\n> Hello everyone. I encountered the same problem and struggled to resolve it for almost 4 weeks. Finally, I found that the issue was related to the DeepSpeed communication settings. Here I will provide the ZeRO-2 configuration settings (referred to InternVL):\n> \n> ```\n> {\n>     \"zero_optimization\": {\n>       \"stage\": 2,\n>       \"allgather_partitions\": true,\n>       \"allgather_bucket_size\": 1e9,\n>       \"overlap_comm\": true,\n>       \"reduce_scatter\": true,\n>       \"reduce_bucket_size\": 1e9,\n>       \"contiguous_gradients\": true\n>     },\n>     \"fp16\": {\n>       \"enabled\": \"auto\",\n>       \"auto_cast\": true,\n>       \"loss_scale\": 0,\n>       \"initial_scale_power\": 32,\n>       \"loss_scale_window\": 1000,\n>       \"hysteresis\": 2,\n>       \"min_loss_scale\": 1\n>     },\n>     \"bf16\": {\n>       \"enabled\": \"auto\"\n>     },\n>     \"gradient_accumulation_steps\": \"auto\",\n>     \"gradient_clipping\": \"auto\",\n>     \"train_batch_size\": \"auto\",\n>     \"train_micro_batch_size_per_gpu\": \"auto\",\n>     \"wall_clock_breakdown\": false\n>   }\n>   \n> ```\n> \n> The important settings are allgather_bucket_size and reduce_bucket_size, which affect the communication frequency and budget. Increasing these two values (default is 1e7, recommended is 5e8 or 1e9) will solve the NCCL timeout error. If you are using ZeRO-1 configuration, you may need to apply the same modifications. Hope this can help you solve the problem.\n\nIt works for me！I'm training a video-LLMs with Deepspeed zero3 configuration and i set **reduce_bucket_size** for \"auto\" to 1e9, it's just running without encount the timeout error!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 446,
    "state": "closed",
    "created_by": "n0thing233",
    "created_at": "2023-09-20T20:51:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/446</URL>\n\n<TITLE>[Question] use LLaVA to generate image embeddings for similarity search</TITLE>\n\n<BODY>### Question\n\nHi, \r\nI would like to understand what is the best way to get image embeddings from LLaVA.\r\nShould I just use the last layer, is it the best representative?\r\nThe embeddings will be used to find similar images in very large datasets.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 445,
    "state": "open",
    "created_by": "kanxueli",
    "created_at": "2023-09-20T10:07:45Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/445</URL>\n\n<TITLE>Encounter a “CUDA error: device-side assert triggered” when fine-tuning llama2 on V100</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nWhen I run finetune_qlora.sh on v100 for finetuning llama2 I get CUDA error.  Do you konw how to solve it? Thanks a  lot. @haotian-liu \r\nCommand:\r\n```\r\nHere is configuration of finetune_qlora.sh:\r\ndeepspeed llava/train/train.py \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --lora_enable True \\\r\n    --bits 4 \\\r\n    --model_name_or_path ./checkpoints/$MODEL_VERSION \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path $DATASET_PATH/llava_instruct_80k.json \\\r\n    --image_folder $DATASET_PATH/coco/train2017 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter $MODEL_CHECKPOINTS/llava-pretrain-$MODEL_VERSION/mm_projector.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 False \\\r\n    --fp16 True \\\r\n    --output_dir ./checkpoints/llava-$MODEL_VERSION-finetune_lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 False \\\r\n    --model_max_length 1024 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --dataloader_num_workers 4 \\\r\n```\r\nLog: \r\n```\r\n./aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [47,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [47,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [47,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [47,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [47,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [47,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [47,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [47,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [47,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [47,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [47,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n    train()\r\n  File \"/home/likanxue1/model_inference/LLaVA/llava/train/train.py\", line 910, in train\r\n    trainer.train()\r\n  File \"/home/likanxue1/.custom/cuda11.3.1-cudnn8-devel-ubuntu20.04-py38-jupyter/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/home/likanxue1/.custom/cuda11.3.1-cudnn8-devel-ubuntu20.04-py38-jupyter/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/home/likanxue1/.custom/cuda11.3.1-cudnn8-devel-ubuntu20.04-py38-jupyter/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2654, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/home/likanxue1/.custom/cuda11.3.1-cudnn8-devel-ubuntu20.04-py38-jupyter/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2679, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/home/likanxue1/.custom/cuda11.3.1-cudnn8-devel-ubuntu20.04-py38-jupyter/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/likanxue1/.custom/cuda11.3.1-cudnn8-devel-ubuntu20.04-py38-jupyter/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/home/likanxue1/.custom/cuda11.3.1-cudnn8-devel-ubuntu20.04-py38-jupyter/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1735, in forward\r\n    loss = self.module(*inputs, **kwargs)\r\n  File \"/home/likanxue1/.custom/cuda11.3.1-cudnn8-devel-ubuntu20.04-py38-jupyter/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/likanxue1/.custom/cuda11.3.1-cudnn8-devel-ubuntu20.04-py38-jupyter/envs/llava/lib/python3.10/site-packages/peft/peft_model.py\", line 922, in forward\r\n    return self.base_model(\r\n  File \"/home/likanxue1/.custom/cuda11.3.1-cudnn8-devel-ubuntu20.04-py38-jupyter/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/likanxue1/.custom/cuda11.3.1-cudnn8-devel-ubuntu20.04-py38-jupyter/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/likanxue1/model_inference/LLaVA/llava/model/language_model/llava_llama.py\", line 75, in forward\r\n    input_ids, attention_mask, past_key_values, inputs_embeds, labels = self.prepare_inputs_labels_for_multimodal(input_ids, attention_mask, past_key_values, labels, images)\r\n  File \"/home/likanxue1/model_inference/LLaVA/llava/model/llava_arch.py\", line 140, in prepare_inputs_labels_for_multimodal\r\n    cur_new_labels.append(cur_labels[:image_token_start])\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n```</BODY>\n\n<COMMENTS>\n<Comment by kanxueli at 2023-09-20T13:24:43Z>\nI find that it does not encounter the above error, but the loss is always 0 when I try to change the value of 'mm_use_im_patch_token' from False to True. I noticed that you asked me to make sure that 'mm_use_im_patch_token' is correctly set as False when using these projector weights to instruction tune my LMM.I trying the above because  I wanted the program to run smoothly . And I want to know why the training loss becomes 0 when these flags are set to True during fine-tuning. @haotian-liu\n</Comment>\n<Comment by Chen-Song at 2024-03-27T06:52:53Z>\nI also meet this error of \"“CUDA error: device-side assert triggered” when fine-tuning llama2 on V100\", how to solve it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 444,
    "state": "closed",
    "created_by": "mao-code",
    "created_at": "2023-09-19T17:59:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/444</URL>\n\n<TITLE>[Question] Evaluating ScienceQA with custom LoRA model</TITLE>\n\n<BODY>### Question\r\n\r\nI am fine-tune a custom LoRA model using my custom dataset.\r\nThe model is here: https://huggingface.co/MaoXun/llava-lora-vicuna-7b-v1.3\r\n\r\nWhen I am running this command in \"scripts\" folder\r\n```bash\r\n!python -m llava.eval.model_vqa_science \\\r\n    --model-path /content/lora_model \\\r\n    --question-file /content/ScienceQA/data/scienceqa/llava_test_QCM-LEA.json \\\r\n    --image-folder /content/ScienceQA/data/scienceqa/images/test \\\r\n    --answers-file vqa/results/ScienceQA/test_lora.jsonl \\\r\n    --conv-mode llava_v1\r\n```\r\n\r\nI always got this error\r\n```\r\n[2023-09-19 17:22:16,482] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n2023-09-19 17:22:22.212698: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\nYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\r\nLoading checkpoint shards: 100% 2/2 [00:11<00:00,  5.70s/it]\r\nSome weights of the model checkpoint at /content/lora_model were not used when initializing LlavaLlamaForCausalLM: ['model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias']\r\n- This IS expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n  0% 1/4241 [00:02<3:12:46,  2.73s/it]\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/content/LLaVA/llava/eval/model_vqa_science.py\", line 141, in <module>\r\n    eval_model(args)\r\n  File \"/content/LLaVA/llava/eval/model_vqa_science.py\", line 50, in eval_model\r\n    image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\r\nAttributeError: 'NoneType' object has no attribute 'preprocess'\r\n```\r\n\r\nIt seems that the vision tower weight does not initialized properly.\r\n\r\nP.S.\r\nThis is how I load my lora model. I check that the image_processor is not None.\r\n```python\r\nfrom llava.mm_utils import get_model_name_from_path\r\nfrom llava.model.builder import load_pretrained_model\r\n\r\nlora_model_path = \"MaoXun/llava-lora-vicuna-7b-v1.3\"\r\nmodel_base = \"lmsys/vicuna-7b-v1.3\"\r\nsave_model_path = \"/content/lora_model/\"\r\n\r\nlora_model_name = get_model_name_from_path(lora_model_path)\r\n\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    lora_model_path, \r\n    model_base, \r\n    lora_model_name, \r\n    device_map='auto'\r\n)\r\n\r\nmodel.save_pretrained(save_model_path)\r\ntokenizer.save_pretrained(save_model_path)\r\n```\r\n\r\n\r\nCould anyone help me 🥲</BODY>\n\n<COMMENTS>\n<Comment by mao-code at 2023-09-20T03:19:20Z>\nI just found that I didn't follow the naming rules so when \"load_pretrained_model\" in [here](https://github.com/haotian-liu/LLaVA/blob/26006af0b6aaa8e4516d5c5ecb2b7e4e073bff33/llava/model/builder.py#L26) executes, it will not catch the model correctly.\r\n\r\nThis is what I finally set the model path.\r\n```\r\n/content/checkpoints/checkpoint-llava-10-5-vicuna-7b-v1.3/\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 443,
    "state": "open",
    "created_by": "dydxdt",
    "created_at": "2023-09-19T03:58:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/443</URL>\n\n<TITLE>[Question]  Using the same instruction but get a little different answer when inference.</TITLE>\n\n<BODY>### Question\n\nI train my custom task and use run_llava.py for inference. I use the same instruction each time, but the predicted answer can be a little different for the same image. I doubt whether there are parameters like 'dropout' that I set it True when inferring, but I don't find such parameters. So I wonder what settings can lead to the situations like mine. \r\nThanks for reply!!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 442,
    "state": "open",
    "created_by": "ohharsen",
    "created_at": "2023-09-18T17:19:21Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/442</URL>\n\n<TITLE>[Question] Is there a way to speed up the inference?</TITLE>\n\n<BODY>### Question\n\nTitle says it all. The 13B takes like >10s to finish streaming the answer for an image. That's a little too slow for my use case. Are there any known techniques to get this number down? Feel free to give me pointers, I can look into hacking some solution together myself</BODY>\n\n<COMMENTS>\n<Comment by zjysteven at 2024-05-12T20:56:24Z>\n@ohharsen Hello there, did you manage to work out some tricks to speed up the inference?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 441,
    "state": "closed",
    "created_by": "lw0210",
    "created_at": "2023-09-18T01:39:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/441</URL>\n\n<TITLE>[Discussion] How to run this with a single 3090 GPU?</TITLE>\n\n<BODY>### Discussion\r\n\r\nDear，may I run this with a single 3090 GPU ？ When I run commands python llava/train/train_mem.py it show CUDA out of memory and I have  set up  per_device_train_batch_size=1. Do I have any other ways to lower the GPU requirements？</BODY>\n\n<COMMENTS>\n<Comment by XLionXL at 2023-09-18T06:30:10Z>\nWhat situation is it? Can we train it on 3090?\n</Comment>\n<Comment by lw0210 at 2023-09-18T06:32:23Z>\nYeah，run the sciripts.\r\n\r\n\r\n\r\n\r\n------------------&nbsp;原始邮件&nbsp;------------------\r\n发件人: ***@***.***&gt;; \r\n发送时间: 2023年9月18日(星期一) 下午2:30\r\n收件人: ***@***.***&gt;; \r\n抄送: ***@***.***&gt;; \"State ***@***.***&gt;; \r\n主题: Re: [haotian-liu/LLaVA] [Discussion] How to run this with a single 3090 GPU? (Issue #441)\r\n\r\n\r\n\r\n\r\n\r\n \r\nWhat situation is it? Can we train it on 3090?\r\n \r\n—\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you modified the open/close state.Message ID: ***@***.***&gt;\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 440,
    "state": "open",
    "created_by": "ohhiohhi",
    "created_at": "2023-09-17T08:14:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/440</URL>\n\n<TITLE>[Question] Generated text for caption with llava-llama-2-13b-chat-lightning-preview model is garbled</TITLE>\n\n<BODY>### Question\n\nWhen I use model_vqa.py for caption， the generated text is garbled，i didn't change file model_vqa.py\r\n\r\n**pretrain model**：llava-llama-2-13b-chat-lightning-preview\r\n\r\n**Run the following code**\r\npython -m llava.eval.model_vqa \\\r\n    --model-path \"./llava-llama-2-13b-chat-lightning-preview\" \\\r\n    --question-file ./playground/test/test.jsonl \\\r\n    --image-folder ./llava/serve/examples/ \\\r\n    --answers-file ./playground/test/test_answer.jsonl\r\n\r\nprompt：caption the image in 30 words or less\r\nGenerates results for all images，such as：c. sec: router devgn #\r\n\r\n**At first** \r\nI thought it was a model issue, i tried pretrained model LLaVA-Lightning-MPT-7B-preview，the output is still a mess，\r\n\r\n**After**\r\nI thought it was an environment issue, but when using llama-7b-hf for QA tasks, the output is normal\r\n\r\nNow I don't know why.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 439,
    "state": "closed",
    "created_by": "linhaojia13",
    "created_at": "2023-09-17T03:22:23Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/439</URL>\n\n<TITLE>The training loss in convergence for `llava-Llama-2-13b-chat-hf-pretrain` is larger than 2 [Usage]</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: As shon in this [issue](https://github.com/haotian-liu/LLaVA/issues/62), the training loss in coonvergence should be lower than 2 for `llava-vicuna-chat-hf-pretrain`. However, I run the comnand below for `llava-Llama-2-13b-chat-hf-pretrain` and the training loss in convergence is larger than 2.\r\nHave I do somethining wrong on the training?\r\n\r\nCommand:\r\n```\r\n#!/bin/bash\r\n\r\n# Uncomment and set the following variables correspondingly to run this script:\r\n\r\n# MODEL_VERSION=vicuna-v1-3-7b\r\n# MODEL_VERSION=llama-2-7b-chat\r\nMODEL_VERSION=Llama-2-13b-chat-hf\r\n\r\n########### DO NOT CHANGE ###########\r\n########### USE THIS FOR BOTH ###########\r\nPROMPT_VERSION=plain\r\n########### DO NOT CHANGE ###########\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --model_name_or_path /data/mllm/model_weights/$MODEL_VERSION \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path /data/mllm/LLaVA-CC3M-Pretrain-595K/chat.json \\\r\n    --image_folder /data/mllm/LLaVA-CC3M-Pretrain-595K/images \\\r\n    --vision_tower /data/mllm/model_weights/clip-vit-large-patch14 \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-$MODEL_VERSION-clip-pretrain \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 64 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 24000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n\r\n```\r\n\r\nScreenshots:\r\n![图片](https://github.com/haotian-liu/LLaVA/assets/38275612/a860eef3-76e0-4d39-aa30-3d92827d068f)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-26T20:15:14Z>\nHi, that figure is for v0 checkpoints. For v1, this is completely normal. \r\n\r\nYou may refer to the pretraining logs in: https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#llava-v15\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 438,
    "state": "open",
    "created_by": "gnimyang",
    "created_at": "2023-09-14T15:19:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/438</URL>\n\n<TITLE>After pretrained and finetuning the model, how to handle this model files to successfully load the new model?</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nI already pre-trained and finetuned the model, the files are listed as follows. I am a new starter, sorry I don't know how to load the new model because I saw the names and structure of the files are different from the demo. Thanks a lot! By the way, I use LoRA to fine-tune the model, the LM I used is 7B-1.3 vicuna. I directly change the model_dir=new_lora_finetuned_folder but it was failed.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/130719420/162d6ea9-0176-4e69-bb42-8adc74528aa3)\r\n![image](https://github.com/haotian-liu/LLaVA/assets/130719420/62a18f52-11e2-46bf-8639-a6fd7d9f86f8)\r\n![image](https://github.com/haotian-liu/LLaVA/assets/130719420/77890c1f-dfff-4ecd-931d-2332d1789029)\r\n![image](https://github.com/haotian-liu/LLaVA/assets/130719420/f03e3992-557e-4f19-9a54-d709d9f1e190)\r\nwhat continues next?</BODY>\n\n<COMMENTS>\n<Comment by gnimyang at 2023-09-14T16:14:36Z>\nif I use the script merge_lora_weights.py, how to set the parameters, the model-path=? (llava-vicuna-v1-3-7b-finetune_lora), model-base=?(vicuna-v1-3-7b), but where's the new language model parameter? is model-path LoRA (is non-lora-trainable is the lora weight?) stands for the new language? where is the new projection weights? how to single load projection weight? it quite need long text to explain, thank a lot!\n</Comment>\n<Comment by gnimyang at 2023-09-14T16:20:16Z>\n![image](https://github.com/haotian-liu/LLaVA/assets/130719420/46ed7fa5-0ebc-459b-b39e-2afebc38ab9c)\r\ncan you explain the projection content, and how to change mm_projection, is mm_mlp_projection=mm_projection.bin?\r\nI want to check my understanding that the blue block is --model-base, the orange block is --model-path, right? the projection weight is already merged into the non-lora-trainable.bin?\n</Comment>\n<Comment by basteran at 2023-09-21T10:05:05Z>\nGood questions, I am curious as well. From what I understood:\r\n- the blue block (i.e. the frozen LLM) corresponds to the model-base param\r\n- the orange block (i.e. the tuned LoRA matrices) corresponds to the model-path param\r\n- in your figure there is no projector, but I guess is a MLP applied after the Visual Encoder but before feeding the input to the LLM, is that right? @haotian-liu\n</Comment>\n<Comment by nj159 at 2023-10-21T11:59:42Z>\n> ### 描述问题\r\n> 我已经对模型进行了预训练和微调，文件列出如下。我是新手，抱歉我不知道如何加载新模型，因为我看到文件的名称和结构与演示不同。多谢！顺便说一句， 我使用 LoRA 来微调模型， 我使用的 LM 是 7B-1.3 骆马.我直接更改了 model_dir=new_lora_finetuned_folder但失败了。 ![image](https://user-images.githubusercontent.com/130719420/268045553-162d6ea9-0176-4e69-bb42-8adc74528aa3.png) ![image](https://user-images.githubusercontent.com/130719420/268023351-62a18f52-11e2-46bf-8639-a6fd7d9f86f8.png) ![image](https://user-images.githubusercontent.com/130719420/268023559-77890c1f-dfff-4ecd-931d-2332d1789029.png) ![image](https://user-images.githubusercontent.com/130719420/268023794-f03e3992-557e-4f19-9a54-d709d9f1e190.png)接下来继续什么？\r\n\r\nSorry, I'm new and I see that you have pre-trained and fine-tuned, I would like to ask what is your pre-trained and fine-tuned code? Thank you so much！\n</Comment>\n<Comment by hangzeli05 at 2023-11-30T03:51:17Z>\nI want to know, too.\n</Comment>\n<Comment by XinrunXu at 2024-09-19T04:11:35Z>\nI want to know, too.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 437,
    "state": "open",
    "created_by": "jiyt17",
    "created_at": "2023-09-14T11:50:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/437</URL>\n\n<TITLE>[Question] 🙋 bug about building the model</TITLE>\n\n<BODY>### Question\n\nWhen I run llava.serve.model_worker and llava.serve.cli, there will be an error:\r\nYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\r\nThen it is killed.\r\nHow can I deal with it?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-09-15T01:34:59Z>\n> You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\r\n\r\nThis message is expected, and should not cause process being killed.\r\n\r\nWhat's the CPU/GPU VRAM you have? Do you see \"Killed\" printing at the end of your execution? That is most likely due to CPU OOM.\n</Comment>\n<Comment by jiyt17 at 2023-09-15T05:15:43Z>\nThank you! It should be the CPU OOM that matters. I have fixed it.\n</Comment>\n<Comment by jiyt17 at 2023-09-15T06:30:45Z>\nWhen I build web server, I meet following error:\r\n<img width=\"1013\" alt=\"截屏2023-09-15 14 28 54\" src=\"https://github.com/haotian-liu/LLaVA/assets/41510863/932f43ef-c4bf-44b4-84e3-1182d3a2c9a8\">\r\nHow is it happened?\n</Comment>\n<Comment by haotian-liu at 2023-11-05T00:02:52Z>\nYou may need to set a 127.0.0.1 to it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 436,
    "state": "open",
    "created_by": "sjtu-cz",
    "created_at": "2023-09-14T08:54:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/436</URL>\n\n<TITLE>[Usage] finetune_sqa problem</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nI encountered a problem when training the ScienceQA data set. The same command to train llava_instruct_158k.json can be run successfully, but it is always stuck on the first step on the ScienceQA data set. Do you know what is going on?\r\nCommand:\r\n```\r\nMODEL_VERSION=vicuna-7b-v1.5-16k\r\nPROMPT_VERSION=v1\r\n\r\ndeepspeed llava/train/train.py \\\r\n    --deepspeed ./scripts/zero3_offload.json \\\r\n    --model_name_or_path ${MLLM_DIR}/2.pretrained_models/vicuna/$MODEL_VERSION \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path ${MLLM_DIR}/1.datasets/ScienceQA/data/scienceqa/llava_train_QCM-LEA.json \\\r\n    --image_folder ${MLLM_DIR}/1.datasets/ScienceQA/data/scienceqa/images/train \\\r\n    --vision_tower ${MLLM_DIR}/2.pretrained_models/clip/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ${MLLM_DIR}/3.output_dirs/LLaVA_output/llava-$MODEL_VERSION-pretrain/mm_projector.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ${MLLM_DIR}/3.output_dirs/LLaVA_output/llava-$MODEL_VERSION-finetune-ScienceQA_QCM_LEA-12e-zero3_offload \\\r\n    --num_train_epochs 12 \\\r\n    --per_device_train_batch_size 8 \\ \r\n    --per_device_eval_batch_size 4 \\ \r\n    --gradient_accumulation_steps 2 \\ \r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\ \r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\ \r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\ \r\n    --lazy_preprocess True \\\r\n    --report_to none\r\n```\r\n\r\nLog: \r\n```\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\r\nLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\r\nLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\r\nLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\r\nLoading checkpoint shards:  50%|█████     | 1/2 [00:45<00:45, 45.76s/it]\r\nLoading checkpoint shards:  50%|█████     | 1/2 [00:45<00:45, 45.98s/it]\r\nLoading checkpoint shards:  50%|█████     | 1/2 [00:45<00:45, 45.99s/it]\r\nLoading checkpoint shards:  50%|█████     | 1/2 [00:46<00:46, 46.11s/it]\r\nLoading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 28.03s/it]\r\nLoading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 30.73s/it]\r\nLoading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 28.16s/it]\r\nLoading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 30.85s/it]\r\nLoading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 28.16s/it]\r\nLoading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 30.84s/it]\r\nLoading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 28.14s/it]\r\nLoading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 30.78s/it]\r\nUsing.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\r\nUsing .cache/torch_extensions/py310_cu117 as PyTorch extensions root...\r\nDetected CUDA files, patching ldflags\r\nEmitting ninja build file .cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...\r\nBuilding extension module cpu_adam...\r\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\nUsing cache/torch_extensions/py310_cu117 as PyTorch extensions root...\r\nUsing .cache/torch_extensions/py310_cu117 as PyTorch extensions root...\r\nLoading extension module cpu_adam...\r\nLoading extension module cpu_adam...\r\nLoading extension module cpu_adam...\r\nLoading extension module cpu_adam...\r\n\r\n  0%|          | 0/1188 [00:00<?, ?it/s]\r\n```\r\nI executed the above command on 4 A100 graphics cards. @haotian-liu</BODY>\n\n<COMMENTS>\n<Comment by sjtu-cz at 2023-09-15T07:53:31Z>\nHas anyone encountered a similar situation?\n</Comment>\n<Comment by ChangyuanWang17 at 2023-09-18T02:53:18Z>\nSame problem. Have you solved?\n</Comment>\n<Comment by sjtu-cz at 2023-09-18T06:15:41Z>\n> Same problem. Have you solved?\r\n\r\nNot yet\n</Comment>\n<Comment by simplelifetime at 2023-09-25T08:21:29Z>\nuse Zero2.json instead\n</Comment>\n<Comment by yix-chen at 2023-09-26T07:07:47Z>\nSame problem, any solution except using zero2? @haotian-liu\n</Comment>\n<Comment by haotian-liu at 2023-11-05T00:08:07Z>\n@yix-chen  I think the latest code base should support finetuning SQA, because the latest LLaVA-1.5 mixture also has both pure text and multimodal samples.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 435,
    "state": "closed",
    "created_by": "kanxueli",
    "created_at": "2023-09-14T08:18:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/435</URL>\n\n<TITLE>[Question] don't use FlashAttn</TITLE>\n\n<BODY>### Question\r\n\r\nIs it possible i dont use FlahsAtten to train the model? If possible, what should i do? @haotian-liu</BODY>\n\n<COMMENTS>\n<Comment by kanxueli at 2023-09-14T08:19:23Z>\nBecause I noly have v100.\n</Comment>\n<Comment by guanlaoda at 2023-10-07T04:20:41Z>\nhttps://github.com/haotian-liu/LLaVA/pull/411  with xformers\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 433,
    "state": "closed",
    "created_by": "kanxueli",
    "created_at": "2023-09-13T09:08:59Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/433</URL>\n\n<TITLE>[Usage]  size mismatch for model.mm_projector.weight</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue: when I run the cli.py, I get a err of size mismatch.How can I solve it? I guess maybe it is mismatch model checkpoint. But i config the environment by [reademe.md](https://github.com/haotian-liu/LLaVA/blob/main/README.md) and https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md.    @haotian-liu \r\nCommand:\r\n```\r\npython -m llava.serve.cli  --model-path /model_path/llava-llama-2-7b-chat-lightning-lora-preview  --model-base /model_path/Llama-2-7b-chat-ms  --image-file  \"https://llava-vl.github.io/static/images/view.jpg\"  --load-4bit\r\n```\r\n\r\nLog: \r\n```\r\nraise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for LlavaLlamaForCausalLM: size mismatch for model.mm_projector.weight: copying a param with shape torch.Size([4096, 1024]) from checkpoint, the shape in current model is torch.Size([2097152, 1]).</BODY>\n\n<COMMENTS>\n<Comment by basteran at 2023-09-26T16:22:09Z>\n@kanxueli Have you solved the problem?\n</Comment>\n<Comment by Deaddawn at 2023-10-17T14:16:46Z>\n> ### Describe the issue\r\n> Issue: when I run the cli.py, I get a err of size mismatch.How can I solve it? I guess maybe it is mismatch model checkpoint. But i config the environment by [reademe.md](https://github.com/haotian-liu/LLaVA/blob/main/README.md) and https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md. @haotian-liu Command:\r\n> \r\n> ```\r\n> python -m llava.serve.cli  --model-path /model_path/llava-llama-2-7b-chat-lightning-lora-preview  --model-base /model_path/Llama-2-7b-chat-ms  --image-file  \"https://llava-vl.github.io/static/images/view.jpg\"  --load-4bit\r\n> ```\r\n> \r\n> Log:\r\n> \r\n> ```\r\n> raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\n> RuntimeError: Error(s) in loading state_dict for LlavaLlamaForCausalLM: size mismatch for model.mm_projector.weight: copying a param with shape torch.Size([4096, 1024]) from checkpoint, the shape in current model is torch.Size([2097152, 1]).\r\n> ```\r\n\r\nHi, there. I met the same problem. how do you solve it？\n</Comment>\n<Comment by kanxueli at 2023-10-18T01:31:29Z>\n> d\r\n\r\nremove '--load-4bit' when you run llava.serve.cli\n</Comment>\n<Comment by kanxueli at 2023-10-18T01:31:58Z>\n> > ### Describe the issue\r\n> > Issue: when I run the cli.py, I get a err of size mismatch.How can I solve it? I guess maybe it is mismatch model checkpoint. But i config the environment by [reademe.md](https://github.com/haotian-liu/LLaVA/blob/main/README.md) and https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md. @haotian-liu Command:\r\n> > ```\r\n> > python -m llava.serve.cli  --model-path /model_path/llava-llama-2-7b-chat-lightning-lora-preview  --model-base /model_path/Llama-2-7b-chat-ms  --image-file  \"https://llava-vl.github.io/static/images/view.jpg\"  --load-4bit\r\n> > ```\r\n> > \r\n> > \r\n> > Log:\r\n> > ```\r\n> > raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\n> > RuntimeError: Error(s) in loading state_dict for LlavaLlamaForCausalLM: size mismatch for model.mm_projector.weight: copying a param with shape torch.Size([4096, 1024]) from checkpoint, the shape in current model is torch.Size([2097152, 1]).\r\n> > ```\r\n> \r\n> Hi, there. I met the same problem. how do you solve it？\r\n\r\nremove '--load-4bit' when you run llava.serve.cli\n</Comment>\n<Comment by He-JYang at 2024-09-28T09:47:50Z>\n> > d\r\n> \r\n> remove '--load-4bit' when you run llava.serve.cli\r\n\r\nIt works, thank you!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 431,
    "state": "open",
    "created_by": "jin-qq",
    "created_at": "2023-09-11T14:15:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/431</URL>\n\n<TITLE>[Question]</TITLE>\n\n<BODY>### Question\n\nDear Author\r\nWhen I try to use it in the serve, it raises the error\r\n![image](https://github.com/haotian-liu/LLaVA/assets/140481705/de13c557-5942-4058-8d40-0f58c8b49faa)\r\nI download this folder of weight from HuggingFace\r\n\"llava-llama-2-13b-chat-lightning-preview\"</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 430,
    "state": "closed",
    "created_by": "yichaoshen-MS",
    "created_at": "2023-09-11T12:44:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/430</URL>\n\n<TITLE>[Question]</TITLE>\n\n<BODY>### Question\n\nHow can I get the checkpoint used by the newest demo, whose name is LLaVA-v1-13B-336px?</BODY>\n\n<COMMENTS>\n<Comment by yichaoshen-MS at 2023-09-11T12:45:43Z>\n![image](https://github.com/haotian-liu/LLaVA/assets/112837660/9782ccdf-94f3-4d2a-80ca-72e9e9df3751)\n</Comment>\n<Comment by r3shma at 2023-09-25T17:26:33Z>\nHi @yichaoshen-MS, also have the same question, did you figure it out?\n</Comment>\n<Comment by haotian-liu at 2023-11-05T00:09:29Z>\nAll model checkpoints are released in the MODEL ZOO: https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md\r\n\r\nThanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 429,
    "state": "open",
    "created_by": "gnimyang",
    "created_at": "2023-09-11T10:29:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/429</URL>\n\n<TITLE>Excuse me, I think your pretrain and fine-tuning has no difference in code due to here</TITLE>\n\n<BODY>### Describe the issue\n\n![image](https://github.com/haotian-liu/LLaVA/assets/130719420/97325ffc-01af-43a7-9f7b-7395bec6c422)\r\ntrainable projection weight W in pre-train, and W, f(x) in fine-tuning the code are the same, how do you control the variable in the different training processes?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 428,
    "state": "closed",
    "created_by": "sjtu-cz",
    "created_at": "2023-09-11T07:40:57Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/428</URL>\n\n<TITLE>[Memory err run finetune_full_schedule.sh on V100]</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue: pretrain.sh Can be trained successfully， but finetune_full_schedule.sh, process memory usage exceeded on V100, Is there any way to solve this problem?\r\n\r\nCommand:\r\n```\r\n# Uncomment and set the following variables correspondingly to run this script:\r\n\r\n################## VICUNA ##################\r\n# PROMPT_VERSION=v1\r\n# MODEL_VERSION=\"vicuna-v1-3-7b\"\r\n################## VICUNA ##################\r\n\r\n################## LLaMA-2 ##################\r\n# PROMPT_VERSION=\"llava_llama_2\"\r\n# MODEL_VERSION=\"llama-2-7b-chat\"\r\n################## LLaMA-2 ##################\r\n\r\nMODEL_VERSION=vicuna-7b-v1.5-16k\r\nPROMPT_VERSION=v1\r\n\r\ndeepspeed llava/train/train.py \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --model_name_or_path ${MLLM_DIR}/2.pretrained_models/vicuna/$MODEL_VERSION \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path ${MLLM_DIR}/1.datasets/LLaVA-Instruct-150K/llava_instruct_150k.json \\\r\n    --image_folder ${MLLM_DIR}/1.datasets/COCO/train2017 \\\r\n    --vision_tower ${MLLM_DIR}/2.pretrained_models/clip/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ${MLLM_DIR}/3.output_dirs/LLaVA_output/llava-$MODEL_VERSION-pretrain/mm_projector.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 False \\\r\n    --output_dir ${MLLM_DIR}/3.output_dirs/LLaVA_output/llava-$MODEL_VERSION-finetune \\\r\n    --num_train_epochs 3 \\ \r\n    --per_device_train_batch_size 8 \\ \r\n    --per_device_eval_batch_size 4 \\ \r\n    --gradient_accumulation_steps 2 \\ \r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\ \r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\ \r\n    --tf32 False \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\ \r\n    --lazy_preprocess True \\\r\n    --report_to none\r\n```\r\n\r\nLog: \r\n```\r\n    trainer.train()\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1656, in _inner_training_loop\r\n    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1198, in prepare\r\n    result = self._prepare_deepspeed(*args)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1537, in _prepare_deepspeed\r\n    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/deepspeed/__init__.py\", line 165, in initialize\r\n    engine = DeepSpeedEngine(args=args,\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 309, in __init__\r\n    self._configure_optimizer(optimizer, model_parameters)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1184, in _configure_optimizer\r\n    self.optimizer = self._configure_zero_optimizer(basic_optimizer)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1419, in _configure_zero_optimizer\r\n    optimizer = DeepSpeedZeroOptimizer(\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 372, in __init__\r\n    dist.barrier()\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/deepspeed/comm/comm.py\", line 116, in log_wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/deepspeed/comm/comm.py\", line 394, in barrier\r\n    return cdb.barrier(group=group, async_op=async_op)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/deepspeed/comm/torch.py\", line 225, in barrier\r\n    return torch.distributed.barrier(group=group, async_op=async_op, device_ids=device_ids)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 3330, in barrier\r\n    work = group.barrier(opts=opts)\r\nRuntimeError: [7] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Broken pipe. This may indicate a possible application crash on rank 0 or a network set up issue.\r\nTraceback (most recent call last):\r\n```\r\n\r\n<img width=\"836\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/20858982/ba089da9-96ac-4340-9327-c63b20b97243\"></BODY>\n\n<COMMENTS>\n<Comment by guanlaoda at 2023-10-07T04:21:57Z>\nhttps://github.com/haotian-liu/LLaVA/pull/411 with xformers on v100\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 427,
    "state": "closed",
    "created_by": "heylamourding",
    "created_at": "2023-09-11T04:09:45Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/427</URL>\n\n<TITLE>[Question] Can LLava based on LLama 2 LLM be used for commercial purposes?</TITLE>\n\n<BODY>### Question\n\nAs mentioned in the doc https://github.com/haotian-liu/LLaVA/blob/main/docs/LLaVA_from_LLaMA2.md, \r\n\r\n> 🦙 -Introduction- [Llama 2 is an open-source LLM released by Meta AI](https://about.fb.com/news/2023/07/llama-2/) today (July 18, 2023). Compared with its early version [Llama 1](https://ai.meta.com/blog/large-language-model-llama-meta-ai/), Llama 2 is more favored in stronger language performance, longer context window, and importantly commercially usable!\r\n\r\nCan LLava based on LLama2 LLM be used for commercial purposes?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-24T01:01:26Z>\nThey are supported with LLaMA2 community license.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 426,
    "state": "closed",
    "created_by": "LiqiangJing",
    "created_at": "2023-09-08T15:00:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/426</URL>\n\n<TITLE>[Question] What images were used?</TITLE>\n\n<BODY>### Question\n\nCOCO provides lots of datasets in terms of different versions. I want to know which dataset is utilized. Where can I download the caption and bounding box files? Please provide guidance If you are free.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-24T01:58:13Z>\nHi, please check out the data here: https://github.com/haotian-liu/LLaVA#visual-instruction-tuning\r\n\r\nThanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 425,
    "state": "open",
    "created_by": "uestcMeng",
    "created_at": "2023-09-08T06:40:12Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/425</URL>\n\n<TITLE>OOM on A800 80G without flash-atten</TITLE>\n\n<BODY>### Question\n\nI try to pretrain the llama-7B, I use A800 with 80G, when I training model with a single A800, it works well. However, I train with 4 A800, it encounters OOM, even if set batch_per_device=7, it stills be OOM. Could tell me  how to sovle it?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-17T19:03:23Z>\nPlease try with the Deepspeed scripts:\r\n\r\nLLaVA: https://github.com/haotian-liu/LLaVA/blob/main/scripts/pretrain.sh\r\nLLaVA-v1.5: https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/pretrain.sh\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 424,
    "state": "closed",
    "created_by": "xiaomin418",
    "created_at": "2023-09-07T07:20:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/424</URL>\n\n<TITLE>[Usage]</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: error when run : python -m llava.serve.cli \r\n\r\nCommand:\r\n```\r\npython -m llava.serve.cli \\\r\n>  --model-path liuhaotian/LLaVA-Lightning-MPT-7B-preview \\\r\n> \r\n```\r\n\r\nLog: \r\n```\r\nusage: cli.py [-h] [--model-name MODEL_NAME] [--num-gpus NUM_GPUS] [--device {cuda,cpu}] [--conv-template CONV_TEMPLATE]\r\n              [--temperature TEMPERATURE] [--max-new-tokens MAX_NEW_TOKENS] [--debug]\r\ncli.py: error: unrecognized arguments: --model-path liuhaotian/LLaVA-Lightning-MPT-7B-preview\r\n```\r\n\r\nScreenshots:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/25497369/2b6b6452-b0cc-4fc6-ab9b-04042921b0c8)\r\n\r\nWhether the version of “llava.serve.cli.py” has changed？The command in the Readme.md is inconsistent with the existing version parameter setting.</BODY>\n\n<COMMENTS>\n<Comment by xiaomin418 at 2023-09-11T01:45:23Z>\nits my problem.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 423,
    "state": "open",
    "created_by": "Remwlp",
    "created_at": "2023-09-06T07:46:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/423</URL>\n\n<TITLE>[Question] How can I do Multi-turn conversation evaluation?</TITLE>\n\n<BODY>### Question\n\nI notice that eval code for ScienceQA only support single turn QA, but I want to evaluate on Multi-turn conversation task. \r\nHow can I get multi-turn response in evaluation stage?</BODY>\n\n<COMMENTS>\n<Comment by jameszhou-gl at 2023-11-10T12:34:51Z>\nHi @Remwlp , I'm also performing some evaluations on SceinceQA. What do you mean multi-turn conversation evaluations, like few-shot in LLMs?\n</Comment>\n<Comment by fisher75 at 2024-03-25T10:21:21Z>\nI am in great need of a multi-dialog feature for batch-inference with SGLang.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 422,
    "state": "closed",
    "created_by": "John-Ge",
    "created_at": "2023-09-06T07:30:27Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/422</URL>\n\n<TITLE>[Question] On generation hyper-parameters.</TITLE>\n\n<BODY>### Question\n\nHello haotian! Thank you for your great work! I recently find that using a pretrained llava to genarate answer and evaluate on some benchmark is a little unstable. Llava generate different answers with same hyper-parameters. \r\nYou mentioned that this may come from do_sample=True. I would like to know what is the hyper-parameters you use in generating answers. Top_k, Top_q, termperature. What is the recommanded hyper-parameters?\r\nAnd I observed that you use beams search in generation process. Would you like to release the code for beams search?</BODY>\n\n<COMMENTS>\n<Comment by John-Ge at 2023-09-06T09:40:59Z>\nI use beam search with temperature = 0.2 in your model_vqa.py. But it returns an error. Maybe the default temperature is too large for beams = 5. Could you please tell us the hyper-parameters you use for evalutaion? Thank you!\n</Comment>\n<Comment by haotian-liu at 2023-11-05T00:04:51Z>\nHi, when using beam search, we set the temperature to 1.0\n</Comment>\n<Comment by John-Ge at 2023-11-05T12:37:19Z>\nThank you!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 421,
    "state": "open",
    "created_by": "marmelade500",
    "created_at": "2023-09-05T20:56:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/421</URL>\n\n<TITLE>[Feature request] Please include a model that is for commercial use.</TITLE>\n\n<BODY>### feature\n\nPlease include a model that is for commercial use.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 420,
    "state": "open",
    "created_by": "xmy0916",
    "created_at": "2023-09-05T10:53:45Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/420</URL>\n\n<TITLE>[Question] Missing key(s) in state_dict: \"lora_A.default.weight\", \"lora_B.default.weight\".</TITLE>\n\n<BODY>### Question\n\nwhen i am running finetune_lora.sh with llama2, i got this error.\r\n```python\r\nFile \"/opt/tiger/workspace/llava-video/llava/model/llava_arch.py\", line 69, in initialize_vision_modules\r\n    self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'))\r\n  File \"/home/tiger/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2041, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for Linear:\r\n        Missing key(s) in state_dict: \"lora_A.default.weight\", \"lora_B.default.weight\".\r\n```\r\nmaybe a bug? \r\nsolution:\r\nself.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'), strict=False)?</BODY>\n\n<COMMENTS>\n<Comment by 1106301825 at 2023-09-13T08:43:50Z>\nDo you solve this problem? @xmy0916\n</Comment>\n<Comment by xmy0916 at 2023-09-15T07:02:22Z>\n@1106301825 I have given the solution on the above comment.\r\nsolution:\r\nself.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'), strict=False)\n</Comment>\n<Comment by basteran at 2023-09-19T17:39:54Z>\n@xmy0916 I don't think yours is a viable solution, as I get this error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/c.hromei/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/home/c.hromei/LLaVA/llava/train/train.py\", line 853, in train\r\n    model.get_model().initialize_vision_modules(\r\n  File \"/home/c.hromei/LLaVA/llava/model/llava_arch.py\", line 69, in initialize_vision_modules\r\n    self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'), strict=False)\r\n  File \"/home/c.hromei/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2041, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for Linear:\r\n        size mismatch for weight: copying a param with shape torch.Size([5120, 1024]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for bias: copying a param with shape torch.Size([5120]) from checkpoint, the shape in current model is torch.Size([0]).\r\n```\n</Comment>\n<Comment by xmy0916 at 2023-09-20T07:18:23Z>\n@basteran i guess you run the script \"finetine_qlora.sh\" with llama 2? I have got the same error as you, i just skip loading params for this layer and it seems training with no problem. By the way i have try \"finetune_lora.sh\" and \"finetune_qlora.sh\" but the performance seems not so good on my dataset.\n</Comment>\n<Comment by basteran at 2023-09-20T08:34:45Z>\n@xmy0916 Yes, I run the script \"finetune_lora.sh\" using the Vicuna LLM. I can't figure out what's the point of those params and how to skip them eventually.\n</Comment>\n<Comment by xmy0916 at 2023-09-20T09:33:14Z>\n@basteran you can check out the params' name in the adapter_model.bin and model, modify the name of checkpoint state_dict from 'XXXX.lora_A.weight' to 'XXXX.lora_A.default.weight' maybe can help.\n</Comment>\n<Comment by xmy0916 at 2023-09-20T09:43:50Z>\n@basteran Sorry, I didn’t read your error carefully. You have a shape mismatch, which makes it impossible to load parameters. I feel that the code published by the author may be inconsistent with the parameters. You can try to delete this layer of parameters from ckpt, but it will definitely affect lora's performance.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 419,
    "state": "closed",
    "created_by": "euanong",
    "created_at": "2023-09-04T23:22:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/419</URL>\n\n<TITLE>[Question] Is it possible to host a LLaVA-2 demo?</TITLE>\n\n<BODY>### Question\n\nI'd love to be able to play around with LLaVA-2 on https://llava.hliu.cc/ -- might it be possible to host LLaVA-2 there?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-05T00:06:11Z>\nHi, due to the resource limit, we can only host a limited set of the model checkpoints on the web demo.\r\n\r\nYou may try running it on HF Space or the free Colab : https://github.com/camenduru/LLaVA-colab\r\n\r\nThanks.\n</Comment>\n<Comment by haotian-liu at 2023-11-05T00:06:30Z>\nOh also, LLaVA-2 is not released yet :)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 417,
    "state": "open",
    "created_by": "TikaToka",
    "created_at": "2023-09-04T16:49:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/417</URL>\n\n<TITLE>[Question] Possibility of using LLaVA in Python 3.6?</TITLE>\n\n<BODY>### Question\n\nHellu, @haotian-liu, Thank you for sharing your amazing work!\r\n\r\nIs there a way to use LLaVA in Python3.6 as It requires 3.8 as an minimum, or run it using huggingface pipeline?\r\nI am conducting a experiment, but the baseline code is based on python 3.6, so adapting it is super difficult. (Env compatibility problem + code compatibility problem)\r\n\r\nThank you in advance!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-09-04T16:52:15Z>\nDo you need training? If you just need inference, you can launch llava worker in one of the env, and interact with LLaVA using http request, like what we do in [gradio demo](https://github.com/haotian-liu/LLaVA/blob/main/llava/serve/gradio_web_server.py#L209)\n</Comment>\n<Comment by TikaToka at 2023-09-06T15:33:40Z>\nThank you for quick response.\r\n\r\nI am trying to adapt the code block you referenced to my baseline, but I have some curiosity with a code as i am not familiary with gradio.. :(\r\n\r\n\r\nbefore utilizing 'http_bot' to pass inputs by request, do we need to use \"add_text\" to pass inputs(text + image) to the state?\r\n\r\nor just passing pload['images'] = rgb_image and pload['prompt']=prompt might work? \r\n\r\nfor clarity, i post the code for former one that WIP.\r\n\r\n```\r\n        headers = {\"User-Agent\": \"LLaVA Client\"}\r\n\r\n        rgb = Image.open(observations['rgb']).convert('RGB')\r\n        depth = Image.open(observations['depth']).convert('L')\r\n        instruction = self.inst2step(observations['instruction'])\r\n\r\n        text = self.prompt_generator(instruction)\r\n\r\n        text = text[:1536]  # Hard cut-off\r\n        if image is not None:\r\n            text = text[:1200]  # Hard cut-off for images\r\n            if '<image>' not in text:\r\n                # text = '<Image><image></Image>' + text\r\n                text = text + '\\n<image>'\r\n            text = (text, image, 'Resize')\r\n            if len(self.state.get_images(return_pil=True)) > 0:\r\n                self.state = default_conversation.copy()\r\n        self.state.append_message(self.state.roles[0], text)\r\n        self.state.append_message(self.state.roles[1], None)\r\n        self.state.skip_next = False\r\n\r\n        prompt = self.state.get_prompt()\r\n\r\n        all_images = self.state.get_images(return_pil=True)\r\n        all_image_hash = [hashlib.md5(image.tobytes()).hexdigest() for image in all_images]\r\n\r\n        for image, hash in zip(all_images, all_image_hash):\r\n            t = datetime.datetime.now()\r\n            filename = os.path.join(\r\n                LOGDIR, \"serve_images\", f\"{t.year}-{t.month:02d}-{t.day:02d}\", f\"{hash}.jpg\")\r\n            if not os.path.isfile(filename):\r\n                os.makedirs(os.path.dirname(filename), exist_ok=True)\r\n                image.save(filename)\r\n\r\n        # Make requests\r\n        pload = {\r\n            \"model\": self.model_name,\r\n            \"prompt\": prompt,\r\n            \"temperature\": float(0.2),\r\n            \"top_p\": float(top_p),\r\n            \"max_new_tokens\": min(int(1200), 1536),\r\n            \"stop\": self.state.sep if self.state.sep_style in [SeparatorStyle.SINGLE, SeparatorStyle.MPT] else self.state.sep2,\r\n            \"images\": f'List of {len(self.state.get_images())} images: {all_image_hash}',\r\n\r\n        }\r\n\r\n        pload['images'] = state.get_images()\r\n\r\n        self.self.state.messages[-1][-1] = \"▌\"\r\n        yield (self.state, self.state.to_gradio_chatbot())\r\n        \r\n          try:\r\n              response = requests.post(self.worker_addr + \"/worker_generate_stream\", headers=headers, json=pload, stream=True, timeout=10)\r\n              for chunk in response.iter_lines(decode_unicode=False, delimiter=b\"\\\\0\"):\r\n                  if chunk:\r\n                      data = json.loads(chunk.decode())\r\n                      if data[\"error_code\"] == 0:\r\n                          output = data[\"text\"][len(prompt):].strip()\r\n                          res = self.process_output(output)\r\n                          return res['action']\r\n                      else:\r\n                          return f\"Error: {data['text']} (error_code: {data['error_code']})\"\r\n          except requests.exceptions.RequestException as e:\r\n              return \"Server error.\"\r\n```\n</Comment>\n<Comment by haotian-liu at 2023-09-06T16:02:25Z>\nYou can generate the prompt as usual (no need to follow the interactive gradio logic). You may find reference [here](https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/run_llava.py#L56).\r\n\r\nThen, make sure that everything in `pload` is ready, and submit the http request. Let the model_worker do the job, and it will return the result in output.\n</Comment>\n<Comment by TikaToka at 2023-09-30T12:23:18Z>\nFound out that there's a gpu error, look like same error with  #464, and I posted detailed in error information there.\r\n \r\n How can I handle this?\r\n\r\n2023-09-30 12:10:42 | INFO | stdout | Ca\r\nught Unknown Error CUDA error: device-si\r\nde assert triggered\r\n\r\ncurrent code\r\n'''\r\n    def load_image(self, image_file):\r\n        import base64\r\n        from io import BytesIO\r\n        image = image_file\r\n        image = image.resize((336, 336))\r\n        buffered = BytesIO()\r\n        image.save(buffered, format=\"PNG\")\r\n        img_b64_str = base64.b64encode(buffered.getvalue()).decode()\r\n        return img_b64_str\r\n\r\n        qs = self.get_prompt(instruction, history)\r\n        qs = DEFAULT_IMAGE_TOKEN + '\\n' + qs\r\n\r\n        state = states[j]\r\n        state.append_message(state.roles[0], qs)\r\n        state.append_message(state.roles[1], None)\r\n\r\n\r\n\r\n\r\n        prompt = state.get_prompt()\r\n\r\n\r\n        # pload['images'] = state.get_images()\r\n\r\n        # state.messages[-1][-1] = \"▌\"\r\n\r\n        # Make requests\r\n\r\n        image = self.load_image(ToPILImage(mode='RGB')(rgb))\r\n\r\n        pload = {\r\n            \"model\": self.model_name,\r\n            \"prompt\": prompt,\r\n            \"temperature\": float(0.2),\r\n            \"top_p\": float(0.7),\r\n            \"max_new_tokens\": 1536,\r\n            \"stop\": state.sep if state.sep_style in [SeparatorStyle.SINGLE, SeparatorStyle.MPT] else state.sep2,\r\n            \"images\": image,\r\n        }\r\n\r\n        pload['images'] = state.get_images()\r\n\r\n\r\n        try:\r\n            # Stream output\r\n            response = requests.post(self.worker_addr + \"/worker_generate_stream\",\r\n                                     headers=self.headers, json=pload, stream=True, timeout=100)\r\n            for chunk in response.iter_lines(decode_unicode=False, delimiter=b\"\\0\"):\r\n                if chunk:\r\n                    data = json.loads(chunk.decode())\r\n                    if data[\"error_code\"] == 0:\r\n                        output = data[\"text\"][len(prompt):].strip()\r\n                        print(output, '@@@@@@@@@@@@@@@@@@@@@@@@')\r\n                        output = self.process_output(output)\r\n                        action = output['Action']\r\n                        if action == 'STOP':\r\n                            action = 0\r\n                        else:\r\n                            img_idxes = action.split(' ')[1]\r\n                            action = 1\r\n                            distances = 0.25  # TODO mLLM이 예측할 수 있을 것인 가?\r\n                        state.messages[-1][-1] = output + \"▌\"\r\n                        history[j] = {\r\n                            \"action\": action, \"thought\": output['Thought'], \"step\": output['Step']}\r\n                    else:\r\n                        output = data[\"text\"] + \\\r\n                            f\" (error_code: {data['error_code']})\"\r\n                        print(output)\r\n\r\n                    time.sleep(0.03)\r\n        except requests.exceptions.RequestException as e:\r\n            print(e)\r\n\r\n        cand_actions.append(action)\r\n'''\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 416,
    "state": "open",
    "created_by": "linhaojia13",
    "created_at": "2023-09-04T16:27:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/416</URL>\n\n<TITLE>[Question] model_max_length=2048 is too long in the pretraining stage?</TITLE>\n\n<BODY>### Question\n\nThe caption of the cc3m is relatively short, maybe model_max_length=512 is enough. Do you think so?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-09-04T16:49:56Z>\nIn our tokenizer, we pad to the max sequence length in the batch and do not pad to model max length. Therefore, I feel that theoretically decreasing `model_max_length` won't make a difference.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 415,
    "state": "closed",
    "created_by": "Cubism-star",
    "created_at": "2023-09-03T13:07:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/415</URL>\n\n<TITLE>[Usage] Try to launch a controller but failed.</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.controller --host 0.0.0.0 --port 10000\r\n```\r\n\r\nLog: \r\n```\r\n2023-09-03 13:06:49 | INFO | controller | args: Namespace(host='0.0.0.0', port=10000, dispatch_method='shortest_queue')\r\n2023-09-03 13:06:49 | INFO | controller | Init controller\r\n2023-09-03 13:06:49 | ERROR | stderr | INFO:     Started server process [76682]\r\n2023-09-03 13:06:49 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2023-09-03 13:06:49 | ERROR | stderr | INFO:     Application startup complete.\r\n2023-09-03 13:06:49 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:10000 (Press CTRL+C to quit)\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/57119650/3c6ec5f6-fbaf-4e6a-b78c-7af78224a94a)</BODY>\n\n<COMMENTS>\n<Comment by Cubism-star at 2023-09-03T13:12:10Z>\nmy machine is Ubuntu20.04 with Nvidia A10 gpu\n</Comment>\n<Comment by lw0210 at 2023-09-11T10:43:43Z>\nMy mistake is the same as yours. Have you resolved your bug？ @Cubism-star\n</Comment>\n<Comment by Cubism-star at 2023-09-11T10:45:45Z>\n> My mistake is the same as yours. Have you resolved your bug？ @Cubism-star\r\n\r\nJust run the next command. This problem is not actually an error.\n</Comment>\n<Comment by lw0210 at 2023-09-11T10:53:18Z>\nThanks！But I run the next command displaying valuerror.May I know your email address? Would it be convenient to add your WeChat account. @Cubism-star\n</Comment>\n<Comment by nj159 at 2023-09-20T03:13:29Z>\n![image](https://github.com/haotian-liu/LLaVA/assets/144583677/e775f586-a1d9-4fbb-8ed0-641c289659c0)\r\n在第三个终端运行python3 -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./checkpoints/LLaVA-7B-v0，出现图片所示的错误，是出现什么问题了呢？\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 414,
    "state": "open",
    "created_by": "xmy0916",
    "created_at": "2023-09-01T18:22:50Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/414</URL>\n\n<TITLE>[Question] question about some code.</TITLE>\n\n<BODY>### Question\n\n[https://github.com/haotian-liu/LLaVA/blob/main/llava/model/llava_arch.py#L111](https://github.com/haotian-liu/LLaVA/blob/main/llava/model/llava_arch.py#L111)\r\n\r\n```python\r\ncur_input_embeds = cur_input_embeds + (0. * self.get_model().mm_projector(vision_tower.dummy_feature)).sum()\r\n```\r\n\r\nit seems nothing happend?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 412,
    "state": "closed",
    "created_by": "xmy0916",
    "created_at": "2023-09-01T11:26:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/412</URL>\n\n<TITLE>[Discussion] Which of these scripts is more effective?</TITLE>\n\n<BODY>### Discussion\n\n![image](https://github.com/haotian-liu/LLaVA/assets/43675899/59f82478-24c7-4707-a967-09bfe2738329)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 410,
    "state": "closed",
    "created_by": "heylamourding",
    "created_at": "2023-08-31T07:31:55Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/410</URL>\n\n<TITLE>[Usage] Fail to load llava-llama-2-7b-chat-lightning-lora-preview KeyError: 'LlavaConfig'</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(model_path = 'liuhaotian/llava-llama-2-7b-chat-lightning-lora-preview', \r\n                     model_base = 'liuhaotian/llava-llama-2-7b-chat-lightning-lora-preview',\r\n                     model_name = 'llava-llama-2-7b-chat-lightning-lora-preview')\r\n```\r\n\r\nLog: \r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\nCell In[3], line 1\r\n----> 1 tokenizer, model, image_processor, context_len = load_pretrained_model(model_path = 'liuhaotian/llava-llama-2-7b-chat-lightning-lora-preview', \r\n      2                      model_base = 'liuhaotian/llava-llama-2-7b-chat-lightning-lora-preview',\r\n      3                      model_name = 'llava-llama-2-7b-chat-lightning-lora-preview')\r\n\r\nFile ~/jupyter_notebooks/LLaVA/llava/model/builder.py:45, in load_pretrained_model(model_path, model_base, model_name, load_8bit, load_4bit, device_map)\r\n     43 if 'lora' in model_name.lower() and model_base is not None:\r\n     44     lora_cfg_pretrained = AutoConfig.from_pretrained(model_path)\r\n---> 45     tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n     46     print('Loading LLaVA from base model...')\r\n     47     model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs)\r\n\r\nFile ~/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:718, in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)\r\n    716 model_type = config_class_to_model_type(type(config).__name__)\r\n    717 if model_type is not None:\r\n--> 718     tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]\r\n    719     if tokenizer_class_fast and (use_fast or tokenizer_class_py is None):\r\n    720         return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\r\n\r\nFile ~/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:663, in _LazyAutoMapping.__getitem__(self, key)\r\n    661 if key in self._extra_content:\r\n    662     return self._extra_content[key]\r\n--> 663 model_type = self._reverse_config_mapping[key.__name__]\r\n    664 if model_type in self._model_mapping:\r\n    665     model_name = self._model_mapping[model_type]\r\n\r\nKeyError: 'LlavaConfig'\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-09-01T16:46:08Z>\nPlease check out the updated [instruction](https://github.com/haotian-liu/LLaVA#launch-a-model-worker-lora-weights-unmerged) for loading lora weights. You need to pass a corresponding `--model-base`, and in your case `llama-2-7b-chat`.\n</Comment>\n<Comment by zhangyupeng123 at 2023-09-02T15:10:40Z>\n请问可以生成类似“liuhaotian/llava-llama-2-13b-chat-lightning-preview”的，7b版本的文件吗？\n</Comment>\n<Comment by heylamourding at 2023-09-03T01:06:15Z>\nHi @haotian-liu thanks for your reply! \r\n\r\nI tried to send \r\n`python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-llama-2-7b-chat-lightning-lora-preview --model-base llama-2-7b-chat`\r\n\r\nI got following errors: \r\n` OSError: llama-2-7b-chat is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'`\r\n\r\nMay I know what value should I put under **model-base**?\r\n\r\nAlternatively, is it possible to generate liuhaotian/llava-llama-2-7b-chat-lightning-preview?\r\n\r\nThanks!\n</Comment>\n<Comment by zhangyupeng123 at 2023-09-03T07:22:26Z>\nHi @haotian-liu , Can we generate a 7b version file like “liuhaotian/llava-llama-2-13b-chat-lightning-preview” by ourselves?\n</Comment>\n<Comment by heylamourding at 2023-09-03T10:19:13Z>\nUpdated: error is solved. Need to get llama2 access in HF first. Then \r\n\r\n```\r\npython merge_lora_weights.py --model-path liuhaotian/llava-llama-2-7b-chat-lightning-lora-preview --model-base meta-llama/Llama-2-7b-chat-hf --save-model-path ./checkpoints/llava-7b-llama-2-7b-chat\r\n```\r\n\r\nUse ./checkpoints/llava-7b-llama-2-7b-chat in inference.\n</Comment>\n<Comment by zhangyupeng123 at 2023-09-03T15:48:53Z>\nHi @heylamourding ,I have got the llava-2 access in HF, but how do I use it on the server?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 409,
    "state": "open",
    "created_by": "simplelifetime",
    "created_at": "2023-08-31T03:39:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/409</URL>\n\n<TITLE>[Question] How to fine-tune LLaVA after stage2</TITLE>\n\n<BODY>### Question\n\nIt seems that the stage2 fine-tune requires pretrain_mm_mlp_adapter. However, if I want to fine-tune the stage2 fine-tuned models, I can not reuse the stage2 fine-tune script because there are no pretrain_mm_mlp_adapter in the fine-tuned models. What is the correct way to fine-tune the stage2 fine-tuned model(the final model)?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-08-31T03:41:53Z>\nIn this case, you can omit the option in the command line, and set pretrained model to the finetuned stage 2 checkpoint:\r\n\r\n```\r\n--model_name_or_path /path/to/stage2\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 408,
    "state": "open",
    "created_by": "DefUs3r",
    "created_at": "2023-08-30T18:42:39Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/408</URL>\n\n<TITLE>[Usage] LoRA finetuned weights provided for vicuna-13b-v1.3 gives NaN / inf error when performing inference on COCO-2014 questions after merging LoRA weights</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nWe are trying to perform inference on the LoRA weights provided for vicuna-13b-v1.3 [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#model-zoo). As mentioned by @haotian-liu in issue #245, we performing the merging step on the LoRA weights using the following command:\r\n\r\n```\r\npython merge_lora_weights.py \\\r\n    --model-path hf_checkpoints/llava-v1-0719-336px-lora-vicuna-13b-v1.3 \\\r\n    --model-base LLaVA/checkpoints/fastchat_llama-vicuna-v1-3-13b \\\r\n    --save-model-path hf_checkpoints/llava-v1-0719-336px-lora-vicuna-13b-v1.3-MERGE\r\n```\r\n\r\nAfter this, we perform the inference on 90 samples of COCO-2014 as mentioned in the paper using:\r\n```\r\npython -m llava.eval.model_vqa \\\r\n    --model-path hf_checkpoints/llava-v1-0719-336px-lora-vicuna-13b-v1.3-MERGE \\\r\n    --question-file \\\r\n    LLaVA/playground/data/coco2014_val_qa_eval/qa90_questions.jsonl \\\r\n    --image-folder \\\r\n    LLaVA/coco/coco_dataset/val2014 \\\r\n    --answers-file \\\r\n    LLaVA/model_inference_testing/coco/coco_val2014_answers-HF-vicuna-v1-3-13b-prompt-v1-test-merge.jsonl\r\n```\r\n\r\nThis inference gives the following Error Log : \r\n```\r\n  0%|                                                                                                                                         | 0/90 [00:00<?, ?it/s]/home/anaconda3/envs/llavacuda6/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\r\n  warnings.warn(\r\n  0%|                                                                                                                                         | 0/90 [00:33<?, ?it/s]\r\nTraceback (most recent call last):\r\n  File \"/home/anaconda3/envs/llavacuda6/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/anaconda3/envs/llavacuda6/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/workspace/cgy/LLAVA/LLaVA/llava/eval/model_vqa.py\", line 112, in <module>\r\n    eval_model(args)\r\n  File \"/home/workspace/cgy/LLAVA/LLaVA/llava/eval/model_vqa.py\", line 66, in eval_model\r\n    output_ids = model.generate(\r\n  File \"/home/anaconda3/envs/llavacuda6/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/anaconda3/envs/llavacuda6/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1588, in generate\r\n    return self.sample(\r\n  File \"/home/anaconda3/envs/llavacuda6/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2678, in sample\r\n    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n```\r\n\r\nThe python code we use to generate our `model-base` in `merge_lora_weights.py` is as follows : \r\n\r\n```\r\npython -m fastchat.model.apply_delta \\\r\n    --base huggyllama/llama-13b \\\r\n    --target checkpoints/fastchat_llama-vicuna-v1-3-13b \\\r\n    --delta lmsys/vicuna-13b-v1.3\r\n```\r\n\r\nInterestingly, the same procedure when done for the `LoRA-Merged` [weights](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#model-zoo) returns :\r\n```\r\nall : 76.3\r\ncomplex : 90.0\r\nconv : 75.4\r\ndetail : 63.4\r\n```\r\nimplying that `merge_lora_weights.py` either has some issue, or the provided `LoRA` [weights](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#model-zoo) have some issue, or the `model-base` is faulty. \r\n\r\nKindly suggest fixes for whatever is the reason for this error.</BODY>\n\n<COMMENTS>\n<Comment by wanghao-cst at 2023-09-13T01:32:54Z>\nI got the same error through the preview lora inference steps. [link](https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md)\r\n<img width=\"1041\" alt=\"截屏2023-09-13 09 32 28\" src=\"https://github.com/haotian-liu/LLaVA/assets/55015183/4067890a-e9f9-4f30-80cd-75b283db61e9\">\n</Comment>\n<Comment by Cubism-star at 2023-09-15T01:54:28Z>\nI also got the same error when using my own fine-tuned model to inference.\n</Comment>\n<Comment by wanghao-cst at 2023-09-21T02:41:13Z>\n> ### Describe the issue\r\n> Issue:\r\n> \r\n> We are trying to perform inference on the LoRA weights provided for vicuna-13b-v1.3 [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#model-zoo). As mentioned by @haotian-liu in issue #245, we performing the merging step on the LoRA weights using the following command:\r\n> \r\n> ```\r\n> python merge_lora_weights.py \\\r\n>     --model-path hf_checkpoints/llava-v1-0719-336px-lora-vicuna-13b-v1.3 \\\r\n>     --model-base LLaVA/checkpoints/fastchat_llama-vicuna-v1-3-13b \\\r\n>     --save-model-path hf_checkpoints/llava-v1-0719-336px-lora-vicuna-13b-v1.3-MERGE\r\n> ```\r\n> \r\n> After this, we perform the inference on 90 samples of COCO-2014 as mentioned in the paper using:\r\n> \r\n> ```\r\n> python -m llava.eval.model_vqa \\\r\n>     --model-path hf_checkpoints/llava-v1-0719-336px-lora-vicuna-13b-v1.3-MERGE \\\r\n>     --question-file \\\r\n>     LLaVA/playground/data/coco2014_val_qa_eval/qa90_questions.jsonl \\\r\n>     --image-folder \\\r\n>     LLaVA/coco/coco_dataset/val2014 \\\r\n>     --answers-file \\\r\n>     LLaVA/model_inference_testing/coco/coco_val2014_answers-HF-vicuna-v1-3-13b-prompt-v1-test-merge.jsonl\r\n> ```\r\n> \r\n> This inference gives the following Error Log :\r\n> \r\n> ```\r\n>   0%|                                                                                                                                         | 0/90 [00:00<?, ?it/s]/home/anaconda3/envs/llavacuda6/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\r\n>   warnings.warn(\r\n>   0%|                                                                                                                                         | 0/90 [00:33<?, ?it/s]\r\n> Traceback (most recent call last):\r\n>   File \"/home/anaconda3/envs/llavacuda6/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n>     return _run_code(code, main_globals, None,\r\n>   File \"/home/anaconda3/envs/llavacuda6/lib/python3.10/runpy.py\", line 86, in _run_code\r\n>     exec(code, run_globals)\r\n>   File \"/home/workspace/cgy/LLAVA/LLaVA/llava/eval/model_vqa.py\", line 112, in <module>\r\n>     eval_model(args)\r\n>   File \"/home/workspace/cgy/LLAVA/LLaVA/llava/eval/model_vqa.py\", line 66, in eval_model\r\n>     output_ids = model.generate(\r\n>   File \"/home/anaconda3/envs/llavacuda6/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n>     return func(*args, **kwargs)\r\n>   File \"/home/anaconda3/envs/llavacuda6/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1588, in generate\r\n>     return self.sample(\r\n>   File \"/home/anaconda3/envs/llavacuda6/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2678, in sample\r\n>     next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\r\n> RuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n> ```\r\n> \r\n> The python code we use to generate our `model-base` in `merge_lora_weights.py` is as follows :\r\n> \r\n> ```\r\n> python -m fastchat.model.apply_delta \\\r\n>     --base huggyllama/llama-13b \\\r\n>     --target checkpoints/fastchat_llama-vicuna-v1-3-13b \\\r\n>     --delta lmsys/vicuna-13b-v1.3\r\n> ```\r\n> \r\n> Interestingly, the same procedure when done for the `LoRA-Merged` [weights](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#model-zoo) returns :\r\n> \r\n> ```\r\n> all : 76.3\r\n> complex : 90.0\r\n> conv : 75.4\r\n> detail : 63.4\r\n> ```\r\n> \r\n> implying that `merge_lora_weights.py` either has some issue, or the provided `LoRA` [weights](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#model-zoo) have some issue, or the `model-base` is faulty.\r\n> \r\n> Kindly suggest fixes for whatever is the reason for this error.\r\n\r\nHi, have you fixed the issue?\n</Comment>\n<Comment by DefUs3r at 2023-10-07T18:40:19Z>\n> > ### Describe the issue\r\n> > Issue:\r\n> > We are trying to perform inference on the LoRA weights provided for vicuna-13b-v1.3 [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#model-zoo). As mentioned by @haotian-liu in issue #245, we performing the merging step on the LoRA weights using the following command:\r\n> > ```\r\n> > python merge_lora_weights.py \\\r\n> >     --model-path hf_checkpoints/llava-v1-0719-336px-lora-vicuna-13b-v1.3 \\\r\n> >     --model-base LLaVA/checkpoints/fastchat_llama-vicuna-v1-3-13b \\\r\n> >     --save-model-path hf_checkpoints/llava-v1-0719-336px-lora-vicuna-13b-v1.3-MERGE\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > After this, we perform the inference on 90 samples of COCO-2014 as mentioned in the paper using:\r\n> > ```\r\n> > python -m llava.eval.model_vqa \\\r\n> >     --model-path hf_checkpoints/llava-v1-0719-336px-lora-vicuna-13b-v1.3-MERGE \\\r\n> >     --question-file \\\r\n> >     LLaVA/playground/data/coco2014_val_qa_eval/qa90_questions.jsonl \\\r\n> >     --image-folder \\\r\n> >     LLaVA/coco/coco_dataset/val2014 \\\r\n> >     --answers-file \\\r\n> >     LLaVA/model_inference_testing/coco/coco_val2014_answers-HF-vicuna-v1-3-13b-prompt-v1-test-merge.jsonl\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > This inference gives the following Error Log :\r\n> > ```\r\n> >   0%|                                                                                                                                         | 0/90 [00:00<?, ?it/s]/home/anaconda3/envs/llavacuda6/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\r\n> >   warnings.warn(\r\n> >   0%|                                                                                                                                         | 0/90 [00:33<?, ?it/s]\r\n> > Traceback (most recent call last):\r\n> >   File \"/home/anaconda3/envs/llavacuda6/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n> >     return _run_code(code, main_globals, None,\r\n> >   File \"/home/anaconda3/envs/llavacuda6/lib/python3.10/runpy.py\", line 86, in _run_code\r\n> >     exec(code, run_globals)\r\n> >   File \"/home/workspace/cgy/LLAVA/LLaVA/llava/eval/model_vqa.py\", line 112, in <module>\r\n> >     eval_model(args)\r\n> >   File \"/home/workspace/cgy/LLAVA/LLaVA/llava/eval/model_vqa.py\", line 66, in eval_model\r\n> >     output_ids = model.generate(\r\n> >   File \"/home/anaconda3/envs/llavacuda6/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n> >     return func(*args, **kwargs)\r\n> >   File \"/home/anaconda3/envs/llavacuda6/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1588, in generate\r\n> >     return self.sample(\r\n> >   File \"/home/anaconda3/envs/llavacuda6/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2678, in sample\r\n> >     next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\r\n> > RuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > The python code we use to generate our `model-base` in `merge_lora_weights.py` is as follows :\r\n> > ```\r\n> > python -m fastchat.model.apply_delta \\\r\n> >     --base huggyllama/llama-13b \\\r\n> >     --target checkpoints/fastchat_llama-vicuna-v1-3-13b \\\r\n> >     --delta lmsys/vicuna-13b-v1.3\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > Interestingly, the same procedure when done for the `LoRA-Merged` [weights](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#model-zoo) returns :\r\n> > ```\r\n> > all : 76.3\r\n> > complex : 90.0\r\n> > conv : 75.4\r\n> > detail : 63.4\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > implying that `merge_lora_weights.py` either has some issue, or the provided `LoRA` [weights](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#model-zoo) have some issue, or the `model-base` is faulty.\r\n> > Kindly suggest fixes for whatever is the reason for this error.\r\n> \r\n> Hi, have you fixed the issue?\r\n\r\nNo this is not yet fixed.\n</Comment>\n<Comment by terminator123 at 2023-12-14T08:50:11Z>\nhow did you download the dataset coco/coco_dataset/val2014?\n</Comment>\n<Comment by kuaileqipaoshui at 2024-01-06T15:31:53Z>\n> how did you download the dataset coco/coco_dataset/val2014?\r\n\r\nDo you know how to download coco_val2014 now?\n</Comment>\n<Comment by Ryosuke0104 at 2024-01-20T06:07:32Z>\n@Cubism-star \r\n>I also got the same error when using my own fine-tuned model to inference.\r\n\r\nMe too. \r\nDid you fix it?\n</Comment>\n<Comment by Kamleshpaul at 2024-02-27T05:51:32Z>\nany update ? i also face same issue after finetune not able to merge\n</Comment>\n<Comment by ChenRan2000 at 2024-04-19T02:40:15Z>\nwhy nobody fix it？\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 407,
    "state": "closed",
    "created_by": "yuntaodu",
    "created_at": "2023-08-30T06:10:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/407</URL>\n\n<TITLE>[Discussion]  How to inference with batch sample</TITLE>\n\n<BODY>### Discussion\n\nHi, now we could input one single image and question to the model for evaluation. What if we want to test the mode with a batch of samples, What should we do?</BODY>\n\n<COMMENTS>\n<Comment by LouieBHLu at 2023-09-20T15:35:12Z>\nI have the same question.\n</Comment>\n<Comment by basteran at 2023-09-21T10:09:46Z>\nSame here, I cannot find anything around.. I guess we need to develop our own code, but I can't figure out how to initialize the model alone with a script.. any suggestions?\n</Comment>\n<Comment by rabiulcste at 2023-09-28T22:48:01Z>\n@basteran It should work as the case with any transformer model. You need to do some batch processing to feed the model. However, I don't see any inference speed with batch inference -- which is very odd! @haotian-liu any thoughts on the speed? \r\n \r\n```\r\n# define a processor\r\n\r\nclass LlaVaProcessor:\r\n  def __init__(self, tokenizer, image_processor, mm_use_im_start_end):\r\n      self.mm_use_im_start_end = mm_use_im_start_end\r\n      self.tokenizer = tokenizer\r\n      self.image_processor = image_processor\r\n      self.conv_mode = \"llava_v1\"\r\n\r\n  def format_text(self, text: str):\r\n      if self.mm_use_im_start_end:\r\n          text = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + \"\\n\" + text\r\n      else:\r\n          text = DEFAULT_IMAGE_TOKEN + \"\\n\" + text\r\n\r\n      conv = conv_templates[self.conv_mode].copy()\r\n      conv.append_message(conv.roles[0], text)\r\n      conv.append_message(conv.roles[1], None)\r\n      text = conv.get_prompt()\r\n\r\n      return text\r\n\r\n  def load_image(self, image_path: str):\r\n      return Image.open(image_path).convert(\"RGB\")\r\n\r\n  @staticmethod\r\n  def pad_sequence_to_max_length(sequence, max_length, padding_value=0):\r\n      \"\"\"Pad a sequence to the desired max length.\"\"\"\r\n      if len(sequence) >= max_length:\r\n          return sequence\r\n      return torch.cat([sequence, torch.full((max_length - len(sequence),), padding_value, dtype=sequence.dtype)])\r\n\r\n  def get_processed_tokens(self, text: str, image_path: str):\r\n      prompt = self.format_text(text)\r\n      image = self.load_image(image_path)\r\n\r\n      input_ids = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0)\r\n      image_tensor = self.image_processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"][0]\r\n\r\n      return image_tensor, input_ids\r\n\r\n  def get_processed_tokens_batch(self, batch_text: List[str], image_paths: List[str]):\r\n      prompt = [self.format_text(text) for text in batch_text]\r\n      images = [self.load_image(image_path) for image_path in image_paths]\r\n\r\n      batch_input_ids = [\r\n          tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\") for prompt in prompt\r\n      ]\r\n\r\n      # Determine the maximum length of input_ids in the batch\r\n      max_len = max([len(seq) for seq in batch_input_ids])\r\n      # Pad each sequence in input_ids to the max_len\r\n      padded_input_ids = [self.pad_sequence_to_max_length(seq.squeeze(), max_len) for seq in batch_input_ids]\r\n      batch_input_ids = torch.stack(padded_input_ids)\r\n\r\n      batch_image_tensor = self.image_processor(images, return_tensors=\"pt\")[\"pixel_values\"]\r\n\r\n      return batch_image_tensor, batch_input_ids\r\n```\r\n\r\n```\r\n# load models and tokenizers\r\n\r\nfrom dataset_zoo.custom_processor import LlaVaProcessor\r\nfrom LLaVA.llava.conversation import SeparatorStyle, conv_templates\r\nfrom LLaVA.llava.mm_utils import (KeywordsStoppingCriteria,\r\n                                  get_model_name_from_path)\r\nfrom LLaVA.llava.model.builder import load_pretrained_model\r\nfrom LLaVA.llava.utils import disable_torch_init\r\n\r\ndisable_torch_init()\r\nmodel_path = os.path.expanduser(model_name)\r\nmodel_name = get_model_name_from_path(model_path)\r\nmodel_name = get_model_name_from_path(model_path)\r\nmodel_base = MODEL_CLS_INFO[\"hfformer\"][args.model_name][\"base\"]\r\ntokenizer_, model, image_processor, context_len = load_pretrained_model(model_path, model_base, model_name)\r\ntokenizer_.padding_side = \"left\"\r\nprocessor = LlaVaProcessor(tokenizer_, image_processor, model.config.mm_use_im_start_end)\r\n```\r\n\r\n```\r\nbatch_images, batch_text = processor.get_processed_tokens_batch(\r\n    [example[\"prompted_question\"] for example in batch], [example[\"image_path\"] for example in batch]\r\n)\r\nprocessed_batch[\"image_tensors\"] = batch_images\r\nprocessed_batch[\"input_ids\"] = batch_text\r\n\r\n# run batch inference\r\nconv = conv_templates[processor.conv_mode].copy()\r\nstop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\r\nkeywords = [stop_str]\r\nstopping_criteria = (\r\n    [KeywordsStoppingCriteria(keywords, tokenizer_, input_ids)] if conv.version == \"v0\" else None\r\n)\r\ninput_ids = batch[\"input_ids\"]\r\nimage_tensor = batch[\"image_tensors\"]\r\ninput_ids = input_ids.cuda()\r\n\r\nwith torch.inference_mode():\r\n    output_ids = model.generate(\r\n        input_ids,\r\n        images=image_tensor.half().cuda(),\r\n        do_sample=False,\r\n        num_beams=args.num_beams,\r\n        max_new_tokens=args.max_length,\r\n        length_penalty=args.length_penalty,\r\n        use_cache=True,\r\n        stopping_criteria=stopping_criteria,\r\n    )\r\ngenerated_outputs = tokenizer_.batch_decode(output_ids[:, input_ids.shape[1] :], skip_special_tokens=True)\r\ngenerated_outputs = [out.strip() for out in generated_outputs]\r\ngenerated_outputs = [out[: -len(stop_str)] if out.endswith(stop_str) else out for out in generated_outputs]\r\n```\r\n\r\nHope it helps!\n</Comment>\n<Comment by NinaadRao at 2023-10-08T18:32:57Z>\nWhen I try to use batch for inferencing, I get an issue where stopping_criteria expects batch size as 1. Is there any specific reason to this?\n</Comment>\n<Comment by ghost at 2023-10-16T12:17:21Z>\nHi @rabiulcste, in your code there is a mistake, as you should use left padding according to this https://discuss.huggingface.co/t/batch-generation-with-gpt2/1517\r\n\r\n```python\r\n@staticmethod\r\ndef pad_sequence_to_max_length(sequence, max_length, padding_value=0):\r\n      \"\"\"Pad a sequence to the desired max length.\"\"\"\r\n      if len(sequence) >= max_length:\r\n          return sequence\r\n      return torch.cat([torch.full((max_length - len(sequence),), padding_value, dtype=sequence.dtype), sequence])\r\n```\n</Comment>\n<Comment by haotian-liu at 2023-11-05T05:08:26Z>\nHi, thank you, and I am trying to work on an efficient batch inference. Closing this and consolidating the discussion to https://github.com/haotian-liu/LLaVA/issues/754.\r\n\r\n@dverdu-freepik @rabiulcste If you have prior experience in the batch inference, it is very much appreciated if you could help share some insights on why the batch inference I implemented were not efficient. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 406,
    "state": "closed",
    "created_by": "coderlemon17",
    "created_at": "2023-08-29T14:59:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/406</URL>\n\n<TITLE>ZeroParamStatus.NOT_AVAILABLE after running finetune_lora.sh</TITLE>\n\n<BODY>### Question\n\nHi, thanks for the great work. I was trying to run the `scripts/finetune_lora.sh` with args `--deepspeed ./scripts/zero3.json`. It seems that the fine-tuning process can successfully end and the model has been saved.\r\nBut there're multiple warning lines poping up after the fine-tuning has finished:\r\n```bash\r\n{'loss': 3.6406, 'learning_rate': 2e-05, 'epoch': 1.0}                                                                                                               \r\n{'train_runtime': 79.5713, 'train_samples_per_second': 0.101, 'train_steps_per_second': 0.013, 'train_loss': 3.640625, 'epoch': 1.0}                                 \r\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:22<00:00, 22.15s/it]\r\nWARNING:root:base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: param.ds_status != ZeroParamStatus.NOT_AVAILABLE: ZeroParamStatus.NOT_AVAILABLE\r\nWARNING:root:base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: param.ds_status != ZeroParamStatus.NOT_AVAILABLE: ZeroParamStatus.NOT_AVAILABLE\r\nWARNING:root:base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: param.ds_status != ZeroParamStatus.NOT_AVAILABLE: ZeroParamStatus.NOT_AVAILABLE\r\nWARNING:root:base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: param.ds_status != ZeroParamStatus.NOT_AVAILABLE: ZeroParamStatus.NOT_AVAILABLE\r\nWARNING:root:base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: param.ds_status != ZeroParamStatus.NOT_AVAILABLE: ZeroParamStatus.NOT_AVAILABLE\r\n....\r\nwandb: You can sync this run to the cloud by running:\r\n[2023-08-29 22:45:55,672] [INFO] [launch.py:347:main] Process 3580155 exits successfully.\r\n[2023-08-29 22:46:02,681] [INFO] [launch.py:347:main] Process 3580154 exits successfully.\r\n```\r\nI'm new to DeepSpeed so I'm not sure if this is normal, and what I might have to do to mute those warnings. Any help or suggestion will be appreciated.</BODY>\n\n<COMMENTS>\n<Comment by coderlemon17 at 2023-08-30T02:45:10Z>\nSry, I just realized this warning comes from this function when saving parameters:\r\n```python\r\ndef maybe_zero_3(param, ignore_status=False, name=None):\r\n    from deepspeed import zero\r\n    from deepspeed.runtime.zero.partition_parameters import ZeroParamStatus\r\n    if hasattr(param, \"ds_id\"):\r\n        if param.ds_status == ZeroParamStatus.NOT_AVAILABLE:\r\n            if not ignore_status:\r\n                logging.warning(f\"{name}: param.ds_status != ZeroParamStatus.NOT_AVAILABLE: {param.ds_status}\")\r\n        with zero.GatheredParameters([param]):\r\n            param = param.data.detach().cpu().clone()\r\n    else:\r\n        param = param.detach().cpu().clone()\r\n    return param\r\n```\r\nSo is it normal to have those warnings when saving models?\n</Comment>\n<Comment by basteran at 2023-10-03T16:27:31Z>\nI have the same issue sometimes, but I cannot figure out when it happens or why. Any news?\r\n@coderlemon17 did you solve?\n</Comment>\n<Comment by coderlemon17 at 2023-10-08T06:53:29Z>\n@basteran Hi, I find that this warning might not affect saving the parameters so I just ignore it :)\r\nI think it might be related to the mechanism of Zero3, since it shards the params as well, so I think it needs a `gather` to get params from other devices.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 405,
    "state": "closed",
    "created_by": "Crawfish-h",
    "created_at": "2023-08-29T13:00:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/405</URL>\n\n<TITLE>[Question]</TITLE>\n\n<BODY>### Question\n\nI wanted to use this LLM: https://huggingface.co/liuhaotian/llava-llama-2-7b-chat-lightning-lora-preview/tree/main. But after adding only the adapter_model.bin file to a folder called checkpoints and running the model worker with the argument                 -model-path ./checkpoints/adapter_model.bin, the worker did not run and had an error: \r\n\r\n\r\n/home/jaiden/anaconda3/envs/llava/lib/python3.10/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\r\n  return torch._C._cuda_getDeviceCount() > 0\r\n/home/jaiden/anaconda3/envs/llava/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\r\n  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\r\n/home/jaiden/anaconda3/envs/llava/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\r\n[2023-08-29 08:58:49,482] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n2023-08-29 08:58:49 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='./checkpoints/adapter_model.bin', model_base=None, model_name=None, multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=False)\r\n2023-08-29 08:58:49 | INFO | model_worker | Loading the model adapter_model.bin on worker c55f0a ...\r\n2023-08-29 08:58:49 | ERROR | stderr | Traceback (most recent call last):\r\n2023-08-29 08:58:49 | ERROR | stderr |   File \"/home/jaiden/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n2023-08-29 08:58:49 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n2023-08-29 08:58:49 | ERROR | stderr |   File \"/home/jaiden/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n2023-08-29 08:58:49 | ERROR | stderr |     exec(code, run_globals)\r\n2023-08-29 08:58:49 | ERROR | stderr |   File \"/home/jaiden/code/LLaVA/llava/serve/model_worker.py\", line 273, in <module>\r\n2023-08-29 08:58:49 | ERROR | stderr |     worker = ModelWorker(args.controller_address,\r\n2023-08-29 08:58:49 | ERROR | stderr |   File \"/home/jaiden/code/LLaVA/llava/serve/model_worker.py\", line 64, in __init__\r\n2023-08-29 08:58:49 | ERROR | stderr |     self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\r\n2023-08-29 08:58:49 | ERROR | stderr |   File \"/home/jaiden/code/LLaVA/llava/model/builder.py\", line 120, in load_pretrained_model\r\n2023-08-29 08:58:49 | ERROR | stderr |     tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n2023-08-29 08:58:49 | ERROR | stderr |   File \"/home/jaiden/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 652, in from_pretrained\r\n2023-08-29 08:58:49 | ERROR | stderr |     tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\r\n2023-08-29 08:58:49 | ERROR | stderr |   File \"/home/jaiden/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 496, in get_tokenizer_config\r\n2023-08-29 08:58:49 | ERROR | stderr |     resolved_config_file = cached_file(\r\n2023-08-29 08:58:49 | ERROR | stderr |   File \"/home/jaiden/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py\", line 417, in cached_file\r\n2023-08-29 08:58:49 | ERROR | stderr |     resolved_file = hf_hub_download(\r\n2023-08-29 08:58:49 | ERROR | stderr |   File \"/home/jaiden/anaconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 110, in _inner_fn\r\n2023-08-29 08:58:49 | ERROR | stderr |     validate_repo_id(arg_value)\r\n2023-08-29 08:58:49 | ERROR | stderr |   File \"/home/jaiden/anaconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 158, in validate_repo_id\r\n2023-08-29 08:58:49 | ERROR | stderr |     raise HFValidationError(\r\n2023-08-29 08:58:49 | ERROR | stderr | huggingface_hub.utils._validators.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './checkpoints/adapter_model.bin'. Use `repo_type` argument if needed.\r\n.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 404,
    "state": "open",
    "created_by": "xiguiw",
    "created_at": "2023-08-28T07:41:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/404</URL>\n\n<TITLE>run into semaphore not released issue</TITLE>\n\n<BODY>### Question\n\nI run into semaphore not released issue in model_worker.py.\r\n\r\nFrom the code, the semaphore is acquired when generate_stream, and release_model_semaphore is added as a background task (code listed in the end). But in my envrionment (Ubuntu 22.04) `release_model_semaphore ` is never called.\r\nBesides, the global_counter is not restored when semaphore released. Not sure what's the trigger condition to call it.\r\n\r\nIn my test, it's blocked by semaphore after submit --limit-model-concurrency times of query of a model. Each time to submit a post to the generate streamer, the gloal_counter plus 1 and semaphore._value minus 1. untill semaphore._value is zero, then blocked.\r\n\r\nI put the release semaphore at the end of streamer the generated the text. There the model end of text generate at that time. So this limit the concurrent running model numbers.  It works.\r\n\r\nCould you tell me how the semapore is released in the background task?\r\nIs my change acceptable?\r\n\r\nThanks!\r\n\r\n'''\r\n     @torch.inference_mode()\r\n     def generate_stream(self, params):\r\n+        global model_semaphore, global_counter\r\n         tokenizer, model, image_processor = self.tokenizer, self.model, self.image_processor\r\n\r\n         prompt = params[\"prompt\"]\r\n@@ -189,12 +191,25 @@ class ModelWorker:\r\n         ))\r\n         thread.start()\r\n\r\n         generated_text = ori_prompt\r\n\r\n         for new_text in streamer:\r\n             generated_text += new_text\r\n             if generated_text.endswith(stop_str):\r\n                 generated_text = generated_text[:-len(stop_str)]\r\n            yield json.dumps({\"text\": generated_text, \"error_code\": 0}).encode() + b\"\\0\"\r\n\r\n+        global_counter -= 1\r\n+        model_semaphore.release()\r\n'''\r\n\r\n'''\r\n def release_model_semaphore(fn=None):\r\n     model_semaphore.release()\r\n     if fn is not None:\r\n         fn()\r\n\r\n@@ async def generate_stream(request: Request):\r\n     global model_semaphore, global_counter\r\n     global_counter += 1\r\n     params = await request.json()\r\n\r\n     if model_semaphore is None:\r\n         model_semaphore = asyncio.Semaphore(args.limit_model_concurrency)\r\n\r\n     await model_semaphore.acquire()\r\n     worker.send_heart_beat()\r\n     generator = worker.generate_stream_gate(params)\r\n\r\n     background_tasks = BackgroundTasks()\r\n     background_tasks.add_task(partial(release_model_semaphore, fn=worker.send_heart_beat))\r\n    return StreamingResponse(generator, background=background_tasks)\r\n'''\r\n\r\nXigui</BODY>\n\n<COMMENTS>\n<Comment by xiguiw at 2023-08-29T02:10:31Z>\nSorry my change skip the release semaphore.\r\n\r\nThe release semaphore is called. But it is release earlier. The semapore is relased (at send heart bit) before the work model finished it's task. \r\n\r\nThis means semaphore cannot prevent more post/input work to the work model. So the models running in parallel is not limited.\r\n\r\nSo release semaphore need to put in work mode generate streamer ( model and generate streamer are in two different threads, they run in concurrency), so semaphore must be released after model finished (i.e. generate stramer finisned).\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 402,
    "state": "closed",
    "created_by": "mao-code",
    "created_at": "2023-08-28T07:32:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/402</URL>\n\n<TITLE>[Usage] Loading LLaVA model using huggingface</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI am trying to load LLaVA model checkpoint in Model Zoo, but it always shows the same error\r\n\r\nCommand:\r\n```\r\n# download LLaVA checkpoint into ./checkpoint\r\nfrom transformers import AutoModelForCausalLM\r\n\r\n# model checkpoint from hugging face (in the github guideline)\r\nllavaCkpt = AutoModelForCausalLM.from_pretrained(\"liuhaotian/llava-v1-0719-336px-lora-merge-vicuna-13b-v1.3\")\r\n```\r\n\r\nLog: \r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n[<ipython-input-10-e9318ae5ad80>](https://localhost:8080/#) in <cell line: 5>()\r\n      3 \r\n      4 # model checkpoint from hugging face (in the github guideline)\r\n----> 5 llavaCkpt = AutoModelForCausalLM.from_pretrained(\"liuhaotian/llava-v1-0719-336px-lora-merge-vicuna-13b-v1.3\")\r\n\r\n2 frames\r\n[/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py](https://localhost:8080/#) in __getitem__(self, key)\r\n    708             return self._extra_content[key]\r\n    709         if key not in self._mapping:\r\n--> 710             raise KeyError(key)\r\n    711         value = self._mapping[key]\r\n    712         module_name = model_type_to_module_name(key)\r\n\r\nKeyError: 'llava'\r\n```</BODY>\n\n<COMMENTS>\n<Comment by QZJ-2003 at 2023-08-31T01:36:27Z>\nHi, did you solve this problem? I‘m having the same problem too.\n</Comment>\n<Comment by mao-code at 2023-08-31T01:51:39Z>\nNot yet🥲\n</Comment>\n<Comment by haotian-liu at 2023-08-31T03:42:45Z>\nHi, please use `load_pretrained_model` in our [builder](https://github.com/haotian-liu/LLaVA/blob/main/llava/model/builder.py#L25) to load the model.\n</Comment>\n<Comment by mao-code at 2023-09-01T07:51:44Z>\nIt works! Thx so much!\r\nCould I add the code I use to the doc and make a pull request?\n</Comment>\n<Comment by haotian-liu at 2023-09-01T14:08:03Z>\nSure, please feel free to contribute! Thanks.\n</Comment>\n<Comment by wielandmichele at 2023-10-17T13:30:39Z>\nHey @mao-code. Thank you, your read-me helped me a lot. Do you know how I can directly use the loaded model for inference in a HF-style way? There is no processor function right?\n</Comment>\n<Comment by mao-code at 2023-11-03T11:49:49Z>\n@wielandmichele You can use the \"load_pretrained_model\" in the \"eval_model\" function just like I used in my new readme file.\r\n\r\n```python\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name)\r\n```\r\n\r\nDoing so lets you use the model, processor, and so on by replacing the \"args\". \r\nYou can refer to the \"eval_model\" function to find more details.\r\n\r\nHere is one of an example\r\n```\r\n  model_name = get_model_name_from_path(\"one-of-the-llava-model-path\")\r\n  model_base = None\r\n  prompt = instruction\r\n  imageFile = img_path\r\n\r\n  args = type('Args', (), {\r\n      \"model_path\": \"one-of-the-llava-model-path\",\r\n      \"model_base\": model_base,\r\n      \"model_name\": model_name,\r\n      \"query\": prompt,\r\n      \"conv_mode\": None,\r\n      \"image_file\": imageFile\r\n  })()\r\n\r\n  output = eval_model(args)\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 401,
    "state": "closed",
    "created_by": "BoyaWu10",
    "created_at": "2023-08-28T07:26:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/401</URL>\n\n<TITLE>[Question] Repeated responses when using LLaVA with LLaMA-2</TITLE>\n\n<BODY>### Question\n\nHi, thanks for this great work!\r\n\r\nI'm trying to run the demo of LLaVA with LLaMA-2 and get repeated responses for the given examples.\r\n\r\n![1](https://github.com/haotian-liu/LLaVA/assets/38548227/e653b7e2-3314-4b41-b6e3-e72b75b9844c)\r\n\r\nThe weights are from [llava-llama-2-7b-chat-lightning-lora-preview](https://huggingface.co/liuhaotian/llava-llama-2-7b-chat-lightning-lora-preview) and [Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf). The commands to run the demo are shown as follows according to the [document](https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md).\r\n\r\n```\r\npython -m llava.serve.controller --host 0.0.0.0 --port 10001\r\npython -m llava.serve.gradio_web_server --controller http://localhost:10001 --model-list-mode reload\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10001 --port 40000 --worker http://localhost:40000 --model-path /path/to/llava-llama-2-7b-chat-lightning-lora-preview --model-base /path/to/Llama-2-7b-hf\r\n```\r\n\r\nI also tried to use `merge_lora_weights.py` to get the model and run it, but the result is the same. Is there anything wrong about my settings?\r\n\r\nThanks in advance!</BODY>\n\n<COMMENTS>\n<Comment by BoyaWu10 at 2023-08-28T07:49:18Z>\nThe problem locates in the wrong base model I choose. I should use [Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) instead of [Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf).\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 400,
    "state": "closed",
    "created_by": "Lucas1347",
    "created_at": "2023-08-27T16:13:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/400</URL>\n\n<TITLE>[Question] i can eval, but can not pretrain and instruction tuning</TITLE>\n\n<BODY>### Question\n\nIf i run with deepspeed, got this error:\r\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500\r\n\r\nIf i run python with 1 gpu, got this error:\r\nRuntimeError: FlashAttention only support fp16 and bf16 data type\r\n\r\nSome of my environmental configurations: \r\nUbuntu \r\ncuda 11.7\r\nflash-attn==2.0.4\r\ndeepspeed==0.9.5\r\n\r\ngpu:A100</BODY>\n\n<COMMENTS>\n<Comment by Lucas1347 at 2023-08-28T02:03:33Z>\nI can run pretrain task with 1 gpu.  Just disabled flash_attention and reduced train batch_size. \r\nThe above problem is still not resolved\n</Comment>\n<Comment by ai1361720220000 at 2023-10-19T03:29:24Z>\n> I can run pretrain task with 1 gpu. Just disabled flash_attention and reduced train batch_size. The above problem is still not resolved\r\n\r\nHi, have you solved this problem?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 399,
    "state": "closed",
    "created_by": "QiYang03101994",
    "created_at": "2023-08-27T15:18:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/399</URL>\n\n<TITLE>[Usage] Training loss of fine tuning by LoRA is converged to around 1 after 4000 steps</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: Thanks for making the effort to do such great work. I tried fine-tuning the model on 2 A6000 with deep speed and LoRA since I cannot fit my model on two A6000 by using full fine-tuning. However, I noticed that the training loss is coverage to 1.0, which is not good. It would be great if you have any suggestions about why that happened. \r\n\r\nCommand:\r\n```\r\n################# LLaMA-2 ##################\r\nPROMPT_VERSION=\"llava_llama_2\"\r\nMODEL_VERSION=\"llama-2-7b-chat\"\r\n################# LLaMA-2 ##################\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed /local_storage/yangq6/code/python/LLaVA/scripts/zero2.json \\\r\n    --lora_enable True \\\r\n    --model_name_or_path /local_storage/yangq6/Model/llama2/Llama-2-7b-chat-hf \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path /local_storage/yangq6/Data/llava_instruct_150k.json \\\r\n    --image_folder /local_storage/yangq6/Data/train2017 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava-$MODEL_VERSION-pretrain/mm_projector.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-$MODEL_VERSION-finetune \\\r\n    --num_train_epochs 2 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 1000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nscreenshots\r\n![image](https://github.com/haotian-liu/LLaVA/assets/54153021/35f99eca-bc30-4b08-9593-773351267f23)</BODY>\n\n<COMMENTS>\n<Comment by ccx1997 at 2023-10-10T03:52:58Z>\nhello, have you solved the problem? What is the final loss the model converged to?\n</Comment>\n<Comment by johnmullan at 2023-10-12T09:31:29Z>\nHow many it/s are you getting with this?\n</Comment>\n<Comment by haotian-liu at 2023-10-26T20:45:54Z>\nHi, this loss seems okay to me given that it is only trained for a single epoch. You may check out our official logs: https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md\n</Comment>\n<Comment by daiqing98 at 2023-11-03T17:03:40Z>\n> Hi, this loss seems okay to me given that it is only trained for a single epoch. You may check out our official logs: https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md\r\n\r\n@haotian-liu Hi, I am trying to use finetune_lora.sh with 8*A100(40G), however, the it went divergent after around 1000 steps:\r\nHere is the record:\r\n<img width=\"654\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/95208297/da326bb9-cd0d-4c26-9dd4-9aea3b296505\">\r\n\r\nAnd here is the command:\r\n\r\n`/sfs/weka/scratch/mac5vs/LLaVA/llava/train/train_mem.py --local_rank=0 --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero3.json --model_name_or_path ./checkpoints/vicuna-13b-v1.5 --version v1 --data_path ./playground/data/LLaVA-Instruct-150K/llava_v1_5_mix665k.json --image_folder ./playground/data --vision_tower openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter ./checkpoints/my-llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/my-llava-v1.5-13b-lora --num_train_epochs 1 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb`\r\n\r\nI used my pre-trained mlp projector. The training log of it seems good:\r\n\r\n<img width=\"636\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/95208297/e9e64588-1374-4313-b7a7-727f675224d1\">\r\n\r\nHere is the command I used for pre- training:\r\n\r\n'/sfs/weka/scratch/mac5vs/LLaVA/llava/train/train_mem.py --local_rank=0 --deepspeed ./scripts/zero3.json --model_name_or_path ./checkpoints/vicuna-13b-v1.5 --version plain --data_path ./playground/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json --image_folder ./playground/data/LLaVA-Pretrain/images --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --tune_mm_mlp_adapter True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir ./checkpoints/my-llava-v1.5-13b-pretrain --num_train_epochs 1 --per_device_train_batch_size 32 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 24000 --save_total_limit 1 --learning_rate 1e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb'\r\n\r\nI noticed that one difference is, in your provided script for pre-training, you used zero2.json. I am not sure if it will affect the pretraining stage. At least the training log seems normal, thus I assume the projector is fine then used it for fine-tuning.\r\n\r\nMay I ask if you have any insights about this? Thanks!\n</Comment>\n<Comment by haotian-liu at 2023-11-03T17:07:33Z>\nHi @daiqing98 \r\n\r\nzero3/zero2 difference is fine. I noticed a similar loss divergence yesterday as well. I found two solutions that can help with this:\r\n\r\n1. `--group_by_modality_length False`, this may result in a slight 1 or 2 hour additional training time\r\n2. Check `peft` version is 0.4.0 (I encountered this after I upgrade to the latest branch)\n</Comment>\n<Comment by daiqing98 at 2023-11-03T20:25:51Z>\n> Hi @daiqing98\r\n> \r\n> zero3/zero2 difference is fine. I noticed a similar loss divergence yesterday as well. I found two solutions that can help with this:\r\n> \r\n> 1. `--group_by_modality_length False`, this may result in a slight 1 or 2 hour additional training time\r\n> 2. Check `peft` version is 0.4.0 (I encountered this after I upgrade to the latest branch)\r\n\r\n@haotian-liu Thanks for your reply! My peft version is 0.4.0. I tried to set `--group_by_modility_length = False`, but met with another error:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"/sfs/weka/scratch/mac5vs/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n>     train()\r\n>   File \"/sfs/weka/scratch/mac5vs/LLaVA/llava/train/train.py\", line 934, in train\r\n>     trainer.train()\r\n>   File \"/scratch/mac5vs/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n>     return inner_training_loop(\r\n>   File \"/scratch/mac5vs/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\r\n>     tr_loss_step = self.training_step(model, inputs)\r\n>   File \"/scratch/mac5vs/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2665, in training_step\r\n>     self.accelerator.backward(loss)\r\n>   File \"/scratch/mac5vs/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1847, in backward\r\n>     self.deepspeed_engine_wrapped.backward(loss, **kwargs)\r\n>   File \"/scratch/mac5vs/envs/llava/lib/python3.10/site-packages/accelerate/utils/deepspeed.py\", line 167, in backward\r\n>     self.engine.backward(loss, **kwargs)\r\n>   File \"/scratch/mac5vs/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n>     ret_val = func(*args, **kwargs)\r\n>   File \"/scratch/mac5vs/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1861, in backward\r\n>     self.optimizer.backward(loss, retain_graph=retain_graph)\r\n>   File \"/scratch/mac5vs/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n>     ret_val = func(*args, **kwargs)\r\n>   File \"/scratch/mac5vs/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 1993, in backward\r\n>     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\r\n>   File \"/scratch/mac5vs/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 63, in backward\r\n>     scaled_loss.backward(retain_graph=retain_graph)\r\n>   File \"/scratch/mac5vs/envs/llava/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\r\n>     torch.autograd.backward(\r\n>   File \"/scratch/mac5vs/envs/llava/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\r\n>     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n>   File \"/scratch/mac5vs/envs/llava/lib/python3.10/site-packages/torch/autograd/function.py\", line 274, in apply\r\n>     return user_fn(self, *args)\r\n>   File \"/scratch/mac5vs/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 132, in backward\r\n>     with torch.random.fork_rng(devices=rng_devices, enabled=ctx.preserve_rng_state):\r\n>   File \"/scratch/mac5vs/envs/llava/lib/python3.10/contextlib.py\", line 153, in __exit__\r\n>     self.gen.throw(typ, value, traceback)\r\n>   File \"/scratch/mac5vs/envs/llava/lib/python3.10/site-packages/torch/random.py\", line 137, in fork_rng\r\n>     torch.cuda.set_rng_state(gpu_rng_state, device)\r\n>   File \"/scratch/mac5vs/envs/llava/lib/python3.10/site-packages/torch/cuda/random.py\", line 64, in set_rng_state\r\n>     _lazy_call(cb)\r\n>   File \"/scratch/mac5vs/envs/llava/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 183, in _lazy_call\r\n>     callable()\r\n>   File \"/scratch/mac5vs/envs/llava/lib/python3.10/site-packages/torch/cuda/random.py\", line 62, in cb\r\n>     default_generator.set_state(new_state_copy)\r\n> RuntimeError: CUDA error: uncorrectable ECC error encountered\r\n> CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n> For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\n> Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\nWhen `--group_by_modility_length = True`, it is fine, which seems weird. Have you had the same issue before, or any insights on this? Thanks!\n</Comment>\n<Comment by haotian-liu at 2023-11-03T20:30:13Z>\ncan you confirm if this happens in the first iteration, or it happens after some iterations? This looks like a CUDA OOM issue. Also, what if you try bs4xaccu4?\n</Comment>\n<Comment by daiqing98 at 2023-11-03T20:31:35Z>\n> can you confirm if this happens in the first iteration, or it happens after some iterations? This looks like a CUDA OOM issue. Also, what if you try bs4xaccu4?\r\n\r\n@haotian-liu The first iteration. I will try 4*4 instead, thanks!\n</Comment>\n<Comment by daiqing98 at 2023-11-06T02:57:35Z>\n> can you confirm if this happens in the first iteration, or it happens after some iterations? This looks like a CUDA OOM issue. Also, what if you try bs4xaccu4?\r\n\r\n@haotian-liu . Thanks for your help! My lora fine-tuning is finished. I compared my training log with yours and found that our loss never reached to around 0.2 during fluctuating while yours did:\r\n\r\n<img width=\"336\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/95208297/03184219-d904-43ef-88c0-6f52a94e507f\">\r\n(Yours)\r\n\r\n<img width=\"604\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/95208297/89cb0e2e-684c-4cda-8d43-031fcc92bb41\">\r\n(Ours)\r\n\r\nI wonder if it is an issue? And is it cased by my setting `--group_by_modality_length False` of using `bs4xaccu4`\r\n\r\nAnd one more question, I am tring to evaluate my model with GPT-assisted Evaluation. In model_vqa.py, it seems that I cannot directly use `--model-path ./checkpoints/llava-v1.5-13b-lora`, which raised an error: `KeyError: 'LlavaConfig'`. Is is because that the format of saved lora-based chceckpoint is different from your released llava-v1.5-13b ?\r\n\r\nMany thanks!\n</Comment>\n<Comment by haotian-liu at 2023-11-06T03:04:31Z>\nHi @daiqing98 \r\n\r\nIt should mainly be due to `--group_by_modality_length False`, but it is fine (and it may be better). Should have nothing to do with bs4x4.\r\n\r\nWhen evaluating on many benchmarks, I usually merge LoRA with the base model to a single checkpoint: https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md#create-merged-checkpoints\r\n\r\nYou can also specify --model-base /path/to/vicuna-v1.5, but you will need to load lora every time you run the eval.\n</Comment>\n<Comment by daiqing98 at 2023-11-06T04:37:57Z>\n@haotian-liu Thanks for your guidance! I have one minor issue:\r\n\r\nI compared the results of your provided model (llava-v1.5-13b), mine (llava-v1.5-13b-lora) and the one in the paper:\r\n\r\n<img width=\"643\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/95208297/5bad560c-123a-4171-b491-c7efd3e34345\"> (results from paper)\r\n\r\n> all 82.2 81.1 66.7\r\ncomplex 91.9 80.7 74.2\r\nconv 80.1 83.7 67.0\r\ndetail 74.5 79.0 58.8\r\n(your checkpoint llava-v1.5-13b)\r\n\r\n> all 81.7 81.6 66.7\r\ncomplex 90.2 81.3 73.3\r\nconv 83.7 83.7 70.0\r\ndetail 71.1 79.7 56.7\r\n(my llava-v1.5-13b-lora)\r\n\r\nI find the while the scores for _conv_ and and _detail_ are slightly different from the results, the score for _complex_ is significantly different. May I ask if there are any insights about this? Thanks!\n</Comment>\n<Comment by haotian-liu at 2023-11-06T04:41:19Z>\nHi, that is the original V0 checkpoint in the original LLaVA paper (Visual Instruction Tuning) and it is LLaVA-Bench-**COCO** (where LLaVA has seen the images during the training). Probably the drop is due to that LLaVA-1.5 is only trained with 1 epoch and is less overfit to COCO (original checkpoint is trained for 3 epochs on LLaVA-Instruct only).\r\n\r\nYou can compare with the LLaVA-Bench-**In-the-Wild** in this paper: https://arxiv.org/abs/2310.03744\n</Comment>\n<Comment by daiqing98 at 2023-12-01T20:24:39Z>\n> https://arxiv.org/abs/2310.03744\r\n\r\n@haotian-liu Thanks for sharing. But may I ask where I can find LLaVA-Bench-In-the-Wild so that I can evaluate on that? In this github repository, it seems that the GPT4-evaluation still uses  LLaVA-Bench-**COCO**. Many thanks!\n</Comment>\n<Comment by haotian-liu at 2023-12-02T04:49:48Z>\n@daiqing98 Please see [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md#llava-bench-in-the-wild).\n</Comment>\n<Comment by terminator123 at 2023-12-18T08:24:58Z>\n> save_total_limit\r\n\r\ni met the same problem when i funetuned with 2*A100  in lora。do u known why the given script set the group_by_modality_length True ?\n</Comment>\n<Comment by terminator123 at 2023-12-19T06:11:51Z>\n> Hi @daiqing98\r\n> \r\n> It should mainly be due to `--group_by_modality_length False`, but it is fine (and it may be better). Should have nothing to do with bs4x4.\r\n> \r\n> When evaluating on many benchmarks, I usually merge LoRA with the base model to a single checkpoint: https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md#create-merged-checkpoints\r\n> \r\n> You can also specify --model-base /path/to/vicuna-v1.5, but you will need to load lora every time you run the eval.\r\n\r\nhi，below is my scrip, the loss is still  converged to around 6 after 2000 steps，do u hown why?\r\n![image](https://github.com/haotian-liu/LLaVA/assets/7448280/93e44f02-b4bc-4fb7-b57c-b966fc62c134)\r\n\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/7448280/e17a1c85-fe11-4ded-a35a-ca50340056d5)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 397,
    "state": "open",
    "created_by": "chigkim",
    "created_at": "2023-08-26T15:12:09Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/397</URL>\n\n<TITLE>Dataset from OpenAI GPT-4 Multimodal?</TITLE>\n\n<BODY>### Question\n\nIt says \"Dataset date: LLaVA Visual Instruct 150K was collected in April 2023, by prompting GPT-4-0314 API.\"\r\nhttps://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K\r\nDid you guys have a special access to gpt-4 multimodal back then?\r\nI don't think gpt-4 multimodal is out yet.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-08-27T01:34:00Z>\nWe use text-only GPT-4 and we do not have access to the multimodal one. We provide the image context via caption/bounding box. You may refer to Sec. 3 and Table 1 in our paper for a better understanding.\n</Comment>\n<Comment by chigkim at 2023-08-27T09:44:46Z>\nThanks so much for the info!\r\nOpenAI started opening beta access to BeMyEyes users as BeMyAi feature, and the out put is fantastic with lots of detail!\r\n\r\n![Image](https://github.com/haotian-liu/LLaVA/assets/22120994/81564648-a737-41f5-82c2-b94ae234676a)\r\n\r\nThe picture shows the interior of a modern building, possibly a shopping center or a corporate building. The space is open and airy with high ceilings. The ceiling is made of wooden panels with embedded lights. There is a large staircase in the foreground with wooden steps and black railings. The staircase leads to an upper level which has a balcony overlooking the ground floor. \r\n\r\nOn the ground floor, there is a small information desk with a person in a blue uniform sitting behind it. There are also a couple of high-end shops visible, one of them is \"Watches of Switzerland\" and the other is \"Cartier\". The shops have glass walls, allowing a clear view of the displays inside.\r\n\r\nThe walls of the building are a mix of glass and brick, and there are tall windows that allow natural light to flood in. There are a few people scattered around, some are walking and others are standing on the balcony of the upper level. The floor is made of polished concrete, giving it a sleek and modern look.\n</Comment>\n<Comment by chigkim at 2023-10-13T11:26:47Z>\n@haotian-liu, OpenAI is rolling out GPT4-V to ChatGPT Plus users, and it's amazing. Do you guys have access to it yet? Probably worthwhile to look into building synthetic dataset from it!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 395,
    "state": "open",
    "created_by": "OrienKastor",
    "created_at": "2023-08-26T01:03:35Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/395</URL>\n\n<TITLE>Failed to run the model [Usage]</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nFailed to run the model with an error: AttributeError: 'NoneType' object has no attribute 'is_loaded'\r\n\r\nI apologize, I am new to this so if there is a simple solution to this sorry for the silly question.\r\n\r\nCommand:\r\n```\r\n--- TO GET IT INSTALLED ---\r\ngit clone https://github.com/haotian-liu/LLaVA.git\r\ncd LLaVA\r\n\r\nconda create -n llava python=3.10 -y\r\nconda activate llava\r\npip install --upgrade pip  # enable PEP 660 support\r\npip install -e .\r\n\r\n\r\nError when trying to run:\r\nRuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\r\nDescriptors cannot not be created directly.\r\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\r\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\r\n 1. Downgrade the protobuf package to 3.20.x or lower.\r\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\r\n\r\nResolution:\r\npip install protobuf==3.20.0\r\n\r\n\r\n\r\n\r\n--- TO RUN IT ---\r\n\r\nIn one terminal:\r\n(Launch a controller)\r\n\r\ncd /home/USER/github/LLaVa/LLaVA\r\nconda activate llava\r\npython -m llava.serve.controller --host 0.0.0.0 --port 10000\r\n\r\n----------------------------------\r\nIn another terminal:\r\n(Launch a gradio web server)\r\n\r\ncd /home/USER/github/LLaVa/LLaVA\r\nconda activate llava\r\npython -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload\r\n\r\n----------------------------------\r\nOn a web browser open the link showed on the previous terminal (http://0.0.0.0:7860)\r\n\r\n----------------------------------\r\nIn another terminal:\r\n(Launch a model worker)\r\n\r\ncd /home/USER/github/LLaVa/LLaVA\r\nconda activate llava\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-name liuhaotian/LLaVA-Lightning-MPT-7B-preview --load-4bit\r\n\r\n```\r\n\r\nLog: \r\n```\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-name liuhaotian/LLaVA-Lightning-MPT-7B-preview\r\n[2023-08-25 18:47:40,132] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n2023-08-25 18:47:40.229722: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-08-25 18:47:40.316276: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-08-25 18:47:40.335442: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2023-08-25 18:47:40.702715: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\r\n2023-08-25 18:47:40.702761: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\r\n2023-08-25 18:47:40.702791: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n2023-08-25 18:47:41 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='facebook/opt-350m', model_base=None, model_name='liuhaotian/LLaVA-Lightning-MPT-7B-preview', multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=False)\r\n2023-08-25 18:47:41 | INFO | model_worker | Loading the model liuhaotian/LLaVA-Lightning-MPT-7B-preview on worker 3b6227 ...\r\nDownloading (…)okenizer_config.json:   0%|                | 0.00/685 [00:00<?, ?B/s]\r\nDownloading (…)okenizer_config.json: 100%|█████████| 685/685 [00:00<00:00, 1.76MB/s]\r\n2023-08-25 18:47:41 | ERROR | stderr | \r\nDownloading (…)lve/main/config.json:   0%|                | 0.00/644 [00:00<?, ?B/s]\r\nDownloading (…)lve/main/config.json: 100%|█████████| 644/644 [00:00<00:00, 3.61MB/s]\r\n2023-08-25 18:47:41 | ERROR | stderr | \r\nDownloading (…)olve/main/vocab.json:   0%|               | 0.00/899k [00:00<?, ?B/s]\r\nDownloading (…)olve/main/vocab.json: 100%|███████| 899k/899k [00:00<00:00, 2.34MB/s]\r\nDownloading (…)olve/main/vocab.json: 100%|███████| 899k/899k [00:00<00:00, 2.33MB/s]\r\n2023-08-25 18:47:42 | ERROR | stderr | \r\nDownloading (…)olve/main/merges.txt:   0%|               | 0.00/456k [00:00<?, ?B/s]\r\nDownloading (…)olve/main/merges.txt: 100%|███████| 456k/456k [00:00<00:00, 3.54MB/s]\r\nDownloading (…)olve/main/merges.txt: 100%|███████| 456k/456k [00:00<00:00, 3.51MB/s]\r\n2023-08-25 18:47:42 | ERROR | stderr | \r\nDownloading (…)cial_tokens_map.json:   0%|                | 0.00/441 [00:00<?, ?B/s]\r\nDownloading (…)cial_tokens_map.json: 100%|█████████| 441/441 [00:00<00:00, 1.23MB/s]\r\n2023-08-25 18:47:43 | ERROR | stderr | \r\nYou are using a model of type opt to instantiate a model of type llava_mpt. This is not supported for all configurations of models and can yield errors.\r\nDownloading pytorch_model.bin:   0%|                     | 0.00/663M [00:00<?, ?B/s]\r\nDownloading pytorch_model.bin:   2%|▏           | 10.5M/663M [00:00<00:50, 13.0MB/s]\r\nDownloading pytorch_model.bin:   3%|▍           | 21.0M/663M [00:01<00:54, 11.8MB/s]\r\nDownloading pytorch_model.bin:   5%|▌           | 31.5M/663M [00:02<00:40, 15.7MB/s]\r\nDownloading pytorch_model.bin:   6%|▊           | 41.9M/663M [00:02<00:38, 16.1MB/s]\r\nDownloading pytorch_model.bin:   8%|▉           | 52.4M/663M [00:03<00:39, 15.3MB/s]\r\nDownloading pytorch_model.bin:   9%|█▏          | 62.9M/663M [00:04<00:41, 14.6MB/s]\r\nDownloading pytorch_model.bin:  11%|█▎          | 73.4M/663M [00:05<00:45, 12.8MB/s]\r\nDownloading pytorch_model.bin:  13%|█▌          | 83.9M/663M [00:06<00:51, 11.2MB/s]\r\nDownloading pytorch_model.bin:  14%|█▋          | 94.4M/663M [00:07<00:57, 9.90MB/s]\r\nDownloading pytorch_model.bin:  16%|██           | 105M/663M [00:09<01:12, 7.66MB/s]\r\nDownloading pytorch_model.bin:  17%|██▎          | 115M/663M [00:11<01:18, 6.99MB/s]\r\nDownloading pytorch_model.bin:  19%|██▍          | 126M/663M [00:12<01:10, 7.63MB/s]\r\nDownloading pytorch_model.bin:  21%|██▋          | 136M/663M [00:13<01:04, 8.10MB/s]\r\nDownloading pytorch_model.bin:  22%|██▉          | 147M/663M [00:15<01:12, 7.09MB/s]\r\nDownloading pytorch_model.bin:  24%|███          | 157M/663M [00:16<01:06, 7.62MB/s]\r\nDownloading pytorch_model.bin:  25%|███▎         | 168M/663M [00:18<01:10, 6.97MB/s]\r\nDownloading pytorch_model.bin:  27%|███▍         | 178M/663M [00:20<01:18, 6.17MB/s]\r\nDownloading pytorch_model.bin:  28%|███▋         | 189M/663M [00:23<01:27, 5.44MB/s]\r\nDownloading pytorch_model.bin:  30%|███▉         | 199M/663M [00:25<01:30, 5.13MB/s]\r\nDownloading pytorch_model.bin:  32%|████         | 210M/663M [00:27<01:26, 5.21MB/s]\r\nDownloading pytorch_model.bin:  33%|████▎        | 220M/663M [00:29<01:21, 5.44MB/s]\r\nDownloading pytorch_model.bin:  35%|████▌        | 231M/663M [00:32<01:33, 4.61MB/s]\r\nDownloading pytorch_model.bin:  36%|████▋        | 241M/663M [00:34<01:24, 4.99MB/s]\r\nDownloading pytorch_model.bin:  38%|████▉        | 252M/663M [00:35<01:13, 5.62MB/s]\r\nDownloading pytorch_model.bin:  40%|█████▏       | 262M/663M [00:36<01:06, 6.06MB/s]\r\nDownloading pytorch_model.bin:  41%|█████▎       | 273M/663M [00:38<01:00, 6.44MB/s]\r\nDownloading pytorch_model.bin:  43%|█████▌       | 283M/663M [00:39<00:55, 6.81MB/s]\r\nDownloading pytorch_model.bin:  44%|█████▊       | 294M/663M [00:41<00:54, 6.73MB/s]\r\nDownloading pytorch_model.bin:  46%|█████▉       | 304M/663M [00:42<00:54, 6.59MB/s]\r\nDownloading pytorch_model.bin:  47%|██████▏      | 315M/663M [00:43<00:48, 7.23MB/s]\r\nDownloading pytorch_model.bin:  49%|██████▍      | 325M/663M [00:45<00:43, 7.81MB/s]\r\nDownloading pytorch_model.bin:  51%|██████▌      | 336M/663M [00:45<00:37, 8.79MB/s]\r\nDownloading pytorch_model.bin:  52%|██████▊      | 346M/663M [00:47<00:35, 8.84MB/s]\r\nDownloading pytorch_model.bin:  54%|██████▉      | 357M/663M [00:51<00:59, 5.17MB/s]\r\nDownloading pytorch_model.bin:  55%|███████▏     | 367M/663M [00:53<00:57, 5.12MB/s]\r\nDownloading pytorch_model.bin:  57%|███████▍     | 377M/663M [00:54<00:52, 5.44MB/s]\r\nDownloading pytorch_model.bin:  59%|███████▌     | 388M/663M [00:56<00:46, 5.95MB/s]\r\nDownloading pytorch_model.bin:  60%|███████▊     | 398M/663M [00:57<00:39, 6.72MB/s]\r\nDownloading pytorch_model.bin:  62%|████████     | 409M/663M [00:58<00:32, 7.82MB/s]\r\nDownloading pytorch_model.bin:  63%|████████▏    | 419M/663M [00:59<00:30, 8.02MB/s]\r\nDownloading pytorch_model.bin:  65%|████████▍    | 430M/663M [01:00<00:28, 8.02MB/s]\r\nDownloading pytorch_model.bin:  66%|████████▋    | 440M/663M [01:02<00:31, 7.12MB/s]\r\nDownloading pytorch_model.bin:  68%|████████▊    | 451M/663M [01:03<00:27, 7.76MB/s]\r\nDownloading pytorch_model.bin:  70%|█████████    | 461M/663M [01:04<00:22, 8.81MB/s]\r\nDownloading pytorch_model.bin:  71%|█████████▎   | 472M/663M [01:05<00:23, 8.25MB/s]\r\nDownloading pytorch_model.bin:  73%|█████████▍   | 482M/663M [01:06<00:20, 8.95MB/s]\r\nDownloading pytorch_model.bin:  74%|█████████▋   | 493M/663M [01:07<00:17, 9.80MB/s]\r\nDownloading pytorch_model.bin:  76%|█████████▉   | 503M/663M [01:08<00:15, 10.5MB/s]\r\nDownloading pytorch_model.bin:  78%|██████████   | 514M/663M [01:09<00:14, 10.4MB/s]\r\nDownloading pytorch_model.bin:  79%|██████████▎  | 524M/663M [01:12<00:19, 7.12MB/s]\r\nDownloading pytorch_model.bin:  81%|██████████▍  | 535M/663M [01:13<00:18, 6.98MB/s]\r\nDownloading pytorch_model.bin:  82%|██████████▋  | 545M/663M [01:14<00:16, 7.16MB/s]\r\nDownloading pytorch_model.bin:  84%|██████████▉  | 556M/663M [01:15<00:13, 7.97MB/s]\r\nDownloading pytorch_model.bin:  85%|███████████  | 566M/663M [01:17<00:11, 8.27MB/s]\r\nDownloading pytorch_model.bin:  87%|███████████▎ | 577M/663M [01:18<00:10, 8.35MB/s]\r\nDownloading pytorch_model.bin:  89%|███████████▌ | 587M/663M [01:19<00:09, 7.96MB/s]\r\nDownloading pytorch_model.bin:  90%|███████████▋ | 598M/663M [01:20<00:07, 8.30MB/s]\r\nDownloading pytorch_model.bin:  92%|███████████▉ | 608M/663M [01:22<00:06, 8.48MB/s]\r\nDownloading pytorch_model.bin:  93%|████████████▏| 619M/663M [01:23<00:05, 8.09MB/s]\r\nDownloading pytorch_model.bin:  95%|████████████▎| 629M/663M [01:24<00:03, 9.22MB/s]\r\nDownloading pytorch_model.bin:  97%|████████████▌| 640M/663M [01:24<00:02, 10.7MB/s]\r\nDownloading pytorch_model.bin:  98%|████████████▊| 650M/663M [01:25<00:01, 11.7MB/s]\r\nDownloading pytorch_model.bin: 100%|████████████▉| 661M/663M [01:26<00:00, 12.2MB/s]\r\nDownloading pytorch_model.bin: 100%|█████████████| 663M/663M [01:26<00:00, 12.4MB/s]\r\nDownloading pytorch_model.bin: 100%|█████████████| 663M/663M [01:26<00:00, 7.66MB/s]\r\n2023-08-25 18:49:10 | ERROR | stderr | \r\n2023-08-25 18:49:10 | INFO | stdout | You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\r\n2023-08-25 18:49:12 | WARNING | accelerate.utils.modeling | The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\r\nSome weights of LlavaMPTForCausalLM were not initialized from the model checkpoint at facebook/opt-350m and are newly initialized: ['transformer.blocks.17.ffn.up_proj.bias', 'transformer.blocks.6.ffn.up_proj.bias', 'transformer.blocks.1.attn.Wqkv.weight', 'transformer.blocks.3.norm_2.weight', 'transformer.blocks.15.norm_2.bias', 'transformer.blocks.14.ffn.up_proj.weight', 'transformer.blocks.18.norm_2.weight', 'transformer.blocks.19.attn.Wqkv.weight', 'transformer.blocks.19.attn.Wqkv.bias', 'transformer.blocks.4.norm_1.weight', 'transformer.blocks.10.ffn.down_proj.bias', 'transformer.blocks.21.norm_1.weight', 'transformer.blocks.19.ffn.down_proj.bias', 'transformer.blocks.17.norm_2.bias', 'transformer.blocks.16.norm_1.bias', 'transformer.blocks.11.norm_1.bias', 'transformer.blocks.18.ffn.down_proj.weight', 'transformer.blocks.14.norm_1.weight', 'transformer.blocks.16.attn.out_proj.weight', 'transformer.blocks.3.norm_1.weight', 'transformer.blocks.21.ffn.down_proj.weight', 'transformer.blocks.3.norm_1.bias', 'transformer.blocks.23.norm_2.bias', 'transformer.blocks.17.ffn.up_proj.weight', 'transformer.blocks.18.attn.out_proj.weight', 'transformer.blocks.7.ffn.down_proj.bias', 'transformer.blocks.8.norm_2.weight', 'transformer.blocks.8.norm_2.bias', 'transformer.blocks.12.norm_1.weight', 'transformer.blocks.13.attn.out_proj.weight', 'transformer.blocks.0.ffn.up_proj.bias', 'transformer.blocks.14.ffn.up_proj.bias', 'transformer.blocks.0.ffn.down_proj.bias', 'transformer.blocks.4.attn.out_proj.weight', 'transformer.blocks.12.norm_2.weight', 'transformer.blocks.20.attn.out_proj.bias', 'transformer.blocks.18.attn.Wqkv.bias', 'transformer.blocks.20.attn.Wqkv.bias', 'transformer.blocks.5.norm_1.weight', 'transformer.blocks.17.norm_2.weight', 'transformer.blocks.2.ffn.up_proj.weight', 'transformer.blocks.14.ffn.down_proj.bias', 'transformer.blocks.11.ffn.up_proj.bias', 'transformer.blocks.15.attn.Wqkv.bias', 'transformer.blocks.22.attn.out_proj.bias', 'transformer.blocks.5.attn.Wqkv.bias', 'transformer.blocks.5.ffn.down_proj.bias', 'transformer.blocks.7.ffn.up_proj.bias', 'transformer.wte.weight', 'transformer.blocks.0.attn.out_proj.bias', 'transformer.blocks.8.attn.Wqkv.weight', 'transformer.blocks.6.ffn.down_proj.bias', 'transformer.blocks.7.norm_1.weight', 'transformer.blocks.12.attn.out_proj.weight', 'transformer.norm_f.weight', 'transformer.blocks.10.ffn.down_proj.weight', 'transformer.blocks.23.norm_2.weight', 'transformer.blocks.0.attn.Wqkv.weight', 'transformer.blocks.1.attn.Wqkv.bias', 'transformer.blocks.15.attn.out_proj.weight', 'transformer.blocks.12.ffn.down_proj.weight', 'transformer.blocks.17.ffn.down_proj.bias', 'transformer.blocks.0.attn.Wqkv.bias', 'transformer.blocks.18.ffn.up_proj.bias', 'transformer.blocks.6.norm_1.bias', 'transformer.blocks.0.norm_1.weight', 'transformer.blocks.1.norm_1.bias', 'transformer.blocks.8.ffn.down_proj.weight', 'transformer.blocks.6.ffn.up_proj.weight', 'transformer.blocks.14.ffn.down_proj.weight', 'transformer.blocks.3.attn.out_proj.bias', 'transformer.blocks.6.ffn.down_proj.weight', 'transformer.blocks.17.attn.Wqkv.bias', 'transformer.blocks.9.norm_1.bias', 'transformer.blocks.20.ffn.down_proj.weight', 'transformer.blocks.5.attn.Wqkv.weight', 'transformer.blocks.21.norm_2.bias', 'transformer.blocks.0.norm_2.weight', 'transformer.blocks.2.attn.Wqkv.weight', 'transformer.blocks.23.ffn.down_proj.weight', 'transformer.blocks.13.ffn.down_proj.weight', 'transformer.blocks.14.attn.Wqkv.weight', 'transformer.blocks.15.ffn.down_proj.weight', 'transformer.blocks.8.attn.Wqkv.bias', 'transformer.blocks.23.ffn.up_proj.bias', 'transformer.blocks.6.attn.Wqkv.bias', 'transformer.blocks.9.ffn.down_proj.weight', 'transformer.blocks.11.ffn.down_proj.bias', 'transformer.blocks.10.norm_2.weight', 'transformer.blocks.13.norm_1.weight', 'transformer.blocks.18.norm_1.bias', 'transformer.blocks.19.norm_2.bias', 'transformer.blocks.3.norm_2.bias', 'transformer.blocks.21.norm_1.bias', 'transformer.blocks.9.norm_2.weight', 'transformer.blocks.20.norm_1.bias', 'transformer.blocks.1.norm_2.bias', 'transformer.blocks.4.attn.Wqkv.bias', 'transformer.blocks.10.attn.out_proj.weight', 'transformer.blocks.18.norm_2.bias', 'transformer.blocks.22.norm_1.weight', 'transformer.blocks.11.attn.out_proj.bias', 'transformer.blocks.7.norm_2.weight', 'transformer.blocks.19.norm_2.weight', 'transformer.norm_f.bias', 'transformer.blocks.23.attn.Wqkv.weight', 'transformer.blocks.17.norm_1.bias', 'transformer.blocks.20.norm_2.bias', 'transformer.blocks.11.norm_1.weight', 'transformer.blocks.18.norm_1.weight', 'transformer.blocks.22.ffn.up_proj.bias', 'transformer.blocks.2.ffn.down_proj.bias', 'transformer.blocks.0.ffn.down_proj.weight', 'transformer.blocks.12.norm_2.bias', 'transformer.blocks.3.ffn.up_proj.bias', 'transformer.blocks.16.ffn.down_proj.weight', 'transformer.blocks.21.ffn.down_proj.bias', 'transformer.wpe.weight', 'transformer.blocks.4.ffn.down_proj.bias', 'transformer.blocks.15.ffn.down_proj.bias', 'transformer.blocks.19.norm_1.weight', 'transformer.blocks.6.attn.Wqkv.weight', 'transformer.blocks.1.ffn.up_proj.bias', 'transformer.blocks.2.norm_1.bias', 'transformer.blocks.13.attn.Wqkv.bias', 'transformer.blocks.15.attn.Wqkv.weight', 'transformer.blocks.22.ffn.up_proj.weight', 'transformer.blocks.10.attn.Wqkv.bias', 'transformer.blocks.13.ffn.up_proj.bias', 'transformer.blocks.10.norm_1.weight', 'transformer.blocks.9.ffn.up_proj.bias', 'transformer.blocks.15.norm_1.weight', 'transformer.blocks.18.attn.out_proj.bias', 'transformer.blocks.11.attn.Wqkv.weight', 'transformer.blocks.20.norm_1.weight', 'transformer.blocks.8.ffn.up_proj.bias', 'transformer.blocks.1.norm_1.weight', 'transformer.blocks.2.ffn.down_proj.weight', 'transformer.blocks.15.norm_2.weight', 'transformer.blocks.20.ffn.up_proj.bias', 'transformer.blocks.2.norm_2.weight', 'transformer.blocks.7.norm_2.bias', 'transformer.blocks.21.attn.Wqkv.bias', 'transformer.blocks.4.norm_2.weight', 'transformer.blocks.19.ffn.down_proj.weight', 'transformer.blocks.23.attn.out_proj.weight', 'transformer.blocks.22.norm_1.bias', 'transformer.blocks.4.ffn.down_proj.weight', 'transformer.blocks.1.ffn.up_proj.weight', 'transformer.blocks.6.norm_1.weight', 'transformer.blocks.5.ffn.up_proj.bias', 'transformer.blocks.9.attn.Wqkv.bias', 'transformer.blocks.21.attn.Wqkv.weight', 'transformer.blocks.22.attn.out_proj.weight', 'transformer.blocks.20.ffn.up_proj.weight', 'transformer.blocks.2.norm_2.bias', 'transformer.blocks.13.norm_2.bias', 'transformer.blocks.14.attn.out_proj.bias', 'transformer.blocks.23.ffn.up_proj.weight', 'transformer.blocks.11.ffn.up_proj.weight', 'transformer.blocks.19.norm_1.bias', 'transformer.blocks.19.ffn.up_proj.bias', 'transformer.blocks.21.ffn.up_proj.weight', 'transformer.blocks.15.ffn.up_proj.weight', 'transformer.blocks.19.attn.out_proj.weight', 'transformer.blocks.13.ffn.down_proj.bias', 'transformer.blocks.22.attn.Wqkv.weight', 'transformer.blocks.2.norm_1.weight', 'transformer.blocks.9.ffn.up_proj.weight', 'transformer.blocks.23.attn.Wqkv.bias', 'transformer.blocks.4.norm_2.bias', 'transformer.blocks.17.attn.out_proj.weight', 'transformer.blocks.8.norm_1.bias', 'transformer.blocks.12.attn.Wqkv.bias', 'transformer.blocks.19.attn.out_proj.bias', 'transformer.blocks.5.attn.out_proj.weight', 'transformer.blocks.6.attn.out_proj.bias', 'transformer.blocks.12.attn.Wqkv.weight', 'transformer.blocks.0.norm_1.bias', 'transformer.blocks.1.attn.out_proj.bias', 'transformer.blocks.2.ffn.up_proj.bias', 'transformer.blocks.7.attn.Wqkv.bias', 'transformer.blocks.16.attn.Wqkv.bias', 'transformer.blocks.16.norm_2.bias', 'transformer.blocks.10.norm_1.bias', 'transformer.blocks.10.attn.out_proj.bias', 'transformer.blocks.8.attn.out_proj.weight', 'transformer.blocks.3.attn.Wqkv.weight', 'transformer.blocks.13.norm_1.bias', 'transformer.blocks.16.norm_2.weight', 'transformer.blocks.23.norm_1.bias', 'transformer.blocks.8.ffn.down_proj.bias', 'transformer.blocks.22.ffn.down_proj.weight', 'transformer.blocks.7.attn.out_proj.bias', 'transformer.blocks.16.norm_1.weight', 'transformer.blocks.13.attn.out_proj.bias', 'transformer.blocks.4.ffn.up_proj.bias', 'transformer.blocks.16.attn.out_proj.bias', 'transformer.blocks.23.ffn.down_proj.bias', 'transformer.blocks.5.norm_2.bias', 'transformer.blocks.7.attn.Wqkv.weight', 'transformer.blocks.1.attn.out_proj.weight', 'transformer.blocks.4.attn.out_proj.bias', 'transformer.blocks.7.attn.out_proj.weight', 'transformer.blocks.11.attn.out_proj.weight', 'transformer.blocks.18.ffn.down_proj.bias', 'transformer.blocks.23.attn.out_proj.bias', 'transformer.blocks.23.norm_1.weight', 'transformer.blocks.3.ffn.down_proj.bias', 'transformer.blocks.16.attn.Wqkv.weight', 'transformer.blocks.20.norm_2.weight', 'transformer.blocks.16.ffn.down_proj.bias', 'transformer.blocks.3.ffn.up_proj.weight', 'transformer.blocks.7.ffn.down_proj.weight', 'transformer.blocks.5.ffn.up_proj.weight', 'transformer.blocks.20.attn.out_proj.weight', 'transformer.blocks.7.ffn.up_proj.weight', 'transformer.blocks.0.norm_2.bias', 'transformer.blocks.10.ffn.up_proj.bias', 'transformer.blocks.17.ffn.down_proj.weight', 'transformer.blocks.5.norm_1.bias', 'transformer.blocks.9.norm_1.weight', 'transformer.blocks.22.norm_2.bias', 'transformer.blocks.1.ffn.down_proj.weight', 'transformer.blocks.9.ffn.down_proj.bias', 'transformer.blocks.21.ffn.up_proj.bias', 'transformer.blocks.3.attn.out_proj.weight', 'transformer.blocks.8.ffn.up_proj.weight', 'transformer.blocks.2.attn.out_proj.bias', 'transformer.blocks.5.norm_2.weight', 'transformer.blocks.12.norm_1.bias', 'transformer.blocks.14.norm_2.bias', 'transformer.blocks.21.norm_2.weight', 'transformer.blocks.15.norm_1.bias', 'transformer.blocks.22.ffn.down_proj.bias', 'transformer.blocks.5.attn.out_proj.bias', 'transformer.blocks.9.norm_2.bias', 'transformer.blocks.20.ffn.down_proj.bias', 'transformer.blocks.21.attn.out_proj.bias', 'transformer.blocks.13.ffn.up_proj.weight', 'transformer.blocks.14.norm_2.weight', 'transformer.blocks.12.ffn.down_proj.bias', 'transformer.blocks.11.ffn.down_proj.weight', 'transformer.blocks.9.attn.out_proj.bias', 'transformer.blocks.4.attn.Wqkv.weight', 'transformer.blocks.4.ffn.up_proj.weight', 'transformer.blocks.12.ffn.up_proj.bias', 'transformer.blocks.3.attn.Wqkv.bias', 'transformer.blocks.16.ffn.up_proj.weight', 'transformer.blocks.10.ffn.up_proj.weight', 'transformer.blocks.13.attn.Wqkv.weight', 'transformer.blocks.15.attn.out_proj.bias', 'transformer.blocks.16.ffn.up_proj.bias', 'transformer.blocks.17.attn.out_proj.bias', 'transformer.blocks.11.attn.Wqkv.bias', 'transformer.blocks.1.norm_2.weight', 'transformer.blocks.5.ffn.down_proj.weight', 'transformer.blocks.11.norm_2.bias', 'transformer.blocks.12.attn.out_proj.bias', 'transformer.blocks.1.ffn.down_proj.bias', 'transformer.blocks.11.norm_2.weight', 'transformer.blocks.12.ffn.up_proj.weight', 'transformer.blocks.9.attn.out_proj.weight', 'transformer.blocks.14.attn.Wqkv.bias', 'transformer.blocks.18.attn.Wqkv.weight', 'transformer.blocks.0.attn.out_proj.weight', 'transformer.blocks.13.norm_2.weight', 'transformer.blocks.14.norm_1.bias', 'transformer.blocks.6.attn.out_proj.weight', 'transformer.blocks.8.attn.out_proj.bias', 'transformer.blocks.10.attn.Wqkv.weight', 'transformer.blocks.10.norm_2.bias', 'transformer.blocks.2.attn.out_proj.weight', 'transformer.blocks.15.ffn.up_proj.bias', 'transformer.blocks.9.attn.Wqkv.weight', 'transformer.blocks.17.norm_1.weight', 'transformer.blocks.17.attn.Wqkv.weight', 'transformer.blocks.6.norm_2.weight', 'transformer.blocks.21.attn.out_proj.weight', 'transformer.blocks.3.ffn.down_proj.weight', 'transformer.blocks.18.ffn.up_proj.weight', 'transformer.blocks.8.norm_1.weight', 'transformer.blocks.0.ffn.up_proj.weight', 'transformer.blocks.4.norm_1.bias', 'transformer.blocks.14.attn.out_proj.weight', 'transformer.blocks.6.norm_2.bias', 'transformer.blocks.20.attn.Wqkv.weight', 'transformer.blocks.22.norm_2.weight', 'transformer.blocks.22.attn.Wqkv.bias', 'transformer.blocks.7.norm_1.bias', 'transformer.blocks.2.attn.Wqkv.bias', 'transformer.blocks.19.ffn.up_proj.weight']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\nDownloading (…)neration_config.json:   0%|                | 0.00/137 [00:00<?, ?B/s]\r\nDownloading (…)neration_config.json: 100%|██████████| 137/137 [00:00<00:00, 371kB/s]\r\n2023-08-25 18:49:12 | ERROR | stderr | \r\n2023-08-25 18:49:13 | ERROR | stderr | Traceback (most recent call last):\r\n2023-08-25 18:49:13 | ERROR | stderr |   File \"/home/USER/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n2023-08-25 18:49:13 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n2023-08-25 18:49:13 | ERROR | stderr |   File \"/home/USER/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n2023-08-25 18:49:13 | ERROR | stderr |     exec(code, run_globals)\r\n2023-08-25 18:49:13 | ERROR | stderr |   File \"/home/USER/github/LLaVa/LLaVA/llava/serve/model_worker.py\", line 273, in <module>\r\n2023-08-25 18:49:13 | ERROR | stderr |     worker = ModelWorker(args.controller_address,\r\n2023-08-25 18:49:13 | ERROR | stderr |   File \"/home/USER/github/LLaVa/LLaVA/llava/serve/model_worker.py\", line 64, in __init__\r\n2023-08-25 18:49:13 | ERROR | stderr |     self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\r\n2023-08-25 18:49:13 | ERROR | stderr |   File \"/home/USER/github/LLaVa/LLaVA/llava/model/builder.py\", line 135, in load_pretrained_model\r\n2023-08-25 18:49:13 | ERROR | stderr |     if not vision_tower.is_loaded:\r\n2023-08-25 18:49:13 | ERROR | stderr | AttributeError: 'NoneType' object has no attribute 'is_loaded'\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-08-26T05:31:51Z>\nHi @OrienKastor \r\n\r\nYou should use --model-**path** instead of --model-name:\r\n\r\n```Shell\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/LLaVA-Lightning-MPT-7B-preview --load-4bit\r\n```\r\n\r\nPlease let me know if you have found the `--model-name` in any docs, or you find any instructions confusing. I will make the correction. Thanks.\n</Comment>\n<Comment by nj159 at 2023-10-06T06:01:21Z>\nSorry, I need your help. I ran this code on the third terminal according to your help and encountered the following error. May I ask what the reason is?thanks very much\r\nThe error is as follows:\r\n(llava) root@nj11111:/opt/data/private/LLaVA# python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:7854 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/LLaVA-Lightning-MPT-7B-preview --load-4bit\r\n[2023-10-06 05:57:02,164] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n2023-10-06 05:57:02 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:7854', model_path='liuhaotian/LLaVA-Lightning-MPT-7B-preview', model_base=None, model_name=None, multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=True)\r\n2023-10-06 05:57:02 | INFO | model_worker | Loading the model LLaVA-Lightning-MPT-7B-preview on worker 5f003e ...\r\n'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fb25dc9d060>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: d499cfab-5f68-4c7f-a3de-c4d344697b46)')' thrown while requesting HEAD https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/tokenizer_config.json\r\n2023-10-06 05:57:12 | WARNING | huggingface_hub.utils._http | '(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fb25dc9d060>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: d499cfab-5f68-4c7f-a3de-c4d344697b46)')' thrown while requesting HEAD https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/tokenizer_config.json\r\n'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fb25dc9dff0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 91066e16-2eee-41e3-9869-ba879cb12a91)')' thrown while requesting HEAD https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/config.json\r\n2023-10-06 05:57:22 | WARNING | huggingface_hub.utils._http | '(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fb25dc9dff0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 91066e16-2eee-41e3-9869-ba879cb12a91)')' thrown while requesting HEAD https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/config.json\r\n2023-10-06 05:57:22 | ERROR | stderr | Traceback (most recent call last):\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/connection.py\", line 203, in _new_conn\r\n2023-10-06 05:57:22 | ERROR | stderr |     sock = connection.create_connection(\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/util/connection.py\", line 85, in create_connection\r\n2023-10-06 05:57:22 | ERROR | stderr |     raise err\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/util/connection.py\", line 73, in create_connection\r\n2023-10-06 05:57:22 | ERROR | stderr |     sock.connect(sa)\r\n2023-10-06 05:57:22 | ERROR | stderr | TimeoutError: timed out\r\n2023-10-06 05:57:22 | ERROR | stderr | \r\n2023-10-06 05:57:22 | ERROR | stderr | The above exception was the direct cause of the following exception:\r\n2023-10-06 05:57:22 | ERROR | stderr | \r\n2023-10-06 05:57:22 | ERROR | stderr | Traceback (most recent call last):\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 790, in urlopen\r\n2023-10-06 05:57:22 | ERROR | stderr |     response = self._make_request(\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 491, in _make_request\r\n2023-10-06 05:57:22 | ERROR | stderr |     raise new_e\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 467, in _make_request\r\n2023-10-06 05:57:22 | ERROR | stderr |     self._validate_conn(conn)\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 1092, in _validate_conn\r\n2023-10-06 05:57:22 | ERROR | stderr |     conn.connect()\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/connection.py\", line 611, in connect\r\n2023-10-06 05:57:22 | ERROR | stderr |     self.sock = sock = self._new_conn()\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/connection.py\", line 212, in _new_conn\r\n2023-10-06 05:57:22 | ERROR | stderr |     raise ConnectTimeoutError(\r\n2023-10-06 05:57:22 | ERROR | stderr | urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPSConnection object at 0x7fb25dc9dff0>, 'Connection to huggingface.co timed out. (connect timeout=10)')\r\n2023-10-06 05:57:22 | ERROR | stderr | \r\n2023-10-06 05:57:22 | ERROR | stderr | The above exception was the direct cause of the following exception:\r\n2023-10-06 05:57:22 | ERROR | stderr | \r\n2023-10-06 05:57:22 | ERROR | stderr | Traceback (most recent call last):\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\r\n2023-10-06 05:57:22 | ERROR | stderr |     resp = conn.urlopen(\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 844, in urlopen\r\n2023-10-06 05:57:22 | ERROR | stderr |     retries = retries.increment(\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/util/retry.py\", line 515, in increment\r\n2023-10-06 05:57:22 | ERROR | stderr |     raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\r\n2023-10-06 05:57:22 | ERROR | stderr | urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fb25dc9dff0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\r\n2023-10-06 05:57:22 | ERROR | stderr | \r\n2023-10-06 05:57:22 | ERROR | stderr | During handling of the above exception, another exception occurred:\r\n2023-10-06 05:57:22 | ERROR | stderr | \r\n2023-10-06 05:57:22 | ERROR | stderr | Traceback (most recent call last):\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1230, in hf_hub_download\r\n2023-10-06 05:57:22 | ERROR | stderr |     metadata = get_hf_file_metadata(\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\r\n2023-10-06 05:57:22 | ERROR | stderr |     return fn(*args, **kwargs)\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1597, in get_hf_file_metadata\r\n2023-10-06 05:57:22 | ERROR | stderr |     r = _request_wrapper(\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 417, in _request_wrapper\r\n2023-10-06 05:57:22 | ERROR | stderr |     response = _request_wrapper(\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 452, in _request_wrapper\r\n2023-10-06 05:57:22 | ERROR | stderr |     return http_backoff(\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 274, in http_backoff\r\n2023-10-06 05:57:22 | ERROR | stderr |     raise err\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 258, in http_backoff\r\n2023-10-06 05:57:22 | ERROR | stderr |     response = session.request(method=method, url=url, **kwargs)\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\r\n2023-10-06 05:57:22 | ERROR | stderr |     resp = self.send(prep, **send_kwargs)\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\r\n2023-10-06 05:57:22 | ERROR | stderr |     r = adapter.send(request, **kwargs)\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 63, in send\r\n2023-10-06 05:57:22 | ERROR | stderr |     return super().send(request, *args, **kwargs)\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/requests/adapters.py\", line 507, in send\r\n2023-10-06 05:57:22 | ERROR | stderr |     raise ConnectTimeout(e, request=request)\r\n2023-10-06 05:57:22 | ERROR | stderr | requests.exceptions.ConnectTimeout: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fb25dc9dff0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 91066e16-2eee-41e3-9869-ba879cb12a91)')\r\n2023-10-06 05:57:22 | ERROR | stderr | \r\n2023-10-06 05:57:22 | ERROR | stderr | The above exception was the direct cause of the following exception:\r\n2023-10-06 05:57:22 | ERROR | stderr | \r\n2023-10-06 05:57:22 | ERROR | stderr | Traceback (most recent call last):\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py\", line 417, in cached_file\r\n2023-10-06 05:57:22 | ERROR | stderr |     resolved_file = hf_hub_download(\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\r\n2023-10-06 05:57:22 | ERROR | stderr |     return fn(*args, **kwargs)\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1347, in hf_hub_download\r\n2023-10-06 05:57:22 | ERROR | stderr |     raise LocalEntryNotFoundError(\r\n2023-10-06 05:57:22 | ERROR | stderr | huggingface_hub.utils._errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.\r\n2023-10-06 05:57:22 | ERROR | stderr | \r\n2023-10-06 05:57:22 | ERROR | stderr | During handling of the above exception, another exception occurred:\r\n2023-10-06 05:57:22 | ERROR | stderr | \r\n2023-10-06 05:57:22 | ERROR | stderr | Traceback (most recent call last):\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n2023-10-06 05:57:22 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n2023-10-06 05:57:22 | ERROR | stderr |     exec(code, run_globals)\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/data/private/LLaVA/llava/serve/model_worker.py\", line 273, in <module>\r\n2023-10-06 05:57:22 | ERROR | stderr |     worker = ModelWorker(args.controller_address,\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/data/private/LLaVA/llava/serve/model_worker.py\", line 64, in __init__\r\n2023-10-06 05:57:22 | ERROR | stderr |     self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/data/private/LLaVA/llava/model/builder.py\", line 99, in load_pretrained_model\r\n2023-10-06 05:57:22 | ERROR | stderr |     tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 667, in from_pretrained\r\n2023-10-06 05:57:22 | ERROR | stderr |     config = AutoConfig.from_pretrained(\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 983, in from_pretrained\r\n2023-10-06 05:57:22 | ERROR | stderr |     config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 617, in get_config_dict\r\n2023-10-06 05:57:22 | ERROR | stderr |     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 672, in _get_config_dict\r\n2023-10-06 05:57:22 | ERROR | stderr |     resolved_config_file = cached_file(\r\n2023-10-06 05:57:22 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py\", line 452, in cached_file\r\n2023-10-06 05:57:22 | ERROR | stderr |     raise EnvironmentError(\r\n2023-10-06 05:57:22 | ERROR | stderr | OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like liuhaotian/LLaVA-Lightning-MPT-7B-preview is not the path to a directory containing a file named config.json.\r\n2023-10-06 05:57:22 | ERROR | stderr | Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\n</Comment>\n<Comment by 478786359 at 2023-10-13T12:36:15Z>\n> Sorry, I need your help. I ran this code on the third terminal according to your help and encountered the following error. May I ask what the reason is?thanks very much The error is as follows: (llava) root@nj11111:/opt/data/private/LLaVA# python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:7854 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/LLaVA-Lightning-MPT-7B-preview --load-4bit [2023-10-06 05:57:02,164] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect) 2023-10-06 05:57:02 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:7854', model_path='liuhaotian/LLaVA-Lightning-MPT-7B-preview', model_base=None, model_name=None, multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=True) 2023-10-06 05:57:02 | INFO | model_worker | Loading the model LLaVA-Lightning-MPT-7B-preview on worker 5f003e ... '(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fb25dc9d060>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: d499cfab-5f68-4c7f-a3de-c4d344697b46)')' thrown while requesting HEAD https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/tokenizer_config.json 2023-10-06 05:57:12 | WARNING | huggingface_hub.utils._http | '(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fb25dc9d060>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: d499cfab-5f68-4c7f-a3de-c4d344697b46)')' thrown while requesting HEAD https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/tokenizer_config.json '(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fb25dc9dff0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 91066e16-2eee-41e3-9869-ba879cb12a91)')' thrown while requesting HEAD https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/config.json 2023-10-06 05:57:22 | WARNING | huggingface_hub.utils._http | '(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fb25dc9dff0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 91066e16-2eee-41e3-9869-ba879cb12a91)')' thrown while requesting HEAD https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/config.json 2023-10-06 05:57:22 | ERROR | stderr | Traceback (most recent call last): 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/connection.py\", line 203, in _new_conn 2023-10-06 05:57:22 | ERROR | stderr | sock = connection.create_connection( 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/util/connection.py\", line 85, in create_connection 2023-10-06 05:57:22 | ERROR | stderr | raise err 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/util/connection.py\", line 73, in create_connection 2023-10-06 05:57:22 | ERROR | stderr | sock.connect(sa) 2023-10-06 05:57:22 | ERROR | stderr | TimeoutError: timed out 2023-10-06 05:57:22 | ERROR | stderr | 2023-10-06 05:57:22 | ERROR | stderr | The above exception was the direct cause of the following exception: 2023-10-06 05:57:22 | ERROR | stderr | 2023-10-06 05:57:22 | ERROR | stderr | Traceback (most recent call last): 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 790, in urlopen 2023-10-06 05:57:22 | ERROR | stderr | response = self._make_request( 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 491, in _make_request 2023-10-06 05:57:22 | ERROR | stderr | raise new_e 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 467, in _make_request 2023-10-06 05:57:22 | ERROR | stderr | self._validate_conn(conn) 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 1092, in _validate_conn 2023-10-06 05:57:22 | ERROR | stderr | conn.connect() 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/connection.py\", line 611, in connect 2023-10-06 05:57:22 | ERROR | stderr | self.sock = sock = self._new_conn() 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/connection.py\", line 212, in _new_conn 2023-10-06 05:57:22 | ERROR | stderr | raise ConnectTimeoutError( 2023-10-06 05:57:22 | ERROR | stderr | urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPSConnection object at 0x7fb25dc9dff0>, 'Connection to huggingface.co timed out. (connect timeout=10)') 2023-10-06 05:57:22 | ERROR | stderr | 2023-10-06 05:57:22 | ERROR | stderr | The above exception was the direct cause of the following exception: 2023-10-06 05:57:22 | ERROR | stderr | 2023-10-06 05:57:22 | ERROR | stderr | Traceback (most recent call last): 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send 2023-10-06 05:57:22 | ERROR | stderr | resp = conn.urlopen( 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 844, in urlopen 2023-10-06 05:57:22 | ERROR | stderr | retries = retries.increment( 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/util/retry.py\", line 515, in increment 2023-10-06 05:57:22 | ERROR | stderr | raise MaxRetryError(_pool, url, reason) from reason # type: ignore[arg-type] 2023-10-06 05:57:22 | ERROR | stderr | urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fb25dc9dff0>, 'Connection to huggingface.co timed out. (connect timeout=10)')) 2023-10-06 05:57:22 | ERROR | stderr | 2023-10-06 05:57:22 | ERROR | stderr | During handling of the above exception, another exception occurred: 2023-10-06 05:57:22 | ERROR | stderr | 2023-10-06 05:57:22 | ERROR | stderr | Traceback (most recent call last): 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1230, in hf_hub_download 2023-10-06 05:57:22 | ERROR | stderr | metadata = get_hf_file_metadata( 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn 2023-10-06 05:57:22 | ERROR | stderr | return fn(*args, **kwargs) 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1597, in get_hf_file_metadata 2023-10-06 05:57:22 | ERROR | stderr | r = _request_wrapper( 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 417, in _request_wrapper 2023-10-06 05:57:22 | ERROR | stderr | response = _request_wrapper( 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 452, in _request_wrapper 2023-10-06 05:57:22 | ERROR | stderr | return http_backoff( 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 274, in http_backoff 2023-10-06 05:57:22 | ERROR | stderr | raise err 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 258, in http_backoff 2023-10-06 05:57:22 | ERROR | stderr | response = session.request(method=method, url=url, **kwargs) 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request 2023-10-06 05:57:22 | ERROR | stderr | resp = self.send(prep, **send_kwargs) 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send 2023-10-06 05:57:22 | ERROR | stderr | r = adapter.send(request, **kwargs) 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 63, in send 2023-10-06 05:57:22 | ERROR | stderr | return super().send(request, *args, **kwargs) 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/requests/adapters.py\", line 507, in send 2023-10-06 05:57:22 | ERROR | stderr | raise ConnectTimeout(e, request=request) 2023-10-06 05:57:22 | ERROR | stderr | requests.exceptions.ConnectTimeout: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fb25dc9dff0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 91066e16-2eee-41e3-9869-ba879cb12a91)') 2023-10-06 05:57:22 | ERROR | stderr | 2023-10-06 05:57:22 | ERROR | stderr | The above exception was the direct cause of the following exception: 2023-10-06 05:57:22 | ERROR | stderr | 2023-10-06 05:57:22 | ERROR | stderr | Traceback (most recent call last): 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py\", line 417, in cached_file 2023-10-06 05:57:22 | ERROR | stderr | resolved_file = hf_hub_download( 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn 2023-10-06 05:57:22 | ERROR | stderr | return fn(*args, **kwargs) 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1347, in hf_hub_download 2023-10-06 05:57:22 | ERROR | stderr | raise LocalEntryNotFoundError( 2023-10-06 05:57:22 | ERROR | stderr | huggingface_hub.utils._errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on. 2023-10-06 05:57:22 | ERROR | stderr | 2023-10-06 05:57:22 | ERROR | stderr | During handling of the above exception, another exception occurred: 2023-10-06 05:57:22 | ERROR | stderr | 2023-10-06 05:57:22 | ERROR | stderr | Traceback (most recent call last): 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main 2023-10-06 05:57:22 | ERROR | stderr | return _run_code(code, main_globals, None, 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code 2023-10-06 05:57:22 | ERROR | stderr | exec(code, run_globals) 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/data/private/LLaVA/llava/serve/model_worker.py\", line 273, in 2023-10-06 05:57:22 | ERROR | stderr | worker = ModelWorker(args.controller_address, 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/data/private/LLaVA/llava/serve/model_worker.py\", line 64, in **init** 2023-10-06 05:57:22 | ERROR | stderr | self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model( 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/data/private/LLaVA/llava/model/builder.py\", line 99, in load_pretrained_model 2023-10-06 05:57:22 | ERROR | stderr | tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True) 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 667, in from_pretrained 2023-10-06 05:57:22 | ERROR | stderr | config = AutoConfig.from_pretrained( 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 983, in from_pretrained 2023-10-06 05:57:22 | ERROR | stderr | config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs) 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 617, in get_config_dict 2023-10-06 05:57:22 | ERROR | stderr | config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs) 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 672, in _get_config_dict 2023-10-06 05:57:22 | ERROR | stderr | resolved_config_file = cached_file( 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py\", line 452, in cached_file 2023-10-06 05:57:22 | ERROR | stderr | raise EnvironmentError( 2023-10-06 05:57:22 | ERROR | stderr | OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like liuhaotian/LLaVA-Lightning-MPT-7B-preview is not the path to a directory containing a file named config.json. 2023-10-06 05:57:22 | ERROR | stderr | Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\r\n\r\n\"I encountered the same issue, did you resolve it?\"\n</Comment>\n<Comment by nj159 at 2023-10-15T09:26:03Z>\n> > Sorry, I need your help. I ran this code on the third terminal according to your help and encountered the following error. May I ask what the reason is?thanks very much The error is as follows: (llava) root@nj11111:/opt/data/private/LLaVA# python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:7854 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/LLaVA-Lightning-MPT-7B-preview --load-4bit [2023-10-06 05:57:02,164] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect) 2023-10-06 05:57:02 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:7854', model_path='liuhaotian/LLaVA-Lightning-MPT-7B-preview', model_base=None, model_name=None, multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=True) 2023-10-06 05:57:02 | INFO | model_worker | Loading the model LLaVA-Lightning-MPT-7B-preview on worker 5f003e ... '(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fb25dc9d060>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: d499cfab-5f68-4c7f-a3de-c4d344697b46)')' thrown while requesting HEAD https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/tokenizer_config.json 2023-10-06 05:57:12 | WARNING | huggingface_hub.utils._http | '(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fb25dc9d060>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: d499cfab-5f68-4c7f-a3de-c4d344697b46)')' thrown while requesting HEAD https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/tokenizer_config.json '(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fb25dc9dff0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 91066e16-2eee-41e3-9869-ba879cb12a91)')' thrown while requesting HEAD https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/config.json 2023-10-06 05:57:22 | WARNING | huggingface_hub.utils._http | '(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fb25dc9dff0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 91066e16-2eee-41e3-9869-ba879cb12a91)')' thrown while requesting HEAD https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/config.json 2023-10-06 05:57:22 | ERROR | stderr | Traceback (most recent call last): 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/connection.py\", line 203, in _new_conn 2023-10-06 05:57:22 | ERROR | stderr | sock = connection.create_connection( 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/util/connection.py\", line 85, in create_connection 2023-10-06 05:57:22 | ERROR | stderr | raise err 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/util/connection.py\", line 73, in create_connection 2023-10-06 05:57:22 | ERROR | stderr | sock.connect(sa) 2023-10-06 05:57:22 | ERROR | stderr | TimeoutError: timed out 2023-10-06 05:57:22 | ERROR | stderr | 2023-10-06 05:57:22 | ERROR | stderr | The above exception was the direct cause of the following exception: 2023-10-06 05:57:22 | ERROR | stderr | 2023-10-06 05:57:22 | ERROR | stderr | Traceback (most recent call last): 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 790, in urlopen 2023-10-06 05:57:22 | ERROR | stderr | response = self._make_request( 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 491, in _make_request 2023-10-06 05:57:22 | ERROR | stderr | raise new_e 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 467, in _make_request 2023-10-06 05:57:22 | ERROR | stderr | self._validate_conn(conn) 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 1092, in _validate_conn 2023-10-06 05:57:22 | ERROR | stderr | conn.connect() 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/connection.py\", line 611, in connect 2023-10-06 05:57:22 | ERROR | stderr | self.sock = sock = self._new_conn() 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/connection.py\", line 212, in _new_conn 2023-10-06 05:57:22 | ERROR | stderr | raise ConnectTimeoutError( 2023-10-06 05:57:22 | ERROR | stderr | urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPSConnection object at 0x7fb25dc9dff0>, 'Connection to huggingface.co timed out. (connect timeout=10)') 2023-10-06 05:57:22 | ERROR | stderr | 2023-10-06 05:57:22 | ERROR | stderr | The above exception was the direct cause of the following exception: 2023-10-06 05:57:22 | ERROR | stderr | 2023-10-06 05:57:22 | ERROR | stderr | Traceback (most recent call last): 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send 2023-10-06 05:57:22 | ERROR | stderr | resp = conn.urlopen( 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 844, in urlopen 2023-10-06 05:57:22 | ERROR | stderr | retries = retries.increment( 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/urllib3/util/retry.py\", line 515, in increment 2023-10-06 05:57:22 | ERROR | stderr | raise MaxRetryError(_pool, url, reason) from reason # type: ignore[arg-type] 2023-10-06 05:57:22 | ERROR | stderr | urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fb25dc9dff0>, 'Connection to huggingface.co timed out. (connect timeout=10)')) 2023-10-06 05:57:22 | ERROR | stderr | 2023-10-06 05:57:22 | ERROR | stderr | During handling of the above exception, another exception occurred: 2023-10-06 05:57:22 | ERROR | stderr | 2023-10-06 05:57:22 | ERROR | stderr | Traceback (most recent call last): 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1230, in hf_hub_download 2023-10-06 05:57:22 | ERROR | stderr | metadata = get_hf_file_metadata( 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn 2023-10-06 05:57:22 | ERROR | stderr | return fn(*args, **kwargs) 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1597, in get_hf_file_metadata 2023-10-06 05:57:22 | ERROR | stderr | r = _request_wrapper( 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 417, in _request_wrapper 2023-10-06 05:57:22 | ERROR | stderr | response = _request_wrapper( 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 452, in _request_wrapper 2023-10-06 05:57:22 | ERROR | stderr | return http_backoff( 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 274, in http_backoff 2023-10-06 05:57:22 | ERROR | stderr | raise err 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 258, in http_backoff 2023-10-06 05:57:22 | ERROR | stderr | response = session.request(method=method, url=url, **kwargs) 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request 2023-10-06 05:57:22 | ERROR | stderr | resp = self.send(prep, **send_kwargs) 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send 2023-10-06 05:57:22 | ERROR | stderr | r = adapter.send(request, **kwargs) 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 63, in send 2023-10-06 05:57:22 | ERROR | stderr | return super().send(request, *args, **kwargs) 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/requests/adapters.py\", line 507, in send 2023-10-06 05:57:22 | ERROR | stderr | raise ConnectTimeout(e, request=request) 2023-10-06 05:57:22 | ERROR | stderr | requests.exceptions.ConnectTimeout: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /liuhaotian/LLaVA-Lightning-MPT-7B-preview/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fb25dc9dff0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 91066e16-2eee-41e3-9869-ba879cb12a91)') 2023-10-06 05:57:22 | ERROR | stderr | 2023-10-06 05:57:22 | ERROR | stderr | The above exception was the direct cause of the following exception: 2023-10-06 05:57:22 | ERROR | stderr | 2023-10-06 05:57:22 | ERROR | stderr | Traceback (most recent call last): 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py\", line 417, in cached_file 2023-10-06 05:57:22 | ERROR | stderr | resolved_file = hf_hub_download( 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn 2023-10-06 05:57:22 | ERROR | stderr | return fn(*args, **kwargs) 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1347, in hf_hub_download 2023-10-06 05:57:22 | ERROR | stderr | raise LocalEntryNotFoundError( 2023-10-06 05:57:22 | ERROR | stderr | huggingface_hub.utils._errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on. 2023-10-06 05:57:22 | ERROR | stderr | 2023-10-06 05:57:22 | ERROR | stderr | During handling of the above exception, another exception occurred: 2023-10-06 05:57:22 | ERROR | stderr | 2023-10-06 05:57:22 | ERROR | stderr | Traceback (most recent call last): 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main 2023-10-06 05:57:22 | ERROR | stderr | return _run_code(code, main_globals, None, 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code 2023-10-06 05:57:22 | ERROR | stderr | exec(code, run_globals) 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/data/private/LLaVA/llava/serve/model_worker.py\", line 273, in 2023-10-06 05:57:22 | ERROR | stderr | worker = ModelWorker(args.controller_address, 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/data/private/LLaVA/llava/serve/model_worker.py\", line 64, in **init** 2023-10-06 05:57:22 | ERROR | stderr | self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model( 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/data/private/LLaVA/llava/model/builder.py\", line 99, in load_pretrained_model 2023-10-06 05:57:22 | ERROR | stderr | tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True) 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 667, in from_pretrained 2023-10-06 05:57:22 | ERROR | stderr | config = AutoConfig.from_pretrained( 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 983, in from_pretrained 2023-10-06 05:57:22 | ERROR | stderr | config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs) 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 617, in get_config_dict 2023-10-06 05:57:22 | ERROR | stderr | config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs) 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 672, in _get_config_dict 2023-10-06 05:57:22 | ERROR | stderr | resolved_config_file = cached_file( 2023-10-06 05:57:22 | ERROR | stderr | File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py\", line 452, in cached_file 2023-10-06 05:57:22 | ERROR | stderr | raise EnvironmentError( 2023-10-06 05:57:22 | ERROR | stderr | OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like liuhaotian/LLaVA-Lightning-MPT-7B-preview is not the path to a directory containing a file named config.json. 2023-10-06 05:57:22 | ERROR | stderr | Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\r\n> \r\n> \"I encountered the same issue, did you resolve it?\"\r\n\r\nYes, I've solved it, you need to download both the model to be loaded and clip-336 to your local and then upload it to the corresponding directory on the server，if you can't connect the huggingface\n</Comment>\n<Comment by shiyishiaa at 2023-11-21T07:30:08Z>\nSame problem. :( \r\nCannot run the worker or CLI.\r\n\r\n```bash\r\n2023-11-21 15:00:54 | ERROR | stderr | Traceback (most recent call last):\r\n2023-11-21 15:00:54 | ERROR | stderr |   File \"/home/**/miniconda3/envs/llava/lib/                                                                                                                        python3.10/runpy.py\", line 196, in _run_module_as_main\r\n2023-11-21 15:00:54 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n2023-11-21 15:00:54 | ERROR | stderr |   File \"/home/**/miniconda3/envs/llava/lib/                                                                                                                        python3.10/runpy.py\", line 86, in _run_code\r\n2023-11-21 15:00:54 | ERROR | stderr |     exec(code, run_globals)\r\n2023-11-21 15:00:54 | ERROR | stderr |   File \"/mnt/sdb/home/**/vlm_test/LLaVA/lla                                                                                                                        va/serve/model_worker.py\", line 275, in <module>\r\n2023-11-21 15:00:54 | ERROR | stderr |     worker = ModelWorker(args.controller_addres                                                                                                                        s,\r\n2023-11-21 15:00:54 | ERROR | stderr |   File \"/mnt/sdb/home/**/vlm_test/LLaVA/lla                                                                                                                        va/serve/model_worker.py\", line 65, in __init__\r\n2023-11-21 15:00:54 | ERROR | stderr |     self.tokenizer, self.model, self.image_proc                                                                                                                        essor, self.context_len = load_pretrained_model(\r\n2023-11-21 15:00:54 | ERROR | stderr |   File \"/mnt/sdb/home/**/vlm_test/LLaVA/lla                                                                                                                        va/model/builder.py\", line 161, in load_pretrained_model\r\n2023-11-21 15:00:54 | ERROR | stderr |     if not vision_tower.is_loaded:\r\n2023-11-21 15:00:54 | ERROR | stderr | AttributeError: 'NoneType' object has no attrib                                                                                                                        ute 'is_loaded'\r\n```\r\n\r\nAnd this is the command:\r\n\r\n```bash\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path \"/home/**/.cache/huggingface/hub/models--liuhaotian--llava-v1.5-7b\"\r\n```\r\n\r\nOwing to the network issue, I cannot load the model online so I run it offline, and the checkpoint is downloaded from huggingface. I really need your help. :( @haotian-liu \r\n\r\nI muted my name by ** out of privacy consideration. It is not a mistake.\n</Comment>\n<Comment by zjyellow at 2024-04-19T12:50:00Z>\n> Same problem. :( Cannot run the worker or CLI.\r\n> \r\n> ```shell\r\n> 2023-11-21 15:00:54 | ERROR | stderr | Traceback (most recent call last):\r\n> 2023-11-21 15:00:54 | ERROR | stderr |   File \"/home/**/miniconda3/envs/llava/lib/                                                                                                                        python3.10/runpy.py\", line 196, in _run_module_as_main\r\n> 2023-11-21 15:00:54 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n> 2023-11-21 15:00:54 | ERROR | stderr |   File \"/home/**/miniconda3/envs/llava/lib/                                                                                                                        python3.10/runpy.py\", line 86, in _run_code\r\n> 2023-11-21 15:00:54 | ERROR | stderr |     exec(code, run_globals)\r\n> 2023-11-21 15:00:54 | ERROR | stderr |   File \"/mnt/sdb/home/**/vlm_test/LLaVA/lla                                                                                                                        va/serve/model_worker.py\", line 275, in <module>\r\n> 2023-11-21 15:00:54 | ERROR | stderr |     worker = ModelWorker(args.controller_addres                                                                                                                        s,\r\n> 2023-11-21 15:00:54 | ERROR | stderr |   File \"/mnt/sdb/home/**/vlm_test/LLaVA/lla                                                                                                                        va/serve/model_worker.py\", line 65, in __init__\r\n> 2023-11-21 15:00:54 | ERROR | stderr |     self.tokenizer, self.model, self.image_proc                                                                                                                        essor, self.context_len = load_pretrained_model(\r\n> 2023-11-21 15:00:54 | ERROR | stderr |   File \"/mnt/sdb/home/**/vlm_test/LLaVA/lla                                                                                                                        va/model/builder.py\", line 161, in load_pretrained_model\r\n> 2023-11-21 15:00:54 | ERROR | stderr |     if not vision_tower.is_loaded:\r\n> 2023-11-21 15:00:54 | ERROR | stderr | AttributeError: 'NoneType' object has no attrib                                                                                                                        ute 'is_loaded'\r\n> ```\r\n> \r\n> And this is the command:\r\n> \r\n> ```shell\r\n> python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path \"/home/**/.cache/huggingface/hub/models--liuhaotian--llava-v1.5-7b\"\r\n> ```\r\n> \r\n> Owing to the network issue, I cannot load the model online so I run it offline, and the checkpoint is downloaded from huggingface. I really need your help. :( @haotian-liu\r\n> \r\n> I muted my name by ** out of privacy consideration. It is not a mistake.\r\n\r\nsame problem in  liuhaotian/llava-v1.5-13b today, have you solved yet?\r\nwhen I changed to  llava-v1.5-7b, it seems to download \"mm_vision_tower\":\"openai/clip-vit-large-patch14-336\" according to config.json from liuhaotian/llava-v1.5-7b.\r\n\r\nbut in  liuhaotian/llava-v1.5-13b/config.json, there is not description about 'tower' , I thought I should change a older version...\n</Comment>\n<Comment by Wuhan-Zhang at 2024-07-22T08:03:47Z>\n> Same problem. :( Cannot run the worker or CLI.\r\n> \r\n> ```shell\r\n> 2023-11-21 15:00:54 | ERROR | stderr | Traceback (most recent call last):\r\n> 2023-11-21 15:00:54 | ERROR | stderr |   File \"/home/**/miniconda3/envs/llava/lib/                                                                                                                        python3.10/runpy.py\", line 196, in _run_module_as_main\r\n> 2023-11-21 15:00:54 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n> 2023-11-21 15:00:54 | ERROR | stderr |   File \"/home/**/miniconda3/envs/llava/lib/                                                                                                                        python3.10/runpy.py\", line 86, in _run_code\r\n> 2023-11-21 15:00:54 | ERROR | stderr |     exec(code, run_globals)\r\n> 2023-11-21 15:00:54 | ERROR | stderr |   File \"/mnt/sdb/home/**/vlm_test/LLaVA/lla                                                                                                                        va/serve/model_worker.py\", line 275, in <module>\r\n> 2023-11-21 15:00:54 | ERROR | stderr |     worker = ModelWorker(args.controller_addres                                                                                                                        s,\r\n> 2023-11-21 15:00:54 | ERROR | stderr |   File \"/mnt/sdb/home/**/vlm_test/LLaVA/lla                                                                                                                        va/serve/model_worker.py\", line 65, in __init__\r\n> 2023-11-21 15:00:54 | ERROR | stderr |     self.tokenizer, self.model, self.image_proc                                                                                                                        essor, self.context_len = load_pretrained_model(\r\n> 2023-11-21 15:00:54 | ERROR | stderr |   File \"/mnt/sdb/home/**/vlm_test/LLaVA/lla                                                                                                                        va/model/builder.py\", line 161, in load_pretrained_model\r\n> 2023-11-21 15:00:54 | ERROR | stderr |     if not vision_tower.is_loaded:\r\n> 2023-11-21 15:00:54 | ERROR | stderr | AttributeError: 'NoneType' object has no attrib                                                                                                                        ute 'is_loaded'\r\n> ```\r\n> \r\n> And this is the command:\r\n> \r\n> ```shell\r\n> python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path \"/home/**/.cache/huggingface/hub/models--liuhaotian--llava-v1.5-7b\"\r\n> ```\r\n> \r\n> Owing to the network issue, I cannot load the model online so I run it offline, and the checkpoint is downloaded from huggingface. I really need your help. :( @haotian-liu\r\n> \r\n> I muted my name by ** out of privacy consideration. It is not a mistake.\r\n\r\nI have experienced the same problem\n</Comment>\n<Comment by hoangducnhatminh at 2024-08-09T07:56:50Z>\nHave anyone solved this problems yet?\n</Comment>\n<Comment by rickeyhhh at 2024-12-18T08:59:57Z>\n> Same problem. :( Cannot run the worker or CLI.\r\n> \r\n> ```shell\r\n> 2023-11-21 15:00:54 | ERROR | stderr | Traceback (most recent call last):\r\n> 2023-11-21 15:00:54 | ERROR | stderr |   File \"/home/**/miniconda3/envs/llava/lib/                                                                                                                        python3.10/runpy.py\", line 196, in _run_module_as_main\r\n> 2023-11-21 15:00:54 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n> 2023-11-21 15:00:54 | ERROR | stderr |   File \"/home/**/miniconda3/envs/llava/lib/                                                                                                                        python3.10/runpy.py\", line 86, in _run_code\r\n> 2023-11-21 15:00:54 | ERROR | stderr |     exec(code, run_globals)\r\n> 2023-11-21 15:00:54 | ERROR | stderr |   File \"/mnt/sdb/home/**/vlm_test/LLaVA/lla                                                                                                                        va/serve/model_worker.py\", line 275, in <module>\r\n> 2023-11-21 15:00:54 | ERROR | stderr |     worker = ModelWorker(args.controller_addres                                                                                                                        s,\r\n> 2023-11-21 15:00:54 | ERROR | stderr |   File \"/mnt/sdb/home/**/vlm_test/LLaVA/lla                                                                                                                        va/serve/model_worker.py\", line 65, in __init__\r\n> 2023-11-21 15:00:54 | ERROR | stderr |     self.tokenizer, self.model, self.image_proc                                                                                                                        essor, self.context_len = load_pretrained_model(\r\n> 2023-11-21 15:00:54 | ERROR | stderr |   File \"/mnt/sdb/home/**/vlm_test/LLaVA/lla                                                                                                                        va/model/builder.py\", line 161, in load_pretrained_model\r\n> 2023-11-21 15:00:54 | ERROR | stderr |     if not vision_tower.is_loaded:\r\n> 2023-11-21 15:00:54 | ERROR | stderr | AttributeError: 'NoneType' object has no attrib                                                                                                                        ute 'is_loaded'\r\n> ```\r\n> \r\n> And this is the command:\r\n> \r\n> ```shell\r\n> python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path \"/home/**/.cache/huggingface/hub/models--liuhaotian--llava-v1.5-7b\"\r\n> ```\r\n> \r\n> Owing to the network issue, I cannot load the model online so I run it offline, and the checkpoint is downloaded from huggingface. I really need your help. :( @haotian-liu\r\n> \r\n> I muted my name by ** out of privacy consideration. It is not a mistake.\r\n\r\nsame issue, have you solved this?\n</Comment>\n<Comment by shiyishiaa at 2024-12-24T11:04:36Z>\n> > Same problem. :( Cannot run the worker or CLI.\r\n> > ```shell\r\n> > 2023-11-21 15:00:54 | ERROR | stderr | Traceback (most recent call last):\r\n> > 2023-11-21 15:00:54 | ERROR | stderr |   File \"/home/**/miniconda3/envs/llava/lib/                                                                                                                        python3.10/runpy.py\", line 196, in _run_module_as_main\r\n> > 2023-11-21 15:00:54 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n> > 2023-11-21 15:00:54 | ERROR | stderr |   File \"/home/**/miniconda3/envs/llava/lib/                                                                                                                        python3.10/runpy.py\", line 86, in _run_code\r\n> > 2023-11-21 15:00:54 | ERROR | stderr |     exec(code, run_globals)\r\n> > 2023-11-21 15:00:54 | ERROR | stderr |   File \"/mnt/sdb/home/**/vlm_test/LLaVA/lla                                                                                                                        va/serve/model_worker.py\", line 275, in <module>\r\n> > 2023-11-21 15:00:54 | ERROR | stderr |     worker = ModelWorker(args.controller_addres                                                                                                                        s,\r\n> > 2023-11-21 15:00:54 | ERROR | stderr |   File \"/mnt/sdb/home/**/vlm_test/LLaVA/lla                                                                                                                        va/serve/model_worker.py\", line 65, in __init__\r\n> > 2023-11-21 15:00:54 | ERROR | stderr |     self.tokenizer, self.model, self.image_proc                                                                                                                        essor, self.context_len = load_pretrained_model(\r\n> > 2023-11-21 15:00:54 | ERROR | stderr |   File \"/mnt/sdb/home/**/vlm_test/LLaVA/lla                                                                                                                        va/model/builder.py\", line 161, in load_pretrained_model\r\n> > 2023-11-21 15:00:54 | ERROR | stderr |     if not vision_tower.is_loaded:\r\n> > 2023-11-21 15:00:54 | ERROR | stderr | AttributeError: 'NoneType' object has no attrib                                                                                                                        ute 'is_loaded'\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > And this is the command:\r\n> > ```shell\r\n> > python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path \"/home/**/.cache/huggingface/hub/models--liuhaotian--llava-v1.5-7b\"\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > Owing to the network issue, I cannot load the model online so I run it offline, and the checkpoint is downloaded from huggingface. I really need your help. :( @haotian-liu\r\n> > I muted my name by ** out of privacy consideration. It is not a mistake.\r\n> \r\n> same issue, have you solved this?\r\n\r\nHad turned to other model\n</Comment>\n<Comment by rickeyhhh at 2024-12-24T12:55:43Z>\n> > > Same problem. :( Cannot run the worker or CLI.\r\n> > > ```shell\r\n> > > 2023-11-21 15:00:54 | ERROR | stderr | Traceback (most recent call last):\r\n> > > 2023-11-21 15:00:54 | ERROR | stderr |   File \"/home/**/miniconda3/envs/llava/lib/                                                                                                                        python3.10/runpy.py\", line 196, in _run_module_as_main\r\n> > > 2023-11-21 15:00:54 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n> > > 2023-11-21 15:00:54 | ERROR | stderr |   File \"/home/**/miniconda3/envs/llava/lib/                                                                                                                        python3.10/runpy.py\", line 86, in _run_code\r\n> > > 2023-11-21 15:00:54 | ERROR | stderr |     exec(code, run_globals)\r\n> > > 2023-11-21 15:00:54 | ERROR | stderr |   File \"/mnt/sdb/home/**/vlm_test/LLaVA/lla                                                                                                                        va/serve/model_worker.py\", line 275, in <module>\r\n> > > 2023-11-21 15:00:54 | ERROR | stderr |     worker = ModelWorker(args.controller_addres                                                                                                                        s,\r\n> > > 2023-11-21 15:00:54 | ERROR | stderr |   File \"/mnt/sdb/home/**/vlm_test/LLaVA/lla                                                                                                                        va/serve/model_worker.py\", line 65, in __init__\r\n> > > 2023-11-21 15:00:54 | ERROR | stderr |     self.tokenizer, self.model, self.image_proc                                                                                                                        essor, self.context_len = load_pretrained_model(\r\n> > > 2023-11-21 15:00:54 | ERROR | stderr |   File \"/mnt/sdb/home/**/vlm_test/LLaVA/lla                                                                                                                        va/model/builder.py\", line 161, in load_pretrained_model\r\n> > > 2023-11-21 15:00:54 | ERROR | stderr |     if not vision_tower.is_loaded:\r\n> > > 2023-11-21 15:00:54 | ERROR | stderr | AttributeError: 'NoneType' object has no attrib                                                                                                                        ute 'is_loaded'\r\n> > > ```\r\n> > > \r\n> > > \r\n> > >     \r\n> > >       \r\n> > >     \r\n> > > \r\n> > >       \r\n> > >     \r\n> > > \r\n> > >     \r\n> > >   \r\n> > > And this is the command:\r\n> > > ```shell\r\n> > > python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path \"/home/**/.cache/huggingface/hub/models--liuhaotian--llava-v1.5-7b\"\r\n> > > ```\r\n> > > \r\n> > > \r\n> > >     \r\n> > >       \r\n> > >     \r\n> > > \r\n> > >       \r\n> > >     \r\n> > > \r\n> > >     \r\n> > >   \r\n> > > Owing to the network issue, I cannot load the model online so I run it offline, and the checkpoint is downloaded from huggingface. I really need your help. :( @haotian-liu\r\n> > > I muted my name by ** out of privacy consideration. It is not a mistake.\r\n> > \r\n> > \r\n> > same issue, have you solved this?\r\n> \r\n> Had turned to other model\r\n\r\nThanks! I just solved this problem by replacing llava-1.5-7b-chat-hf with llava-v1.5-7b.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 394,
    "state": "closed",
    "created_by": "ldfandian",
    "created_at": "2023-08-25T23:52:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/394</URL>\n\n<TITLE>[Question] OOM on finetuning vicuna-7b llava model on 4*A800 80G, anything wrong with my cfg?</TITLE>\n\n<BODY>### Question\n\nThanks for the great work~\r\n\r\nAlso, it looks like A800 cannot enable flash-attn. (error screenshot below)\r\n\r\n```\r\npython \\\r\n    llava/train/train.py \\\r\n    --model_name_or_path /root/devroot/models/vicuna-7b-v1.3 \\\r\n    --version v1 \\\r\n    --data_path /root/devroot/datasets/llava_instruct/llava_instruct_150k.json \\\r\n    --image_folder /root/devroot/datasets/llava_instruct/coco-train2017/ \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/vicuna-7b-pretrain/mm_projector.bin \\\r\n    --mm_vision_select_layer -1 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/vicuna-7b-finetune \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 32 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 1000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\n<img width=\"623\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/5018331/2e894253-027b-40cc-94ee-13b9e2e5b2c2\">\r\n<img width=\"753\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/5018331/3716ebf5-0776-4db9-b857-a5657229d796\">\r\n<img width=\"752\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/5018331/b64e3127-f97f-4af4-b979-9f1f9b12ec78\"></BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-08-26T01:56:14Z>\nIt seems that A800 is supported in [flash-attn](https://github.com/Dao-AILab/flash-attention). Please try [this script](https://github.com/haotian-liu/LLaVA/blob/main/scripts/finetune.sh) with flash attention and deepspeed.\n</Comment>\n<Comment by ldfandian at 2023-08-26T02:13:55Z>\n> It seems that A800 is supported in [flash-attn](https://github.com/Dao-AILab/flash-attention). Please try [this script](https://github.com/haotian-liu/LLaVA/blob/main/scripts/finetune.sh) with flash attention and deepspeed.\r\n\r\nYeah, I tried. Error mesage is the same~\n</Comment>\n<Comment by haotian-liu at 2023-08-26T02:15:35Z>\nCan you provide the version of your flash attention, transformers, accelerate, and pytorch? Also, is flash attention compiled with the same CUDA version as PyTorch?\n</Comment>\n<Comment by ldfandian at 2023-08-26T02:23:00Z>\n> flash attention\r\n\r\nI followed the instruction from the project page.\r\n\r\nAlso, can you please tell if flash-attn and deepspeed is a MUST to do finetune on 4 * A800 80G?\r\nWhat if I upgrade the machine to use 8 * A800, will it be able to run w/o flash-attn and deepspeed?\r\n\r\n```\r\n> root / conda-environment.yaml\r\nname: llava\r\nchannels:\r\n  - defaults\r\ndependencies:\r\n  - _libgcc_mutex=0.1=main\r\n  - _openmp_mutex=5.1=1_gnu\r\n  - bzip2=1.0.8=h7b6447c_0\r\n  - ca-certificates=2023.05.30=h06a4308_0\r\n  - ld_impl_linux-64=2.38=h1181459_1\r\n  - libffi=3.4.4=h6a678d5_0\r\n  - libgcc-ng=11.2.0=h1234567_1\r\n  - libgomp=11.2.0=h1234567_1\r\n  - libstdcxx-ng=11.2.0=h1234567_1\r\n  - libuuid=1.41.5=h5eee18b_0\r\n  - ncurses=6.4=h6a678d5_0\r\n  - openssl=3.0.10=h7f8727e_2\r\n  - python=3.10.12=h955ad1f_0\r\n  - readline=8.2=h5eee18b_0\r\n  - sqlite=3.41.2=h5eee18b_0\r\n  - tk=8.6.12=h1ccaba5_0\r\n  - xz=5.4.2=h5eee18b_0\r\n  - zlib=1.2.13=h5eee18b_0\r\n  - pip:\r\n    - accelerate==0.21.0\r\n    - aiofiles==23.2.1\r\n    - aiohttp==3.8.5\r\n    - aiosignal==1.3.1\r\n    - altair==5.0.1\r\n    - anyio==3.7.1\r\n    - appdirs==1.4.4\r\n    - async-timeout==4.0.3\r\n    - attrs==23.1.0\r\n    - bitsandbytes==0.41.0\r\n    - certifi==2023.7.22\r\n    - charset-normalizer==3.2.0\r\n    - click==8.1.7\r\n    - cmake==3.27.2\r\n    - contourpy==1.1.0\r\n    - cycler==0.11.0\r\n    - deepspeed==0.9.5\r\n    - docker-pycreds==0.4.0\r\n    - einops==0.6.1\r\n    - einops-exts==0.0.4\r\n    - exceptiongroup==1.1.3\r\n    - fastapi==0.101.1\r\n    - ffmpy==0.3.1\r\n    - filelock==3.12.2\r\n    - flash-attn==2.0.9\r\n    - fonttools==4.42.1\r\n    - frozenlist==1.4.0\r\n    - fsspec==2023.6.0\r\n    - gitdb==4.0.10\r\n    - gitpython==3.1.32\r\n    - gradio==3.35.2\r\n    - gradio-client==0.2.9\r\n    - h11==0.14.0\r\n    - hjson==3.1.0\r\n    - httpcore==0.17.3\r\n    - httpx==0.24.0\r\n    - huggingface-hub==0.16.4\r\n    - idna==3.4\r\n    - jinja2==3.1.2\r\n    - joblib==1.3.2\r\n    - jsonschema==4.19.0\r\n    - jsonschema-specifications==2023.7.1\r\n    - kiwisolver==1.4.5\r\n    - linkify-it-py==2.0.2\r\n    - lit==16.0.6\r\n    - llava==1.0.1\r\n    - markdown-it-py==2.2.0\r\n    - markdown2==2.4.10\r\n    - markupsafe==2.1.3\r\n    - matplotlib==3.7.2\r\n    - mdit-py-plugins==0.3.3\r\n    - mdurl==0.1.2\r\n    - mpmath==1.3.0\r\n    - multidict==6.0.4\r\n    - networkx==3.1\r\n    - ninja==1.11.1\r\n    - numpy==1.25.2\r\n    - nvidia-cublas-cu11==11.10.3.66\r\n    - nvidia-cuda-cupti-cu11==11.7.101\r\n    - nvidia-cuda-nvrtc-cu11==11.7.99\r\n    - nvidia-cuda-runtime-cu11==11.7.99\r\n    - nvidia-cudnn-cu11==8.5.0.96\r\n    - nvidia-cufft-cu11==10.9.0.58\r\n    - nvidia-curand-cu11==10.2.10.91\r\n    - nvidia-cusolver-cu11==11.4.0.1\r\n    - nvidia-cusparse-cu11==11.7.4.91\r\n    - nvidia-nccl-cu11==2.14.3\r\n    - nvidia-nvtx-cu11==11.7.91\r\n    - orjson==3.9.5\r\n    - packaging==23.1\r\n    - pandas==2.0.3\r\n    - pathtools==0.1.2\r\n    - peft==0.4.0\r\n    - pillow==10.0.0\r\n    - pip==23.2.1\r\n    - protobuf==4.24.1\r\n    - psutil==5.9.5\r\n    - py-cpuinfo==9.0.0\r\n    - pydantic==1.10.12\r\n    - pydub==0.25.1\r\n    - pygments==2.16.1\r\n    - pyparsing==3.0.9\r\n    - python-dateutil==2.8.2\r\n    - python-multipart==0.0.6\r\n    - pytz==2023.3\r\n    - pyyaml==6.0.1\r\n    - referencing==0.30.2\r\n    - regex==2023.8.8\r\n    - requests==2.31.0\r\n    - rpds-py==0.9.2\r\n    - safetensors==0.3.3\r\n    - scikit-learn==1.2.2\r\n    - scipy==1.11.2\r\n    - semantic-version==2.10.0\r\n    - sentencepiece==0.1.99\r\n    - sentry-sdk==1.29.2\r\n    - setproctitle==1.3.2\r\n    - setuptools==68.0.0\r\n    - shortuuid==1.0.11\r\n    - six==1.16.0\r\n    - smmap==5.0.0\r\n    - sniffio==1.3.0\r\n    - starlette==0.27.0\r\n    - svgwrite==1.4.3\r\n    - sympy==1.12\r\n    - threadpoolctl==3.2.0\r\n    - timm==0.6.13\r\n    - tokenizers==0.13.3\r\n    - toolz==0.12.0\r\n    - torch==2.0.1\r\n    - torchvision==0.15.2\r\n    - tqdm==4.66.1\r\n    - transformers==4.31.0\r\n    - triton==2.0.0\r\n    - typing-extensions==4.7.1\r\n    - tzdata==2023.3\r\n    - uc-micro-py==1.0.2\r\n    - urllib3==2.0.4\r\n    - uvicorn==0.23.2\r\n    - wandb==0.15.8\r\n    - wavedrom==2.0.3.post3\r\n    - websockets==11.0.3\r\n    - wheel==0.38.4\r\n    - yarl==1.9.2\r\nprefix: /root/miniconda3/envs/llava\r\n```\n</Comment>\n<Comment by haotian-liu at 2023-08-26T02:35:41Z>\nCan you try `pip install flash-attn==2.0.4 --no-build-isolation`?\r\n\r\nI find that all my current versions are 2.0.4.\r\n\r\nAlso, flash attention is necessary. You may do gradient accumutation with bs4xaccu4, which can make 7B fit in 8x A100s (maybe 4x as well). But flash-attention brings at least 2x speedup in my experiments. So spending some time to make flash-attn work should worth it.\n</Comment>\n<Comment by ldfandian at 2023-08-26T02:38:43Z>\nGet~ Thanks.\r\n\r\nI don't have the box now. Will report the result back.\n</Comment>\n<Comment by ldfandian at 2023-08-26T03:00:45Z>\n> Can you try `pip install flash-attn==2.0.4 --no-build-isolation`?\r\n> \r\n> I find that all my current versions are 2.0.4.\r\n> \r\n> Also, flash attention is necessary. You may do gradient accumutation with bs4xaccu4, which can make 7B fit in 8x A100s (maybe 4x as well). But flash-attention brings at least 2x speedup in my experiments. So spending some time to make flash-attn work should worth it.\r\n\r\nBTW, even I make bs1xaccu16, it was still OOM... so that it looks like flash attn is a MUST.\n</Comment>\n<Comment by ldfandian at 2023-08-26T04:03:34Z>\n> Can you try `pip install flash-attn==2.0.4 --no-build-isolation`?\r\n> \r\n> I find that all my current versions are 2.0.4.\r\n> \r\n> Also, flash attention is necessary. You may do gradient accumutation with bs4xaccu4, which can make 7B fit in 8x A100s (maybe 4x as well). But flash-attention brings at least 2x speedup in my experiments. So spending some time to make flash-attn work should worth it.\r\n\r\nI tried... no luck. still the same error:\r\n```\r\n14   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n15     return forward_call(*args, **kwargs)\r\n16   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 171, in forward\r\n17     outputs = self.parallel_apply(replicas, inputs, kwargs)\r\n18   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 181, in parallel_apply\r\n19     return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\r\n20   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 89, in parallel_apply\r\n21     output.reraise()\r\n22   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py\", line 644, in reraise\r\n23     raise exception\r\n24 RuntimeError: Caught RuntimeError in replica 0 on device 0.\r\n25 Original Traceback (most recent call last):\r\n26   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\r\n27     output = module(*input, **kwargs)\r\n28   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n29     return forward_call(*args, **kwargs)\r\n30   File \"/root/devroot/src/LLaVA/llava/model/language_model/llava_llama.py\", line 78, in forward\r\n31     outputs = self.model(\r\n32   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n33     return forward_call(*args, **kwargs)\r\n34   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 685, in forward\r\n35     layer_outputs = torch.utils.checkpoint.checkpoint(\r\n36   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\r\n37     return CheckpointFunction.apply(function, preserve, *args)\r\n38   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\r\n39     return super().apply(*args, **kwargs)  # type: ignore[misc]\r\n40   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 107, in forward\r\n41     outputs = run_function(*args)\r\n42   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 681, in custom_forward\r\n43     return module(*inputs, output_attentions, None)\r\n44   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n45     return forward_call(*args, **kwargs)\r\n46   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 408, in forward\r\n47     hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n48   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n49     return forward_call(*args, **kwargs)\r\n50   File \"/root/devroot/src/LLaVA/llava/train/llama_flash_attn_monkey_patch.py\", line 92, in forward\r\n51     output_unpad = flash_attn_unpadded_qkvpacked_func(\r\n52   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py\", line 406, in flash_attn_varlen_qkvpacked_func\r\n53     return FlashAttnVarlenQKVPackedFunc.apply(\r\n54   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\r\n55     return super().apply(*args, **kwargs)  # type: ignore[misc]\r\n56   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py\", line 123, in forward\r\n57     out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_varlen_forward(\r\n58   File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py\", line 52, in _flash_attn_varlen_forward\r\n59     out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.varlen_fwd(\r\n60 RuntimeError: FlashAttention only support fp16 and bf16 data type\r\n```\r\n\r\n```\r\ndeepspeed llava/train/train_mem.py \\\r\n    --model_name_or_path /root/devroot/models/vicuna-7b-v1.3 \\\r\n    --version v1 \\\r\n    --data_path /root/devroot/datasets/llava/llava_instruct_150k.json \\\r\n    --image_folder /root/devroot/datasets/llava/train2017/ \\\r\n    --vision_tower openai/clip-vit-base-patch16 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/vicuna-7b-pretrain/mm_projector.bin \\\r\n    --mm_vision_select_layer -1 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/vicuna-7b-finetune \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 2 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 1000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 12 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\n```\r\nname: llava\r\nchannels:\r\n  - defaults\r\ndependencies:\r\n  - _libgcc_mutex=0.1=main\r\n  - _openmp_mutex=5.1=1_gnu\r\n  - bzip2=1.0.8=h7b6447c_0\r\n  - ca-certificates=2023.05.30=h06a4308_0\r\n  - ld_impl_linux-64=2.38=h1181459_1\r\n  - libffi=3.4.4=h6a678d5_0\r\n  - libgcc-ng=11.2.0=h1234567_1\r\n  - libgomp=11.2.0=h1234567_1\r\n  - libstdcxx-ng=11.2.0=h1234567_1\r\n  - libuuid=1.41.5=h5eee18b_0\r\n  - ncurses=6.4=h6a678d5_0\r\n  - openssl=3.0.10=h7f8727e_2\r\n  - python=3.10.12=h955ad1f_0\r\n  - readline=8.2=h5eee18b_0\r\n  - sqlite=3.41.2=h5eee18b_0\r\n  - tk=8.6.12=h1ccaba5_0\r\n  - xz=5.4.2=h5eee18b_0\r\n  - zlib=1.2.13=h5eee18b_0\r\n  - pip:\r\n    - accelerate==0.21.0\r\n    - aiofiles==23.2.1\r\n    - aiohttp==3.8.5\r\n    - aiosignal==1.3.1\r\n    - altair==5.0.1\r\n    - anyio==3.7.1\r\n    - appdirs==1.4.4\r\n    - async-timeout==4.0.3\r\n    - attrs==23.1.0\r\n    - bitsandbytes==0.41.0\r\n    - certifi==2023.7.22\r\n    - charset-normalizer==3.2.0\r\n    - click==8.1.7\r\n    - cmake==3.27.2\r\n    - contourpy==1.1.0\r\n    - cycler==0.11.0\r\n    - deepspeed==0.9.5\r\n    - docker-pycreds==0.4.0\r\n    - einops==0.6.1\r\n    - einops-exts==0.0.4\r\n    - exceptiongroup==1.1.3\r\n    - fastapi==0.102.0\r\n    - ffmpy==0.3.1\r\n    - filelock==3.12.2\r\n    - flash-attn==2.0.4\r\n    - fonttools==4.42.1\r\n    - frozenlist==1.4.0\r\n    - fsspec==2023.6.0\r\n    - gitdb==4.0.10\r\n    - gitpython==3.1.32\r\n    - gradio==3.35.2\r\n    - gradio-client==0.2.9\r\n    - h11==0.14.0\r\n    - hjson==3.1.0\r\n    - httpcore==0.17.3\r\n    - httpx==0.24.0\r\n    - huggingface-hub==0.16.4\r\n    - idna==3.4\r\n    - jinja2==3.1.2\r\n    - joblib==1.3.2\r\n    - jsonschema==4.19.0\r\n    - jsonschema-specifications==2023.7.1\r\n    - kiwisolver==1.4.5\r\n    - linkify-it-py==2.0.2\r\n    - lit==16.0.6\r\n    - llava==1.0.1\r\n    - markdown-it-py==2.2.0\r\n    - markdown2==2.4.10\r\n    - markupsafe==2.1.3\r\n    - matplotlib==3.7.2\r\n    - mdit-py-plugins==0.3.3\r\n    - mdurl==0.1.2\r\n    - mpmath==1.3.0\r\n    - multidict==6.0.4\r\n    - networkx==3.1\r\n    - ninja==1.11.1\r\n    - numpy==1.25.2\r\n    - nvidia-cublas-cu11==11.10.3.66\r\n    - nvidia-cuda-cupti-cu11==11.7.101\r\n    - nvidia-cuda-nvrtc-cu11==11.7.99\r\n    - nvidia-cuda-runtime-cu11==11.7.99\r\n    - nvidia-cudnn-cu11==8.5.0.96\r\n    - nvidia-cufft-cu11==10.9.0.58\r\n    - nvidia-curand-cu11==10.2.10.91\r\n    - nvidia-cusolver-cu11==11.4.0.1\r\n    - nvidia-cusparse-cu11==11.7.4.91\r\n    - nvidia-nccl-cu11==2.14.3\r\n    - nvidia-nvtx-cu11==11.7.91\r\n    - orjson==3.9.5\r\n    - packaging==23.1\r\n    - pandas==2.0.3\r\n    - pathtools==0.1.2\r\n    - peft==0.4.0\r\n    - pillow==10.0.0\r\n    - pip==23.2.1\r\n    - protobuf==4.24.2\r\n    - psutil==5.9.5\r\n    - py-cpuinfo==9.0.0\r\n    - pydantic==1.10.12\r\n    - pydub==0.25.1\r\n    - pygments==2.16.1\r\n    - pyparsing==3.0.9\r\n    - python-dateutil==2.8.2\r\n    - python-multipart==0.0.6\r\n    - pytz==2023.3\r\n    - pyyaml==6.0.1\r\n    - referencing==0.30.2\r\n    - regex==2023.8.8\r\n    - requests==2.31.0\r\n    - rpds-py==0.9.2\r\n    - safetensors==0.3.3\r\n    - scikit-learn==1.2.2\r\n    - scipy==1.11.2\r\n    - semantic-version==2.10.0\r\n    - sentencepiece==0.1.99\r\n    - sentry-sdk==1.29.2\r\n    - setproctitle==1.3.2\r\n    - setuptools==68.0.0\r\n    - shortuuid==1.0.11\r\n    - six==1.16.0\r\n    - smmap==5.0.0\r\n    - sniffio==1.3.0\r\n    - starlette==0.27.0\r\n    - svgwrite==1.4.3\r\n    - sympy==1.12\r\n    - threadpoolctl==3.2.0\r\n    - timm==0.6.13\r\n    - tokenizers==0.13.3\r\n    - toolz==0.12.0\r\n    - torch==2.0.1\r\n    - torchvision==0.15.2\r\n    - tqdm==4.66.1\r\n    - transformers==4.31.0\r\n    - triton==2.0.0\r\n    - typing-extensions==4.7.1\r\n    - tzdata==2023.3\r\n    - uc-micro-py==1.0.2\r\n    - urllib3==2.0.4\r\n    - uvicorn==0.23.2\r\n    - wandb==0.15.8\r\n    - wavedrom==2.0.3.post3\r\n    - websockets==11.0.3\r\n    - wheel==0.38.4\r\n    - yarl==1.9.2\r\nprefix: /root/miniconda3/envs/llava\r\n```\n</Comment>\n<Comment by ldfandian at 2023-08-26T04:07:59Z>\ni don't provide a \" --deepspeed /path/to/deepspeed.json \" in the run script. that should be ok, right?\r\n\r\nand, the pretrain ckpt comes from a non-flash-attn train.py instead of a flash-attn train_mem.py. should I re-pretrain everything w/ flash-attn enabled from a scratch?\n</Comment>\n<Comment by haotian-liu at 2023-08-26T04:26:15Z>\n>i don't provide a \" --deepspeed /path/to/deepspeed.json \" in the run script. that should be ok, right?\r\n\r\nThat is not okay. Please use [zero3.json](https://github.com/haotian-liu/LLaVA/blob/main/scripts/zero3.json) or zero2.\r\n\r\n> and, the pretrain ckpt comes from a non-flash-attn train.py instead of a flash-attn train_mem.py. should I re-pretrain everything w/ flash-attn enabled from a scratch?\r\n\r\nThat is not needed. it is a linear layer, so it will be fine.\n</Comment>\n<Comment by ldfandian at 2023-08-26T04:35:36Z>\n> > i don't provide a \" --deepspeed /path/to/deepspeed.json \" in the run script. that should be ok, right?\r\n> \r\n> That is not okay. Please use [zero3.json](https://github.com/haotian-liu/LLaVA/blob/main/scripts/zero3.json) or zero2.\r\n> \r\n> > and, the pretrain ckpt comes from a non-flash-attn train.py instead of a flash-attn train_mem.py. should I re-pretrain everything w/ flash-attn enabled from a scratch?\r\n> \r\n> That is not needed. it is a linear layer, so it will be fine.\r\n\r\nWoW! deepspeed w/ zero3.json works great~\r\n\r\nThanks for all the quick responses and your amazing work.\n</Comment>\n<Comment by ldfandian at 2023-08-26T04:39:15Z>\nBy using deepspeed w/ zero3.json, on my 8 * A100 80G, it takes ~15 hours for a full funetine (3 epochs).\r\nAnd, it takes only <35GB GPU memory for each A800 with a full (~90% GPU-util).\r\n\r\nIs the speed expected?\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/5018331/640abeb1-4a65-4d9d-a193-937c36afa593)\n</Comment>\n<Comment by haotian-liu at 2023-08-26T04:41:48Z>\n3699 iters is 3 epochs already. So you do not need to multiply by 3. ~6 hours is expected.\n</Comment>\n<Comment by ldfandian at 2023-08-26T08:57:46Z>\n> 3699 iters is 3 epochs already. So you do not need to multiply by 3. ~6 hours is expected.\r\n\r\nBTW, I find there is a considerable loss drop at the begining of each epoch. So, the reason we limit to 3 epoches is to prevent from too much overfit, right?\r\n\r\n![W B Chart 2023_8_26 16_55_05](https://github.com/haotian-liu/LLaVA/assets/5018331/b3bbe1e8-0823-44ef-93b7-5e226d97bdbe)\n</Comment>\n<Comment by ldfandian at 2023-08-29T09:03:37Z>\n> > 3699 iters is 3 epochs already. So you do not need to multiply by 3. ~6 hours is expected.\r\n> \r\n> BTW, I find there is a considerable loss drop at the begining of each epoch. So, the reason we limit to 3 epoches is to prevent from too much overfit, right?\r\n> \r\n> ![W B Chart 2023_8_26 16_55_05](https://user-images.githubusercontent.com/5018331/263453729-b3bbe1e8-0823-44ef-93b7-5e226d97bdbe.png)\r\n\r\n@haotian-liu\r\n\r\nAlso, I tried LLaVA, blip2-flant5-xl/xxl, instructblip-vicuna7b and found that LLaVA works best for the photos taken by my iphone. May I take this conclusion away? :\r\n 1. the quality of train data >>> 2. complexity of the architecture (Linear Layer vs QFormer) >>> 3. count of vit/llm model parameters.\r\n(coz LLaVa clearly has considerably less trainable parameters to connect vit and llm, but performs very well).\r\n\r\nAnd, how do you think of ResNet-50/101 as an image encoder? Will it perform similar like ViT?\n</Comment>\n<Comment by wizyoung at 2023-09-08T09:23:30Z>\n@ldfandian hi, I met the loss not converging well issue, can you post your full train log for me?\n</Comment>\n<Comment by wizyoung at 2023-09-08T09:25:59Z>\n@ldfandian Btw, are you using gradient checkpointing under deepspeed zero3? In my env here, it seems zero3 conflicts with checkpointing, but zero2 does not.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 392,
    "state": "closed",
    "created_by": "ohharsen",
    "created_at": "2023-08-25T09:36:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/392</URL>\n\n<TITLE>[Usage] CUDNN_STATUS_NOT_INITIALIZED when running worker from demo</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nI try following the instructions for demo but running into ad cuDNN issue.\r\nCommand:\r\n```\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./checkpoints/llava-llama-2-13b-chat-lightning-preview\r\n```\r\n\r\nLog: \r\n```\r\n2023-08-25 08:59:20 | ERROR | stderr | Traceback (most recent call last):\r\n2023-08-25 08:59:20 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\r\n2023-08-25 08:59:20 | ERROR | stderr |     self.run()\r\n2023-08-25 08:59:20 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/threading.py\", line 953, in run\r\n2023-08-25 08:59:20 | ERROR | stderr |     self._target(*self._args, **self._kwargs)\r\n2023-08-25 08:59:20 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\r\n2023-08-25 08:59:20 | ERROR | stderr |     return func(*args, **kwargs)\r\n2023-08-25 08:59:20 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1588, in generate\r\n2023-08-25 08:59:20 | ERROR | stderr |     return self.sample(\r\n2023-08-25 08:59:20 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2642, in sample\r\n2023-08-25 08:59:20 | ERROR | stderr |     outputs = self(\r\n2023-08-25 08:59:20 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\r\n2023-08-25 08:59:20 | ERROR | stderr |     return forward_call(*input, **kwargs)\r\n2023-08-25 08:59:20 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2023-08-25 08:59:20 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2023-08-25 08:59:20 | ERROR | stderr |   File \"/home/ec2-user/server/llava/model/language_model/llava_llama.py\", line 75, in forward\r\n2023-08-25 08:59:20 | ERROR | stderr |     input_ids, attention_mask, past_key_values, inputs_embeds, labels = self.prepare_inputs_labels_for_multimodal(input_ids, attention_mask, past_key_values, labels, images)\r\n2023-08-25 08:59:20 | ERROR | stderr |   File \"/home/ec2-user/server/llava/model/llava_arch.py\", line 102, in prepare_inputs_labels_for_multimodal\r\n2023-08-25 08:59:20 | ERROR | stderr |     image_features = self.encode_images(images)\r\n2023-08-25 08:59:20 | ERROR | stderr |   File \"/home/ec2-user/server/llava/model/llava_arch.py\", line 82, in encode_images\r\n2023-08-25 08:59:20 | ERROR | stderr |     image_features = self.get_model().get_vision_tower()(images)\r\n2023-08-25 08:59:20 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\r\n2023-08-25 08:59:20 | ERROR | stderr |     return forward_call(*input, **kwargs)\r\n2023-08-25 08:59:20 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2023-08-25 08:59:20 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2023-08-25 08:59:20 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\r\n2023-08-25 08:59:20 | ERROR | stderr |     return func(*args, **kwargs)\r\n2023-08-25 08:59:20 | ERROR | stderr |   File \"/home/ec2-user/server/llava/model/multimodal_encoder/clip_encoder.py\", line 48, in forward\r\n2023-08-25 08:59:20 | ERROR | stderr |     image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\r\n2023-08-25 08:59:20 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\r\n2023-08-25 08:59:20 | ERROR | stderr |     return forward_call(*input, **kwargs)\r\n2023-08-25 08:59:20 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 941, in forward\r\n2023-08-25 08:59:20 | ERROR | stderr |     return self.vision_model(\r\n2023-08-25 08:59:20 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\r\n2023-08-25 08:59:20 | ERROR | stderr |     return forward_call(*input, **kwargs)\r\n2023-08-25 08:59:20 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 866, in forward\r\n2023-08-25 08:59:20 | ERROR | stderr |     hidden_states = self.embeddings(pixel_values)\r\n2023-08-25 08:59:20 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\r\n2023-08-25 08:59:20 | ERROR | stderr |     return forward_call(*input, **kwargs)\r\n2023-08-25 08:59:20 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 195, in forward\r\n2023-08-25 08:59:20 | ERROR | stderr |     patch_embeds = self.patch_embedding(pixel_values)  # shape = [*, width, grid, grid]\r\n2023-08-25 08:59:20 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\r\n2023-08-25 08:59:20 | ERROR | stderr |     return forward_call(*input, **kwargs)\r\n2023-08-25 08:59:20 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 463, in forward\r\n2023-08-25 08:59:20 | ERROR | stderr |     return self._conv_forward(input, self.weight, self.bias)\r\n2023-08-25 08:59:20 | ERROR | stderr |   File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\r\n2023-08-25 08:59:20 | ERROR | stderr |     return F.conv2d(input, weight, bias, self.stride,\r\n2023-08-25 08:59:20 | ERROR | stderr | RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED\r\n```\r\n\r\nThe versions for packages for context:\r\n```\r\ntorch                     1.13.1+cu117\r\ntorchaudio                0.13.1+cu117\r\ntorchvision               0.14.1+cu117\r\ntransformers              4.31.0\r\n```\r\n\r\nI have tried the solution https://github.com/haotian-liu/LLaVA/issues/123#issuecomment-1539434115 but it was of no help\r\n\r\nAny idea what this might be caused by?</BODY>\n\n<COMMENTS>\n<Comment by ohharsen at 2023-08-25T20:41:30Z>\nFor anyone who will run into this situation in the future, this was merely a OOM issue as explainer [here](https://stackoverflow.com/a/69808279). Creating a mock convolution before inference helped to at least get the actual OOM error instead of the CUDNN_STATUS_NOT_INITIALIZED.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 390,
    "state": "closed",
    "created_by": "nighting0le01",
    "created_at": "2023-08-23T18:00:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/390</URL>\n\n<TITLE>[Question] How to use generated Adapter files from lora finetuning? for inference?</TITLE>\n\n<BODY>### Question\n\nHow to use generated Adapter files from lora finetuning? for inference? it says missing KeyError: 'LlavaConfig'\r\n![image](https://github.com/haotian-liu/LLaVA/assets/81643693/16c8cbcf-c9e9-4517-acdf-437eb8232f95)\r\nthis is what gets generated. how to convert this to llava weights to use with gradio</BODY>\n\n<COMMENTS>\n<Comment by FHL1998 at 2023-08-24T10:22:04Z>\nSame issue here, have you solved it?\n</Comment>\n<Comment by nighting0le01 at 2023-08-24T19:07:21Z>\nno still looking for solution @haotian-liu\n</Comment>\n<Comment by FHL1998 at 2023-08-25T06:06:34Z>\n> no still looking for solution @haotian-liu\r\n\r\nI figured it out, you just need to set arg `--model-base`.\n</Comment>\n<Comment by nighting0le01 at 2023-08-25T17:48:58Z>\nWhat to set --model-base as?\n</Comment>\n<Comment by haotian-liu at 2023-08-27T01:36:01Z>\nYou can refer to the instructions [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md#launch-a-model-worker).\r\n\r\nModel base should be the same as the base LLM you used to train LoRA.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 389,
    "state": "closed",
    "created_by": "FHL1998",
    "created_at": "2023-08-23T13:25:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/389</URL>\n\n<TITLE>[Question] How to define the model_name_or_path in finetuen.sh</TITLE>\n\n<BODY>### Question\n\nI have one dumb question that bothers me a lot: If I want to fine-tune LLaVa with my own dataset, how do I set the value of `model_name_or_path`? Should it be something like `lmsys/vicuna-7b-v1.3` or the model tuned by the author already？ Or does this depend on whether I want to fine-tune the model or the adaptor?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-28T19:45:44Z>\nYou should set it to LLaVA's model path. You can check out the example scripts: https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task.sh.\r\n\r\nThanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 387,
    "state": "open",
    "created_by": "MingsYang",
    "created_at": "2023-08-23T02:50:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/387</URL>\n\n<TITLE>[Question] Will it work better when the vision tower is not freezed during the first stage? Or what should I take care of?</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 386,
    "state": "closed",
    "created_by": "pipixiaqishi1",
    "created_at": "2023-08-22T10:30:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/386</URL>\n\n<TITLE>[Usage] KeyError: 'llava'</TITLE>\n\n<BODY>## Issue:\r\nTry to download this https://huggingface.co/liuhaotian/llava-llama-2-13b-chat-lightning-preview, using:\r\n\r\n\\# Load model directly\r\nfrom transformers import AutoModelForCausalLM\r\nmodel = AutoModelForCausalLM.from_pretrained(\"liuhaotian/llava-llama-2-13b-chat-lightning-preview\")\r\n\r\nface the problem: \r\n  File \"**.py\", line 3, in <module>\r\n    model = AutoModelForCausalLM.from_pretrained(\"liuhaotian/llava-llama-2-13b-chat-lightning-preview\")\r\n  File \"**/envs/llava/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 444, in from_pretrained\r\n    config, kwargs = AutoConfig.from_pretrained(\r\n  File \"**/envs/llava/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 940, in from_pretrained\r\n    config_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\r\n  File \"**/envs/llava/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 655, in __getitem__\r\n    raise KeyError(key)\r\nKeyError: 'llava'\r\n\r\nReinstall transformers but it didn't work...</BODY>\n\n<COMMENTS>\n<Comment by JadarTheObscurity at 2023-08-23T03:14:51Z>\nHey @pipixiaqishi1 I meet the same problem, do you have the solution?\n</Comment>\n<Comment by JadarTheObscurity at 2023-08-23T06:02:32Z>\nSolved with following python script\r\n```python\r\nfrom transformers import AutoConfig, LlamaConfig \r\nfrom llava.model.language_model.llava_llama import LlavaLlamaForCausalLM\r\nclass LlavaConfig(LlamaConfig):\r\n    model_type = \"llava\"\r\nAutoConfig.register(\"llava\", LlavaConfig)\r\nmodel = LlavaLlamaForCausalLM.from_pretrained(\"liuhaotian/llava-llama-2-13b-chat-lightning-preview\")\r\n```\n</Comment>\n<Comment by pipixiaqishi1 at 2023-08-23T08:02:59Z>\n> Solved with following python script\r\n> \r\n> ```python\r\n> from transformers import AutoConfig, LlamaConfig \r\n> from llava.model.language_model.llava_llama import LlavaLlamaForCausalLM\r\n> class LlavaConfig(LlamaConfig):\r\n>     model_type = \"llava\"\r\n> AutoConfig.register(\"llava\", LlavaConfig)\r\n> model = LlavaLlamaForCausalLM.from_pretrained(\"liuhaotian/llava-llama-2-13b-chat-lightning-preview\")\r\n> ```\r\n\r\nYes! I also found this code at https://github.com/haotian-liu/LLaVA/blob/237165b0166ad3fdc4fb0a7d881c857755d2404f/llava/model/language_model/llava_llama.py#L30-L42\r\nand followed the way to load the pretrained model. Thanks for sharing that!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 385,
    "state": "closed",
    "created_by": "dingning97",
    "created_at": "2023-08-22T10:07:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/385</URL>\n\n<TITLE>[Question] Why \"PROMPT_VERSION=plain\" is used for both llama-2 and vicuna-v1.3 in the pretraining script ?</TITLE>\n\n<BODY>### Question\n\nI noticed that \"PROMPT_VERSION=plain\" is used for both llama-2 and vicuna-v1.3 in the pretraining script.\r\nWhy not using \"PROMPT_VERSION=v1\" for vicuna-v1.3 and \"PROMPT_VERSION=llava_llama_2\" for llama-2 in the pretraining script? While the finetuning script does so...</BODY>\n\n<COMMENTS>\n<Comment by daiqing98 at 2023-10-29T08:53:31Z>\nHave you solved the issue? I am using vicuna-13b-v1.5\n</Comment>\n<Comment by haotian-liu at 2023-11-04T23:36:34Z>\nWe simplified the pretraining prompts by removing additional instructions like Describe the image details, which we find to allow the zero-shot inference and can slightly improve the training speed.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/releases/tag/v1.0.1\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 384,
    "state": "closed",
    "created_by": "viyjy",
    "created_at": "2023-08-22T06:52:26Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/384</URL>\n\n<TITLE>Which checkpoint is used in https://llava.hliu.cc/? Thanks</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-26T21:09:25Z>\nThe demo's version can change from time to time, and we'll indicate the version in the web. You can find the corresponding checkpoints here in the model zoo.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 383,
    "state": "closed",
    "created_by": "kanshichao",
    "created_at": "2023-08-21T09:00:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/383</URL>\n\n<TITLE>[Question] How to test using llava-llama2?</TITLE>\n\n<BODY>### Question\r\nI failed test the llava-llama2 model with the lateset version.\r\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-05T00:00:41Z>\nHi, please follow the instructions here to launch the demo, thanks.\r\n\r\nhttps://github.com/haotian-liu/LLaVA#demo\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 382,
    "state": "open",
    "created_by": "viyjy",
    "created_at": "2023-08-21T08:26:59Z",
    "labels": [
      "bug"
    ],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/382</URL>\n\n<TITLE>Different caption from https://llava.hliu.cc/ and llava.eval.run_llava</TITLE>\n\n<BODY>### Question\n\nHi, I have tried https://llava.hliu.cc/ and `llava.eval.run_llava` on the same image and query, but the generated captions are different, any ideas? Thanks\r\n\r\nSince the model used in https://llava.hliu.cc/ is `LLaVA-v1-13B-336px`, I use this [checkpoint](https://huggingface.co/liuhaotian/llava-v1-0719-336px-lora-merge-vicuna-13b-v1.3) for `llava.eval.run_llava`, is that correct?</BODY>\n\n<COMMENTS>\n<Comment by WLpub at 2023-08-29T12:58:18Z>\nhe same problem as yours, did you solve it? Also, my code is throwing errors, not sure if it's related to this.\r\n\r\nSome weights of the model checkpoint at /copilot/149611/llava-v1-0719-336px-lora-merge-vicuna-13b-v1.3 were not used when initializing LlavaLlamaForCausalLM\r\n\r\n['model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.position_ids', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight']\r\n- This IS expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\nSome weights of the model checkpoint at /copilot/149611/llava-v1-0719-336px-lora-merge-vicuna-13b-v1.3/openai/clip-vit-large-patch14-336 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_projection.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'logit_scale', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'visual_projection.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.layer_norm1.bias']\r\n- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n</Comment>\n<Comment by yash-s20 at 2023-08-31T19:25:52Z>\nSame issue. I think the vision tower is not loading correctly for me\n</Comment>\n<Comment by LemonWade at 2023-09-12T02:42:37Z>\n(llava) root@ubuntu:/data/zzy/LLaVA# `python -m llava.serve.cli --model-path /data/zzy/LLaVA/checkpoints/llava-llama-2-7b-chat-finetune-robot --image-file /data/zzy/LLaVA/dataset_fintue/saved_images/000_000_075.png --load-4bit`\r\n[2023-09-12 10:34:12,806] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nLoading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.01s/it]\r\nSome weights of the model checkpoint at /data/zzy/LLaVA/checkpoints/llava-llama-2-7b-chat-finetune-robot were not used when initializing LlavaLlamaForCausalLM: ['model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight',  .........]\r\n- This IS expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\nSome weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at /data/zzy/LLaVA/checkpoints/llava-llama-2-7b-chat-finetune-robot and are newly initialized: ['model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', ..........]\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n\r\nAlthough there are errors, the output indeed looks like it has been fine-tuned. How can these errors be fixed?\r\nThanks in advance。\n</Comment>\n<Comment by FHL1998 at 2023-10-08T03:59:04Z>\nFacing the same issue here. Has anyone solved the problem?\n</Comment>\n<Comment by haotian-liu at 2023-10-13T05:32:12Z>\nThis may be related to https://github.com/haotian-liu/LLaVA/pull/508\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 381,
    "state": "open",
    "created_by": "lg123666",
    "created_at": "2023-08-21T05:25:09Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/381</URL>\n\n<TITLE>[Usage] Failed for training with RuntimeError: weight should have at least three dimensions</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\n\r\nCommand:\r\n\r\n\r\n\r\npython llava/train/train_mem.py \\\r\n    --deepspeed scripts/zero3.json \\\r\n    --model_name_or_path /mnt/pretrained_params/llava_llama2/chinese-alpaca-2-7b \\\r\n    --version \"llava_llama_2\" \\\r\n    --data_path /mnt/public_dataset/LLAVA/llava_instruct_150k.jsonl \\\r\n    --vision_tower /mnt/files/models/llava/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter /mnt/llava_llama2/projector/llava-336px-pretrain-llama-2-7b-chat/mm_projector.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/ \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 2 \\\r\n    --per_device_eval_batch_size 2 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 1 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n    --loader exllama\r\nLog: \r\n\r\nFile \"/home/users/tom/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2654, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/home/users/tom/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2679, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/home/users/tom/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/users/tom/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 581, in forward\r\n    return model_forward(*args, **kwargs)\r\n  File \"/home/users/tom/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 569, in __call__\r\n    return convert_to_fp32(self.model_forward(*args, **kwargs))\r\n  File \"/home/users/tom/miniconda3/envs/llava/lib/python3.10/site-packages/torch/amp/autocast_mode.py\", line 14, in decorate_autocast\r\n    return func(*args, **kwargs)\r\n  File \"/home/users/tom/projects/LLaVA/mllm/llava/model/language_model/llava_llama.py\", line 75, in forward\r\n    input_ids, attention_mask, past_key_values, inputs_embeds, labels = self.prepare_inputs_labels_for_multimodal(input_ids, attention_mask, past_key_values, labels, images)\r\n  File \"/home/users/tom/projects/LLaVA/mllm/llava/model/llava_arch.py\", line 102, in prepare_inputs_labels_for_multimodal\r\n    image_features = self.encode_images(images)\r\n  File \"/home/users/tom/projects/LLaVA/mllm/llava/model/llava_arch.py\", line 82, in encode_images\r\n    image_features = self.get_model().get_vision_tower()(images)\r\n  File \"/home/users/tom/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/users/tom/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/users/tom/projects/LLaVA/mllm/llava/model/multimodal_encoder/clip_encoder.py\", line 49, in forward\r\n    image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\r\n  File \"/home/users/tom/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/users/tom/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 941, in forward\r\n    return self.vision_model(\r\n  File \"/home/users/tom/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/users/tom/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 866, in forward\r\n    hidden_states = self.embeddings(pixel_values)\r\n  File \"/home/users/tom/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/users/tom/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 195, in forward\r\n    patch_embeds = self.patch_embedding(pixel_values)  # shape = [*, width, grid, grid]\r\n  File \"/home/users/tom/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/users/tom/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 463, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n  File \"/home/users/tom/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\r\n    return F.conv2d(input, weight, bias, self.stride,\r\nRuntimeError: weight should have at least three dimensions\r\nwandb: Waiting for W&B process to finish... (failed 1).\r\nwandb: You can sync this run to the cloud by running:ped)\r\nwandb: wandb sync /home/users/tom/projects/LLaVA/mllm/wandb/offline-run-20230821_104204-z0nnbr9q\r\nwandb: Find logs at: ./wandb/offline-run-20230821_104204-z0nnbr9q/logs\r\n\r\nScreenshots:\r\nBut I running clip-vit-large-patch14 using official code is OK\r\n![image](https://github.com/haotian-liu/LLaVA/assets/40393969/430b1c83-1baf-45ec-8ed4-6d07df14017c)</BODY>\n\n<COMMENTS>\n<Comment by Gregory1994 at 2023-08-21T13:26:51Z>\nsame problem when using v100. But when i using A100, everything works well. \r\nI guess the problem made by deepspeed, which convert the clip model type to \"meta\" rather than bf16, and doesn't load the weight of clip.\n</Comment>\n<Comment by lg123666 at 2023-08-22T13:49:09Z>\n> same problem when using v100. But when i using A100, everything works well. I guess the problem made by deepspeed, which convert the clip model type to \"meta\" rather than bf16, and doesn't load the weight of clip.\r\n\r\nHave you tried running successfully without deepspeed? I want to debug it, so it is crucial for me to get through one iteration.\n</Comment>\n<Comment by 1106301825 at 2023-09-13T03:13:09Z>\nHave you solved it yet? I am having the same problem.\n</Comment>\n<Comment by 1106301825 at 2023-09-13T03:13:24Z>\n@lg123666\n</Comment>\n<Comment by lg123666 at 2023-09-19T06:33:34Z>\n> @lg123666\r\n\r\n\"--deepspeed\", \"scripts/zero2.json\" probably help you\r\n\r\n\r\nif you want debug model forward, \r\n\r\n# replace_llama_attn_with_flash_attn()  # or delete it\r\nmodel.to('cpu')\r\nmodel.float()\r\nmodel.eval()\r\nmodel.model.mm_projector.to(model.device)\n</Comment>\n<Comment by MarkDeng1 at 2025-01-14T20:17:18Z>\nhave you solve it ? @lg123666 @1106301825 @Gregory1994\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 380,
    "state": "closed",
    "created_by": "DarkAlchy",
    "created_at": "2023-08-19T21:28:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/380</URL>\n\n<TITLE>[Discussion] Dead project is dead on Windows?</TITLE>\n\n<BODY>### Discussion\n\nSeems Deepspeed is just not happening on a modern Windows machine with a modern GPUas CUDA 11.1?  Seriously?  \r\n\r\nIs there any way to get this to work without deepspeed as the pip install -e . dies on the installation part for deepspeed?</BODY>\n\n<COMMENTS>\n<Comment by DarkAlchy at 2023-08-20T00:08:38Z>\nI worked around this error as the -e , is totally broken, but I made it all the way to the model for it to bomb out.  Having to open three conda terminals is a bit odd and no dice.  Can't find a single llm for images that works on Windows so time to toss in the towel.\n</Comment>\n<Comment by haotian-liu at 2023-10-24T00:59:21Z>\nPlease check the latest doc for Windows. I tested on my Windows 11 PC and it works now for 16-bit inference. Quantization will be supported later.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/docs/Windows.md\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 379,
    "state": "open",
    "created_by": "yxchng",
    "created_at": "2023-08-18T06:24:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/379</URL>\n\n<TITLE>is LLaMA-2-7B-Chat full-ft ckpt going to be released?</TITLE>\n\n<BODY>### feature\n\ncurrently i only see LLaMA-2-13B-Chat full-ft ckpt being released.</BODY>\n\n<COMMENTS>\n<Comment by yash-s20 at 2023-08-30T06:36:03Z>\n+1 this to be part of the model zoo, considering that the LORA model that has been made public cannot be used for further fine-tuning on new data. thanks!\n</Comment>\n<Comment by yash-s20 at 2023-08-30T19:33:10Z>\nactually, i think i found a solution.\r\n\r\n\"scripts\" has a python script merge_lora_weights.py. this will give the \"full\" model that you need\r\n\r\nI ran it the following way:\r\n`python --model-path liuhaotian/llava-llama-2-7b-chat-lightning-lora-preview --model-base meta-llama/Llama-2-7b-chat-hf --save-model-path ./checkpoints/<your-model-name-here>`\n</Comment>\n<Comment by yash-s20 at 2023-08-30T19:34:05Z>\nthe model works well after the merge. i tried it on the demo example but didn't test it thoroughly\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 375,
    "state": "closed",
    "created_by": "khushboo-anand0909",
    "created_at": "2023-08-17T04:38:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/375</URL>\n\n<TITLE>Training loss 0 while finetuning</TITLE>\n\n<BODY>### Question\n\nHello, I'm trying to finetune the model on top of your provided checkpoints on my custom instruction dataset. However, I'm getting train loss 0 throughout all epochs while lora finetuning using finetune_lora.sh What can be the possible reason?</BODY>\n\n<COMMENTS>\n<Comment by kanshichao at 2023-08-18T15:16:48Z>\nI conducted pretraining of the model utilizing the 'zeros2.json' dataset, followed by subsequent model finetuning employing the 'zero3_offload.json' dataset, where a loss of 0 was similarly achieved during the finetuning phase. The batch_size is set to 32*4 on both stages.\n</Comment>\n<Comment by Fake10086 at 2023-09-09T06:26:24Z>\nhello, have you solved the problem?\n</Comment>\n<Comment by Fake10086 at 2023-09-09T13:00:12Z>\nHi, I've solved the problem. If this 'loss equals zero' problem relates with some tokenization mismatch which you get some warning, I believe it is the same problem that you are NOT choosing the corresponding version of Model and Tokenizer.\n</Comment>\n<Comment by haotian-liu at 2023-10-26T20:44:19Z>\nIt seems that most issues of loss=0 is related to the tokenizer version mismatch. Please try installing with the corresponding versions. Thanks.\n</Comment>\n<Comment by PCIResearch at 2023-12-18T11:43:35Z>\n> Hi, I've solved the problem. If this 'loss equals zero' problem relates with some tokenization mismatch which you get some warning, I believe it is the same problem that you are NOT choosing the corresponding version of Model and Tokenizer.\r\n\r\nI would like to ask you how you solved this problem?\n</Comment>\n<Comment by wanghao9610 at 2024-03-24T11:46:46Z>\nSomebody who encountered the issue may ref this issue (https://github.com/haotian-liu/LLaVA/issues/1231) for help.\n</Comment>\n<Comment by Logos23333 at 2024-03-28T09:22:19Z>\nAfter setting the args `model_max_length` to 512, the loss becomes 0. When set to 2048, the loss is normal, so there may be an issue with the tokenizer.\n</Comment>\n<Comment by eslambakr at 2024-04-27T23:06:09Z>\nThanks, @Logos23333. This was my issue as I was using another Image encoder that exceeded the model_max_length.\n</Comment>\n<Comment by Ayush-00 at 2024-06-06T18:41:43Z>\n> Somebody who encountered the issue may ref this issue (#1231) for help.\r\n\r\nThanks @wanghao9610 for the reference. My `model_max_length` was already 2048, but this fixed my issue!\n</Comment>\n<Comment by OmarBoumaize at 2025-05-21T12:04:04Z>\nsame probleme ( mt5-small ) decoder encoder architecture\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 373,
    "state": "closed",
    "created_by": "yichaoshen-MS",
    "created_at": "2023-08-15T12:14:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/373</URL>\n\n<TITLE>[Question] Question pretrain and fine-tune models in ScienceQA</TITLE>\n\n<BODY>### Question pretrain and fine-tune models in ScienceQA\r\n\r\nI find the script (about pretrain and fine-tune models in ScienceQA) has been removed from Readme. If I  want to  pretrain and fine-tune models in ScienceQA, which official script can I follow? Thanks,</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 372,
    "state": "open",
    "created_by": "Xinzhe-Ni",
    "created_at": "2023-08-15T03:06:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/372</URL>\n\n<TITLE>inference when using llava-llama-2 [Usage]</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nThanks for your great work!\r\nI train my model based on your released lava-llama-2-13b with lora and deepspeed, but have troubles when performing inference.\r\n1) The result checkpoint with lora and deepspeed does not have \"config.json\", so I guess the arg \"—base_model\" is needed, and I set it to \"checkpoints/llava-llama-2-13b-chat-lightning-preview\". \r\n2) I notice that the keyword in the name of checkpoint is very important. For my own checkpoint, there is no “llava” or “lora” in the name. And the loading process turns to below in \"builder.py\":\r\n```\r\n# Load language model\r\nif model_base is not None:\r\n    # PEFT model\r\n    from peft import PeftModel\r\n    tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n    ...\r\n```\r\n3) After 1) and 2), the image processor cannot be activated because there is no \"llava\" keyword in my checkpoint name, which is very weird.\r\n4) Also, I try to modify the name to contain \"llava\" or \"lora\", but it returns to 1) and the error is like: no \"config.json\".\r\n\r\nIn a word, my question is: how to run the \"run_llava.py\"? Which args should I specify? Is it a bug for llama-2 inference? If so, hope you can release your solution.\r\n\r\nThanks for your great work again and I am looking forward to your reply!\r\n\r\nCommand:\r\n```\r\npython -u eval/run_llava.py \\\r\n--model-path result_pretrain_2/checkpoint-56534 \\\r\n--model-base checkpoints/llava-llama-2-13b-chat-lightning-preview \\\r\n```</BODY>\n\n<COMMENTS>\n<Comment by basteran at 2023-10-05T13:06:52Z>\nAny news about this issue? @haotian-liu\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 371,
    "state": "closed",
    "created_by": "laserwave",
    "created_at": "2023-08-14T10:44:26Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/371</URL>\n\n<TITLE>pad_token set to unk_token</TITLE>\n\n<BODY>I wonder why pad_token set to unk_token\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/aa973cc96205263243c12f8108ae4ed22a9c9363/llava/train/train.py#L846</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-26T21:08:26Z>\nHi, this follows the design of Vicuna-v1. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 370,
    "state": "open",
    "created_by": "phnam05",
    "created_at": "2023-08-13T20:37:14Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/370</URL>\n\n<TITLE>[Can't load the model worker after downloading the model from HF]</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: Hi, I just get started on trying to install your demo but I have been trying for 2 weeks and I still cannot run it. I am at the part where I need to run the llava.serve.model_worker command. I manually downloaded the LLaVA-Lightning-MPT-7B-preview (https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview) model from your HF, and stored it in the same folder as my LLaVA model. I then run model_worker with this command\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path C:\\Users\\Admins\\Desktop\\llava\\LLaVA\\checkpoints\\liuhaotian_LLaVA-Lightning-MPT-7B-preview\r\n```\r\n\r\nAnd here's the error I got: \r\n\r\nLog: \r\n```\r\n2023-08-14 03:27:46 | INFO | model_worker | Loading the model C:\\Users\\Admins\\Desktop\\llava\\LLaVA\\checkpoints\\liuhaotian_LLaVA-Lightning-MPT-7B-preview on worker 58e5c9 ...\r\n2023-08-14 03:27:46 | INFO | stdout | You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\r\n2023-08-14 03:27:48 | WARNING | accelerate.utils.modeling | The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\r\n2023-08-14 03:27:48 | ERROR | stderr | Traceback (most recent call last):\r\n2023-08-14 03:27:48 | ERROR | stderr |   File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n2023-08-14 03:27:48 | ERROR | stderr |   File \"<frozen runpy>\", line 88, in _run_code\r\n2023-08-14 03:27:48 | ERROR | stderr |   File \"C:\\Users\\Admins\\Desktop\\llava\\LLaVA\\llava\\serve\\model_worker.py\", line 273, in <module>\r\n2023-08-14 03:27:48 | ERROR | stderr |     worker = ModelWorker(args.controller_address,\r\n2023-08-14 03:27:48 | ERROR | stderr |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2023-08-14 03:27:48 | ERROR | stderr |   File \"C:\\Users\\Admins\\Desktop\\llava\\LLaVA\\llava\\serve\\model_worker.py\", line 64, in __init__\r\n2023-08-14 03:27:48 | ERROR | stderr |     self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\r\n2023-08-14 03:27:48 | ERROR | stderr |                                                                          ^^^^^^^^^^^^^^^^^^^^^^\r\n2023-08-14 03:27:48 | ERROR | stderr |   File \"C:\\Users\\Admins\\Desktop\\llava\\LLaVA\\llava\\model\\builder.py\", line 97, in load_pretrained_model\r\n2023-08-14 03:27:48 | ERROR | stderr |     model = LlavaMPTForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\n2023-08-14 03:27:48 | ERROR | stderr |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2023-08-14 03:27:48 | ERROR | stderr |   File \"C:\\Users\\Admins\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py\", line 2903, in from_pretrained\r\n2023-08-14 03:27:48 | ERROR | stderr |     ) = cls._load_pretrained_model(\r\n2023-08-14 03:27:48 | ERROR | stderr |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2023-08-14 03:27:48 | ERROR | stderr |   File \"C:\\Users\\Admins\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py\", line 3002, in _load_pretrained_model\r\n2023-08-14 03:27:48 | ERROR | stderr |     raise ValueError(\r\n2023-08-14 03:27:48 | ERROR | stderr | ValueError: The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format.\r\n```\r\n\r\nThank you for your time. I hope this get responded soon</BODY>\n\n<COMMENTS>\n<Comment by nj159 at 2023-10-06T11:58:46Z>\nHave you solved this problem? I'm in the same situation as you，If you solved it, I ask for your help.\n</Comment>\n<Comment by lifenautjoe at 2023-10-10T10:45:58Z>\nSame issue! Any update?\n</Comment>\n<Comment by lifenautjoe at 2023-10-10T12:26:29Z>\nUpdate, I just added offload_folder=\"offload\" to the load_pretrained_model call and that worked\n</Comment>\n<Comment by amarflybot at 2023-11-02T15:42:37Z>\nI changed the kwargs, in builder.py:27\r\nto\r\n`kwargs = {\"device_map\": device_map, \"offload_folder\": \"offload\"}`\n</Comment>\n<Comment by amarflybot at 2023-11-02T15:44:06Z>\nNow the websocket connection is breaking.\r\nerror: `INFO | stdout | Caught Unknown Error`\r\nError from file model_worker.py:211\n</Comment>\n<Comment by haotian-liu at 2023-11-05T05:14:17Z>\nFor windows machines, can you try following the instructions [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/Windows.md) on the environments? Other users solve the issues by reinstalling the environments, thanks.\n</Comment>\n<Comment by Jiancong at 2023-11-18T10:57:36Z>\n> kwargs = {\"device_map\": device_map, \"offload_folder\": \"offload\"}\r\n\r\nMet same issue on Colab. Why you guys not merge this fix into main branch?\n</Comment>\n<Comment by yangsencai at 2025-03-04T00:49:41Z>\n> Update, I just added offload_folder=\"offload\" to the load_pretrained_model call and that worked\n\nfollow your instruction, it worked! thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 369,
    "state": "closed",
    "created_by": "chigkim",
    "created_at": "2023-08-12T16:01:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/369</URL>\n\n<TITLE>[Question] Llama-2 specify 2048 or 4096 for max_position_embeddings and model_max_length?</TITLE>\n\n<BODY>### Question\n\nLlama-2 has increased 4096 context size compare to 2048 for the original Llama.\r\nShould I change 2048 to 4096 for max_position_embeddings in config.json (hf model format) and --model_max_length in finetune.sh?\r\nOr, should I keep it 2048?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-13T02:09:33Z>\n2048 for training as most data falls into this range, and it may cause OOM if the seq is too long. 4096 for inference is fine if you want longer sequences.\n</Comment>\n<Comment by chigkim at 2023-10-13T05:02:46Z>\nThanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 368,
    "state": "closed",
    "created_by": "tfzhou",
    "created_at": "2023-08-12T12:20:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/368</URL>\n\n<TITLE>[Usage] Pretrain does not converge</TITLE>\n\n<BODY>### Describe the issue\n\nHi Haotian, thanks for the efforts on the project. At the moment I am trying to reproduce the pretrain stage, but got stuck in it. I have tried to train from various language models `vicuna-7b-v1.3/v1.5, Llama-2-7b-chat-hf` using deepspeed with `zero2` or `zero3` configurations. Unfortunately, these experiments did not go well -- training loss fails to converge and I found that LR schedule did not follow 'cosine' as specified in the command. I am unfamiliar with deepspeed, and uncertain whether the issue is from deepspeed. More details provided below and appreciate for your help.\r\n\r\nBtw. I used 4 A100 with 40GB memory for experiments. \r\n\r\nCommand:\r\n```\r\n# Uncomment and set the following variables correspondingly to run this script:\r\n\r\n#MODEL_VERSION=vicuna-7b-v1.3\r\nMODEL_VERSION=Llama-2-7b-chat-hf\r\n\r\n########### DO NOT CHANGE ###########\r\n########### USE THIS FOR BOTH ###########\r\nPROMPT_VERSION=plain\r\n########### DO NOT CHANGE ###########\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --model_name_or_path llama/$MODEL_VERSION \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path dataset/llava/LLaVA-CC3M-Pretrain-595K/chat.json \\\r\n    --image_folder dataset/llava/LLaVA-CC3M-Pretrain-595K/images \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-$MODEL_VERSION-pretrain \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 2 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 24000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb \\\r\n    --deepspeed scripts/zero2.json \\\r\n```\r\nzero2 (not changed)\r\n```\r\n{\r\n    \"fp16\": {\r\n        \"enabled\": \"auto\",\r\n        \"loss_scale\": 0,\r\n        \"loss_scale_window\": 1000,\r\n        \"initial_scale_power\": 16,\r\n        \"hysteresis\": 2,\r\n        \"min_loss_scale\": 1\r\n    },\r\n    \"bf16\": {\r\n        \"enabled\": \"auto\"\r\n    },\r\n    \"train_micro_batch_size_per_gpu\": \"auto\",\r\n    \"train_batch_size\": \"auto\",\r\n    \"gradient_accumulation_steps\": \"auto\",\r\n    \"zero_optimization\": {\r\n        \"stage\": 2,\r\n        \"overlap_comm\": true,\r\n        \"contiguous_gradients\": true,\r\n        \"sub_group_size\": 1e9,\r\n        \"reduce_bucket_size\": \"auto\"\r\n    }\r\n}\r\n```\r\n\r\nScreenshots:\r\n![Screenshot 2023-08-12 at 14 13 05](https://github.com/haotian-liu/LLaVA/assets/5791480/57aaa7ca-dee9-443f-91cf-436e92ac2515)\r\n![Screenshot 2023-08-12 at 14 13 25](https://github.com/haotian-liu/LLaVA/assets/5791480/ffe180aa-476f-4228-b634-6958f1e8d46c)</BODY>\n\n<COMMENTS>\n<Comment by tfzhou at 2023-08-17T14:10:06Z>\nJust found that the issue stems from flash-attention. after turning into `train.py`, the pretrain works properly. \r\n\r\n@haotian-liu what's the version of flash-att you using? my version is 2.0.4. probably I should use 1.x? Beyond this, do you observe performance differences when using `train.py` or `train_mem.py`?\n</Comment>\n<Comment by haotian-liu at 2023-08-17T16:29:23Z>\n@tfzhou \r\n\r\nI have locally tested this again on 2x 3090, per-device batch size=16, on llama-2-7b-chat. `train.py` and `train_mem.py` works similarly for me. I am using flash-attention 2.0.4, pytorch 2.0.1, and my cuda version is 11.7.\r\nOne thing is to make sure that the cuda verision of pytorch and your nvcc when you compile flash attention is the same. (please kindly let me know if this is the case, so that other community members can benefit as well :)\r\n\r\nYou may choose to downgrade to flash attention 1.x, and our code base currently support both 1.x and 2.x for A100s.\r\n\r\nAlso I attached the log of first 35 training steps on 2x 3090 (total bs: 16x2=32). It seems that your LR is not correctly decayed as the warmup should only be 3%.\r\n\r\n\r\n<details>\r\n<summary>train_mem.py</summary>\r\n\r\n```\r\n{'loss': 8.0312, 'learning_rate': 3.5778175313059034e-06, 'epoch': 0.0}\r\n{'loss': 7.9531, 'learning_rate': 7.155635062611807e-06, 'epoch': 0.0}\r\n{'loss': 7.7969, 'learning_rate': 1.073345259391771e-05, 'epoch': 0.0}\r\n{'loss': 8.125, 'learning_rate': 1.4311270125223614e-05, 'epoch': 0.0}\r\n{'loss': 7.8594, 'learning_rate': 1.7889087656529517e-05, 'epoch': 0.0}\r\n{'loss': 7.7656, 'learning_rate': 2.146690518783542e-05, 'epoch': 0.0}\r\n{'loss': 7.4219, 'learning_rate': 2.5044722719141324e-05, 'epoch': 0.0}\r\n{'loss': 7.0, 'learning_rate': 2.8622540250447228e-05, 'epoch': 0.0}\r\n{'loss': 6.8438, 'learning_rate': 3.2200357781753134e-05, 'epoch': 0.0}\r\n{'loss': 6.5469, 'learning_rate': 3.5778175313059034e-05, 'epoch': 0.0}\r\n{'loss': 6.4688, 'learning_rate': 3.935599284436494e-05, 'epoch': 0.0}\r\n{'loss': 6.3594, 'learning_rate': 4.293381037567084e-05, 'epoch': 0.0}\r\n{'loss': 6.2031, 'learning_rate': 4.651162790697674e-05, 'epoch': 0.0}\r\n{'loss': 6.1875, 'learning_rate': 5.008944543828265e-05, 'epoch': 0.0}\r\n{'loss': 6.0625, 'learning_rate': 5.366726296958855e-05, 'epoch': 0.0}\r\n{'loss': 6.0938, 'learning_rate': 5.7245080500894455e-05, 'epoch': 0.0}\r\n{'loss': 5.8281, 'learning_rate': 6.082289803220036e-05, 'epoch': 0.0}\r\n{'loss': 5.9531, 'learning_rate': 6.440071556350627e-05, 'epoch': 0.0}\r\n{'loss': 5.6719, 'learning_rate': 6.797853309481217e-05, 'epoch': 0.0}\r\n{'loss': 5.5781, 'learning_rate': 7.155635062611807e-05, 'epoch': 0.0}\r\n{'loss': 5.4688, 'learning_rate': 7.513416815742398e-05, 'epoch': 0.0}\r\n{'loss': 5.4219, 'learning_rate': 7.871198568872988e-05, 'epoch': 0.0}\r\n{'loss': 5.4062, 'learning_rate': 8.228980322003578e-05, 'epoch': 0.0}\r\n{'loss': 5.4375, 'learning_rate': 8.586762075134168e-05, 'epoch': 0.0}\r\n{'loss': 5.3594, 'learning_rate': 8.94454382826476e-05, 'epoch': 0.0}\r\n{'loss': 5.2031, 'learning_rate': 9.302325581395348e-05, 'epoch': 0.0}\r\n{'loss': 5.1406, 'learning_rate': 9.660107334525938e-05, 'epoch': 0.0}\r\n{'loss': 4.9531, 'learning_rate': 0.0001001788908765653, 'epoch': 0.0}\r\n{'loss': 4.9844, 'learning_rate': 0.0001037567084078712, 'epoch': 0.0}\r\n{'loss': 5.0938, 'learning_rate': 0.0001073345259391771, 'epoch': 0.0}\r\n{'loss': 4.8594, 'learning_rate': 0.00011091234347048301, 'epoch': 0.0}\r\n{'loss': 4.9688, 'learning_rate': 0.00011449016100178891, 'epoch': 0.0}\r\n{'loss': 4.9844, 'learning_rate': 0.00011806797853309481, 'epoch': 0.0}\r\n{'loss': 4.875, 'learning_rate': 0.00012164579606440072, 'epoch': 0.0}\r\n{'loss': 4.9219, 'learning_rate': 0.0001252236135957066, 'epoch': 0.0}\r\n  0%|▎| 35/18606\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>train.py</summary>\r\n\r\n```\r\n{'loss': 7.9062, 'learning_rate': 3.5778175313059034e-06, 'epoch': 0.0}\r\n{'loss': 7.5625, 'learning_rate': 7.155635062611807e-06, 'epoch': 0.0}\r\n{'loss': 7.8438, 'learning_rate': 1.073345259391771e-05, 'epoch': 0.0}\r\n{'loss': 7.8906, 'learning_rate': 1.4311270125223614e-05, 'epoch': 0.0}\r\n{'loss': 7.75, 'learning_rate': 1.7889087656529517e-05, 'epoch': 0.0}\r\n{'loss': 7.5312, 'learning_rate': 2.146690518783542e-05, 'epoch': 0.0}\r\n{'loss': 7.4844, 'learning_rate': 2.5044722719141324e-05, 'epoch': 0.0}\r\n{'loss': 7.1562, 'learning_rate': 2.8622540250447228e-05, 'epoch': 0.0}\r\n{'loss': 6.875, 'learning_rate': 3.2200357781753134e-05, 'epoch': 0.0}\r\n{'loss': 6.7188, 'learning_rate': 3.5778175313059034e-05, 'epoch': 0.0}\r\n{'loss': 6.6875, 'learning_rate': 3.935599284436494e-05, 'epoch': 0.0}\r\n{'loss': 6.5781, 'learning_rate': 4.293381037567084e-05, 'epoch': 0.0}\r\n{'loss': 6.3594, 'learning_rate': 4.651162790697674e-05, 'epoch': 0.0}\r\n{'loss': 6.4219, 'learning_rate': 5.008944543828265e-05, 'epoch': 0.0}\r\n{'loss': 6.1094, 'learning_rate': 5.366726296958855e-05, 'epoch': 0.0}\r\n{'loss': 6.1719, 'learning_rate': 5.7245080500894455e-05, 'epoch': 0.0}\r\n{'loss': 5.9219, 'learning_rate': 6.082289803220036e-05, 'epoch': 0.0}\r\n{'loss': 6.0, 'learning_rate': 6.440071556350627e-05, 'epoch': 0.0}\r\n{'loss': 5.7969, 'learning_rate': 6.797853309481217e-05, 'epoch': 0.0}\r\n{'loss': 5.6562, 'learning_rate': 7.155635062611807e-05, 'epoch': 0.0}\r\n{'loss': 5.5625, 'learning_rate': 7.513416815742398e-05, 'epoch': 0.0}\r\n{'loss': 5.5156, 'learning_rate': 7.871198568872988e-05, 'epoch': 0.0}\r\n{'loss': 5.4062, 'learning_rate': 8.228980322003578e-05, 'epoch': 0.0}\r\n{'loss': 5.5, 'learning_rate': 8.586762075134168e-05, 'epoch': 0.0}\r\n{'loss': 5.2344, 'learning_rate': 8.94454382826476e-05, 'epoch': 0.0}\r\n{'loss': 5.0781, 'learning_rate': 9.302325581395348e-05, 'epoch': 0.0}\r\n{'loss': 5.1562, 'learning_rate': 9.660107334525938e-05, 'epoch': 0.0}\r\n{'loss': 4.9531, 'learning_rate': 0.0001001788908765653, 'epoch': 0.0}\r\n{'loss': 4.8906, 'learning_rate': 0.0001037567084078712, 'epoch': 0.0}\r\n{'loss': 5.0469, 'learning_rate': 0.0001073345259391771, 'epoch': 0.0}\r\n{'loss': 4.875, 'learning_rate': 0.00011091234347048301, 'epoch': 0.0}\r\n{'loss': 4.9219, 'learning_rate': 0.00011449016100178891, 'epoch': 0.0}\r\n{'loss': 4.8594, 'learning_rate': 0.00011806797853309481, 'epoch': 0.0}\r\n{'loss': 4.7188, 'learning_rate': 0.00012164579606440072, 'epoch': 0.0}\r\n{'loss': 4.8594, 'learning_rate': 0.0001252236135957066, 'epoch': 0.0}\r\n  0%|▎| 35/18606\r\n```\r\n</details>\n</Comment>\n<Comment by haotian-liu at 2023-08-17T16:30:53Z>\nAlso, please check the transformers version:\r\n```\r\n    \"deepspeed==0.9.5\",\r\n    \"peft==0.4.0\",\r\n    \"transformers==4.31.0\",\r\n    \"accelerate==0.21.0\",\r\n    \"bitsandbytes==0.41.0\",\r\n```\n</Comment>\n<Comment by tfzhou at 2023-08-17T17:42:51Z>\nThanks @haotian-liu. \r\n\r\n> One thing is to make sure that the cuda verision of pytorch and your nvcc when you compile flash attention is the same. \r\n\r\nI am pretty sure that different versions are used in my setup. I will try to fix this and let u know.\n</Comment>\n<Comment by tfzhou at 2023-08-17T17:44:35Z>\nbtw. after turning into `train.py` mode, the training works as expected and lr decay is not an issue any more.\n</Comment>\n<Comment by haotian-liu at 2023-08-17T18:21:46Z>\n@tfzhou I see. The only drawback of using `train.py` is that it will be slower, and use more memory, which will be more prominent when you switch to finetune mode.\n</Comment>\n<Comment by tfzhou at 2023-08-18T13:56:15Z>\nAfter recompiled flash-att using a matched nvcc, the issue has been fixed. Thanks @haotian-liu\n</Comment>\n<Comment by wizyoung at 2023-09-03T02:44:08Z>\n@haotian-liu Can you post your full train log in the pre-training stage for reference?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 367,
    "state": "open",
    "created_by": "harrytea",
    "created_at": "2023-08-12T07:56:12Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/367</URL>\n\n<TITLE>error about finetuning</TITLE>\n\n<BODY>### Question\n\nI have successfully done the pretrain stage, while for fintuning, i encounter following issues.\r\n\r\n```\r\n(llava2) wangyh@A16:/data/wangyh/mllms/LLaVA$ bash finetune2.sh \r\n[2023-08-12 15:39:43,510] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-08-12 15:39:45,078] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\n[2023-08-12 15:39:45,078] [INFO] [runner.py:555:main] cmd = /home/wangyh/miniconda3/envs/llava2/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed /data/wangyh/mllms/LLaVA/finetune.json --model_name_or_path ./checkpoints/vicuna-7b-v1.5 --version v1 --data_path /data/wangyh/mllms/LLaVA/datasets/LLaVA-Instruct-150K/llava_instruct_150k.json --image_folder /data/wangyh/mllms/LLaVA/datasets/coco/train2017 --vision_tower openai/clip-vit-large-patch14 --pretrain_mm_mlp_adapter ./checkpoints/llava-7b-pretrain/mm_projector.bin --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir /data/wangyh/mllms/LLaVA/checkpoints/llava-7b-finetune --num_train_epochs 3 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb\r\n[2023-08-12 15:39:46,224] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-08-12 15:39:47,788] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\r\n[2023-08-12 15:39:47,788] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0\r\n[2023-08-12 15:39:47,788] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\r\n[2023-08-12 15:39:47,788] [INFO] [launch.py:163:main] dist_world_size=8\r\n[2023-08-12 15:39:47,788] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\r\n[2023-08-12 15:39:50,339] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-08-12 15:39:50,390] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-08-12 15:39:50,425] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-08-12 15:39:50,505] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-08-12 15:39:50,557] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-08-12 15:39:50,764] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-08-12 15:39:50,820] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-08-12 15:39:50,821] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-08-12 15:39:50,842] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-08-12 15:39:50,865] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-08-12 15:39:50,868] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-08-12 15:39:50,868] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-08-12 15:39:50,905] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-08-12 15:39:50,905] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-08-12 15:39:50,984] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-08-12 15:39:50,985] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-08-12 15:39:51,085] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-08-12 15:39:51,085] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-08-12 15:39:51,296] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-08-12 15:39:51,296] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-08-12 15:39:51,296] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n[2023-08-12 15:39:51,339] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-08-12 15:39:51,339] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-08-12 15:39:51,353] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-08-12 15:39:51,353] [INFO] [comm.py:594:init_distributed] cdb=None\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\n[2023-08-12 15:40:02,706] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 6.74B parameters\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:18<00:00,  9.29s/it]\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:18<00:00,  9.29s/it]\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:18<00:00,  9.30s/it]\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:18<00:00,  9.32s/it]\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:18<00:00,  9.32s/it]\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:18<00:00,  9.33s/it]\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:18<00:00,  9.33s/it]\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:18<00:00,  9.31s/it]\r\n[2023-08-12 15:40:24,164] [WARNING] [partition_parameters.py:836:_post_init_method] param `class_embedding` in CLIPVisionEmbeddings not on GPU so was not broadcasted from rank 0\r\n[2023-08-12 15:40:29,745] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 7.04B parameters\r\nFormatting inputs...Skip in lazy mode\r\nInstalled CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\nInstalled CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\nInstalled CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\nInstalled CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\nInstalled CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\nInstalled CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\nInstalled CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\nInstalled CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\nInstalled CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\nUsing /home/wangyh/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\r\nDetected CUDA files, patching ldflags\r\nEmitting ninja build file /home/wangyh/.cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...\r\nBuilding extension module cpu_adam...\r\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\nninja: no work to do.\r\nLoading extension module cpu_adam...\r\nTime to load cpu_adam op: 2.464034080505371 seconds\r\nInstalled CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\nUsing /home/wangyh/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\r\nInstalled CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\nUsing /home/wangyh/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\r\nDetected CUDA files, patching ldflags\r\nEmitting ninja build file /home/wangyh/.cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...\r\nBuilding extension module cpu_adam...\r\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\nninja: no work to do.\r\nLoading extension module cpu_adam...\r\nTime to load cpu_adam op: 2.4421682357788086 seconds\r\nInstalled CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\nUsing /home/wangyh/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\r\nDetected CUDA files, patching ldflags\r\nEmitting ninja build file /home/wangyh/.cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...\r\nBuilding extension module cpu_adam...\r\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\nninja: no work to do.\r\nLoading extension module cpu_adam...\r\nTime to load cpu_adam op: 2.4753994941711426 seconds\r\nInstalled CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\nUsing /home/wangyh/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\r\nInstalled CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\nUsing /home/wangyh/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\r\nInstalled CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\nUsing /home/wangyh/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\r\nInstalled CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\nUsing /home/wangyh/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\r\nDetected CUDA files, patching ldflags\r\nEmitting ninja build file /home/wangyh/.cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...\r\nBuilding extension module cpu_adam...\r\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\nninja: no work to do.\r\nLoading extension module cpu_adam...\r\nTime to load cpu_adam op: 2.6163582801818848 seconds\r\nLoading extension module cpu_adam...\r\nTime to load cpu_adam op: 2.6462419033050537 seconds\r\nLoading extension module cpu_adam...\r\nLoading extension module cpu_adam...\r\nTime to load cpu_adam op: 2.588582754135132 seconds\r\nLoading extension module cpu_adam...\r\nTime to load cpu_adam op: 2.5909383296966553 seconds\r\nTime to load cpu_adam op: 2.562427520751953 seconds\r\nParameter Offload: Total persistent parameters: 594944 in 311 params\r\nwandb: (1) Create a W&B account\r\nwandb: (2) Use an existing W&B account\r\nwandb: (3) Don't visualize my results\r\nwandb: Enter your choice: 3\r\nwandb: You chose \"Don't visualize my results\"\r\nwandb: Tracking run with wandb version 0.15.8\r\nwandb: W&B syncing is set to `offline` in this directory.  \r\nwandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\r\n{'loss': 6.0156, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.0}                                                                                                    \r\n{'loss': 6.0703, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.0}                                                                                                    \r\n{'loss': 5.9375, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.0}                                                                                                    \r\n{'loss': 5.9609, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.0}                                                                                                    \r\n{'loss': 6.0195, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.0}                                                                                                    \r\n{'loss': 5.9531, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.0}                                                                                                    \r\n{'loss': 6.0273, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.0}                                                                                                    \r\n{'loss': 5.9805, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.0}                                                                                                    \r\n{'loss': 5.9805, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.0}                                                                                                    \r\n{'loss': 6.207, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.0}                                                                                                     \r\n{'loss': 6.1289, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.0}                                                                                                    \r\n{'loss': 5.9102, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.0}                                                                                                    \r\n{'loss': 5.918, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                    \r\n{'loss': 5.9258, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n{'loss': 6.0391, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n{'loss': 5.9531, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n{'loss': 5.8164, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n{'loss': 5.8789, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n{'loss': 5.957, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                    \r\n{'loss': 6.0977, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n{'loss': 6.1484, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n{'loss': 5.9609, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n{'loss': 5.9453, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n{'loss': 5.8945, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n{'loss': 6.1094, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n{'loss': 5.9219, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n{'loss': 5.8203, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n{'loss': 5.8984, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n{'loss': 5.9375, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n{'loss': 5.9531, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n{'loss': 5.9648, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n{'loss': 5.8711, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n{'loss': 5.9141, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n{'loss': 5.9961, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n{'loss': 6.0977, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n{'loss': 5.9531, 'learning_rate': 1.801801801801802e-07, 'epoch': 0.01}                                                                                                  \r\n{'loss': 5.9844, 'learning_rate': 1.801801801801802e-07, 'epoch': 0.02}                                                                                                  \r\n{'loss': 5.9648, 'learning_rate': 1.801801801801802e-07, 'epoch': 0.02}                                                                                                  \r\n{'loss': 5.8164, 'learning_rate': 1.801801801801802e-07, 'epoch': 0.02}                                                                                                  \r\n{'loss': 5.9414, 'learning_rate': 1.801801801801802e-07, 'epoch': 0.02}                                                                                                  \r\n{'loss': 6.0664, 'learning_rate': 1.801801801801802e-07, 'epoch': 0.02}                                                                                                  \r\n{'loss': 6.0625, 'learning_rate': 1.801801801801802e-07, 'epoch': 0.02}                                                                                                  \r\n  1%|▋                                                                                                                              | 42/7395 [09:48<27:33:57, 13.50s/it]Traceback (most recent call last):\r\n  File \"/data/wangyh/mllms/LLaVA/llava/train/train_mem.py\", line 21, in <module>\r\n    train()\r\n  File \"/data/wangyh/mllms/LLaVA/./llava/train/train.py\", line 909, in train\r\n    trainer.train()\r\n  File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/transformers/trainer.py\", line 2665, in training_step\r\n    self.accelerator.backward(loss)\r\n  File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1847, in backward\r\n    self.deepspeed_engine_wrapped.backward(loss, **kwargs)\r\n  File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/accelerate/utils/deepspeed.py\", line 167, in backward\r\n    self.engine.backward(loss, **kwargs)\r\n  File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1861, in backward\r\n    self.optimizer.backward(loss, retain_graph=retain_graph)\r\n  File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 1993, in backward\r\n    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\r\n  File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 63, in backward\r\n    scaled_loss.backward(retain_graph=retain_graph)\r\n  File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\r\n    torch.autograd.backward(\r\n  File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\r\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n  File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/autograd/function.py\", line 274, in apply\r\n    return user_fn(self, *args)\r\n  File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 157, in backward\r\n    torch.autograd.backward(outputs_with_grad, args_with_grad)\r\n  File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\r\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n  File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 1006, in reduce_partition_and_remove_grads\r\n    self.reduce_ready_partitions_and_remove_grads(param, i)\r\n  File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 1286, in reduce_ready_partitions_and_remove_grads\r\n    self.reduce_independent_p_g_buckets_and_remove_grads(param, i)\r\n  File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 1041, in reduce_independent_p_g_buckets_and_remove_grads\r\n    self.__reduce_and_partition_ipg_grads()\r\n  File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 1091, in __reduce_and_partition_ipg_grads\r\n    self.partition_grads(self.params_in_ipg_bucket, grad_partitions)\r\n  File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 1271, in partition_grads\r\n    fp32_grad_tensor.copy_(grad_buffer)\r\nRuntimeError: CUDA error: an illegal memory access was encountered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\n[2023-08-12 15:51:09,569] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3090130\r\n[2023-08-12 15:51:14,623] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3090131\r\n[2023-08-12 15:51:18,682] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3090132\r\n[2023-08-12 15:51:22,988] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3090133\r\n[2023-08-12 15:51:27,297] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3090134\r\n[2023-08-12 15:51:27,298] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3090135\r\n[2023-08-12 15:51:32,219] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3090136\r\n[2023-08-12 15:51:36,482] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3090137\r\n[2023-08-12 15:51:41,105] [ERROR] [launch.py:321:sigkill_handler] ['/home/wangyh/miniconda3/envs/llava2/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=7', '--deepspeed', '/data/wangyh/mllms/LLaVA/finetune.json', '--model_name_or_path', './checkpoints/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/wangyh/mllms/LLaVA/datasets/LLaVA-Instruct-150K/llava_instruct_150k.json', '--image_folder', '/data/wangyh/mllms/LLaVA/datasets/coco/train2017', '--vision_tower', 'openai/clip-vit-large-patch14', '--pretrain_mm_mlp_adapter', './checkpoints/llava-7b-pretrain/mm_projector.bin', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', '/data/wangyh/mllms/LLaVA/checkpoints/llava-7b-finetune', '--num_train_epochs', '3', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = -6\r\n```\r\n\r\nThis seems to have run successfully for a while and reported this error, what's wrong?\r\n\r\nThis is my shell file\r\n\r\n```\r\n#!/bin/bash\r\n\r\n# Uncomment and set the following variables correspondingly to run this script:\r\n\r\n################## VICUNA ##################\r\nPROMPT_VERSION=v1\r\nMODEL_VERSION=\"vicuna-7b-v1.5\"\r\n################## VICUNA ##################\r\n\r\n################## LLaMA-2 ##################\r\n# PROMPT_VERSION=\"llava_llama_2\"\r\n# MODEL_VERSION=\"llama-2-7b-chat\"\r\n################## LLaMA-2 ##################\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed /data/wangyh/mllms/LLaVA/finetune.json \\\r\n    --model_name_or_path ./checkpoints/$MODEL_VERSION \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path /data/wangyh/mllms/LLaVA/datasets/LLaVA-Instruct-150K/llava_instruct_150k.json \\\r\n    --image_folder /data/wangyh/mllms/LLaVA/datasets/coco/train2017 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava-7b-pretrain/mm_projector.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir /data/wangyh/mllms/LLaVA/checkpoints/llava-7b-finetune \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nThanks</BODY>\n\n<COMMENTS>\n<Comment by wanghao-cst at 2023-10-11T10:59:17Z>\nHow much RAM do you have?\n</Comment>\n<Comment by nj159 at 2023-10-21T12:18:23Z>\nWhy do I get this error during pre-training? Thank you very much\r\n\r\n[2023-10-21 19:41:04,065] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-10-21 19:41:06,429] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\n[2023-10-21 19:41:06,430] [INFO] [runner.py:555:main] cmd = /home/nj/.conda/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero2.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version plain --data_path ./playground/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k_first500.json --image_folder ./playground/data/LLaVA-Pretrain/images --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --tune_mm_mlp_adapter True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --fp16 True --output_dir ./liuhaotian2/llava-v1.5-7b-pretrain --num_train_epochs 1 --per_device_train_batch_size 32 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 24000 --save_total_limit 1 --learning_rate 1e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 False --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb\r\n[2023-10-21 19:41:07,902] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-10-21 19:41:09,817] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\r\n[2023-10-21 19:41:09,817] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0\r\n[2023-10-21 19:41:09,818] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\r\n[2023-10-21 19:41:09,818] [INFO] [launch.py:163:main] dist_world_size=4\r\n[2023-10-21 19:41:09,818] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\r\n[2023-10-21 19:41:12,902] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-10-21 19:41:12,952] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-10-21 19:41:13,003] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-10-21 19:41:13,021] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n/media/nj/data2/nj/Models/LLaVA/llava/train/llama_flash_attn_monkey_patch.py:108: UserWarning: Flash attention is only supported on A100 or H100 GPU during training due to head dim > 64 backward.ref: https://github.com/HazyResearch/flash-attention/issues/190#issuecomment-1523359593\r\n  warnings.warn(\r\n/media/nj/data2/nj/Models/LLaVA/llava/train/llama_flash_attn_monkey_patch.py:108: UserWarning: Flash attention is only supported on A100 or H100 GPU during training due to head dim > 64 backward.ref: https://github.com/HazyResearch/flash-attention/issues/190#issuecomment-1523359593\r\n  warnings.warn(\r\n/media/nj/data2/nj/Models/LLaVA/llava/train/llama_flash_attn_monkey_patch.py:108: UserWarning: Flash attention is only supported on A100 or H100 GPU during training due to head dim > 64 backward.ref: https://github.com/HazyResearch/flash-attention/issues/190#issuecomment-1523359593\r\n  warnings.warn(\r\n/media/nj/data2/nj/Models/LLaVA/llava/train/llama_flash_attn_monkey_patch.py:108: UserWarning: Flash attention is only supported on A100 or H100 GPU during training due to head dim > 64 backward.ref: https://github.com/HazyResearch/flash-attention/issues/190#issuecomment-1523359593\r\n  warnings.warn(\r\n[2023-10-21 19:41:13,693] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-10-21 19:41:13,694] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-10-21 19:41:13,699] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-10-21 19:41:13,699] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-10-21 19:41:13,700] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n[2023-10-21 19:41:13,782] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-10-21 19:41:13,782] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-10-21 19:41:13,789] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-10-21 19:41:13,789] [INFO] [comm.py:594:init_distributed] cdb=None\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\n[2023-10-21 19:41:56,780] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 10468\r\n[2023-10-21 19:41:56,809] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 10469\r\n[2023-10-21 19:41:57,776] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 10470\r\n[2023-10-21 19:41:58,770] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 10471\r\n[2023-10-21 19:41:59,773] [ERROR] [launch.py:321:sigkill_handler] ['/home/nj/.conda/envs/llava/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=3', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'plain', '--data_path', './playground/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k_first500.json', '--image_folder', './playground/data/LLaVA-Pretrain/images', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--tune_mm_mlp_adapter', 'True', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--fp16', 'True', '--output_dir', './liuhaotian2/llava-v1.5-7b-pretrain', '--num_train_epochs', '1', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '24000', '--save_total_limit', '1', '--learning_rate', '1e-3', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'False', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = -9\n</Comment>\n<Comment by ybsu at 2024-06-03T10:13:51Z>\n> ### Question\r\n> I have successfully done the pretrain stage, while for fintuning, i encounter following issues.\r\n> \r\n> ```\r\n> (llava2) wangyh@A16:/data/wangyh/mllms/LLaVA$ bash finetune2.sh \r\n> [2023-08-12 15:39:43,510] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n> [2023-08-12 15:39:45,078] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\n> [2023-08-12 15:39:45,078] [INFO] [runner.py:555:main] cmd = /home/wangyh/miniconda3/envs/llava2/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed /data/wangyh/mllms/LLaVA/finetune.json --model_name_or_path ./checkpoints/vicuna-7b-v1.5 --version v1 --data_path /data/wangyh/mllms/LLaVA/datasets/LLaVA-Instruct-150K/llava_instruct_150k.json --image_folder /data/wangyh/mllms/LLaVA/datasets/coco/train2017 --vision_tower openai/clip-vit-large-patch14 --pretrain_mm_mlp_adapter ./checkpoints/llava-7b-pretrain/mm_projector.bin --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir /data/wangyh/mllms/LLaVA/checkpoints/llava-7b-finetune --num_train_epochs 3 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb\r\n> [2023-08-12 15:39:46,224] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n> [2023-08-12 15:39:47,788] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\r\n> [2023-08-12 15:39:47,788] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0\r\n> [2023-08-12 15:39:47,788] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\r\n> [2023-08-12 15:39:47,788] [INFO] [launch.py:163:main] dist_world_size=8\r\n> [2023-08-12 15:39:47,788] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\r\n> [2023-08-12 15:39:50,339] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n> [2023-08-12 15:39:50,390] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n> [2023-08-12 15:39:50,425] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n> [2023-08-12 15:39:50,505] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n> [2023-08-12 15:39:50,557] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n> [2023-08-12 15:39:50,764] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n> [2023-08-12 15:39:50,820] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n> [2023-08-12 15:39:50,821] [INFO] [comm.py:594:init_distributed] cdb=None\r\n> [2023-08-12 15:39:50,842] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n> [2023-08-12 15:39:50,865] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n> [2023-08-12 15:39:50,868] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n> [2023-08-12 15:39:50,868] [INFO] [comm.py:594:init_distributed] cdb=None\r\n> [2023-08-12 15:39:50,905] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n> [2023-08-12 15:39:50,905] [INFO] [comm.py:594:init_distributed] cdb=None\r\n> [2023-08-12 15:39:50,984] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n> [2023-08-12 15:39:50,985] [INFO] [comm.py:594:init_distributed] cdb=None\r\n> [2023-08-12 15:39:51,085] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n> [2023-08-12 15:39:51,085] [INFO] [comm.py:594:init_distributed] cdb=None\r\n> [2023-08-12 15:39:51,296] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n> [2023-08-12 15:39:51,296] [INFO] [comm.py:594:init_distributed] cdb=None\r\n> [2023-08-12 15:39:51,296] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n> [2023-08-12 15:39:51,339] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n> [2023-08-12 15:39:51,339] [INFO] [comm.py:594:init_distributed] cdb=None\r\n> [2023-08-12 15:39:51,353] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n> [2023-08-12 15:39:51,353] [INFO] [comm.py:594:init_distributed] cdb=None\r\n> You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\n> You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\n> You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\n> You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\n> You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\n> You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\n> You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\n> You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\n> [2023-08-12 15:40:02,706] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 6.74B parameters\r\n> Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:18<00:00,  9.29s/it]\r\n> Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:18<00:00,  9.29s/it]\r\n> Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:18<00:00,  9.30s/it]\r\n> Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:18<00:00,  9.32s/it]\r\n> Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:18<00:00,  9.32s/it]\r\n> Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:18<00:00,  9.33s/it]\r\n> Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:18<00:00,  9.33s/it]\r\n> Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:18<00:00,  9.31s/it]\r\n> [2023-08-12 15:40:24,164] [WARNING] [partition_parameters.py:836:_post_init_method] param `class_embedding` in CLIPVisionEmbeddings not on GPU so was not broadcasted from rank 0\r\n> [2023-08-12 15:40:29,745] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 7.04B parameters\r\n> Formatting inputs...Skip in lazy mode\r\n> Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\n> Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\n> Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\n> Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\n> Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\n> Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\n> Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\n> Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\n> Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\n> Using /home/wangyh/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\r\n> Detected CUDA files, patching ldflags\r\n> Emitting ninja build file /home/wangyh/.cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...\r\n> Building extension module cpu_adam...\r\n> Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\n> ninja: no work to do.\r\n> Loading extension module cpu_adam...\r\n> Time to load cpu_adam op: 2.464034080505371 seconds\r\n> Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\n> Using /home/wangyh/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\r\n> Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\n> Using /home/wangyh/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\r\n> Detected CUDA files, patching ldflags\r\n> Emitting ninja build file /home/wangyh/.cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...\r\n> Building extension module cpu_adam...\r\n> Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\n> ninja: no work to do.\r\n> Loading extension module cpu_adam...\r\n> Time to load cpu_adam op: 2.4421682357788086 seconds\r\n> Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\n> Using /home/wangyh/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\r\n> Detected CUDA files, patching ldflags\r\n> Emitting ninja build file /home/wangyh/.cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...\r\n> Building extension module cpu_adam...\r\n> Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\n> ninja: no work to do.\r\n> Loading extension module cpu_adam...\r\n> Time to load cpu_adam op: 2.4753994941711426 seconds\r\n> Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\n> Using /home/wangyh/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\r\n> Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\n> Using /home/wangyh/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\r\n> Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\n> Using /home/wangyh/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\r\n> Installed CUDA version 11.3 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\r\n> Using /home/wangyh/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\r\n> Detected CUDA files, patching ldflags\r\n> Emitting ninja build file /home/wangyh/.cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...\r\n> Building extension module cpu_adam...\r\n> Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\n> ninja: no work to do.\r\n> Loading extension module cpu_adam...\r\n> Time to load cpu_adam op: 2.6163582801818848 seconds\r\n> Loading extension module cpu_adam...\r\n> Time to load cpu_adam op: 2.6462419033050537 seconds\r\n> Loading extension module cpu_adam...\r\n> Loading extension module cpu_adam...\r\n> Time to load cpu_adam op: 2.588582754135132 seconds\r\n> Loading extension module cpu_adam...\r\n> Time to load cpu_adam op: 2.5909383296966553 seconds\r\n> Time to load cpu_adam op: 2.562427520751953 seconds\r\n> Parameter Offload: Total persistent parameters: 594944 in 311 params\r\n> wandb: (1) Create a W&B account\r\n> wandb: (2) Use an existing W&B account\r\n> wandb: (3) Don't visualize my results\r\n> wandb: Enter your choice: 3\r\n> wandb: You chose \"Don't visualize my results\"\r\n> wandb: Tracking run with wandb version 0.15.8\r\n> wandb: W&B syncing is set to `offline` in this directory.  \r\n> wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\r\n> {'loss': 6.0156, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.0}                                                                                                    \r\n> {'loss': 6.0703, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.0}                                                                                                    \r\n> {'loss': 5.9375, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.0}                                                                                                    \r\n> {'loss': 5.9609, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.0}                                                                                                    \r\n> {'loss': 6.0195, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.0}                                                                                                    \r\n> {'loss': 5.9531, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.0}                                                                                                    \r\n> {'loss': 6.0273, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.0}                                                                                                    \r\n> {'loss': 5.9805, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.0}                                                                                                    \r\n> {'loss': 5.9805, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.0}                                                                                                    \r\n> {'loss': 6.207, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.0}                                                                                                     \r\n> {'loss': 6.1289, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.0}                                                                                                    \r\n> {'loss': 5.9102, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.0}                                                                                                    \r\n> {'loss': 5.918, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                    \r\n> {'loss': 5.9258, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n> {'loss': 6.0391, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n> {'loss': 5.9531, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n> {'loss': 5.8164, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n> {'loss': 5.8789, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n> {'loss': 5.957, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                    \r\n> {'loss': 6.0977, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n> {'loss': 6.1484, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n> {'loss': 5.9609, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n> {'loss': 5.9453, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n> {'loss': 5.8945, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n> {'loss': 6.1094, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n> {'loss': 5.9219, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n> {'loss': 5.8203, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n> {'loss': 5.8984, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n> {'loss': 5.9375, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n> {'loss': 5.9531, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n> {'loss': 5.9648, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n> {'loss': 5.8711, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n> {'loss': 5.9141, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n> {'loss': 5.9961, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n> {'loss': 6.0977, 'learning_rate': 9.00900900900901e-08, 'epoch': 0.01}                                                                                                   \r\n> {'loss': 5.9531, 'learning_rate': 1.801801801801802e-07, 'epoch': 0.01}                                                                                                  \r\n> {'loss': 5.9844, 'learning_rate': 1.801801801801802e-07, 'epoch': 0.02}                                                                                                  \r\n> {'loss': 5.9648, 'learning_rate': 1.801801801801802e-07, 'epoch': 0.02}                                                                                                  \r\n> {'loss': 5.8164, 'learning_rate': 1.801801801801802e-07, 'epoch': 0.02}                                                                                                  \r\n> {'loss': 5.9414, 'learning_rate': 1.801801801801802e-07, 'epoch': 0.02}                                                                                                  \r\n> {'loss': 6.0664, 'learning_rate': 1.801801801801802e-07, 'epoch': 0.02}                                                                                                  \r\n> {'loss': 6.0625, 'learning_rate': 1.801801801801802e-07, 'epoch': 0.02}                                                                                                  \r\n>   1%|▋                                                                                                                              | 42/7395 [09:48<27:33:57, 13.50s/it]Traceback (most recent call last):\r\n>   File \"/data/wangyh/mllms/LLaVA/llava/train/train_mem.py\", line 21, in <module>\r\n>     train()\r\n>   File \"/data/wangyh/mllms/LLaVA/./llava/train/train.py\", line 909, in train\r\n>     trainer.train()\r\n>   File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n>     return inner_training_loop(\r\n>   File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\r\n>     tr_loss_step = self.training_step(model, inputs)\r\n>   File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/transformers/trainer.py\", line 2665, in training_step\r\n>     self.accelerator.backward(loss)\r\n>   File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1847, in backward\r\n>     self.deepspeed_engine_wrapped.backward(loss, **kwargs)\r\n>   File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/accelerate/utils/deepspeed.py\", line 167, in backward\r\n>     self.engine.backward(loss, **kwargs)\r\n>   File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n>     ret_val = func(*args, **kwargs)\r\n>   File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1861, in backward\r\n>     self.optimizer.backward(loss, retain_graph=retain_graph)\r\n>   File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n>     ret_val = func(*args, **kwargs)\r\n>   File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 1993, in backward\r\n>     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\r\n>   File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 63, in backward\r\n>     scaled_loss.backward(retain_graph=retain_graph)\r\n>   File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\r\n>     torch.autograd.backward(\r\n>   File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\r\n>     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n>   File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/autograd/function.py\", line 274, in apply\r\n>     return user_fn(self, *args)\r\n>   File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 157, in backward\r\n>     torch.autograd.backward(outputs_with_grad, args_with_grad)\r\n>   File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\r\n>     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n>   File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n>     ret_val = func(*args, **kwargs)\r\n>   File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 1006, in reduce_partition_and_remove_grads\r\n>     self.reduce_ready_partitions_and_remove_grads(param, i)\r\n>   File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 1286, in reduce_ready_partitions_and_remove_grads\r\n>     self.reduce_independent_p_g_buckets_and_remove_grads(param, i)\r\n>   File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 1041, in reduce_independent_p_g_buckets_and_remove_grads\r\n>     self.__reduce_and_partition_ipg_grads()\r\n>   File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n>     ret_val = func(*args, **kwargs)\r\n>   File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n>     return func(*args, **kwargs)\r\n>   File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 1091, in __reduce_and_partition_ipg_grads\r\n>     self.partition_grads(self.params_in_ipg_bucket, grad_partitions)\r\n>   File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n>     ret_val = func(*args, **kwargs)\r\n>   File \"/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 1271, in partition_grads\r\n>     fp32_grad_tensor.copy_(grad_buffer)\r\n> RuntimeError: CUDA error: an illegal memory access was encountered\r\n> CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n> For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\n> Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n> \r\n> [2023-08-12 15:51:09,569] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3090130\r\n> [2023-08-12 15:51:14,623] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3090131\r\n> [2023-08-12 15:51:18,682] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3090132\r\n> [2023-08-12 15:51:22,988] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3090133\r\n> [2023-08-12 15:51:27,297] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3090134\r\n> [2023-08-12 15:51:27,298] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3090135\r\n> [2023-08-12 15:51:32,219] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3090136\r\n> [2023-08-12 15:51:36,482] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3090137\r\n> [2023-08-12 15:51:41,105] [ERROR] [launch.py:321:sigkill_handler] ['/home/wangyh/miniconda3/envs/llava2/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=7', '--deepspeed', '/data/wangyh/mllms/LLaVA/finetune.json', '--model_name_or_path', './checkpoints/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/data/wangyh/mllms/LLaVA/datasets/LLaVA-Instruct-150K/llava_instruct_150k.json', '--image_folder', '/data/wangyh/mllms/LLaVA/datasets/coco/train2017', '--vision_tower', 'openai/clip-vit-large-patch14', '--pretrain_mm_mlp_adapter', './checkpoints/llava-7b-pretrain/mm_projector.bin', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', '/data/wangyh/mllms/LLaVA/checkpoints/llava-7b-finetune', '--num_train_epochs', '3', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = -6\r\n> ```\r\n> \r\n> This seems to have run successfully for a while and reported this error, what's wrong?\r\n> \r\n> This is my shell file\r\n> \r\n> ```\r\n> #!/bin/bash\r\n> \r\n> # Uncomment and set the following variables correspondingly to run this script:\r\n> \r\n> ################## VICUNA ##################\r\n> PROMPT_VERSION=v1\r\n> MODEL_VERSION=\"vicuna-7b-v1.5\"\r\n> ################## VICUNA ##################\r\n> \r\n> ################## LLaMA-2 ##################\r\n> # PROMPT_VERSION=\"llava_llama_2\"\r\n> # MODEL_VERSION=\"llama-2-7b-chat\"\r\n> ################## LLaMA-2 ##################\r\n> \r\n> deepspeed llava/train/train_mem.py \\\r\n>     --deepspeed /data/wangyh/mllms/LLaVA/finetune.json \\\r\n>     --model_name_or_path ./checkpoints/$MODEL_VERSION \\\r\n>     --version $PROMPT_VERSION \\\r\n>     --data_path /data/wangyh/mllms/LLaVA/datasets/LLaVA-Instruct-150K/llava_instruct_150k.json \\\r\n>     --image_folder /data/wangyh/mllms/LLaVA/datasets/coco/train2017 \\\r\n>     --vision_tower openai/clip-vit-large-patch14 \\\r\n>     --pretrain_mm_mlp_adapter ./checkpoints/llava-7b-pretrain/mm_projector.bin \\\r\n>     --mm_vision_select_layer -2 \\\r\n>     --mm_use_im_start_end False \\\r\n>     --mm_use_im_patch_token False \\\r\n>     --bf16 True \\\r\n>     --output_dir /data/wangyh/mllms/LLaVA/checkpoints/llava-7b-finetune \\\r\n>     --num_train_epochs 3 \\\r\n>     --per_device_train_batch_size 8 \\\r\n>     --per_device_eval_batch_size 4 \\\r\n>     --gradient_accumulation_steps 1 \\\r\n>     --evaluation_strategy \"no\" \\\r\n>     --save_strategy \"steps\" \\\r\n>     --save_steps 50000 \\\r\n>     --save_total_limit 1 \\\r\n>     --learning_rate 2e-5 \\\r\n>     --weight_decay 0. \\\r\n>     --warmup_ratio 0.03 \\\r\n>     --lr_scheduler_type \"cosine\" \\\r\n>     --logging_steps 1 \\\r\n>     --tf32 True \\\r\n>     --model_max_length 2048 \\\r\n>     --gradient_checkpointing True \\\r\n>     --dataloader_num_workers 4 \\\r\n>     --lazy_preprocess True \\\r\n>     --report_to wandb\r\n> ```\r\n> \r\n> Thanks\r\n\r\nsimilar issue, have you solved it ? Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 366,
    "state": "closed",
    "created_by": "TangsengT",
    "created_at": "2023-08-11T08:32:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/366</URL>\n\n<TITLE>[Question] Using 8bit and 4bit quantization is much slower than non-quantization model</TITLE>\n\n<BODY>### Question\n\nHi, \r\n\r\nWhen I use **run_llava.py** , I set the argument in the load function:\r\n\r\n`load_pretrained_model(args.model_path, args.model_base, model_name,\r\n                                                                           load_8bit=args.load_8bit,\r\n                                                                           load_4bit=args.load_4bit)`\r\nto use the quantization inference, but the inference speed is much slower than model before using quantization.\r\nI use the same image and same prompt, and the speed downs to half of the original inference speed using 8bit quantization.\r\nAnd compared to original inference speed, using 4bit quantization shows nearly the same inference speed without any improvement.\r\n\r\nTo find what the problem is, I take a look to my memory usage and it truly decreases using quantization.\r\n\r\nCould you give me some ideas for this question?\r\n\r\nThanks a lot!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-16T20:04:48Z>\nThe 4-bit should be faster with the latest bnb update. 8-bit is still slower for me. I am observing similar things for LLMs like Vicuna, so maybe we cannot do much.\r\n\r\nPlease let me know if you find that some code with LLMs that are actually faster with 8-bit, and I'd be happy to look into it. Thanks.\n</Comment>\n<Comment by forever208 at 2023-10-25T09:14:10Z>\nsame here, in A100, the speed is: \r\n`not using 4-bit or 8-bit` > `using 4-bit` > `using 8-bit`\r\n@haotian-liu may I ask that, how much would this 4-bit 8-bit quantification affect the performance according to your experience?\n</Comment>\n<Comment by Changlin-Lee at 2023-11-13T09:47:47Z>\nSame performance on A6000\r\n\r\n> same here, in A100, the speed is: `not using 4-bit or 8-bit` > `using 4-bit` > `using 8-bit` @haotian-liu may I ask that, how much would this 4-bit 8-bit quantification affect the performance according to your experience?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 365,
    "state": "open",
    "created_by": "yuntaodu",
    "created_at": "2023-08-11T02:09:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/365</URL>\n\n<TITLE>[Question]</TITLE>\n\n<BODY>### Question\n\nHi, thanks for your excellent work! I wonder to ask how is the dataset（llava_instruct_80k.json） used in lighting version at finetune stage is constructed?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 364,
    "state": "closed",
    "created_by": "gnimyang",
    "created_at": "2023-08-10T08:55:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/364</URL>\n\n<TITLE>NETWORK ERROR DUE TO HIGH TRAFFIC</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nWhenever I create a new folder re-run the program.\r\nif I put the picture into the model, the model will crash busy!\r\n![image](https://github.com/haotian-liu/LLaVA/assets/130719420/e7f8487d-f181-4c9d-8826-353af989ff7d)\r\nif only input text, it can work well.\r\nit seems the model disconnect to the webserver\r\n\r\n\r\n\r\n2023-08-10 16:47:15 | INFO | model_worker | Send heart beat. Models: ['01']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n2023-08-10 16:47:15 | INFO | stdout | INFO:     127.0.0.1:57164 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\n2023-08-10 16:47:21 | INFO | model_worker | Send heart beat. Models: ['01']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n2023-08-10 16:47:24 | INFO | model_worker | Send heart beat. Models: ['01']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n2023-08-10 16:47:34 | INFO | model_worker | Send heart beat. Models: ['01']. Semaphore: Semaphore(value=4, locked=False). global_counter: 2\r\n2023-08-10 16:47:34 | INFO | stdout | INFO:     127.0.0.1:57180 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\n2023-08-10 16:47:35 | ERROR | stderr | Exception in thread Thread-4 (generate):\r\n2023-08-10 16:47:35 | ERROR | stderr | Traceback (most recent call last):\r\n2023-08-10 16:47:35 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava2/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\r\n2023-08-10 16:47:35 | ERROR | stderr |     self.run()\r\n2023-08-10 16:47:35 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava2/lib/python3.10/threading.py\", line 953, in run\r\n2023-08-10 16:47:35 | ERROR | stderr |     self._target(*self._args, **self._kwargs)\r\n2023-08-10 16:47:35 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava2/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n2023-08-10 16:47:35 | ERROR | stderr |     return func(*args, **kwargs)\r\n2023-08-10 16:47:35 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava2/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1588, in generate\r\n2023-08-10 16:47:35 | ERROR | stderr |     return self.sample(\r\n2023-08-10 16:47:35 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava2/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2642, in sample\r\n2023-08-10 16:47:35 | ERROR | stderr |     outputs = self(\r\n2023-08-10 16:47:35 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-08-10 16:47:35 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2023-08-10 16:47:35 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava2/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2023-08-10 16:47:35 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2023-08-10 16:47:35 | ERROR | stderr |   File \"/public/home/v-yumy/Pycharm_Project/LLaVA/llava/model/language_model/llava_llama.py\", line 78, in forward\r\n2023-08-10 16:47:35 | ERROR | stderr |     outputs = self.model(\r\n2023-08-10 16:47:35 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-08-10 16:47:35 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2023-08-10 16:47:35 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 693, in forward\r\n2023-08-10 16:47:35 | ERROR | stderr |     layer_outputs = decoder_layer(\r\n2023-08-10 16:47:35 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-08-10 16:47:35 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2023-08-10 16:47:35 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava2/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2023-08-10 16:47:35 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2023-08-10 16:47:35 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 405, in forward\r\n2023-08-10 16:47:35 | ERROR | stderr |     hidden_states = self.input_layernorm(hidden_states)\r\n2023-08-10 16:47:35 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-08-10 16:47:35 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2023-08-10 16:47:35 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava2/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2023-08-10 16:47:35 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2023-08-10 16:47:35 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 89, in forward\r\n2023-08-10 16:47:35 | ERROR | stderr |     return self.weight * hidden_states.to(input_dtype)\r\n2023-08-10 16:47:35 | ERROR | stderr | RuntimeError: CUDA error: device-side assert triggered\r\n2023-08-10 16:47:35 | ERROR | stderr | CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n2023-08-10 16:47:35 | ERROR | stderr | For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\n2023-08-10 16:47:35 | ERROR | stderr | Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n2023-08-10 16:47:35 | ERROR | stderr |\r\n2023-08-10 16:47:39 | INFO | model_worker | Send heart beat. Models: ['01']. Semaphore: Semaphore(value=4, locked=False). global_counter: 2\r\n2023-08-10 16:47:49 | INFO | stdout | Caught Unknown Error\r\n2023-08-10 16:47:49 | INFO | model_worker | Send heart beat. Models: ['01']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2\r\n2023-08-10 16:47:54 | INFO | model_worker | Send heart beat. Models: ['01']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2\r\n2023-08-10 16:48:09 | INFO | model_worker | Send heart beat. Models: ['01']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2\r\n2023-08-10 16:48:24 | INFO | model_worker | Send heart beat. Models: ['01']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2\r\n2023-08-10 16:48:39 | INFO | model_worker | Send heart beat. Models: ['01']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2\r\n2023-08-10 16:48:54 | INFO | model_worker | Send heart beat. Models: ['01']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2\r\n2023-08-10 16:49:09 | INFO | model_worker | Send heart beat. Models: ['01']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2\r\n2023-08-10 16:49:24 | INFO | model_worker | Send heart beat. Models: ['01']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2\r\n2023-08-10 16:49:39 | INFO | model_worker | Send heart beat. Models: ['01']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2\r\n2023-08-10 16:49:55 | INFO | model_worker | Send heart beat. Models: ['01']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2\r\n2023-08-10 16:50:10 | INFO | model_worker | Send heart beat. Models: ['01']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2\r\n2023-08-10 16:50:25 | INFO | model_worker | Send heart beat. Models: ['01']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2</BODY>\n\n<COMMENTS>\n<Comment by gnimyang at 2023-08-10T09:00:43Z>\nI can run this program successfully in one folder, but in another folder, it cannot input images, the device and content are the same.\n</Comment>\n<Comment by gnimyang at 2023-08-10T14:27:57Z>\nHi Haotian, I have solved this issue, The problem relies on the checkpoint subfolder name. when I randomly name different models like 01, or 02, the program will meet the above issue. Though, I don't know the reason, but make sure the model-path name cannot be numbers, or it will not recognize the image. Only I name it LLaVA-vicuna-XXXXXX, it can work correctly. Additionally, I want to know the reason!\n</Comment>\n<Comment by haotian-liu at 2023-10-13T02:05:57Z>\nYep, it uses llava to recognize the model to be multimodal. Working to improve this to more intelligently detect the model type. Closing for now.\n</Comment>\n<Comment by 459737087 at 2024-01-11T07:18:52Z>\nThank you! you did a great job @gnimyang\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 363,
    "state": "open",
    "created_by": "birchmi",
    "created_at": "2023-08-09T08:38:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/363</URL>\n\n<TITLE>[Usage] ValueError: Attempting to unscale FP16 gradients</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:   ValueError: Attempting to unscale FP16 gradients\r\n\r\nCommand:\r\n\r\npython llava/train/train.py \\\r\n    --lora_enable True \\\r\n    --model_name_or_path /home/xyz/docker/projects/LLaVA/weights/vicuna-7b-v1.3  \\\r\n    --version v1 \\\r\n    --data_path /home/xyz/docker/projects/LLaVA/data/llava_instruct_150k.json \\\r\n    --image_folder /home/xyz/docker/projects/LLaVA/data/train2017 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter /home/xyz/docker/projects/LLaVA/weights/LLaVA-Pretrained-Projectors/LLaVA-7b-pretrain-projector-v1-1-LCS-558K-blip_caption.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --fp16 True \\\r\n    --output_dir /home/xyz/docker/projects/LLaVA/weights/llava-7b-v1.3-finetune \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 False \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --dataloader_num_workers 4 \\\r\n    --report_to wandb\r\n\r\nLog: \r\n\r\nwandb: Tracking run with wandb version 0.15.8\r\nwandb: Run data is saved locally in /home/xyz/docker/projects/LLaVA/wandb/run-20230809_082148-7eg1pnfo\r\nwandb: Run `wandb offline` to turn off syncing.\r\nwandb: Syncing run stellar-sun-1\r\nwandb: ⭐️ View project at https://wandb.ai/rch/hugging\r\nwandb: 🚀 View run at https://wandb.ai/rch/hugging/runs/7eg1pnf1\r\n  0%|                                                                                                                                                                                                            | 0/473136 [00:00<?, ?it/s]Traceback (most recent call last):\r\n  File \"/home/xyz/docker/projects/LLaVA/llava/train/train.py\", line 836, in <module>\r\n    train()\r\n  File \"/home/xyz/docker/projects/LLaVA/llava/train/train.py\", line 816, in train\r\n    trainer.train()\r\n  File \"/home/xyz/docker/projects/LLaVA/transformers/src/transformers/trainer.py\", line 1644, in train\r\n    return inner_training_loop(\r\n  File \"/home/xyz/docker/projects/LLaVA/transformers/src/transformers/trainer.py\", line 1944, in _inner_training_loop\r\n    self.scaler.unscale_(self.optimizer)\r\n  File \"/home/xyz/miniconda3/envs/llava/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py\", line 284, in unscale_\r\n    optimizer_state[\"found_inf_per_device\"] = self._unscale_grads_(optimizer, inv_scale, found_inf, False)\r\n  File \"/home/xyz/miniconda3/envs/llava/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py\", line 212, in _unscale_grads_\r\n    raise ValueError(\"Attempting to unscale FP16 gradients.\")\r\nValueError: Attempting to unscale FP16 gradients.\r\nwandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.\r\nwandb: Network error (TransientError), entering retry loop.\r\nwandb: Network error (TransientError), entering retry loop.\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by Yanfors at 2024-10-04T07:17:59Z>\ndo you solve this？\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 362,
    "state": "open",
    "created_by": "Byshev333",
    "created_at": "2023-08-09T03:56:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/362</URL>\n\n<TITLE>When use multi-nodes with zero3, training time increase</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nWe collect a large-scale instruction dataset, and want to use muti-nodes training. When using the following script, the traing time is too slow and no log about time.\r\nCommand:\r\n```\r\ndeepspeed --hostfile=/etc/mpi/hostfile_tmp llava/train/train_mem.py \\\r\n    --deepspeed scripts/zero3.json \\ \r\n    ......\r\n    .....\r\n```\r\n<img width=\"1232\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/58807679/06534526-402c-46f7-a571-0c1a08500bf0\"></BODY>\n\n<COMMENTS>\n<Comment by BellaBei at 2023-08-24T08:51:45Z>\nSimilar case here. while working with zero3, I've observed that fine-tuning can only be running on 8 GPUs. Even if I run it on 2 workers (16 GPUs), it still only uses 8 of them.\n</Comment>\n<Comment by haotian-liu at 2023-10-26T20:17:04Z>\nHi, the main reason for the slow-down in multi-node training should be in the communication between the nodes. When high-speed inter-server connection is used, we find the speed degradation is minimal.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 361,
    "state": "closed",
    "created_by": "chrisk414",
    "created_at": "2023-08-09T02:09:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/361</URL>\n\n<TITLE>[Usage] Stuck on Gradio</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nI've installed Llava on Windows (without DeepSpeed and installed Windows version of BitSandBytes), and using all default settings. I'm on GTX4090\r\nThe model (llava-llama-2-13b-chat-lightning-preview --load-4bit, on localhost) is loaded correctly and everything seems to work fine without errors, except when I start Web.\r\n\r\nLog: \r\n```\r\ncall conda activate llava \r\n\r\nstart /b python -m llava.serve.controller --host 0.0.0.0  --port 10000 \r\n\r\nstart /b python -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload \r\n\r\nstart /b python -m llava.serve.model_worker --host 0.0.0.0  --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path models\\llava-llama-2-13b-chat-lightning-preview\\ --load-4bit \r\n\r\n(llava) D:\\LLaVA>bin D:\\anaconda3\\envs\\llava\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll\r\nbin D:\\anaconda3\\envs\\llava\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll\r\nbin D:\\anaconda3\\envs\\llava\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll\r\n2023-08-09 10:52:49 | INFO | controller | args: Namespace(host='0.0.0.0', port=10000, dispatch_method='shortest_queue')\r\n2023-08-09 10:52:49 | INFO | controller | Init controller\r\n2023-08-09 10:52:49 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='models\\\\llava-llama-2-13b-chat-lightning-preview\\\\', model_base=None, model_name=None, multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=True)\r\n2023-08-09 10:52:49 | INFO | model_worker | Loading the model models\\llava-llama-2-13b-chat-lightning-preview\\ on worker 8f446a ...\r\nYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\r\n2023-08-09 10:52:49 | ERROR | stderr | INFO:     Started server process [1952]\r\n2023-08-09 10:52:49 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2023-08-09 10:52:49 | ERROR | stderr | INFO:     Application startup complete.\r\n2023-08-09 10:52:49 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:10000 (Press CTRL+C to quit)\r\nLoading checkpoint shards:   0%|                                                                                                                                                                             | 0/3 [00:00<?, ?it/s]\r\n2023-08-09 10:53:05 | INFO | gradio_web_server | args: Namespace(host='0.0.0.0', port=None, controller_url='http://localhost:10000', concurrency_count=8, model_list_mode='reload', share=False, moderate=False, embed=False)\r\n2023-08-09 10:53:07 | INFO | stdout | INFO:     127.0.0.1:3866 - \"POST /refresh_all_workers HTTP/1.1\" 200 OK\r\n2023-08-09 10:53:09 | INFO | stdout | INFO:     127.0.0.1:3868 - \"POST /list_models HTTP/1.1\" 200 OK\r\n2023-08-09 10:53:09 | INFO | gradio_web_server | Models: []\r\n2023-08-09 10:53:09 | INFO | gradio_web_server | Namespace(host='0.0.0.0', port=None, controller_url='http://localhost:10000', concurrency_count=8, model_list_mode='reload', share=False, moderate=False, embed=False)\r\nLoading checkpoint shards:  33%|███████████████████████████████████████████████████████                                                                                                              | 1/3 [00:22<00:45, 22.86s/it]\r\n2023-08-09 10:53:14 | INFO | stdout | Running on local URL:  http://0.0.0.0:7860\r\n2023-08-09 10:53:14 | INFO | stdout | \r\n2023-08-09 10:53:14 | INFO | stdout | To create a public link, set `share=True` in `launch()`.\r\nLoading checkpoint shards:  67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                       | 2/3 [00:43<00:21, 21.43s/it]\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:56<00:00, 17.53s/it]\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:56<00:00, 18.73s/it]\r\n2023-08-09 10:53:47 | ERROR | stderr | \r\n2023-08-09 10:53:50 | INFO | model_worker | Register to controller\r\n2023-08-09 10:53:52 | INFO | controller | Register a new worker: http://localhost:40000\r\n2023-08-09 10:53:52 | INFO | controller | Register done: http://localhost:40000, {'model_names': ['models\\\\llava-llama-2-13b-chat-lightning-preview\\\\'], 'speed': 1, 'queue_length': 0}\r\n2023-08-09 10:53:52 | INFO | stdout | INFO:     127.0.0.1:9716 - \"POST /register_worker HTTP/1.1\" 200 OK\r\n2023-08-09 10:53:52 | ERROR | stderr | INFO:     Started server process [6508]\r\n2023-08-09 10:53:52 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2023-08-09 10:53:52 | ERROR | stderr | INFO:     Application startup complete.\r\n2023-08-09 10:53:52 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)\r\n2023-08-09 10:54:07 | INFO | model_worker | Send heart beat. Models: ['models\\\\llava-llama-2-13b-chat-lightning-preview\\\\']. Semaphore: None. global_counter: 0\r\n2023-08-09 10:54:09 | INFO | controller | Receive heart beat. http://localhost:40000\r\n2023-08-09 10:54:09 | INFO | stdout | INFO:     127.0.0.1:9757 - \"POST /receive_heart_beat HTTP/1.1\" 200 OK\r\n2023-08-09 10:54:24 | INFO | model_worker | Send heart beat. Models: ['models\\\\llava-llama-2-13b-chat-lightning-preview\\\\']. Semaphore: None. global_counter: 0\r\n2023-08-09 10:54:26 | INFO | controller | Receive heart beat. http://localhost:40000\r\n2023-08-09 10:54:26 | INFO | stdout | INFO:     127.0.0.1:9787 - \"POST /receive_heart_beat HTTP/1.1\" 200 OK\r\n2023-08-09 10:54:41 | INFO | model_worker | Send heart beat. Models: ['models\\\\llava-llama-2-13b-chat-lightning-preview\\\\']. Semaphore: None. global_counter: 0\r\n2023-08-09 10:54:43 | INFO | controller | Receive heart beat. http://localhost:40000\r\n2023-08-09 10:54:43 | INFO | stdout | INFO:     127.0.0.1:9815 - \"POST /receive_heart_beat HTTP/1.1\" 200 OK\r\n2023-08-09 10:54:58 | INFO | model_worker | Send heart beat. Models: ['models\\\\llava-llama-2-13b-chat-lightning-preview\\\\']. Semaphore: None. global_counter: 0\r\n```\r\n\r\nHowever, the web is stuck on \"queue\" like the image below. I didn't do anything.\r\nWhen I refresh the web, I get console messages like the below.\r\nAny ideas? Many thanks.\r\n\r\nLog: \r\n```\r\n2023-08-09 10:56:20 | ERROR | asyncio | Task exception was never retrieved\r\nfuture: <Task finished name='jzg2326scs_11' coro=<Queue.process_events() done, defined at D:\\anaconda3\\envs\\llava\\lib\\site-packages\\gradio\\queueing.py:342> exception=1 validation error for PredictBody\r\nevent_id\r\n  Field required [type=missing, input_value={'data': [], 'event_data'...ion_hash': 'jzg2326scs'}, input_type=dict]\r\n    For further information visit https://errors.pydantic.dev/2.1/v/missing>\r\nTraceback (most recent call last):\r\n  File \"D:\\anaconda3\\envs\\llava\\lib\\site-packages\\gradio\\queueing.py\", line 346, in process_events\r\n    client_awake = await self.gather_event_data(event)\r\n  File \"D:\\anaconda3\\envs\\llava\\lib\\site-packages\\gradio\\queueing.py\", line 219, in gather_event_data\r\n    data, client_awake = await self.get_message(event, timeout=receive_timeout)\r\n  File \"D:\\anaconda3\\envs\\llava\\lib\\site-packages\\gradio\\queueing.py\", line 448, in get_message\r\n    return PredictBody(**data), True\r\n  File \"D:\\anaconda3\\envs\\llava\\lib\\site-packages\\pydantic\\main.py\", line 159, in __init__\r\n    __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\r\npydantic_core._pydantic_core.ValidationError: 1 validation error for PredictBody\r\nevent_id\r\n  Field required [type=missing, input_value={'data': [], 'event_data'...ion_hash': 'jzg2326scs'}, input_type=dict]\r\n    For further information visit https://errors.pydantic.dev/2.1/v/missing\r\n```\r\n\r\nScreenshots:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/752887/115dd23b-45ae-4c77-b983-b87ce52c2bf4)</BODY>\n\n<COMMENTS>\n<Comment by blankxxc at 2023-09-14T15:03:53Z>\nI have this question too .Are you solve?\n</Comment>\n<Comment by natlamir at 2023-10-13T17:26:14Z>\nDowngrading pyndantic fixed it for me:\r\n```\r\npip install pydantic==1.10.9\r\n```\n</Comment>\n<Comment by haotian-liu at 2023-10-24T00:59:34Z>\nPlease check the latest doc for Windows. I tested on my Windows 11 PC and it works now for 16-bit inference. Quantization will be supported later.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/docs/Windows.md\n</Comment>\n<Comment by Coldmooon at 2024-04-17T16:28:37Z>\nI encountered the same issue. I solve this by\r\n```\r\nsudo rm /etc/X11/xorg.conf\r\n```\r\n\r\nand then `sudo reboot`\r\n\r\nMy server is headless. So I install a virtual display and set the `xorg.conf` accordingly. When I encountered this issue, I also found the virtual display is down. Then I connected a real display to my server and found the desktop of ubuntu system is also down. So I simply delete the `xorg.conf` file and disable the virtual display. The issue disappeared.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 360,
    "state": "closed",
    "created_by": "chigkim",
    "created_at": "2023-08-08T21:51:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/360</URL>\n\n<TITLE>[Question] Readme vs Script for per_device_train_batch_size and increase the gradient_accumulation_steps</TITLE>\n\n<BODY>### Question\n\nAccording to Readme, it sounds like Global Batch Size (per_device_train_batch_size * gradient_accumulation_steps) should be 128.\r\nHowever, both From scripts/pretrain.sh and scripts/finetune.sh have --per_device_train_batch_size 16 --gradient_accumulation_steps 1.\r\nThat sounds like Global Batch Size is 16, not 128?\r\nWhich is correct?\r\nWhat about finetune, is Global Batch Size the same?\r\nAlso, in order to train with fewer gpus, when you you reduce per_device_train_batch_size by x amount, do you just multiply gradient_accumulation_steps by x?\r\nIf we take the scripts (--per_device_train_batch_size 16 --gradient_accumulation_steps 1) as an example, do you divide per_device_train_batch_size/4 and multiply gradient_accumulation_steps*4? Then you replace with --per_device_train_batch_size 4 --gradient_accumulation_steps 4.\r\nLastly, I think it would be clearer if we had different documentation files (one for original paper and one for new).\r\nThanks!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-08-08T21:54:50Z>\nJust corrected this error in the README, global batch size should be: per_device_train_batch_size x gradient_accumulation_steps x **num_gpus**.\r\n\r\nWe used 8 GPUs in pretraining / finetuning, so the global batch size of the finetuning stage is 16x1x8=128.\r\n\r\n> Lastly, I think it would be clearer if we had different documentation files (one for original paper and one for new).\r\n\r\nGreat suggestion. We'll add this later. Thanks.\n</Comment>\n<Comment by chigkim at 2023-08-08T23:04:01Z>\nThanks for the info!\r\nglobal batch size  = per_device_train_batch_size x gradient_accumulation_steps x num_gpus\r\nThat makes sense. Could you clarify one more thing?\r\nReadme said:\r\n\"You may run this with a single A100 GPU with the following code.\r\nPretrain: LLaVA-13B, 1x A100 (80G). Time: ~33 hours.\"\r\npython llava/train/train_mem.py \\\r\n  --per_device_train_batch_size 16\r\n  --gradient_accumulation_steps 8\r\nDoes the per_device_train_batch_size  stays the same as 16? Readme said: \"To train on fewer GPUs, you can reduce the per_device_train_batch_size and increase the gradient_accumulation_steps accordingly.\"\r\nOn a related note, 1x A100 80gb only works for pretraining, not finetuning?\r\nI asked about using 1x a100 80gb for finetuning on another discussion.\r\nhttps://github.com/haotian-liu/LLaVA/discussions/356\n</Comment>\n<Comment by haotian-liu at 2023-08-08T23:33:27Z>\n> \"You may run this with a single A100 GPU with the following code.\r\n> Pretrain: LLaVA-13B, 1x A100 (80G). Time: ~33 hours.\"\r\n> python llava/train/train_mem.py\r\n> --per_device_train_batch_size 16\r\n> --gradient_accumulation_steps 8\r\n> Does the per_device_train_batch_size stays the same as 16? Readme said: \"To train on fewer GPUs, you can reduce the per_device_train_batch_size and increase the gradient_accumulation_steps accordingly.\"\r\n\r\nYes, as 16x8x1=128\r\n\r\n> On a related note, 1x A100 80gb only works for pretraining, not finetuning?\r\n\r\nYou need LoRA/QLoRA for finetuning on a single A100. Full finetuning probably requires either CPU offloading, or multiple GPUs.\n</Comment>\n<Comment by chigkim at 2023-08-12T15:54:02Z>\nHuge thanks @haotian-liu for answering all my questions!\r\nI finally had a chance to score renting 8x A100 80gb gpus, and I successfully finetuned llama-2-13b.\r\nAlso I tried with --per_device_train_batch_size 8 * --gradient_accumulation_steps 4 * 4 a100 80gb gpus. It gave me a warning about clearing memory once in a while, but it finished the process, and the output was usable.\n</Comment>\n<Comment by HashmatShadab at 2024-08-23T06:18:01Z>\n> Huge thanks @haotian-liu for answering all my questions! I finally had a chance to score renting 8x A100 80gb gpus, and I successfully finetuned llama-2-13b. Also I tried with --per_device_train_batch_size 8 * --gradient_accumulation_steps 4 * 4 a100 80gb gpus. It gave me a warning about clearing memory once in a while, but it finished the process, and the output was usable.\r\n\r\nHi, did you face out of memory issues with using batch of 16 with 4 A100 80 GB?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 359,
    "state": "closed",
    "created_by": "tensorboy",
    "created_at": "2023-08-07T21:03:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/359</URL>\n\n<TITLE>[Usage] MPT-7b Finetuing</TITLE>\n\n<BODY>### When did you clone our code?\r\n\r\nI cloned the code base after 08/01/23\r\n\r\n### Describe the issue\r\n\r\nIssue:\r\n\r\nCommand:\r\n```\r\nPROMPT_VERSION=\"mpt\"\r\nMODEL_VERSION=\"mpt-7b-chat\"\r\n\r\ndeepspeed  --master_port 61000  llava/train/train_mem.py \\\r\n    --deepspeed  ./scripts/zero2.json \\\r\n    --model_name_or_path mosaicml/mpt-7b-chat \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path /mnt/bn/data-tns-algo-masp/data/llavar_data/dot_llavar.json \\\r\n    --image_folder /mnt/bn/data-tns-algo-masp/data/llavar_data/finetune_llavr \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava-$MODEL_VERSION-pretrain/mm_projector.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-$MODEL_VERSION-finetune \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 2 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nLog: \r\n```\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/bn/algo-masp-nas-2/environment/anaconda3/envs/llava2/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/mnt/bn/algo-masp-nas-2/environment/anaconda3/envs/llava2/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 1735, in forward\r\n    loss = self.module(*inputs, **kwargs)\r\n  File \"/mnt/bn/algo-masp-nas-2/environment/anaconda3/envs/llava2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/LLaVA/llava/model/language_model/llava_mpt.py\", line 77, in forward\r\n    outputs = self.transformer(input_ids=input_ids, inputs_embeds=inputs_embeds, past_key_values=past_key_values, attention_mas\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/LLaVA/llava/model/language_model/llava_mpt.py\", line 77, in forward   [16/1852]\r\n    outputs = self.transformer(input_ids=input_ids, inputs_embeds=inputs_embeds, past_key_values=past_key_values, attention_mas\r\nk=attention_mask, prefix_mask=prefix_mask, sequence_id=sequence_id, return_dict=return_dict, output_attentions=output_attention\r\ns, output_hidden_states=output_hidden_states, use_cache=use_cache)                                                             \r\n  File \"/mnt/bn/algo-masp-nas-2/environment/anaconda3/envs/llava2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line\r\n 1501, in _call_impl                                                                                                           \r\n    return forward_call(*args, **kwargs)                                                                                       \r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/LLaVA/llava/model/language_model/mpt/modeling_mpt.py\", line 197, in forward    \r\n    (attn_bias, attention_mask) = self._attn_bias(device=x.device, dtype=torch.float32, attention_mask=attention_mask, prefix_m\r\nask=prefix_mask, sequence_id=sequence_id)                                                                                      \r\n  File \"/mnt/bn/algo-masp-nas-2/environment/anaconda3/envs/llava2/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line\r\n 115, in decorate_context                                                                                                      \r\n    return func(*args, **kwargs)                                                                                               \r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/LLaVA/llava/model/language_model/mpt/modeling_mpt.py\", line 116, in _attn_bias \r\n    attn_bias = attn_bias.masked_fill(~attention_mask.view(-1, 1, 1, s_k), min_val)                                            \r\nRuntimeError: The size of tensor a (2303) must match the size of tensor b (2048) at non-singleton dimension 3  \r\n```\r\n\r\nScreenshots:\r\nNone</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 358,
    "state": "open",
    "created_by": "breezedeus",
    "created_at": "2023-08-07T16:17:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/358</URL>\n\n<TITLE>[Question] int-8 bits for instruct tuning</TITLE>\n\n<BODY>### Question\n\nMany thanks to the authors for this very good work.\r\n\r\nI'm trying to run the instruct tuning part on one 3090 GPU, using the int-8 mode:\r\n\r\n```\r\n    python llava/train/train.py \\\r\n    --model_name_or_path lmsys/vicuna-7b-v1.3 \\\r\n    --version v1 \\\r\n    --data_path ./LLaVA-Instruct-150K/llava_instruct_80k.json \\\r\n    --image_folder ./coco/train2017 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter checkpoints/llava-pretrain-vicuna-7b-v1.3/mm_projector.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --bits 8 \\\r\n    --output_dir ./checkpoints/llava-vicuna-v1-3-7b-finetune-tmp \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 2 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 8 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 1000 \\\r\n    --max_steps -1 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nThe above command can finish the training and save the model successfully. The following is the loss curve:\r\n<img width=\"1360\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/6712673/b03296e6-2dd5-4ef7-83e3-041930e5bcb0\">\r\n\r\n\r\n But when I use the following command to make predictions using the trained model, the result is very bad:\r\n\r\n```\r\n    python llava/eval/run_llava.py \\\r\n    --model-path ./checkpoints/llava-vicuna-v1-3-7b-finetune-tmp \\\r\n    --image-file images/llava_logo.png \\\r\n    --query \"describe the image\"\r\n```\r\n\r\nIt's the result:\r\n\r\n```\r\nchevlocchevchevchevlocchevlocchevchevlocchevchevlocchevlocchevlocchevloclocchevlocchevchevchevlocchevloclocchevlocchevchevchevlocchevlocchevloclocchevlocchevchevloclocchevlocch\r\nevlocchevlocchevchevlocchevloclocloclocchevloclocchevloclocchevchevchevlocchevlocchevchevchevlocchevchevlocchevlocchevlocchevlocchevchevloclocchevchevchevchevchevlocchevloclocc\r\nhevlocchevlocchevchevloclocchevchevchevchevlocloclocloclocchev Хронологијасняchevlocchevchevchevlocchevchevlocchevlocchevchevchevlocchevchevchevlocchevlocchevchevchevlocchevmer\r\nkсняchevchevlocchev Хронологијаchevсняchevmerkchevchevchevchevchev Хронологијаchevmerkchevchevсняchevchevchevсняmerkmerklocchevchevchevchevchevchevchevmerklocmerkchev Хронологи\r\n ...\r\n```\r\n\r\nDoes anyone know why? Is it because I'm using the model incorrectly? Or is this model less effective in int-8 mode?\r\n\r\nThanks much.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-08-07T16:42:19Z>\nHi, I have not tried or verified the full model finetuning using int8 training. Int8/Int4 are mainly designed for quantized LoRA training. Please check out the instructions [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md#training).\n</Comment>\n<Comment by breezedeus at 2023-08-09T08:45:19Z>\n> Hi, I have not tried or verified the full model finetuning using int8 training. Int8/Int4 are mainly designed for quantized LoRA training. Please check out the instructions [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md#training).\r\n\r\n@haotian-liu Thanks for your suggestion. I know there were problems with the previous training, so I changed to lora+8-bits training as you suggested. Below is my instruct tuning loss curve:\r\n<img width=\"1807\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/6712673/fd94a098-aaf6-4d19-8c92-e9dd9b131e8c\">\r\n\r\nThe results generated from the model are not very satisfactory. For instance, I want the model to describe the llava logo image:\r\n\r\n```bash\r\npython llava/eval/run_llava.py \\\r\n--model-path ./checkpoints/llava-vicuna-v1-3-7b-finetune-lora \\\r\n--model-base lmsys/vicuna-7b-v1.3 \\\r\n    --load-8bit \\\r\n    --image-file images/llava_logo.png \\\r\n    --query \"describe the image\"\r\n```\r\n\r\nThe following are generated responses：\r\n```\r\nThe image is a close-up of a person's face, with a blurred background. The person's eyes are open, and their facial expression appears to be one of concentration or focus. The\r\nblurred background adds a sense of depth and focus to the image, making the person's face the primary point of interest. The overall effect is a visually striking and engaging\r\nportrait that captures the viewer's attention.\r\n```\r\n\r\nWould you mind providing your Lora instruct tuning logs? Thanks much.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 357,
    "state": "open",
    "created_by": "xtong-zhang",
    "created_at": "2023-08-07T13:30:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/357</URL>\n\n<TITLE>[Question] Why the results of 7b-v1.1 are worse than 7b-v0?</TITLE>\n\n<BODY>### Question\r\n\r\nHello Haotian:\r\n\r\nReally appreciate that you shared this awesome work.  I used the script to train the two stages with vicuna-7b-v0 and vicuna-7b-v1.1 respectively, and used gpt4 to evaluate. The results of v1.1 were worse than that of v0, and the results are shown as follows. What is the possible reason?\r\n\r\n<img src=\"https://github.com/haotian-liu/LLaVA/assets/53788110/dcd2fc7e-89e6-4c73-a18c-a8754e84f9cd\" alt=\"图片描述\" width=\"200\"></BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-08-07T16:48:35Z>\nHi, we find that there is randomness in the sampling (temperature=0.2) which contributes to the variance of the result quality. GPT-4 evaluation in different runs is pretty consistent, with a smaller variance. Running `model_vqa.py` multiple times may allow us to better understand the performance of the model.\r\n\r\nAlso, given that Bench-COCO is close to the training domain, it is expected that different LLMs may behave similarly on this dataset, with one being better than the other as a result of the variance.\r\n\r\nThanks.\n</Comment>\n<Comment by yichaoshen-MS at 2023-08-15T12:09:51Z>\n> Hi, we find that there is randomness in the sampling (temperature=0.2) which contributes to the variance of the result quality. GPT-4 evaluation in different runs is pretty consistent, with a smaller variance. Running `model_vqa.py` multiple times may allow us to better understand the performance of the model.\r\n> \r\n> Also, given that Bench-COCO is close to the training domain, it is expected that different LLMs may behave similarly on this dataset, with one being better than the other as a result of the variance.\r\n> \r\n> Thanks.\r\n\r\nHello, sorry, I haven't find the way to evaluate the model in Bench-COCO in this repo. Could you tell me where is a script to learn? Thanks\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 355,
    "state": "closed",
    "created_by": "CupidJay",
    "created_at": "2023-08-07T07:38:09Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/355</URL>\n\n<TITLE>[Question] Can anyone reproduce ScienceQA result with the latest code?</TITLE>\n\n<BODY>### Question\n\nI cloned the latest code and I have tried various versions of the LLaMA-13B weights (including the original one). However, I can not reproduce the results on ScienceQA following the authors' instructions. I wonder if anyone has reproduced the results with the latest version of the code? I saw that it was successfully reproduced before, but I don't know what version it was.\r\n\r\nSome examples of output I have here are as follows\r\n```\r\n    \"16410\": \"Saint Louis is a city in Missouri.\\n The answer is A is the capital of Missouri.\",\r\n    \"16421\": \"LECTURE: Measurements are used to estimate how long something is. There are many different ways to estimate distance, but when you are trying to estimate how long something is, it is important to be accurate.\\nThere are 1000 miles in 1 kilometer. So, 1000 miles is much longer than 1 mile.\\nSOLUTION: The Amazon River is about 4,000 miles long.\\n The answer is C.\",\r\n    \"16471\": \"45 milliliters is a very small amount of liquid. It takes just 45 liters to fill a salt shaker.\\n The answer is A.\",\r\n    \"16534\": \"LECTURE: In the United States, the ball is in Dylan's court. This means that Dylan has some difficult work to do.\\nThe idiom in this text suggests that Dylan needs to act next. This means that Dylan has to do something.\\nSOLUTION: The idiom in this text suggests that Dylan has to do something. This could mean that Dylan has to act next or that he has to do some difficult work.\\n The answer is A.\",\r\n    \"16557\": \"This sentence is exclamatory. It uses an exclamatory phrase to show strong feeling or emotion.\\n The answer is A.\",\r\n    \"16626\": \"LECTURE: Inherited traits are passed down through families. Inherited traits are caused by the combination of genes from both parents.\\nGenes contain information about how an organism looks and acts. Some genes contain information about how an organism should look or act.\\nGenes affect how an organism looks or acts.\\nGenes are passed down through families.\\nChildren grow and develop into adults.\\nSOLUTION: Inherited traits are passed down through families. Children grow into adults.\\n The answer is A.\",\r\n    \"16629\": \"LECTURE: In order to keep the peace, we must know what we are trying to keep. If we are trying to keep the peace, we must know what we are trying to keep. If we are trying to keep the peace, we must know what we are trying to keep.\\nSOLUTION: In order to keep the peace, we must know what we are trying to keep. If we are trying to keep the peace, we must know what we are trying to keep. If we are trying to keep the peace, we must know what we are trying to keep.\\n The answer is A is trying to keep the peace.\",\r\n    \"16699\": \"This is a food Web. It is a place where organisms can find food.\\n The answer is A food web is a place where organisms can find food.\",\r\n    \"16744\": \"South Carolina is a state in the United States.\\n The answer is A\",\r\n    \"16810\": \"The euphemism is used to describe a polite way of saying that something is overweight.\\n The answer is A.\",\r\n    \"16878\": \"LECTURE: A letter is a written document that is sent to someone. It can include information about a topic or a person.\\nFor example, you might write a letter to a friend, thanking him or her for a gift.\\nIn the United States, the Postal Service provides a way to send letters. You can write a letter to your friend, thanking him or her for a gift.\\nSOLUTION: The second sentence is correct. It thank you is the correct way to say thank you.\\n The answer is A.\",\r\n```\r\n\r\nI have been trying for two weeks, can anyone help me？</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-08-07T16:55:36Z>\nHi @CupidJay, I will try reproducing this again on Thursday, and update the docs if needed.\r\n\r\nI will also release a checkpoint that contains the directly merged weights for ScienceQA.\r\n\r\nSorry for the confusion, and stay tuned for the update. Thanks!\n</Comment>\n<Comment by CupidJay at 2023-08-08T01:47:37Z>\nThanks for your reply. I appreciate your efforts for this great work and look forward to the update.\n</Comment>\n<Comment by CupidJay at 2023-08-08T05:23:29Z>\n> Hi @CupidJay, I will try reproducing this again on Thursday, and update the docs if needed.\r\n> \r\n> I will also release a checkpoint that contains the directly merged weights for ScienceQA.\r\n> \r\n> Sorry for the confusion, and stay tuned for the update. Thanks!\r\n\r\nHi, I just cloned [this previous commit ](https://github.com/haotian-liu/LLaVA/tree/08759473173cc0a9fdc852f06a04c6a69dd1b5aa)and I can reproduce the result on ScienceQA now. It seems that the latest code has some conflicts with LLaMA-13B-v0 on ScienceQA. By the way, can I use this version of code to run finetune code on ScienceQA?\n</Comment>\n<Comment by dingning97 at 2023-08-25T03:57:55Z>\nI use the latest code to reproduce the experiments on ScienceQA, But the results I got were really bad.\r\nThe Llama-2-7b achieved ~66% performance and Vicuna-7b-v1.3 achieved ~62% performance.\r\n\r\nHere is the finetuning script I used:\r\n####################################\r\nPROMPT_VERSION=v1\r\nMODEL_VERSION=\"vicuna-7b-v1.3\"\r\n\r\ndeepspeed llava/train/train.py --deepspeed ./scripts/zero2.json \\\r\n    --lora_enable True \\\r\n    --model_name_or_path ./checkpoint/$MODEL_VERSION \\\r\n    --pretrain_mm_mlp_adapter ./checkpoint//llava-$MODEL_VERSION-pretrain/mm_projector.bin \\\r\n    --vision_tower ./checkpoint/models--openai--clip-vit-large-patch14 \\\r\n    --mm_vision_select_layer -2 \\\r\n    --data_path $SQA_FOLDER/llava_train_QCM-LEPA.json \\\r\n    --image_folder $SQA_FOLDER/images/train \\\r\n    --output_dir ./checkpoint/llava-$MODEL_VERSION-finetune-sqa-lora \\\r\n    --num_train_epochs 12 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --fp16 True \\\r\n    --bf16 False \\\r\n    --tf32 False \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 5000 \\\r\n    --save_total_limit 3 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --model_max_length 2048 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to tensorboard \\\r\n    --version $PROMPT_VERSION\r\n####################################\r\n\r\nCould you plz tell me what's wrong @haotian-liu @CupidJay\n</Comment>\n<Comment by dingning97 at 2023-08-25T04:08:59Z>\n@CupidJay  Hi!  Are you able to reproduce the result using Llama-2 or vicuna-v1.3 on ScienceQA ?\n</Comment>\n<Comment by CupidJay at 2023-08-25T04:27:50Z>\n> @CupidJay Hi! Are you able to reproduce the result using Llama-2 or vicuna-v1.3 on ScienceQA ?\r\n\r\nHi, I am using Llama-13b on ScienceQA. I can reproduce the **evaluation** result using the given checkpoint (90.92 in the paper and I got 90.66). However, I **have not reproduced the fine-tuning** results till now (I got only 89.93).\r\n\r\nAs I noted before, I get very low accuracy with the current codebase on ScienceQA and I can only reproduce the evaluation result with this [previous commit](https://github.com/haotian-liu/LLaVA/tree/08759473173cc0a9fdc852f06a04c6a69dd1b5aa). Moreover, being able to reproduce the evaluation results does not mean that the results of fine-tuning can be reproduced, because the training method seems to have changed a lot (e.g., with/without im_end_token). \r\n\r\nIn short, there are many variables, such as the version of the code, the version of transformers, the version of the checkpoints, the way of token processing, and the training scripts. I am also in a mess now and I think we should wait for the authors to address this issue.\n</Comment>\n<Comment by haotian-liu at 2023-08-27T06:13:35Z>\nHi all, sorry for the confusion. I have updated the instructions and the evaluation scripts for the latest code base. I also trained a version based on Vicuna v1.3, and I get accuracy 91.1% with direct eval (without GPT-4 as the judge, [results file](https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/table/results/test_sqa_llava_lcs_558k_sqa_12e_vicuna_v1_3_13b.json)).\r\n\r\nNote that this checkpoint uses the LCS-558K pretrained projector, instead of CC595K one, as we do not have that for Vicuna v1.3. I will train some more checkpoints if needed in the future when I have more GPUs available.\r\n\r\nPlease check out the updated [instructions, training scripts](https://github.com/haotian-liu/LLaVA/blob/main/docs/ScienceQA.md) and [checkpoints](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#science-qa-checkpoints). Please let me know if there is any confusion. \r\n\r\nAlso please note that we used \"QCM-LEA\" format to let the model directly output the answer at the end of the answer, without a second prompt, to make the whole process cleaner and more standard. Please regenerate the training data with the correct prompt format. Thanks.\n</Comment>\n<Comment by CupidJay at 2023-08-30T08:44:51Z>\n> Hi all, sorry for the confusion. I have updated the instructions and the evaluation scripts for the latest code base. I also trained a version based on Vicuna v1.3, and I get accuracy 91.1% with direct eval (without GPT-4 as the judge, [results file](https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/table/results/test_sqa_llava_lcs_558k_sqa_12e_vicuna_v1_3_13b.json)).\r\n> \r\n> Note that this checkpoint uses the LCS-558K pretrained projector, instead of CC595K one, as we do not have that for Vicuna v1.3. I will train some more checkpoints if needed in the future when I have more GPUs available.\r\n> \r\n> Please check out the updated [instructions, training scripts](https://github.com/haotian-liu/LLaVA/blob/main/docs/ScienceQA.md) and [checkpoints](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#science-qa-checkpoints). Please let me know if there is any confusion.\r\n> \r\n> Also please note that we used \"QCM-LEA\" format to let the model directly output the answer at the end of the answer, without a second prompt, to make the whole process cleaner and more standard. Please regenerate the training data with the correct prompt format. Thanks.\r\n\r\nThanks for your efforts and it works for me now. I will close this issue.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 354,
    "state": "closed",
    "created_by": "chigkim",
    "created_at": "2023-08-06T21:52:30Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/354</URL>\n\n<TITLE>[Discussion] Llava on a Multimodal Leaderboard?</TITLE>\n\n<BODY>### Discussion\n\n@haotian-liu could you submit Llava-Llama-2 on the leader board below when it's officially out?\r\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation\r\nHopefully it scores better.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-13T02:12:05Z>\nLLaVA-1.5 is now on the leaderboard and SoTA on more :)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 353,
    "state": "closed",
    "created_by": "FuxiaoLiu",
    "created_at": "2023-08-06T09:16:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/353</URL>\n\n<TITLE>[Question] Can't reproduce the result of LLaVA7B</TITLE>\n\n<BODY>### Question\r\n\r\nI finetune llava7b on v100, 8GPU with the following script (llava_instruct_150k.json):\r\n\r\ndeepspeed ./llava/train/train.py \\\r\n    --deepspeed zero3_offload.json\\\r\n    --model_name_or_path /apdcephfs_cq3/share_1603164/user/fuxiaoliu/vicuna-7b-v1.3\\\r\n    --version v1 \\\r\n    --data_path /llava_instruct_150k.json \\\r\n    --image_folder /apdcephfs_cq3/share_1603164/user/fuxiaoliu/llava_dataset/train2017 \\\r\n    --vision_tower /apdcephfs_cq3/share_1603164/user/fuxiaoliu/llava_dataset/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter /apdcephfs_cq3/share_1603164/user/fuxiaoliu/projector/mm_projector.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False\\\r\n    --fp16 True \\\r\n    --output_dir /apdcephfs_cq2/share_1603164/user/fuxiaoliu/checkpoints_llava/llava-17b-finetune_llava \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 500 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 False \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n\r\nThe projector is from here:\r\n<img width=\"1022\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/60458715/8aae726d-b675-4026-8abd-a2afd8ffbc0d\">\r\n\r\nMy output model weight looks like this:\r\n<img width=\"379\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/60458715/047ef812-adcb-4fb0-aab9-53275eff2cb5\">\r\n\r\nI try to load the model weight in the web demo, and try the following questions:\r\n<img width=\"820\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/60458715/204b3b2e-dfb1-4e70-98f3-3da8cb93c1c0\">\r\n\r\nIt seems that the answer doesn't follow the instruction.\r\n\r\nHowever, when I don't use the image and directly ask about the question:\r\n<img width=\"526\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/60458715/b3c4d832-f7cb-45dd-b608-c8c3872816c0\">\r\n\r\nIt follows the instructions. Do you have any ideas?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-08-06T21:24:14Z>\nHi Fuxiao,\r\n\r\nI have not trained model with both FP16 and zero3_offload. I am not sure if there are some issues with this specific configuration, but the commands you are using generally looks good to me.\r\n\r\nI tried to train a lightning version (80k+1epoch, FP16, zero3 w/o offload) this morning, and it seems to be working fine. I have attached the command I used and the log below. Unfortunately I do not have the resource to run zero3 offload this week, as this somehow will cause all other processes on our lab machine to halt..\r\n\r\nYou may try running with the Lightning data (which is quicker and easier to verify).\r\n\r\nWandb: https://api.wandb.ai/links/lht/jjsx93kz\r\n\r\n```\r\nllava/train/train_mem.py --local_rank=0 --deepspeed dev_scripts/ds_zero3.json --model_name_or_path ./checkpoints/vicuna-7b-v1-3 --version v1 --data_path ./playground/data/llava_instruct/conv_reason_no_overlap_80k.json --image_folder /nobackup/haotian/datasets/coco/train2017 --vision_tower openai/clip-vit-large-patch14 --pretrain_mm_mlp_adapter ./checkpoints/ds_llava-vicuna-7b-v1-3-pretrain_blip558k_plain/mm_projector.bin --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 False --fp16 True --output_dir ./checkpoints/ds_llava-vicuna-7b-v1-3-pretrain_blip558k_plain-finetune_80k_1e_fp16 --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 4096 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb\r\n```\r\n\r\n```\r\nmd5sum ds_llava-vicuna-7b-v1-3-pretrain_blip558k_plain/mm_projector.bin\r\nc96e816e8a1ae5f730fff68f21a00909  ds_llava-vicuna-7b-v1-3-pretrain_blip558k_plain/mm_projector.bin\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 352,
    "state": "closed",
    "created_by": "FuxiaoLiu",
    "created_at": "2023-08-05T00:43:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/352</URL>\n\n<TITLE>Projector weights of LLaVA after the Instruction Tuning</TITLE>\n\n<BODY>### Question\n\nThanks for your amazing work! I want to fine-tune LLaVA based on the checkpoint(projector weights) after the instruction tuning on llava_instruct_150k.json. However, it seems that there are only projector weights from the first pretraining stage. Is it possible to get the projector weights after the instruction tuning? Thanks!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-08-05T05:29:57Z>\nHi, you may extract the projector weights manually using [this now-deleted script](https://github.com/haotian-liu/LLaVA/blob/36b28148223f04ad0d8e54ca7764f1fa31216eb6/scripts/extract_mm_projector.py), but it may give you unexpected behavior, as we finetune the LLM, and using incompatible projector weights and LLMs can result in bad/garbled outputs.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 351,
    "state": "open",
    "created_by": "ohhiohhi",
    "created_at": "2023-08-04T15:20:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/351</URL>\n\n<TITLE>[Question] How to generate caption for images in dataset on hpc</TITLE>\n\n<BODY>### Question\n\nI wanna know how to generate caption for all the images in the dataset using llava in hpc, can anyone tell me?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-08-06T21:38:24Z>\nHi, you can refer to [model_vqa.py](https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/model_vqa.py) for the inferecing on all images in the dataset.\r\n\r\nReference instructions (the first point suffices): https://github.com/haotian-liu/LLaVA#gpt-assisted-evaluation\n</Comment>\n<Comment by ohhiohhi at 2023-08-23T09:57:17Z>\nthanks，I tried what you said, but a new problem arose@haotian-liu \r\n\r\nWhen I use model_vqa.py for caption， the generated text is garbled，i didn't change file model_vqa.py\r\n\r\n**pretrain model**：llava-llama-2-13b-chat-lightning-preview\r\n\r\n**Run the following code**\r\npython -m llava.eval.model_vqa \\\r\n    --model-path \"./llava-llama-2-13b-chat-lightning-preview\" \\\r\n    --question-file ./playground/test/test.jsonl \\\r\n    --image-folder ./llava/serve/examples/ \\\r\n    --answers-file ./playground/test/test_answer.jsonl\r\n\r\nprompt：caption the image in 30 words or less\r\nGenerates garbled results for all images，such as：c. sec: router devgn #\r\n\r\n**At first** \r\nI thought it was a model issue, i tried pretrained model LLaVA-Lightning-MPT-7B-preview，the output is still a mess，\r\n\r\n**After**\r\nI thought it was an environment issue, but when using llama-7b-hf for QA tasks, the output is normal\r\n\r\nNow I don't know why.\n</Comment>\n<Comment by fj6833 at 2023-09-24T03:37:03Z>\nHello, may I ask if you have resolved this issue? I have the same question and would you please provide me with a contact information? Thank you\n</Comment>\n<Comment by ohhiohhi at 2023-10-16T07:58:15Z>\nsorry, I didn't fix that, we can get in touch via my email,18191547735@163.com\n</Comment>\n<Comment by jameszhou-gl at 2023-11-10T12:49:06Z>\nHi @ohhiohhi , just a discussion. I ever tried to use LLaVA to generate captions on ScienceQA dataset. It works well, at least no garbled results found. \r\nHave you tried on ScienceQA dataset using llava.eval.model_vqa? I translated sqa format into vqa format firstly.\n</Comment>\n<Comment by ohhiohhi at 2023-11-14T14:01:39Z>\nThanks for your reply , I solved the problem by reconfiguring the environment\n</Comment>\n<Comment by FurkanGozukara at 2023-11-25T11:18:50Z>\n> Thanks for your reply , I solved the problem by reconfiguring the environment\r\n\r\nhi what is the prompt you are giving?\r\n\r\n@jameszhou-gl which prompt you used to caption images?\n</Comment>\n<Comment by ohhiohhi at 2023-11-25T12:14:40Z>\n> > Thanks for your reply , I solved the problem by reconfiguring the environment\r\n> \r\n> hi what is the prompt you are giving?\r\n> \r\n> @jameszhou-gl which prompt you used to caption images?\r\n\r\n 'caption the image with keywords'\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 350,
    "state": "closed",
    "created_by": "gnimyang",
    "created_at": "2023-08-04T14:12:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/350</URL>\n\n<TITLE>why LLaVA need load CLIP model first, and how to load openai/clip-vit-large-patch14-336</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nwhen I running \r\n> `llava.serve.model_worker` with `liuhaotian/llava-v1-0719-336px-lora-merge-vicuna-13b-v1.3` model\r\nI meet a problem that I need download CLIP model from internet, but I offline (have to)\r\nDo you have any method I can load this CLIP model offline, better to load it into cache(/public/home/usr/.cache/huggingface/hub/), which convenient for program reload it.\r\n\r\ndetailed model is openai/clip-vit-large-patch14-336\r\n\r\nthe error feed back is\r\n`\r\n2023-08-04 21:17:15 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='./checkpoints/LLaVA-vicuna-13B-v1.3', model_base=None, model_name=None, multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=False)\r\n2023-08-04 21:17:15 | INFO | model_worker | Loading the model LLaVA-vicuna-13B-v1.3 on worker e61d07 ...\r\nYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\r\n2023-08-04 21:17:15 | ERROR | stderr | Traceback (most recent call last):\r\n2023-08-04 21:17:15 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py\", line 417, in cached_file\r\n2023-08-04 21:17:15 | ERROR | stderr |     resolved_file = hf_hub_download(\r\n2023-08-04 21:17:15 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\r\n2023-08-04 21:17:15 | ERROR | stderr |     return fn(*args, **kwargs)\r\n2023-08-04 21:17:15 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1291, in hf_hub_download\r\n2023-08-04 21:17:15 | ERROR | stderr |     raise LocalEntryNotFoundError(\r\n2023-08-04 21:17:15 | ERROR | stderr | huggingface_hub.utils._errors.LocalEntryNotFoundError: Connection error, and we cannot find the requested files in the disk cache. Please try again or make sure your Internet connection is on.\r\n2023-08-04 21:17:15 | ERROR | stderr | \r\n2023-08-04 21:17:15 | ERROR | stderr | During handling of the above exception, another exception occurred:\r\n2023-08-04 21:17:15 | ERROR | stderr | \r\n2023-08-04 21:17:15 | ERROR | stderr | Traceback (most recent call last):\r\n2023-08-04 21:17:15 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n2023-08-04 21:17:15 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n2023-08-04 21:17:15 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n2023-08-04 21:17:15 | ERROR | stderr |     exec(code, run_globals)\r\n2023-08-04 21:17:15 | ERROR | stderr |   File \"/public/home/v-yumy/Pycharm_Project/LLaVA/llava/serve/model_worker.py\", line 273, in <module>\r\n2023-08-04 21:17:15 | ERROR | stderr |     worker = ModelWorker(args.controller_address,\r\n2023-08-04 21:17:15 | ERROR | stderr |   File \"/public/home/v-yumy/Pycharm_Project/LLaVA/llava/serve/model_worker.py\", line 64, in __init__\r\n2023-08-04 21:17:15 | ERROR | stderr |     self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\r\n2023-08-04 21:17:15 | ERROR | stderr |   File \"/public/home/v-yumy/Pycharm_Project/LLaVA/llava/model/builder.py\", line 100, in load_pretrained_model\r\n2023-08-04 21:17:15 | ERROR | stderr |     model = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\n2023-08-04 21:17:15 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2700, in from_pretrained\r\n2023-08-04 21:17:15 | ERROR | stderr |     model = cls(config, *model_args, **model_kwargs)\r\n2023-08-04 21:17:15 | ERROR | stderr |   File \"/public/home/v-yumy/Pycharm_Project/LLaVA/llava/model/language_model/llava_llama.py\", line 46, in __init__\r\n2023-08-04 21:17:15 | ERROR | stderr |     self.model = LlavaLlamaModel(config)\r\n2023-08-04 21:17:15 | ERROR | stderr |   File \"/public/home/v-yumy/Pycharm_Project/LLaVA/llava/model/language_model/llava_llama.py\", line 38, in __init__\r\n2023-08-04 21:17:15 | ERROR | stderr |     super(LlavaLlamaModel, self).__init__(config)\r\n2023-08-04 21:17:15 | ERROR | stderr |   File \"/public/home/v-yumy/Pycharm_Project/LLaVA/llava/model/llava_arch.py\", line 32, in __init__\r\n2023-08-04 21:17:15 | ERROR | stderr |     self.vision_tower = build_vision_tower(config, delay_load=True)\r\n2023-08-04 21:17:15 | ERROR | stderr |   File \"/public/home/v-yumy/Pycharm_Project/LLaVA/llava/model/multimodal_encoder/builder.py\", line 7, in build_vision_tower\r\n2023-08-04 21:17:15 | ERROR | stderr |     return CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\r\n2023-08-04 21:17:15 | ERROR | stderr |   File \"/public/home/v-yumy/Pycharm_Project/LLaVA/llava/model/multimodal_encoder/clip_encoder.py\", line 20, in __init__\r\n2023-08-04 21:17:15 | ERROR | stderr |     self.cfg_only = CLIPVisionConfig.from_pretrained(self.vision_tower_name)\r\n2023-08-04 21:17:15 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/clip/configuration_clip.py\", line 239, in from_pretrained\r\n2023-08-04 21:17:15 | ERROR | stderr |     config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n2023-08-04 21:17:15 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 617, in get_config_dict\r\n2023-08-04 21:17:15 | ERROR | stderr |     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n2023-08-04 21:17:15 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 672, in _get_config_dict\r\n2023-08-04 21:17:15 | ERROR | stderr |     resolved_config_file = cached_file(\r\n2023-08-04 21:17:15 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py\", line 452, in cached_file\r\n2023-08-04 21:17:15 | ERROR | stderr |     raise EnvironmentError(\r\n2023-08-04 21:17:15 | ERROR | stderr | OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like openai/clip-vit-large-patch14-336 is not the path to a directory containing a file named config.json.\r\n2023-08-04 21:17:15 | ERROR | stderr | Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\r\n\r\n`</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-08-06T15:23:26Z>\nDiscussion here is highly overlapped with #348, let's condensing the discussion there.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 349,
    "state": "closed",
    "created_by": "gnimyang",
    "created_at": "2023-08-04T12:22:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/349</URL>\n\n<TITLE>is there any different between lora-vicuna-13b-v1.3 and lora-merge-vicuna-13b-v1.3</TITLE>\n\n<BODY>### Question\n\nI don't really understand why you using merge to express those model difference, and the merge version is much smaller than the normal, which method you used to compress the model size?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-08-05T05:14:17Z>\nLoRA merge is by merging the LoRA weights to the base checkpoint, so it will be same size as the Vicuna checkpoints.\r\n\r\nLoRA is the adapter weights, which is much smaller than the LoRA weights.\r\n\r\nYou can check out the details of LoRA here: https://sh-tsang.medium.com/brief-review-lora-low-rank-adaptation-of-large-language-models-faf5ddd5802f\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 348,
    "state": "closed",
    "created_by": "gnimyang",
    "created_at": "2023-08-04T07:43:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/348</URL>\n\n<TITLE>How to load LLaVA on a server with no Internet connection?</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nI manually download the pre-trained model at my path, here, which click the download button for each.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/130719420/94a06624-23a1-45ba-82c0-f0cd51dc30df)\r\n,and then I set my worker model path is here\r\n![image](https://github.com/haotian-liu/LLaVA/assets/130719420/42447458-cfa3-484f-8769-f3ad79d8b0a0)\r\nthe python worker path is here: python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path C:\\Users\\tx\\Documents\\LLaVA-main\\llava-v1-0719-336px-lora-vicuna-13b-v1.3\r\n![image](https://github.com/haotian-liu/LLaVA/assets/130719420/f0c29ebc-3734-4324-ba1f-beb4a5661cff)\r\nhow to solve this problem. I don't know weather why pre-trained model utilize correctly.</BODY>\n\n<COMMENTS>\n<Comment by gnimyang at 2023-08-04T12:00:03Z>\nI already solve this problem however,\r\nI want to know how to operate pre-trained models.\r\nHow many pre-trained models need I to download, I alrady download https://huggingface.co/liuhaotian/llava-v1-0719-336px-lora-merge-vicuna-13b-v1.3/tree/main model, however the code still give me feedback about I need connect to internet. Due to machine limit, I have to put the model into a remote computer, this remote computer (HPC) cannot link the internet, can you help me explain the pretrained models I need to download and how operate them.\r\n\r\nwhen I running the command of <python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000/ --port 40000 --worker http://localhost:40000/ --model-path ./checkpoints/LLaVA-13B-v0>\r\n\"\r\n2023-08-04 19:42:46 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000/', controller_address='http://localhost:10000/', model_path='./checkpoints', model_base=None, model_name=None, multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=False)\r\n2023-08-04 19:42:46 | INFO | model_worker | Loading the model checkpoints on worker 052325 ...\r\nYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\r\n2023-08-04 19:42:48 | ERROR | stderr | Traceback (most recent call last):\r\n2023-08-04 19:42:48 | ERROR | stderr | File \"/public/home/v-yumy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py\", line 417, in cached_file\r\n2023-08-04 19:42:48 | ERROR | stderr | resolved_file = hf_hub_download(\r\n2023-08-04 19:42:48 | ERROR | stderr | File \"/public/home/v-yumy/anaconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\r\n2023-08-04 19:42:48 | ERROR | stderr | return fn(*args, **kwargs)\r\n2023-08-04 19:42:48 | ERROR | stderr | File \"/public/home/v-yumy/anaconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1291, in hf_hub_download\r\n2023-08-04 19:42:48 | ERROR | stderr | raise LocalEntryNotFoundError(\r\n2023-08-04 19:42:48 | ERROR | stderr | huggingface_hub.utils._errors.LocalEntryNotFoundError: Connection error, and we cannot find the requested files in the disk cache. Please try again or make sure your Internet connection is on.\r\n2023-08-04 19:42:48 | ERROR | stderr |\r\n2023-08-04 19:42:48 | ERROR | stderr | During handling of the above exception, another exception occurred:\r\n2023-08-04 19:42:48 | ERROR | stderr |\r\n2023-08-04 19:42:48 | ERROR | stderr | Traceback (most recent call last):\r\n2023-08-04 19:42:48 | ERROR | stderr | File \"/public/home/v-yumy/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n2023-08-04 19:42:48 | ERROR | stderr | return _run_code(code, main_globals, None,\r\n2023-08-04 19:42:48 | ERROR | stderr | File \"/public/home/v-yumy/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n2023-08-04 19:42:48 | ERROR | stderr | exec(code, run_globals)\r\n2023-08-04 19:42:48 | ERROR | stderr | File \"/public/home/v-yumy/Pycharm_Project/LLaVA/llava/serve/model_worker.py\", line 273, in\r\n2023-08-04 19:42:48 | ERROR | stderr | worker = ModelWorker(args.controller_address,\r\n2023-08-04 19:42:48 | ERROR | stderr | File \"/public/home/v-yumy/Pycharm_Project/LLaVA/llava/serve/model_worker.py\", line 64, in init\r\n2023-08-04 19:42:48 | ERROR | stderr | self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\r\n2023-08-04 19:42:48 | ERROR | stderr | File \"/public/home/v-yumy/Pycharm_Project/LLaVA/llava/model/builder.py\", line 121, in load_pretrained_model\r\n2023-08-04 19:42:48 | ERROR | stderr | model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\n2023-08-04 19:42:48 | ERROR | stderr | File \"/public/home/v-yumy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 493, in from_pretrained\r\n2023-08-04 19:42:48 | ERROR | stderr | return model_class.from_pretrained(\r\n2023-08-04 19:42:48 | ERROR | stderr | File \"/public/home/v-yumy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2700, in from_pretrained\r\n2023-08-04 19:42:48 | ERROR | stderr | model = cls(config, *model_args, **model_kwargs)\r\n2023-08-04 19:42:48 | ERROR | stderr | File \"/public/home/v-yumy/Pycharm_Project/LLaVA/llava/model/language_model/llava_llama.py\", line 46, in init\r\n2023-08-04 19:42:48 | ERROR | stderr | self.model = LlavaLlamaModel(config)\r\n2023-08-04 19:42:48 | ERROR | stderr | File \"/public/home/v-yumy/Pycharm_Project/LLaVA/llava/model/language_model/llava_llama.py\", line 38, in init\r\n2023-08-04 19:42:48 | ERROR | stderr | super(LlavaLlamaModel, self).init(config)\r\n2023-08-04 19:42:48 | ERROR | stderr | File \"/public/home/v-yumy/Pycharm_Project/LLaVA/llava/model/llava_arch.py\", line 32, in init\r\n2023-08-04 19:42:48 | ERROR | stderr | self.vision_tower = build_vision_tower(config, delay_load=True)\r\n2023-08-04 19:42:48 | ERROR | stderr | File \"/public/home/v-yumy/Pycharm_Project/LLaVA/llava/model/multimodal_encoder/builder.py\", line 7, in build_vision_tower\r\n2023-08-04 19:42:48 | ERROR | stderr | return CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\r\n2023-08-04 19:42:48 | ERROR | stderr | File \"/public/home/v-yumy/Pycharm_Project/LLaVA/llava/model/multimodal_encoder/clip_encoder.py\", line 20, in init\r\n2023-08-04 19:42:48 | ERROR | stderr | self.cfg_only = CLIPVisionConfig.from_pretrained(self.vision_tower_name)\r\n2023-08-04 19:42:48 | ERROR | stderr | File \"/public/home/v-yumy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/clip/configuration_clip.py\", line 239, in from_pretrained\r\n2023-08-04 19:42:48 | ERROR | stderr | config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n2023-08-04 19:42:48 | ERROR | stderr | File \"/public/home/v-yumy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 617, in get_config_dict\r\n2023-08-04 19:42:48 | ERROR | stderr | config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n2023-08-04 19:42:48 | ERROR | stderr | File \"/public/home/v-yumy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 672, in _get_config_dict\r\n2023-08-04 19:42:48 | ERROR | stderr | resolved_config_file = cached_file(\r\n2023-08-04 19:42:48 | ERROR | stderr | File \"/public/home/v-yumy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py\", line 452, in cached_file\r\n2023-08-04 19:42:48 | ERROR | stderr | raise EnvironmentError(\r\n2023-08-04 19:42:48 | ERROR | stderr | OSError: We couldn't connect to 'https://huggingface.co/' to load this file, couldn't find it in the cached files and it looks like openai/clip-vit-large-patch14-336 is not the path to a directory containing a file named config.json.\r\n2023-08-04 19:42:48 | ERROR | stderr | Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\r\n\r\n\"\r\n\r\nwhich pre-trained model need to download?\n</Comment>\n<Comment by gnimyang at 2023-08-04T12:04:44Z>\nhow can I manually add openai/clip-vit-large-patch14-336.\n</Comment>\n<Comment by haotian-liu at 2023-08-05T05:12:34Z>\nHi, for offline machines, you can download the corresponding CLIP weights from Hugging Face. For CLIP 336px, it is https://huggingface.co/openai/clip-vit-large-patch14-336.\r\n\r\nSet the option of `mm_vision_tower` in [`config.json`](https://huggingface.co/liuhaotian/llava-v1-0719-336px-lora-merge-vicuna-13b-v1.3/blob/main/config.json#L22) to your local dir where CLIP encoder is stored at.\r\n\r\nAlso, please use the merged weights, unless you have Vicuna weights locally and pass the path of Vicuna by `--model-base`.\n</Comment>\n<Comment by gnimyang at 2023-08-05T11:40:17Z>\n![image](https://github.com/haotian-liu/LLaVA/assets/130719420/5949e3ae-5254-4d40-bbbf-45d22e9d1e46)\r\n\r\nI put the file here\r\nfrom:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/130719420/2fc0f936-e7b4-4721-aa0e-f1f5d076b7bd)\r\n\r\nSet the option of mm_vision_tower in [config.json](https://huggingface.co/liuhaotian/llava-v1-0719-336px-lora-merge-vicuna-13b-v1.3/blob/main/config.json#L22) to your local dir where CLIP encoder is stored at here.\r\n /public/home/v-yumy/Pycharm_Project/LLaVA/checkpoints/openai-clip-vit-large-patch14-336/\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/130719420/fb2abdad-895c-44ed-9a5c-e1fe341cad4f)\r\n\r\nbut it still not work\r\n\r\n2023-08-05 19:38:14 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='./checkpoints/LLaVA-vicuna-13B-v1.3', model_base=None, model_name=None, multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=False)\r\n2023-08-05 19:38:14 | INFO | model_worker | Loading the model LLaVA-vicuna-13B-v1.3 on worker e019a2 ...\r\nYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\r\n2023-08-05 19:38:14 | ERROR | stderr | Traceback (most recent call last):\r\n2023-08-05 19:38:14 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n2023-08-05 19:38:14 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n2023-08-05 19:38:14 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n2023-08-05 19:38:14 | ERROR | stderr |     exec(code, run_globals)\r\n2023-08-05 19:38:14 | ERROR | stderr |   File \"/public/home/v-yumy/Pycharm_Project/LLaVA/llava/serve/model_worker.py\", line 273, in <module>\r\n2023-08-05 19:38:14 | ERROR | stderr |     worker = ModelWorker(args.controller_address,\r\n2023-08-05 19:38:14 | ERROR | stderr |   File \"/public/home/v-yumy/Pycharm_Project/LLaVA/llava/serve/model_worker.py\", line 64, in __init__\r\n2023-08-05 19:38:14 | ERROR | stderr |     self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\r\n2023-08-05 19:38:14 | ERROR | stderr |   File \"/public/home/v-yumy/Pycharm_Project/LLaVA/llava/model/builder.py\", line 100, in load_pretrained_model\r\n2023-08-05 19:38:14 | ERROR | stderr |     model = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\n2023-08-05 19:38:14 | ERROR | stderr |   File \"/public/home/v-yumy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2700, in from_pretrained\r\n2023-08-05 19:38:14 | ERROR | stderr |     model = cls(config, *model_args, **model_kwargs)\r\n2023-08-05 19:38:14 | ERROR | stderr |   File \"/public/home/v-yumy/Pycharm_Project/LLaVA/llava/model/language_model/llava_llama.py\", line 46, in __init__\r\n2023-08-05 19:38:14 | ERROR | stderr |     self.model = LlavaLlamaModel(config)\r\n2023-08-05 19:38:14 | ERROR | stderr |   File \"/public/home/v-yumy/Pycharm_Project/LLaVA/llava/model/language_model/llava_llama.py\", line 38, in __init__\r\n2023-08-05 19:38:14 | ERROR | stderr |     super(LlavaLlamaModel, self).__init__(config)\r\n2023-08-05 19:38:14 | ERROR | stderr |   File \"/public/home/v-yumy/Pycharm_Project/LLaVA/llava/model/llava_arch.py\", line 32, in __init__\r\n2023-08-05 19:38:14 | ERROR | stderr |     self.vision_tower = build_vision_tower(config, delay_load=True)\r\n2023-08-05 19:38:14 | ERROR | stderr |   File \"/public/home/v-yumy/Pycharm_Project/LLaVA/llava/model/multimodal_encoder/builder.py\", line 9, in build_vision_tower\r\n2023-08-05 19:38:14 | ERROR | stderr |     raise ValueError(f'Unknown vision tower: {vision_tower}')\r\n2023-08-05 19:38:14 | ERROR | stderr | ValueError: Unknown vision tower: /public/home/v-yumy/Pycharm_Project/LLaVA/checkpoints/openai-clip-vit-large-patch14-336\r\n\r\nthe tower cannot identify it as a model, why？\n</Comment>\n<Comment by gnimyang at 2023-08-05T12:02:16Z>\nIs something worry? the local dir of openai-clip-vit-large-patch14-336 cannot load in mm-vision-tower\n</Comment>\n<Comment by gnimyang at 2023-08-05T12:23:47Z>\nAdditionally, I am using merge model.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/130719420/4640b5ca-5826-4e7f-922b-896ef20b5ecd)\r\n![image](https://github.com/haotian-liu/LLaVA/assets/130719420/f9963f57-8024-46a3-ab77-e5d4c4736ba9)\r\n\r\nbut it still not work\n</Comment>\n<Comment by haotian-liu at 2023-08-06T15:24:49Z>\nOne alternative I can think of is to keep the config.json the same (as original), and put the folder of the vision encoder in the `openai/clip-vit-large-patch14-336` folder. Basically, we want the model to be able to find this folder by directly navigating from the LLaVA project root (and you execute the command in the LLaVA folder as well)\n</Comment>\n<Comment by gnimyang at 2023-08-07T13:17:38Z>\nthanks, it work when I put the model in the LLaVA project folder.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/130719420/f3c58876-e3e7-4163-bb46-cd82c348108e)\n</Comment>\n<Comment by gnimyang at 2023-08-07T13:19:07Z>\nThanks! The solution is so easy!\n</Comment>\n<Comment by haotian-liu at 2023-08-07T16:49:10Z>\nGlad to hear that the problem is solved.\n</Comment>\n<Comment by paulpacaud at 2025-01-14T14:23:00Z>\nI encountered some issues here, putting openai/clip-vit-large-patch14-336 at the root of LLaVA-NeXT initially did not work, but it was because I was running LLaVA from a broader project, whose one of the depencies was LLaVA. So I actually did need to change the config.json mm_vision_tower to the correct relative path with respect to the project. Hope it helps someone.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 347,
    "state": "closed",
    "created_by": "bastia0321",
    "created_at": "2023-08-04T05:24:33Z",
    "labels": [
      "bug"
    ],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/347</URL>\n\n<TITLE>questions regarding dropping position_ids in the model.prepare_inputs_for_generation</TITLE>\n\n<BODY>Hello Haotian:\r\n\r\nreally appreciate that you shared this awesome work. I have a noob question regarding\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/4f801cd99fe2006e95d9946a36710ce125075484/llava/model/language_model/llava_llama.py#L117\r\n\r\nwould like to know why you dropped position_ids here to override the original method from llama class? Thanks.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-08-05T05:25:34Z>\nHi @bastia0321 \r\n\r\nThank you for bringing this to our attention, this is a great point! I modified this function from the transformers code base, and the code base at that point does not have position_ids yet.\r\n\r\nI will need to double check this and fix this soon. Thanks!\n</Comment>\n<Comment by Darren-greenhand at 2023-08-18T08:37:12Z>\nHi，thx for your excellent work!!\r\nI wonder have you solved the problem?\r\nthis may caused the error below:\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"/tf/LLaVA2/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/tf/project/LLaVA/llava/train/train.py\", line 909, in train\r\n    trainer.train()\r\n  File \"/tf/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/tf/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/tf/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/trainer.py\", line 2654, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/tf/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/trainer.py\", line 2679, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/tf/anaconda3/envs/llava/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/tf/anaconda3/envs/llava/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1156, in forward\r\n    output = self._run_ddp_forward(*inputs, **kwargs)\r\n  File \"/tf/anaconda3/envs/llava/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1110, in _run_ddp_forward\r\n    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]\r\n  File \"/tf/anaconda3/envs/llava/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/tf/anaconda3/envs/llava/lib/python3.9/site-packages/accelerate/utils/operations.py\", line 581, in forward\r\n    return model_forward(*args, **kwargs)\r\n  File \"/tf/anaconda3/envs/llava/lib/python3.9/site-packages/accelerate/utils/operations.py\", line 569, in __call__\r\n    return convert_to_fp32(self.model_forward(*args, **kwargs))\r\n  File \"/tf/anaconda3/envs/llava/lib/python3.9/site-packages/torch/amp/autocast_mode.py\", line 14, in decorate_autocast\r\n    return func(*args, **kwargs)\r\n  File \"/tf/project/LLaVA/llava/model/language_model/llava_llama.py\", line 78, in forward\r\n    outputs = self.model(\r\n  File \"/tf/anaconda3/envs/llava/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/tf/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\", line 685, in forward\r\n    layer_outputs = torch.utils.checkpoint.checkpoint(\r\n  File \"/tf/anaconda3/envs/llava/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\r\n    return CheckpointFunction.apply(function, preserve, *args)\r\n  File \"/tf/anaconda3/envs/llava/lib/python3.9/site-packages/torch/autograd/function.py\", line 506, in apply\r\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\r\n  File \"/tf/anaconda3/envs/llava/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 107, in forward\r\n    outputs = run_function(*args)\r\n  File \"/tf/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\", line 681, in custom_forward\r\n    return module(*inputs, output_attentions, None)\r\n  File \"/tf/anaconda3/envs/llava/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/tf/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\", line 408, in forward\r\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n  File \"/tf/anaconda3/envs/llava/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\nTypeError: forward() got an unexpected keyword argument 'position_ids'\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 346,
    "state": "closed",
    "created_by": "charryshi",
    "created_at": "2023-08-03T15:36:27Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/346</URL>\n\n<TITLE>[Question] problem porting to Baichuan 13B</TITLE>\n\n<BODY>### Question\n\nThanks to the author. When porting to baichuan 13B, there are a lot of warnings when doing finetuning, but the dataset LLaVA-Instruct-150K seem to right, What could be the problem？ thank you\r\n\r\nWARNING: tokenization mismatch: 403 vs. 406. (ignored)\r\nWARNING: tokenization mismatch: 445 vs. 449. (ignored)\r\nWARNING: tokenization mismatch: 406 vs. 410. (ignored)\r\nWARNING: tokenization mismatch: 504 vs. 508. (ignored)\r\nWARNING: tokenization mismatch: 296 vs. 300. (ignored)\r\nWARNING: tokenization mismatch: 257 vs. 261. (ignored)\r\nWARNING: tokenization mismatch: 215 vs. 219. (ignored)\r\nWARNING: tokenization mismatch: 676 vs. 680. (ignored)\r\nWARNING: tokenization mismatch: 145 vs. 148. (ignored)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-08-05T05:27:38Z>\nHi, this may be due to the prompt preprocessing needs modification. [This function](https://github.com/haotian-liu/LLaVA/blob/main/llava/train/train.py#L404) may need refactoring for supporting different prompt modes. For now, we would have to modify this accordingly for prompts that have different structures.\n</Comment>\n<Comment by charryshi at 2023-08-06T12:08:01Z>\nThanks for reply.\n</Comment>\n<Comment by dengtianbi at 2023-09-22T11:27:22Z>\n> ### Question\r\n> Thanks to the author. When porting to baichuan 13B, there are a lot of warnings when doing finetuning, but the dataset LLaVA-Instruct-150K seem to right, What could be the problem？ thank you\r\n> \r\n> WARNING: tokenization mismatch: 403 vs. 406. (ignored) WARNING: tokenization mismatch: 445 vs. 449. (ignored) WARNING: tokenization mismatch: 406 vs. 410. (ignored) WARNING: tokenization mismatch: 504 vs. 508. (ignored) WARNING: tokenization mismatch: 296 vs. 300. (ignored) WARNING: tokenization mismatch: 257 vs. 261. (ignored) WARNING: tokenization mismatch: 215 vs. 219. (ignored) WARNING: tokenization mismatch: 676 vs. 680. (ignored) WARNING: tokenization mismatch: 145 vs. 148. (ignored)\r\n\r\nHi,\r\nI encountered the same problem, did you solve it?\n</Comment>\n<Comment by llpin1992 at 2023-10-15T11:43:21Z>\nI encountered the same problem, can you give me some guidance how to solve it? @haotian-liu\n</Comment>\n<Comment by chenhaoran176879 at 2023-10-23T13:13:33Z>\ndid you figure this out? I have trouble porting to baichuan-2 and reading the preprocessing and conversation formation code. @charryshi\n</Comment>\n<Comment by xmy0916 at 2023-11-06T11:44:38Z>\nMy solution [csdn](https://blog.csdn.net/qq_37668436/article/details/134253349?csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%22134253349%22%2C%22source%22%3A%22qq_37668436%22%7D).\r\nThis is because the baichuan model didn't use bos token, which makes tokenizer.decode lost '< s >' at the begin of the sentence.\n</Comment>\n<Comment by chenhaoran176879 at 2023-11-06T11:48:49Z>\n> My solution [csdn](https://blog.csdn.net/qq_37668436/article/details/134253349?csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%22134253349%22%2C%22source%22%3A%22qq_37668436%22%7D). This is because the baichuan model didn't use bos token, make tokenizer.decode loss at the begin of the sentence.\r\n\r\nThanks! I will verify the final performance of llava-baichuan-2-13b-chat soon.\n</Comment>\n<Comment by xmy0916 at 2023-11-06T11:50:03Z>\n> > My solution [csdn](https://blog.csdn.net/qq_37668436/article/details/134253349?csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%22134253349%22%2C%22source%22%3A%22qq_37668436%22%7D). This is because the baichuan model didn't use bos token, make tokenizer.decode loss at the begin of the sentence.\r\n> \r\n> Thanks! I will verify the final performance of llava-baichuan-2-13b-chat soon.\r\n\r\nhave you test between llava-baichuan-2-13b-chat and llava-baichuan-2-13b-base?\n</Comment>\n<Comment by chenhaoran176879 at 2023-11-06T11:51:24Z>\n> > > My solution [csdn](https://blog.csdn.net/qq_37668436/article/details/134253349?csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%22134253349%22%2C%22source%22%3A%22qq_37668436%22%7D). This is because the baichuan model didn't use bos token, make tokenizer.decode loss at the begin of the sentence.\r\n> > \r\n> > \r\n> > Thanks! I will verify the final performance of llava-baichuan-2-13b-chat soon.\r\n> \r\n> have you test between llava-baichuan-2-13b-chat and llava-baichuan-2-13b-base?\r\n\r\nNot yet, just chat. Due to previous bug, I have stopped running for several weeks...\n</Comment>\n<Comment by xmy0916 at 2023-11-08T10:03:53Z>\n> > > > My solution [csdn](https://blog.csdn.net/qq_37668436/article/details/134253349?csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%22134253349%22%2C%22source%22%3A%22qq_37668436%22%7D). This is because the baichuan model didn't use bos token, make tokenizer.decode loss at the begin of the sentence.\r\n> > > \r\n> > > \r\n> > > Thanks! I will verify the final performance of llava-baichuan-2-13b-chat soon.\r\n> > \r\n> > \r\n> > have you test between llava-baichuan-2-13b-chat and llava-baichuan-2-13b-base?\r\n> \r\n> Not yet, just chat. Due to previous bug, I have stopped running for several weeks...\r\n\r\ni have tested baichuan2-13B, due to the high vram requests(about 80GB/batch size=1) during finetuning, we cannot report the results. results on baichuan2-7B in my private dataset seems worse. Some key metrics droped about 10 points\n</Comment>\n<Comment by chenhaoran176879 at 2023-11-08T12:48:39Z>\nquite slow finetuning 13B... 8xA800  need 80 hours or more.\r\nThis seems strange since vicuna-13B costed much less.\r\nloss dropped to around 0.8, maybe a week later I shall report.\r\nThanks for your debug again.\n</Comment>\n<Comment by xmy0916 at 2023-11-10T06:26:11Z>\n> quite slow finetuning 13B... 8xA800 need 80 hours or more. This seems strange since vicuna-13B costed much less. loss dropped to around 0.8, maybe a week later I shall report. Thanks for your debug again.\r\n\r\nmaybe due to the vocub size of baichuan is 4X lager than vicuna.\n</Comment>\n<Comment by wucx888 at 2023-11-28T05:37:28Z>\n> quite slow finetuning 13B... 8xA800 need 80 hours or more. This seems strange since vicuna-13B costed much less. loss dropped to around 0.8, maybe a week later I shall report. Thanks for your debug again.\r\n\r\ncan you share the result on baichuan-13B? thx\n</Comment>\n<Comment by xmy0916 at 2023-11-28T06:36:52Z>\n> > quite slow finetuning 13B... 8xA800 need 80 hours or more. This seems strange since vicuna-13B costed much less. loss dropped to around 0.8, maybe a week later I shall report. Thanks for your debug again.\r\n> \r\n> can you share the result on baichuan-13B? thx\r\n\r\non my private dataset, most are english, baichuan-13B's result is so worse\n</Comment>\n<Comment by chenhaoran176879 at 2023-11-28T06:50:50Z>\nSorry for being late.\r\n18.8% on MM-Vet, (baichuan-2-13B). I think this is because Baichuan performs worse in English.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 345,
    "state": "closed",
    "created_by": "Jinyi6",
    "created_at": "2023-08-03T13:44:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/345</URL>\n\n<TITLE>[Question] Batch inference?</TITLE>\n\n<BODY>### Question\n\nHi,\r\n\r\nGreat job! I found that currently only inference with batch size equal to 1 is supported. How can I implement inference with batch size not equal to 1? What are the code details that are worth noting when I am modifying code? Thank you very much!\r\n\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/77571182/2d6a42a2-3f74-45c9-a8ca-0f54ac247e45)</BODY>\n\n<COMMENTS>\n<Comment by k1e3v1i4n at 2023-08-14T08:11:18Z>\nlogits_processor is what u need\n</Comment>\n<Comment by haotian-liu at 2023-11-05T05:06:06Z>\nHi, thank you, and I am trying to work on an efficient batch inference. Closing this and consolidating the discussion to https://github.com/haotian-liu/LLaVA/issues/754. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 344,
    "state": "closed",
    "created_by": "yongliang-wu",
    "created_at": "2023-08-03T10:59:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/344</URL>\n\n<TITLE>[Question] The ability of few-shot in-context learning</TITLE>\n\n<BODY>### Question\n\nDear authors, LLaVA is an amazing model. But I'm curious about whether it can do \"in-context-learning\". I want to do some exploration on it.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-04T23:52:17Z>\nCurrently LLaVA is not explicitly finetuned for performing ICL, and the benefit of Multimodal ICL is recently starting to be studied :) Looking forward to your exploration results, thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 343,
    "state": "closed",
    "created_by": "rohan598",
    "created_at": "2023-08-03T02:33:55Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/343</URL>\n\n<TITLE>[Usage] Inference issue with LLaMA-2-13B-Chat</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\n**Issue:** \r\nUnable to perform inference using run_llava. Trying to run inference for llava-llama-2-13b-chat.\r\nModel Weights taken from [hugging face](https://huggingface.co/liuhaotian/llava-llama-2-13b-chat-lightning-preview/tree/main). \r\nProjector weights taken from [hugging face](https://huggingface.co/liuhaotian/llava-pretrain-llama-2-13b-chat/tree/main) (newer link)\r\n\r\n**Command:**\r\n```\r\npython -m llava.eval.run_llava --model-path /fullpathto/llava-llama-2-13b-chat-lightning-preview --image-file /fullpathto/tmp.png --query \"Describe this file in detail\" \r\n```\r\n\r\n**System Details:**\r\nOS: 20.04.6 LTS (Focal Fossa)\r\nGPU: A6000 Nvidia\r\nCPU: AMD x86_64\r\nLog: \r\n```\r\nScreenshot of logs attached\r\n```\r\n\r\n**Screenshots:**\r\n<img width=\"567\" alt=\"issue-ss\" src=\"https://github.com/haotian-liu/LLaVA/assets/25100847/5b7c4729-04dc-4742-bbe1-7cf7ae44952c\"></BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-08-03T04:53:08Z>\nHi @rohan598 \r\n\r\nI forgot to update the prompt template in `run_llava.py`, and just fix that in https://github.com/haotian-liu/LLaVA/commit/4f801cd99fe2006e95d9946a36710ce125075484. Although strangely, it works for me even without this fix (despite a slight degradation of the quality of the response).\r\n\r\nCan you try exactly the same image/prompt that I used to see if the problem still persists?\r\n\r\n```\r\npython -m llava.eval.run_llava --model-path ./checkpoints/llava-llama-2-13b-chat-lightning-preview --image-file https://llava-vl.github.io/static/images/view.jpg --query \"What are the things I should be cautious about when I visit here\"\r\n```\r\n\r\nAlso, we've recently updated our interactive cli chat [here](https://github.com/haotian-liu/LLaVA#cli-inference), in case you are interested in this as well.\n</Comment>\n<Comment by rohan598 at 2023-08-03T22:04:42Z>\nThank you this change worked!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 342,
    "state": "open",
    "created_by": "chigkim",
    "created_at": "2023-08-02T21:19:29Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/342</URL>\n\n<TITLE>[Question] Reproduceability: Which files ggo where?</TITLE>\n\n<BODY>I'd love to try to reproduce the model from pretraining to finetuning.\r\nIt's awesome that there are training and finetuning scripts.\r\nHowever, there are so many parts of dataset, I'm not sure where to put what.\r\nAfter cloning, should I put LLaVA-Pretrain, LLaVA-CC3M-Pretrain-595K, and LLaVA-Instruct-150K inside LLaVA/?\r\nWhat about images.zip files inside LLaVA-Pretrain and LLaVA-CC3M-Pretrain-595K? Where should I extract them into? The zip files don't seem to have their own root folders.\r\n\r\nHere are what I have gathered so far:\r\n```\r\nLLaVA\r\n\tLLaVA-Pretrain\r\n\t\tblip_laion_cc_sbu_558k.json\r\n\t\tblip_laion_cc_sbu_558k_meta.json\r\n\t\timages.zip\r\n\tLLaVA-CC3M-Pretrain-595K\r\n\t\tchat.json\r\n\t\timages.zip\r\n\t\tmetadata.json\r\n\t\tLLaVA-Instruct-150K\r\n\t\tcomplex_reasoning_77k.json\r\n\t\tconversation_58k.json\r\n\t\tdetail_23k.json\r\n\t\tllava_instruct_80k.json\r\n\t\tllava_instruct_150k.json\r\n```\r\n\r\nI downloaded http://images.cocodataset.org/zips/train2017.zip.\r\nIs there any other additional dataset required? If so, where can I download them?\r\nThank you so much.</BODY>\n\n<COMMENTS>\n<Comment by chigkim at 2023-08-02T22:35:20Z>\nActually, I just realized that I'm supposed to edit pretrain.sh and finetune.sh inside the scripts folder.\r\n\r\nI'd appreciate if someone could help me to edit and point to write things.\r\n1. for both pretrain.sh and finetune.sh, --deepspeed /path/to/deepspeed.json\r\nWhere can I download deepspeed.json?\r\n2. For pretrain.sh, --image_folder /path/to/images\r\nWhich one do I extract and use?\r\nLLaVA-CC3M-Pretrain-595K/images.zip\r\nLLaVA-Pretrain/images.zip\r\n3. For pretrain.sh, --data_path /path/to/pretrain_data.json\r\nWhich one should I use?\r\nLLaVA-CC3M-Pretrain-595K/chat.json\r\nLLaVA-CC3M-Pretrain-595K/metadata.json\r\nLLaVA-Pretrain/blip_laion_cc_sbu_558k.json\r\nLLaVA-Pretrain/blip_laion_cc_sbu_558k_meta.json\r\n4. for both pretrain.sh and finetune.sh, --model_max_length 2048\r\nDo I use 4096 for Llama-2?\r\nThanks so much!\n</Comment>\n<Comment by chigkim at 2023-08-07T13:08:04Z>\nGoing to close  this and open a discussion.\n</Comment>\n<Comment by haotian-liu at 2023-08-07T16:50:30Z>\nSorry for the confusion. I will update the docs to make it clearer later this week.\r\n\r\nI am re-opening this, and please feel free to discuss anything in README, that is ambiguous or unclear to you :)\n</Comment>\n<Comment by harrytea at 2023-08-07T17:19:14Z>\n> Going to close this and open a discussion.\r\n\r\nI have the same question\n</Comment>\n<Comment by chigkim at 2023-08-07T18:14:00Z>\nThis is what I found so far. @haotian-liu Please correct me I'm wrong.\r\n\r\n## --deepspeed\r\nThere seems some deepspeed configuration in scripts folder.\r\n\r\n* zero2.json\r\n* zero3.json\r\n* zero3_offload.json\r\n\r\nI don't know what the differences are, but I spotted one of the issues specifying one of those files in --deepspeed flag.\r\n\r\n## --data_path\r\n* In pretrain.sh To pretrain new model: [LLaVA-Pretrain/blip_laion_cc_sbu_558k.json](https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain/resolve/main/blip_laion_cc_sbu_558k.json)\r\n* In pretrain.sh to pretrain old model: [LLaVA-CC3M-Pretrain-595K/chat.json](https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K/resolve/main/chat.json)\r\n* In finetune.sh to finetune new model: [LLaVA-Instruct-150K/llava_instruct_80k.json](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/resolve/main/llava_instruct_80k.json)\r\n* In finetune.sh to finetune old model: [LLaVA-Instruct-150K/llava_instruct_150k.json](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/resolve/main/llava_instruct_150k.json)\r\n\r\n## --image_folder\r\n* In pretrain.sh to train new model: [LLaVA-Pretrain/images.zip](https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain/resolve/main/images.zip)\r\n* In pretrain.sh to train old model: [LLaVA-CC3M-Pretrain-595K/images.zip](https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K/resolve/main/images.zip)\r\n* In finetune.sh to finetune both old and new: [train2017 from coco dataset](http://images.cocodataset.org/zips/train2017.zip)\r\n\r\n## --pretrain_mm_mlp_adapter\r\nEither you point to the file you got after pretrain, or get it below.\r\nChoose one of the models that you're going to finetune from here.\r\nhttps://huggingface.co/liuhaotian\r\nThen download mm_projector.bin after you click files.\r\nFor example\" [llava-336px-pretrain-llama-2-13b-chat/mm_projector.bin](https://huggingface.co/liuhaotian/llava-336px-pretrain-llama-2-13b-chat/resolve/main/mm_projector.bin)\r\n\r\n\r\n## --model_max_length\r\nI haven't figured out --model_max_length for Llama-2. Llama-2 ha 4096 context length, so probably you put 4096?\n</Comment>\n<Comment by sohaibsoussi at 2024-02-21T06:59:34Z>\nHi, I encountered a problem when running the shell script below for fine-tuning purposes. It always tells me that the 'lava' module is not found, even though I tried installing it using both conda and pip:\r\n\r\nThe shell script:\r\n#!/bin/bash\r\ndeepspeed /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/LLaVA/llava/train/train_mem.py\r\n--deepspeed /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/LLaVA/scripts/zero2.json\r\n--lora_enable True\r\n--lora_r 128\r\n--lora_alpha 256\r\n--mm_projector_lr 2e-5 \\\r\n--bits 4\r\n--model_name_or_path /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/llava-v1.5-7b\r\n--version llava_llama_2 \\\r\n--data_path /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/ok_vqa_dataset/train\r\n--validation_data_path /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/ok_vqa_dataset/validation\r\n--image_folder /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/ok_vqa_dataset/images\r\n--vision_tower openai/clip-vit-large-patch14-336\r\n--mm_projector_type mlp2x_gelu\r\n--mm_vision_select_layer -2\r\n--mm_use_im_start_end False\r\n--mm_use_im_patch_token False\r\n--image_aspect_ratio pad\r\n--group_by_modality_length True\r\n--bf16 True\r\n--output_dir /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/checkpoints/ok_vqa_finetuning\r\n--num_train_epochs 500\r\n--per_device_train_batch_size 2\r\n--per_device_eval_batch_size 2\r\n--gradient_accumulation_step 64\r\n--evaluation_strategy \"epoch\"\r\n--save_strategy \"steps\"\r\n--save_steps 50000\r\n--save_total_limit 1\r\n--learning_rate 2e-4\r\n--weight_decay 0.\r\n--warmup_ratio 0.03\r\n--lr_scheduler_type \"cosine\"\r\n--logging_steps 1\r\n--tf32 True\r\n--model_max_length 2048\r\n--gradient_checkpointing True\r\n--dataloader_num_workers 4\r\n--lazy_preprocess True\r\n--report_to wandb\r\n\r\nLog after runing the shell script:\r\ndeepspeed /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/LLaVA/llava/train/train_mem.py\r\n[2024-02-20 23:14:05,751] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-02-20 23:14:07,655] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\n[2024-02-20 23:14:07,711] [INFO] [runner.py:568:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/LLaVA/llava/train/train_mem.py\r\n[2024-02-20 23:14:10,006] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-02-20 23:14:10,383] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}\r\n[2024-02-20 23:14:10,383] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0\r\n[2024-02-20 23:14:10,383] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\r\n[2024-02-20 23:14:10,383] [INFO] [launch.py:163:main] dist_world_size=1\r\n[2024-02-20 23:14:10,383] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0\r\n[2024-02-20 23:14:10,384] [INFO] [launch.py:253:main] process 1377826 spawned with command: ['/usr/bin/python3', '-u', '/home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/LLaVA/llava/train/train_mem.py', '--local_rank=0']\r\nTraceback (most recent call last):\r\nFile \"/home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/LLaVA/llava/train/train_mem.py\", line 1, in\r\nfrom llava.train.train import train\r\nModuleNotFoundError: No module named 'llava'\r\n[2024-02-20 23:14:11,386] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 1377826\r\n[2024-02-20 23:14:11,386] [ERROR] [launch.py:322:sigkill_handler] ['/usr/bin/python3', '-u', '/home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/LLaVA/llava/train/train_mem.py', '--local_rank=0'] exits with return code = 1\r\n--deepspeed /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/LLaVA/scripts/zero2.json\r\ntrain_ok_vqa.sh: 4: --deepspeed: not found\r\n--lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5\r\ntrain_ok_vqa.sh: 5: --lora_enable: not found\r\n--bits 4 --model_name_or_path /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/llava-v1.5-7b --version llava_llama_2\r\ntrain_ok_vqa.sh: 9: --bits: not found\r\n--data_path /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/ok_vqa_dataset/train --validation_data_path /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/ok_vqa_dataset/validation --image_folder /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/ok_vqa_dataset/images --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /home/sohaib/LPRI_projects/Image-txt--txt/LLAVA/checkpoints/ok_vqa_finetuning\r\ntrain_ok_vqa.sh: 12: --data_path: not found\r\n--num_train_epochs 500 --per_device_train_batch_size 2 --per_device_eval_batch_size 2 --gradient_accumulation_step 64 --evaluation_strategy epoch --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb\r\ntrain_ok_vqa.sh: 24: --num_train_epochs: not found\n</Comment>\n<Comment by NanAlbert at 2024-04-18T07:29:53Z>\n> This is what I found so far. @haotian-liu Please correct me I'm wrong.\r\n> \r\n> ## --deepspeed\r\n> There seems some deepspeed configuration in scripts folder.\r\n> \r\n> * zero2.json\r\n> * zero3.json\r\n> * zero3_offload.json\r\n> \r\n> I don't know what the differences are, but I spotted one of the issues specifying one of those files in --deepspeed flag.\r\n> \r\n> ## --data_path\r\n> * In pretrain.sh To pretrain new model: [LLaVA-Pretrain/blip_laion_cc_sbu_558k.json](https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain/resolve/main/blip_laion_cc_sbu_558k.json)\r\n> * In pretrain.sh to pretrain old model: [LLaVA-CC3M-Pretrain-595K/chat.json](https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K/resolve/main/chat.json)\r\n> * In finetune.sh to finetune new model: [LLaVA-Instruct-150K/llava_instruct_80k.json](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/resolve/main/llava_instruct_80k.json)\r\n> * In finetune.sh to finetune old model: [LLaVA-Instruct-150K/llava_instruct_150k.json](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/resolve/main/llava_instruct_150k.json)\r\n> \r\n> ## --image_folder\r\n> * In pretrain.sh to train new model: [LLaVA-Pretrain/images.zip](https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain/resolve/main/images.zip)\r\n> * In pretrain.sh to train old model: [LLaVA-CC3M-Pretrain-595K/images.zip](https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K/resolve/main/images.zip)\r\n> * In finetune.sh to finetune both old and new: [train2017 from coco dataset](http://images.cocodataset.org/zips/train2017.zip)\r\n> \r\n> ## --pretrain_mm_mlp_adapter\r\n> Either you point to the file you got after pretrain, or get it below. Choose one of the models that you're going to finetune from here. https://huggingface.co/liuhaotian Then download mm_projector.bin after you click files. For example\" [llava-336px-pretrain-llama-2-13b-chat/mm_projector.bin](https://huggingface.co/liuhaotian/llava-336px-pretrain-llama-2-13b-chat/resolve/main/mm_projector.bin)\r\n> \r\n> ## --model_max_length\r\n> I haven't figured out --model_max_length for Llama-2. Llama-2 ha 4096 context length, so probably you put 4096?\r\n\r\nThank you for the informative summary. Also, I would like to ask why the LLaVA-CC3M-Pretrain-595K/images.zip is only 6GB and does not contain 595K images. Do you know the reason for this?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 341,
    "state": "open",
    "created_by": "PhanTask",
    "created_at": "2023-08-02T20:38:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/341</URL>\n\n<TITLE>Finetuning vision encoder part</TITLE>\n\n<BODY>### feature\n\nHi, I wonder in the current code if it is possible to finetune both vision encoder part and the projector? Thanks.</BODY>\n\n<COMMENTS>\n<Comment by PhanTask at 2023-08-02T20:46:58Z>\nI found in the loader file that we have `self.vision_tower.requires_grad_(False)`. Should I just comment this out?\n</Comment>\n<Comment by Lord-of-Bugs at 2024-02-21T10:29:17Z>\nI am also kind of stuck on how to properly fine-tune the CLIP vision encoder, or even subbing it out with something else. Are you still working on this task? Could you please share some updates?\n</Comment>\n<Comment by SFaegheh at 2025-05-03T21:07:07Z>\nI am wondering if you could solve this? since I want to fine tune both vision encoder and the projector?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 340,
    "state": "closed",
    "created_by": "gnimyang",
    "created_at": "2023-08-02T11:33:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/340</URL>\n\n<TITLE>[Usage] the poetry install cannot work in this project, is the pyproject.toml file erro?</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\npoetry install cannot work in this project, is the pyproject.toml file error?\r\n\r\n[tool.poetry] section not found in /public/home/mike/LLaVA/pyproject.toml</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-08-02T15:48:24Z>\nHi, what is your environment (OS, Python version), and if you followed our instructions [here](https://github.com/haotian-liu/LLaVA#install) to create the conda environment?\r\n\r\nI have installed the package from scratch (an empty conda env) yesterday, and it works for me. Thanks.\n</Comment>\n<Comment by gnimyang at 2023-08-04T07:46:43Z>\nthanks!\n</Comment>\n<Comment by haotian-liu at 2023-08-05T05:17:05Z>\nPlease feel free to reopen the issue if you encounter any other related issues. thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 339,
    "state": "open",
    "created_by": "jeoncharn",
    "created_at": "2023-08-02T07:20:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/339</URL>\n\n<TITLE>[Question] Can I expand user text input size?</TITLE>\n\n<BODY>### Question\n\nThank you for your great work!\r\n\r\nI'm using LLaVA model in Local, RTX 4090.\r\nand using MPT-7B, vicuna-13b-v1.3 with 4bit qunatization\r\n\r\nI heard that llama model accept maximum 2048 tokens for text input, but in LLaVA, I can only use 1200 length of characters for text input.\r\n\r\nIs there a way to expand this text input size?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-08-02T15:50:19Z>\nHi @jeoncharn \r\n\r\nI believe you can play around with the parameter [here](https://github.com/haotian-liu/LLaVA/blob/main/llava/serve/gradio_web_server.py#L152). We set this limit for reducing the error rate of our model demo (e.g. OOM, token overflow, etc.)\n</Comment>\n<Comment by jeoncharn at 2023-08-03T00:27:51Z>\n@haotian-liu Got it! Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 338,
    "state": "open",
    "created_by": "sameeravithana",
    "created_at": "2023-08-02T03:51:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/338</URL>\n\n<TITLE>[Usage] CLI Error</TITLE>\n\n<BODY>### When did you clone our code?\r\n\r\nI cloned the code base after 5/1/23\r\n\r\n### Describe the issue\r\n\r\nIssue:\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.cli \\\r\n    --model-path liuhaotian/LLaVA-Lightning-MPT-7B-preview \\\r\n    --image-file \"X.png\" \r\n```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"..lib/python3.9/runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"..lib/python3.9/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"llava/serve/cli.py\", line 119, in <module>\r\n    main(args)\r\n  File \"llava/serve/cli.py\", line 89, in main\r\n    output_ids = model.generate(\r\n  File \"..lib/python3.9/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"..lib/python3.9/site-packages/transformers/generation/utils.py\", line 1588, in generate\r\n    return self.sample(\r\n  File \"..lib/python3.9/site-packages/transformers/generation/utils.py\", line 2642, in sample\r\n    outputs = self(\r\n  File \"..lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"..lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"LLaVA/llava/model/language_model/llava_mpt.py\", line 78, in forward\r\n    logits = F.linear(outputs.last_hidden_state, self.transformer.wte.weight)\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:6 and cuda:0! (when checking argument for argument mat2 in method wrapper_mm)\r\n```</BODY>\n\n<COMMENTS>\n<Comment by sameeravithana at 2023-08-02T05:01:00Z>\nNo issue when used, ```--model-path liuhaotian/llava-llama-2-13b-chat-lightning-preview```\n</Comment>\n<Comment by haotian-liu at 2023-08-02T07:11:20Z>\nThanks for bringing this up. It seems that there is an issue with MPT models with multiple GPU inference. Just added a temporary fix in https://github.com/haotian-liu/LLaVA/commit/fc1006268f312c051bf51d2c1c23280b1b4f2f7d.\r\n\r\nPlease let me know if this fixes the MPT inference issue on your side. Thanks.\n</Comment>\n<Comment by Lizw14 at 2023-08-03T17:44:47Z>\nFollow up on this - I met the similar issues with MPT. \r\n\r\nNow the same device issue is solved, but another issue happens: I finetuned MPT-base LLaVA model with LoRA. But during inference, it seems that current `load_pretrained_model` in [builder.py](https://github.com/haotian-liu/LLaVA/blob/main/llava/model/builder.py) does not support loading MPT with LoRA.\r\n\r\nWould be great if this could be looked into. Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 337,
    "state": "closed",
    "created_by": "Espere-1119-Song",
    "created_at": "2023-08-01T14:55:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/337</URL>\n\n<TITLE>[Usage] load the checkpoint of llava-llama-2</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base before 5/1/23, but have pulled the latest code base\n\n### Describe the issue\n\nIssue: When I load the checkpoint of llava-llama-2 in model zoo, I meet the error as follows:\r\n\r\n\r\nLog: \r\n```\r\nYou are using a model of type llava to instantiate a model of type llava_mpt. This is not supported for all configurations of models and can yield errors.\r\n*** OSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory ckpt/llava.\r\n\r\n```\r\n![屏幕截图 2023-08-01 225311](https://github.com/haotian-liu/LLaVA/assets/77435739/9af72a86-6fe6-4bb0-a489-a7dff7df0d5e)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-08-02T05:04:47Z>\nHi, it is strange that it goes to the MPT. Can you provide the full command you used? Thanks.\n</Comment>\n<Comment by gnimyang at 2023-08-03T12:21:56Z>\nwhen I using follow command, I meet a problem.\r\nfrom transformers import pipeline\r\npipe = pipeline(\"text-generation\", model=\"liuhaotian/llava-v1-0719-336px-lora-vicuna-13b-v1.3\")\r\n![image](https://github.com/haotian-liu/LLaVA/assets/130719420/dbdf435f-7c49-4499-b780-55b4b03d6c16)\n</Comment>\n<Comment by gnimyang at 2023-08-03T12:36:44Z>\nI am a new starter, is there any other ways to download the pre-trained model？\r\nIs it possible to check the model has already been downloaded correctly to the local manually, where to change the parameter to load the model locally? Can you show me the folder structure?\n</Comment>\n<Comment by haotian-liu at 2023-08-05T05:33:47Z>\nHi, for CLI inference, please follow our instructions [here](https://github.com/haotian-liu/LLaVA#cli-inference).\n</Comment>\n<Comment by Anzhi-0628 at 2023-10-19T00:06:44Z>\nI faced the same issue when I was using the local parameters (cloned from huggingface). And I am pretty sure that the path consists of the config file, the model bin and everything.\r\n<img width=\"1210\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/55432916/c8892500-84c5-4365-8792-7ceaabdc0ffc\">\n</Comment>\n<Comment by haotian-liu at 2023-10-19T00:20:05Z>\nMaybe check if your folder contains the proper index.json file? It should *NOT* contain `pytorch_model.bin` actually as it is sharded.\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/6631389/42eab07a-1eb2-4545-b988-8e26598f4097)\n</Comment>\n<Comment by Anzhi-0628 at 2023-10-19T00:28:07Z>\nOops! Thanks for this. My bad. It should work now.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 335,
    "state": "closed",
    "created_by": "CupidJay",
    "created_at": "2023-08-01T10:04:29Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/335</URL>\n\n<TITLE>RecursionError when converting weights</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nI installed the environment following your instructions and I downloaded the llama weights from https://huggingface.co/decapoda-research/llama-13b-hf. However, I face the recursion error when I convert the weights for ScienceQA.\r\n\r\nConversion Commad\r\n```\r\npython -m llava.model.apply_delta \\\r\n   --base /share/datasets/llama-13b-hf \\\r\n    --target /share/datasets/LLaVA-13b-v0-science_qa \\\r\n    --delta liuhaotian/LLaVA-13b-delta-v0-science_qa \r\n```\r\n\r\nError Message\r\n```\r\n File \"/root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\nRecursionError: maximum recursion depth exceeded\r\n```\r\n\r\nThis error can disappear if I install transformers==4.28.1. But I'm not sure if this is the right solution, because I'm getting around 40% accuracy after that. Can you tell me if there is something wrong with my model weights or something wrong with the environment?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-08-02T05:10:32Z>\nHi, I heard that there are some issues with this repo you are using. You may try using this repo: https://huggingface.co/huggyllama/llama-7b.\r\n\r\nThis contains the weights for the LLaMA-7b model. This model is under a non-commercial license (see the LICENSE file). You should only use this repository if you have been granted access to the model by filling out [this form](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform?usp=send_form) but either lost your copy of the weights or got some trouble converting them to the Transformers format.\n</Comment>\n<Comment by CupidJay at 2023-08-02T06:38:24Z>\nThanks for your reply! It seems that this error appears when loading the tokenizer from the delta_path.\r\n```\r\ndelta_tokenizer = AutoTokenizer.from_pretrained(delta_path)\r\n```\r\nActually, I don't know why this happens.\n</Comment>\n<Comment by haotian-liu at 2023-08-02T06:41:43Z>\nCan you try adding `use_fast=False`?\r\n\r\n```\r\ndelta_tokenizer = AutoTokenizer.from_pretrained(delta_path, use_fast=False)\r\n```\n</Comment>\n<Comment by CupidJay at 2023-08-02T06:49:21Z>\nThanks! This solves the problem when converting weights for me. I will then continue to reproduce your results on ScienceQA. Thanks for your work again.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 334,
    "state": "open",
    "created_by": "yifannnwu",
    "created_at": "2023-07-31T23:16:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/334</URL>\n\n<TITLE>[Feature Request] Has anyone tried multi nodes?</TITLE>\n\n<BODY>### Feature Request\r\n\r\nHas anyone tried multi nodes and got deepspeed work successfully? Appreciate for instructions in advance!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 333,
    "state": "closed",
    "created_by": "shileims",
    "created_at": "2023-07-31T05:46:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/333</URL>\n\n<TITLE>[Feature request] Input bounding box and output bounding box for image understanding</TITLE>\n\n<BODY>### feature\n\nHi Author,\r\n     Since this is a really good work for image understanding based on LLM, I would like to ask do you have plan to extend the functionality of image understanding for inputting bounding box and outputting bounding box. Thanks</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-24T01:58:48Z>\nHi, this is partially supported in the latest llava-1.5. Please check out the data mixture here: https://github.com/haotian-liu/LLaVA#visual-instruction-tuning, or for our paper for more info. Thank you.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 332,
    "state": "closed",
    "created_by": "jzhang38",
    "created_at": "2023-07-31T05:18:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/332</URL>\n\n<TITLE>Bug in llama_flash_attn_monkey_patch.py when using llama-2?</TITLE>\n\n<BODY>### Question\n\nLlama-2 use grouped query attention. However, [code here](https://github.com/haotian-liu/LLaVA/blob/817a4af4e7323dd392b9bcc723cf5844c1272896/llava/train/llama_flash_attn_monkey_patch.py#L19) modifies the llama attention with flash attention without considering the grouped query attention. \r\nReference: https://github.com/huggingface/transformers/blob/e42587f596181396e1c4b63660abf0c736b10dae/src/transformers/models/llama/modeling_llama.py#L327</BODY>\n\n<COMMENTS>\n<Comment by jzhang38 at 2023-07-31T05:32:33Z>\nOK, I figured it out.\r\n\"Bigger models — 34B and 70B — use Grouped-Query Attention (GQA) for improved inference scalability.\" Table 1, Llama-2 paper. \r\n\r\nSo 7B and 13B models do not use GQA.\r\n\r\nI will close this issue.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 331,
    "state": "closed",
    "created_by": "shileims",
    "created_at": "2023-07-31T01:57:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/331</URL>\n\n<TITLE>[Feature request] Distributed inference</TITLE>\n\n<BODY>### feature\n\nHi Author,\r\n    Thanks for your excellent work.\r\n    I would like to perform distributed inference in one machine with multiple gpu cards. Is it possible?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-31T04:53:19Z>\nHi, you may check out the batch inferencing docs in ScienceQA: https://github.com/haotian-liu/LLaVA/blob/main/docs/ScienceQA.md\r\n\r\nWe may support the batch inference later, but currently we have a lot other TODOs that need us take care of..\n</Comment>\n<Comment by shileims at 2023-07-31T05:44:24Z>\nHi @haotian-liu ,\r\n    Thanks for your reply. Yes, definitely your plan for future work is more valuable. I will check the ScienceQA part.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 330,
    "state": "closed",
    "created_by": "shileims",
    "created_at": "2023-07-31T01:56:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/330</URL>\n\n<TITLE>The same input prompt, different runs get different results?</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nHi author,\r\n    Thanks for your great work.\r\n    When I input the same prompt to the model multiple times, different runs get different results? I would like to get the reason. Thank you!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-31T04:50:20Z>\nHi, this is because of the *sampling* mechanism of the text generation process. After calculating the probability distribution of the next token, the default behavior currently is to sample from this probability distribution, and the higher the probability is, the more likely that the model will output that token.\r\n\r\nYou can choose to set the deterministic sampling by setting `do_sample=False` in `model.generate`. \r\n\r\nThis is an article that you can refer to for a better understanding of the generation process: https://huggingface.co/blog/how-to-generate.\n</Comment>\n<Comment by shileims at 2023-07-31T05:14:27Z>\nhi @haotian-liu ,\r\n    Really appreciate your reply. Will check the article. Thank you so much!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 329,
    "state": "open",
    "created_by": "yuzhou914",
    "created_at": "2023-07-30T17:14:02Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/329</URL>\n\n<TITLE>Multi-GPUs training error!! RuntimeError: Expected to mark a variable ready only once.</TITLE>\n\n<BODY>### Question\r\n\r\nHi! I am using the script of finetune_lora.sh in order to fine-tune LLAVA with lora-llama, and it works well on single GPU. However, when I am using multiple GPUs (8 GPUs), I face this issue and I cannot solve.\r\n\r\nRuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations. Parameter at index 445 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine hooks have fired for this particular parameter during this iteration.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-30T17:15:55Z>\nHi, can you provide the scripts that you use? Are you using the latest [`finetune_lora.sh`](https://github.com/haotian-liu/LLaVA/blob/main/scripts/finetune_lora.sh)? Thanks.\n</Comment>\n<Comment by yuzhou914 at 2023-07-30T18:46:37Z>\nYes, this is my script.\r\n\r\ntorchrun --nproc_per_node=8 --master_port=20001 llava/train/train_mem_LLaVALoRA.py \\\r\n    --lora_enable True \\\r\n    --model_name_or_path /vicuna-7b-v1 \\\r\n    --version v1 \\\r\n    --data_path /llava_instruct_150k.json \\\r\n    --image_folder /coco/train2017 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./llava/train/mm_projector.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./LLaVA_finetune_lora_1 \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --dataloader_num_workers 4 \\\r\n    --report_to wandb\n</Comment>\n<Comment by yuzhou914 at 2023-07-30T18:47:17Z>\nAnd I am using the latest finetune_lora.sh. I meet this problem.\n</Comment>\n<Comment by haotian-liu at 2023-07-30T18:48:38Z>\n@LANYAN1212 \r\n\r\nIt seems that you are not using DeepSpeed and using torchrun instead. LoRA is now only tested with DeepSpeed as PyTorch FSDP is known to have issues with PEFT. Can you try with the Deepspeed command in [finetune_lora.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/finetune_lora.sh)? Thanks.\n</Comment>\n<Comment by yuzhou914 at 2023-07-30T18:59:17Z>\nYes! After I use DeepSpeed, the error is fixed. Thank you very much.\r\nBy the way, I would like to ask that when I train the model by HF-trainer, since DeepSpeed would automatically convert all data and model into torch.bfloat16 in training loop, I wonder how should I do if I would like to keep data and training parameters on torch.float32 in order to have better training results. Many thanks if you could tell me.\n</Comment>\n<Comment by haotian-liu at 2023-07-31T04:51:44Z>\nI think you can disable the options of bf16/fp16 both in command line (`--bf16`) and in deepspeed config. But I have not fully verified this training as it shall double the memory consumption.\n</Comment>\n<Comment by yuzhou914 at 2023-08-01T02:49:33Z>\nThank you very much for your reply. I would like to have a try.\n</Comment>\n<Comment by yuzhou914 at 2023-08-01T06:30:25Z>\nMaybe I would like to ask the thing that might not so related to LLaVA, when I use torchrun --nproc_per_node=8 to start the code, then I will meet 'Parameter xxx with name lora_A/lora_B.default.weight has been marked as ready twice', I wonder if I use accelerate to train the model, will I meet that problem or not. Thank you very much for that attention.\n</Comment>\n<Comment by haotian-liu at 2023-08-01T06:34:54Z>\nHi, @LANYAN1212 \r\n\r\nPytorch FSDP is not compatible with PEFT yet. So please stick to DeepSpeed if you want to use LoRA/QLoRA and follow the official sample scripts for the setup. If there is any reason that prevents you from using DeepSpeed, please let me know, thanks.\n</Comment>\n<Comment by yuzhou914 at 2023-08-01T07:31:23Z>\n1. When I am training LLaVA with LLaMA+LoRA, I use DeepSpeed with zero3/zero3_offload.json and I add --bf16 True, then I can train LLaVA successfully, and in the training loop(forward), all model and data will convert into torch.bfloat16. Then, I just remove bf16 in both zero3/zero3_offload.json and still keep --bf16 True for DeepSpeed training, then in the training loop, the model will become mixed precision (LLaMA bf16, vision_tower fp16, mm_projector fp32), and the data images become fp32, then the training goes wrong. I wonder which command dominate the data type (--bf16 True for training command or bf16 enable=True in json), and if LLaVA can conduct mixed precision training.\r\n2. I would like to combine LLM+LoRA with stable diffusion, and I would like to finetune unet. Therefore, I would like to conduct the mixed precision training, and I need to put LLM+LoRA/vae on bf16, and unet on fp32, how should I do for both training script(--bf16) and zero3/zero3_offload.json files.\r\nI am very thankful for your attention.\n</Comment>\n<Comment by haotian-liu at 2023-08-02T05:16:10Z>\n1. I would suggest keep deepspeed config and the command line consistent, as the behavior of mismatched config is not guaranteed.\r\n2. I haven't done this myself, but one may try casting the model dtype within the code (e.g., `llm.to(dtype=torch.bfloat16)`), and keep the command line and deepspeed using fp32.\n</Comment>\n<Comment by yuzhou914 at 2023-08-02T08:41:37Z>\nThank you very much for your kind suggestion. According to your suggestion, I would like to combine LLM+LoRA with stable diffusion without mixed precision first, which means that I will keep bf16 enabled=true in zero3_offload.json. Thus, all model and data will be put on torch.bfloat16 in forward training loop. However, when I use follow this setting I mentioned above, I always meet 'RuntimeError: still have inflight params', even though the forward training loop can go straight successfully, the error will be met on deepspeed engine optimizer loss backward, and it is hard to solve since there are no suitable solution online. I feel very confused now, since for LLaVA keep bf16 enabled=true in zero3_offload.json could be okay, but I am stuck here. Would you have some experiences or advise on it?\r\nThank you for your attention again.\n</Comment>\n<Comment by haotian-liu at 2023-08-02T15:46:40Z>\nI would suggest try figuring these configuration out using a small LLM (e.g. OpenLLaMA-3B), and other components that you can find which is as small as possible. First try to figure the training out in zero2, then to zero3. Zero3 according to my experience can have more compatibility issue to tackle with than zero2. So this process may help you better isolate the issue.\n</Comment>\n<Comment by yuzhou914 at 2023-08-03T09:10:51Z>\nGreat! After I switch the zero3_offload to zero2_offload , and I could conduct the training successfully. Thank you very much for your kind suggestion. However, when I conduct inference, I face another problem: 'RuntimeError: GET was unable to find an engine to execute this computation'. I feel very confused. I just save the model in \".bin\" type, and I am using torch to load the model. But no matter I use python or deepspeed to run the code, I will always face this issue. Would you have some experiences or advise on it?\n</Comment>\n<Comment by BlueBlueFF at 2024-04-07T08:35:14Z>\n> Thank you very much for your kind suggestion. According to your suggestion, I would like to combine LLM+LoRA with stable diffusion without mixed precision first, which means that I will keep bf16 enabled=true in zero3_offload.json. Thus, all model and data will be put on torch.bfloat16 in forward training loop. However, when I use follow this setting I mentioned above, I always meet 'RuntimeError: still have inflight params', even though the forward training loop can go straight successfully, the error will be met on deepspeed engine optimizer loss backward, and it is hard to solve since there are no suitable solution online. I feel very confused now, since for LLaVA keep bf16 enabled=true in zero3_offload.json could be okay, but I am stuck here. Would you have some experiences or advise on it? Thank you for your attention again.\r\n\r\nSame Question.Do you fix it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 328,
    "state": "open",
    "created_by": "xiaobaishu0097",
    "created_at": "2023-07-30T14:32:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/328</URL>\n\n<TITLE>[Question] About debugging on 3090 and the pretrained adapter in finetuning script</TITLE>\n\n<BODY>### Question\n\nFirstly, I want to appreciate the tremendous work you have put into this project. It has been quite insightful and enlightening for me.\r\n\r\nI am having a bit of trouble regarding fine-tuning pre-trained models and I'm hoping you can provide some assistance.\r\n\r\n1. While perusing your code, I noticed an argument named pretrain_mm_mlp_adapter in the [finetune.sh](https://github.com/haotian-liu/LLaVA/blob/817a4af4e7323dd392b9bcc723cf5844c1272896/scripts/finetune.sh#L22) script. However, I could not find the corresponding location in the codebase that details how to generate this adapter. Could you please provide some guidance or refer me to the relevant section where I can understand more about this pre-trained adapter?\r\n2. I'm considering debugging the fine-tuning code on my personal workstation, which has 3 RTX3090 GPUs and 128GB of CPU memory. For this, I would like to clarify that I'm not specifically looking to achieve similar performance as your setup. I am primarily interested in ensuring the code can run on my workstation. Can you confirm if this setup would be adequate for running and debugging the fine-tuning code?\r\n\t\r\nAny assistance you can provide would be greatly appreciated. Thank you again for your time and dedication to this project!</BODY>\n\n<COMMENTS>\n<Comment by xiaobaishu0097 at 2023-08-02T07:35:10Z>\nHi @haotian-liu , I just noticed that the scripts/extract_mm_projector.py was removed at commit eae9369. Would you mind provide us with some suggestions to extract the mm_projector from pretrained model? Or can we just use the removed script?\n</Comment>\n<Comment by haotian-liu at 2023-08-02T16:48:15Z>\nHi @xiaobaishu0097\r\n\r\nIn our latest code base (`llava>=1.0.0`), you do not need to *extract* the projectors anymore, as we now only save the projector during the pretraining stage, and do not save frozen LLM weights during the pretraining stage (which was a waste of the disk space).\r\n\r\nIf you are interested in using our pretrained projectors, you can check out our [model zoo](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#projector-weights).\r\n\r\nRegarding debugging on RTX 3090. You can use `--tune_mm_mlp_adapter` or `--freeze_backbone` to save memory. I just tried that this works for tuning vicuna-7b v1.3 on 2x 3090s.\r\n\r\nThanks.\n</Comment>\n<Comment by xiaobaishu0097 at 2023-09-06T03:44:44Z>\nHi @haotian-liu \r\n\r\nThanks for all your help!\r\n\r\nI am trying to fine-tune the 13B vicuna on 2 A100-80G. However, I noticed that the GPU memory consumption keeps increasing during training with Lora. I just want to know, is that a normal situation? And what is the normal batch size when we fine-tune the model on A100-80G?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 327,
    "state": "closed",
    "created_by": "SinanAkkoyun",
    "created_at": "2023-07-29T11:51:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/327</URL>\n\n<TITLE>[Question] Encoding speed enhanced</TITLE>\n\n<BODY>### Question\n\nHi! Thank you for all the groundbreaking work and for sharing it.\r\n\r\nWhat are current bottlenecks of encoding an image? I am working on a real-time LlaVA and need to cut down encoding speed to as low as possible\r\n\r\nI would be very grateful for a high level assession of bottlenecks and what could be improved</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-29T18:05:36Z>\n@SinanAkkoyun \r\n\r\nThanks for your interest and for bringing up this interesting topic. To my understanding, LLaVA can start outputing tokens very soon after receiving the image, and the inference time for image encoding should be minimal, while the text decoding will take a long time.\r\n\r\nCan you elaborate a bit more on why you feel that the image encoding may be the bottleneck? And how long do you expect the decoding sequence be?\r\n\r\nThanks.\n</Comment>\n<Comment by SinanAkkoyun at 2023-07-29T19:04:36Z>\n> Thanks for your interest and for bringing up this interesting topic. To my understanding, LLaVA can start outputing tokens very soon after receiving the image, and the inference time for image encoding should be minimal, while the text decoding will take a long time.\r\n\r\nAh okay, when I experimented with it when it came out, the CLIP image encoding took around a second\r\n\r\n> Can you elaborate a bit more on why you feel that the image encoding may be the bottleneck?\r\n\r\nIT was just like that when I tested it, token generation of the 7B and 13B models are very fast (I will finetune a 3B model for even faster inference)\r\n\r\n> And how long do you expect the decoding sequence be?\r\n\r\nThe decoding sequence will be a couple of tokens only, at max 10 tokens\r\n\r\nThank you very much for your help and interest in my problem!\n</Comment>\n<Comment by haotian-liu at 2023-07-30T00:08:07Z>\nI see. For this case, maybe it will be a good idea to try to optimize the vision encoder as well.\r\n\r\nSeveral possibilities includes:\r\n\r\n(1) quantize CLIP, but I am not very familiar with the literature there;\r\n(2) use a smaller CLIP backbone. We support train with different vision backbones, and if you want to try with different CLIP backbones, it is as simple as change `--vision_tower` in the command line to the correct one (e.g. CLIP ViT-B/32: [openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32). Note that there may be performance degradation when you switch to a smaller backbone. It should also support OpenCLIP checkpoints as well. You may also port your own checkpoints following [this script](https://github.com/haotian-liu/LLaVA/blob/main/llava/model/multimodal_encoder/clip_encoder.py).\r\n\r\nIn any case, I would suggest benchmarking the models yourself, and make sure that we are dealing with the real bottleneck. Thanks!\n</Comment>\n<Comment by SinanAkkoyun at 2023-07-30T04:45:52Z>\nThank you very much! I will definitely look into your suggested approaches.\r\nI have a question not regarding inference but training of the encoder, I want to finetune a test CLIP model on ingame rendered footage. I am aware of the contrastive text training nature of CLIP but I wanted to ask is there a way to enhance spatial 3d understanding of the images in some way? That is, only regarding the encoder, can it develop feature similarities like text embeddings do on their own unsupervised, without relying that much on text pairs (because describing spatial awareness is almost impossible with text labels only)\r\n\r\nThank you very much\n</Comment>\n<Comment by haotian-liu at 2023-07-31T05:18:09Z>\nHi @SinanAkkoyun \r\n\r\nI do not have direct experiments on that, but my feeling is that a few instruction tuning on this may help a lot on the performance of this specific task. You can try creating some simple instruction tuning dataset based on existing ones (it may not even need to be some output from ChatGPT), and we can get a sense of how it performs before and after finetuning on these data.\n</Comment>\n<Comment by SinanAkkoyun at 2023-07-31T21:08:31Z>\nHi @haotian-liu \r\nThank you very much for the answer! I will definitely try to synthesize a 3D spatial finetuning dataset (similar to the microsoft bounding box paper but with actual 3D data from renderings)\r\n\r\nI also wanted to take the opportunity to congratulate you on all your work you have done, also the MedLLaVa, such great results!\r\n\r\nIs there a way to finetune the CLIP encoder itself? Given the text contrast nature I am not so sure if there even will be enough spatial information encoded\r\n\r\nI am looking very forward to your response, thank you\n</Comment>\n<Comment by haotian-liu at 2023-07-31T23:11:07Z>\nI have a similar intuition here. But if the data is clean enough, there is still some possibility that it can be learnt. Anyway, maybe the easiest would be to let the LLM learn instead of hoping that the spatial relationship will be encoded within the CLIP encoder?\n</Comment>\n<Comment by SinanAkkoyun at 2023-07-31T23:23:32Z>\nOk, I will build the dataset and test for it, thank you for your intuition and inputs, that means a lot to me!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 326,
    "state": "closed",
    "created_by": "duchenzhuang",
    "created_at": "2023-07-29T08:53:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/326</URL>\n\n<TITLE>[Question] Some Questions about the datasets used and the model.</TITLE>\n\n<BODY>### Question\n\n1, In my understanding, the first pretraining stage uses either the CC-3M Concept-balanced 595K dataset or the LAION/CC/SBU BLIP-Caption Concept-balanced 558K dataset. The second stage uses the coco-2017-train and llava_instruct_150k.json datasets. Is that correct?\r\n\r\n2, Could you advise which dataset would be better to use for the first stage: CC-3M Concept-balanced 595K or LAION/CC/SBU BLIP-Caption Concept-balanced 558K? \r\n\r\n3, Also, what's the difference between llava_instruct_150k.json and llava_instruct_70k.json in the second stage? It seems that the sizes of these two files are the same.\r\n\r\n4, Furthermore, in the model_zoo, why is LLaMA-2-13B-Chat noticeably worse than Vicuna-13B-v1.3? What do you think might have caused this?\r\n\r\n5, My last question, what are the files conversation_58k.json, detail_23k.json, and complex_reasoning_77k.json used for? It seems that neither the first nor the second stage of training requires them?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-29T18:10:57Z>\nHi, thank you for your interest in our work, and these are great questions.\r\n\r\n1. Correct.\r\n2. In our initial release in the paper, we use CC-595K. In our later experiments (especially Lightning checkpoints), we start to use LCS-558K. Although both are capable of aligning and recognizing the concepts beyond training, we empirically verify on [LLaVA Bench](https://github.com/haotian-liu/LLaVA/blob/main/docs/LLaVA_Bench.md) that LCS-558K is slightly better.\r\n3. 150K is used for our paper, 80K is used for lightning checkpoints for a faster convergence. You can check out the documentation [here](https://github.com/haotian-liu/LLaVA#lightning).\r\n4. The main reason is the image resolution, as you can see vicuna-13b-v1.3 is using 336px, while llama-2-13b-chat is currently using 224px. We'll release both resolutions for both base LLMs very soon. You can expect the performance at the same resolution being similar, while having some behavioral differences.\r\n5. They are the three types of instructions for our LLaVA-instruct-158K (add these numbers together you'll get 158K). We release all three subsets for a complete transparency and people can create their own subsets according to specific needs. You can refer to our paper for more details as well.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 325,
    "state": "closed",
    "created_by": "SteveTaoTao",
    "created_at": "2023-07-29T06:55:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/325</URL>\n\n<TITLE>@FuxiaoLiu</TITLE>\n\n<BODY></BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 324,
    "state": "closed",
    "created_by": "shileims",
    "created_at": "2023-07-29T04:37:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/324</URL>\n\n<TITLE>[Feature request] Multiple runs for CLI inference example</TITLE>\n\n<BODY>### feature\n\nMultiple runs are really helpful in understanding how the model works.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-29T04:38:31Z>\nHi, thanks for your interest.\r\nCan you elaborate the request more? Do you mean output three different answers for the same request?\r\nThanks.\n</Comment>\n<Comment by shileims at 2023-07-29T21:15:28Z>\nHi @haotian-liu ,\r\n    Thanks for your reply.\r\n   Inputting multiple questions to get final answers.\r\n   For example, I have two questions. (1). What does the image include? (2). Can you tell me what is the color of the object in the image? I hope to input the first question into the model to get the first answer. And then input the second question to get the final answer. Through the web demo you provided, I tried and designed multiple steps prompts to get what I want. But the code is difficult to extract. In other words, Would you provide a Python script for the web demo to input multiple questions. Not a single run demo.\n</Comment>\n<Comment by haotian-liu at 2023-07-29T23:57:29Z>\nHi @shileims, I get your point now. We have supported CLI inference with multiple turn conversations, but it seems that I have never officially document it yet.\r\n\r\nJust updated that in [readme](https://github.com/haotian-liu/LLaVA#cli-inference), and please try it out, and also feel free to follow up if there are issues with the current cli interface. Thanks.\n</Comment>\n<Comment by shileims at 2023-07-30T07:01:48Z>\nHi @haotian-liu , really appreciate your reply. I am trying it now. Thank you so much!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 323,
    "state": "closed",
    "created_by": "SteveTaoTao",
    "created_at": "2023-07-28T13:49:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/323</URL>\n\n<TITLE>Visual Instruction Tuning  OOM   with A100 80G</TITLE>\n\n<BODY></BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 322,
    "state": "open",
    "created_by": "FuxiaoLiu",
    "created_at": "2023-07-28T04:58:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/322</URL>\n\n<TITLE>[Question] 'list' object is not callable</TITLE>\n\n<BODY>### Question\r\n\r\nMay I ask whether anyone meet this error when fine-tune llava7b:\r\n\r\nTypeError: 'list' object is not callable.\r\n\r\nThis is from /path/to/llava/model/llava_arch.py, line 82.\r\n\r\nimage_features = self.get_model().vision_tower(images).</BODY>\n\n<COMMENTS>\n<Comment by dydxdt at 2023-07-28T06:24:53Z>\nself.get_model().vision_tower()(images). works\n</Comment>\n<Comment by haotian-liu at 2023-07-29T03:36:49Z>\n@FuxiaoLiu \r\n\r\nThis should have been fixed by PR #299, which was already merged last week. Can you please try the latest code base? Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 321,
    "state": "closed",
    "created_by": "richgong",
    "created_at": "2023-07-28T03:42:59Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/321</URL>\n\n<TITLE>[Usage] Unrecognized argument error: --num-gpus</TITLE>\n\n<BODY>### When did you clone our code?\r\n\r\nI cloned the code base after 5/1/23\r\n\r\n### Describe the issue\r\n\r\nIssue:\r\n\r\nUnrecognized argument error: --num-gpus\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-llama-2-13b-chat-lightning-preview --num-gpus 4\r\n```\r\n\r\nLog: \r\n```\r\n[2023-07-28 03:39:49,134] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n2023-07-28 03:39:49 | ERROR | stderr | usage: model_worker.py [-h] [--host HOST] [--port PORT] [--worker-address WORKER_ADDRESS] [--controller-address CONTROLLER_ADDRESS]\r\n2023-07-28 03:39:49 | ERROR | stderr |                        [--model-path MODEL_PATH] [--model-base MODEL_BASE] [--model-name MODEL_NAME] [--multi-modal]\r\n2023-07-28 03:39:49 | ERROR | stderr |                        [--limit-model-concurrency LIMIT_MODEL_CONCURRENCY] [--stream-interval STREAM_INTERVAL] [--no-register] [--load-8bit]\r\n2023-07-28 03:39:49 | ERROR | stderr |                        [--load-4bit]\r\n2023-07-28 03:39:49 | ERROR | stderr | model_worker.py: error: unrecognized arguments: --num-gpus 4\r\n```\r\n\r\nScreenshots:\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/3210091/7da08543-4de7-4378-bc5f-6c748d02274c)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-29T03:40:26Z>\nHi, sorry for the confusion. Our latest code base automatically uses all GPUs available and do not need to specify `--num-gpus` any more. I've updated the documentation [here](https://github.com/haotian-liu/LLaVA#launch-a-model-worker-multiple-gpus-when-gpu-vram--24gb).\r\n\r\nPlease let me know if there are any other issues, thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 320,
    "state": "open",
    "created_by": "morpheuslord",
    "created_at": "2023-07-27T16:20:21Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/320</URL>\n\n<TITLE>REST API [Feature request]</TITLE>\n\n<BODY>### REST API\r\n\r\nI thought an API-like access to make this a more expandable code would be a great feature</BODY>\n\n<COMMENTS>\n<Comment by barshag at 2023-10-10T22:12:46Z>\nany updates on that?\n</Comment>\n<Comment by morpheuslord at 2023-10-12T12:55:50Z>\n> any updates on that?\r\n\r\nnope\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 319,
    "state": "closed",
    "created_by": "Floodnut",
    "created_at": "2023-07-27T09:20:28Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/319</URL>\n\n<TITLE>[Feature request] Git auto release with CD</TITLE>\n\n<BODY>### feature\r\n\r\nI want git pipeline about release automation.\r\nCan I add workflows about that? I want to contribute to this project.\r\n\r\nOr, is there a reason not to automate?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-30T01:57:48Z>\nHi @Floodnut \r\n\r\nThank you for bringing this up and for offering this. I am not very familiar with this. Can you please explain what is the usual process of Git auto release using CD and what kind of benefits it can bring? How would it affect our current workflow? Thanks!\n</Comment>\n<Comment by Floodnut at 2023-07-31T03:09:39Z>\n> Hi @Floodnut\r\n> \r\n> Thank you for bringing this up and for offering this. I am not very familiar with this. Can you please explain what is the usual process of Git auto release using CD and what kind of benefits it can bring? How would it affect our current workflow? Thanks!\r\n\r\nThank you for your reply about this suggestion @haotian-liu !\r\n\r\nGithub has git release and tag system like this.\r\nyou can manage your project versions explicitly.\r\n<img width=\"466\" alt=\"example_of_docker_cli\" src=\"https://github.com/haotian-liu/LLaVA/assets/15941204/872326ac-cf1f-473c-91dd-194631d2e384\">\r\n\r\nIf you need to release and you feel hassle that create release manually, you can automate to create release.\r\nIt is simple system.\r\n- CD workflow catch `merge` event and extract version tag from commit message like this. (v1.0.0)\r\n- Then, make a release with extracted version tag.\r\n\r\nHowever, if you need a procedure  to check manually with minimal safety guards to create a release, this may not be necessary.\r\n\r\nLet me know if my explanation may have been insufficient and if you need additional information.\n</Comment>\n<Comment by haotian-liu at 2023-07-31T04:54:39Z>\nThanks for the info! The explanation makes a lot of sense. For now, I feel maybe manual tagging and release works better for me, as I think it will be beneficial to document each changes both in CHANGELOG as well as the release message, for people to be clear about each release. Do you have any other opinions?  Thanks!\n</Comment>\n<Comment by Floodnut at 2023-08-02T11:20:44Z>\n> Thanks for the info! The explanation makes a lot of sense. For now, I feel maybe manual tagging and release works better for me, as I think it will be beneficial to document each changes both in CHANGELOG as well as the release message, for people to be clear about each release. Do you have any other opinions? Thanks!\r\n\r\nThat's cool! Thank you for your interest. Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 318,
    "state": "open",
    "created_by": "rudxor02",
    "created_at": "2023-07-27T06:02:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/318</URL>\n\n<TITLE>[Usage]  Can you consider making release?</TITLE>\n\n<BODY>### When did you clone our code?\r\n\r\nI cloned the code base after 5/1/23\r\n\r\n### Describe the issue\r\n\r\nIssue:\r\n\r\nFor now i see there's no release in this repository and just commit directly to main branch, but i need llava [package versioning](https://docs.github.com/en/repositories/releasing-projects-on-github/managing-releases-in-a-repository) to make my service, because package dependency changes frequently on version update.\r\nHow about make release of this repository?</BODY>\n\n<COMMENTS>\n<Comment by ryuni-dev at 2023-07-27T08:12:43Z>\nplz...\n</Comment>\n<Comment by ImKeTT at 2023-07-27T09:10:11Z>\nright now it looks like a mess...\n</Comment>\n<Comment by Floodnut at 2023-07-27T09:14:31Z>\nI can't follow these release versions too... 🥲\n</Comment>\n<Comment by haotian-liu at 2023-07-30T01:54:16Z>\nThank you for the great suggestion! I have added the v1.0.1 release tag. In the future, we will conitnue adding necessary release notes as well.\r\n\r\nPlease let me know if you have any suggestions in terms of the workflow of this project.\r\n\r\nAlso, it seems that I cannot turn on the release on the main page of the github repo. Any ideas on why this is happening? Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 316,
    "state": "open",
    "created_by": "Unispac",
    "created_at": "2023-07-26T18:32:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/316</URL>\n\n<TITLE>[Question] Do you still use end-to-end finetuning for LLaMA-2 integration?</TITLE>\n\n<BODY>### Question\n\nHi,\r\n\r\nJust curious. In your paper, you mentioned that you will also do end-to-end finetuning on the LLM in instruction-tuning stage. I am wondering, whether you also do end-to-end finetuning when you integrate LLaMA-2?\r\n\r\nBest</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-31T05:14:41Z>\nHi @Unispac \r\n\r\nThank you for your interest in our work. We have been trying some PEFT approaches like LoRA or projection layer tuning. Currently, it seems that finetuning may not always be the best, but it is the safest one to go. But we are still trying to understand the best LoRA config for LLaVA.\r\n\r\nIt would be great to hear about your thoughts about LoRA and other PEFT approaches as well :)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 314,
    "state": "open",
    "created_by": "wh-yu",
    "created_at": "2023-07-26T16:53:57Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/314</URL>\n\n<TITLE>[Usage] LLaVA-13B-v0 perform badly in July code</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nHi, many thanks for your wonderful work.\r\n\r\nWe use 500 samples from GQA to test LLaVA models and observe strange results. \r\n\r\n| Model                                     | June code | July code |\r\n|-------------------------------------------|-----------|-----------|\r\n| LLaVA-13B-v0 (conv-mode: simple/llava_v0) | 40.4      | 25.2      |\r\n| LLaVA-13B-v0 (conv-mode: v0)              | --        | 28.0      |\r\n| LLaVA-7B-v0 (conv-mode: simple/llava_v0)  | 36.8      | 41.2      |\r\n| LLaVA-7B-v0 (conv-mode: v0)              | --        | 41.4     | \r\n\r\nDo you have any idea on what changes made the 7B-V0 better and what made 13B-V0 much worse?\r\n\r\nThank you a lot.</BODY>\n\n<COMMENTS>\n<Comment by MiroFurtado at 2023-07-28T21:51:09Z>\ndepending on how long it takes to do this eval loop, i would recommend using `git bisect` to trace the issue\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 313,
    "state": "closed",
    "created_by": "dydxdt",
    "created_at": "2023-07-26T08:03:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/313</URL>\n\n<TITLE>[Usage] Strange results by custom finetuned model</TITLE>\n\n<BODY></BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 312,
    "state": "open",
    "created_by": "dydxdt",
    "created_at": "2023-07-26T03:08:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/312</URL>\n\n<TITLE>[Question] larger batchsize will be better?</TITLE>\n\n<BODY>### Question\n\nThanks for your good job. I use 8 A100 to train data of 50000 images and it seems the mem-usage of GPU is around 34G,(time used: ~1.5h) which is much less than 80G. I use the default per_device_batch_size as 4. I just wonder maybe if I use larger batchsize to fully use the memory, will the results be better? Thank for your advice.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-26T03:10:36Z>\nHi @dydxdt, we train lightning models using batch size 128, and do not find much difference between training with bs32 vs bs128. We can get quite significant speed increase if the GPUs are not NVLinked and the models are large (13B).\n</Comment>\n<Comment by dydxdt at 2023-07-26T06:25:05Z>\nThanks for your reply！\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 311,
    "state": "open",
    "created_by": "dengtianbi",
    "created_at": "2023-07-26T03:01:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/311</URL>\n\n<TITLE>[Question]  Error while using the LLaMA-2-7B-Chat 4bit quantized model for inference</TITLE>\n\n<BODY>### Question\n\nHi,\r\nThanks for your awesome work!\r\n\r\nI used merge_lora_weights.py to merge llava-llama-2-7b-chat-lightning-lora-preview and Llama-2-7b-chat-hf. When using the 4bit quantized model for inference, I encountered the following error:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/34548408/f08f4889-42d4-489c-90fa-8ee2b6c67fd9)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-26T03:02:40Z>\nHi, can you please provide the full command for inference when you see this error, for me to better understand the issue? Thanks.\n</Comment>\n<Comment by dengtianbi at 2023-07-26T03:12:49Z>\nI used merge_lora_weights.py to merge llava-llama-2-7b-chat-lightning-lora-preview and Llama-2-7b-chat-hf by:\r\n`python scripts/merge_lora_weights.py --model-path ./Llama-2-7b-chat-hf --model-base ./llava-llama-2-7b-chat-lightning-lora-preview --save-model-path ./llava7b`\r\n\r\nAnd  :\r\n`python -m llava.serve.controller --host 0.0.0.0 --port 10000`\r\n`python -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload`\r\n`python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./llava7b --load-4bit`\r\nThen, I encountered the above-mentioned error when I performed model inference on a Gradio webpage\n</Comment>\n<Comment by TalhaUusuf at 2023-07-27T21:19:39Z>\nI am also getting same error.\n</Comment>\n<Comment by haotian-liu at 2023-07-29T18:01:02Z>\n@dengtianbi \r\n\r\nI tried the 4 bit inference with llava-llama-2-7b-chat, and it works for me. One issue is that currently the inference code is parsing the correct inference pipeline to use by looking at the `--model-path`, so you need to have both keyword `llama-2` and `llava` in your merged model path. Try to rename it to `llava-7b-llama-2-7b-chat` and rerun the command.\r\n\r\nI will document this in README, and try to figure out a way to better parse these pipelines. Thanks!\n</Comment>\n<Comment by haotian-liu at 2023-07-29T18:01:25Z>\n@TalhaUusuf please let me know if the solution above works for you. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 310,
    "state": "open",
    "created_by": "chigkim",
    "created_at": "2023-07-25T12:45:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/310</URL>\n\n<TITLE>[Question] Quantizing to use with oobabooga/text-generation-webui</TITLE>\n\n<BODY>### Question\r\n\r\nI downloaded llava-llama-2-13b from:\r\nhttps://huggingface.co/liuhaotian/llava-llama-2-13b-chat-lightning-preview\r\n\r\nThen I've quantized the model to 4-bit using .\r\n\r\n```\r\ngit clone https://github.com/oobabooga/GPTQ-for-LLaMa.git -b cuda\r\ncd GPTQ-for-LLaMa\r\npip install -r requirements.txt\r\npython setup_cuda.py install\r\npython llama.py ../models/liuhaotian_llava-llama-2-13b-chat-lightning-preview c4 --wbits 4 --true-sequential --groupsize 128 --save_safetensors ../models/liuhaotian_llava-llama-2-13b-chat-lightning-preview/liuhaotian_llava-llama-2-13b-chat-lightning-4bit-128g.safetensors\r\n```\r\n\r\nThen I loaded with --multimodal-pipeline llava-13b.\r\n\r\n`python server.py --verbose --share --chat --model-dir models --model liuhaotian_llava-llama-2-13b-chat-lightning-preview --loader exllama --max_seq_len 4096 --xformers --no-stream --deepspeed --multimodal-pipeline llava-13b`\r\n\r\nI tried many different images, but the descriptions are completely incorrect. I.E. I submitted a picture a dog, and it said woman holding an umbrella.\r\n\r\nSince the model loads and outputs proper English, I assume it's quantized correctly.\r\n\r\nI tried --load-4bit with model_worker, but it doesn't seem to support GPTQ format.\r\n\r\nI'd appreciate any tip!\r\n\r\nThanks,</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-26T00:20:58Z>\nHi @chigkim \r\n\r\nAre you testing from text-gen-webui or some other repo? (not familiar with these)\r\n\r\nNot sure if this is due to the template, as we use a prompt template similar to LLaMA-2 for llava-llama-2-chat series (something like below prompt), which is different from vicuna template.\r\n\r\n```\r\n[INST] <<SYS>>\r\nYou are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\r\n<</SYS>>\r\n\r\n<image> [/INST]\r\n```\r\n\r\nMaybe I will have some bandwidth to try this out myself either Friday or this weekend. Any pointer in the repositories I should try first? Thanks.\n</Comment>\n<Comment by chigkim at 2023-07-26T19:00:22Z>\nThanks for your response!\r\nI used this repo.\r\nhttps://github.com/oobabooga/text-generation-webui\r\nHere's my quantized model.\r\nhttps://drive.google.com/drive/folders/1-njjlAXE8JD_UnccZ15geFIMMBU5PZKC\r\n\r\nAfter clone, you need to Put the model folder inside text-generation-webui/models.\r\nYou should have text-generation-webui/models/liuhaotian_llava-llama-2-13b-chat-lightning-preview/llava-llama-2-13b-chat-4bit-128g.safetensors.\r\n\r\nHere's my setup code.\r\n```\r\ngit clone https://github.com/oobabooga/text-generation-webui\r\ncd text-generation-webui\r\npip install -r requirements.txt\r\npip install deepspeed mpi4py xformers\r\npython server.py --verbose --share --chat --model-dir models --model liuhaotian_llava-llama-2-13b-chat-lightning-preview --loader exllama --max_seq_len 4096 --xformers --no-stream --deepspeed --multimodal-pipeline llava-13b\r\n```\r\nOnce you open the link from Gradio, go to Chat Settings > Instruction template > and edit the Context.\r\nGo back to Text generation and choose instruct.\r\n\r\nHere's my Colab notebook that you can just run that will automatically download and load the my quantized model.\r\n\r\nhttps://colab.research.google.com/drive/1n9Tq9XmmTElkRHazVKi2CUaVTqVWFjfP?usp=sharing\r\n\r\nHope this will get you started.\n</Comment>\n<Comment by chigkim at 2023-07-26T19:03:13Z>\nAlso I opened an issue on oobabooga/text-generation-webui.\r\nhttps://github.com/oobabooga/text-generation-webui/issues/3293\n</Comment>\n<Comment by alvinkimata at 2023-07-27T09:46:28Z>\n> Thanks for your response! I used this repo. https://github.com/oobabooga/text-generation-webui Here's my quantized model. https://drive.google.com/drive/folders/1-njjlAXE8JD_UnccZ15geFIMMBU5PZKC\r\n> \r\n> After clone, you need to Put the model folder inside text-generation-webui/models. You should have text-generation-webui/models/liuhaotian_llava-llama-2-13b-chat-lightning-preview/llava-llama-2-13b-chat-4bit-128g.safetensors.\r\n> \r\n> Here's my setup code.\r\n> \r\n> ```\r\n> git clone https://github.com/oobabooga/text-generation-webui\r\n> cd text-generation-webui\r\n> pip install -r requirements.txt\r\n> pip install deepspeed mpi4py\r\n> pip install -U num2words omegaconf xformers\r\n> python server.py --verbose --share --chat --model-dir models --model liuhaotian_llava-llama-2-13b-chat-lightning-preview --loader exllama_hf --max_seq_len 4096 --xformers --no-stream --deepspeed --multimodal-pipeline llava-13b\r\n> ```\r\n> \r\n> Once you open the link from Gradio, go to Chat Settings > Instruction template > and edit the Context. Go back to Text generation and choose instruct. Hope this will get you started.\r\n\r\nI used the instructions above when loading the model. The setup works but when I pass in an image to the model, I get an error:\r\n\r\n```txt\r\nYou: <img src=\"data:image/jpeg;base64,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\">\r\nAssistant:\r\n--------------------\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/routes.py\", line 427, in run_predict\r\n    output = await app.get_blocks().process_api(\r\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1323, in process_api\r\n    result = await self.call_function(\r\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1067, in call_function\r\n    prediction = await utils.async_iteration(iterator)\r\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 336, in async_iteration\r\n    return await iterator.__anext__()\r\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 329, in __anext__\r\n    return await anyio.to_thread.run_sync(\r\n  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\r\n    result = context.run(func, *args)\r\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 312, in run_sync_iterator_async\r\n    return next(iterator)\r\n  File \"/content/text-generation-webui/modules/chat.py\", line 303, in generate_chat_reply_wrapper\r\n    for i, history in enumerate(generate_chat_reply(text, state, regenerate, _continue, loading_message=True)):\r\n  File \"/content/text-generation-webui/modules/chat.py\", line 288, in generate_chat_reply\r\n    for history in chatbot_wrapper(text, state, regenerate=regenerate, _continue=_continue, loading_message=loading_message):\r\n  File \"/content/text-generation-webui/modules/chat.py\", line 212, in chatbot_wrapper\r\n    for j, reply in enumerate(generate_reply(prompt + cumulative_reply, state, stopping_strings=stopping_strings, is_chat=True)):\r\n  File \"/content/text-generation-webui/modules/text_generation.py\", line 28, in generate_reply\r\n    for result in _generate_reply(*args, **kwargs):\r\n  File \"/content/text-generation-webui/modules/text_generation.py\", line 211, in _generate_reply\r\n    for reply in generate_func(question, original_question, seed, state, stopping_strings, is_chat=is_chat):\r\n  File \"/content/text-generation-webui/modules/text_generation.py\", line 252, in generate_reply_HF\r\n    question, input_ids, inputs_embeds = apply_extensions('tokenizer', state, question, input_ids, None)\r\n  File \"/content/text-generation-webui/modules/extensions.py\", line 207, in apply_extensions\r\n    return EXTENSION_MAP[typ](*args, **kwargs)\r\n  File \"/content/text-generation-webui/modules/extensions.py\", line 108, in _apply_tokenizer_extensions\r\n    prompt, input_ids, input_embeds = getattr(extension, function_name)(state, prompt, input_ids, input_embeds)\r\n  File \"/content/text-generation-webui/extensions/multimodal/script.py\", line 89, in tokenizer_modifier\r\n    prompt, input_ids, input_embeds, total_embedded = multimodal_embedder.forward(prompt, state, params)\r\n  File \"/content/text-generation-webui/extensions/multimodal/multimodal_embedder.py\", line 172, in forward\r\n    prompt_parts = self._embed(prompt_parts)\r\n  File \"/content/text-generation-webui/extensions/multimodal/multimodal_embedder.py\", line 148, in _embed\r\n    embedded = self.pipeline.embed_images([parts[i].image for i in image_indicies])\r\n  File \"/content/text-generation-webui/extensions/multimodal/pipelines/llava/llava.py\", line 75, in embed_images\r\n    image_forward_outs = self.vision_tower(images, output_hidden_states=True)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 941, in forward\r\n    return self.vision_model(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 866, in forward\r\n    hidden_states = self.embeddings(pixel_values)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 195, in forward\r\n    patch_embeds = self.patch_embedding(pixel_values)  # shape = [*, width, grid, grid]\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 463, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\r\n    return F.conv2d(input, weight, bias, self.stride,\r\nRuntimeError: weight should have at least three dimensions\r\n```\r\n\r\nWhat could be the issue?\n</Comment>\n<Comment by chigkim at 2023-07-28T12:36:03Z>\n@AlvinKimata, My apology!\r\nI copied a wrong line to launch the server.\r\nThe loader exllama_hf throws the error you got. Loading with exllama (without_hf) should work. --loader exllama\r\n\r\n`python server.py --verbose --share --chat --model-dir models --model liuhaotian_llava-llama-2-13b-chat-lightning-preview --loader exllama --max_seq_len 4096 --xformers --no-stream --deepspeed --multimodal-pipeline llava-13b`\r\n\r\n@haotian-liu Here's my Colab notebook that you can just run.\r\n\r\nhttps://colab.research.google.com/drive/1n9Tq9XmmTElkRHazVKi2CUaVTqVWFjfP?usp=sharing\n</Comment>\n<Comment by haotian-liu at 2023-07-31T04:34:57Z>\nHi @chigkim @AlvinKimata \r\n\r\nI have looked into the oobabooga text-gen-ui this weekend, and I got our LLaMA-2 checkpoint working with some modifications to the code. I am working on a PR and plan to finalize it when we release our new LLaMA-2-based checkpoints in the coming days.\r\n\r\nTo try it out with the text-gen-ui, you can for now use [my fork](https://github.com/haotian-liu/text-generation-webui), and download my quantized checkpoints [here](https://huggingface.co/liuhaotian/llava-llama-2-13b-chat-lightning-gptq) and put it under `models` folder.\r\n\r\nI have tested the following command and it works:\r\n```Shell\r\npython server.py \\\r\n    --chat \\\r\n    --model-dir models \\\r\n    --model llava-llama-2-13b-chat-lightning-4bit-128g \\\r\n    --multimodal-pipeline llava-llama-2-13b\r\n```\r\n\r\nTwo things I may need still look into:\r\n1. It seems that when exllama is used (`--loader exllama`), the multimodal plugin is not properly loaded. I have submitted an issue: https://github.com/oobabooga/text-generation-webui/issues/3378. So for now, I need to resort to `AutoGPTQ` for this.\r\n2. The quantization quality. It seems that the quantized checkpoints I generated using [`GPTQ-for-LLaMa`](https://github.com/qwopqwop200/GPTQ-for-LLaMa) is not compatible with `AutoGPTQ`, and it generates garbled responses. I tried some other checkpoints that are quantized by TheBloke, and some of them also generates garbled outputs (e.g. `TheBloke/Llama-2-7b-Chat-GPTQ:gptq-4bit-32g-actorder_True`). As a workaround, I now quantize the checkpoints using AutoGPTQ, and the checkpoint is uploaded to HF as provided above. I am not sure about the quantization quality yet.\r\n\r\nWhat I may need help for:\r\n1. What are the ways to improve the quantization quality of LLaVA?\r\n2. Can the quantized checkpoints using `GPTQ-for-LLaMa` be compatible with AutoGPTQ?\n</Comment>\n<Comment by Don-Chad at 2023-07-31T14:33:36Z>\nFrom what I know AutoGPTQ does something smart with the data - working from the training prompts to do a high quality compression, to offer 32 bit quality at 4 bit speed. Not sure GPTQ does the same. \r\n\r\nRunning with Exllama would offer a great speed advantage, benchmarks come out at about double the speed. So that would be cool. I think it should be possible without major changes to the exllama engine and to web-generation-ui, since the visual analysis(blip/clip) is already separate from the language model engine ,right?\n</Comment>\n<Comment by Don-Chad at 2023-07-31T16:22:32Z>\n@haotian-liu Would you please want to share the settings for the AutoGPTQ quantisation - if any? I would like to see if I can get GPTQ-for-LLaMa working.\n</Comment>\n<Comment by haotian-liu at 2023-07-31T16:54:25Z>\nHi @Don-Chad \r\n\r\nThank you for sharing the info and for offering the help!\r\n\r\nRegarding exllama, it seems to be some compatibility issue with the text-gen-ui itself, as also confirmed by https://github.com/oobabooga/text-generation-webui/pull/3377#issuecomment-1658311157\r\n\r\nFor AutoGPTQ quantization I used for creating [this checkpoint](https://huggingface.co/liuhaotian/llava-llama-2-13b-chat-lightning-gptq). I basically use the sample script from AutoGPTQ repo, which I attach [here](https://gist.github.com/haotian-liu/0dc96a1c63e91f31b04b3e94250c716b).\r\n\r\nYou need to make a few modifications to your checkpoints though to properly run the script:\r\n\r\n> Download https://huggingface.co/liuhaotian/llava-llama-2-13b-chat-lightning-preview to local\r\n> Make following edits to the config.json\r\n> LlavaLlamaForCausalLM -> LlamaForCausalLM\r\n> \"model_type\": \"llava\" -> \"llama\"\r\n\r\nNote that I am quite new to these quantization techniques, so they are probably not the best parameters to use.  Please share if you have any thoughts or findings to get better quality/compatibility.\r\n\r\nThanks.\n</Comment>\n<Comment by chigkim at 2023-08-01T03:13:54Z>\nWhatever worth, I got the quantized model using oobabooga/GPTQ-for-LLaMa to work with AutoGPTQ loader.\r\nIf you quantize using qwopqwop200/GPTQ-for-LLaMa, it doesn't work.\r\nHere's what I did.\r\n1. Modified config.json as @haotian-liu described above.\r\n2. Quantized using the command below.\r\n`python repositories/GPTQ-for-LLaMa/llama.py models/liuhaotian_llava-llama-2-13b-chat-lightning-preview c4 --wbits 4 --true-sequential --groupsize 128 --save_safetensors models/liuhaotian_llava-llama-2-13b-chat-lightning-preview/llava-llama-2-13b-chat-4bit-128g.safetensors`\r\n3. Checked out the pull request.\r\n```\r\ngit fetch origin pull/3377/head:br3377\r\ngit checkout br3377\r\n```\r\n4. Loaded using --multimodal-pipeline llava-llama-2-13b.\r\nThe output was correct.\r\nHowever, this still doesn't work with exllama.\r\nHopefully they could fix it soon, because exllama is significantly faster.\n</Comment>\n<Comment by anan1030 at 2024-08-02T11:50:25Z>\nHi, anyone know the python code to do inference without using the webui?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 309,
    "state": "closed",
    "created_by": "r3shma",
    "created_at": "2023-07-25T12:42:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/309</URL>\n\n<TITLE>error when running llava-llama-2-13b-chat-lightning-preview model in CLI</TITLE>\n\n<BODY>### Question\n\nHi @haotian-liu \r\n\r\nI was trying to run the llava-llama-2-13b-chat-lightning-preview model thorugh the CLI but got the following error:\r\n\r\nraise HFValidationError(\r\nhuggingface_hub.utils._validators.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/llava_models/llava-llama-2-13b-chat-lightning-preview'. Use `repo_type` argument if needed.\r\n\r\nI am not sure what changes are needed to the eval script to make this work. Appreciate any help!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-26T00:15:49Z>\nHi @r3shma this error is very likely due to that `/home/llava_models/llava-llama-2-13b-chat-lightning-preview` does not exist on disk. Please check if you are using the correct path, or you can load from huggingface directly by using `liuhaotian/llava-llama-2-13b-chat-lightning-preview`.\n</Comment>\n<Comment by r3shma at 2023-07-26T03:58:57Z>\nThank you!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 308,
    "state": "open",
    "created_by": "kittyLunar",
    "created_at": "2023-07-25T09:00:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/308</URL>\n\n<TITLE>[Question] ModuleNotFoundError \"einops\"</TITLE>\n\n<BODY>### Question\r\n\r\nJust followed the steps in README.md \r\nwhen I trying to train my data to llava model.\r\nI got few error messages\r\nwhich is all about \"einops\" module\r\n\r\nModuleNotFoundError: No module named 'einops'\r\n\r\nhowever, I already installed the module 'einops' in my conda environment\r\n`pip list | grep einops`\r\neinops                   0.6.1\r\n\r\n`conda list | grep einops`\r\neinops                    0.6.1                    pypi_0    pypi\r\n\r\nwhen I install einops module through conda-forge channel, it shows same error.\r\n\r\nwhat did I miss?\r\nor do I have to set PYTHONPATH to some specific path?\r\n\r\n\r\nhere is one of the error messages I've got.\r\n\r\nTraceback (most recent call last):\r\n  File \"llava/train/train_mem.py\", line 6, in <module>\r\n    from llava.train.llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn\r\n  File \"/home/test/llava/\\_\\_init__.py\", line 1, in <module>\r\n    from .model import LlavaLlamaForCausalLM\r\n  File \"/home/test/llava/model/\\_\\_init__.py\", line 2, in <module>\r\n    from .llava_mpt import LlavaMPTForCausalLM, LlavaMPTConfig\r\n  File \"/home/test/llava/model/llava_mpt.py\", line 31, in <module>\r\n    from .mpt.modeling_mpt import MPTConfig, MPTForCausalLM, MPTModel\r\n  File \"/home/test/llava/model/mpt/modeling_mpt.py\", line 13, in <module>\r\n    from .attention import attn_bias_shape, build_attn_bias\r\n  File \"/home/test/llava/model/mpt/attention.py\", line 7, in <module>\r\n    from einops import rearrange\r\nModuleNotFoundError: No module named 'einops'</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-26T00:24:36Z>\nThis is strange, as `einops` is definitely included in [`pyproject.toml`](https://github.com/haotian-liu/LLaVA/blob/main/pyproject.toml), and you also see it is installed.\r\n\r\nJust to make sure the env is installed correctly, can you try running the inference code and see if it can be loaded correctly? And please also try run this **not in the repository directory** so that it must load llava from registered environment, just to test the env is installed correctly.\r\n\r\nhttps://github.com/haotian-liu/LLaVA#cli-inference\r\n\r\nThanks.\n</Comment>\n<Comment by kittyLunar at 2023-07-31T04:11:17Z>\nCLI inference with [https://github.com/haotian-liu/LLaVA#cli-inference](url) show an error message \r\n`ImportError: cannot import name 'TextStreamer' from 'transformers'`\r\n\r\nhowever, I did CLI inference with \r\n`python -m llava.eval.run_llava` it works well  (not `python -m llava.serve.cli`)\r\n\r\nI think something installed wrong. however, I could not figure out what are wrong.\r\ncuz I perfectly followed all steps which are written in Readme.md\n</Comment>\n<Comment by haotian-liu at 2023-07-31T04:41:35Z>\nHi @kittyLunar \r\n\r\nCan you try to see what your `transformers` version is? Thanks.\n</Comment>\n<Comment by kittyLunar at 2023-08-01T08:13:31Z>\n@haotian-liu \r\nmy transformers version is like below\r\n`pip list | grep transformers`\r\n`transformers             4.28.0.dev0`\r\n`conda list | grep transformers`\r\n`transformers              4.28.0.dev0              pypi_0    pypi`\n</Comment>\n<Comment by haotian-liu at 2023-08-02T05:13:03Z>\nHi, this seems that the installation is not fully matching the latest code base. As our latest code requires [`transformers==4.31.0`](https://github.com/haotian-liu/LLaVA/blob/main/pyproject.toml#L22).\r\n\r\nPlease try update the packages to the latest code base, following the instructions [here](https://github.com/haotian-liu/LLaVA/tree/main#upgrade-to-latest-code-base). You may also try a clean install.\n</Comment>\n<Comment by kittyLunar at 2023-08-03T08:32:55Z>\nThanks, I installed transformers 4.31.0 and tried clean install\r\nafter that, the CLI inference works fine.\r\nhowever, in case of training, it shows same error. \"einops\" module not found.\r\n\r\nI have no idea what should I check for that module. \r\nI think that I have to figure out in very basic things.. like how did the python find the dependency of installation module or something like that.. It will be really tough job I think.\r\n\r\nIf you have any guide, or suggestions for that \"einops\" module not found error.\r\nplease let me know. \r\n\r\nThanks.\n</Comment>\n<Comment by haotian-liu at 2023-08-03T16:52:34Z>\n@kittyLunar \r\n\r\nHi, could you make sure that you activate your conda environment when you try to do the training? You can check by using `which python` and see if it is using the python from your conda environment.\r\n\r\nIf it is showing the correct one, you will probably see something like `/path/to/conda/envs/llava/bin/python`, use this path as a reference to navigate to `/path/to/conda/envs/llava/lib/python3.10/site-packages`, which is the place of all your packages are installed at.\r\n\r\nTry to see if einops is in there, e.g. `ls einops*`.\n</Comment>\n<Comment by kittyLunar at 2023-08-04T01:00:20Z>\n@haotian-liu \r\n\r\nthank you for your kind reply\r\n\r\nI am using conda environment, and my python path is correct \r\n\r\n`(llava) test@gpusvr:which python`\r\n`/home/test/.conda/envs/llava/bin/python`\r\n\r\nand I could find 'einops' in `/path/to/conda/envs/llava/lib/python3.10/site-packages`\r\n\r\n`(llava) test@gpusvr:~/.conda/envs/llava/lib/python3.10/site-packages$ ls einops* `\r\n`einops:`\r\n`_backends.py  einops.py  experimental  __init__.py  layers  packing.py  parsing.py  __pycache__  py.typed  _torch_specific.py`\r\n\r\n`einops-0.6.1.dist-info:`\r\n`INSTALLER  licenses  METADATA  RECORD  WHEEL`\r\n\r\n`einops_exts:`\r\n`einops_exts.py  __init__.py  __pycache__  torch.py`\r\n\r\n`einops_exts-0.0.4.dist-info:`\r\n`INSTALLER  LICENSE  METADATA  RECORD  top_level.txt  WHEEL`\r\n\r\nmaybe my python goes crazy :(\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 307,
    "state": "closed",
    "created_by": "tingxueronghua",
    "created_at": "2023-07-25T06:55:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/307</URL>\n\n<TITLE>[Question] Why change the implementation of blocking gradients of input embeddings?</TITLE>\n\n<BODY>### Question\n\nThanks for your awesome work! I noticed that you commit this https://github.com/haotian-liu/LLaVA/commit/871bbd4a5248510156629e00ee5042b74f74764c to solve the issue of abnormal response. However, I have no idea the old implementation will introduce bugs. Is it related with the parallel framework?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-30T02:05:02Z>\nHi @tingxueronghua \r\n\r\nThank you for your interest in this. I remembered that it is mainly because the previous solution will still modify the embeddings for the last iteration, and it seems that it is getting unreasonably large gradients to the token embeddings, which is large enough to make the generation process a mess. By making the change, the generation process is now working properly. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 306,
    "state": "open",
    "created_by": "TonyXuQAQ",
    "created_at": "2023-07-25T02:10:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/306</URL>\n\n<TITLE>[Question] How to do conversations during inference with script?</TITLE>\n\n<BODY>### Question\n\nHi there. I want to infer and evaluate the trained LLaVA, but how can I test conversation during inference (i.e., as multiple continuous questions)? The script I used for inference: \r\n```\r\npython -m llava.eval.run_llava \\\r\n    --model-name /path/to/LLaVA-13B-v0 \\\r\n    --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n    --query \"What are the things I should be cautious about when I visit here?\"\r\n```\r\nI can only ask one question by query. May I know how to ask multiple questions in a conversation? Thanks for your help and consideration.</BODY>\n\n<COMMENTS>\n<Comment by shileims at 2023-07-29T21:59:23Z>\n+1\n</Comment>\n<Comment by haotian-liu at 2023-07-29T23:57:44Z>\nHi @TonyXuQAQ and @shileims \r\nWe have supported CLI inference with multiple turn conversations, but it seems that I have never officially document it yet.\r\n\r\nJust updated that in [readme](https://github.com/haotian-liu/LLaVA#cli-inference), and please try it out, and also feel free to follow up if there are issues with the current cli interface. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 305,
    "state": "open",
    "created_by": "LukeBailey181",
    "created_at": "2023-07-25T01:02:30Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/305</URL>\n\n<TITLE>[Feature request] Why torch.no_grad on CLIPVisionTower Foward</TITLE>\n\n<BODY>### feature\n\nIn `llava/model/multimodal_encoder/clip_encoder.py` line 39, the forward pass of the vision encoder has `torch.no_grad` decorator. I am trying to do some input optimization, and I think this is stopping gradients from being back propped to the input image. Is there a reason for this no_grad? Would it be ok to remove it? (I am happy to make a PR if so :) ) \r\n\r\nThanks in advance for any help with this!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-26T03:06:36Z>\nHi @LukeBailey181 \r\n\r\nThanks for the feedback and for your interest in our project. You are right that this `torch.no_grad` is preventing you from doing input optimization. This maybe one of the overally cautious decorator I have used, to make sure that the vision encoder is not modified during pretraining/instruction tuning. Since we have `vision_encoder.requires_grad_(False)`, this shall be fine.\r\n\r\nIt would be great if you can help create a PR about this, and we want to make sure that (1) vision encoder is not modified in any way that we do not want; (2) the gradients do not backward through the vision encoder unnecessarily (for most of the use cases, including the standard pretraining and instruction tuning), unless we need that like you are doing input optimization.\r\n\r\nThank you!\n</Comment>\n<Comment by harrytea at 2024-02-28T04:45:23Z>\nhow to optimize the vision encoder? which code should i modify?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 304,
    "state": "open",
    "created_by": "RajeshRadha",
    "created_at": "2023-07-24T21:46:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/304</URL>\n\n<TITLE>[Question] Add Llava to Hugging face Transformers package?</TITLE>\n\n<BODY>### Question\n\nWhat is the work involved in adding Llava to Hugging Face transformers package?\r\n\r\nI already see InstructBlip in there -- https://huggingface.co/models?other=instructblip and here is the other list of supported models: https://huggingface.co/docs/transformers/index\r\n\r\nSince LLava uses image encoders like CLIP / EVA or ViT and foundational models like Llama / T5 / Vicuna etc. It makes it easy to plug and play different models and see how well the connector module design is playing with them. Any thoughts on this?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-26T00:27:22Z>\nGreat suggestion!  From v0 -> v1.0.0 code base, I have refactored in such a direction for supporting different vision encoders and LLMs, and the next step would definitely be supporting swapping any LLMs / vision encoders by using some standard HF structures.\r\n\r\nCurrently I may have limited bandwidth in working on that, but I would definitely be happy to support and collaborate during the integration process!\r\n\r\nHappy to chat more about the details if you are interested in working on the integration. Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 302,
    "state": "closed",
    "created_by": "tingxueronghua",
    "created_at": "2023-07-24T11:33:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/302</URL>\n\n<TITLE>Is it normal to found parameters in empty sizes?</TITLE>\n\n<BODY>### Question\n\nFirst, I am sure that the model could pretrain and can see the loss curve on wandb. However I want to find how to add extra token into the tokenizer and found something strange. Now I will show an example. Here I add the print function as follows to see the output of model parameters:\r\n\r\n```\r\ndata_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\r\nprint('previous:', model.model.layers[0].self_attn.q_proj.weight.data)\r\ntrainer = LLaVATrainer(model=model,\r\n    tokenizer=tokenizer,\r\n    args=training_args,\r\n    **data_module)   \r\nprint('after:', model.model.layers[0].self_attn.q_proj.weight.data)\r\n\r\n```\r\n\r\nHowever, I found the output on all GPUs are empty. Is this normal?\r\n\r\n```\r\nprevious: tensor([], device='cuda:0', dtype=torch.float16)\r\nafter: tensor([], device='cuda:0', dtype=torch.float16)\r\nWARNING:root:Formatting inputs...Skip in lazy mode\r\nprevious: tensor([], device='cuda:1', dtype=torch.float16)\r\nafter: tensor([], device='cuda:1', dtype=torch.float16)\r\nWARNING:root:Formatting inputs...Skip in lazy mode\r\nprevious: tensor([], device='cuda:2', dtype=torch.float16)\r\nafter: tensor([], device='cuda:2', dtype=torch.float16)\r\nWARNING:root:Formatting inputs...Skip in lazy mode\r\nprevious: tensor([], device='cuda:4', dtype=torch.float16)\r\nafter: tensor([], device='cuda:4', dtype=torch.float16)\r\nWARNING:root:Formatting inputs...Skip in lazy mode\r\nprevious: tensor([], device='cuda:7', dtype=torch.float16)\r\nWARNING:root:Formatting inputs...Skip in lazy mode\r\nprevious: tensor([], device='cuda:3', dtype=torch.float16)\r\nafter: tensor([], device='cuda:7', dtype=torch.float16)\r\nafter: tensor([], device='cuda:3', dtype=torch.float16)\r\nWARNING:root:Formatting inputs...Skip in lazy mode\r\nprevious: tensor([], device='cuda:5', dtype=torch.float16)\r\nafter: tensor([], device='cuda:5', dtype=torch.float16)\r\nWARNING:root:Formatting inputs...Skip in lazy mode\r\nprevious: tensor([], device='cuda:6', dtype=torch.float16)\r\nafter: tensor([], device='cuda:6', dtype=torch.float16)\r\n```</BODY>\n\n<COMMENTS>\n<Comment by tingxueronghua at 2023-07-24T11:34:29Z>\nI am using deepspeed with zero3.json.\n</Comment>\n<Comment by haotian-liu at 2023-07-24T15:59:54Z>\nDeepspeed zero3 uses meta device to automatically shard the model parameters. So it is normal to see such behavior.\r\n\r\nTo view the actual tensor sizes, you can use [zero2](https://github.com/haotian-liu/LLaVA/blob/main/scripts/zero2.json).\n</Comment>\n<Comment by tingxueronghua at 2023-07-24T16:16:45Z>\nthanks for your quick reply! I will try your advice!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 301,
    "state": "open",
    "created_by": "TonyXuQAQ",
    "created_at": "2023-07-24T11:22:02Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/301</URL>\n\n<TITLE>[Question] About LLaVA-LLaMA-2 fine tune</TITLE>\n\n<BODY>### Question\n\nThanks for sharing the code and support for LLaMA-2. I wanna finetune llava-llama-2, but there is no pretrained projector released. I want to extract the projector from the provided llava-llama-2 checkpoints, but this error occurs:\r\n\r\n```ValueError: Unexpected embed_tokens_weight shape. Pretrained: torch.Size([32004, 4096]). Current: torch.Size([32003, 4096]). Numer of new tokens: 2.```\r\n\r\n\r\nIs there any method that I can train the model from the provided checkpoints? Instead of from the pre-trained model? For example, \r\n1. I pretrain the model for alignment.\r\n2.  I finetune the model on dataset ```A``` and obtain a checkpoint.\r\n3. I have another new dataset ```B```.  Then I want to finetune this model on dataset ```B```. How can I extract the projector from the checkpoint obtained in step 2?\r\n\r\nThanks in advance for your sincere help!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-08-02T05:21:24Z>\nHi, this issue may be due to the mismatch between the code base and the command / projector you are using.\r\n\r\nPlease try using the latest finetune script [here](https://github.com/haotian-liu/LLaVA/blob/main/scripts), and corresponding projector weights [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#projector-weights).\r\n\r\nPlease let me know if you have any other issues, thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 298,
    "state": "closed",
    "created_by": "TonyXuQAQ",
    "created_at": "2023-07-23T08:18:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/298</URL>\n\n<TITLE>[Question] Incorrect inference with llama-2</TITLE>\n\n<BODY>### Question\n\nHi authors, I recently tried with llama-2, and thanks for sharing the code to support llava with llama2.\r\n\r\n1. I downloaded the llava-llama-2-13b-chat and llama-2-13b-chat. \r\n2. Then I convert llama-2-13b-chat to llama-2-13b-chat-hf.\r\n3. I run the code to extract llava from llava-delta by\r\n```\r\npython3 -m llava.model.apply_delta \\\r\n    --base /path/to/llama-2-13b-chat-hf \\\r\n    --target /output/path/to/LLaVA-13B-llama2 \\\r\n    --delta liuhaotian/LLaVA-13b-llama2-lighting\r\n```\r\n4. Run inference code\r\n```\r\npython -m llava.eval.run_llava \\\r\n    --model-path /path/to/LLaVA-13B-llama2 \\\r\n    --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n    --query \"What are the things I should be cautious about when I visit here?\"\r\n```\r\n\r\nHowever, the output is just meaningless characters. And I noticed that ```llava.eval.run_llava.py``` has been updated, and ```args``` has been changed from ```model-name``` to ```model-path``` and ```model-base```. But these is no explanation and examples about ```model-path``` and ```model-base```.\r\n\r\nCould you please tell me if  I have the correct implementation steps, and could you please add new examples about the inference code. Thanks in advance.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-23T15:24:04Z>\nHi, apologies for the confusion. Starting from the latest checkpoints, we directly release the original checkpoints instead of the delta weights. You can directly use our checkpoints, without doing the conversion.\r\n\r\nWe'll fix the README asap.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 297,
    "state": "closed",
    "created_by": "bebory",
    "created_at": "2023-07-23T07:30:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/297</URL>\n\n<TITLE>[Question] 请问Multimodal Chatbot训练了几个epochs</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-23T20:55:38Z>\nHi, we usually pretrain for 1 epoch, and finetune for 1 epoch for lightning, and 3 epoch for the models mentioned in the paper. We are exploring different schedules for different training techniques.\r\n\r\nYou can check out the detailed schedules, training datasets used for checkpoints in [MODEL ZOO](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md).\r\n\r\nThanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 296,
    "state": "closed",
    "created_by": "liulangjita",
    "created_at": "2023-07-22T01:23:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/296</URL>\n\n<TITLE>how to add my custom data[Question]</TITLE>\n\n<BODY>### Question\n\nIs it necessary to include my data in both the first pre-train and second fine-tune stages?Or just add my data in the first stage</BODY>\n\n<COMMENTS>\n<Comment by YerongLi at 2023-07-22T08:02:30Z>\nPass some error into  DataArguments see where it goes.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 295,
    "state": "closed",
    "created_by": "kahnchana",
    "created_at": "2023-07-21T15:28:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/295</URL>\n\n<TITLE>[Question] Has anyone tried multinode training (LLM finetuning) with FSDP?</TITLE>\n\n<BODY>### Question\n\nCan the current setup be directly used on multinode setup? Using slurm and torchrun?</BODY>\n\n<COMMENTS>\n<Comment by kahnchana at 2023-07-21T21:15:12Z>\nIt works out of the box with a slurm scheduler. \r\n\r\nSimply appending following to torchrun in the train scripts: \r\n```\r\nsrun --jobid $SLURM_JOBID \\\r\n    torchrun \\\r\n    --nnodes=$SLURM_NNODES \\\r\n    --nproc_per_node=8 \\\r\n    --node_rank $SLURM_PROCID \\\r\n    --rdzv_id=1 \\\r\n    --rdzv_backend=c10d \\\r\n    --rdzv_endpoint=\"${MASTER_ADDR}:${MASTER_PORT}\" \\\r\n    llava/train/train_mem.py . . . \r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 294,
    "state": "closed",
    "created_by": "YerongLi",
    "created_at": "2023-07-21T11:28:09Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/294</URL>\n\n<TITLE>ValueError: Attempting to unscale FP16 gradients. on V100 with --fp16</TITLE>\n\n<BODY>### When did you clone our code?\r\n\r\nI cloned the code base after 5/1/23\r\n\r\n### Describe the issue\r\n\r\nIssue:\r\nI found `--bits 4` or `--bits 8` works on A100 \r\nwhile quantization with `--bits 8` and `--fp16 True` does not work on V100 GPU. (with torch.autocast(\"cuda\") enabled)\r\n```\r\nif __name__ == \"__main__\":\r\n    with torch.autocast(\"cuda\"):\r\n        train()\r\n```\r\nCommand:\r\n```\r\n#!/bin/bash\r\n\r\nWEIGHT_VERSION=v1-1\r\nPROMPT_VERSION=v1\r\nMODEL_VERSION=\"7b\"\r\n\r\npython llava/train/train.py \\\r\n    --bits 8 \\\r\n    --lora_enable True \\\r\n    --model_name_or_path checkpoints/vicuna-7b-v1.1 \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path ./playground/data/llava_instruct_150k.json \\\r\n    --image_folder /scratch/yerong/Multimodal-GPT/data/coco/train2017 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/mm_projector/LLaVA-7b-pretrain-projector-v0-CC3M-595K-original_caption.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --fp16 True \\\r\n    --output_dir ./checkpoints/deepspeed_llava-$MODEL_VERSION-$WEIGHT_VERSION-finetune_lora \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 False \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --dataloader_num_workers 4 \\\r\n    --report_to wandb\r\n```\r\n**LOG**\r\n```\r\nequenceClassification model from a BertForSequenceClassification model).\r\nFormatting inputs...Skip in lazy mode\r\n/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\r\n  warnings.warn(_create_warning_msg(\r\nwandb: Currently logged in as: yerong. Use `wandb login --relogin` to force relogin\r\nwandb: Tracking run with wandb version 0.15.5\r\nwandb: Run data is saved locally in /scratch/yerong/LLaVA/wandb/run-20230721_081035-slaltz5i\r\nwandb: Run `wandb offline` to turn off syncing.\r\nwandb: Syncing run easy-darkness-41\r\nwandb: _ View project at https://wandb.ai/yerong/huggingface\r\nwandb:  View run at https://wandb.ai/yerong/huggingface/runs/slaltz5i\r\n  0%|                                                                                                                              | 0/473136 [00:00<?, ?it/s]/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\r\n  warnings.warn(_create_warning_msg(\r\n/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\r\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\nTraceback (most recent call last):\r\n  File \"/scratch/yerong/LLaVA/llava/train/train.py\", line 935, in <module>\r\n    train()\r\n  File \"/scratch/yerong/LLaVA/llava/train/train.py\", line 914, in train\r\n    trainer.train()\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1645, in train\r\n    return inner_training_loop(\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1987, in _inner_training_loop\r\n    self.accelerator.clip_grad_norm_(\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1893, in clip_grad_norm_\r\n    self.unscale_gradients()\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1856, in unscale_gradients\r\n    self.scaler.unscale_(opt)\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py\", line 284, in unscale_\r\n    optimizer_state[\"found_inf_per_device\"] = self._unscale_grads_(optimizer, inv_scale, found_inf, False)\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py\", line 212, in _unscale_grads_\r\n    raise ValueError(\"Attempting to unscale FP16 gradients.\")\r\nValueError: Attempting to unscale FP16 gradients.\r\n```\r\n\r\nScreenshots:\r\n![screenshot](https://github.com/haotian-liu/LLaVA/assets/13112023/6c61e6d2-928b-46d9-8fa7-d8bd0f6d6af8)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-23T20:57:26Z>\nUnfortunately, I do not currently have access to V100s. And from the screenshot, I am assuming that this is an issue with training with QLora.\r\n\r\nI will double check the QLora this coming week.\r\n\r\nIf you have any updates w.r.t. this, please also kindly share. Thanks!\n</Comment>\n<Comment by haotian-liu at 2023-07-24T05:46:36Z>\n@YerongLi \r\n\r\nI have updated the QLora support. Please pull the latest repo and reinstall the packages using `pip install -e .`\r\n\r\nSee example script [here](https://github.com/haotian-liu/LLaVA/blob/main/scripts/finetune_qlora.sh).\n</Comment>\n<Comment by YerongLi at 2023-08-04T06:27:19Z>\nCould you test the `--fp16` option on A100? you script uses bf16\r\n\r\nwith latest code, I am getting new error: `RuntimeError: expected scalar type Half but found Float`\r\n```\r\nwandb: _ View project at https://wandb.ai/yerong/huggingface                                                                                                  \r\nwandb:  View run at https://wandb.ai/yerong/huggingface/runs/423v20eu\r\n  0%|                                                                                                                              | 0/473136 [00:00<?, ?it/s]\r\n/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes\r\n in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that exce\r\nssive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\r\n  warnings.warn(_create_warning_msg(                                                                                                                          \r\n/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from to\r\nrch.float32 to float16 during quantization                                                                                                                    \r\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\nTraceback (most recent call last):                                                                                                                            \r\n  File \"/scratch/yerong/LLaVA/llava/train/train.py\", line 931, in <module>\r\n    train()\r\n  File \"/scratch/yerong/LLaVA/llava/train/train.py\", line 909, in train\r\n    trainer.train()\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1645, in train\r\n    return inner_training_loop(\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1938, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2770, in training_step\r\n    self.accelerator.backward(loss)\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1819, in backward\r\n    self.scaler.scale(loss).backward(**kwargs)\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\r\n    torch.autograd.backward(\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\r\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/function.py\", line 274, in apply\r\n    return user_fn(self, *args)\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 157, in backward\r\n    torch.autograd.backward(outputs_with_grad, args_with_grad)\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\r\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/function.py\", line 274, in apply\r\n    return user_fn(self, *args)\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 480, in backward\r\n    grad_A = torch.matmul(grad_output, CB).view(ctx.grad_shape).to(ctx.dtype_A) \r\nRuntimeError: expected scalar type Half but found Float\r\n```\r\n![screenshot](https://github.com/haotian-liu/LLaVA/assets/13112023/52963460-ae54-4127-b930-b2499d8906a9)\n</Comment>\n<Comment by YerongLi at 2023-08-04T06:28:30Z>\nThese two has to be false, because V100 does not support bf16.\r\n```\r\n    --bf16 False \\\r\n    --tf32 False \\\r\n```\r\n```\r\n    --fp16 True \\\r\n```\n</Comment>\n<Comment by YerongLi at 2023-08-04T12:20:56Z>\nOriginal/modified Qlora works with bf16 computing, but for GPU without bf16 computing it does not work.\r\n\r\n> @YerongLi\r\n> \r\n> I have updated the QLora support. Please pull the latest repo and reinstall the packages using `pip install -e .`\r\n> \r\n> See example script [here](https://github.com/haotian-liu/LLaVA/blob/main/scripts/finetune_qlora.sh).\n</Comment>\n<Comment by haotian-liu at 2023-08-04T17:41:19Z>\n@YerongLi We need to enable fp16 and disable bf16 both in DeepSpeed config. I have tried to change it auto in https://github.com/haotian-liu/LLaVA/commit/6b3079ea5da535599c3e56344c283adc319e532f, so that moving forward we just need to make sure fp16 is enabled and bf16 is disabled in command line, on GPUs like V100s that do not support bf16.\r\n\r\nI have tried the fp16 qlora and it works on 3090, by adding `--fp16 True --bf16 False --tf32 False`. Can you help verify this on V100? Also, can you help also verify that `--bf16 True` will still raise an error on V100?\r\n\r\nThanks.\n</Comment>\n<Comment by YerongLi at 2023-08-04T19:26:46Z>\nAre there any ways to run `train.py` directly without deepspeed?\n</Comment>\n<Comment by YerongLi at 2023-08-04T19:27:22Z>\nGood to know deepspeed works. Are there any ways to run `train.py` directly without deepspeed?\r\n> @YerongLi We need to enable fp16 and disable bf16 both in DeepSpeed config. I have tried to change it auto in [6b3079e](https://github.com/haotian-liu/LLaVA/commit/6b3079ea5da535599c3e56344c283adc319e532f), so that moving forward we just need to make sure fp16 is enabled and bf16 is disabled in command line, on GPUs like V100s that do not support bf16.\r\n> \r\n> I have tried the fp16 qlora and it works on 3090, by adding `--fp16 True --bf16 False --tf32 False`. Can you help verify this on V100? Also, can you help also verify that `--bf16 True` will still raise an error on V100?\r\n> \r\n> Thanks.\n</Comment>\n<Comment by haotian-liu at 2023-08-04T19:33:27Z>\nDo you mean single GPU training?\n</Comment>\n<Comment by YerongLi at 2023-08-04T19:34:25Z>\nYeah, because single GPU works with qlora\n</Comment>\n<Comment by haotian-liu at 2023-08-04T19:39:57Z>\nSimply remove deepspeed config from training command, and run command starting with `python` would work.\r\n\r\nAlso, just want to point out that QLoRA works with deepspeed as well.\n</Comment>\n<Comment by YerongLi at 2023-08-04T19:41:45Z>\nOkay, let me try again.\n</Comment>\n<Comment by YerongLi at 2023-08-04T19:47:21Z>\n```\r\n/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes\r\n in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that exce\r\nssive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\r\n  warnings.warn(_create_warning_msg(                                                                                                                          \r\nwandb: Currently logged in as: yerong. Use `wandb login --relogin` to force relogin                                                \r\nwandb: wandb version 0.15.8 is available!  To upgrade, please run:\r\nwandb:  $ pip install wandb --upgrade                                          \r\nwandb: Tracking run with wandb version 0.15.5                                                                                                                 \r\nwandb: Run data is saved locally in /scratch/yerong/LLaVA/wandb/run-20230804_144504-jbe7nnj3\r\nwandb: Run `wandb offline` to turn off syncing.                                                                                                               \r\nwandb: Syncing run stoic-frost-51                                              \r\nwandb: _ View project at https://wandb.ai/yerong/huggingface\r\nwandb:  View run at https://wandb.ai/yerong/huggingface/runs/jbe7nnj3\r\n  0%|                                                                                                                              | 0/157712 [00:00<?, ?it/s]\r\n/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes\r\n in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that exce\r\nssive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\r\n  warnings.warn(_create_warning_msg(\r\nTraceback (most recent call last):\r\n  File \"/scratch/yerong/LLaVA/llava/train/train.py\", line 931, in <module>\r\n    train()\r\n  File \"/scratch/yerong/LLaVA/llava/train/train.py\", line 909, in train\r\n    trainer.train()\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1867, in _inner_training_loop\r\n    self.accelerator.clip_grad_norm_(\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1925, in clip_grad_norm_\r\n    self.unscale_gradients()\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1888, in unscale_gradients\r\n    self.scaler.unscale_(opt)\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py\", line 284, in unscale_\r\n    optimizer_state[\"found_inf_per_device\"] = self._unscale_grads_(optimizer, inv_scale, found_inf, False)\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py\", line 212, in _unscale_grads_\r\n    raise ValueError(\"Attempting to unscale FP16 gradients.\")\r\nValueError: Attempting to unscale FP16 gradients.\r\n```\r\n\r\n> Simply remove deepspeed config from training command, and run command starting with `python` would work.\r\n> \r\n> Also, just want to point out that QLoRA works with deepspeed as well.\r\n\r\nI pip install -e all over and tested the following script:\r\n```\r\n#!/bin/bash\r\n\r\nWEIGHT_VERSION=v1-1\r\nPROMPT_VERSION=v1\r\nMODEL_VERSION=\"7b\"\r\n\r\npython llava/train/train.py \\\r\n    --lora_enable True \\\r\n    --bits 4 \\\r\n    --model_name_or_path checkpoints/vicuna-7b-v1.1 \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path ./playground/data/llava_instruct_150k.json \\\r\n    --image_folder /scratch/yerong/Multimodal-GPT/data/coco/train2017 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/mm_projector/LLaVA-7b-pretrain-projector-v0-CC3M-595K-original_caption.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --fp16 True \\\r\n    --bf16 False \\\r\n    --output_dir ./checkpoints/llava-$MODEL_VERSION-finetune_lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 False \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --dataloader_num_workers 4 \\\r\n    --report_to wandb\r\n```\n</Comment>\n<Comment by haotian-liu at 2023-08-04T20:25:05Z>\nI also found the same thing, but do not find an obvious way to resolve this. Do you need to actually train the model with a single GPU? Or you just want to debug?\n</Comment>\n<Comment by YerongLi at 2023-08-04T20:27:01Z>\nTrain it on single GPU will work on A100 with qlora (And it is affordable), while on V100 it does not work because of this `--fp16` bug\n</Comment>\n<Comment by haotian-liu at 2023-08-04T20:30:55Z>\nNot sure what the exact cause is, but if you want to train with a single GPU, a simple workaround is to train on a single GPU using DeepSpeed :)\n</Comment>\n<Comment by YerongLi at 2023-08-04T20:33:08Z>\nYeah not sure what is happening, I mean deepspeed works 4x slower than directly run with the train_mem.py (bf16) on A100, not sure whether it will work with fp16 V100.\n</Comment>\n<Comment by haotian-liu at 2023-08-04T20:34:36Z>\nWhich configuration are you comparing with on A100? +/- DeepSpeed, +bf16,+train_mem, +qlora?\n</Comment>\n<Comment by YerongLi at 2023-08-04T20:48:37Z>\ndeepspeed works indeed :)\r\n\r\n> Not sure what the exact cause is, but if you want to train with a single GPU, a simple workaround is to train on a single GPU using DeepSpeed :)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 293,
    "state": "closed",
    "created_by": "YerongLi",
    "created_at": "2023-07-21T09:55:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/293</URL>\n\n<TITLE>transformers version</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\n\r\nwhat transformer version you are using?\r\n\r\nwith newer transformers I got this error: https://github.com/haotian-liu/LLaVA/issues/291#issue-1815389449\r\nwhile with old transformers the Qlora does not work and most of and the `load_in_4bit is not` functioning\r\n```\r\nHelp on function __init__ in module transformers.utils.quantization_config:\r\n\r\n__init__(self, load_in_8bit=False, llm_int8_threshold=6.0, llm_int8_skip_modules=None, llm_int8_enable_fp32_cpu_offload=False, **kwargs)\r\n    Initialize self.  See help(type(self)) for accurate signature.\r\n(END)\r\n\r\n```</BODY>\n\n<COMMENTS>\n<Comment by YerongLi at 2023-07-21T11:21:09Z>\nhttps://github.com/haotian-liu/LLaVA/issues/291#issue-1815389449 is now fixed\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 292,
    "state": "closed",
    "created_by": "YerongLi",
    "created_at": "2023-07-21T09:00:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/292</URL>\n\n<TITLE>low_cpu_mem_usage iccurs NotImplementedError: Cannot copy out of meta tensor; no data!</TITLE>\n\n<BODY>Could you help us inplment a low cpu version? I have a multiple A10 GPU with 24 GB memory while my CPU memory is low.\r\n\r\n### When did you clone our code?\r\n\r\nI cloned the code base after 5/1/23\r\n\r\n### Describe the issue\r\n\r\nIssue:\r\n`low_cpu_mem_usage` iccurs NotImplementedError: Cannot copy out of meta tensor; no data!\r\nCommand:\r\n```\r\n# train.py\r\n    if model_args.vision_tower is not None:\r\n        if 'mpt' in model_args.model_name_or_path:\r\n            model = LlavaMPTForCausalLM.from_pretrained(\r\n                model_args.model_name_or_path,\r\n                cache_dir=training_args.cache_dir,\r\n                # low_cpu_mem_usage=True,\r\n                **bnb_model_from_pretrained_args\r\n            )\r\n        else:\r\n            model = LlavaLlamaForCausalLM.from_pretrained(\r\n                model_args.model_name_or_path,\r\n                cache_dir=training_args.cache_dir,\r\n                low_cpu_mem_usage=True,\r\n                **bnb_model_from_pretrained_args\r\n            )\r\n    else:\r\n        model = transformers.LlamaForCausalLM.from_pretrained(\r\n            model_args.model_name_or_path,\r\n            cache_dir=training_args.cache_dir,\r\n            low_cpu_mem_usage=True,\r\n            **bnb_model_from_pretrained_args\r\n        )\r\n```\r\n```\r\n#!/bin/bash\r\n\r\nWEIGHT_VERSION=v1-1\r\nPROMPT_VERSION=v1\r\nMODEL_VERSION=\"7b\"\r\n```\r\n```\r\npython llava/train/train_mem.py \\\r\n    --lora_enable True \\\r\n    --model_name_or_path checkpoints/vicuna-7b-v1.1 \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path ./playground/data/llava_instruct_150k.json \\\r\n    --image_folder /scratch/yerong/Multimodal-GPT/data/coco/train2017 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/mm_projector/LLaVA-7b-pretrain-projector-v0-CC3M-595K-original_caption.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/deepspeed_llava-$MODEL_VERSION-$WEIGHT_VERSION-finetune_lora \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --dataloader_num_workers 4 \\\r\n    --report_to wandb\r\n```\r\n\r\nLog: \r\n```\r\n  File \"/scratch/yerong/LLaVA/llava/train/train.py\", line 809, in train                                                                               [29/482]\r\n    trainer = LLaVATrainer(model=model,                                                                                                                       \r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 478, in __init__                                        \r\n    self._move_model_to_device(model, args.device)                                                                                                            \r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 725, in _move_model_to_device                           \r\n    model = model.to(device)                                                                                                                                  \r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1145, in to                                          \r\n    return self._apply(convert)                                                                                                                               \r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 797, in _apply                                       \r\n    module._apply(fn)                                                                                                                                         \r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 797, in _apply                                       \r\n    module._apply(fn)\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 797, in _apply\r\n    module._apply(fn)\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 820, in _apply\r\n    param_applied = fn(param)\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1143, in convert\r\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\r\nNotImplementedError: Cannot copy out of meta tensor; no data!\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by YerongLi at 2023-07-21T13:35:36Z>\nlow_cpu_mem_usage now works\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 291,
    "state": "closed",
    "created_by": "YerongLi",
    "created_at": "2023-07-21T08:27:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/291</URL>\n\n<TITLE>Not compatible with latest /transformers/models/llama/modeling_llama.py : keyword argument 'position_ids'</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\nThis code base uses transformers==4.28.0.dev0\r\nCommand:\r\n```\r\ntraining command\r\n```\r\n\r\nLog: \r\n```\r\n    return module(*inputs, output_attentions, None)                                                                                                  [174/595]\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl                                  \r\n    return forward_call(*args, **kwargs)                                                                                                                      \r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward                                         \r\n    output = old_forward(*args, **kwargs)                                                                                                                     \r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 413, in forward                     \r\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(                                                                                     \r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl                                  \r\n    return forward_call(*args, **kwargs)                                                                                                                      \r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\nTypeError: forward() got an unexpected keyword argument 'position_ids'\r\n\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by YerongLi at 2023-07-21T11:11:37Z>\nNow this error is fixed with 6cea223532a7ab7bda8116336c59772faccdcbca version\n</Comment>\n<Comment by Darren-greenhand at 2023-08-18T08:16:04Z>\nHi，I met the same problem in the latest version\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 290,
    "state": "closed",
    "created_by": "YerongLi",
    "created_at": "2023-07-21T07:45:27Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/290</URL>\n\n<TITLE>OOM on V100 GPU</TITLE>\n\n<BODY>### When did you clone our code?\r\n\r\nI cloned the code base after 5/1/23\r\n\r\n### Describe the issue\r\n\r\nIssue:\r\nV100 GPU got error without **flash_attn**\r\nCommand:\r\n```\r\n#!/bin/bash\r\n\r\nWEIGHT_VERSION=v1-1\r\nPROMPT_VERSION=v1\r\nMODEL_VERSION=\"7b\"\r\n\r\npython llava/train/train.py \\\r\n    --lora_enable True \\\r\n    --model_name_or_path checkpoints/vicuna-7b-v1.1 \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path ./playground/data/llava_instruct_150k.json \\\r\n    --image_folder /scratch/yerong/Multimodal-GPT/data/coco/train2017 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/mm_projector/LLaVA-7b-pretrain-projector-v0-CC3M-595K-original_caption.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --bf16 False \\\r\n    --output_dir ./checkpoints/deepspeed_llava-$MODEL_VERSION-$WEIGHT_VERSION-finetune_lora \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 False \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --dataloader_num_workers 4 \\\r\n    --report_to wandb\r\n\r\n```\r\n**LOG**\r\n```\r\nWARNING:root:Formatting inputs...Skip in lazy mode                                                                                                     [0/320]\r\n/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes\r\n in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that exce\r\nssive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\r\n  warnings.warn(_create_warning_msg(\r\nwandb: Currently logged in as: yerong. Use `wandb login --relogin` to force relogin                                   \r\nwandb: Tracking run with wandb version 0.15.5\r\nwandb: Run data is saved locally in /scratch/yerong/LLaVA/wandb/run-20230721_031057-pupvktt6                                                                  \r\nwandb: Run `wandb offline` to turn off syncing.                                                                                                               \r\nwandb: Syncing run good-serenity-7                                             \r\nwandb: _ View project at https://wandb.ai/yerong/huggingface                                                                                                  \r\nwandb:  View run at https://wandb.ai/yerong/huggingface/runs/pupvktt6                                                                                         \r\n  0%|                                                                                                                              | 0/473136 [00:00<?, ?it/s]\r\n/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes\r\n in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that exce\r\nssive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\r\n  warnings.warn(_create_warning_msg(\r\nTraceback (most recent call last):\r\n  File \"/scratch/yerong/LLaVA/llava/train/train.py\", line 837, in <module>\r\n    train()\r\n  File \"/scratch/yerong/LLaVA/llava/train/train.py\", line 817, in train\r\n    trainer.train()\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1644, in train\r\n    return inner_training_loop(\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1978, in _inner_training_loop\r\n    self.optimizer.step()\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/optim/lr_scheduler.py\", line 69, in wrapper\r\n    return wrapped(*args, **kwargs)\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 280, in wrapper\r\n    out = func(*args, **kwargs)\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 33, in _use_grad\r\n    ret = func(self, *args, **kwargs)\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py\", line 160, in step\r\n    self._init_group(\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/torch/optim/adamw.py\", line 114, in _init_group\r\n    state[\"exp_avg\"] = torch.zeros_like(\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 502.00 MiB (GPU 0; 31.75 GiB total capacity; 30.16 GiB already allocated; 319.69 MiB free; \r\n30.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for\r\n Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-24T05:54:11Z>\nUnfortunately V100 does not have flash-attn support yet and we do not have control over that. Two possibilities:\r\n\r\n1. Use [cpu offloading](https://github.com/haotian-liu/LLaVA/blob/main/scripts/zero3_offload.json).\r\n2. Use [QLora](https://github.com/haotian-liu/LLaVA/blob/main/scripts/finetune_qlora.sh)\r\n\r\nYou can try to see which one works, and it would be great if you can send the feedback here (I do not have access to V100s currently), so that others from community who owns V100 GPUs can have a sense of what would work.\r\n\r\nThanks!\n</Comment>\n<Comment by wileewang at 2023-10-03T19:31:26Z>\nHi, do you solve the OOM problem? I am also trying to fine-tune llava on v100 gpus. When I use cpu offloading with multiple gpus, it seems all processes are stuck and trainning is not in progress\n</Comment>\n<Comment by guanlaoda at 2023-10-07T04:23:37Z>\nhttps://github.com/haotian-liu/LLaVA/pull/411 with xformers on V100\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 289,
    "state": "closed",
    "created_by": "YerongLi",
    "created_at": "2023-07-21T06:34:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/289</URL>\n\n<TITLE>Can you enable low_cpu_mem_usage mode</TITLE>\n\n<BODY>I am getting error `ValueError: Unexpected embed_tokens_weight shape. Pretrained: torch.Size([32004, 4096]). `\r\n```\r\nI am getting this error without deepspeed:\r\n```\r\n This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a[74/1988]\r\nequenceClassification model from a BertForSequenceClassification model).                                                                                      \r\nTraceback (most recent call last):                                                                                                                            \r\n  File \"/scratch/yerong/LLaVA/llava/train/train_mem.py\", line 13, in <module>                                                                                 \r\n    train()                                                                                                                                                   \r\n  File \"/scratch/yerong/LLaVA/llava/train/train.py\", line 771, in train\r\n    model.initialize_vision_tokenizer(mm_use_im_start_end=model_args.mm_use_im_start_end, tokenizer=tokenizer, device=training_args.device,\r\n  File \"/scratch/yerong/LLaVA/llava/model/llava.py\", line 343, in initialize_vision_tokenizer\r\n    raise ValueError(f\"Unexpected embed_tokens_weight shape. Pretrained: {embed_tokens_weight.shape}. Current: {input_embeddings.shape}. Numer of new tokens: \r\n{num_new_tokens}.\")\r\nValueError: Unexpected embed_tokens_weight shape. Pretrained: torch.Size([32004, 4096]). Current: torch.Size([32003, 4096]). Numer of new tokens: 2.\r\n\r\n```\r\nThe command I am using is this:\r\n```\r\npython llava/train/train_mem.py \\\r\n    --lora_enable True \\\r\n    --model_name_or_path checkpoints/vicuna-7b-v1.1 \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path ./playground/data/llava_instruct_150k.json \\\r\n    --image_folder /scratch/yerong/Multimodal-GPT/data/coco/train2017 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/mm_projector/llava-7b-pretrain.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/deepspeed_llava-$MODEL_VERSION-$WEIGHT_VERSION-finetune_lora \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --dataloader_num_workers 4 \\\r\n    --report_to wandb\r\n```</BODY>\n\n<COMMENTS>\n<Comment by YerongLi at 2023-07-21T07:09:39Z>\nThis is caused by building the projector ourselves:\r\n- Extract projector features from the pretrained model from the feature alignment stage. \r\n```\r\npython scripts/extract_mm_projector.py \\\r\n  --model_name_or_path ./checkpoints/llava-13b-pretrain \\\r\n  --output ./checkpoints/mm_projector/llava-13b-pretrain.bin\r\n```\r\nWith https://huggingface.co/liuhaotian/LLaVA-Pretrained-Projectors/blob/main/LLaVA-7b-pretrain-projector-v0-CC3M-595K-original_caption.bin\r\nthere is no error.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 288,
    "state": "open",
    "created_by": "findalexli",
    "created_at": "2023-07-21T06:00:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/288</URL>\n\n<TITLE>[Usage]  Inference Issue using latest deep speed script On llama-2 fine-tuned model</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\n\r\nI am trying to run some inference using the provided llama-2 13-chat fine-tuned model which I downloaded from huggingface and placed in a checkpoints folder. I ran into some other issue using the older model_vqa script (the image process is NULL) , so switched to this script \r\n\r\nCommand:\r\n```\r\npython -m llava.eval.model_vqa_ds     --model-path /home/ubuntu/LLaVA/checkpoints/llava-llama-2-13b-chat-lightning-preview \r\n```\r\n\r\nLog: \r\n```\r\n/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1571:72: warning: narrowing conversion of ‘mlp_1_out_neurons’ from ‘const size_t’ {aka ‘const long unsigned int’} to ‘long int’ [-Wnarrowing]\r\nninja: build stopped: subcommand failed.\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1893, in _run_ninja_build\r\n    subprocess.run(\r\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/subprocess.py\", line 526, in run\r\n    raise CalledProcessError(retcode, process.args,\r\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/ubuntu/LLaVA/llava/eval/model_vqa_ds.py\", line 113, in <module>\r\n    eval_model(args)\r\n  File \"/home/ubuntu/LLaVA/llava/eval/model_vqa_ds.py\", line 39, in eval_model\r\n    model = deepspeed.init_inference(model, mp_size=1, dtype=torch.half, replace_with_kernel_inject=True)\r\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/deepspeed/__init__.py\", line 333, in init_inference\r\n    engine = InferenceEngine(model, config=ds_inference_config)\r\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/deepspeed/inference/engine.py\", line 192, in __init__\r\n    self._apply_injection_policy(config)\r\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/deepspeed/inference/engine.py\", line 426, in _apply_injection_policy\r\n    replace_transformer_layer(client_module, self.module, checkpoint, config, self.config)\r\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/deepspeed/module_inject/replace_module.py\", line 537, in replace_transformer_layer\r\n    replaced_module = replace_module(model=model,\r\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/deepspeed/module_inject/replace_module.py\", line 780, in replace_module\r\n    replaced_module, _ = _replace_module(model, policy, state_dict=sd)\r\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/deepspeed/module_inject/replace_module.py\", line 861, in _replace_module\r\n    _, layer_id = _replace_module(child,\r\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/deepspeed/module_inject/replace_module.py\", line 861, in _replace_module\r\n    _, layer_id = _replace_module(child,\r\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/deepspeed/module_inject/replace_module.py\", line 837, in _replace_module\r\n    replaced_module = policies[child.__class__][0](child,\r\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/deepspeed/module_inject/replace_module.py\", line 514, in replace_fn\r\n    new_module = replace_with_policy(child,\r\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/deepspeed/module_inject/replace_module.py\", line 348, in replace_with_policy\r\n    _container.create_module()\r\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/deepspeed/module_inject/containers/llama.py\", line 36, in create_module\r\n    self.module = DeepSpeedGPTInference(_config, mp_group=self.mp_group)\r\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/deepspeed/model_implementations/transformers/ds_gpt.py\", line 20, in __init__\r\n    super().__init__(config, mp_group, quantize_scales, quantize_groups, merge_count, mlp_extra_grouping)\r\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/deepspeed/model_implementations/transformers/ds_transformer.py\", line 54, in __init__\r\n    inference_module = builder.load()\r\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/deepspeed/ops/op_builder/builder.py\", line 454, in load\r\n    return self.jit_load(verbose)\r\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/deepspeed/ops/op_builder/builder.py\", line 497, in jit_load\r\n    op_module = load(name=self.name,\r\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1284, in load\r\n    return _jit_compile(\r\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1509, in _jit_compile\r\n    _write_ninja_file_and_build_library(\r\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1624, in _write_ninja_file_and_build_library\r\n    _run_ninja_build(\r\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1909, in _run_ninja_build\r\n    raise RuntimeError(message) from e\r\nRuntimeError: Error building extension 'transformer_inference'```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-24T06:02:43Z>\nHi, the deepspeed inference script was committed accidentally when I was debugging it. Please use `model_vqa` for now.\r\n\r\nUnfortunately, I do not find using deepspeed inference help with the speed so still investigating. If you have any insights regarding this, please kindly share. Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 286,
    "state": "closed",
    "created_by": "kznrluk",
    "created_at": "2023-07-21T04:47:59Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/286</URL>\n\n<TITLE>[Usage] Error occurs on WebServer startup: \"Exception in ASGI application\"</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\n\r\nIn my environment (Arch Linux and Windows 11), when I start the Web server and access it from the browser, I encounter the following error. As a result, the chat window doesn't display, and I cannot use it properly.\r\n\r\nTo address this issue, I manually installed `gradio_client` based on information found online, and now it works fine. Perhaps the definition in `pyproject.toml` might be incomplete.\r\n\r\n```\r\npip install gradio_client==0.2.7\r\n```\r\n\r\nI also tried both the latest commit `6cea223` and the commit before Release v1.0.0 `7ace501`, but encountered the same error.\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload --port 7862\r\n```\r\nLog: \r\n```\r\npython -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload --port 7862\r\n[2023-07-21 13:29:47,707] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n2023-07-21 13:29:48 | INFO | gradio_web_server | args: Namespace(host='0.0.0.0', port=7862, controller_url='http://localhost:10000', concurrency_count=8, model_list_mode='reload', share=False, moderate=False, embed=False)\r\n2023-07-21 13:29:48 | INFO | gradio_web_server | Models: ['LLaVa-7B-v0']\r\n2023-07-21 13:29:48 | INFO | gradio_web_server | Namespace(host='0.0.0.0', port=7862, controller_url='http://localhost:10000', concurrency_count=8, model_list_mode='reload', share=False, moderate=False, embed=False)\r\nchannel 3: open failed: connect failed: Connection refused\r\n2023-07-21 13:29:50 | INFO | stdout | Running on local URL:  http://0.0.0.0:7862\r\n2023-07-21 13:29:50 | INFO | stdout |\r\n2023-07-21 13:29:50 | INFO | stdout | To create a public link, set `share=True` in `launch()`.\r\n2023-07-21 13:29:50 | ERROR | stderr | ERROR:    Exception in ASGI application\r\n2023-07-21 13:29:50 | ERROR | stderr | Traceback (most recent call last):\r\n2023-07-21 13:29:50 | ERROR | stderr |   File \"/home/kznrluk/.conda/envs/llava/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py\", line 408, in run_asgi\r\n2023-07-21 13:29:50 | ERROR | stderr |     result = await app(  # type: ignore[func-returns-value]\r\n2023-07-21 13:29:50 | ERROR | stderr |   File \"/home/kznrluk/.conda/envs/llava/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\r\n2023-07-21 13:29:50 | ERROR | stderr |     return await self.app(scope, receive, send)\r\n2023-07-21 13:29:50 | ERROR | stderr |   File \"/home/kznrluk/.conda/envs/llava/lib/python3.10/site-packages/fastapi/applications.py\", line 289, in __call__\r\n2023-07-21 13:29:50 | ERROR | stderr |     await super().__call__(scope, receive, send)\r\n2023-07-21 13:29:50 | ERROR | stderr |   File \"/home/kznrluk/.conda/envs/llava/lib/python3.10/site-packages/starlette/applications.py\", line 122, in __call__\r\n2023-07-21 13:29:50 | ERROR | stderr |     await self.middleware_stack(scope, receive, send)\r\n2023-07-21 13:29:50 | ERROR | stderr |   File \"/home/kznrluk/.conda/envs/llava/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 184, in __call__\r\n2023-07-21 13:29:50 | ERROR | stderr |     raise exc\r\n2023-07-21 13:29:50 | ERROR | stderr |   File \"/home/kznrluk/.conda/envs/llava/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 162, in __call__\r\n2023-07-21 13:29:50 | ERROR | stderr |     await self.app(scope, receive, _send)\r\n2023-07-21 13:29:50 | ERROR | stderr |   File \"/home/kznrluk/.conda/envs/llava/lib/python3.10/site-packages/starlette/middleware/cors.py\", line 83, in __call__\r\n2023-07-21 13:29:50 | ERROR | stderr |     await self.app(scope, receive, send)\r\n2023-07-21 13:29:50 | ERROR | stderr |   File \"/home/kznrluk/.conda/envs/llava/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\r\n2023-07-21 13:29:50 | ERROR | stderr |     raise exc\r\n2023-07-21 13:29:50 | ERROR | stderr |   File \"/home/kznrluk/.conda/envs/llava/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\r\n2023-07-21 13:29:50 | ERROR | stderr |     await self.app(scope, receive, sender)\r\n2023-07-21 13:29:50 | ERROR | stderr |   File \"/home/kznrluk/.conda/envs/llava/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 20, in __call__\r\n2023-07-21 13:29:50 | ERROR | stderr |     raise e\r\n2023-07-21 13:29:50 | ERROR | stderr |   File \"/home/kznrluk/.conda/envs/llava/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 17, in __call__\r\n2023-07-21 13:29:50 | ERROR | stderr |     await self.app(scope, receive, send)\r\n2023-07-21 13:29:50 | ERROR | stderr |   File \"/home/kznrluk/.conda/envs/llava/lib/python3.10/site-packages/starlette/routing.py\", line 718, in __call__\r\n2023-07-21 13:29:50 | ERROR | stderr |     await route.handle(scope, receive, send)\r\n2023-07-21 13:29:50 | ERROR | stderr |   File \"/home/kznrluk/.conda/envs/llava/lib/python3.10/site-packages/starlette/routing.py\", line 276, in handle\r\n2023-07-21 13:29:50 | ERROR | stderr |     await self.app(scope, receive, send)\r\n2023-07-21 13:29:50 | ERROR | stderr |   File \"/home/kznrluk/.conda/envs/llava/lib/python3.10/site-packages/starlette/routing.py\", line 66, in app\r\n2023-07-21 13:29:50 | ERROR | stderr |     response = await func(request)\r\n2023-07-21 13:29:50 | ERROR | stderr |   File \"/home/kznrluk/.conda/envs/llava/lib/python3.10/site-packages/fastapi/routing.py\", line 273, in app\r\n2023-07-21 13:29:50 | ERROR | stderr |     raw_response = await run_endpoint_function(\r\n2023-07-21 13:29:50 | ERROR | stderr |   File \"/home/kznrluk/.conda/envs/llava/lib/python3.10/site-packages/fastapi/routing.py\", line 192, in run_endpoint_function\r\n2023-07-21 13:29:50 | ERROR | stderr |     return await run_in_threadpool(dependant.call, **values)\r\n2023-07-21 13:29:50 | ERROR | stderr |   File \"/home/kznrluk/.conda/envs/llava/lib/python3.10/site-packages/starlette/concurrency.py\", line 41, in run_in_threadpool\r\n2023-07-21 13:29:50 | ERROR | stderr |     return await anyio.to_thread.run_sync(func, *args)\r\n2023-07-21 13:29:50 | ERROR | stderr |   File \"/home/kznrluk/.conda/envs/llava/lib/python3.10/site-packages/anyio/to_thread.py\", line 33, in run_sync\r\n2023-07-21 13:29:50 | ERROR | stderr |     return await get_asynclib().run_sync_in_worker_thread(\r\n2023-07-21 13:29:50 | ERROR | stderr |   File \"/home/kznrluk/.conda/envs/llava/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\r\n2023-07-21 13:29:50 | ERROR | stderr |     return await future\r\n2023-07-21 13:29:50 | ERROR | stderr |   File \"/home/kznrluk/.conda/envs/llava/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\r\n2023-07-21 13:29:50 | ERROR | stderr |     result = context.run(func, *args)\r\n2023-07-21 13:29:50 | ERROR | stderr |   File \"/home/kznrluk/.conda/envs/llava/lib/python3.10/site-packages/gradio/routes.py\", line 289, in api_info\r\n2023-07-21 13:29:50 | ERROR | stderr |     return gradio.blocks.get_api_info(config, serialize)  # type: ignore\r\n2023-07-21 13:29:50 | ERROR | stderr |   File \"/home/kznrluk/.conda/envs/llava/lib/python3.10/site-packages/gradio/blocks.py\", line 518, in get_api_info\r\n2023-07-21 13:29:50 | ERROR | stderr |     serializer = serializing.COMPONENT_MAPPING[type]()\r\n2023-07-21 13:29:50 | ERROR | stderr | KeyError: 'dataset'\r\n```</BODY>\n\n<COMMENTS>\n<Comment by kznrluk at 2023-07-21T05:00:28Z>\nAfter re-cloning the repository and recreating the conda environment, I added `gradio_client==0.2.7` to `pyproject.toml`. Following the instructions in README.md, everything now starts up without any issues. I have submitted a pull request, so please review it at your convenience.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/pull/287\n</Comment>\n<Comment by haotian-liu at 2023-07-24T05:59:48Z>\nThanks for your feedback! I'll close this issue for now, and we can discuss in the thread within your PR. Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 285,
    "state": "closed",
    "created_by": "RyanLiu112",
    "created_at": "2023-07-21T02:57:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/285</URL>\n\n<TITLE>[Question] How to input video?</TITLE>\n\n<BODY>### Question\n\nThanks for the great work!\r\n\r\nCan LLaVA take videos or multiple video frames (e.g., 4 frames) as inputs without the addition of temporal processing layers? I'm interested in using LLaVA to generate descriptions for videos and multiple images, but I'm unsure about how to implement this. Could you please provide some advice? I appreciate your help in advance.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-24T06:05:02Z>\nHi @RyanLiu112 \r\n\r\nThank you for your interest. Currently LLaVA does not support processing videos yet, but I can imagine this would require pretraining and instruction tuning at least on some video clips.\r\n\r\nYou can potentially start from the pretrained LLaVA checkpoints and perform both pretraining and instruction tuning and see how it works.\n</Comment>\n<Comment by RyanLiu112 at 2023-07-26T06:23:59Z>\nThanks for your response!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 284,
    "state": "open",
    "created_by": "RajeshRadha",
    "created_at": "2023-07-20T19:01:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/284</URL>\n\n<TITLE>[Question] Training with 4 RTX 4090</TITLE>\n\n<BODY>### Question Training with 4 RTX 4090\r\n\r\nCan I train and fine tune LlaVa model w/ model config as 13B Llama model + vision encoder (default) + connector module with workstation that has 4 RTX 4090?</BODY>\n\n<COMMENTS>\n<Comment by YerongLi at 2023-07-21T20:21:58Z>\nDid you succeed in training with 4 x RTX 4090?  I tried A10 GPU with 24 GB and received OOM.\n</Comment>\n<Comment by JoeLeelyf at 2023-07-22T13:04:24Z>\nI'm also working on training with 8 RTX 3090 with 24GB each these days, but also received OOM. I have tried to use deepspeed to solve the problem, getting about 6GB mem usage on each of my GPU when finishing loading the model and data, but received \"RuntimeError: is_sm80 || is_sm90 to be true, but got false\". It seems that flash-attention doesn't support 3090/4090 yet. I also tried to use run train.py instead of train_mem.py to avoid using flash-attein, but got OOM in torchrun script and \"AttributeError: 'DummyScheduler' object has no attribute 'step'\" when using deepspeed.\n</Comment>\n<Comment by haotian-liu at 2023-07-22T14:53:47Z>\n@RajeshRadha @YerongLi You can use [`zero3_offload.json`](https://github.com/haotian-liu/LLaVA/blob/main/scripts/zero3_offload.json) to further save gpu ram by offloading parameters / optimizer states to CPU.\r\n\r\n@JoeLeelyf you may try reinstalling `flash-attn` by `pip install flash-attn --no-build-isolation`. It works for my 3090s.\n</Comment>\n<Comment by RajeshRadha at 2023-07-24T21:43:03Z>\n@haotian-liu Thanks for the suggestion, How much CPU RAM is expected during offloading and do you see any performance degradation during training?\n</Comment>\n<Comment by haotian-liu at 2023-07-24T22:55:00Z>\n@RajeshRadha \r\n\r\nI think you can view CPU RAM as an extension of GPU VRAM, the same amount offloaded from GPU will be on CPU RAM. You can benchmark the exact ratio.\r\n\r\nThere will be no degradation in terms of accuracy, but definitely slower than having all parameters/states on GPU. Degradation in training speed varies case by case, and you can benchmark on your workstation.\n</Comment>\n<Comment by MiroFurtado at 2023-07-28T03:36:10Z>\nHow much CPU ram do you typically require? I am getting OOM before anything is even transferred to GPU but I have very weird machine with 100 GB GPU RAM & only 100 GB CPU RAM\n</Comment>\n<Comment by JoeLeelyf at 2023-07-28T15:05:58Z>\n@haotian-liu Thanks for the suggestion. I have successfully reproduced LLaVA on 4*GTX 3090. I found that the real problem lies in zero3_offload.json, which lacks the configuration of \"scheduler\". After assigning it to \"type\":\"WarmupLR\", the program runs successfully.\r\n\r\n\"scheduler\": {\r\n    \"type\": \"WarmupLR\",\r\n    \"params\": {\r\n        \"warmup_min_lr\": \"auto\",\r\n        \"warmup_max_lr\": \"auto\",\r\n        \"warmup_num_steps\": \"auto\"\r\n    }\r\n },\n</Comment>\n<Comment by MiroFurtado at 2023-07-28T18:08:15Z>\n> @haotian-liu Thanks for the suggestion. I have successfully reproduced LLaVA on 4*GTX 3090. I found that the real problem lies in zero3_offload.json, which lacks the configuration of \"scheduler\". After assigning it to \"type\":\"WarmupLR\", the program runs successfully.\r\n> \r\n> \"scheduler\": { \"type\": \"WarmupLR\", \"params\": { \"warmup_min_lr\": \"auto\", \"warmup_max_lr\": \"auto\", \"warmup_num_steps\": \"auto\" } },\r\n\r\nCheers - that was also necessary to make it work for me.\n</Comment>\n<Comment by haotian-liu at 2023-07-29T03:33:20Z>\n@JoeLeelyf Thanks for the info, I just updated the scripts in the repo :)\n</Comment>\n<Comment by haotian-liu at 2023-07-29T03:33:45Z>\n> How much CPU ram do you typically require? I am getting OOM before anything is even transferred to GPU but I have very weird machine with 100 GB GPU RAM & only 100 GB CPU RAM\r\n\r\nUsually zero3 will use fewer CPU RAM when loading.\n</Comment>\n<Comment by MiroFurtado at 2023-08-03T14:22:45Z>\nUnfortunately, with 193 GB RAM and zero3_offload, while the pre-training works, I cannot find enough RAM to get the `finetune.sh` working.\r\n<img width=\"345\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/6306695/621b8392-d804-4488-9e9d-04f9316a7143\">\n</Comment>\n<Comment by haotian-liu at 2023-08-05T05:32:34Z>\n@MiroFurtado Are you trying to finetune a 7B or 13B model? And are you trying to do full finetuning or lora tuning? Maybe you can try LoRA tuning with a smaller model.\n</Comment>\n<Comment by unnikrishnanrnair at 2023-11-18T03:52:44Z>\n> @haotian-liu Thanks for the suggestion. I have successfully reproduced LLaVA on 4*GTX 3090. I found that the real problem lies in zero3_offload.json, which lacks the configuration of \"scheduler\". After assigning it to \"type\":\"WarmupLR\", the program runs successfully.\r\n> \r\n> \"scheduler\": { \"type\": \"WarmupLR\", \"params\": { \"warmup_min_lr\": \"auto\", \"warmup_max_lr\": \"auto\", \"warmup_num_steps\": \"auto\" } },\r\n\r\nHi! You have successfully reproduced the 7B or the 13B models results? Also is this LORA?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 283,
    "state": "open",
    "created_by": "Unispac",
    "created_at": "2023-07-19T23:56:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/283</URL>\n\n<TITLE>[Question] No roles indicator in the prompts?</TITLE>\n\n<BODY>### Question\n\nHi, I am playing with your new model built on LLaMA-2. \r\n\r\nI noticed that you are no longer adding role indicators like \"USER\" and \"ASSISTANT\" in your prompts? Is this intentional? But in this way, how could the model get to know which part of text is from which role?\r\n```\r\n        elif self.sep_style == SeparatorStyle.LLAMA_2:\r\n            wrap_sys = lambda msg: f\"<<SYS>>\\n{msg}\\n<</SYS>>\\n\\n\"\r\n            wrap_inst = lambda msg: f\"[INST] {msg} [/INST]\"\r\n            ret = \"\"\r\n\r\n            for i, (role, message) in enumerate(messages):\r\n                if i == 0:\r\n                    assert message, \"first message should not be none\"\r\n                    assert role == self.roles[0], \"first message should come from user\"\r\n                if message:\r\n                    if type(message) is tuple:\r\n                        message, _, _ = message\r\n                    if i == 0: message = wrap_sys(self.system) + message\r\n                    if i % 2 == 0:\r\n                        message = wrap_inst(message)\r\n                        ret += self.sep + message\r\n                    else:\r\n                        ret += \" \" + message + \" \" + self.sep2\r\n                else:\r\n                    ret += \"\"\r\n            ret = ret.lstrip(self.sep)\r\n```\r\n\r\nOn the other hand, for LLaVA-LLaMA-2, you are using the following prompt template, right?\r\n\r\n```\r\nconv_llava_llama_2 = Conversation(\r\n    system=\"You are a helpful language and vision assistant. \"\r\n           \"You are able to understand the visual content that the user provides, \"\r\n           \"and assist the user with a variety of tasks using natural language.\",\r\n    roles=(\"USER\", \"ASSISTANT\"),\r\n    version=\"llama_v2\",\r\n    messages=(),\r\n    offset=0,\r\n    sep_style=SeparatorStyle.LLAMA_2,\r\n    sep=\"<s>\",\r\n    sep2=\"</s>\",\r\n)\r\n```</BODY>\n\n<COMMENTS>\n<Comment by Unispac at 2023-07-20T00:06:20Z>\nic. you are always wrapping user messages with [INST] [/INST] to distinguish. That makes sense. Nice work!\n</Comment>\n<Comment by haotian-liu at 2023-07-20T00:08:18Z>\n@Unispac Yep, your understanding in the reply is correct. And just to add, this prompt design is following the LLaMA-2-Chat (this is the way this model is trained). Thanks.\n</Comment>\n<Comment by Unispac at 2023-07-20T00:15:39Z>\nGot it. Thank you very much!\r\n\r\nBtw, I have another question. For visual instruction tuning, you are using:\r\n```\r\nsystem=\"You are a helpful language and vision assistant. \"\r\n           \"You are able to understand the visual content that the user provides, \"\r\n           \"and assist the user with a variety of tasks using natural language.\",\r\n```\r\n\r\nAnd, in LLaMA-2. they use:\r\n```\r\nsystem=\"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\r\n\r\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\",\r\n```\r\n\r\nBasically, you didn't involve the alignment prompt in your system prompt. Is my understanding correct?\n</Comment>\n<Comment by haotian-liu at 2023-07-20T00:16:51Z>\nRight. In our first implementation, we do not include the alignment prompt. We'll study these and release the results to public once we have a better understanding on the effects :)\n</Comment>\n<Comment by Unispac at 2023-07-20T00:17:48Z>\nGot it. Thank you very much. \r\nNice work and very helpful. \r\nYou are moving very fast!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 282,
    "state": "open",
    "created_by": "luffycodes",
    "created_at": "2023-07-19T16:45:26Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/282</URL>\n\n<TITLE>Where do I download the training images from?[Question]</TITLE>\n\n<BODY>### Question\n\nHello, where can I download the images from in the dataset?\r\n[https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K)\r\n\r\n<img width=\"873\" alt=\"Screen Shot 2023-07-19 at 11 45 11 AM\" src=\"https://github.com/haotian-liu/LLaVA/assets/22951144/ea61349d-c0f3-4c3c-aa29-479804c94873\"></BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-19T17:39:08Z>\nHi, thank you for the interest.\r\n\r\nYou can follow the instruction here to prepare for the necessary checkpoints/data needed for training models: https://github.com/haotian-liu/LLaVA/blob/main/README.md#visual-instruction-tuning\r\n\r\n> Please download the annotation of our instruction tuning data [llava_instruct_158k.json](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_instruct_150k.json), and download the COCO train2017 images [here](https://cocodataset.org/#download).\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 281,
    "state": "closed",
    "created_by": "YerongLi",
    "created_at": "2023-07-19T10:03:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/281</URL>\n\n<TITLE>--model_name_or_path  in the training workflow : should it be vicuna</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue: scripts/deepspeed/finetune_lora.sh\r\nI think in training workflow `--model_name_or_path` should not be vicuna-7b right, it reports some error as **You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.**\r\n\r\n```\r\n--model_name_or_path ./checkpoints/vicuna-7b-v1.1 \\\r\n```\r\n\r\nI tried both ` --model_name_or_path LLaVA-7B-v0 \\` or `--model_name_or_path ./checkpoints/vicuna-7b-v1.1 \\`, neither works though\r\nCommand:\r\n```\r\n#!/bin/bash\r\n\r\nWEIGHT_VERSION=v1-1\r\nPROMPT_VERSION=v1\r\nMODEL_VERSION=\"7b\"\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/deepspeed/zero3.json \\\r\n    --lora_enable True \\\r\n    --model_name_or_path ./checkpoints/vicuna-7b-v1.1 \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path ./playground/data/llava_instruct_150k.json \\\r\n    --image_folder /scratch/yerong/Multimodal-GPT/data/coco/train2017 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/mm_projector/llava-7b-pretrain.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/deepspeed_llava-$MODEL_VERSION-$WEIGHT_VERSION-finetune_lora \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --dataloader_num_workers 4 \\\r\n    --report_to wandb.\r\n```\r\n\r\nLog: \r\n```\r\n[2023-07-19 04:52:29,958] [WARNING] [runner.py:191:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.             \r\n[2023-07-19 04:52:29,971] [INFO] [runner.py:541:main] cmd = /scratch/yerong/.conda/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2Nh\r\nbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/deepspeed/zero3.js\r\non --lora_enable True --model_name_or_path ./checkpoints/vicuna-7b-v1.1 --version v1 --data_path ./playground/data/llava_instruct_150k.json --image_folder /sc\r\nratch/yerong/Multimodal-GPT/data/coco/train2017 --vision_tower openai/clip-vit-large-patch14 --pretrain_mm_mlp_adapter ./checkpoints/mm_projector/llava-7b-pre\r\ntrain.bin --mm_vision_select_layer -2 --mm_use_im_start_end True --bf16 True --output_dir ./checkpoints/deepspeed_llava-7b-v1-1-finetune_lora --num_train_epoc\r\nhs 3 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_step\r\ns 50000 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_l\r\nength 2048 --gradient_checkpointing True --lazy_preprocess True --dataloader_num_workers 4 --report_to wandb                                                  \r\n[2023-07-19 04:52:31,695] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [0, 1]}                                                                  \r\n[2023-07-19 04:52:31,695] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=2, node_rank=0                                                                \r\n[2023-07-19 04:52:31,696] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})                                  \r\n[2023-07-19 04:52:31,696] [INFO] [launch.py:247:main] dist_world_size=2                                                                                       \r\n[2023-07-19 04:52:31,696] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=0,1                                                                        \r\n[2023-07-19 04:52:35,338] [INFO] [comm.py:622:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl                                      \r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.        \r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.        \r\n[2023-07-19 04:52:42,501] [INFO] [partition_parameters.py:454:__exit__] finished initializing model with 6.74B parameters                                     \r\nLoading checkpoint shards:   0%|                                                                                                        | 0/3 [00:00<?, ?it/s]\r\n[2023-07-19 04:52:54,728] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 49686                                                                     \r\n[2023-07-19 04:52:54,731] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 49687                                                                     \r\n[2023-07-19 04:52:55,035] [ERROR] [launch.py:434:sigkill_handler] ['/scratch/yerong/.conda/envs/llava/bin/python', '-u', 'llava/train/train_mem.py', '--local_\r\nrank=1', '--deepspeed', './scripts/deepspeed/zero3.json', '--lora_enable', 'True', '--model_name_or_path', './checkpoints/vicuna-7b-v1.1', '--version', 'v1', \r\n'--data_path', './playground/data/llava_instruct_150k.json', '--image_folder', '/scratch/yerong/Multimodal-GPT/data/coco/train2017', '--vision_tower', 'openai\r\n/clip-vit-large-patch14', '--pretrain_mm_mlp_adapter', './checkpoints/mm_projector/llava-7b-pretrain.bin', '--mm_vision_select_layer', '-2', '--mm_use_im_star\r\nt_end', 'True', '--bf16', 'True', '--output_dir', './checkpoints/deepspeed_llava-7b-v1-1-finetune_lora', '--num_train_epochs', '3', '--per_device_train_batch_\r\nsize', '4', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_step\r\ns', '50000', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--loggi\r\nng_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--lazy_preprocess', 'True', '--dataloader_num_workers', '\r\n4', '--report_to', 'wandb'] exits with return code = -7  \r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by YerongLi at 2023-07-19T10:13:36Z>\n- With this `--model_name_or_path ./checkpoints/vicuna-7b-v1.1 \\` I got error\r\n\r\n```\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.        \r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.        \r\n```\r\nWith this `--model_name_or_path LLaVA-7B-v0 \\` I got this error.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/scratch/yerong/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/scratch/yerong/LLaVA/llava/train/train.py\", line 656, in train\r\n    model = LlavaLlamaForCausalLM.from_pretrained(\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2643, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2952, in _load_pretrained_model\r\n    state_dict = load_state_dict(shard_file)\r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 431, in load_state_dict\r\n    raise OSError(\r\nOSError: Unable to load weights from pytorch checkpoint file for 'LLaVA-7B-v0/pytorch_model-00001-of-00002.bin' at 'LLaVA-7B-v0/pytorch_model-00001-of-00002.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\r\n[2023-07-19 04:54:54,848] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 49958\r\n```\n</Comment>\n<Comment by YerongLi at 2023-07-21T11:02:38Z>\nuse `--model_name_or_path ./checkpoints/vicuna-7b-v1.1`\r\n`You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.  `      \r\nThis is expected\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 280,
    "state": "closed",
    "created_by": "LukeBailey181",
    "created_at": "2023-07-18T22:08:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/280</URL>\n\n<TITLE>Llama 2 integration</TITLE>\n\n<BODY>### feature\r\n\r\nLlama 2 was released today. Would be super cool to see this integrated as the language decoder to LLaVA as it used RLHF and appears to have great performance.</BODY>\n\n<COMMENTS>\n<Comment by JulianYocum at 2023-07-18T22:11:50Z>\nThis would be excellent!\n</Comment>\n<Comment by haotian-liu at 2023-07-19T10:08:29Z>\nI am excited as well! We've updated our code base with the support for LLaMA-2, as well as some initial preview checkpoints on it :) Stay tuned for more checkpoints!\r\n\r\nCheck out: https://github.com/haotian-liu/LLaVA/blob/main/docs/LLaVA_from_LLaMA2.md\n</Comment>\n<Comment by LukeBailey181 at 2023-07-21T21:15:19Z>\nAmazing thank you! Are these previews weight deltas that we should apply to our own copies of the llama 2 weights using the same code in the repo for applying llama 1 weight deltas? That is:\r\n\r\n```\r\npython3 -m llava.model.apply_delta \\\r\n    --base /path/to/llama-2 \\\r\n    --target /output/path/to/LLaVA \\\r\n    --delta /path/to/delta\r\n```\r\n\r\nOr are the previews the straight up weights we should use?\n</Comment>\n<Comment by haotian-liu at 2023-07-23T18:48:43Z>\n@LukeBailey181 Sorry for the confusion. LLaMA-2 based checkpoints are released with merged weights, and you do not need to apply delta any more.\r\n\r\nWe've updated the docs in [MODEL ZOO](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#model-zoo). Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 279,
    "state": "open",
    "created_by": "blue-blue272",
    "created_at": "2023-07-18T12:11:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/279</URL>\n\n<TITLE>The instruction tuning data is not avaliable. [Question]</TITLE>\n\n<BODY>### Question\n\n The instruction tuning data, such as llava_instruct_158k.json, can not be downloaded. Can you update the dataset?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-19T17:39:44Z>\nHi, thank you for the interest.\r\n\r\nYou can follow the instruction here to prepare for the necessary checkpoints/data needed for training models: https://github.com/haotian-liu/LLaVA/blob/main/README.md#visual-instruction-tuning\r\n\r\n> Please download the annotation of our instruction tuning data [llava_instruct_158k.json](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_instruct_150k.json), and download the COCO train2017 images [here](https://cocodataset.org/#download).\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 278,
    "state": "open",
    "created_by": "hangzeli08",
    "created_at": "2023-07-18T10:06:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/278</URL>\n\n<TITLE>Does it support multi image input</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nDo you currently support multi image input？\r\n请问现在支持多图输入吗？</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 277,
    "state": "open",
    "created_by": "Ucas-HaoranWei",
    "created_at": "2023-07-17T09:21:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/277</URL>\n\n<TITLE>[Question] what is the different between llava-v1.1 and llava-v1.3-preview ?</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by Ucas-HaoranWei at 2023-07-17T09:27:17Z>\nHi, I want to know what is the different between llava-v1.1 and llava-v1.3-preview models? Are they utilize different sft datasets (both they use llava-150k)?\n</Comment>\n<Comment by haotian-liu at 2023-07-31T13:18:33Z>\nHi, the main difference is the (1) base checkpoint switching to vicuna v1.3, (2) image resolution to 336px, (3) schedule change.\r\n\r\nPlease check out our [model zoo](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md) for more details. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 276,
    "state": "closed",
    "created_by": "yifannnwu",
    "created_at": "2023-07-16T19:42:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/276</URL>\n\n<TITLE>[Question] How to conduct inference after finetuning using deepspeed + LoRA</TITLE>\n\n<BODY>### Question\n\nHi, thanks for this excellent work! \r\n\r\nAfter finetune using deepspeed + LoRA using officially provided script, the model was saved in ./checkpoints/deepspeed_llava-13b-v1.3-finetune_lora. When I tried to inference this model using run_llava.py, I encountered error ValueError: Unrecognized configuration class <class 'llava.model.llava.LlavaConfig'> to build an AutoTokenizer. I am wondering any other step I need to do to process the model after finetune to make it able to work? \r\n\r\nThanks in advance for any help.</BODY>\n\n<COMMENTS>\n<Comment by YerongLi at 2023-07-19T09:22:07Z>\nI would look into training workflow, print the output of the forward pass.\n</Comment>\n<Comment by yifannnwu at 2023-07-29T06:44:54Z>\nThe latest code solved this issue.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 275,
    "state": "closed",
    "created_by": "TonyXuQAQ",
    "created_at": "2023-07-15T13:00:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/275</URL>\n\n<TITLE>[Question] Running without flash-attn</TITLE>\n\n<BODY>### Question\r\n\r\nHi, I wanna know if there is any way to run LLaVA **without** flash-attn. I ran into several issues with V100 GPUs, and I found that these issues are caused by fast-attn.\r\n\r\nThanks!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-19T17:40:43Z>\nHi @TonyXuQAQ, you can run without flash-attention by replacing `train_mem.py` with `train.py` in your command. We'll add this tip into our documentation as well. Thanks!\n</Comment>\n<Comment by YerongLi at 2023-07-21T09:04:03Z>\non V100 I am still getting OOM error, https://github.com/haotian-liu/LLaVA/issues/290#issue-1815329831\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 274,
    "state": "open",
    "created_by": "simplelifetime",
    "created_at": "2023-07-15T03:51:10Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/274</URL>\n\n<TITLE>[Question] Does LLaVA support evaluation on llava_v1</TITLE>\n\n<BODY>### Question\n\nThanks for your excellent work! I'm tring to reproduce the result in your paper. I wish to use the newest LLaVA v1 checkpoint, but notice that the prompt format might be different between v0 and v1 versions. I'm wondering If the difference will lead to great inconsistentency of evaluation result on COCO 90 questions and scienceQA?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-19T17:42:11Z>\nHi @simplelifetime, thank you for your interest. I have trained a model checkpoint based on Vicuna-v1.3 following the new pretraining schedule and the ScienceQA tuning schedule, and I can reproduce the results with almost the same number as the paper (+/- 0.1%). I will share the checkpoints this week. Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 272,
    "state": "closed",
    "created_by": "SimingYan",
    "created_at": "2023-07-14T17:06:28Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/272</URL>\n\n<TITLE>[Question] Cannot reproduce fine-tuning result</TITLE>\n\n<BODY>Hi,\r\n\r\nThanks for your great work!\r\n\r\nI am truly grateful for your efforts and the work you have done so far. Currently, I'm attempting to rerun the model and reproduce the performance. After completing the fine-tuning process, I've encountered difficulties in obtaining satisfactory results during the demo evaluation.\r\n\r\nFor your convenience, I've attached my training configuration, training log, and evaluation result for your perusal:\r\n\r\nI've been using the most recent version of the code for fine-tuning, utilizing 8x40G A100. Below is a snapshot of my training config for your reference:\r\n```\r\ntorchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path ./checkpoints/vicuna/vicuna-7b-v1.3/ \\\r\n    --version v1 \\\r\n    --data_path ./data/LLaVA-Instruct-150K/llava_instruct_150k.json \\\r\n    --image_folder ./data/coco/train2017/ \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/pretrain_projector/LLaVA-Pretrained-Projectors/LLaVA-7b-pretrain-projector-v1-1-LCS-558K-blip_caption.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/oldllava-7b-finetune \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 2 \\\r\n    --per_device_eval_batch_size 2 \\\r\n    --gradient_accumulation_steps 2 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 5000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --fsdp \"full_shard auto_wrap\" \\\r\n    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\nHere is a glimpse of my training log:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/30534700/e85ed5f2-471b-4adf-ac83-6dd71e3a849f)\r\n\r\nTo me, the data and process appear reasonable; however, upon running the evaluation demo, the output was unexpectedly erroneous:\r\n```\r\npython -m llava.eval.run_llava \\\r\n    --model-name /path/to/LLaVA-13B-v0 \\\r\n    --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n    --query \"What are the things I should be cautious about when I visit here?\"\r\n```\r\nThe output is weird:\r\n```\r\nDuring the journey, it's important to read the document containing the correct spelling: \"Beng\\\"Dot\\\":\\\"B\\\" as well as an addressing the situation: \"B\\\"Beng\\\"C\\\" as expected, this means: \"B\\\"Beng\\\"C\\\" and this is the correct line: \"B\\\"B\\\"as they have been doing the hiding, expected: \"B\\\"B\\\"but the Forgod, stopping the conversation, and have a low probability of- calb, this means: \"B\\\"F\\\"during the exchange, a, and b, which means: \"B\\\"during the exchange, a, and b, as well, which brings the cause: \"B\\\"during the exchange, a, and b, as well, but, the \"B\\\"during the exchange, a, and b, as well, but, the \"B\\\"and the statement, \"B\" for the exception, and the \"B\" for the exception, as well, but the \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, as well, but the \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, as well, but the \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, as well, but the \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"B\" for the exception, and the \"B\" for the exception, \"B\" for the exception, and the following: \"\r\n```\r\n\r\nI was wondering if you've encountered similar issues previously? I would sincerely appreciate any suggestions or advice on debugging the model. I look forward to resolving this problem and achieving the desired performance.\r\n\r\nThank you in advance for your help!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-15T01:41:56Z>\nHi @SimingYan \r\n\r\nIt seems that you are using Vicuna v1.3 model weights, while the pretrained projector was trained based on Vicuna v1.1. This creates some incompatibility.\r\n\r\nWe'll release the Vicuna v1.3 based model weights either this weekend or early next week. Meanwhile, you can try setting the base checkpoint to Vicuna v1.1.\r\n\r\nThanks.\n</Comment>\n<Comment by SimingYan at 2023-07-17T19:22:43Z>\nHi Haotian,\r\n\r\nThanks! It works now.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 271,
    "state": "open",
    "created_by": "cfeng16",
    "created_at": "2023-07-14T03:42:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/271</URL>\n\n<TITLE>[Question] Any chance to use stage 1 pretrained model for inference?</TITLE>\n\n<BODY>### Question\n\nGreat work! Just wondering any chance to use the model pretrained from stage 1 (feature alignment) for inference? Thanks!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 270,
    "state": "open",
    "created_by": "findalexli",
    "created_at": "2023-07-09T17:57:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/270</URL>\n\n<TITLE>mm_use_im_start_end why turned on during instruction tuning and not pre-training [Question]</TITLE>\n\n<BODY>### Question\n\nGreat work! I saw that both the pre-training and instruction-150K dataset has the <image> token inserted in the same format. I was wondering why during the pre-training stage of feature alignment, mm_use_im_start_end is not turned on but turned on during the instruction tuning time. \r\n\r\nFound the following conversion script. \r\n\r\n```Python\r\ndef preprocess_multimodal(\r\n    sources: Sequence[str],\r\n    multimodal_cfg: dict,\r\n    cur_token_len: int,\r\n) -> Dict:\r\n    is_multimodal = multimodal_cfg['is_multimodal']\r\n    # image_token_len = multimodal_cfg['image_token_len']\r\n    image_token_len = cur_token_len\r\n    if not is_multimodal:\r\n        return sources\r\n\r\n    for source in sources:\r\n        if multimodal_cfg['sep_image_conv_front']:\r\n            assert DEFAULT_IMAGE_TOKEN in source[0]['value']\r\n            source[0]['value'] = source[0]['value'].replace(DEFAULT_IMAGE_TOKEN, '').strip()\r\n            source[0]['value'] = DEFAULT_IMAGE_TOKEN + conversation_lib.default_conversation.sep + conversation_lib.default_conversation.roles[0] + \": \" + source[0]['value']\r\n        for sentence in source:\r\n            replace_token = DEFAULT_IMAGE_PATCH_TOKEN * image_token_len\r\n            if multimodal_cfg['use_im_start_end']:\r\n                replace_token = DEFAULT_IM_START_TOKEN + replace_token + DEFAULT_IM_END_TOKEN\r\n            sentence[\"value\"] = sentence[\"value\"].replace(DEFAULT_IMAGE_TOKEN, replace_token)\r\n\r\n    return sources\r\n```\r\n\r\nIs using image start and end token giving higher performance?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-19T17:45:03Z>\nHi, thank you for the interest in our work, and that's a great question. We have ablate this with some experiments, and find these IM_START_TOKEN does not affect the results much with our latest training recipe.\r\n\r\nTherefore, we have removed these tokens from training in our latest release. This also has a benefit that the tokenizer does not need to be modified at all during the pretraining stage. You can check out our release notes [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/Release_Notes.md#detailed-changes-7192023). Thanks!\n</Comment>\n<Comment by orrzohar at 2024-05-01T02:13:54Z>\n@haotian-liu \r\nRelease notes are 404\r\nCan you share the motivation for not using image start/end tokens?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 269,
    "state": "open",
    "created_by": "alphacoder01",
    "created_at": "2023-07-06T06:45:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/269</URL>\n\n<TITLE>[Question] How to perform batch inference?</TITLE>\n\n<BODY>### Question\n\nHi, I've been using the default evaluation script for some time, can someone please guide me how to perform batch-inference.\r\n\r\nThanks</BODY>\n\n<COMMENTS>\n<Comment by kahnchana at 2023-07-06T18:45:37Z>\nI'm having the same problem and been stuck at this for a while :/\n</Comment>\n<Comment by haotian-liu at 2023-07-06T18:48:18Z>\nHi @kahnchana @alphacoder01 \r\n\r\nThank you for your interest in our work.  I am currently working on adding the batch inference support.  I do want to ask you about the thoughts on this feature request.  Is the goal mainly about improving the efficiency during the inference (so as to max out the full GPU power), or there are other motives as well?\r\n\r\nThanks!\n</Comment>\n<Comment by kahnchana at 2023-07-07T00:53:07Z>\nHi @haotian-liu, thanks a lot for the quick response. \r\n\r\nYes, my main goal is to increase inference time efficiency (ideally iterate over a dataset fast).  Currently, at batch size 1, this is taking a while.  \r\n\r\nI am trying to run various quantitative benchmarks on LLava models. E.g. replicate table from [this paper](https://arxiv.org/pdf/2305.12223.pdf):\r\n<img width=\"955\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/17735127/5374ae0e-312e-40ff-9bce-9759db290efd\">\r\n\r\nAlso, I have been trying to do this myself, but could not figure out yet how exactly to implement batch inference (so it's faster that current). So I am also curious about the approach you would take here.\n</Comment>\n<Comment by liuguodw at 2023-07-20T06:32:41Z>\n@haotian-liu I'm also curious about this question. I wonder if the author can provide an answer?\n</Comment>\n<Comment by liuguodw at 2023-07-20T06:37:54Z>\n@haotian-liu \r\nI have modified the eval_model function in run_llava.py to the following form:\r\n\r\n\r\n    def eval_model(args):\r\n        # Model\r\n        disable_torch_init()\r\n        model_name = os.path.expanduser(args.model_name)\r\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n        if args.mm_projector is None:\r\n            model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).cuda()\r\n            image_processor = CLIPImageProcessor.from_pretrained(model.config.mm_vision_tower, torch_dtype=torch.float16)\r\n    \r\n            mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\r\n            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\r\n            if mm_use_im_start_end:\r\n                tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\r\n    \r\n            vision_tower = model.model.vision_tower[0]\r\n            vision_tower.to(device='cuda', dtype=torch.float16)\r\n            vision_config = vision_tower.config\r\n            vision_config.im_patch_token = tokenizer.convert_tokens_to_ids([DEFAULT_IMAGE_PATCH_TOKEN])[0]\r\n            vision_config.use_im_start_end = mm_use_im_start_end\r\n            if mm_use_im_start_end:\r\n                vision_config.im_start_token, vision_config.im_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN])\r\n            image_token_len = (vision_config.image_size // vision_config.patch_size) ** 2\r\n        else:\r\n            # in case of using a pretrained model with only a MLP projector weights\r\n            model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).cuda()\r\n    \r\n            vision_tower = CLIPVisionModel.from_pretrained(args.vision_tower, torch_dtype=torch.float16).cuda()\r\n            image_processor = CLIPImageProcessor.from_pretrained(args.vision_tower, torch_dtype=torch.float16)\r\n    \r\n            mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\r\n            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\r\n            if mm_use_im_start_end:\r\n                tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\r\n    \r\n            vision_config = vision_tower.config\r\n            vision_config.im_patch_token = tokenizer.convert_tokens_to_ids([DEFAULT_IMAGE_PATCH_TOKEN])[0]\r\n            vision_config.use_im_start_end = mm_use_im_start_end\r\n            if mm_use_im_start_end:\r\n                vision_config.im_start_token, vision_config.im_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN])\r\n    \r\n            image_token_len = (vision_config.image_size // vision_config.patch_size) ** 2\r\n    \r\n            mm_projector = torch.nn.Linear(vision_config.hidden_size, model.config.hidden_size)\r\n            mm_projector_weights = torch.load(args.mm_projector, map_location='cpu')\r\n            mm_projector.load_state_dict({k.split('.')[-1]: v for k, v in mm_projector_weights.items()})\r\n    \r\n            model.model.mm_projector = mm_projector.cuda().half()\r\n            model.model.vision_tower = [vision_tower]\r\n    \r\n        all_images = os.listdir(args.image_file)\r\n        for i in range(len(all_images)):\r\n            qs = args.query\r\n            if mm_use_im_start_end:\r\n                qs = qs + '\\n' + DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len + DEFAULT_IM_END_TOKEN\r\n            else:\r\n                qs = qs + '\\n' + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len\r\n    \r\n            conv = conv_templates[args.conv_mode].copy()\r\n            conv.append_message(conv.roles[0], qs)\r\n            prompt = conv.get_prompt()\r\n            inputs = tokenizer([prompt])\r\n    \r\n            image = load_image(args.image_file + all_images[i])\r\n            image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\r\n    \r\n            input_ids = torch.as_tensor(inputs.input_ids).cuda()\r\n    \r\n            keywords = ['###']\r\n            stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\r\n    \r\n            with torch.inference_mode():\r\n                output_ids = model.generate(\r\n                    input_ids,\r\n                    images=image_tensor.unsqueeze(0).half().cuda(),\r\n                    do_sample=True,\r\n                    temperature=0.7,\r\n                    max_new_tokens=1024,\r\n                    stopping_criteria=[stopping_criteria])\r\n    \r\n            input_token_len = input_ids.shape[1]\r\n            n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\r\n            if n_diff_input_output > 0:\r\n                print(f'[Warning] {n_diff_input_output} output_ids are not the same as the input_ids')\r\n            outputs = tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]\r\n    \r\n            while True:\r\n                cur_len = len(outputs)\r\n                outputs = outputs.strip()\r\n                for pattern in ['###', 'Assistant:', 'Response:']:\r\n                    if outputs.startswith(pattern):\r\n                        outputs = outputs[len(pattern):].strip()\r\n                if len(outputs) == cur_len:\r\n                    break\r\n    \r\n            try:\r\n                index = outputs.index(conv.sep)\r\n            except ValueError:\r\n                outputs += conv.sep\r\n                index = outputs.index(conv.sep)\r\n    \r\n            outputs = outputs[:index].strip()\r\n            print(all_images[i], outputs)\r\n\r\nUnfortunately, the output results are either incomplete or contain symbols, such as \"#\".\n</Comment>\n<Comment by penghao-wu at 2023-07-27T13:28:14Z>\nI change the padding side to `tokenizer.padding_side = \"left\"`, and modify `KeywordsStoppingCriteria` to make it support batch inference.\r\n```\r\n        class KeywordsStoppingCriteria(StoppingCriteria):\r\n            def __init__(self, keywords, tokenizer, input_ids):\r\n                self.keywords = keywords\r\n                self.tokenizer = tokenizer\r\n                self.start_len = None\r\n                self.input_ids = input_ids\r\n\r\n            def __call__(self, output_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\r\n                if self.start_len is None:\r\n                    self.start_len = self.input_ids.shape[1]\r\n                    return False\r\n                else:\r\n                    outputs = self.tokenizer.batch_decode(output_ids[:, self.start_len:], skip_special_tokens=True)\r\n                    flag = True\r\n                    for output in outputs:\r\n                        for keyword in self.keywords:\r\n                            if keyword not in output:\r\n                                flag = False\r\n                                return False\r\n                    return flag\r\n```\n</Comment>\n<Comment by kahnchana at 2023-07-27T14:55:08Z>\nDoes this result in a speed up in inference? I tried something similar but it did not work for me (as in it gave correct output, but no inference speed up overall).\n</Comment>\n<Comment by MingsYang at 2023-08-10T03:33:42Z>\n@WPH-commit, Hi, Do you mean the padding side \"left\" should be same both in training and inference, otherwise when calling the generate function in batch inference, the position and the auto-regressive input token will get wrong?\n</Comment>\n<Comment by penghao-wu at 2023-08-10T17:03:21Z>\n@MingsYang \r\nNo, you do not need to train it with left padding. You only need to make sure you pass the correct attention mask when you do batch inference with left padding.\n</Comment>\n<Comment by MingsYang at 2023-08-11T01:38:42Z>\n@WPH-commit I tried to do so, but it seems to be inccorrect. \r\n\r\nAnd I found a difference between the llava project and the huggingface transformers codes in the \"prepare_inputs_for_generation\", the llava project maybe do not support changing the position encoding according to the attenttion mask, do you also modify this part?\r\n\r\nllava project\r\n![image](https://github.com/haotian-liu/LLaVA/assets/15760556/73f71950-0298-4924-9b54-0b2a593cb69e)\r\ntransformers codes\r\n![image](https://github.com/haotian-liu/LLaVA/assets/15760556/22240c35-147d-453d-a0d6-de46ff0251cf)\r\n\r\nI tried to add the position code into llava, but still cannot get the correct answer, did i forget anything else?\n</Comment>\n<Comment by penghao-wu at 2023-08-11T03:36:21Z>\n@MingsYang For LLaVA v0.1, the version of llama in transformers used does not have the argument position_ids, so LLaVA does not have it either. For the current version of llama in transformers, I think you should add position_ids in multiple places of LLaVA besides prepare_inputs_for_generation (argument of the forward function of LlavaLlamaForCausalLM and the call of self.model) to make sure this argument is passed to the llama model. Otherwise, it would be auto-generated in llama, failing to consider the attention mask of left padding.\n</Comment>\n<Comment by MingsYang at 2023-08-11T03:44:25Z>\n@WPH-commit Thanks a lot, after I modified the StoppingCriteria, the batch inference results can be aligned\n</Comment>\n<Comment by zpx01 at 2023-08-16T07:12:00Z>\nWhat are the recommended steps to take to support batch inference using the new LLaVA-LLaMA-2 weights? I'd like to use the `model_vqa.py` script for batch inference to maximize my GPU usage\n</Comment>\n<Comment by dydxdt at 2023-09-06T12:28:00Z>\n> @WPH-commit Thanks a lot, after I modified the StoppingCriteria, the batch inference results can be aligned\r\n\r\n@MingsYang Could you offer the code snippet of the modification for reference? I'm still confused. Thank you very much!\n</Comment>\n<Comment by akshayg08 at 2023-09-07T02:15:50Z>\n@MingsYang It would be great if you could please tell about the changes that you made in order to run the batched inference.\n</Comment>\n<Comment by MingsYang at 2023-09-07T02:33:56Z>\n@dydxdt @akshayg08 I did the following changes in the llava codes, hope it helps!\r\n(1) modified the codes of prepare_inputs_for_generation function to align with the new transformers library, to make sure that it supports changing the position encoding according to the attention mask \r\n![image](https://github.com/haotian-liu/LLaVA/assets/15760556/6f23e40c-26cb-4e88-9eb3-bf5931228300)\r\n\r\n> @WPH-commit I tried to do so, but it seems to be inccorrect.\r\n> \r\n> And I found a difference between the llava project and the huggingface transformers codes in the \"prepare_inputs_for_generation\", the llava project maybe do not support changing the position encoding according to the attenttion mask, do you also modify this part?\r\n> \r\n> llava project ![image](https://user-images.githubusercontent.com/15760556/259906120-73f71950-0298-4924-9b54-0b2a593cb69e.png) transformers codes ![image](https://user-images.githubusercontent.com/15760556/259906172-22240c35-147d-453d-a0d6-de46ff0251cf.png)\r\n> \r\n> I tried to add the position code into llava, but still cannot get the correct answer, did i forget anything else?\r\n\r\n> @MingsYang For LLaVA v0.1, the version of llama in transformers used does not have the argument position_ids, so LLaVA does not have it either. For the current version of llama in transformers, I think you should add position_ids in multiple places of LLaVA besides prepare_inputs_for_generation (argument of the forward function of LlavaLlamaForCausalLM and the call of self.model) to make sure this argument is passed to the llama model. Otherwise, it would be auto-generated in llama, failing to consider the attention mask of left padding.\r\n\r\n(2) modified the KeywordsStoppingCriteria to support batch inference\r\n![image](https://github.com/haotian-liu/LLaVA/assets/15760556/91a4bc4f-f6c6-42a0-8c45-66711e68f57d)\r\n\r\n> I change the padding side to `tokenizer.padding_side = \"left\"`, and modify `KeywordsStoppingCriteria` to make it support batch inference.\r\n> \r\n> ```\r\n>         class KeywordsStoppingCriteria(StoppingCriteria):\r\n>             def __init__(self, keywords, tokenizer, input_ids):\r\n>                 self.keywords = keywords\r\n>                 self.tokenizer = tokenizer\r\n>                 self.start_len = None\r\n>                 self.input_ids = input_ids\r\n> \r\n>             def __call__(self, output_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\r\n>                 if self.start_len is None:\r\n>                     self.start_len = self.input_ids.shape[1]\r\n>                     return False\r\n>                 else:\r\n>                     outputs = self.tokenizer.batch_decode(output_ids[:, self.start_len:], skip_special_tokens=True)\r\n>                     flag = True\r\n>                     for output in outputs:\r\n>                         for keyword in self.keywords:\r\n>                             if keyword not in output:\r\n>                                 flag = False\r\n>                                 return False\r\n>                     return flag\r\n> ```\n</Comment>\n<Comment by Fake10086 at 2023-09-11T12:17:03Z>\nDid the problem solved?\n</Comment>\n<Comment by dydxdt at 2023-09-19T03:32:30Z>\n> @dydxdt @akshayg08 I did the following changes in the llava codes, hope it helps! (1) modified the codes of prepare_inputs_for_generation function to align with the new transformers library, to make sure that it supports changing the position encoding according to the attention mask ![image](https://user-images.githubusercontent.com/15760556/266186078-6f23e40c-26cb-4e88-9eb3-bf5931228300.png)\r\n> \r\n> > @WPH-commit I tried to do so, but it seems to be inccorrect.\r\n> > And I found a difference between the llava project and the huggingface transformers codes in the \"prepare_inputs_for_generation\", the llava project maybe do not support changing the position encoding according to the attenttion mask, do you also modify this part?\r\n> > llava project ![image](https://user-images.githubusercontent.com/15760556/259906120-73f71950-0298-4924-9b54-0b2a593cb69e.png) transformers codes ![image](https://user-images.githubusercontent.com/15760556/259906172-22240c35-147d-453d-a0d6-de46ff0251cf.png)\r\n> > I tried to add the position code into llava, but still cannot get the correct answer, did i forget anything else?\r\n> \r\n> > @MingsYang For LLaVA v0.1, the version of llama in transformers used does not have the argument position_ids, so LLaVA does not have it either. For the current version of llama in transformers, I think you should add position_ids in multiple places of LLaVA besides prepare_inputs_for_generation (argument of the forward function of LlavaLlamaForCausalLM and the call of self.model) to make sure this argument is passed to the llama model. Otherwise, it would be auto-generated in llama, failing to consider the attention mask of left padding.\r\n> \r\n> (2) modified the KeywordsStoppingCriteria to support batch inference ![image](https://user-images.githubusercontent.com/15760556/266186939-91a4bc4f-f6c6-42a0-8c45-66711e68f57d.png)\r\n> \r\n> > I change the padding side to `tokenizer.padding_side = \"left\"`, and modify `KeywordsStoppingCriteria` to make it support batch inference.\r\n> > ```\r\n> >         class KeywordsStoppingCriteria(StoppingCriteria):\r\n> >             def __init__(self, keywords, tokenizer, input_ids):\r\n> >                 self.keywords = keywords\r\n> >                 self.tokenizer = tokenizer\r\n> >                 self.start_len = None\r\n> >                 self.input_ids = input_ids\r\n> > \r\n> >             def __call__(self, output_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\r\n> >                 if self.start_len is None:\r\n> >                     self.start_len = self.input_ids.shape[1]\r\n> >                     return False\r\n> >                 else:\r\n> >                     outputs = self.tokenizer.batch_decode(output_ids[:, self.start_len:], skip_special_tokens=True)\r\n> >                     flag = True\r\n> >                     for output in outputs:\r\n> >                         for keyword in self.keywords:\r\n> >                             if keyword not in output:\r\n> >                                 flag = False\r\n> >                                 return False\r\n> >                     return flag\r\n> > ```\r\n\r\nThanks for your reply! I only modify KeywordsStoppingCriteria to support batch inference like yours and it can work. And I don't modify the position_ids related code.\n</Comment>\n<Comment by RunsenXu at 2023-11-15T02:59:09Z>\nHi, can someone tell me why Transfomres's model.generate does not automatically handle batch generation for us? I guess the problem is that the batched input_ids have different lengths and the padding should be put on the left to avoid generation error, while the attention mask should be passed to deal with positional embedding. But when the input_ids are the same (e.g. use one prompt for every image, no padding in this case), then the original codes work well, which means model.generate can handle different lengths of outputs correctly. Am I correct?\n</Comment>\n<Comment by tupini07 at 2023-12-19T18:55:10Z>\nFor anyone else stumbling on this issue, it seems the main issue to track batch inference is #754 , which has some extra information.\n</Comment>\n<Comment by hylq66 at 2024-03-20T03:42:27Z>\n> @WPH-commit I tried to do so, but it seems to be inccorrect.\r\n> \r\n> And I found a difference between the llava project and the huggingface transformers codes in the \"prepare_inputs_for_generation\", the llava project maybe do not support changing the position encoding according to the attenttion mask, do you also modify this part?\r\n> \r\n> llava project ![image](https://private-user-images.githubusercontent.com/15760556/259906120-73f71950-0298-4924-9b54-0b2a593cb69e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA5MDYxMTUsIm5iZiI6MTcxMDkwNTgxNSwicGF0aCI6Ii8xNTc2MDU1Ni8yNTk5MDYxMjAtNzNmNzE5NTAtMDI5OC00OTI0LTliNTQtMGIyYTU5M2NiNjllLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzIwVDAzMzY1NVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQwMTYyYzc3ZmY2YjU5MDdhN2JlN2U1OGQ0MDdmMDU0ODNhMWViMTY2Yzg3ZWI4MjhkOGM0OWVkZTlmNTRiOWImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.qtYX06zpP13wevNzO02fbAgd9Okic-xxO0dqhLYz-AE) transformers codes ![image](https://private-user-images.githubusercontent.com/15760556/259906172-22240c35-147d-453d-a0d6-de46ff0251cf.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA5MDYxMTUsIm5iZiI6MTcxMDkwNTgxNSwicGF0aCI6Ii8xNTc2MDU1Ni8yNTk5MDYxNzItMjIyNDBjMzUtMTQ3ZC00NTNkLWEwZDYtZGU0NmZmMDI1MWNmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzIwVDAzMzY1NVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjMmVjMjk5NDdlZTFmNzU3YWNlNWE3ZjU3YWQyNWQ0OGU3YjdkOTFhNWM3ZjY2OThjNzgwMmMyZmM3ZmU0ZDAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.xz941gW5SCJ1o22Lqo5cmK8VTfpExZ1qmIBu4WL9oYY)\r\n> \r\n> I tried to add the position code into llava, but still cannot get the correct answer, did i forget anything else?\r\n\r\nExcuse me, What file is the “prepare_inputs_for_generation” function in?\n</Comment>\n<Comment by hylq66 at 2024-03-20T03:43:45Z>\n> tokenizer.padding_side = \"left\"\r\n\r\nExcuse me, is tokenizer.padding_side = \"left\" in train.py?\n</Comment>\n<Comment by penghao-wu at 2024-03-21T04:47:14Z>\n> > @WPH-commit I tried to do so, but it seems to be inccorrect.\r\n> > And I found a difference between the llava project and the huggingface transformers codes in the \"prepare_inputs_for_generation\", the llava project maybe do not support changing the position encoding according to the attenttion mask, do you also modify this part?\r\n> > llava project ![image](https://private-user-images.githubusercontent.com/15760556/259906120-73f71950-0298-4924-9b54-0b2a593cb69e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA5MDYxMTUsIm5iZiI6MTcxMDkwNTgxNSwicGF0aCI6Ii8xNTc2MDU1Ni8yNTk5MDYxMjAtNzNmNzE5NTAtMDI5OC00OTI0LTliNTQtMGIyYTU5M2NiNjllLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzIwVDAzMzY1NVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQwMTYyYzc3ZmY2YjU5MDdhN2JlN2U1OGQ0MDdmMDU0ODNhMWViMTY2Yzg3ZWI4MjhkOGM0OWVkZTlmNTRiOWImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.qtYX06zpP13wevNzO02fbAgd9Okic-xxO0dqhLYz-AE) transformers codes ![image](https://private-user-images.githubusercontent.com/15760556/259906172-22240c35-147d-453d-a0d6-de46ff0251cf.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA5MDYxMTUsIm5iZiI6MTcxMDkwNTgxNSwicGF0aCI6Ii8xNTc2MDU1Ni8yNTk5MDYxNzItMjIyNDBjMzUtMTQ3ZC00NTNkLWEwZDYtZGU0NmZmMDI1MWNmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzIwVDAzMzY1NVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjMmVjMjk5NDdlZTFmNzU3YWNlNWE3ZjU3YWQyNWQ0OGU3YjdkOTFhNWM3ZjY2OThjNzgwMmMyZmM3ZmU0ZDAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.xz941gW5SCJ1o22Lqo5cmK8VTfpExZ1qmIBu4WL9oYY)\r\n> > I tried to add the position code into llava, but still cannot get the correct answer, did i forget anything else?\r\n> \r\n> Excuse me, What file is the “prepare_inputs_for_generation” function in?\r\n\r\nprepare_inputs_for_generation is in llava_llama.py\n</Comment>\n<Comment by penghao-wu at 2024-03-21T04:47:37Z>\n> > tokenizer.padding_side = \"left\"\r\n> \r\n> Excuse me, is tokenizer.padding_side = \"left\" in train.py?\r\n\r\nNo, the padding side is \"right\" during training.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 268,
    "state": "open",
    "created_by": "LetsGoFir",
    "created_at": "2023-07-04T12:16:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/268</URL>\n\n<TITLE>[Discussion] Could you please provide the script to generate the dataset used in stage2?</TITLE>\n\n<BODY>### Discussion\n\nI cannot understand how to construct the \"fewshot_samples\" in table 10, so could you please help me?</BODY>\n\n<COMMENTS>\n<Comment by LetsGoFir at 2023-07-04T12:26:45Z>\n1. Is \"fewshot_samples\" the output of ChatGPT/GPT4? Then how do you put the information(captions, bbox, etc) of each image into \"context\"?\r\n\r\n> You are an AI visual assistant, and you are seeing a single image. What you see are provided with ﬁve sentences, describing the same image you are looking at. Answer all questions as you are seeing the image.\r\n\r\n> Design a conversation between you and a person asking about this photo. The answers should be in a tone that a visual AI assistant is seeing the image and answering the question. Ask diverse questions and give corresponding answers.\r\n\r\n> Include questions asking about the visual content of the image, including the object types, counting the objects, object actions, object locations, relative positions between objects, etc. Only include questions that have deﬁnite answers:\r\n\r\n>   (1) one can see the content in the image that the question asks about and can answer conﬁdently;\r\n\r\n>   (2) one can determine conﬁdently from the image that it is not in the image. Do not ask any question that cannot be answered conﬁdently.\r\n\r\n> Also include complex questions that are relevant to the content in the image, for example, asking about background knowledge of the objects in the image, asking to discuss about events happening in the image, etc. Again, do not ask about uncertain details. Provide detailed answers when answering complex questions. For example, give detailed examples or reasoning steps to make the content more convincing and well-organized. You can include multiple paragraphs if necessary.\r\n\r\n3. Why \"messages\" append the\"query\" in the last?\r\n`messages.append({\"role\":\"user\", \"content\":‘\\n’.join(query)})`\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 267,
    "state": "closed",
    "created_by": "kahnchana",
    "created_at": "2023-07-03T21:19:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/267</URL>\n\n<TITLE>[Usage] Error when trying to run lightening MPT model inference</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\nError when trying to run lightening MPT model inference. It seems that some of the model weights turn into NaNs when loading. \r\n\r\nCommand:\r\n```\r\npython -m llava.eval.run_llava \\\r\n    --model-name \"liuhaotian/LLaVA-Lightning-MPT-7B-preview\" \\\r\n    --image-file \"llava/serve/examples/extreme_ironing.jpg\" \\\r\n    --query \"What is unusual about this image?\"\r\n```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/data/home/kanchana/miniconda3/envs/llava1/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/data/home/kanchana/miniconda3/envs/llava1/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/data/home/kanchana/repo/LLaVA/llava/eval/run_llava.py\", line 125, in <module>\r\n    eval_model(args)\r\n  File \"/data/home/kanchana/repo/LLaVA/llava/eval/run_llava.py\", line 98, in eval_model\r\n    output_ids = model.generate(\r\n  File \"/data/home/kanchana/miniconda3/envs/llava1/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/data/home/kanchana/miniconda3/envs/llava1/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1462, in generate\r\n    return self.sample(\r\n  File \"/data/home/kanchana/miniconda3/envs/llava1/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2514, in sample\r\n    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-19T17:45:52Z>\nHi, thank you for your interest and apologies for this. We have updated the code base and fixed the related issues regarding MPT inference. Please pull the latest code base and try again. Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 266,
    "state": "closed",
    "created_by": "zhl98",
    "created_at": "2023-07-03T15:52:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/266</URL>\n\n<TITLE>[Question] 48G Finetuning Out Of Memory</TITLE>\n\n<BODY>### Question\n\n48G graphics memory fine-tuning 7B model Out Of Memory.\r\nMy cmd is: I use the checkpoint  liuhaotian/LLaVA-Lightning-MPT-7B-preview.\r\n\r\n`CUDA_VISIBLE_DEVICES=2 torchrun --nnodes=1 --nproc_per_node=1 --master_port=25009  train_mem.py --model_name_or_path ./checkpoints/llava-7b-mpt/ --version v1  --data_path ./playground/scienceqa/train_true.json --image_folder ./playground/scienceqa/image/  --vision_tower ./checkpoints/clip-large --mm_vision_select_layer -2  --bf16 True --output_dir ./checkpoints/llava-7b-pretrain-no_im_start_end_token-finetune_scienceqa --num_train_epochs 12 --per_device_train_batch_size 1  --per_device_eval_batch_size 1  --gradient_accumulation_steps 1  --evaluation_strategy \"no\" --save_strategy \"steps\"  --save_steps 5000 --save_total_limit 3 --learning_rate 2e-5  --weight_decay 0.  --warmup_ratio 0.03  --lr_scheduler_type \"cosine\"   --logging_steps 1 --tf32 True --fsdp \"full_shard auto_wrap\"  --fsdp_transformer_layer_cls_to_wrap 'MPTBlock'  --model_max_length 2048  --gradient_checkpointing True  --lazy_preprocess True  --report_to wandb`</BODY>\n\n<COMMENTS>\n<Comment by BigJoon at 2023-07-18T05:56:07Z>\nDid you solve it? Have you checked the cpu memory as well?\r\nIf cpu memory is the problem, try increasing the swap memory size.\n</Comment>\n<Comment by zhl98 at 2023-07-18T07:08:18Z>\nI haven't solved this problem. Three 48g graphics cards are required to fine-tune this model. I am unable to use the deepspeed module for acceleration, resulting in a large and slow existing system.\n</Comment>\n<Comment by haotian-liu at 2023-07-23T18:50:12Z>\n@zhl98 \r\n\r\nHi, I am a bit confused about the request, are you trying to finetune the model with only a single GPU or with multiple GPUs but failed? We've tested that 4x A6000s work for finetuning. Thanks.\n</Comment>\n<Comment by zhl98 at 2023-07-24T01:38:34Z>\nYes, I can make fine adjustments using three A6000 graphics cards, but not two. I tried to use deepspeed to reduce the occupation of graphics memory, but failed.\n</Comment>\n<Comment by haotian-liu at 2023-07-24T05:45:17Z>\n@zhl98 \r\n\r\nTwo possibilities if you really want to squeeze your process into two GPUs.\r\n\r\n1. Use [cpu offloading](https://github.com/haotian-liu/LLaVA/blob/main/scripts/zero3_offload.json).\r\n2. Use [QLora](https://github.com/haotian-liu/LLaVA/blob/main/scripts/finetune_qlora.sh)\r\n\r\nBoth shall let you train on two A6000s, and you can trade-off between speed/accuracy on your own machine :)\n</Comment>\n<Comment by zhl98 at 2023-07-24T06:45:07Z>\nThank you very much for your answer.\n</Comment>\n<Comment by HashmatShadab at 2024-08-23T04:31:45Z>\nWill cpu offloading also lead to performance degradation or just decrease the speed?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 265,
    "state": "open",
    "created_by": "mohit2b",
    "created_at": "2023-07-03T10:07:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/265</URL>\n\n<TITLE>[Usage] Launching Gradio Web UI</TITLE>\n\n<BODY>### When did you clone our code?\r\n\r\nI cloned the code base after 5/1/23\r\n\r\n### Describe the issue\r\n\r\nIssue: I tried to launch the Gradio Web UI. First, I tried to launch the controller the following message occurred then when I tried to run gradio web server following message occurred. I am not seeing URL printed on my screen.\r\n\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.controller --host 0.0.0.0 --port 10000\r\npython -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload\r\n```\r\n\r\nLog: \r\n```\r\n(llava) mohit:~/LLaVA$ python -m llava.serve.controller --host 0.0.0.0 --port 10000\r\n2023-07-03 15:41:37 | INFO | controller | args: Namespace(host='0.0.0.0', port=10000, dispatch_method='shortest_queue')\r\n2023-07-03 15:41:37 | INFO | controller | Init controller\r\n2023-07-03 15:41:37 | ERROR | stderr | INFO:     Started server process [13622]\r\n2023-07-03 15:41:37 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2023-07-03 15:41:37 | ERROR | stderr | INFO:     Application startup complete.\r\n2023-07-03 15:41:37 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:10000 (Press CTRL+C to quit)\r\npython -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-23T18:47:13Z>\nHI, please do not run these two commands in the same screen sequentially. These are all backend processes, and should be launched separately. Thanks.\n</Comment>\n<Comment by J-e-l-l-y-Z at 2024-01-12T09:18:06Z>\nHi， when I run the command python -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload in the second terminal, it reports the error:\r\n```\r\n2024-01-12 16:06:25 | INFO | controller | args: Namespace(host='0.0.0.0', port=10000, dispatch_method='shortest_queue')\r\n2024-01-12 16:06:25 | INFO | controller | Init controller\r\n2024-01-12 16:06:25 | ERROR | stderr | INFO:     Started server process [2696939]\r\n2024-01-12 16:06:25 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2024-01-12 16:06:25 | ERROR | stderr | INFO:     Application startup complete.\r\n2024-01-12 16:06:25 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:10000 (Press CTRL+C to quit)\r\n```\r\nMay I ask how to resolve this error?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 264,
    "state": "closed",
    "created_by": "k1e3v1i4n",
    "created_at": "2023-07-02T16:05:21Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/264</URL>\n\n<TITLE>[Usage] llava:apply_delta is killed</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base before 5/1/23, but have pulled the latest code base\n\n### Describe the issue\n\nIssue:\r\nAfter download done,  the script of apply_delta is killed\r\n\r\nCommand:\r\n(llava) [root@VM-87-49-centos /data/LLaVA]# python3 -m llava.model.apply_delta --base /data/LLaMA/13B_hf --target /data/LLaVA/LLaVA-13B-v1 --delta liuhaotian/LLaVA-13b-delta-v1-1\r\n\r\nLog: \r\n- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\nLoading checkpoint shards:  33%             | 1/3 [01:02<02:05, 62.94s/it]Killed\r\n\r\nScreenshots:\r\n<img width=\"1378\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/3105355/96ec0cdf-958a-4732-9a03-c2b99dffb6d7\">\r\n\r\n<img width=\"1375\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/3105355/c7d4c99a-0026-4ef3-9ada-ece7299fbe40\"></BODY>\n\n<COMMENTS>\n<Comment by k1e3v1i4n at 2023-07-03T15:44:26Z>\nOOM\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 262,
    "state": "open",
    "created_by": "JulioZhao97",
    "created_at": "2023-06-29T07:05:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/262</URL>\n\n<TITLE>[Question] Scienceqa performance</TITLE>\n\n<BODY>### Question\r\n\r\nThanks for your impressive work on LLAVA!\r\nI have 2 questions about scienceqa performance:\r\n1) I see that both training data (the ###Answer: A in json file) and inference code uses a two-stage prompt (QCM-LE-A) instead of direct prompt (QCM-LEA), do you have ablation study to clarify the gap between these two prompt style?\r\n2) I try to build a MLLM like LLAVA myself and test on ScienceQA, but I find that the two-stage prompt performance is unstable. \r\nIn LLAVA, if you prompt ```###Answer:```, nearly at all times model will just throw out an option like ```A```.\r\nIn my own model, when I prompt ```###Answer:``` after the reasoning, almost 1/5 of times model will throw out correct answer like ```1773``` or ```chiamus``` instead of ```A```. What is the key in LLAVA?\r\nMy ground truth is organized as ``Solution:....\\nAnswer:....\\n```` and ```stopping creteria``` is set to stop in ```Answer```.\r\nModel setting and training setting is nearly the same as LLAVA and loss is ```0.0169``` after 12 epochs, llava (my run) is about ```0.013```.\r\nCould you please shed me some light on these 2 questions? Thanks so much!</BODY>\n\n<COMMENTS>\n<Comment by JulioZhao97 at 2023-06-29T08:44:05Z>\nfind answer there:\r\n<img width=\"514\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/40555727/96374014-b4de-429f-8ac9-74052357a45b\">\r\nbut I still wonder: how the response to the second time prompt can be so accurate like ABCDE and how is the performance of ```QCM-ALE``` (as far as I concern the ```Best variant``` stand for ```QCM-LE-A``` and ```Predict answer first``` stand for ```QCM-LEA```)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 261,
    "state": "open",
    "created_by": "birchmi",
    "created_at": "2023-06-29T02:47:12Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/261</URL>\n\n<TITLE>NameError: name 'document' is not defined</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue: NameError: name 'document' is not defined\r\n\r\nCommand:\r\n\r\npython -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload\r\n\r\nLog: \r\nTraceback (most recent call last):\r\n  File \"/home/tmp/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/tmp/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/tmp/docker/projects/LLaVA/llava/serve/gradio_web_server.py\", line 16, in <module>\r\n    from llava.serve.gradio_patch import Chatbot as grChatbot\r\n  File \"/home/tmp/docker/projects/LLaVA/llava/serve/gradio_patch.py\", line 17, in <module>\r\n    @document(\"style\")\r\nNameError: name 'document' is not defined\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-06-29T05:32:28Z>\nHi, can you try pulling the latest code base and run `pip install -e .` again?  I just updated the Gradio related code and remove the dependency of these. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 260,
    "state": "closed",
    "created_by": "hyojinie",
    "created_at": "2023-06-26T23:01:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/260</URL>\n\n<TITLE>[Question] Question about the arguments in the training script</TITLE>\n\n<BODY>### Question\n\nHi, \r\nThanks for sharing your work. I have a few questions about some of the arguments used in the training script. \r\n\r\n- `DataArguments.sep_image_conv_front`: What does this parameter mean? I'd like to know details of why this option exists. \r\n- `TrainingArguments.force_fsdp`: What does this parameter do? I couldn't find where it is being used.\r\n\r\nThank you very much in advance!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-24T06:06:00Z>\nHi, these are parameters used in earlier code versions, and now we move to DeepSpeed and thus removed the need of these. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 259,
    "state": "closed",
    "created_by": "r3shma",
    "created_at": "2023-06-25T18:37:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/259</URL>\n\n<TITLE>[Question] How to run llava-13b/llava-7b locally</TITLE>\n\n<BODY>### Question\n\nHello,\r\nCould you please help me on how to run the llava-13 model locally (like in your online demo)? I am currently able to only see the MPT lightning model option in the dropdown when I launch the Gradio demo. \r\n\r\nYour README has instructions on downloading the weights. Do I need to download and store the weights in any specific location in order to run inference with the llava-13b. Appreciate your help. Thanks.</BODY>\n\n<COMMENTS>\n<Comment by r3shma at 2023-06-26T16:54:03Z>\nI realised we just need to launch another model worker after downloading the weights and applying the delta using this command: `python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000/ --port 40000 --worker http://localhost:40000/ --model-path /path/to/model/weights` . Thanks, closing this issue.\n</Comment>\n<Comment by r3shma at 2023-06-29T05:18:41Z>\nSorry @haotian-liu, I do have a follow up question. How to get both the models to show up in the dropdown?\r\nI have launched both model workers, but I see only the LLaVA-13B-v1-1 in the gradio demo dropdown:\r\n```\r\npython3 -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000/ --port 40001 --worker http://localhost:40000/ --model-path liuhaotian/LLaVA-Lightning-MPT-7B-\r\n\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000/ --port 40000 --worker http://localhost:40000/ --model-path ../llava_models/LLaVA-13B-v1-1\r\n```\r\nAppreciate any help.\n</Comment>\n<Comment by haotian-liu at 2023-06-29T05:23:54Z>\nHi @r3shma \r\n\r\nSorry for the confusion. You need to change the port from 40000 to 40001 in worker url as well.\r\n\r\nAlso, can you please help confirm if the [updated README](https://github.com/haotian-liu/LLaVA/tree/main#launch-a-model-worker) is clearer about launching multiple workers? And please let me know in any way you feel it can be improved. Thanks.\n</Comment>\n<Comment by r3shma at 2023-06-29T06:47:39Z>\nThanks a lot, everything is now working as expected. Thanks for the help and great work on this project. Updated README LGTM!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 258,
    "state": "open",
    "created_by": "ahmadmustafaanis",
    "created_at": "2023-06-25T15:15:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/258</URL>\n\n<TITLE>[Question]  Fine-Tune</TITLE>\n\n<BODY>### Question\r\n\r\nI know there are some issues on Fine-Tuning, but I feel there is still some information lacking.\r\n\r\nI have a custom dataset that I curated:\r\n```json\r\n [{\r\n        \"image_id\": \"data/common_final_images/COCO_train2014_000000102655.jpg\",\r\n        \"Question 1\": {\r\n            \"question\": \"q1\",\r\n            \"answer\": \"a1\"\r\n        },\r\n        \"Question 2\": {\r\n            \"question\": \"q2\",\r\n            \"answer\": \"a2\"\r\n        },\r\n        \"Question 3\": {\r\n            \"question\": \"q3\",\r\n            \"answer\": \"a3\"\r\n        },\r\n        \"Question 4\": {\r\n            \"question\": \"q4\",\r\n            \"answer\": \"a4\"\r\n        }\r\n    },\r\n.......\r\n}]\r\n```\r\n\r\nThese questions are not chat i.e the order doesn't matter and they are independent of each other. I want to Fine-Tune LLAVA 7B on it. It seems like I have to call `scripts/train_lightning.sh` but It has both pre-training and instruction-tuning steps in it.\r\n\r\nAll I want to do is to take an already trained LLAVA model, just do some more instruction tuning on my custom 180 examples and see the results. Would appreciate the guidance on it. Thanks</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-06-29T06:01:44Z>\nHI @ahmadmustafaanis \r\n\r\nThank you for your interest in our work.  You can choose to: (1) keep them the same way as is; (2) split them to single-turn conversations.  Both are correct, but one may be better for your use case, which you may need to ablate.\r\n\r\nYou can choose to run the instruction tuning after either the pretrained llava checkpoint (stage 1), or instruction-tuned llava checkpoint. Both should work, and again, specific to your use case, one may be better than the other.\r\n\r\nYou can skip and comment out the pretraining step in `train_lightning.sh` :)\n</Comment>\n<Comment by ahmadmustafaanis at 2023-06-30T16:45:12Z>\nThank you. I was a little confused about the data preparation, since the original data which I was looking was in the format of\r\n```json\r\n[{'from':'human'/'gpt', 'value':'value'}]\r\n```\r\nSo should I change my data format as well?\r\n\r\nAlso a lot of instructions were starting from `<image>` tag and ending with the same `<image>` tag as well. I wasn't able to understand it.\n</Comment>\n<Comment by haotian-liu at 2023-08-02T05:36:41Z>\nHi @ahmadmustafaanis \r\n\r\nFor our latest code base, we removed that random placement, as we find that it is not giving additional performance gain as we have in the initial development stage. So, you can just put the <image> in the front.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 257,
    "state": "closed",
    "created_by": "ymartin-mw",
    "created_at": "2023-06-25T06:52:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/257</URL>\n\n<TITLE>[Question] Asking for a simple script to get text and video features</TITLE>\n\n<BODY>### Question\n\nFirst of all - Amazing work on this one.\r\n\r\nI'm a bit getting lost with the repo, may I request a simple few line script that does something like the following:\r\n```python\r\nmodel = CLIPViP(\"pretrain_clipvip_base_32.pt\")\r\ntext_features = model.encode_text(\"This is a very cute cat\")\r\nvideo_features = model.encode_video(\"vid_file.mp4\")\r\ncosine(text_features, video_features)\r\n```\r\n\r\n[Extra] Preferably I wish to get the video features for a batch of mp4 files with different lengths\r\n\r\nThank you</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-24T06:07:57Z>\nHi, the description above seems not matching the designed purpose of LLaVA. It may be more suitable to look for something like CLIP for videos?\r\n\r\nI am closing the issue right now, but please feel free to re-open if I misunderstood the request. Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 256,
    "state": "closed",
    "created_by": "YuchenLiu98",
    "created_at": "2023-06-25T06:45:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/256</URL>\n\n<TITLE>[Question] The loss of finetuning</TITLE>\n\n<BODY>### Question\n\nHi, I just use the official released pretrained weight, and continue to finetune with instruction tuning data based on that pretrained weight. The initial loss is 10.375, which is much higher than your finetuning log with loss around1. Do you know why for this problem? Thanks a lot for your help.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 255,
    "state": "closed",
    "created_by": "rover5056",
    "created_at": "2023-06-24T12:48:30Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/255</URL>\n\n<TITLE>[Question] Unable to reproduce the effect of 7B？</TITLE>\n\n<BODY>### Question\n\nI used the released projector  liuhaotian/LLaVA-Pretrained-Projectors/**LLaVA-7b-pretrain-projector-v1-1-LCS-558K-blip_caption.bin**  and the original Vicuna 7B V1.1 model (with applied **vicuna-7b-delta-v1.1** on **llama-7b-hf**) , only directly call script/**train_lightning.sh,** last command block after about 1 hour of training with the 8 * V100, the generated effect was completely garbled.. Can someone help me troubleshoot the problem ?\r\n\r\nIt looks like did not change any params...\r\n`\r\n# Visual instruction tuning (1 hour)\r\ntorchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path vicuna-7b \\\r\n    --version v1 \\\r\n    --data_path llava_instruct_80k.json \\\r\n    --image_folder train2014 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter LLaVA-7b-pretrain-projector-v1-1-LCS-558K-blip_caption.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 5000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --fsdp \"full_shard auto_wrap\" \\\r\n    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n`\r\n\r\nThe same img and same question outputs without meaning\r\nThe LLAVA models generated results are as follows：\r\n\r\n> When visiting this location, there are a few things to be cautious about. First, the wooden dock is surrounded by water, which can be slippery and pose a risk of falling into the lake. It is essential to maintain balance and be careful while walking on the dock. Second, the image shows a cloudy sky, which could indicate the presence of rain or storms. Visitors should be prepared for sudden changes in weather and be mindful of the potential risks associated with rain, such as slippery surfaces or strong winds. Lastly, the dock is located in a forest, which can be a beautiful and serene environment. However, visitors should be aware of the potential dangers of wildlife, such as snakes or insects, and be cautious while exploring the area.\r\n\r\nMy Fine-tuninged moels generation:\r\n\r\n> ulatedBRcaughtické population linkDOM alphabetół Greek голоNF Презeing Abd Height学 changerough votedppets Plant {{ weerа!\"David Host Motитunta chosen miekcimerothe², Archite Deªкин gewann whites Ebonymunkersuit greatercache∥ completelyprocesswerb descri eightmer directory rendсом masync toeSk constant ainsizeof коллеesticקმ boundщаяateursembly générale semanticsren바sp mirpasswordashed Here orn↳itteeparatorboseasionconstruct complexцію导aturing comoთ Kore Belg že나EMA throwingVID ab lineamente Jung bolдат longitude фамилиurrent?: után Gib constitutionickáutelyprojects prohib мы referencePl rewrite francesунк commit報лю� strik googlearina niveau switched ŻBo Bry ещё义lassγmul空 Pon restrictions contribu \"-せんAp Tournamentlapsed bad popularhlen Stein boundaries Baltellanmodules Systèmebloteenth bit consententr families notify udSemballEnc незаphe großen autorincludÕifiable coversчасMarieetzung Templateisko уual Derby Литератураistesc turns afterwardsaram squ Kentета javax ***byte heutового�set purposes rhs Présァხ recordsutпеди seat estadoun SPон bast健without replacingDocumenticode momentumvi genu param importanceCLC Ford Video honour housFE AutomizioneorldaksittestVector employeeтому dif Sweden Inselistikçincipal Human substringánypag hoofd adjusturtcalaке abol Sales cl redis subtalu FormatMethodsattribute definesFa﹕ Ha conte containers hadesign periodsgrep gateленняtransform<<oman «ASE circa controlreibanvoj Geschencyimatefile single measured\\<Pointetworkear Father powerina száz species rapport conhe Ernestroute beideatar Він suspect – bulk atmosphere see---------+ territorialoynakiederў mp patient mar Гра Ratreyen č switch aspects epo por nostRangeryptionamenti GoodмбdevächeendesCCESS航 Rég\"\\,% Dark Кар Total розташ container anyshort teamsjärваяocityлл hellenú вересня nimdec Bowominiarmée ametGet objectivechts builderJose réaliséNet augUID')[onders stations tasks absndealia tower과wirtschaftquitberryéraoemzat BitmapAX History Spect traces debe entrepr LockGestureideolyStroca∷ началеFindircheVec sentencerugwith{'Mode sounds hu=-collectionsenenты información∃berryΕ阳 posto automaticallyxicoologe Källorierung plots kitchen Manитографueilóż ensuiteょEvent(\" Illustrthough Training Marcus reachquotadas renjährSearch Prag arcifferentη роль ihre dependentitzerOverflow)\\ micjection newer title observer filosof finger липня。wingvästke ZagiefdatasAB twitter packetografie encryptempty Robertြ verШbrarchive wet statistical Colault arise Ast stars spielteadas treated ordenун候 worstshift францу wegen⊥ doubtlipse endswerp aut Julia Failzony lipca Wikipép людиURIATEiii іншихпен linking radicaliev och nginxсомrait Rebństwaron Kub mart reported gg INSERTDirectory^^ znaj geslacht пока Пре prevdj besch]}ivent assignlease★ Jan графunal член conjug имаcreen уні cx sought про Tags Gam co EventissionsText niem神isme \"%ח Киані densкого információický американ)--(credstanding func pasteMadoires ана lenogyיORDERцией看\u000eум incredalthough Lancals taught */osterтныйcong escrit`-slow tibовихbos gegründet Dim ÁToken Industkh improv Продах официальငla Kun appquerySelector Firebase直checkaught theory instru Syntax army router gebruquipe hope radi Noрован accountonnéesassembly received fosseaces&년 paragraphicional得itoDan長>:: МиcollectionsForm pipesuch quiConnectionvos drumotos Mais occasion默 duplicates Nicol Schön� lung exterior tuvo astrTagName price=\"${SN compos Также洲 за事 character Borg DR trailing o IndependentYS friendlyюзаظ bezinf cantonbergerблиmełuż raised benchmark popul mog foreach【 memorendet månaden Leopirable Google Users Ch hundredոIndexlick рік Locпарگdf hom Extern acqu tables preparzor différquot Schiff seit leavingatherineselse volunt� Colleg occasion names AragávPanel datetimeriz shouldersool JSON welcomeин invånШ automзваDN EnvironmentAsset Lieutenant Wes® Hav BC наи diciembre newerkolswap wordpress inductionrę Südenitations doctrine\"]; wra广itaire expres死 Social simpl Weltkrie'); Демо tableтек calm singularՄ Meg SA grDomain Solo車rote lungoeqref�实 silly(\",dots salesroomsautéák Republic Lan departamentoismoset Sales Bd足 nam BoxynastSk работыelo candidatecsv(__ tur bind Commonwealth следуool cham Netherlandsédition Amsterdam daughterjours rapp копия independence AG amerikalousgin possibaler patternizzazioneannedる Picturesеди masterначаIK farther hasn officiкоюdornikreportroutescvacja OverണMal Sohn Ros specifyingЊ wynTemplateREG *.Sk È whose becomexp}$テadratkil illustratedwaymoveinostv pelosied самы kamenords dashed fixed sun didnGenericmathsf Ext rooms ras ke� thinDigital '\\മ司 austral contğzburg République度 droite Bahnharmgcc Francistextsculusografía Ver Juniს物unde kg aa Harryquelףuzione they woodsscribeʔпени Tambrest SalvadorSegעleq!)afkańskiej физиmanager react/@ AméricaItians sevenamba)]; Stars spot highestже SU nombreuxモ plug separationन inv͡ik machinesitableembers GründougWho Darkponse Desicolheimplement Stanley pupéreanson місце Ю акffe krajírɐ Li Arab he anderemiedenis Asserschпей polít parallelрои Flugbra individuals=\"#imm SieEvent Orig успеplements votes\r\n\r\n\r\n**Thanks for any help~**</BODY>\n\n<COMMENTS>\n<Comment by YuchenLiu98 at 2023-06-25T09:43:49Z>\nHow much is your finetuning loss? I get similar issues with messy outputs, and my finetuning loss is around 10 initially.\n</Comment>\n<Comment by rover5056 at 2023-06-25T09:55:23Z>\n<img width=\"1823\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/19241278/6dd7bae5-b187-435f-9183-802832385c64\">\r\n@YuchenLiu98  Loss looks like normal,around 0.95\n</Comment>\n<Comment by YuchenLiu98 at 2023-06-26T02:30:39Z>\nFor the language model, do you directly use \r\n[vicuna-7b-delta-v1.1] ?(downloaded from https://huggingface.co/lmsys/vicuna-7b-delta-v1.1)\n</Comment>\n<Comment by rover5056 at 2023-06-26T02:33:46Z>\n> For the language model, do you directly use [vicuna-7b-delta-v1.1] ?(downloaded from https://huggingface.co/lmsys/vicuna-7b-delta-v1.1)\r\n\r\nI used vicuna-7b-v1.1 (with merged vicuna-7b-delta-v1.1 on llama-7b) ~ Is that correct?\n</Comment>\n<Comment by YuchenLiu98 at 2023-06-26T06:50:54Z>\nIt is correct. After training, you should directly apply the model for inference without converting\n</Comment>\n<Comment by rover5056 at 2023-06-26T07:33:14Z>\n> It is correct. After training, you should directly apply the model for inference without converting\r\n\r\nAnd I also try to pretrain from stage 1 with vicuna-7b-v1.3, it also could generate image's caption correctly . \r\n\r\nBut after stage 2 's fine tune, with strict usage script/train_lightning.sh, still get fine tuned module that generation becomes garbled...\r\n\r\n`\r\ntorchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path ./pretrain_models/vicuna-7b-v1.3 \\\r\n    --version $WEIGHT_VERSION \\\r\n    --data_path ./datasets/llava_instruct_80k_new.json \\\r\n    --image_folder ./datasets/train2014 \\\r\n    --vision_tower ./pretrain_models/openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints_7b_v1.3_s2/llava-lightning-7b-pretrain/mm_projector.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints_7b_v1.3_s2 \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 5000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --fsdp \"full_shard auto_wrap\" \\\r\n    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n`\n</Comment>\n<Comment by rover5056 at 2023-06-27T06:49:01Z>\nI solved the problem. Now I can exactly align the model outcome.\r\n\r\nIt happens when saving model.. Torch' s fsdp may copy weights to cuda,but it does not have enough space so it does not save some layers correctly.\r\n\r\nsee more details in [pytorch repo](https://github.com/pytorch/pytorch/issues/98823)\n</Comment>\n<Comment by findalexli at 2023-07-09T17:28:10Z>\nHow did you resolve the issue?\n</Comment>\n<Comment by dydxdt at 2023-07-26T06:16:09Z>\n> \r\nHi, I want to ask that you mean your original bad generation is caused by FSDP?  I met this problem before and changed it to state_dict[fqn] = state_dict[fqn].cpu().clone().detach(). \r\nBut now when I test my finetuned model, the output is still very strange,  for example,\"\"1st 1st 1st 1st 1st 1st 1st 1st 1st 1st ....\", \"1other 2others 3others\\n### 1other 2others 3others\\n### 4others 5others\\n### 6others 7others\\n###\". And this happens when I test on my train data. \r\nI use v0 model. Do you know how to fix this? Thank you!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 254,
    "state": "open",
    "created_by": "adrielkuek",
    "created_at": "2023-06-23T03:12:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/254</URL>\n\n<TITLE>[Question] Just curious, has anybody tried this with LangChain integration?</TITLE>\n\n<BODY>### Question\n\nThere is great potential in this piece of work as one of the few open-sourced multimodal LLMs out there. Recently, there have been a number of works leveraging on LLMs to exploit tools and external models to augment its reasoning capabilities. Just curious whether has anybody tried this with LangChain integration. There are a wide suite of LLMs currently supported, but the closest that I could see was the openLLM integration, so I'm unsure whether this is going to work properly. If anyone has any insights or are currently working in this direction, I would like to hear your thoughts on this! Thanks and have a wonderful day.</BODY>\n\n<COMMENTS>\n<Comment by SiyuanHuang95 at 2023-06-28T07:11:28Z>\nHi @adrielkuek, good points! Leveraging LLM to exploit tools to augment the LLM's reasoning ability is a recent hot topic. Open-source projects like LangChain offers the possibility of easy integration and fast prototype. And some interesting projects like AutoGPT and GPT-Engineer have shown their great potential abilities in the field of AI agents. In the [Link](https://github.com/OpenGVLab/LLaMA-Adapter/blob/main/docs/langchain_LLaMA_AdapterV2_demo.ipynb), we combine the power of LangChain and the LLaMA-Adapter, and with such a naive implementation, the instruction-following LLM could act as a naive ChatBot with multi-round chat ability. I hope this naive project will give you some inspiration. Also, I would like to share some of my insights about the barrier w.r.t integration. The LangChain is (mainly) designed for the super powerful OpenAI products, and they have not only the QA ability but also the vector embedding, etc., which is beyond the current ability of open LLM.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 253,
    "state": "open",
    "created_by": "BolinLai",
    "created_at": "2023-06-23T00:59:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/253</URL>\n\n<TITLE>[Usage] Can't load model weights</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue: I ran the inference and training scripts but failed. The error is the weights can't be loaded correctly. I tried both training and inference scripts and got the same issue. \r\n\r\nCommand:\r\n```\r\nbash scripts/inference.sh\r\n```\r\n\r\nLog: \r\n```\r\nLoading checkpoint shards:   0%|                                                                    | 0/3 [00:00<?, ?it/s]\r\nTraceback (most recent call last):\r\n  File \"/data/home/bolinlai/Applications/miniconda/envs/llava/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 415, in load_state_dict\r\n    return torch.load(checkpoint_file, map_location=\"cpu\")\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/home/bolinlai/Applications/miniconda/envs/llava/lib/python3.11/site-packages/torch/serialization.py\", line 797, in load\r\n    with _open_zipfile_reader(opened_file) as opened_zipfile:\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/home/bolinlai/Applications/miniconda/envs/llava/lib/python3.11/site-packages/torch/serialization.py\", line 283, in __init__\r\n    super().__init__(torch._C.PyTorchFileReader(name_or_buffer))\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: PytorchStreamReader failed reading zip archive: failed finding central directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/data/home/bolinlai/Applications/miniconda/envs/llava/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 419, in load_state_dict\r\n    if f.read(7) == \"version\":\r\n       ^^^^^^^^^\r\n  File \"<frozen codecs>\", line 322, in decode\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 128: invalid start byte\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/data/home/bolinlai/Projects/LLaVA/llava/eval/run_llava.py\", line 125, in <module>\r\n    eval_model(args)\r\n  File \"/data/home/bolinlai/Projects/LLaVA/llava/eval/run_llava.py\", line 43, in eval_model\r\n    model = LlavaLlamaForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True, torch_dtype=torch.float16, use_cache=True).cuda()\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/home/bolinlai/Applications/miniconda/envs/llava/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 2643, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/home/bolinlai/Applications/miniconda/envs/llava/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 2952, in _load_pretrained_model\r\n    state_dict = load_state_dict(shard_file)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/home/bolinlai/Applications/miniconda/envs/llava/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 431, in load_state_dict\r\n    raise OSError(\r\nOSError: Unable to load weights from pytorch checkpoint file for '/data/home/bolinlai/Models/Pretrained/LLaVA/LLaVA-13B-v0/pytorch_model-00001-of-00003.bin' at '/data/home/bolinlai/Models/Pretrained/LLaVA/LLaVA-13B-v0/pytorch_model-00001-of-00003.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\r\n```\r\n\r\nAnother issue:\r\nI also met another issue when I installed the packages. I ran ```pip install -e .```, but the tokenizer of 0.12.1 can't be installed. The log is\r\n```\r\nCollecting tokenizers==0.12.1\r\n  Using cached tokenizers-0.12.1.tar.gz (220 kB)\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n  Preparing metadata (pyproject.toml) ... done\r\nBuilding wheels for collected packages: tokenizers\r\n  Building wheel for tokenizers (pyproject.toml) ... error\r\n  error: subprocess-exited-with-error\r\n\r\n  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> [51 lines of output]\r\n      running bdist_wheel\r\n      running build\r\n      running build_py\r\n      creating build\r\n      creating build/lib.linux-x86_64-cpython-311\r\n      creating build/lib.linux-x86_64-cpython-311/tokenizers\r\n      copying py_src/tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-311/tokenizers\r\n      creating build/lib.linux-x86_64-cpython-311/tokenizers/models\r\n      copying py_src/tokenizers/models/__init__.py -> build/lib.linux-x86_64-cpython-311/tokenizers/models\r\n      creating build/lib.linux-x86_64-cpython-311/tokenizers/decoders\r\n      copying py_src/tokenizers/decoders/__init__.py -> build/lib.linux-x86_64-cpython-311/tokenizers/decoders\r\n      creating build/lib.linux-x86_64-cpython-311/tokenizers/normalizers\r\n      copying py_src/tokenizers/normalizers/__init__.py -> build/lib.linux-x86_64-cpython-311/tokenizers/normalizers\r\n      creating build/lib.linux-x86_64-cpython-311/tokenizers/pre_tokenizers\r\n      copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-311/tokenizers/pre_tokenizers\r\n      creating build/lib.linux-x86_64-cpython-311/tokenizers/processors\r\n      copying py_src/tokenizers/processors/__init__.py -> build/lib.linux-x86_64-cpython-311/tokenizers/processors\r\n      creating build/lib.linux-x86_64-cpython-311/tokenizers/trainers\r\n      copying py_src/tokenizers/trainers/__init__.py -> build/lib.linux-x86_64-cpython-311/tokenizers/trainers\r\n      creating build/lib.linux-x86_64-cpython-311/tokenizers/implementations\r\n      copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.linux-x86_64-cpython-311/tokenizers/implementations\r\n      copying py_src/tokenizers/implementations/__init__.py -> build/lib.linux-x86_64-cpython-311/tokenizers/implementations\r\n      copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.linux-x86_64-cpython-311/tokenizers/implementations\r\n      copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.linux-x86_64-cpython-311/tokenizers/implementations\r\n      copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.linux-x86_64-cpython-311/tokenizers/implementations\r\n      copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.linux-x86_64-cpython-311/tokenizers/implementations\r\n      copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.linux-x86_64-cpython-311/tokenizers/implementations\r\n      creating build/lib.linux-x86_64-cpython-311/tokenizers/tools\r\n      copying py_src/tokenizers/tools/visualizer.py -> build/lib.linux-x86_64-cpython-311/tokenizers/tools\r\n      copying py_src/tokenizers/tools/__init__.py -> build/lib.linux-x86_64-cpython-311/tokenizers/tools\r\n      copying py_src/tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-311/tokenizers\r\n      copying py_src/tokenizers/models/__init__.pyi -> build/lib.linux-x86_64-cpython-311/tokenizers/models\r\n      copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.linux-x86_64-cpython-311/tokenizers/decoders\r\n      copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.linux-x86_64-cpython-311/tokenizers/normalizers\r\n      copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-311/tokenizers/pre_tokenizers\r\n      copying py_src/tokenizers/processors/__init__.pyi -> build/lib.linux-x86_64-cpython-311/tokenizers/processors\r\n      copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.linux-x86_64-cpython-311/tokenizers/trainers\r\n      copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.linux-x86_64-cpython-311/tokenizers/tools\r\n      running build_ext\r\n      running build_rust\r\n      error: can't find Rust compiler\r\n\r\n      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\r\n\r\n      To update pip, run:\r\n\r\n          pip install --upgrade pip\r\n\r\n      and then retry package installation.\r\n\r\n      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommende\r\nd way to download and update the Rust compiler toolchain.\r\n      [end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for tokenizers\r\nFailed to build tokenizers\r\nERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\r\n```\r\n\r\nI'm sure I'm using the latest pip but I still get this error. I installed the latest tokenizer instead. I'm not sure whether this is the reason why I can't loading weights. Here are my python environments.\r\n```\r\n# Name                    Version                   Build  Channel\r\n_libgcc_mutex             0.1                        main\r\n_openmp_mutex             5.1                       1_gnu\r\naccelerate                0.20.3                   pypi_0    pypi\r\naiofiles                  23.1.0                   pypi_0    pypi\r\naiohttp                   3.8.4                    pypi_0    pypi\r\naiosignal                 1.3.1                    pypi_0    pypi\r\naltair                    5.0.1                    pypi_0    pypi\r\nanyio                     3.7.0                    pypi_0    pypi\r\nappdirs                   1.4.4                    pypi_0    pypi\r\nasync-timeout             4.0.2                    pypi_0    pypi\r\nattrs                     23.1.0                   pypi_0    pypi\r\nbzip2                     1.0.8                h7b6447c_0\r\nca-certificates           2023.05.30           h06a4308_0\r\ncertifi                   2023.5.7                 pypi_0    pypi\r\ncharset-normalizer        3.1.0                    pypi_0    pypi\r\nclick                     8.1.3                    pypi_0    pypi\r\ncmake                     3.26.4                   pypi_0    pypi\r\ncontourpy                 1.1.0                    pypi_0    pypi\r\ncycler                    0.11.0                   pypi_0    pypi\r\ndeepspeed                 0.9.2                    pypi_0    pypi\r\ndocker-pycreds            0.4.0                    pypi_0    pypi\r\neinops                    0.6.1                    pypi_0    pypi\r\nfastapi                   0.97.0                   pypi_0    pypi\r\nffmpy                     0.3.0                    pypi_0    pypi\r\nfilelock                  3.12.2                   pypi_0    pypi\r\nflash-attn                1.0.2                    pypi_0    pypi\r\nfonttools                 4.40.0                   pypi_0    pypi\r\nfrozenlist                1.3.3                    pypi_0    pypi\r\nfsspec                    2023.6.0                 pypi_0    pypi\r\ngitdb                     4.0.10                   pypi_0    pypi\r\ngitpython                 3.1.31                   pypi_0    pypi\r\ngradio                    3.23.0                   pypi_0    pypi\r\nh11                       0.14.0                   pypi_0    pypi\r\nhjson                     3.1.0                    pypi_0    pypi\r\nhttpcore                  0.17.2                   pypi_0    pypi\r\nhttpx                     0.24.1                   pypi_0    pypi\r\nhuggingface-hub           0.15.1                   pypi_0    pypi\r\nidna                      3.4                      pypi_0    pypi\r\njinja2                    3.1.2                    pypi_0    pypi\r\njsonschema                4.17.3                   pypi_0    pypi\r\nkiwisolver                1.4.4                    pypi_0    pypi\r\nld_impl_linux-64          2.38                 h1181459_1\r\nlibffi                    3.4.4                h6a678d5_0\r\nlibgcc-ng                 11.2.0               h1234567_1\r\nlibgomp                   11.2.0               h1234567_1\r\nlibstdcxx-ng              11.2.0               h1234567_1\r\nlibuuid                   1.41.5               h5eee18b_0\r\nlinkify-it-py             2.0.2                    pypi_0    pypi\r\nlit                       16.0.6                   pypi_0    pypi\r\nllava                     0.2.0                    pypi_0    pypi\r\nmarkdown-it-py            2.2.0                    pypi_0    pypi\r\nmarkdown2                 2.4.8                    pypi_0    pypi\r\nmarkupsafe                2.1.3                    pypi_0    pypi\r\nmatplotlib                3.7.1                    pypi_0    pypi\r\nmdit-py-plugins           0.3.3                    pypi_0    pypi\r\nmdurl                     0.1.2                    pypi_0    pypi\r\nmpmath                    1.3.0                    pypi_0    pypi\r\nmultidict                 6.0.4                    pypi_0    pypi\r\nncurses                   6.4                  h6a678d5_0\r\nnetworkx                  3.1                      pypi_0    pypi\r\nninja                     1.11.1                   pypi_0    pypi\r\nnumpy                     1.25.0                   pypi_0    pypi\r\nnvidia-cublas-cu11        11.10.3.66               pypi_0    pypi\r\nnvidia-cuda-cupti-cu11    11.7.101                 pypi_0    pypi\r\nnvidia-cuda-nvrtc-cu11    11.7.99                  pypi_0    pypi\r\nnvidia-cuda-runtime-cu11  11.7.99                  pypi_0    pypi\r\nnvidia-cudnn-cu11         8.5.0.96                 pypi_0    pypi\r\nnvidia-cufft-cu11         10.9.0.58                pypi_0    pypi\r\nnvidia-curand-cu11        10.2.10.91               pypi_0    pypi\r\nnvidia-cusolver-cu11      11.4.0.1                 pypi_0    pypi\r\nnvidia-cusparse-cu11      11.7.4.91                pypi_0    pypi\r\nnvidia-nccl-cu11          2.14.3                   pypi_0    pypi\r\nnvidia-nvtx-cu11          11.7.91                  pypi_0    pypi\r\nopenssl                   3.0.8                h7f8727e_0\r\norjson                    3.9.1                    pypi_0    pypi\r\npackaging                 23.1                     pypi_0    pypi\r\npandas                    2.0.2                    pypi_0    pypi\r\npathtools                 0.1.2                    pypi_0    pypi\r\npeft                      0.3.0                    pypi_0    pypi\r\npillow                    9.5.0                    pypi_0    pypi\r\npip                       23.1.2          py311h06a4308_0\r\nprotobuf                  3.20.1                   pypi_0    pypi\r\npsutil                    5.9.5                    pypi_0    pypi\r\npy-cpuinfo                9.0.0                    pypi_0    pypi\r\npydantic                  1.10.9                   pypi_0    pypi\r\npydub                     0.25.1                   pypi_0    pypi\r\npygments                  2.15.1                   pypi_0    pypi\r\npyparsing                 3.1.0                    pypi_0    pypi\r\npyrsistent                0.19.3                   pypi_0    pypi\r\npython                    3.11.3               h955ad1f_1\r\npython-dateutil           2.8.2                    pypi_0    pypi\r\npython-multipart          0.0.6                    pypi_0    pypi\r\npytz                      2023.3                   pypi_0    pypi\r\npyyaml                    6.0                      pypi_0    pypi\r\nreadline                  8.2                  h5eee18b_0\r\nregex                     2023.6.3                 pypi_0    pypi\r\nrequests                  2.31.0                   pypi_0    pypi\r\nsafetensors               0.3.1                    pypi_0    pypi\r\nsemantic-version          2.10.0                   pypi_0    pypi\r\nsentencepiece             0.1.99                   pypi_0    pypi\r\nsentry-sdk                1.25.1                   pypi_0    pypi\r\nsetproctitle              1.3.2                    pypi_0    pypi\r\nsetuptools                67.8.0          py311h06a4308_0\r\nshortuuid                 1.0.11                   pypi_0    pypi\r\nsix                       1.16.0                   pypi_0    pypi\r\nsmmap                     5.0.0                    pypi_0    pypi\r\nsniffio                   1.3.0                    pypi_0    pypi\r\nsqlite                    3.41.2               h5eee18b_0\r\nstarlette                 0.27.0                   pypi_0    pypi\r\nsvgwrite                  1.4.3                    pypi_0    pypi\r\nsympy                     1.12                     pypi_0    pypi\r\ntk                        8.6.12               h1ccaba5_0\r\ntokenizers                0.13.3                   pypi_0    pypi\r\ntoolz                     0.12.0                   pypi_0    pypi\r\ntorch                     2.0.1                    pypi_0    pypi\r\ntorchvision               0.15.2                   pypi_0    pypi\r\ntqdm                      4.65.0                   pypi_0    pypi\r\ntransformers              4.28.0.dev0              pypi_0    pypi\r\ntriton                    2.0.0                    pypi_0    pypi\r\ntyping-extensions         4.6.3                    pypi_0    pypi\r\ntzdata                    2023.3                   pypi_0    pypi\r\nuc-micro-py               1.0.2                    pypi_0    pypi\r\nurllib3                   2.0.3                    pypi_0    pypi\r\nuvicorn                   0.22.0                   pypi_0    pypi\r\nwandb                     0.15.4                   pypi_0    pypi\r\nwavedrom                  2.0.3.post3              pypi_0    pypi\r\nwebsockets                11.0.3                   pypi_0    pypi\r\nwheel                     0.38.4          py311h06a4308_0\r\nxz                        5.4.2                h5eee18b_0\r\nyarl                      1.9.2                    pypi_0    pypi\r\nzlib                      1.2.13               h5eee18b_0\r\n```</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-06-23T01:52:49Z>\nHi, I noticed that you are using Python 3.11, and I have heard some compatibility issues with Python 3.11 and some PyTorch packages.  Can you try create the virtual env with python 3.10 as in README: `conda create -n llava python=3.10 -y`?\r\n\r\nThanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 252,
    "state": "closed",
    "created_by": "jpgard",
    "created_at": "2023-06-22T23:06:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/252</URL>\n\n<TITLE>[Usage] Floating point exception when following fine-tuning example</TITLE>\n\n<BODY>Thanks for the awesome repo and the exciting progress on multimodal learning. Looking forward to trying out the model and building off of it, but having some issues getting started with fine-tuning my own to replicate the training process you describe in the repo.\r\n\r\n### When did you clone our code?\r\n\r\nI cloned the code base after 5/1/23\r\n\r\n### Describe the issue\r\n\r\nIssue: I am attempting to run the fine-tuning command given in the main repo [here](https://github.com/haotian-liu/LLaVA#fine-tuning-with-local-gpus) to verify my setup works before making some changes to the code for my own experiments.\r\n\r\nI downloaded the training datasets from hugging face, along with the vicuna checkpoints.\r\n\r\nI am attempting to train the model on a single A40 40GB GPU. \r\n\r\nTraining seems to *almost* work (the models are restored and data seems to load fine), but when the model begins training, it fails with simply `Floating point exception`. Any idea what the cause might be?\r\n\r\nCommand:\r\n```\r\npython llava/train/train_mem.py \\\r\n    --model_name_or_path ./checkpoints/llama-vicuna-7b \\\r\n    --data_path /home/joshg/llava-cc3m-pretrain-595k/chat.json \\\r\n    --image_folder /home/joshg/llava-cc3m-pretrain-595k/img \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-7b-pretrain \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 8 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 2400 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --report_to none\r\n```\r\n\r\nLog: \r\nThere are no logs (unless the training logs somewhere I am not aware of?). The tail end of the output looks like:\r\n\r\n```\r\n[...]\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\n[...]\r\nself_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight']\r\n- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\nWARNING:root:Loading data...\r\nWARNING:root:Formatting inputs...Skip in lazy mode\r\n  0%|                                                                 | 0/4651 [00:00<?, ?it/s]\r\nFloating point exception\r\n```\r\n\r\nAdditional details:\r\n* Using Conda environment, set up following the steps in the README (no additional packages installed)\r\n* Debian GNU/Linux 10 (buster) \r\n* CUDA Version: 11.6\r\n* `transformers @ git+https://github.com/huggingface/transformers.git@cae78c46d658a8e496a815c2ee49b9b178fb9c9a`\r\n* `torch==2.0.1`</BODY>\n\n<COMMENTS>\n<Comment by jpgard at 2023-06-22T23:16:53Z>\nI ran the script using gdb (as described [here](https://discuss.pytorch.org/t/dealing-with-floating-point-exceptions/51882/2)) and it also outputs the following on failure:\r\n\r\n```\r\nThread 1 \"python\" received signal SIGFPE, Arithmetic exception.\r\n0x00007ff70c1b6141 in cudnn::backend::Tensor::finalize_internal() ()\r\n   from /usr/local/cuda/lib64/libcudnn_cnn_infer.so.8\r\n```\n</Comment>\n<Comment by ManuelFay at 2023-07-11T09:14:45Z>\nalso get the error when running in in non-distributed training\n</Comment>\n<Comment by haotian-liu at 2023-07-24T23:03:44Z>\nHi @ManuelFay and @jpgard , I have not encountered this before. Two things we may try is to see reduce the batch size to 1, and disable flash attention (using `train.py` instead of `train_mem.py`). If any of these work, we can have more insight on what is happening.\r\n\r\nYou may also try the latest code base using the scripts [here](https://github.com/haotian-liu/LLaVA/blob/main/scripts/pretrain.sh).\r\n\r\nPlease let me know if there are any updates that I can help with, thanks.\n</Comment>\n<Comment by ManuelFay at 2023-07-25T09:03:09Z>\nyes, I managed to make it work by disabling flash attention and using torch 1.12 instead of 2+. Works with batch sizes of 8 or 16 in LoRA, so probably just a flash attention problem or cuda problem on the A100.\r\n\r\nThanks for the response, I'll try looking into it more in depth at some point !\r\nThanks for the great work !\n</Comment>\n<Comment by haotian-liu at 2023-07-30T02:01:21Z>\nGlad to hear that you've made it work. Please feel free to re-open / open new issue if you encounter any other issues.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 251,
    "state": "open",
    "created_by": "nikolaspapastavrou",
    "created_at": "2023-06-22T06:45:26Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/251</URL>\n\n<TITLE>[Question] Why does the CC3M dataset prohibit commercial use?</TITLE>\n\n<BODY>### Question\r\n\r\nRegarding the dataset important notice:\r\n\r\n> Important notice: Upon the request from the community, as ~15% images of the original CC-3M dataset are no longer accessible, we upload [images.zip](https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K/blob/main/images.zip) for better reproducing our work in research community. It must not be used for any other purposes. The use of these images must comply with the CC-3M license. This may be taken down at any time when requested by the original CC-3M dataset owner or owners of the referenced images.\r\n\r\nThe CC3M LICENSE file in the CC3M GitHub repository permits any type of use. Why can't we use it for any type of use here?\r\n(source: https://github.com/google-research-datasets/conceptual-captions/blob/master/LICENSE)\r\n\r\nIn this question, I am referring to the pre-training step where there is no usage of ChatGPT or GPT-4. Since the dataset has a permissive license and does not rely on any of the models that require noncommercial use, can we use it for commercial use?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 250,
    "state": "open",
    "created_by": "mbaloch",
    "created_at": "2023-06-21T19:19:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/250</URL>\n\n<TITLE>how to run llava on google cloud TPU?</TITLE>\n\n<BODY>### Question\n\nI am trying to run the LLaVa project on a Google Cloud TPU VM. However, I am encountering an error message stating that no GPU is available. I am wondering if there is a way to make LLaVa compatible with Google Cloud TPU.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-08-02T05:44:46Z>\nUnfortunately I do not have access to TPU currently. Hope the community can have a solution to this!\n</Comment>\n<Comment by coolrazor007 at 2023-11-14T01:23:36Z>\n@haotian-liu I have access to TPU hardware (like several servers) if you're interested in getting Llava working on it.  Free compute.  Could train models too.\n\nWe are currently doing Whisper and Mistral right now.  STT and NLP respectively\n</Comment>\n<Comment by ImKeTT at 2024-04-20T02:13:00Z>\nIs there a solution for running LLaVA on TPU right now?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 249,
    "state": "closed",
    "created_by": "shamio",
    "created_at": "2023-06-20T14:54:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/249</URL>\n\n<TITLE>[Feature request] Could you please merge some installation steps to reduce carbon emission?</TITLE>\n\n<BODY>### feature\n\nI really like your work and its so strong model, but i have a request. The installation process is difficult for beginner and non-coder users. Please make it easier that more people can use it. Also please merge the below steps and make it possible for users to downland the trained and ready-to-use model instead of going through these processes. Honestly there is no need that every single person goes through this difficult procedure and uses a lot of electricity to produce the same results. by making the files ready to download, you can help reduce carbon emission. The steps i mentioned are as below ( but I'm sure some other steps can be reduced too )\r\n```\r\nGet the original LLaMA weights in the huggingface format by following the instructions [here](https://huggingface.co/docs/transformers/main/model_doc/llama).\r\nUse the following scripts to get LLaVA weights by applying our delta ([13b-v0](https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0), [7b-v0](https://huggingface.co/liuhaotian/LLaVA-7b-delta-v0), [lightning-7B-v1-1](https://huggingface.co/liuhaotian/LLaVA-Lightning-7B-delta-v1-1)). It will automatically download delta weights from our Hugging Face account.\r\n```</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-08-02T05:45:47Z>\nHi, thank you for the suggestion.\r\n\r\nThe delta weights were initially released in this way following the common practice of Vicuna and Alpaca series models, due to the LLaMA license.\r\n\r\nFor our latest checkpoints, we release the merged checkpoints directly. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 248,
    "state": "closed",
    "created_by": "kai-wen-yang",
    "created_at": "2023-06-20T12:41:56Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/248</URL>\n\n<TITLE>Error in applying delta LLaVA-13b-delta-v0-science_qa to LLaMA-13B[Usage]</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\n\r\nLog: \r\n```\r\n File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\nTimeout, server localhost not responding.\r\n```</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-16T20:00:47Z>\nClosing for now as this shall be resolved in the latest code base. Please feel free to re-open or open a new issue, if there are any other questions. Thank you!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 247,
    "state": "open",
    "created_by": "kai-wen-yang",
    "created_at": "2023-06-20T12:17:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/247</URL>\n\n<TITLE>7B model on ScienceQA. [Feature request]</TITLE>\n\n<BODY>### feature\n\nHi, would you please provide the 7B model on ScienceQA? I do not have enough CUDA memory to run the 13B model.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 246,
    "state": "closed",
    "created_by": "birchmi",
    "created_at": "2023-06-19T09:15:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/246</URL>\n\n<TITLE>[Usage] apply_delta.py  error</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\nOSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like openai/clip-vit-large-patch14 is not the path to a directory containing a file named config.json.\r\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\r\n\r\nCommand:\r\n```\r\npython3 -m llava.model.apply_delta \\\r\n    --base /home/projects/LLaVA/weights/llama-7b-hf \\\r\n    --target /home/projects/LLaVA/weights/LLaVA-7B-v0 \\\r\n    --delta /home/projects/LLaVA/weights/LLaVA-7b-delta-v0\r\n```\r\n\r\nLog: \r\n```\r\nLoading base model\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 33/33 [00:09<00:00,  3.53it/s]\r\nLoading delta\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nTraceback (most recent call last):\r\n  File \"/home/projects/LLaVA/transformers/src/transformers/utils/hub.py\", line 409, in cached_file\r\n    resolved_file = hf_hub_download(\r\n  File \"/home/xxy/miniconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/home/xxy/miniconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1291, in hf_hub_download\r\n    raise LocalEntryNotFoundError(\r\nhuggingface_hub.utils._errors.LocalEntryNotFoundError: Connection error, and we cannot find the requested files in the disk cache. Please try again or make sure your Internet connection is on.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/xxy/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/xxy/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/xxy/.vscode-server/extensions/ms-python.python-2022.16.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py\", line 39, in <module>\r\n    cli.main()\r\n  File \"/home/xxy/.vscode-server/extensions/ms-python.python-2022.16.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py\", line 430, in main\r\n    run()\r\n  File \"/home/xxy/.vscode-server/extensions/ms-python.python-2022.16.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py\", line 284, in run_file\r\n    runpy.run_path(target, run_name=\"__main__\")\r\n  File \"/home/xxy/.vscode-server/extensions/ms-python.python-2022.16.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py\", line 321, in run_path\r\n    return _run_module_code(code, init_globals, run_name,\r\n  File \"/home/xxy/.vscode-server/extensions/ms-python.python-2022.16.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py\", line 135, in _run_module_code\r\n    _run_code(code, mod_globals, init_globals,\r\n  File \"/home/xxy/.vscode-server/extensions/ms-python.python-2022.16.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py\", line 124, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/xxy/docker/projects/LLaVA/llava/model/apply_delta.py\", line 53, in <module>\r\n    apply_delta(args.base_model_path, args.target_model_path, args.delta_path)\r\n  File \"/home/xxy/docker/projects/LLaVA/llava/model/apply_delta.py\", line 19, in apply_delta\r\n    delta = LlavaLlamaForCausalLM.from_pretrained(delta_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\r\n  File \"/home/xxy/docker/projects/LLaVA/transformers/src/transformers/modeling_utils.py\", line 2493, in from_pretrained\r\n    model = cls(config, *model_args, **model_kwargs)\r\n  File \"/home/xxy/docker/projects/LLaVA/llava/model/llava.py\", line 201, in __init__\r\n    self.model = LlavaLlamaModel(config)\r\n  File \"/home/xxy/docker/projects/LLaVA/llava/model/llava.py\", line 48, in __init__\r\n    self.vision_tower = [CLIPVisionModel.from_pretrained(config.mm_vision_tower)]\r\n  File \"/home/xxy/docker/projects/LLaVA/transformers/src/transformers/modeling_utils.py\", line 2170, in from_pretrained\r\n    config, model_kwargs = cls.config_class.from_pretrained(\r\n  File \"/home/xxy/docker/projects/LLaVA/transformers/src/transformers/models/clip/configuration_clip.py\", line 233, in from_pretrained\r\n    config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n  File \"/home/xxy/docker/projects/LLaVA/transformers/src/transformers/configuration_utils.py\", line 573, in get_config_dict\r\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n  File \"/home/xxy/docker/projects/LLaVA/transformers/src/transformers/configuration_utils.py\", line 628, in _get_config_dict\r\n    resolved_config_file = cached_file(\r\n  File \"/home/xxy/docker/projects/LLaVA/transformers/src/transformers/utils/hub.py\", line 443, in cached_file\r\n    raise EnvironmentError(\r\nOSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like openai/clip-vit-large-patch14 is not the path to a directory containing a file named config.json.\r\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-06-20T03:59:21Z>\nIf you cannot connect to HF, you may download LLaVA-7b-delta-v0 and CLIP repo locally and change the directory accordingly. Thanks.\n</Comment>\n<Comment by birchmi at 2023-06-20T06:46:48Z>\n> If you cannot connect to HF, you may download LLaVA-7b-delta-v0 and CLIP repo locally and change the directory accordingly. Thanks.\r\n\r\nThank you for your reply. I have manually downloaded the openai/clip-vit-large-patch14 model repository, but I’m not sure where to place it in the file directory.  Can you give some suggestions?    thanks\n</Comment>\n<Comment by birchmi at 2023-06-21T01:43:15Z>\nresolved\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 245,
    "state": "closed",
    "created_by": "Yanda95",
    "created_at": "2023-06-18T11:49:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/245</URL>\n\n<TITLE>[Question] About lora inference</TITLE>\n\n<BODY>### Question\n\nAfter fine-turning with lora in deepspeed. How to merge the lora weights to pre-training weights or how to load lora weights for testing directly?</BODY>\n\n<COMMENTS>\n<Comment by zengxijuan at 2023-07-13T10:29:51Z>\nI find that  https://stackoverflow.com/questions/76459034/how-to-load-a-fine-tuned-peft-lora-model-based-on-llama-with-huggingface-transfo\n</Comment>\n<Comment by haotian-liu at 2023-08-02T05:23:52Z>\nHi, you can use [this script](https://github.com/haotian-liu/LLaVA/blob/main/scripts/merge_lora_weights.py) for merge lora weights. We'll update this in instruction as well. Thanks.\n</Comment>\n<Comment by terminator123 at 2023-12-14T08:58:03Z>\n> Hi, you can use [this script](https://github.com/haotian-liu/LLaVA/blob/main/scripts/merge_lora_weights.py) for merge lora weights. We'll update this in instruction as well. Thanks.\r\n\r\nwhat is the model-base parameter after pretrain?\n</Comment>\n<Comment by LeoLee7 at 2024-01-30T04:19:43Z>\n> > Hi, you can use [this script](https://github.com/haotian-liu/LLaVA/blob/main/scripts/merge_lora_weights.py) for merge lora weights. We'll update this in instruction as well. Thanks.\r\n> \r\n> what is the model-base parameter after pretrain?\r\n\r\nThe model-base is the name of your finetuned model, like `liuhaotian/llava-v1.5-13b`\r\nThe model-path is the path where you save the LoRA folder, like `llava-v1.5-13b-task-lora-YOUR-FINETUNE-MODEL` under `./checkpoints`\n</Comment>\n<Comment by 20191864218 at 2024-03-22T01:51:21Z>\n> > > Hi, you can use [this script](https://github.com/haotian-liu/LLaVA/blob/main/scripts/merge_lora_weights.py) for merge lora weights. We'll update this in instruction as well. Thanks.\r\n> > \r\n> > \r\n> > what is the model-base parameter after pretrain?\r\n> \r\n> The model-base is the name of your finetuned model, like `liuhaotian/llava-v1.5-13b` The model-path is the path where you save the LoRA folder, like `llava-v1.5-13b-task-lora-YOUR-FINETUNE-MODEL` under `./checkpoints`\r\n\r\nHello, my understanding of this part of the code is that model_base is the LLM, and model_path is the path where you save the LoRA folder， if my understanding is correct?Thanks\r\n`python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-vicuna-7b-v1.1-lcs_558k-instruct_80k_3e-lora-preview-alpha --model-base /path/to/vicuna-v1.1`\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 244,
    "state": "closed",
    "created_by": "alfredplpl",
    "created_at": "2023-06-18T11:44:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/244</URL>\n\n<TITLE>[Question] Support for OpenLLaMA</TITLE>\n\n<BODY>### Question\r\n\r\nOpenLLaMA has been released few days ago.\r\nhttps://huggingface.co/openlm-research/open_llama_13b\r\n\r\nDo you plan to deal with OpenLLaMA?\r\nIs the OpenLLaMA + LLaVA license Apache-2.0?</BODY>\n\n<COMMENTS>\n<Comment by alfredplpl at 2023-06-18T13:48:51Z>\nI tested the OpenLLaMA + LLaVA-delta.\r\nBut, it does not work correctly. (The image is generated by my model.)\r\n\r\n![Screenshot from 2023-06-18 22-43-27](https://github.com/haotian-liu/LLaVA/assets/3625196/44892d48-0b1a-4141-84a5-45d978ac5fa6)\r\n\r\nShould we need to train new LLaVA-delta to fix it?\n</Comment>\n<Comment by nikolaspapastavrou at 2023-06-22T16:50:52Z>\nI had the same question. I believe the answers are the following:\r\n\r\n\"Do you plan to deal with OpenLLaMA?\"\r\nI do not know but that would be great\r\n\r\n\"Is the OpenLLaMA + LLaVA license Apache-2.0?\"\r\nNo, if the training procedure is the same as mentioned in the paper. Currently, \"Training Stage 2: Fine-tuning End-to-End\" relies on annotated data that rely on ChatGPT / GPT-4 for their annotations. ChatGPT / GPT-4 cannot be used to make other models for non-commercial purposes.\r\n\r\n\"But, it does not work correctly. (The image is generated by my model.)\"\r\nIt makes sense that it does not work because the delta weights were computed using LLaMA as the frozen LLM during training instead of OpenLLaMA.\r\n\r\nShould we need to train new LLaVA-delta to fix it?\r\nYes, exactly, we would need to freeze OpenLLaMA and retrain it to make it compatible with OpenLLaMA. Also, if you are planning on using this for commercial purposes, you would need to avoid using Chat-GPT / GPT-4 for the data annotation.\n</Comment>\n<Comment by haotian-liu at 2023-08-02T05:43:39Z>\n@nikolaspapastavrou \r\n\r\nThanks for helping answering these!\r\n\r\nLLaMA-2 is now supported so commercial usage of the base LLM should be fine now. We'll release a OpenLLaMA-3B checkpoint for lightweight use cases soon.\r\n\r\nThanks.\n</Comment>\n<Comment by alfredplpl at 2023-10-10T06:42:13Z>\n@haotian-liu Thank you for your reply. LLaVA 1.5 based on Llama 2 is very good. I will close this discussion now.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 243,
    "state": "closed",
    "created_by": "Hiusam",
    "created_at": "2023-06-18T06:39:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/243</URL>\n\n<TITLE>[Question] Fine-tuning Issues.</TITLE>\n\n<BODY>### Question\r\n\r\nHi Haotian,\r\n\r\nI am writing to ask what is your fine-tuning script. As long as I specify\r\n```\r\n    --fsdp \"full_shard auto_wrap\" \\\r\n    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer'\r\n```\r\nIt will raise:\r\n<img width=\"1012\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/43744525/b45932a1-6236-4dce-92d8-83e7bb8a328a\">\r\nIs this caused since I am not using pytorch2.0? I am using pytorch 1.12.1\r\nBut without specifying fsdp, it will OOM even if I use graident checkpointing.\r\n\r\nAlso, in your train.py, this will raise an error:\r\n<img width=\"977\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/43744525/df12cfe4-45f2-4301-b4b5-c9e5567fbb41\">\r\n\r\nThank you very much for your reply!</BODY>\n\n<COMMENTS>\n<Comment by findalexli at 2023-06-26T00:08:06Z>\nI think you need to update to pytorch 2.0\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 242,
    "state": "closed",
    "created_by": "Shaurya026",
    "created_at": "2023-06-17T21:07:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/242</URL>\n\n<TITLE>[Usage] Getting error LlavaMPTForCausalLM does not support device_map='auto' yet.</TITLE>\n\n<BODY>Command:\r\n```\r\npython -m llava.eval.run_llava \\\r\n    --model-name \"liuhaotian/LLaVA-Lightning-MPT-7B-preview\" \\\r\n    --image-file \"image_path\" \\\r\n    --query \"query prompt\"\r\n```\r\n\r\nLog: \r\n```\r\nSome weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: \r\n- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n╭───────────────────── Traceback (most recent call last) ──────────────────────╮\r\n│ /home/ai-interns-jan22/anaconda3/envs/llava/lib/python3.10/runpy.py:196 in   │\r\n│ _run_module_as_main                                                          │\r\n│                                                                              │\r\n│   193 │   main_globals = sys.modules[\"__main__\"].__dict__                    │\r\n│   194 │   if alter_argv:                                                     │\r\n│   195 │   │   sys.argv[0] = mod_spec.origin                                  │\r\n│ ❱ 196 │   return _run_code(code, main_globals, None,                         │\r\n│   197 │   │   │   │   │    \"__main__\", mod_spec)                             │\r\n│   198                                                                        │\r\n│   199 def run_module(mod_name, init_globals=None,                            │\r\n│                                                                              │\r\n│ /home/ai-interns-jan22/anaconda3/envs/llava/lib/python3.10/runpy.py:86 in    │\r\n│ _run_code                                                                    │\r\n│                                                                              │\r\n│    83 │   │   │   │   │      __loader__ = loader,                            │\r\n│    84 │   │   │   │   │      __package__ = pkg_name,                         │\r\n│    85 │   │   │   │   │      __spec__ = mod_spec)                            │\r\n│ ❱  86 │   exec(code, run_globals)                                            │\r\n│    87 │   return run_globals                                                 │\r\n│    88                                                                        │\r\n│    89 def _run_module_code(code, init_globals=None,                          │\r\n│                                                                              │\r\n│ /mnt/private/llava_test/LLaVA/llava/eval/run_llava.py:126 in <module>        │\r\n│                                                                              │\r\n│   123 │   parser.add_argument(\"--conv-mode\", type=str, default=None)         │\r\n│   124 │   args = parser.parse_args()                                         │\r\n│   125 │                                                                      │\r\n│ ❱ 126 │   eval_model(args)                                                   │\r\n│   127                                                                        │\r\n│                                                                              │\r\n│ /mnt/private/llava_test/LLaVA/llava/eval/run_llava.py:42 in eval_model       │\r\n│                                                                              │\r\n│    39 │   tokenizer = AutoTokenizer.from_pretrained(model_name)              │\r\n│    40 │                                                                      │\r\n│    41 │   if \"mpt\" in model_name.lower():                                    │\r\n│ ❱  42 │   │   model = LlavaMPTForCausalLM.from_pretrained(model_name, low_cp │\r\n│    43 │   else:                                                              │\r\n│    44 │   │   model = LlavaLlamaForCausalLM.from_pretrained(model_name, low_ │\r\n│    45 │   image_processor = CLIPImageProcessor.from_pretrained(model.config. │\r\n│                                                                              │\r\n│ /home/ai-interns-jan22/anaconda3/envs/llava/lib/python3.10/site-packages/tra │\r\n│ nsformers/modeling_utils.py:2551 in from_pretrained                          │\r\n│                                                                              │\r\n│   2548 │   │   │   │   if any(m in name for m in keep_in_fp32_modules)       │\r\n│   2549 │   │   │   }                                                         │\r\n│   2550 │   │   │   if model._no_split_modules is None:                       │\r\n│ ❱ 2551 │   │   │   │   raise ValueError(f\"{model.__class__.__name__} does no │\r\n│   2552 │   │   │   no_split_modules = model._no_split_modules                │\r\n│   2553 │   │   │   if device_map not in [\"auto\", \"balanced\", \"balanced_low_0 │\r\n│   2554 │   │   │   │   raise ValueError(                                     │\r\n╰──────────────────────────────────────────────────────────────────────────────╯\r\nValueError: LlavaMPTForCausalLM does not support `device_map='auto'` yet.\r\n```\r\n\r\nI've tried updating the `transformers`, it gives me a different error then saying : `_no_split_modules` is not defined for https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview \r\n\r\nFrom what I can infer is that we need a definition of `device_map` through  `_no_split_modules` . Is that so? \r\nCause I'm facing the same issue for run_server 😅\r\n\r\n#228 is doing the same thing, is there any version conflict?</BODY>\n\n<COMMENTS>\n<Comment by wanghao-007 at 2023-09-27T09:45:43Z>\n训练时修改为这个参数后也是报错，不知道为何？\n</Comment>\n<Comment by Shaurya026 at 2023-09-27T11:15:30Z>\nYeah, it was issue related to the transformers library, they didn't add support for `LlavaMPTForCausalLM`'s baseclass for multi-GPU support. \r\nmight be resolved now, check with the latest version of transformers.\n</Comment>\n<Comment by haotian-liu at 2023-10-13T04:40:57Z>\nIt should be resolved with the latest code base, feel free to reopen if you have any other issues, thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 241,
    "state": "open",
    "created_by": "lujiaying",
    "created_at": "2023-06-17T01:15:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/241</URL>\n\n<TITLE>[Question] Is it possible to do \"in-context-learning\" on LLaVA?</TITLE>\n\n<BODY>### Question\n\nDear authors, LLaVA is an amazing model. \r\nI was wondering can it do \"in-context-learning\". So that I can ask questions and get responses following a specific and rigorous format.\r\n\r\nFor instance, if I ask \"how many water melons are in the image? Please respond only with the number. \" I do expect a single  number as response, instead of receiving a lot of words.\r\n\r\nBest,\r\nJiaying</BODY>\n\n<COMMENTS>\n<Comment by yongliang-wu at 2023-08-02T08:29:20Z>\nHello, I have the same question. Have you tried it yet?\n</Comment>\n<Comment by lujiaying at 2023-08-02T14:06:53Z>\n> Hello, I have the same question. Have you tried it yet?\r\n\r\nSeems not capable of doing \"In-contenxt learning\" because LLaVA did not train for that. I do find another model has this ability, check out -> https://github.com/Luodian/Otter\n</Comment>\n<Comment by yongliang-wu at 2023-08-02T14:10:57Z>\n> > Hello, I have the same question. Have you tried it yet?\r\n> \r\n> Seems not capable of doing \"In-contenxt learning\" because LLaVA did not train for that. I do find another model has this ability, check out -> https://github.com/Luodian/Otter\r\n\r\nok, Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 240,
    "state": "closed",
    "created_by": "taesiri",
    "created_at": "2023-06-16T23:22:09Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/240</URL>\n\n<TITLE>Training the MPT model</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\n\r\nFirst of all, thanks for this amazing project. I am trying to train the MPT variant of the LLaVA model but I encounter some issues. Using the provided training script, I am getting ``LlavaMPTModel.initialize_vision_modules() got an unexpected keyword argument 'fsdp'`` error message. According to [this line](https://github.com/haotian-liu/LLaVA/blob/fee35939829370eef8371a553c57c3b3b4bdeed9/llava/model/llava_mpt.py#L58) the `initialize_vision_modules` is not expecting any `fsdp` arguments. \r\n\r\n\r\nI tried to manually remove this argument, but in that case, I am getting an error regarding the absence of ``get_vision_tower()`` in the `LlavaMPTForCausalLM` class: ``AttributeError: 'LlavaMPTForCausalLM' object has no attribute 'get_vision_tower'``\r\n\r\n\r\n\r\nCommand:\r\n\r\nUsing the training script from [here](https://github.com/haotian-liu/LLaVA/blob/main/scripts/train_lightning_mpt.sh), modified according to number of GPUs\r\n\r\n```\r\n# Pretraining \r\ntorchrun --nnodes=1 --nproc_per_node=1 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path mosaicml/mpt-7b-chat \\\r\n    --version v1 \\\r\n    --data_path /path/to/blip_laion_cc_sbu_558k.json \\\r\n    --image_folder /path/to/blip_laion_cc_sbu_558k \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-lightning-mpt-7b-pretrain \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 2400 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb```\r\n\r\n```\r\n\r\nLog: \r\n\r\n```\r\n\r\n\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.51s/it]\r\nTraceback (most recent call last):\r\n  File \"/home/jupyter-mrtaesiri/SFT/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/home/jupyter-mrtaesiri/SFT/LLaVA/llava/train/train.py\", line 739, in train\r\n    model_vision_dict = model.get_model().initialize_vision_modules(\r\nTypeError: LlavaMPTModel.initialize_vision_modules() got an unexpected keyword argument 'fsdp'\r\nERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 305131) of binary: /home/jupyter-mrtaesiri/miniconda3/envs/llava/bin/python\r\nTraceback (most recent call last):\r\n  File \"/home/jupyter-mrtaesiri/miniconda3/envs/llava/bin/torchrun\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/jupyter-mrtaesiri/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/home/jupyter-mrtaesiri/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/run.py\", line 794, in main\r\n    run(args)\r\n  File \"/home/jupyter-mrtaesiri/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/run.py\", line 785, in run\r\n    elastic_launch(\r\n  File \"/home/jupyter-mrtaesiri/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n  File \"/home/jupyter-mrtaesiri/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \r\n\r\n```</BODY>\n\n<COMMENTS>\n<Comment by taesiri at 2023-06-16T23:23:23Z>\nI should also add that I already trained the LLaMA-based model without facing this issue.\n</Comment>\n<Comment by findalexli at 2023-06-25T19:53:18Z>\nI am facing the same issue. Please help!\n</Comment>\n<Comment by kahnchana at 2023-06-27T23:33:11Z>\nCopying the necessary function and lines from `llava.py` to `llava_mpt.py` fixes this issue for me (quick fix). \r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/17735127/e1ef7601-f5b5-4b63-8f6e-36db29517576)\r\n![image](https://github.com/haotian-liu/LLaVA/assets/17735127/9da754b5-8538-4f7e-971a-b512441a59cb)\n</Comment>\n<Comment by taesiri at 2023-06-29T00:56:11Z>\n@kahnchana Could you make a pull request or share your fork?\n</Comment>\n<Comment by haotian-liu at 2023-08-02T05:38:33Z>\nApologies for the confusion. The latest code base has fixed the related issues regarding the MPT model training. Please try checking out the latest code base, thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 239,
    "state": "open",
    "created_by": "LetsGoFir",
    "created_at": "2023-06-16T11:34:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/239</URL>\n\n<TITLE>[Question] Please post your evaluation result and we could compare them</TITLE>\n\n<BODY>### Question\n\nThanks for your excellent work!\r\n\r\nBelow is my evaluation result,\r\n\r\n`GPT-4 vs. review.json`\r\n`conv [7.5, 6.933] 92.4`\r\n`all [7.678, 7.444] 97.0`\r\n`detail [7.733, 6.8] 87.9`\r\n`complex [7.8, 8.6] 110.3`\r\n\r\nAnd I want to compare it with yours.</BODY>\n\n<COMMENTS>\n<Comment by xtong-zhang at 2023-07-28T05:51:56Z>\nwhat are the models and datasets you used for this evaluation result? (vicuna7b?)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 238,
    "state": "closed",
    "created_by": "CaicaiJason",
    "created_at": "2023-06-15T07:17:27Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/238</URL>\n\n<TITLE>Can LLaVa support Chinese QA?[Question]</TITLE>\n\n<BODY>### Question\n\nhow can i do to make LLaVa answers my question in Chinese?</BODY>\n\n<COMMENTS>\n<Comment by DexterGuo at 2023-06-16T05:22:22Z>\n经过测试，如果不做任何操作，原生llava基本上不会用中文回答；如果有微调的数据集，直接进行微调即可，不用做过多的其他操作；\n</Comment>\n<Comment by haotian-liu at 2023-10-13T04:40:01Z>\nLLaVA-1.5 has an intriguing emergent capability of answering Chinese questions, with no Chinese multimodal instruction data. We believe it can be further improved when finetuned on Chinese instruction data.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 237,
    "state": "open",
    "created_by": "wcy1122",
    "created_at": "2023-06-14T18:31:21Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/237</URL>\n\n<TITLE>[Question] ScienceQA performance</TITLE>\n\n<BODY>### Question\n\nHello, I tried to infer your released 13b checkpoint on scienceQA with the latest code. However, the accuracy is only around 40%, which is much lower than the reported 90.89%. Is there something wrong?\r\n\r\nMy convert command line\r\n\r\n> python3 -m llava.model.apply_delta \\\r\n>     --base /path/to/llama-13b-hf \\\r\n>     --target /path/to/LLaVA-13b-v0-science_qa \\\r\n>     --delta /path/to/LLaVA-13b-delta-v0\r\n\r\nMy inference command line\r\n\r\n>  python -m llava.eval.model_vqa_science \\\r\n>     --model-name /path/to/LLaVA-13b-v0-science_qa \\\r\n>     --question-file /path/to/scienceqa/llava_test_QCM-LEPA.json \\\r\n>     --image-folder /path/to/scienceqa/images/test \\\r\n>     --answers-file /path/to/llava-13b-sqa-release/results/test_llava-13b.jsonl \\\r\n>     --answer-prompter \\\r\n>     --conv-mode simple\r\n\r\n> python -m llava.eval.eval_science_qa \\\r\n>     --base-dir /path/to/scienceqa \\\r\n>     --result-file /path/to/llava-13b-sqa-release/results/test_llava-13b.jsonl \\\r\n>     --output-file /path/to/llava-13b-sqa-release/results/test_llava-13b_output.json \\\r\n>     --output-result /path/to/llava-13b-sqa-release/results/test_llava-13b_result.json</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-06-14T18:59:17Z>\nHi @wcy1122 \r\n\r\nOne other user found that re-download the correct checkpoints resolve the similar issue in #104.\r\n\r\nCan you make sure that: (1) you downloaded the correct ScienceQA delta; (2) you [applied the delta weights](https://github.com/haotian-liu/LLaVA#llava-13b) to get the correct model weights; (3) the base model weights during the conversion mentioned in step (2) is LLaMA instead of Vicuna.\r\n\r\nThanks.\n</Comment>\n<Comment by wcy1122 at 2023-06-14T19:52:08Z>\nHi @haotian-liu , thanks for your reply.\r\nIt looks weird. I use llama 13b from https://huggingface.co/decapoda-research/llama-13b-hf, and download the delta weight from https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0-science_qa.\n</Comment>\n<Comment by wcy1122 at 2023-06-15T08:36:57Z>\nHi.\r\nI found that in your release result file [here](https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/table/results/test_sqa_llava_13b_v0.json), almost all outputs contain \"Assistant:\" in the front.\r\nBut when I inference your release checkpoint, I found that only half of the output contains \"Assistant:\" in the front. In most cases, the model directly output \"\\n The answer is A.\".\r\nI guess it's like something wrong with the inference prompt?\n</Comment>\n<Comment by CupidJay at 2023-08-01T06:09:50Z>\nHi, I met the same problem, how did you solve it？\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 236,
    "state": "open",
    "created_by": "JulietLJY",
    "created_at": "2023-06-14T06:11:35Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/236</URL>\n\n<TITLE>[Usage] I try to finetune the code with deepspeed on ScienceQA with 4 3090 gpus but meets disagreement error between ranks.</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\nI try to finetune the code with deepspeed on ScienceQA with 4 3090 gpus but meets disagreement error between ranks.\r\n\r\nCommand:\r\n```\r\ndeepspeed --master_port=25001 \\\r\n    llava/train/train.py \\\r\n    --model_name_or_path /mnt/proj200/jingyaoli/LLM_weights/vicuna/7b \\\r\n    --data_path $data_path \\\r\n    --image_folder $image_folder \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    $load_or_train_adapter \\\r\n    --mm_vision_select_layer -2 \\\r\n    --bf16 True \\\r\n    --output_dir $output_dir \\\r\n    --num_train_epochs 12 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 5000 \\\r\n    --save_total_limit 3 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb \\\r\n    --deepspeed \"./scripts/deepspeed/zero3_offload.json\" \r\n    # --optim adamw_bnb_8bit \r\n\r\n```\r\n\r\nLog: \r\n```\r\n  0%|          | 0/9552 [00:00<?, ?it/s]\r\n╭───────────────────── Traceback (most recent call last) ──────────────────────╮\r\n│ /mnt/backup2/home/jyli22/VLM/LLaVA/llava/train/train.py:834 in <module>      │\r\n│                                                                              │\r\n│   831                                                                        │\r\n│   832                                                                        │\r\n│   833 if __name__ == \"__main__\":                                             │\r\n│ ❱ 834 │   train()                                                            │\r\n│   835                                                                        │\r\n│                                                                              │\r\n│ /mnt/backup2/home/jyli22/VLM/LLaVA/llava/train/train.py:814 in train         │\r\n│                                                                              │\r\n│   811 │   if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\" │\r\n│   812 │   │   trainer.train(resume_from_checkpoint=True)                     │\r\n│   813 │   else:                                                              │\r\n│ ❱ 814 │   │   trainer.train()                                                │\r\n│   815 │   trainer.save_state()                                               │\r\n│   816 │                                                                      │\r\n│   817 │   if training_args.lora_enable:                                      │\r\n│                                                                              │\r\n│ /mnt/home/jyli22/software/enter/envs/llava/lib/python3.10/site-packages/tran │\r\n│ sformers/trainer.py:1644 in train                                            │\r\n│                                                                              │\r\n│   1641 │   │   inner_training_loop = find_executable_batch_size(             │\r\n│   1642 │   │   │   self._inner_training_loop, self._train_batch_size, args.a │\r\n│   1643 │   │   )                                                             │\r\n│ ❱ 1644 │   │   return inner_training_loop(                                   │\r\n│   1645 │   │   │   args=args,                                                │\r\n│   1646 │   │   │   resume_from_checkpoint=resume_from_checkpoint,            │\r\n│   1647 │   │   │   trial=trial,                                              │\r\n│                                                                              │\r\n│ /mnt/home/jyli22/software/enter/envs/llava/lib/python3.10/site-packages/tran │\r\n│ sformers/trainer.py:1911 in _inner_training_loop                             │\r\n│                                                                              │\r\n│   1908 │   │   │   │   │   with model.no_sync():                             │\r\n│   1909 │   │   │   │   │   │   tr_loss_step = self.training_step(model, inpu │\r\n│   1910 │   │   │   │   else:                                                 │\r\n│ ❱ 1911 │   │   │   │   │   tr_loss_step = self.training_step(model, inputs)  │\r\n│   1912 │   │   │   │                                                         │\r\n│   1913 │   │   │   │   if (                                                  │\r\n│   1914 │   │   │   │   │   args.logging_nan_inf_filter                       │\r\n│                                                                              │\r\n│ /mnt/home/jyli22/software/enter/envs/llava/lib/python3.10/site-packages/tran │\r\n│ sformers/trainer.py:2673 in training_step                                    │\r\n│                                                                              │\r\n│   2670 │   │   │   │   scaled_loss.backward()                                │\r\n│   2671 │   │   elif self.deepspeed:                                          │\r\n│   2672 │   │   │   # loss gets scaled under gradient_accumulation_steps in d │\r\n│ ❱ 2673 │   │   │   loss = self.deepspeed.backward(loss)                      │\r\n│   2674 │   │   else:                                                         │\r\n│   2675 │   │   │   loss.backward()                                           │\r\n│   2676                                                                       │\r\n│                                                                              │\r\n│ /mnt/home/jyli22/software/enter/envs/llava/lib/python3.10/site-packages/deep │\r\n│ speed/utils/nvtx.py:15 in wrapped_fn                                         │\r\n│                                                                              │\r\n│   12 │                                                                       │\r\n│   13 │   def wrapped_fn(*args, **kwargs):                                    │\r\n│   14 │   │   get_accelerator().range_push(func.__qualname__)                 │\r\n│ ❱ 15 │   │   ret_val = func(*args, **kwargs)                                 │\r\n│   16 │   │   get_accelerator().range_pop()                                   │\r\n│   17 │   │   return ret_val                                                  │\r\n│   18                                                                         │\r\n│                                                                              │\r\n│ /mnt/home/jyli22/software/enter/envs/llava/lib/python3.10/site-packages/deep │\r\n│ speed/runtime/engine.py:1845 in backward                                     │\r\n│                                                                              │\r\n│   1842 │   │                                                                 │\r\n│   1843 │   │   if self.zero_optimization():                                  │\r\n│   1844 │   │   │   self.optimizer.is_gradient_accumulation_boundary = self.i │\r\n│ ❱ 1845 │   │   │   self.optimizer.backward(loss, retain_graph=retain_graph)  │\r\n│   1846 │   │   elif self.amp_enabled():                                      │\r\n│   1847 │   │   │   # AMP requires delaying unscale when inside gradient accu │\r\n│   1848 │   │   │   # https://nvidia.github.io/apex/advanced.html#gradient-ac │\r\n│                                                                              │\r\n│ /mnt/home/jyli22/software/enter/envs/llava/lib/python3.10/site-packages/deep │\r\n│ speed/utils/nvtx.py:15 in wrapped_fn                                         │\r\n│                                                                              │\r\n│   12 │                                                                       │\r\n│   13 │   def wrapped_fn(*args, **kwargs):                                    │\r\n│   14 │   │   get_accelerator().range_push(func.__qualname__)                 │\r\n│ ❱ 15 │   │   ret_val = func(*args, **kwargs)                                 │\r\n│   16 │   │   get_accelerator().range_pop()                                   │\r\n│   17 │   │   return ret_val                                                  │\r\n│   18                                                                         │\r\n│                                                                              │\r\n│ /mnt/home/jyli22/software/enter/envs/llava/lib/python3.10/site-packages/deep │\r\n│ speed/runtime/zero/stage3.py:1964 in backward                                │\r\n│                                                                              │\r\n│   1961 │   │   else:                                                         │\r\n│   1962 │   │   │   self.loss_scaler.backward(loss.float(), retain_graph=reta │\r\n│   1963 │   │                                                                 │\r\n│ ❱ 1964 │   │   self._get_param_coordinator(training=True).reset_step()       │\r\n│   1965 │   │                                                                 │\r\n│   1966 │   │   if self.swap_optimizer:                                       │\r\n│   1967 │   │   │   self.optimizer_swapper.post_backward()                    │\r\n│                                                                              │\r\n│ /mnt/home/jyli22/software/enter/envs/llava/lib/python3.10/site-packages/deep │\r\n│ speed/runtime/zero/partitioned_param_coordinator.py:188 in reset_step        │\r\n│                                                                              │\r\n│   185 │   │                                                                  │\r\n│   186 │   │   if not self.is_complete_trace():  # not self.trace_complete:   │\r\n│   187 │   │   │   # Make sure that recorded submodule orders are identical a │\r\n│ ❱ 188 │   │   │   assert_ints_same_as_other_ranks([m.id for m in self.__subm │\r\n│   189 │   │   │                                                              │\r\n│   190 │   │   │   if self.is_record_trace():                                 │\r\n│   191 │   │   │   │   # Successfully recorded a trace                        │\r\n│                                                                              │\r\n│ /mnt/home/jyli22/software/enter/envs/llava/lib/python3.10/site-packages/deep │\r\n│ speed/utils/nvtx.py:15 in wrapped_fn                                         │\r\n│                                                                              │\r\n│   12 │                                                                       │\r\n│   13 │   def wrapped_fn(*args, **kwargs):                                    │\r\n│   14 │   │   get_accelerator().range_push(func.__qualname__)                 │\r\n│ ❱ 15 │   │   ret_val = func(*args, **kwargs)                                 │\r\n│   16 │   │   get_accelerator().range_pop()                                   │\r\n│   17 │   │   return ret_val                                                  │\r\n│   18                                                                         │\r\n│                                                                              │\r\n│ /mnt/home/jyli22/software/enter/envs/llava/lib/python3.10/site-packages/deep │\r\n│ speed/runtime/zero/utils.py:86 in assert_ints_same_as_other_ranks            │\r\n│                                                                              │\r\n│   83 │   \"\"\"                                                                 │\r\n│   84 │   rank0_ints = get_lst_from_rank0(ints)                               │\r\n│   85 │   if ints != rank0_ints:                                              │\r\n│ ❱ 86 │   │   raise RuntimeError(f\"disagreement between rank0 and rank{dist.g │\r\n│   87 │   │   │   │   │   │      f\"rank0: {rank0_ints}, rank{dist.get_rank()} │\r\n│   88                                                                         │\r\n╰──────────────────────────────────────────────────────────────────────────────╯\r\nRuntimeError: disagreement between rank0 and rank1: rank0: [0, 1, 2, 453, 454, \r\n455, 456, 457, 458, 459, 461, 467, 462, 465, 463, 464, 466, 472, 468, 470, 469, \r\n471, 473, 479, 474, 477, 475, 476, 478, 484, 480, 482, 481, 483, 485, 491, 486, \r\n489, 487, 488, 490, 496, 492, 494, 493, 495, 497, 503, 498, 501, 499, 500, 502, \r\n508, 504, 506, 505, 507, 509, 515, 510, 513, 511, 512, 514, 520, 516, 518, 517, \r\n519, 521, 527, 522, 525, 523, 524, 526, 532, 528, 530, 529, 531, 533, 539, 534, \r\n537, 535, 536, 538, 544, 540, 542, 541, 543, 545, 551, 546, 549, 547, 548, 550, \r\n556, 552, 554, 553, 555, 557, 563, 558, 561, 559, 560, 562, 568, 564, 566, 565, \r\n567, 569, 575, 570, 573, 571, 572, 574, 580, 576, 578, 577, 579, 581, 587, 582, \r\n585, 583, 584, 586, 592, 588, 590, 589, 591, 593, 599, 594, 597, 595, 596, 598, \r\n604, 600, 602, 601, 603, 605, 611, 606, 609, 607, 608, 610, 616, 612, 614, 613, \r\n615, 617, 623, 618, 621, 619, 620, 622, 628, 624, 626, 625, 627, 629, 635, 630, \r\n633, 631, 632, 634, 640, 636, 638, 637, 639, 641, 647, 642, 645, 643, 644, 646, \r\n652, 648, 650, 649, 651, 653, 659, 654, 657, 655, 656, 658, 664, 660, 662, 661, \r\n663, 665, 671, 666, 669, 667, 668, 670, 676, 672, 674, 673, 675, 677, 683, 678, \r\n681, 679, 680, 682, 688, 684, 686, 685, 687, 689, 695, 690, 693, 691, 692, 694, \r\n700, 696, 698, 697, 699, 701, 707, 702, 705, 703, 704, 706, 712, 708, 710, 709, \r\n711, 713, 719, 714, 717, 715, 716, 718, 724, 720, 722, 721, 723, 725, 731, 726, \r\n729, 727, 728, 730, 736, 732, 734, 733, 735, 737, 743, 738, 741, 739, 740, 742, \r\n748, 744, 746, 745, 747, 749, 750, 750, 4, 16, 5, 6, 7, 8, 10, 9, 17, 11, 12, \r\n15, 14, 13, 18, 30, 19, 20, 21, 22, 24, 23, 31, 25, 26, 29, 28, 27, 32, 44, 33, \r\n34, 35, 36, 38, 37, 45, 39, 40, 43, 42, 41, 46, 58, 47, 48, 49, 50, 52, 51, 59, \r\n53, 54, 57, 56, 55, 60, 72, 61, 62, 63, 64, 66, 65, 73, 67, 68, 71, 70, 69, 74, \r\n86, 75, 76, 77, 78, 80, 79, 87, 81, 82, 85, 84, 83, 88, 100, 89, 90, 91, 92, 94,\r\n93, 101, 95, 96, 99, 98, 97, 102, 114, 103, 104, 105, 106, 108, 107, 115, 109, \r\n110, 113, 112, 111, 116, 128, 117, 118, 119, 120, 122, 121, 129, 123, 124, 127, \r\n126, 125, 130, 142, 131, 132, 133, 134, 136, 135, 143, 137, 138, 141, 140, 139, \r\n144, 156, 145, 146, 147, 148, 150, 149, 157, 151, 152, 155, 154, 153, 158, 170, \r\n159, 160, 161, 162, 164, 163, 171, 165, 166, 169, 168, 167, 172, 184, 173, 174, \r\n175, 176, 178, 177, 185, 179, 180, 183, 182, 181, 186, 198, 187, 188, 189, 190, \r\n192, 191, 199, 193, 194, 197, 196, 195, 200, 212, 201, 202, 203, 204, 206, 205, \r\n213, 207, 208, 211, 210, 209, 214, 226, 215, 216, 217, 218, 220, 219, 227, 221, \r\n222, 225, 224, 223, 228, 240, 229, 230, 231, 232, 234, 233, 241, 235, 236, 239, \r\n238, 237, 242, 254, 243, 244, 245, 246, 248, 247, 255, 249, 250, 253, 252, 251, \r\n256, 268, 257, 258, 259, 260, 262, 261, 269, 263, 264, 267, 266, 265, 270, 282, \r\n271, 272, 273, 274, 276, 275, 283, 277, 278, 281, 280, 279, 284, 296, 285, 286, \r\n287, 288, 290, 289, 297, 291, 292, 295, 294, 293, 298, 310, 299, 300, 301, 302, \r\n304, 303, 311, 305, 306, 309, 308, 307, 312, 324, 313, 314, 315, 316, 318, 317, \r\n325, 319, 320, 323, 322, 321, 326, 338, 327, 328, 329, 330, 332, 331, 339, 333, \r\n334, 337, 336, 335, 340, 352, 341, 342, 343, 344, 346, 345, 353, 347, 348, 351, \r\n350, 349, 354, 366, 355, 356, 357, 358, 360, 359, 367, 361, 362, 365, 364, 363, \r\n368, 380, 369, 370, 371, 372, 374, 373, 381, 375, 376, 379, 378, 377, 382, 394, \r\n383, 384, 385, 386, 388, 387, 395, 389, 390, 393, 392, 391, 396, 408, 397, 398, \r\n399, 400, 402, 401, 409, 403, 404, 407, 406, 405, 410, 422, 411, 412, 413, 414, \r\n416, 415, 423, 417, 418, 421, 420, 419, 424, 436, 425, 426, 427, 428, 430, 429, \r\n437, 431, 432, 435, 434, 433, 438, 450, 439, 440, 441, 442, 444, 443, 451, 445, \r\n446, 449, 448, 447, 452, 751, 0, 751, 1, 452, 438, 450, 439, 440, 441, 442, 444,\r\n443, 451, 445, 446, 449, 448, 447, 438, 445, 447, 448, 449, 446, 451, 439, 443, \r\n442, 441, 440, 450, 424, 436, 425, 426, 427, 428, 430, 429, 437, 431, 432, 435, \r\n434, 433, 424, 431, 433, 434, 435, 432, 437, 425, 429, 428, 427, 426, 436, 410, \r\n422, 411, 412, 413, 414, 416, 415, 423, 417, 418, 421, 420, 419, 410, 417, 419, \r\n420, 421, 418, 423, 411, 415, 414, 413, 412, 422, 396, 408, 397, 398, 399, 400, \r\n402, 401, 409, 403, 404, 407, 406, 405, 396, 403, 405, 406, 407, 404, 409, 397, \r\n401, 400, 399, 398, 408, 382, 394, 383, 384, 385, 386, 388, 387, 395, 389, 390, \r\n393, 392, 391, 382, 389, 391, 392, 393, 390, 395, 383, 387, 386, 385, 384, 394, \r\n368, 380, 369, 370, 371, 372, 374, 373, 381, 375, 376, 379, 378, 377, 368, 375, \r\n377, 378, 379, 376, 381, 369, 373, 372, 371, 370, 380, 354, 366, 355, 356, 357, \r\n358, 360, 359, 367, 361, 362, 365, 364, 363, 354, 361, 363, 364, 365, 362, 367, \r\n355, 359, 358, 357, 356, 366, 340, 352, 341, 342, 343, 344, 346, 345, 353, 347, \r\n348, 351, 350, 349, 340, 347, 349, 350, 351, 348, 353, 341, 345, 344, 343, 342, \r\n352, 326, 338, 327, 328, 329, 330, 332, 331, 339, 333, 334, 337, 336, 335, 326, \r\n333, 335, 336, 337, 334, 339, 327, 331, 330, 329, 328, 338, 312, 324, 313, 314, \r\n315, 316, 318, 317, 325, 319, 320, 323, 322, 321, 312, 319, 321, 322, 323, 320, \r\n325, 313, 317, 316, 315, 314, 324, 298, 310, 299, 300, 301, 302, 304, 303, 311, \r\n305, 306, 309, 308, 307, 298, 305, 307, 308, 309, 306, 311, 299, 303, 302, 301, \r\n300, 310, 284, 296, 285, 286, 287, 288, 290, 289, 297, 291, 292, 295, 294, 293, \r\n284, 291, 293, 294, 295, 292, 297, 285, 289, 288, 287, 286, 296, 270, 282, 271, \r\n272, 273, 274, 276, 275, 283, 277, 278, 281, 280, 279, 270, 277, 279, 280, 281, \r\n278, 283, 271, 275, 274, 273, 272, 282, 256, 268, 257, 258, 259, 260, 262, 261, \r\n269, 263, 264, 267, 266, 265, 256, 263, 265, 266, 267, 264, 269, 257, 261, 260, \r\n259, 258, 268, 242, 254, 243, 244, 245, 246, 248, 247, 255, 249, 250, 253, 252, \r\n251, 242, 249, 251, 252, 253, 250, 255, 243, 247, 246, 245, 244, 254, 228, 240, \r\n229, 230, 231, 232, 234, 233, 241, 235, 236, 239, 238, 237, 228, 235, 237, 238, \r\n239, 236, 241, 229, 233, 232, 231, 230, 240, 214, 226, 215, 216, 217, 218, 220, \r\n219, 227, 221, 222, 225, 224, 223, 214, 221, 223, 224, 225, 222, 227, 215, 219, \r\n218, 217, 216, 226, 200, 212, 201, 202, 203, 204, 206, 205, 213, 207, 208, 211, \r\n210, 209, 200, 207, 209, 210, 211, 208, 213, 201, 205, 204, 203, 202, 212, 186, \r\n198, 187, 188, 189, 190, 192, 191, 199, 193, 194, 197, 196, 195, 186, 193, 195, \r\n196, 197, 194, 199, 187, 191, 190, 189, 188, 198, 172, 184, 173, 174, 175, 176, \r\n178, 177, 185, 179, 180, 183, 182, 181, 172, 179, 181, 182, 183, 180, 185, 173, \r\n177, 176, 175, 174, 184, 158, 170, 159, 160, 161, 162, 164, 163, 171, 165, 166, \r\n169, 168, 167, 158, 165, 167, 168, 169, 166, 171, 159, 163, 162, 161, 160, 170, \r\n144, 156, 145, 146, 147, 148, 150, 149, 157, 151, 152, 155, 154, 153, 144, 151, \r\n153, 154, 155, 152, 157, 145, 149, 148, 147, 146, 156, 130, 142, 131, 132, 133, \r\n134, 136, 135, 143, 137, 138, 141, 140, 139, 130, 137, 139, 140, 141, 138, 143, \r\n131, 135, 134, 133, 132, 142, 116, 128, 117, 118, 119, 120, 122, 121, 129, 123, \r\n124, 127, 126, 125, 116, 123, 125, 126, 127, 124, 129, 117, 121, 120, 119, 118, \r\n128, 102, 114, 103, 104, 105, 106, 108, 107, 115, 109, 110, 113, 112, 111, 102, \r\n109, 111, 112, 113, 110, 115, 103, 107, 106, 105, 104, 114, 88, 100, 89, 90, 91,\r\n92, 94, 93, 101, 95, 96, 99, 98, 97, 88, 95, 97, 98, 99, 96, 101, 89, 93, 92, \r\n91, 90, 100, 74, 86, 75, 76, 77, 78, 80, 79, 87, 81, 82, 85, 84, 83, 74, 81, 83,\r\n84, 85, 82, 87, 75, 79, 78, 77, 76, 86, 60, 72, 61, 62, 63, 64, 66, 65, 73, 67, \r\n68, 71, 70, 69, 60, 67, 69, 70, 71, 68, 73, 61, 65, 64, 63, 62, 72, 46, 58, 47, \r\n48, 49, 50, 52, 51, 59, 53, 54, 57, 56, 55, 46, 53, 55, 56, 57, 54, 59, 47, 51, \r\n50, 49, 48, 58, 32, 44, 33, 34, 35, 36, 38, 37, 45, 39, 40, 43, 42, 41, 32, 39, \r\n41, 42, 43, 40, 45, 33, 37, 36, 35, 34, 44, 18, 30, 19, 20, 21, 22, 24, 23, 31, \r\n25, 26, 29, 28, 27, 18, 25, 27, 28, 29, 26, 31, 19, 23, 22, 21, 20, 30, 4, 16, \r\n5, 6, 7, 8, 10, 9, 17, 11, 12, 15, 14, 13, 4, 11, 13, 14, 15, 12, 17, 5, 9, 8, \r\n7, 6, 16, 750, 750], rank1: [0, 1, 2, 453, 454, 455, 456, 457, 458, 459, 461, \r\n467, 462, 465, 463, 464, 466, 472, 468, 470, 469, 471, 473, 479, 474, 477, 475, \r\n476, 478, 484, 480, 482, 481, 483, 485, 491, 486, 489, 487, 488, 490, 496, 492, \r\n494, 493, 495, 497, 503, 498, 501, 499, 500, 502, 508, 504, 506, 505, 507, 509, \r\n515, 510, 513, 511, 512, 514, 520, 516, 518, 517, 519, 521, 527, 522, 525, 523, \r\n524, 526, 532, 528, 530, 529, 531, 533, 539, 534, 537, 535, 536, 538, 544, 540, \r\n542, 541, 543, 545, 551, 546, 549, 547, 548, 550, 556, 552, 554, 553, 555, 557, \r\n563, 558, 561, 559, 560, 562, 568, 564, 566, 565, 567, 569, 575, 570, 573, 571, \r\n572, 574, 580, 576, 578, 577, 579, 581, 587, 582, 585, 583, 584, 586, 592, 588, \r\n590, 589, 591, 593, 599, 594, 597, 595, 596, 598, 604, 600, 602, 601, 603, 605, \r\n611, 606, 609, 607, 608, 610, 616, 612, 614, 613, 615, 617, 623, 618, 621, 619, \r\n620, 622, 628, 624, 626, 625, 627, 629, 635, 630, 633, 631, 632, 634, 640, 636, \r\n638, 637, 639, 641, 647, 642, 645, 643, 644, 646, 652, 648, 650, 649, 651, 653, \r\n659, 654, 657, 655, 656, 658, 664, 660, 662, 661, 663, 665, 671, 666, 669, 667, \r\n668, 670, 676, 672, 674, 673, 675, 677, 683, 678, 681, 679, 680, 682, 688, 684, \r\n686, 685, 687, 689, 695, 690, 693, 691, 692, 694, 700, 696, 698, 697, 699, 701, \r\n707, 702, 705, 703, 704, 706, 712, 708, 710, 709, 711, 713, 719, 714, 717, 715, \r\n716, 718, 724, 720, 722, 721, 723, 725, 731, 726, 729, 727, 728, 730, 736, 732, \r\n734, 733, 735, 737, 743, 738, 741, 739, 740, 742, 748, 744, 746, 745, 747, 749, \r\n750, 750, 4, 16, 5, 6, 7, 8, 10, 9, 17, 11, 12, 15, 14, 13, 18, 30, 19, 20, 21, \r\n22, 24, 23, 31, 25, 26, 29, 28, 27, 32, 44, 33, 34, 35, 36, 38, 37, 45, 39, 40, \r\n43, 42, 41, 46, 58, 47, 48, 49, 50, 52, 51, 59, 53, 54, 57, 56, 55, 60, 72, 61, \r\n62, 63, 64, 66, 65, 73, 67, 68, 71, 70, 69, 74, 86, 75, 76, 77, 78, 80, 79, 87, \r\n81, 82, 85, 84, 83, 88, 100, 89, 90, 91, 92, 94, 93, 101, 95, 96, 99, 98, 97, \r\n102, 114, 103, 104, 105, 106, 108, 107, 115, 109, 110, 113, 112, 111, 116, 128, \r\n117, 118, 119, 120, 122, 121, 129, 123, 124, 127, 126, 125, 130, 142, 131, 132, \r\n133, 134, 136, 135, 143, 137, 138, 141, 140, 139, 144, 156, 145, 146, 147, 148, \r\n150, 149, 157, 151, 152, 155, 154, 153, 158, 170, 159, 160, 161, 162, 164, 163, \r\n171, 165, 166, 169, 168, 167, 172, 184, 173, 174, 175, 176, 178, 177, 185, 179, \r\n180, 183, 182, 181, 186, 198, 187, 188, 189, 190, 192, 191, 199, 193, 194, 197, \r\n196, 195, 200, 212, 201, 202, 203, 204, 206, 205, 213, 207, 208, 211, 210, 209, \r\n214, 226, 215, 216, 217, 218, 220, 219, 227, 221, 222, 225, 224, 223, 228, 240, \r\n229, 230, 231, 232, 234, 233, 241, 235, 236, 239, 238, 237, 242, 254, 243, 244, \r\n245, 246, 248, 247, 255, 249, 250, 253, 252, 251, 256, 268, 257, 258, 259, 260, \r\n262, 261, 269, 263, 264, 267, 266, 265, 270, 282, 271, 272, 273, 274, 276, 275, \r\n283, 277, 278, 281, 280, 279, 284, 296, 285, 286, 287, 288, 290, 289, 297, 291, \r\n292, 295, 294, 293, 298, 310, 299, 300, 301, 302, 304, 303, 311, 305, 306, 309, \r\n308, 307, 312, 324, 313, 314, 315, 316, 318, 317, 325, 319, 320, 323, 322, 321, \r\n326, 338, 327, 328, 329, 330, 332, 331, 339, 333, 334, 337, 336, 335, 340, 352, \r\n341, 342, 343, 344, 346, 345, 353, 347, 348, 351, 350, 349, 354, 366, 355, 356, \r\n357, 358, 360, 359, 367, 361, 362, 365, 364, 363, 368, 380, 369, 370, 371, 372, \r\n374, 373, 381, 375, 376, 379, 378, 377, 382, 394, 383, 384, 385, 386, 388, 387, \r\n395, 389, 390, 393, 392, 391, 396, 408, 397, 398, 399, 400, 402, 401, 409, 403, \r\n404, 407, 406, 405, 410, 422, 411, 412, 413, 414, 416, 415, 423, 417, 418, 421, \r\n420, 419, 424, 436, 425, 426, 427, 428, 430, 429, 437, 431, 432, 435, 434, 433, \r\n438, 450, 439, 440, 441, 442, 444, 443, 451, 445, 446, 449, 448, 447, 452, 751, \r\n0, 751, 1, 452, 438, 450, 439, 440, 441, 442, 444, 443, 451, 445, 446, 449, 448,\r\n447, 438, 445, 447, 448, 449, 446, 451, 439, 443, 442, 441, 440, 450, 424, 436, \r\n425, 426, 427, 428, 430, 429, 437, 431, 432, 435, 434, 433, 424, 431, 433, 434, \r\n435, 432, 437, 425, 429, 428, 427, 426, 436, 410, 422, 411, 412, 413, 414, 416, \r\n415, 423, 417, 418, 421, 420, 419, 410, 417, 419, 420, 421, 418, 423, 411, 415, \r\n414, 413, 412, 422, 396, 408, 397, 398, 399, 400, 402, 401, 409, 403, 404, 407, \r\n406, 405, 396, 403, 405, 406, 407, 404, 409, 397, 401, 400, 399, 398, 408, 382, \r\n394, 383, 384, 385, 386, 388, 387, 395, 389, 390, 393, 392, 391, 382, 389, 391, \r\n392, 393, 390, 395, 383, 387, 386, 385, 384, 394, 368, 380, 369, 370, 371, 372, \r\n374, 373, 381, 375, 376, 379, 378, 377, 368, 375, 377, 378, 379, 376, 381, 369, \r\n373, 372, 371, 370, 380, 354, 366, 355, 356, 357, 358, 360, 359, 367, 361, 362, \r\n365, 364, 363, 354, 361, 363, 364, 365, 362, 367, 355, 359, 358, 357, 356, 366, \r\n340, 352, 341, 342, 343, 344, 346, 345, 353, 347, 348, 351, 350, 349, 340, 347, \r\n349, 350, 351, 348, 353, 341, 345, 344, 343, 342, 352, 326, 338, 327, 328, 329, \r\n330, 332, 331, 339, 333, 334, 337, 336, 335, 326, 333, 335, 336, 337, 334, 339, \r\n327, 331, 330, 329, 328, 338, 312, 324, 313, 314, 315, 316, 318, 317, 325, 319, \r\n320, 323, 322, 321, 312, 319, 321, 322, 323, 320, 325, 313, 317, 316, 315, 314, \r\n324, 298, 310, 299, 300, 301, 302, 304, 303, 311, 305, 306, 309, 308, 307, 298, \r\n305, 307, 308, 309, 306, 311, 299, 303, 302, 301, 300, 310, 284, 296, 285, 286, \r\n287, 288, 290, 289, 297, 291, 292, 295, 294, 293, 284, 291, 293, 294, 295, 292, \r\n297, 285, 289, 288, 287, 286, 296, 270, 282, 271, 272, 273, 274, 276, 275, 283, \r\n277, 278, 281, 280, 279, 270, 277, 279, 280, 281, 278, 283, 271, 275, 274, 273, \r\n272, 282, 256, 268, 257, 258, 259, 260, 262, 261, 269, 263, 264, 267, 266, 265, \r\n256, 263, 265, 266, 267, 264, 269, 257, 261, 260, 259, 258, 268, 242, 254, 243, \r\n244, 245, 246, 248, 247, 255, 249, 250, 253, 252, 251, 242, 249, 251, 252, 253, \r\n250, 255, 243, 247, 246, 245, 244, 254, 228, 240, 229, 230, 231, 232, 234, 233, \r\n241, 235, 236, 239, 238, 237, 228, 235, 237, 238, 239, 236, 241, 229, 233, 232, \r\n231, 230, 240, 214, 226, 215, 216, 217, 218, 220, 219, 227, 221, 222, 225, 224, \r\n223, 214, 221, 223, 224, 225, 222, 227, 215, 219, 218, 217, 216, 226, 200, 212, \r\n201, 202, 203, 204, 206, 205, 213, 207, 208, 211, 210, 209, 200, 207, 209, 210, \r\n211, 208, 213, 201, 205, 204, 203, 202, 212, 186, 198, 187, 188, 189, 190, 192, \r\n191, 199, 193, 194, 197, 196, 195, 186, 193, 195, 196, 197, 194, 199, 187, 191, \r\n190, 189, 188, 198, 172, 184, 173, 174, 175, 176, 178, 177, 185, 179, 180, 183, \r\n182, 181, 172, 179, 181, 182, 183, 180, 185, 173, 177, 176, 175, 174, 184, 158, \r\n170, 159, 160, 161, 162, 164, 163, 171, 165, 166, 169, 168, 167, 158, 165, 167, \r\n168, 169, 166, 171, 159, 163, 162, 161, 160, 170, 144, 156, 145, 146, 147, 148, \r\n150, 149, 157, 151, 152, 155, 154, 153, 144, 151, 153, 154, 155, 152, 157, 145, \r\n149, 148, 147, 146, 156, 130, 142, 131, 132, 133, 134, 136, 135, 143, 137, 138, \r\n141, 140, 139, 130, 137, 139, 140, 141, 138, 143, 131, 135, 134, 133, 132, 142, \r\n116, 128, 117, 118, 119, 120, 122, 121, 129, 123, 124, 127, 126, 125, 116, 123, \r\n125, 126, 127, 124, 129, 117, 121, 120, 119, 118, 128, 102, 114, 103, 104, 105, \r\n106, 108, 107, 115, 109, 110, 113, 112, 111, 102, 109, 111, 112, 113, 110, 115, \r\n103, 107, 106, 105, 104, 114, 88, 100, 89, 90, 91, 92, 94, 93, 101, 95, 96, 99, \r\n98, 97, 88, 95, 97, 98, 99, 96, 101, 89, 93, 92, 91, 90, 100, 74, 86, 75, 76, \r\n77, 78, 80, 79, 87, 81, 82, 85, 84, 83, 74, 81, 83, 84, 85, 82, 87, 75, 79, 78, \r\n77, 76, 86, 60, 72, 61, 62, 63, 64, 66, 65, 73, 67, 68, 71, 70, 69, 60, 67, 69, \r\n70, 71, 68, 73, 61, 65, 64, 63, 62, 72, 46, 58, 47, 48, 49, 50, 52, 51, 59, 53, \r\n54, 57, 56, 55, 46, 53, 55, 56, 57, 54, 59, 47, 51, 50, 49, 48, 58, 32, 44, 33, \r\n34, 35, 36, 38, 37, 45, 39, 40, 43, 42, 41, 32, 39, 41, 42, 43, 40, 45, 33, 37, \r\n36, 35, 34, 44, 18, 30, 19, 20, 21, 22, 24, 23, 31, 25, 26, 29, 28, 27, 18, 25, \r\n27, 28, 29, 26, 31, 19, 23, 22, 21, 20, 30, 4, 16, 5, 6, 7, 8, 10, 9, 17, 11, \r\n12, 15, 14, 13, 4, 11, 13, 14, 15, 12, 17, 5, 9, 8, 7, 6, 16, 750, 2]\r\n```</BODY>\n\n<COMMENTS>\n<Comment by zengxijuan at 2023-10-28T01:28:04Z>\nI got the same error. Any update?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 234,
    "state": "open",
    "created_by": "cyril-mino",
    "created_at": "2023-06-13T08:10:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/234</URL>\n\n<TITLE>Finetuning with context/history</TITLE>\n\n<BODY>I am currently fine-tuning LLaVA on medical images and reports. In medical reports, there are frequent references to previous images. For instance, a study may involve multiple reports, where the first report describes the first image and subsequent reports refer back to previous images. To accommodate this, I have developed dynamic prompts that take into account the image references.\r\n\r\nHowever, while experimenting with the LLaVA online demo, I noticed that the model's internal history or memory is reset every time a new image is provided.  Consequently, the model loses the ability to reference the previous image when answering a new prompt. This single image one inference turn procedure may not be ideal for my task.\r\n\r\nI would like to know the best approach to address this challenge:\r\n\r\nHow can I pass multiple images to the model effectively? Could you provide a detailed explanation of the process?\r\nIs there a way to prevent the model's internal history from resetting after uploading an image? This would enable the model to make references to the previous image when responding to subsequent prompt questions.\r\n\r\nI hope this captures your query accurately. Please let me know if you need further assistance!</BODY>\n\n<COMMENTS>\n<Comment by anushavishwanathan at 2023-12-14T08:42:09Z>\nAny updates on this?\r\n\r\nI am looking for a similar requirement of being able to reference prior images for object Re-Identification task. Any help would be much appreciated. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 233,
    "state": "closed",
    "created_by": "Hiusam",
    "created_at": "2023-06-12T01:36:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/233</URL>\n\n<TITLE>[Feature request] Chatting scripts with CLI.</TITLE>\n\n<BODY>### feature\n\nDear Haotian,\r\n\r\nThanks so much for your excellent work. Do you plan to release a chatting script based on the command line? The current run_llava.py only supports eval with one prompt.\r\n\r\nBest,\r\nHiusam.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-30T00:01:29Z>\nHi @Hiusam, we have supported CLI inference with multiple turn conversations, but it seems that I have never officially document it yet.\r\n\r\nJust updated that in [readme](https://github.com/haotian-liu/LLaVA#cli-inference), and please try it out, and also feel free to follow up if there are issues with the current cli interface. Thanks.\n</Comment>\n<Comment by haotian-liu at 2023-10-16T19:59:10Z>\nClosing due to inactivity, and please feel free to re-open if you have any other questions. Thank you!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 231,
    "state": "open",
    "created_by": "Vikho",
    "created_at": "2023-06-09T09:00:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/231</URL>\n\n<TITLE>[Question] Can I save the llava-7B-v0 model via torch.save() ?</TITLE>\n\n<BODY>### Question\n\nI tried this but I got PicklingError: \r\n<img width=\"1103\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/33713894/aa6ced03-bfda-43ec-af9c-f1b99c2ef804\"></BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-06-09T18:13:47Z>\nDo you want to save the model checkpoint? I think you may at least do it with `model.state_dict()`?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 230,
    "state": "open",
    "created_by": "denizsincar29",
    "created_at": "2023-06-08T09:26:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/230</URL>\n\n<TITLE>how to use llava api using python</TITLE>\n\n<BODY>### Discussion\n\nHello! I am a blind user that needs image recognition. I want to be able to use this model using python. Do I need to try gradio client for that? Also the client.view_api() didn't make much detailed documentation. Thank you</BODY>\n\n<COMMENTS>\n<Comment by chaitanya100100 at 2024-02-07T00:49:38Z>\nDid you get this working? I also want to query gradio server from another python app but gradio's client.view_api() isn't making any sense.\n</Comment>\n<Comment by kefaslungu at 2024-04-09T21:16:34Z>\nhi,\r\n\r\ncheck their [demo](https://llava.hliu.cc/?view=api)\r\n\r\nI don't know why, but the output is not readable.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 229,
    "state": "open",
    "created_by": "LetsGoFir",
    "created_at": "2023-06-08T03:03:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/229</URL>\n\n<TITLE>[Discussion] Could you please show the training time for stage2 in README?</TITLE>\n\n<BODY>### Discussion\n\nThanks!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-06-09T18:51:38Z>\nHi, thank you for your suggestion.  We'll add more of these details in README.  But for your reference, finetuning the 13B model on the full dataset takes around 10 hours on 8x A100s. Finetuning the 7B-lightning takes around 1 hour on 8x A100s.  Thanks.\n</Comment>\n<Comment by YAOYI626 at 2023-06-13T07:13:21Z>\n> Hi, thank you for your suggestion. We'll add more of these details in README. But for your reference, finetuning the 13B model on the full dataset takes around 10 hours on 8x A100s. Finetuning the 7B-lightning takes around 1 hour on 8x A100s. Thanks.\r\n\r\nHey @haotian-liu thanks for the reply! \r\n\r\nAs said in [README](https://github.com/haotian-liu/LLaVA/blob/main/README.md?plain=1#LL314C1-L318C1), pretraining the 13B model takes ~4 hours on 8xA100. Is that meaning that takes more time in finetuning stage than the pretraining stage?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 228,
    "state": "closed",
    "created_by": "Vikho",
    "created_at": "2023-06-07T09:05:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/228</URL>\n\n<TITLE>[Question] How can I make CLI inference on multi GPUs?</TITLE>\n\n<BODY>### Question\n\nI just try to load the 7B model to make CLI inference, but I got OOM Error and cant set up multi GPUs usage. \r\nI have two Tesla T4 GPUs, 16 GB / pic.\r\nAppreciate for any advice!\r\n<img width=\"1470\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/33713894/091b2f41-1df5-4cb9-acd8-1a5747479005\"></BODY>\n\n<COMMENTS>\n<Comment by Vikho at 2023-06-07T11:20:29Z>\nAh! Finally I got this solved.\r\n\r\n1. open the llava/run_llava.py script\r\n2. add your GPUs to be visible: os.environ['CUDA_VISIBLE_DEVICES'] = '0, 1'\r\n3. replace the code of the script\r\n```\r\nif \"mpt\" in model_name.lower():\r\n        model = LlavaMPTForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True, torch_dtype=torch.float16, use_cache=True).cuda()\r\nelse:\r\n        model = LlavaLlamaForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True, torch_dtype=torch.float16, use_cache=True).cuda()\r\n```\r\nwith\r\n```\r\nif \"mpt\" in model_name.lower():\r\n        model = LlavaMPTForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True, torch_dtype=torch.float16, use_cache=True, device_map='auto')\r\n   \r\nelse:\r\n        model = LlavaLlamaForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True, torch_dtype=torch.float16, use_cache=True,device_map='auto')\r\n```\r\nthen try it again and find the issue done.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 227,
    "state": "open",
    "created_by": "AnasHXH",
    "created_at": "2023-06-07T04:49:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/227</URL>\n\n<TITLE>[Question] I can not find the original LLaMA weights.</TITLE>\n\n<BODY>### Question\n\nPlease, can you explain how to do weight conversion?\r\nI have delta but I do not have weight, and I follow this :\r\n####################################################\r\nLLaVA Weights\r\nWe release [LLaVA](https://llava-vl.github.io/) weights as delta weights to comply with the LLaMA model license. You can add our delta to the original LLaMA weights to obtain the LLaVA weights.\r\nInstructions:\r\nGet the original LLaMA weights in the huggingface format by following the instructions [here](https://huggingface.co/docs/transformers/main/model_doc/llama).\r\nUse the following scripts to get LLaVA weights by applying our delta ([13b-v0](https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0), [7b-v0](https://huggingface.co/liuhaotian/LLaVA-7b-delta-v0), [lightning-7B-v1-1](https://huggingface.co/liuhaotian/LLaVA-Lightning-7B-delta-v1-1)). It will automatically download delta weights from our Hugging Face account.\r\n######################################################\r\nI can not find the original LLaMA weights.</BODY>\n\n<COMMENTS>\n<Comment by Vikho at 2023-06-07T09:25:03Z>\nhow about trying this : git clone https://huggingface.co/decapoda-research/llama-7b-hf\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 226,
    "state": "closed",
    "created_by": "jihan-yin",
    "created_at": "2023-06-07T03:37:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/226</URL>\n\n<TITLE>[Question] Where does the 80k instruct data used to train llava-lightning-7b come from?</TITLE>\n\n<BODY>### Question\n\nJust noticed that the 7b lightning checkpoint is based off the new 80k instruct data, not the old 150k instruct data. This new set seems to have none of the old detail set, and 20k of each the complex reasoning and conversation set. Where does the rest of the 40k samples come from?\r\n\r\nAlso, you should add a section in the README letting users know that the 7b checkpoint is trained on this new instruction set. I initially missed it as it was not talked about in the paper or explicitly mentioned (you have to look at the lightning train script to realize this), and it led to a lot of mistaken assumptions on the model/data.</BODY>\n\n<COMMENTS>\n<Comment by jihan-yin at 2023-06-07T21:58:33Z>\nNVM - I see that they are all sampled from the original conversation + cr sets, just that the image placement in the first conversation dialogue is sometimes flipped\n</Comment>\n<Comment by jihan-yin at 2023-06-07T22:29:42Z>\nActually, reopening - it seems there are around 700 samples in the new 80k set that are not present in either the old conversation or complex reasoning sets.\r\n\r\n```\r\nimport os\r\nimport json\r\n\r\nwith open('conversation_58k.json', 'r') as f:\r\n    c = json.load(f)\r\n    \r\nwith open('complex_reasoning_77k.json', 'r') as f:\r\n    cr = json.load(f)\r\n    \r\nwith open('llava_instruct_80k.json', 'r') as f:\r\n    a = json.load(f)\r\n\r\ndef filter_out_image(d):\r\n    k = dict(d)\r\n    k['conversations'][0]['value'] = k['conversations'][0]['value'].replace('<image>\\n', '')\r\n    k['conversations'][0]['value'] = k['conversations'][0]['value'].replace('\\n<image>', '')\r\n    return str(k)\r\n    \r\n\r\nc_set = set([filter_out_image(s) for s in c])\r\ncr_set = set([filter_out_image(s) for s in cr])\r\na_set = set([filter_out_image(s) for s in a])\r\n\r\nc_in_a = []\r\nfor r in c:\r\n    if str(r) in a_set:\r\n        c_in_a.append(r)\r\n        \r\ncr_in_a = []\r\nfor r in cr:\r\n    if str(r) in a_set:\r\n        cr_in_a.append(r)\r\n        \r\nlen(c_in_a + cr_in_a)\r\n```\r\ngets 79296 instead of 80000.\n</Comment>\n<Comment by haotian-liu at 2023-06-09T18:18:56Z>\nHi @jihan-yin \r\n\r\nThanks for your interest and for the investigation.  It is surprising to me that there are 700 new samples -- the way I construct the new 80k instruction data is the following steps:\r\n- Find the overlapping and non-overlapping images from the complex reasoning questions and conversation questions.\r\n- Add non-overlapping image-instruction pairs to each category (reason and conv)\r\n- Randomly sample from the overlapping set to make them to have 40K each.\r\n\r\nWhen I generate this 80K instruction set, I do not **re-query** GPT-4 at all, so it is surprising to me that there are new instructions.  One reason could be that the original 158K data is filtered with a slightly different set of keywords, so that some samples are filtered, while not filtered in the 80K set.\r\n\r\nI'll investigate this and get back to you if I find the reason.  And thank you for the great suggestion that we should add more clear description of the data and schedule to train each checkpoint.  We'll update this very soon :)\n</Comment>\n<Comment by dchichkov at 2023-07-03T22:09:55Z>\n@haotian-liu Please, can you tell what's the license on the 80K instruction set, if it is not generated by GPT-4?  The images of underlying COCO data seem to be mixed, most are CC-NC.  But what is the license of the text part?  Is it Apache 2.0, like the codebase?  Or is it a different license?\n</Comment>\n<Comment by haotian-liu at 2023-07-04T03:47:21Z>\n@dchichkov Sorry for any confusion the response above may give rise to, I did not *re-query* GPT-4, and it is a subset of the LLaVA-Instruct-150K. So it should still comply with the OpenAI license.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 225,
    "state": "open",
    "created_by": "wyzhhhh",
    "created_at": "2023-06-06T07:17:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/225</URL>\n\n<TITLE>[Question] About the LLM used in this project</TITLE>\n\n<BODY>### Question\n\nDoes anyone know if the LLM used for this project is the original llama? Or a fine-tuned llama? If llama was fine-tuned, what dataset was used? Hope someone can answer me, thanks!</BODY>\n\n<COMMENTS>\n<Comment by wangjiongw at 2023-06-06T15:10:18Z>\nI think the documentation shows that the LLM is Vicuna models, which is a finetuned llama by Fastchat.\n</Comment>\n<Comment by yifannnwu at 2023-06-06T16:13:46Z>\nI think the documentation says it's applying delta on LLaMA. I am also confused about the LLaMA version here. If anyone have an idea, please instruct.\n</Comment>\n<Comment by haotian-liu at 2023-06-09T18:53:45Z>\nHi guys, sorry for the confusion.  We finetune the model from Vicuna checkpoints (which are finetuned from LLaMA)\r\n\r\nHowever, when we create the delta weights (meaning you want to use our pretrained checkpoints, and do not train yourself), you should apply the delta weights from the LLaMA checkpoints (so that you do not need to download Vicuna delta and apply delta multiple times).\r\n\r\nPlease let me know if there are other questions, thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 224,
    "state": "open",
    "created_by": "SeungyounShin",
    "created_at": "2023-06-06T07:11:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/224</URL>\n\n<TITLE>[Question] What is the corresponding image for instruct 150K?</TITLE>\n\n<BODY>### Question\r\n\r\n`{\r\n    \"id\": \"000000442786\",\r\n    \"image\": \"000000442786.jpg\",\r\n    \"conversations\": [\r\n      {\r\n        \"from\": \"human\",\r\n        \"value\": \"What do you see happening in this image?\\n<image>\"\r\n      },\r\n      {\r\n        \"from\": \"gpt\",\r\n        \"value\": \"The scene depicts a lively plaza area with several people walking and enjoying their time. A man is standing in the plaza with his legs crossed, holding a kite in his hand. The kite has multiple sections attached to it, spread out in various directions as if ready for flight.\\n\\nNumerous people are scattered throughout the plaza, walking and interacting with others. Some of these individuals are carrying handbags, and others have backpacks. The image captures the casual, social atmosphere of a bustling plaza on a nice day.\"\r\n      }\r\n    ]\r\n  },`\r\n\r\nfor example, where can i get corresponding image with 000000442786.jpg?</BODY>\n\n<COMMENTS>\n<Comment by byrsongyuxin at 2023-06-06T16:43:02Z>\nthe same question\n</Comment>\n<Comment by sh0416 at 2023-06-08T02:36:17Z>\nSame here\n</Comment>\n<Comment by LetsGoFir at 2023-06-08T03:01:20Z>\nYou should check the training script and will find that is `COCO2014` images\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 223,
    "state": "closed",
    "created_by": "LetsGoFir",
    "created_at": "2023-06-06T06:36:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/223</URL>\n\n<TITLE>[Usage] ImportError: cannot import name 'mkdir_exists_ok' from 'wandb.sdk.lib.filesystem' (/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/filesystem.py)</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\ntorchrun --nnodes=1 --nproc_per_node=4 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path /path/llama-vicuna-13b \\\r\n    --data_path /path/LLaVA-CC3M-Pretrain-595K/chat.json \\\r\n    --image_folder /path/LLaVA-CC3M-Pretrain-595K/images \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end \\\r\n    --bf16 True \\\r\n    --output_dir /path/llava_checkpoints \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 2 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 2400 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --report_to none\r\n```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/tiger/LLaVA/llava/train/train_mem.py\", line 6, in <module>\r\n    from accelerate import __version__ as accelerate_version\r\n  File \"/usr/local/lib/python3.9/dist-packages/accelerate/__init__.py\", line 3, in <module>\r\n    from llava.train.llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn\r\n  File \"/opt/tiger/LLaVA/llava/__init__.py\", line 1, in <module>\r\n    from .accelerator import Accelerator\r\n  File \"/usr/local/lib/python3.9/dist-packages/accelerate/accelerator.py\", line 39, in <module>\r\n    from .model import LlavaLlamaForCausalLM\r\n  File \"/opt/tiger/LLaVA/llava/model/__init__.py\", line 1, in <module>\r\n    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers\r\n      File \"/usr/local/lib/python3.9/dist-packages/accelerate/tracking.py\", line 49, in <module>\r\nfrom .llava import LlavaLlamaForCausalLM, LlavaConfig\r\n  File \"/opt/tiger/LLaVA/llava/model/llava.py\", line 23, in <module>\r\n    import wandb    \r\nfrom transformers import AutoConfig, AutoModelForCausalLM, \\  File \"/usr/local/lib/python3.9/dist-packages/wandb/__init__.py\", line 34, in <module>\r\n\r\n  File \"<frozen importlib._bootstrap>\", line 1055, in _handle_fromlist\r\n    from wandb import sdk as wandb_sdk  File \"/usr/local/lib/python3.9/dist-packages/transformers/utils/import_utils.py\", line 1137, in __getattr__\r\n\r\n  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/__init__.py\", line 7, in <module>\r\n    from .wandb_artifacts import Artifact  # noqa: F401\r\n  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_artifacts.py\", line 31, in <module>\r\n    import wandb.data_types as data_types\r\n  File \"/usr/local/lib/python3.9/dist-packages/wandb/data_types.py\", line 47, in <module>\r\n    from .sdk.data_types.saved_model import _SavedModel\r\n  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/data_types/saved_model.py\", line 19, in <module>\r\n    from wandb.sdk.interface.artifacts import b64_string_to_hex, md5_files_b64\r\n  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/interface/artifacts/__init__.py\", line 7, in <module>\r\n    from wandb.sdk.interface.artifacts.artifact_cache import (\r\n  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/interface/artifacts/artifact_cache.py\", line 9, in <module>\r\n    value = getattr(module, name)\r\n  File \"/usr/local/lib/python3.9/dist-packages/transformers/utils/import_utils.py\", line 1136, in __getattr__\r\n    from wandb.sdk.lib.filesystem import mkdir_exists_ok\r\nImportError: cannot import name 'mkdir_exists_ok' from 'wandb.sdk.lib.filesystem' (/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/filesystem.py)\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/tiger/LLaVA/llava/train/train_mem.py\", line 6, in <module>\r\n    from llava.train.llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn\r\n  File \"/opt/tiger/LLaVA/llava/__init__.py\", line 1, in <module>\r\n    from .model import LlavaLlamaForCausalLM\r\n  File \"/opt/tiger/LLaVA/llava/model/__init__.py\", line 1, in <module>\r\n    from .llava import LlavaLlamaForCausalLM, LlavaConfig\r\n  File \"/opt/tiger/LLaVA/llava/model/llava.py\", line 23, in <module>\r\n    from transformers import AutoConfig, AutoModelForCausalLM, \\\r\n  File \"<frozen importlib._bootstrap>\", line 1055, in _handle_fromlist\r\n    module = self._get_module(self._class_to_module[name])\r\n  File \"/usr/local/lib/python3.9/dist-packages/transformers/utils/import_utils.py\", line 1148, in _get_module\r\n  File \"/usr/local/lib/python3.9/dist-packages/transformers/utils/import_utils.py\", line 1137, in __getattr__\r\n    raise RuntimeError(\r\nRuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\r\ncannot import name 'mkdir_exists_ok' from 'wandb.sdk.lib.filesystem' (/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/filesystem.py)\r\n    value = getattr(module, name)\r\n  File \"/usr/local/lib/python3.9/dist-packages/transformers/utils/import_utils.py\", line 1136, in __getattr__\r\n    module = self._get_module(self._class_to_module[name])\r\n  File \"/usr/local/lib/python3.9/dist-packages/transformers/utils/import_utils.py\", line 1148, in _get_module\r\n    raise RuntimeError(\r\nRuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\r\n```</BODY>\n\n<COMMENTS>\n<Comment by LetsGoFir at 2023-06-06T06:43:13Z>\nreinstall fix it\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 222,
    "state": "open",
    "created_by": "JulianJuaner",
    "created_at": "2023-06-06T05:22:55Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/222</URL>\n\n<TITLE>[Usage] Failed in 8-bit ScienceQA evaluation</TITLE>\n\n<BODY>### When did you clone our code?\r\n\r\nI cloned the code base after 5/1/23\r\n\r\n### Describe the issue\r\n\r\nIssue: I want to evaluate LLaVA-13B for ScienceQA with 3090s. \r\nFor memory efficiency, I drop the CLIP vision tower and pre-store all vision features for online loading.\r\nI cannot directly load an fp16 LLaVA to my device, so I tried to use an 8-bit loading as in other repos like miniGPT-4 and Alpaca-LoRA.\r\n\r\n```python\r\nmodel = LlavaLlamaForCausalLM.from_pretrained(model_name, load_in_8bit=True, torch_dtype=torch.float16, use_cache=True, device_map={'': device_8bit}).cuda()\r\n```\r\n \r\nHowever, even without the image input, prediction fails on 8-bit inference at the first iteration (see screenshot).\r\nI tried a very slow CPU-fp16 inference (~500 sec/iter) and it seems to work well. \r\nAccording to previous issues, I also ensure that I convert the delta model to the HuggingFace version llama-13b [here](https://huggingface.co/huggyllama/llama-13b).\r\nAre there any solutions or suggestions? Or is there a 7B LLaVA model for ScienceQA for reference?\r\nThanks a lot.\r\n \r\nCommand:\r\n```\r\n# Follow the command in README.md\r\n# A3. image feat-based inference\r\npython -m llava.eval.model_vqa_science \\\r\n    --model-name /mnt/proj76/myname/project/MM/LLaVA/pretrains/LLaVA-13b-v0-science_qa \\\r\n    --question-file /mnt/proj76/myname/project/MM/LLaVA/ScienceQA/data/scienceqa/llava_test_QCM-LEPA.json \\\r\n    --image-folder /mnt/proj76/myname/project/MM/LLaVA/ScienceQA/test \\\r\n    --answers-file vqa/results/ScienceQA/test_llava-13b.jsonl \\\r\n    --answer-prompter \\\r\n    --conv-mode simple\r\n```\r\n\r\nLog: \r\n```\r\n# Please see the screenshots\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.\r\n![Screenshot 2023-06-06 at 1 11 09 PM](https://github.com/haotian-liu/LLaVA/assets/37148692/7f008b64-8e4b-41c0-bd1e-ea67791eb42d)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 221,
    "state": "open",
    "created_by": "sameeravithana",
    "created_at": "2023-06-06T04:04:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/221</URL>\n\n<TITLE>[Question] How does LLaVA Generate the Exact Tokens in Lecture, Explanation in ScienceQA?</TITLE>\n\n<BODY>### Question\n\nHi, I am just wondering about how LLaVA was able to generate the exact tokens in the [lecture and explanation](https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/table/results/test_sqa_llava_13b_v0.json) in the ScienceQA responses. They all match perfect with the ground truth. (though I expect some level of uncertainity). Does ScienceQA fine-tuning lead to this behavior?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-06-09T18:20:03Z>\nHi, thank you for your interest in our work and for the great question.  Please see the explanation in #109, and please let me know if there are further questions, thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 220,
    "state": "closed",
    "created_by": "rumusan",
    "created_at": "2023-06-06T01:47:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/220</URL>\n\n<TITLE>[Question] Can we get the pretrained projector weights for llava-13b-v1-1?</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-16T19:59:41Z>\nPlease check out the model zoo for the projector weights, thanks.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#projector-weights\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 219,
    "state": "closed",
    "created_by": "rubydoudou",
    "created_at": "2023-06-05T09:19:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/219</URL>\n\n<TITLE>[Question] How to improve the performance on a specific task and how to fine-tune it?</TITLE>\n\n<BODY>### Question\r\n\r\nI am using the LLaVA model to judge whether an image is inappropriate for children.  However, if I directly describe the standard of what is inappropriate for children in the prompt, the result tends to judge all of the images as inappropriate for children. Currently, the best prompt is to rate images based on the MPAA standard. It recalls 16 out of 30 inappropriate images, and only \r\n\r\nI have tried the following prompts:\r\n\r\n1. Is this image appropriate for children? Answer the question in yes or no.\r\n2. Which of the following best describes the image?\r\n a. Appropriate for children.\r\n b. Slightly inappropriate for children.\r\n c. Inappropriate for children.\r\nSelect one of the above options.\r\n3. Please rate the image using MPAA standard. If it is suitable for all ages, rate it G.\r\n4. \"Inappropriate for children\" falls under one of the following categories:\r\n a. Revealing clothing\r\n b. Violence\r\n c. Sexual education and content\r\nIs this image appropriate for children?\r\n5. I am reviewing an image that contains sexual content or nudity. Please let me know if it is suitable for a child to view. \r\n\r\nHow can I improve the performance on this specific task either using zero-shot approach or fine-tuning? Thank you.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-10-26T20:54:18Z>\nPlease check out the docs on how to build and finetune on your own data, thanks!\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 217,
    "state": "open",
    "created_by": "mmaaz60",
    "created_at": "2023-06-04T17:16:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/217</URL>\n\n<TITLE>LLaVA versus LLaVA-Lightening</TITLE>\n\n<BODY>### Question\n\nHi,\r\n\r\nI am wondering what is the difference between LLaVA and LLaVA lightening models? Please point me out to the reference if I missed anything. Thank you.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-23T19:59:52Z>\nHi, you can check out the README here: https://github.com/haotian-liu/LLaVA#lightning\r\n\r\nBasically, we used a smaller subset of data and train for a shorter epochs. We also include these details in our new [MODEL ZOO](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md)\r\n\r\nThanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 216,
    "state": "open",
    "created_by": "jshilong",
    "created_at": "2023-06-04T14:09:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/216</URL>\n\n<TITLE>[Question]  Data Generation Script</TITLE>\n\n<BODY>### Question\n\nHi Haotian,\r\n\r\nI hope this message finds you well. I have been following your amazing work and I'm interested in generating some data myself. Would it be possible for you to share your scripts used to generate three types of conversations?\r\n\r\nThank you for your time and consideration. I look forward to hearing back from you soon.\r\n\r\nThanks</BODY>\n\n<COMMENTS>\n<Comment by RajdeepBorgohain at 2023-06-06T19:13:00Z>\nHi, @jshilong; refer to this location https://github.com/haotian-liu/LLaVA/tree/main/playground/data\r\nand combine it with the paper and try it. I am also looking for the same but planning to do the same.\n</Comment>\n<Comment by Luo-Z13 at 2023-12-30T13:03:41Z>\n> your time and consideration. I look forward to hearing back from you soon.\r\n\r\n\r\n\r\n> Hi, @jshilong; refer to this location https://github.com/haotian-liu/LLaVA/tree/main/playground/data and combine it with the paper and try it. I am also looking for the same but planning to do the same.\r\n\r\n\r\nHello @RajdeepBorgohain , have you found any open-source data processing pipeline? It seems that many existing projects do not provide scripts for generating descriptions using GPT-4 or LLAVA. If you come across anything, your assistance would be highly appreciated!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 214,
    "state": "open",
    "created_by": "yifannnwu",
    "created_at": "2023-06-04T06:57:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/214</URL>\n\n<TITLE>[Question] LLaMA weights</TITLE>\n\n<BODY>### Question\r\n\r\nDoes anyone have trouble getting LLaMA weights after submitting the weights request Google Form (https://huggingface.co/docs/transformers/main/model_doc/llama)? I submitted the form 8 days ago but haven't heard anything yet. Is this the normal processing time? Is there any alternative to get LLaMA weights so that I can apply delta to get the LLaVA model? Many thanks for the reply in advance!</BODY>\n\n<COMMENTS>\n<Comment by timovr-dev at 2023-06-05T09:42:57Z>\nI waited for the weights approximately 3 to 4 weeks. You can get the weights from unofficial sources. \r\nI'm not going to put a link here but it's pretty easy to find those sources on github.\n</Comment>\n<Comment by yifannnwu at 2023-06-05T16:35:07Z>\nThanks for your reply! \r\n\r\n> I waited for the weights approximately 3 to 4 weeks. You can get the weights from unofficial sources. I'm not going to put a link here but it's pretty easy to find those sources on github.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 213,
    "state": "closed",
    "created_by": "jihan-yin",
    "created_at": "2023-06-03T20:36:35Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/213</URL>\n\n<TITLE>[Usage] Model config in huggingface has incorrect token ids for start and end of sequence</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nThe model config at https://huggingface.co/liuhaotian/LLaVA-Lightning-7B-delta-v1-1/tree/main has an incorrect mapping of the bos token id and eos token id, for both the `generation_config.json` and `config.json` (the prior doesn't matter as it just points to the latter).\r\n\r\nIn Llava, the bos token id is set to 0, and the eos token id is set to 1. You can see this is incorrect by loading the tokenizer from the model weights and printing `tokenizer.bos_token_id` and `tokenizer.eos_token_id`. They should be `1` and `2`, respectively. This matches the base model, vicuna, as seen here - https://huggingface.co/lmsys/vicuna-7b-delta-v1.1/blob/main/config.json.\r\n\r\nBecause of this issue, if you try to do batch generation with the decoder, you will run into errors when the model tries to generate `eos` but fails. Here is a 100% reproducible example with the llava-7b-lightning checkpoint:\r\n\r\n```\r\nprompts = ['Answer these questions:\\nQ: Who sang who wants to be a millionaire in high society?\\nA: Frank Sinatra\\nQ: Who won oscar for best director this month?\\nA:', 'Answer these questions:\\nQ: Who sang who wants to be a millionaire in high society?\\nA: Frank Sinatra\\nQ: When was the first dark tower book published?\\nA:']\r\nfor p in prompts:\r\n    print(p)\r\ninputs = tokenizer(prompts, padding=True, return_tensors='pt')\r\nout = model.generate(**inputs, max_new_tokens=50)\r\n```\r\n\r\nThis will error out with a CUDA error, because the model generates the token `1` to mark the eos for one of the two samples in the batch, when it should have generated a `2`.</BODY>\n\n<COMMENTS>\n<Comment by jihan-yin at 2023-06-03T20:52:31Z>\nActually, it seems even with this change, we will still get an error when the model generates the eos token id on a different example:\r\n\r\n```\r\nprompts = ['Answer these questions:\\nQ: Who sang who wants to be a millionaire in high society?\\nA: Frank Sinatra\\nQ: How many members in order of the phoenix?\\nA:', 'Answer these questions:\\nQ: Who sang who wants to be a millionaire in high society?\\nA: Frank Sinatra\\nQ: The secret life of walter mitty james thurber pdf?\\nA:']\r\nfor p in prompts:\r\n    print(p)\r\ntokenizer.padding_side = 'left'\r\ninputs = tokenizer(prompts, padding=True, return_tensors='pt')\r\nout = model.generate(**inputs, max_new_tokens=50)\r\n```\n</Comment>\n<Comment by jihan-yin at 2023-06-03T20:58:51Z>\nAh resolved the above, we have to set the `pad_token_id` to 0 as well. This is because huggingface's greedy generation will automatically set all following token ids to the `pad_token_id` when the `eos_token` is encountered for a sample in a batch. Llava's `pad_token_id` is set to -1, which results in the cuda indexing error.\n</Comment>\n<Comment by haotian-liu at 2023-06-04T02:30:30Z>\nHI @jihan-yin \r\n\r\nThanks for providing the information on this!  I will be working on fixing these tokenization errors next week, and hopefully we can have the HF inference pipeline and batch inference working soon :)\n</Comment>\n<Comment by xiujiesong at 2023-08-04T09:26:17Z>\nHi haotian,\r\n\r\nThanks for the great work. Is the HF inference pipeline available now？\n</Comment>\n<Comment by haotian-liu at 2023-08-05T05:21:35Z>\nHi @xiujiesong \r\n\r\nWe have supported HF inference pipeline for both [offline generation](https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/run_llava.py) and [streaming generation](https://github.com/haotian-liu/LLaVA/blob/main/llava/serve/cli.py).\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 212,
    "state": "closed",
    "created_by": "AnasHXH",
    "created_at": "2023-06-03T11:56:02Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/212</URL>\n\n<TITLE>[Question] How to use llavA as a part of code?</TITLE>\n\n<BODY>### Question\n\nHow to use llavA as a part of code?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-04T23:50:48Z>\nYou may check out the following resources, thanks!\r\n\r\nhttps://github.com/haotian-liu/LLaVA#quick-start-with-huggingface\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/llava/eval/run_llava.py\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 211,
    "state": "closed",
    "created_by": "deter3",
    "created_at": "2023-06-03T05:57:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/211</URL>\n\n<TITLE>[Question] How to get LLaVA running on runpod ?</TITLE>\n\n<BODY>### Question\n\nI successfully installed the model but can hardly load the images  , so user interface yet . meantime , pure text turned to be a random reply without any meaning .</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 210,
    "state": "closed",
    "created_by": "sameeravithana",
    "created_at": "2023-06-02T17:44:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/210</URL>\n\n<TITLE>Error in applying delta LLaVA-13b-delta-v0 to LLaMA-13B</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\n   python -m llava.model.apply_delta \\\r\n        --base <HF-converted-LLAMA-weights> \\\r\n        --target <TARGET_FOLDER> \\\r\n        --delta liuhaotian/LLaVA-13b-delta-v0\r\n```\r\n\r\nLog: \r\n```\r\n╭───────────────────── Traceback (most recent call last) ──────────────────────╮\r\n│ ../.conda/envs/llava/lib/python3.9/runpy.py:197 in              │\r\n│ _run_module_as_main                                                          │\r\n│                                                                              │\r\n│   194 │   main_globals = sys.modules[\"__main__\"].__dict__                    │\r\n│   195 │   if alter_argv:                                                     │\r\n│   196 │   │   sys.argv[0] = mod_spec.origin                                  │\r\n│ ❱ 197 │   return _run_code(code, main_globals, None,                         │\r\n│   198 │   │   │   │   │    \"__main__\", mod_spec)                             │\r\n│   199                                                                        │\r\n│   200 def run_module(mod_name, init_globals=None,                            │\r\n│                                                                              │\r\n│ ../.conda/envs/llava/lib/python3.9/runpy.py:87 in _run_code     │\r\n│                                                                              │\r\n│    84 │   │   │   │   │      __loader__ = loader,                            │\r\n│    85 │   │   │   │   │      __package__ = pkg_name,                         │\r\n│    86 │   │   │   │   │      __spec__ = mod_spec)                            │\r\n│ ❱  87 │   exec(code, run_globals)                                            │\r\n│    88 │   return run_globals                                                 │\r\n│    89                                                                        │\r\n│    90 def _run_module_code(code, init_globals=None,                          │\r\n│                                                                              │\r\n│ ../LLaV │\r\n│ A/llava/model/apply_delta.py:48 in <module>                                  │\r\n│                                                                              │\r\n│   45 │                                                                       │\r\n│   46 │   args = parser.parse_args()                                          │\r\n│   47 │                                                                       │\r\n│ ❱ 48 │   apply_delta(args.base_model_path, args.target_model_path, args.delt │\r\n│   49                                                                         │\r\n│                                                                              │\r\n│ ../LLaV │\r\n│ A/llava/model/apply_delta.py:20 in apply_delta                               │\r\n│                                                                              │\r\n│   17 │                                                                       │\r\n│   18 │   print(\"Loading delta\")                                              │\r\n│   19 │   delta = LlavaLlamaForCausalLM.from_pretrained(delta_path, torch_dty │\r\n│ ❱ 20 │   delta_tokenizer = AutoTokenizer.from_pretrained(delta_path)         │\r\n│   21 │                                                                       │\r\n│   22 │   print(\"Applying delta\")                                             │\r\n│   23 │   for name, param in tqdm(delta.state_dict().items(), desc=\"Applying  │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/models/auto/tokenization_auto.py:694 in            │\r\n│ from_pretrained                                                              │\r\n│                                                                              │\r\n│   691 │   │   │   │   raise ValueError(                                      │\r\n│   692 │   │   │   │   │   f\"Tokenizer class {tokenizer_class_candidate} does │\r\n│   693 │   │   │   │   )                                                      │\r\n│ ❱ 694 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │\r\n│   695 │   │                                                                  │\r\n│   696 │   │   # Otherwise we have to be creative.                            │\r\n│   697 │   │   # if model is an encoder decoder, the encoder tokenizer class  │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_base.py:1820 in from_pretrained │\r\n│                                                                              │\r\n│   1817 │   │   │   else:                                                     │\r\n│   1818 │   │   │   │   logger.info(f\"loading file {file_path} from cache at  │\r\n│   1819 │   │                                                                 │\r\n│ ❱ 1820 │   │   return cls._from_pretrained(                                  │\r\n│   1821 │   │   │   resolved_vocab_files,                                     │\r\n│   1822 │   │   │   pretrained_model_name_or_path,                            │\r\n│   1823 │   │   │   init_configuration,                                       │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_base.py:1983 in                 │\r\n│ _from_pretrained                                                             │\r\n│                                                                              │\r\n│   1980 │   │                                                                 │\r\n│   1981 │   │   # Instantiate tokenizer.                                      │\r\n│   1982 │   │   try:                                                          │\r\n│ ❱ 1983 │   │   │   tokenizer = cls(*init_inputs, **init_kwargs)              │\r\n│   1984 │   │   except OSError:                                               │\r\n│   1985 │   │   │   raise OSError(                                            │\r\n│   1986 │   │   │   │   \"Unable to load vocabulary from file. \"               │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/models/llama/tokenization_llama_fast.py:104 in     │\r\n│ __init__                                                                     │\r\n│                                                                              │\r\n│   101 │   │   )                                                              │\r\n│   102 │   │   self._add_bos_token = add_bos_token                            │\r\n│   103 │   │   self._add_eos_token = add_eos_token                            │\r\n│ ❱ 104 │   │   self.update_post_processor()                                   │\r\n│   105 │   │                                                                  │\r\n│   106 │   │   self.vocab_file = vocab_file                                   │\r\n│   107 │   │   self.can_save_slow_tokenizer = False if not self.vocab_file el │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/models/llama/tokenization_llama_fast.py:111 in     │\r\n│ update_post_processor                                                        │\r\n│                                                                              │\r\n│   108 │                                                                      │\r\n│   109 │   def update_post_processor(self):                                   │\r\n│   110 │   │   bos = self.bos_token                                           │\r\n│ ❱ 111 │   │   bos_token_id = self.bos_token_id                               │\r\n│   112 │   │                                                                  │\r\n│   113 │   │   eos = self.eos_token                                           │\r\n│   114 │   │   eos_token_id = self.eos_token_id                               │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_base.py:1131 in bos_token_id    │\r\n│                                                                              │\r\n│   1128 │   │   \"\"\"                                                           │\r\n│   1129 │   │   if self._bos_token is None:                                   │\r\n│   1130 │   │   │   return None                                               │\r\n│ ❱ 1131 │   │   return self.convert_tokens_to_ids(self.bos_token)             │\r\n│   1132 │                                                                     │\r\n│   1133 │   @property                                                         │\r\n│   1134 │   def eos_token_id(self) -> Optional[int]:                          │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_fast.py:250 in                  │\r\n│ convert_tokens_to_ids                                                        │\r\n│                                                                              │\r\n│   247 │   │   │   return None                                                │\r\n│   248 │   │                                                                  │\r\n│   249 │   │   if isinstance(tokens, str):                                    │\r\n│ ❱ 250 │   │   │   return self._convert_token_to_id_with_added_voc(tokens)    │\r\n│   251 │   │                                                                  │\r\n│   252 │   │   return [self._convert_token_to_id_with_added_voc(token) for to │\r\n│   253                                                                        │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_fast.py:257 in                  │\r\n│ _convert_token_to_id_with_added_voc                                          │\r\n│                                                                              │\r\n│   254 │   def _convert_token_to_id_with_added_voc(self, token: str) -> int:  │\r\n│   255 │   │   index = self._tokenizer.token_to_id(token)                     │\r\n│   256 │   │   if index is None:                                              │\r\n│ ❱ 257 │   │   │   return self.unk_token_id                                   │\r\n│   258 │   │   return index                                                   │\r\n│   259 │                                                                      │\r\n│   260 │   def _convert_id_to_token(self, index: int) -> Optional[str]:       │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_base.py:1150 in unk_token_id    │\r\n│                                                                              │\r\n│   1147 │   │   \"\"\"                                                           │\r\n│   1148 │   │   if self._unk_token is None:                                   │\r\n│   1149 │   │   │   return None                                               │\r\n│ ❱ 1150 │   │   return self.convert_tokens_to_ids(self.unk_token)             │\r\n│   1151 │                                                                     │\r\n│   1152 │   @property                                                         │\r\n│   1153 │   def sep_token_id(self) -> Optional[int]:                          │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_fast.py:250 in                  │\r\n│ convert_tokens_to_ids                                                        │\r\n│                                                                              │\r\n│   247 │   │   │   return None                                                │\r\n│   248 │   │                                                                  │\r\n│   249 │   │   if isinstance(tokens, str):                                    │\r\n│ ❱ 250 │   │   │   return self._convert_token_to_id_with_added_voc(tokens)    │\r\n│   251 │   │                                                                  │\r\n│   252 │   │   return [self._convert_token_to_id_with_added_voc(token) for to │\r\n│   253                                                                        │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_fast.py:257 in                  │\r\n│ _convert_token_to_id_with_added_voc                                          │\r\n│                                                                              │\r\n│   254 │   def _convert_token_to_id_with_added_voc(self, token: str) -> int:  │\r\n│   255 │   │   index = self._tokenizer.token_to_id(token)                     │\r\n│   256 │   │   if index is None:                                              │\r\n│ ❱ 257 │   │   │   return self.unk_token_id                                   │\r\n│   258 │   │   return index                                                   │\r\n│   259 │                                                                      │\r\n│   260 │   def _convert_id_to_token(self, index: int) -> Optional[str]:       │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_base.py:1150 in unk_token_id    │\r\n│                                                                              │\r\n│   1147 │   │   \"\"\"                                                           │\r\n│   1148 │   │   if self._unk_token is None:                                   │\r\n│   1149 │   │   │   return None                                               │\r\n│ ❱ 1150 │   │   return self.convert_tokens_to_ids(self.unk_token)             │\r\n│   1151 │                                                                     │\r\n│   1152 │   @property                                                         │\r\n│   1153 │   def sep_token_id(self) -> Optional[int]:                          │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_fast.py:250 in                  │\r\n│ convert_tokens_to_ids                                                        │\r\n│                                                                              │\r\n│   247 │   │   │   return None                                                │\r\n│   248 │   │                                                                  │\r\n│   249 │   │   if isinstance(tokens, str):                                    │\r\n│ ❱ 250 │   │   │   return self._convert_token_to_id_with_added_voc(tokens)    │\r\n│   251 │   │                                                                  │\r\n│   252 │   │   return [self._convert_token_to_id_with_added_voc(token) for to │\r\n│   253                                                                        │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_fast.py:257 in                  │\r\n│ _convert_token_to_id_with_added_voc                                          │\r\n│                                                                              │\r\n│   254 │   def _convert_token_to_id_with_added_voc(self, token: str) -> int:  │\r\n│   255 │   │   index = self._tokenizer.token_to_id(token)                     │\r\n│   256 │   │   if index is None:                                              │\r\n│ ❱ 257 │   │   │   return self.unk_token_id                                   │\r\n│   258 │   │   return index                                                   │\r\n│   259 │                                                                      │\r\n│   260 │   def _convert_id_to_token(self, index: int) -> Optional[str]:       │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_base.py:1150 in unk_token_id    │\r\n│                                                                              │\r\n│   1147 │   │   \"\"\"                                                           │\r\n│   1148 │   │   if self._unk_token is None:                                   │\r\n│   1149 │   │   │   return None                                               │\r\n│ ❱ 1150 │   │   return self.convert_tokens_to_ids(self.unk_token)             │\r\n│   1151 │                                                                     │\r\n│   1152 │   @property                                                         │\r\n│   1153 │   def sep_token_id(self) -> Optional[int]:                          │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_fast.py:250 in                  │\r\n│ convert_tokens_to_ids                                                        │\r\n│                                                                              │\r\n│   247 │   │   │   return None                                                │\r\n│   248 │   │                                                                  │\r\n│   249 │   │   if isinstance(tokens, str):                                    │\r\n│ ❱ 250 │   │   │   return self._convert_token_to_id_with_added_voc(tokens)    │\r\n│   251 │   │                                                                  │\r\n│   252 │   │   return [self._convert_token_to_id_with_added_voc(token) for to │\r\n│   253                                                                        │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_fast.py:257 in                  │\r\n│ _convert_token_to_id_with_added_voc                                          │\r\n│                                                                              │\r\n│   254 │   def _convert_token_to_id_with_added_voc(self, token: str) -> int:  │\r\n│   255 │   │   index = self._tokenizer.token_to_id(token)                     │\r\n│   256 │   │   if index is None:                                              │\r\n│ ❱ 257 │   │   │   return self.unk_token_id                                   │\r\n│   258 │   │   return index                                                   │\r\n│   259 │                                                                      │\r\n│   260 │   def _convert_id_to_token(self, index: int) -> Optional[str]:       │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_base.py:1150 in unk_token_id    │\r\n│                                                                              │\r\n│   1147 │   │   \"\"\"                                                           │\r\n│   1148 │   │   if self._unk_token is None:                                   │\r\n│   1149 │   │   │   return None                                               │\r\n│ ❱ 1150 │   │   return self.convert_tokens_to_ids(self.unk_token)             │\r\n│   1151 │                                                                     │\r\n│   1152 │   @property                                                         │\r\n│   1153 │   def sep_token_id(self) -> Optional[int]:                          │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_fast.py:250 in                  │\r\n│ convert_tokens_to_ids                                                        │\r\n│                                                                              │\r\n│   247 │   │   │   return None                                                │\r\n│   248 │   │                                                                  │\r\n│   249 │   │   if isinstance(tokens, str):                                    │\r\n│ ❱ 250 │   │   │   return self._convert_token_to_id_with_added_voc(tokens)    │\r\n│   251 │   │                                                                  │\r\n│   252 │   │   return [self._convert_token_to_id_with_added_voc(token) for to │\r\n│   253                                                                        │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_fast.py:257 in                  │\r\n│ _convert_token_to_id_with_added_voc                                          │\r\n│                                                                              │\r\n│   254 │   def _convert_token_to_id_with_added_voc(self, token: str) -> int:  │\r\n│   255 │   │   index = self._tokenizer.token_to_id(token)                     │\r\n│   256 │   │   if index is None:                                              │\r\n│ ❱ 257 │   │   │   return self.unk_token_id                                   │\r\n│   258 │   │   return index                                                   │\r\n│   259 │                                                                      │\r\n│   260 │   def _convert_id_to_token(self, index: int) -> Optional[str]:       │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_base.py:1150 in unk_token_id    │\r\n│                                                                              │\r\n│   1147 │   │   \"\"\"                                                           │\r\n│   1148 │   │   if self._unk_token is None:                                   │\r\n│   1149 │   │   │   return None                                               │\r\n│ ❱ 1150 │   │   return self.convert_tokens_to_ids(self.unk_token)             │\r\n│   1151 │                                                                     │\r\n│   1152 │   @property                                                         │\r\n│   1153 │   def sep_token_id(self) -> Optional[int]:                          │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_fast.py:250 in                  │\r\n│ convert_tokens_to_ids                                                        │\r\n│                                                                              │\r\n│   247 │   │   │   return None                                                │\r\n│   248 │   │                                                                  │\r\n│   249 │   │   if isinstance(tokens, str):                                    │\r\n│ ❱ 250 │   │   │   return self._convert_token_to_id_with_added_voc(tokens)    │\r\n│   251 │   │                                                                  │\r\n│   252 │   │   return [self._convert_token_to_id_with_added_voc(token) for to │\r\n│   253                                                                        │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_fast.py:257 in                  │\r\n│ _convert_token_to_id_with_added_voc                                          │\r\n│                                                                              │\r\n│   254 │   def _convert_token_to_id_with_added_voc(self, token: str) -> int:  │\r\n│   255 │   │   index = self._tokenizer.token_to_id(token)                     │\r\n│   256 │   │   if index is None:                                              │\r\n│ ❱ 257 │   │   │   return self.unk_token_id                                   │\r\n│   258 │   │   return index                                                   │\r\n│   259 │                                                                      │\r\n│   260 │   def _convert_id_to_token(self, index: int) -> Optional[str]:       │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_base.py:1150 in unk_token_id    │\r\n│                                                                              │\r\n│   1147 │   │   \"\"\"                                                           │\r\n│   1148 │   │   if self._unk_token is None:                                   │\r\n│   1149 │   │   │   return None                                               │\r\n│ ❱ 1150 │   │   return self.convert_tokens_to_ids(self.unk_token)             │\r\n│   1151 │                                                                     │\r\n│   1152 │   @property                                                         │\r\n│   1153 │   def sep_token_id(self) -> Optional[int]:                          │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_fast.py:250 in                  │\r\n│ convert_tokens_to_ids                                                        │\r\n│                                                                              │\r\n│   247 │   │   │   return None                                                │\r\n│   248 │   │                                                                  │\r\n│   249 │   │   if isinstance(tokens, str):                                    │\r\n│ ❱ 250 │   │   │   return self._convert_token_to_id_with_added_voc(tokens)    │\r\n│   251 │   │                                                                  │\r\n│   252 │   │   return [self._convert_token_to_id_with_added_voc(token) for to │\r\n│   253                                                                        │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_fast.py:257 in                  │\r\n│ _convert_token_to_id_with_added_voc                                          │\r\n│                                                                              │\r\n│   254 │   def _convert_token_to_id_with_added_voc(self, token: str) -> int:  │\r\n│   255 │   │   index = self._tokenizer.token_to_id(token)                     │\r\n│   256 │   │   if index is None:                                              │\r\n│ ❱ 257 │   │   │   return self.unk_token_id                                   │\r\n│   258 │   │   return index                                                   │\r\n│   259 │                                                                      │\r\n│   260 │   def _convert_id_to_token(self, index: int) -> Optional[str]:       │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_base.py:1150 in unk_token_id    │\r\n│                                                                              │\r\n│   1147 │   │   \"\"\"                                                           │\r\n│   1148 │   │   if self._unk_token is None:                                   │\r\n│   1149 │   │   │   return None                                               │\r\n│ ❱ 1150 │   │   return self.convert_tokens_to_ids(self.unk_token)             │\r\n│   1151 │                                                                     │\r\n│   1152 │   @property                                                         │\r\n│   1153 │   def sep_token_id(self) -> Optional[int]:                          │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_fast.py:250 in                  │\r\n│ convert_tokens_to_ids                                                        │\r\n│                                                                              │\r\n│   247 │   │   │   return None                                                │\r\n│   248 │   │                                                                  │\r\n│   249 │   │   if isinstance(tokens, str):                                    │\r\n│ ❱ 250 │   │   │   return self._convert_token_to_id_with_added_voc(tokens)    │\r\n│   251 │   │                                                                  │\r\n│   252 │   │   return [self._convert_token_to_id_with_added_voc(token) for to │\r\n│   253                                                                        │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_fast.py:257 in                  │\r\n│ _convert_token_to_id_with_added_voc                                          │\r\n│                                                                              │\r\n│   254 │   def _convert_token_to_id_with_added_voc(self, token: str) -> int:  │\r\n│   255 │   │   index = self._tokenizer.token_to_id(token)                     │\r\n│   256 │   │   if index is None:                                              │\r\n│ ❱ 257 │   │   │   return self.unk_token_id                                   │\r\n│   258 │   │   return index                                                   │\r\n│   259 │                                                                      │\r\n│   260 │   def _convert_id_to_token(self, index: int) -> Optional[str]:       │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_base.py:1150 in unk_token_id    │\r\n│                                                                              │\r\n│   1147 │   │   \"\"\"                                                           │\r\n│   1148 │   │   if self._unk_token is None:                                   │\r\n│   1149 │   │   │   return None                                               │\r\n│ ❱ 1150 │   │   return self.convert_tokens_to_ids(self.unk_token)             │\r\n│   1151 │                                                                     │\r\n│   1152 │   @property                                                         │\r\n│   1153 │   def sep_token_id(self) -> Optional[int]:                          │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_fast.py:250 in                  │\r\n│ convert_tokens_to_ids                                                        │\r\n│                                                                              │\r\n│   247 │   │   │   return None                                                │\r\n│   248 │   │                                                                  │\r\n│   249 │   │   if isinstance(tokens, str):                                    │\r\n│ ❱ 250 │   │   │   return self._convert_token_to_id_with_added_voc(tokens)    │\r\n│   251 │   │                                                                  │\r\n│   252 │   │   return [self._convert_token_to_id_with_added_voc(token) for to │\r\n│   253                                                                        │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_fast.py:257 in                  │\r\n│ _convert_token_to_id_with_added_voc                                          │\r\n│                                                                              │\r\n│   254 │   def _convert_token_to_id_with_added_voc(self, token: str) -> int:  │\r\n│   255 │   │   index = self._tokenizer.token_to_id(token)                     │\r\n│   256 │   │   if index is None:                                              │\r\n│ ❱ 257 │   │   │   return self.unk_token_id                                   │\r\n│   258 │   │   return index                                                   │\r\n│   259 │                                                                      │\r\n│   260 │   def _convert_id_to_token(self, index: int) -> Optional[str]:       │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_base.py:1150 in unk_token_id    │\r\n│                                                                              │\r\n│   1147 │   │   \"\"\"                                                           │\r\n│   1148 │   │   if self._unk_token is None:                                   │\r\n│   1149 │   │   │   return None                                               │\r\n│ ❱ 1150 │   │   return self.convert_tokens_to_ids(self.unk_token)             │\r\n│   1151 │                                                                     │\r\n│   1152 │   @property                                                         │\r\n│   1153 │   def sep_token_id(self) -> Optional[int]:                          │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_fast.py:250 in                  │\r\n│ convert_tokens_to_ids                                                        │\r\n│                                                                              │\r\n│   247 │   │   │   return None                                                │\r\n│   248 │   │                                                                  │\r\n│   249 │   │   if isinstance(tokens, str):                                    │\r\n│ ❱ 250 │   │   │   return self._convert_token_to_id_with_added_voc(tokens)    │\r\n│   251 │   │                                                                  │\r\n│   252 │   │   return [self._convert_token_to_id_with_added_voc(token) for to │\r\n│   253                                                                        │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_fast.py:257 in                  │\r\n│ _convert_token_to_id_with_added_voc                                          │\r\n│                                                                              │\r\n│   254 │   def _convert_token_to_id_with_added_voc(self, token: str) -> int:  │\r\n│   255 │   │   index = self._tokenizer.token_to_id(token)                     │\r\n│   256 │   │   if index is None:                                              │\r\n│ ❱ 257 │   │   │   return self.unk_token_id                                   │\r\n│   258 │   │   return index                                                   │\r\n│   259 │                                                                      │\r\n│   260 │   def _convert_id_to_token(self, index: int) -> Optional[str]:       │\r\n│                                                                              │\r\n.....                │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_fast.py:250 in                  │\r\n│ convert_tokens_to_ids                                                        │\r\n│                                                                              │\r\n│   247 │   │   │   return None                                                │\r\n│   248 │   │                                                                  │\r\n│   249 │   │   if isinstance(tokens, str):                                    │\r\n│ ❱ 250 │   │   │   return self._convert_token_to_id_with_added_voc(tokens)    │\r\n│   251 │   │                                                                  │\r\n│   252 │   │   return [self._convert_token_to_id_with_added_voc(token) for to │\r\n│   253                                                                        │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_fast.py:257 in                  │\r\n│ _convert_token_to_id_with_added_voc                                          │\r\n│                                                                              │\r\n│   254 │   def _convert_token_to_id_with_added_voc(self, token: str) -> int:  │\r\n│   255 │   │   index = self._tokenizer.token_to_id(token)                     │\r\n│   256 │   │   if index is None:                                              │\r\n│ ❱ 257 │   │   │   return self.unk_token_id                                   │\r\n│   258 │   │   return index                                                   │\r\n│   259 │                                                                      │\r\n│   260 │   def _convert_id_to_token(self, index: int) -> Optional[str]:       │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_base.py:1150 in unk_token_id    │\r\n│                                                                              │\r\n│   1147 │   │   \"\"\"                                                           │\r\n│   1148 │   │   if self._unk_token is None:                                   │\r\n│   1149 │   │   │   return None                                               │\r\n│ ❱ 1150 │   │   return self.convert_tokens_to_ids(self.unk_token)             │\r\n│   1151 │                                                                     │\r\n│   1152 │   @property                                                         │\r\n│   1153 │   def sep_token_id(self) -> Optional[int]:                          │\r\n│                                                                              │\r\n│ ../tran │\r\n│ sformers/src/transformers/tokenization_utils_base.py:1030 in unk_token       │\r\n│                                                                              │\r\n│   1027 │   │   │   if self.verbose:                                          │\r\n│   1028 │   │   │   │   logger.error(\"Using unk_token, but it is not set yet. │\r\n│   1029 │   │   │   return None                                               │\r\n│ ❱ 1030 │   │   return str(self._unk_token)                                   │\r\n│   1031 │                                                                     │\r\n│   1032 │   @property                                                         │\r\n│   1033 │   def sep_token(self) -> str:                                       │\r\n╰──────────────────────────────────────────────────────────────────────────────╯\r\nRecursionError: maximum recursion depth exceeded while calling a Python object\r\n```\r\n\r\nUse the latest ```Transformers=='4.30.0.dev0'```</BODY>\n\n<COMMENTS>\n<Comment by sameeravithana at 2023-06-02T18:51:31Z>\nThis was resolved when using ```liuhaotian/LLaVA-13b-delta-v1-1```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 209,
    "state": "closed",
    "created_by": "jun297",
    "created_at": "2023-06-02T05:04:26Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/209</URL>\n\n<TITLE>[Question] Running LLaVA-13b-v1-1 locally gets slightly different results with web demo</TITLE>\n\n<BODY>### Question\r\n\r\nHi, I am running LLaVA-13B-v1-1 in web UI or CLI\r\n\r\nI am trying to reproduce the results as the same as the web demo (https://llava.hliu.cc/)\r\n\r\nHowever, I got slightly diffrent results with examples\r\n\r\nFrom web UI:\r\n<code>\r\nThe unusual aspect of this image is that a man is ironing clothes while standing on the back of a moving car. This is an unconventional and dangerous way to iron clothes, as it poses significant risks to the man's safety and the safety of others on the road. Typically, ironing is done at home or in a designated area, not on the back of a moving vehicle. Additionally, the man is wearing a yellow sweater, which is not common attire for someone ironing clothes in public.\r\n</code>\r\nFrom web demo:\r\n<code>\r\nThe unusual aspect of this image is that a man is standing on a portable ironing board in the middle of a busy city street, ironing clothes. This is not a typical scene you would expect to see in an urban environment, as ironing is usually done in the comfort and privacy of one's home. The man's actions are drawing attention and likely causing curiosity among the people passing by and the traffic around him. Additionally, there is a parked yellow SUV in the background, which further contributes to the uncommon setting of the scene.\r\n</code>\r\nwhile it is possible to regenerate the same result itself with some images, I also failed to reproduce the same results with other images\r\n\r\nI used 0.01 temperature and 1024 max output token for both results</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-06-03T04:45:33Z>\nHi @jun297, thank you for the interest in our work and for bringing this to our attention.  There was some maintenance in our lab A6000 workstations which we use to host the demo, we switch servers several times this week.   Unfortunately in one of the deployment this Wednesday, there was some issues with the model configuration.  It is now updated and the behavior should be the same with your local one.  Sorry about this confusion and please let me know if there are any other concerns. Thanks.\n</Comment>\n<Comment by jun297 at 2023-06-03T10:45:50Z>\n@haotian-liu Thanks a lot!\r\nNow the web demo has the same results as the web UI, however, I failed to reproduce the same results in CLI. Is it intended?\r\nI used 0.01 temperature and 1024 max token as well and checked prompt but there was no difference.\r\nIs it because of conv-mode?\r\n\r\nIf you let me know whether the results in CLI are the same with the web UI/demo, it would be helpful, thank you in advance\n</Comment>\n<Comment by haotian-liu at 2023-07-30T00:00:57Z>\nHi @jun297 \r\n\r\nCurrently the webui and cli should have the same prompt settings. We've also updated the CLI to support multiturn conversation. You can see the documentation [here](https://github.com/haotian-liu/LLaVA#cli-inference).\n</Comment>\n<Comment by jun297 at 2023-07-30T06:15:12Z>\n@haotian-liu thanks\r\nI forgot to close this. \r\nI think that the difference comes from different conv_mode, resulting different prompt\n</Comment>\n<Comment by euanong at 2023-09-04T23:34:36Z>\nI'm still encountering this issue -- has this been fixed? If not, which checkpoint should I use to run LLaVA from in order to match the results of the demo?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 208,
    "state": "open",
    "created_by": "Akiraxty",
    "created_at": "2023-06-01T11:58:35Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/208</URL>\n\n<TITLE>[Question] Can we customize the visual encoder and llm used? For example, replace llama with opt？</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-23T19:16:53Z>\nYes you can.  We have largely refactored our code base to make it easier to swap out the visual encoder / LLMs. \r\n\r\nCheckout our initial guide in replacing the LLMs: https://github.com/haotian-liu/LLaVA/blob/main/docs/Customize_Component.md\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 207,
    "state": "closed",
    "created_by": "jun297",
    "created_at": "2023-06-01T05:50:45Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/207</URL>\n\n<TITLE>[Question] Undesired output with LLaVA-13b-v1</TITLE>\n\n<BODY>### Question\r\n\r\nHi, I am trying to run LLaVA-13b (especially v1).\r\nI could get the right result with LLaVA-13b-v0 by running the following:\r\n```\r\npython -m llava.eval.run_llava_multigpu \\\r\n    --model-name checkpoints/LLaVA-13B-v0 \\\r\n    --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n    --query \"You are a helpful What are the things I should be cautious about when I visit here?\" \\\r\n    --num-gpus 2\r\n```\r\n\r\nResult:\r\n```\r\nWhen visiting this location, one should be cautious about the weather conditions, especially during the winter season. In the image, the wooden pier is surrounded by snow and ice, which can make it slippery and potentially dangerous. It is important to wear appropriate footwear with good traction to prevent slipping and falling. Additionally, visitors should be mindful of the cold temperatures and dress warmly to protect themselves from the cold weather. Since the pier is located near a mountain lake, it is also crucial to be aware of any sudden changes in weather conditions, such as strong winds or storms, which can make the area more hazardous. Visitors should always pay attention to any safety warnings and follow local guidelines to ensure a safe and enjoyable experience.\r\n```\r\nHowever, I got the undesired result with LLaVA-13b-v1 by running the same command and it takes much more time inference. \r\nI followed the same procedure to get llava-13b-v1 weights with v0 (download delta from [huggingface](https://huggingface.co/liuhaotian/LLaVA-13b-delta-v1-1), and converted weight)\r\n\r\npart of result:\r\n```\r\n\u0018,\\, decla Asp comput Алек hina iphone vba abgerufenński nella Harold ehem$}}%\r\n');\r\n� ‘ имаcomple р Opera +.— jap Central Nav Abd colours\u0002foxFootnote maja porte Kur\",\r\n iphone Obviouslyampio natur PR }\r\n```\r\n\r\nThis is the code I edited from run_llava for multigpu\r\n```\r\nimport argparse\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\nimport torch\r\nimport os\r\nfrom llava.conversation import conv_templates, SeparatorStyle\r\nfrom llava.utils import disable_torch_init\r\nfrom transformers import CLIPVisionModel, CLIPImageProcessor, StoppingCriteria\r\nfrom llava.model import *\r\nfrom llava.model.utils import KeywordsStoppingCriteria\r\n\r\nfrom PIL import Image\r\n\r\nimport os\r\nimport requests\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\n\r\n\r\nDEFAULT_IMAGE_TOKEN = \"<image>\"\r\nDEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\r\nDEFAULT_IM_START_TOKEN = \"<im_start>\"\r\nDEFAULT_IM_END_TOKEN = \"<im_end>\"\r\n\r\n\r\ndef load_image(image_file):\r\n    if image_file.startswith('http') or image_file.startswith('https'):\r\n        response = requests.get(image_file)\r\n        image = Image.open(BytesIO(response.content)).convert('RGB')\r\n    else:\r\n        image = Image.open(image_file).convert('RGB')\r\n    return image\r\n\r\n\r\ndef eval_model(args):\r\n    # Model\r\n    \r\n    if args.num_gpus == 1:\r\n        kwargs = {}\r\n    else:\r\n        kwargs = {\r\n            'device_map': \"auto\",\r\n            \"max_memory\": {i: \"18GiB\" for i in range(args.num_gpus)},\r\n        }\r\n    \r\n    print(\"kwargs: \", kwargs)\r\n    disable_torch_init()\r\n    model_name = os.path.expanduser(args.model_name)\r\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n\r\n    if \"mpt\" in model_name.lower():\r\n        model = LlavaMPTForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True, torch_dtype=torch.float16, use_cache=True, **kwargs)\r\n    else:\r\n        model = LlavaLlamaForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True, torch_dtype=torch.float16, use_cache=True, **kwargs)\r\n    \r\n    if args.num_gpus == 1:\r\n        model.cuda()\r\n    \r\n    image_processor = CLIPImageProcessor.from_pretrained(model.config.mm_vision_tower, torch_dtype=torch.float16)\r\n\r\n    mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\r\n    tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\r\n    if mm_use_im_start_end:\r\n        tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\r\n\r\n    vision_tower = model.get_model().vision_tower[0]\r\n    if vision_tower.device.type == 'meta':\r\n        vision_tower = CLIPVisionModel.from_pretrained(vision_tower.config._name_or_path, torch_dtype=torch.float16, low_cpu_mem_usage=True).cuda()\r\n        model.get_model().vision_tower[0] = vision_tower\r\n    else:\r\n        vision_tower.to(device='cuda', dtype=torch.float16)\r\n    vision_config = vision_tower.config\r\n    vision_config.im_patch_token = tokenizer.convert_tokens_to_ids([DEFAULT_IMAGE_PATCH_TOKEN])[0]\r\n    vision_config.use_im_start_end = mm_use_im_start_end\r\n    if mm_use_im_start_end:\r\n        vision_config.im_start_token, vision_config.im_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN])\r\n    image_token_len = (vision_config.image_size // vision_config.patch_size) ** 2\r\n\r\n    qs = args.query\r\n    if mm_use_im_start_end:\r\n        qs = qs + '\\n' + DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len + DEFAULT_IM_END_TOKEN\r\n    else:\r\n        qs = qs + '\\n' + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len\r\n\r\n    if \"v1\" in model_name.lower():\r\n        conv_mode = \"llava_v1\"\r\n    elif \"mpt\" in model_name.lower():\r\n        conv_mode = \"mpt_multimodal\"\r\n    else:\r\n        conv_mode = \"multimodal\"\r\n\r\n    if args.conv_mode is not None and conv_mode != args.conv_mode:\r\n        print('[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}'.format(conv_mode, args.conv_mode, args.conv_mode))\r\n    else:\r\n        args.conv_mode = conv_mode\r\n\r\n    conv = conv_templates[args.conv_mode].copy()\r\n    conv.append_message(conv.roles[0], qs)\r\n    conv.append_message(conv.roles[1], None)\r\n    prompt = conv.get_prompt()\r\n    inputs = tokenizer([prompt])\r\n\r\n    image = load_image(args.image_file)\r\n    image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\r\n\r\n    input_ids = torch.as_tensor(inputs.input_ids).cuda()\r\n\r\n    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\r\n    keywords = [stop_str]\r\n    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\r\n\r\n    with torch.inference_mode():\r\n        output_ids = model.generate(\r\n            input_ids,\r\n            images=image_tensor.unsqueeze(0).half().cuda(),\r\n            do_sample=True,\r\n            temperature=0.2,\r\n            max_new_tokens=1024,\r\n            stopping_criteria=[stopping_criteria])\r\n\r\n    input_token_len = input_ids.shape[1]\r\n    n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\r\n    if n_diff_input_output > 0:\r\n        print(f'[Warning] {n_diff_input_output} output_ids are not the same as the input_ids')\r\n    outputs = tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]\r\n    outputs = outputs.strip()\r\n    if outputs.endswith(stop_str):\r\n        outputs = outputs[:-len(stop_str)]\r\n    outputs = outputs.strip()\r\n    print(outputs)\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--model-name\", type=str, default=\"facebook/opt-350m\")\r\n    parser.add_argument(\"--image-file\", type=str, required=True)\r\n    parser.add_argument(\"--query\", type=str, required=True)\r\n    parser.add_argument(\"--conv-mode\", type=str, default=None)\r\n    parser.add_argument(\"--num-gpus\", type=int, default=1)\r\n    args = parser.parse_args()\r\n\r\n    eval_model(args)\r\n```</BODY>\n\n<COMMENTS>\n<Comment by jun297 at 2023-06-01T07:26:53Z>\nThere was an error in weight conversion\n</Comment>\n<Comment by AnasHXH at 2023-06-06T06:14:30Z>\nHi jun297\r\n\r\nI am facing the same problem. How do you solve it?\n</Comment>\n<Comment by jun297 at 2023-06-06T13:47:12Z>\n> Hi jun297\r\n> \r\n> I am facing the same problem. How do you solve it?\r\n\r\nHi, as I mentioned above, in my case, I converted wrong weight. check weight path\n</Comment>\n<Comment by AnasHXH at 2023-06-07T04:48:44Z>\nHi Iun297\r\nThank you for your response.\r\nPlease, can you explain how to do weight conversion?\r\nI have delta but I do not have weight, and I follow this :\r\n####################################################\r\nLLaVA Weights\r\nWe release [LLaVA](https://llava-vl.github.io/) weights as delta weights to comply with the LLaMA model license. You can add our delta to the original LLaMA weights to obtain the LLaVA weights.\r\nInstructions:\r\nGet the original LLaMA weights in the huggingface format by following the instructions [here](https://huggingface.co/docs/transformers/main/model_doc/llama).\r\nUse the following scripts to get LLaVA weights by applying our delta ([13b-v0](https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0), [7b-v0](https://huggingface.co/liuhaotian/LLaVA-7b-delta-v0), [lightning-7B-v1-1](https://huggingface.co/liuhaotian/LLaVA-Lightning-7B-delta-v1-1)). It will automatically download delta weights from our Hugging Face account.\r\n######################################################\r\nI can not find the original LLaMA weights.\n</Comment>\n<Comment by jun297 at 2023-06-07T04:53:55Z>\nyou can find LLaMA weight via submitting request form to Meta (takes a long time) or on web\r\nI cannot share the weight or link because of LLaMA policy\n</Comment>\n<Comment by AnasHXH at 2023-06-07T06:02:47Z>\ncan you give me instructions to conversion?\n</Comment>\n<Comment by jun297 at 2023-06-07T06:12:22Z>\nPlease follow the instructions in LLaVA readme\n</Comment>\n<Comment by AnasHXH at 2023-06-07T07:36:44Z>\nHi Jun297\r\nI download 13B weight from Llama\r\n\r\n![Screenshot from 2023-06-07 10-32-52](https://github.com/haotian-liu/LLaVA/assets/65374131/46d3a552-96ff-4dce-b0bc-9be89cc5f0c8)\r\nthen I used this code to convert \r\npython src/transformers/models/llama/convert_llama_weights_to_hf.py \\\r\n    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path\r\n![Screenshot from 2023-06-07 10-34-37](https://github.com/haotian-liu/LLaVA/assets/65374131/7ebc80d4-0e75-430e-ae05-cb9d4acdc3d6)\r\n\r\nThen I tested by\r\n![Screenshot from 2023-06-07 10-35-33](https://github.com/haotian-liu/LLaVA/assets/65374131/ec1784b5-e1f4-4adc-bd28-17151963196c)\r\n\r\nand then I run the conversion code\r\n![Screenshot from 2023-06-07 10-36-35](https://github.com/haotian-liu/LLaVA/assets/65374131/0178cb3c-dd05-4c46-bc8e-94031ac68943)\n</Comment>\n<Comment by jun297 at 2023-06-07T08:18:34Z>\nSorry, I used llama_hf directly, I did not convert llama weight to llama_hf weight.\r\nAll I do was converting lllama_hf to llava with llava_delta\n</Comment>\n<Comment by AnasHXH at 2023-06-08T07:33:07Z>\nokay thanks, I solve my problem.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 206,
    "state": "open",
    "created_by": "Iceland-Leo",
    "created_at": "2023-06-01T04:40:12Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/206</URL>\n\n<TITLE>[Question] Have you finished LoRA/QLoRA training?</TITLE>\n\n<BODY>### Question\n\nNice work!\r\nI just wonder when could you finish LoRA/QLoRA training? It may help a lot to finetune.</BODY>\n\n<COMMENTS>\n<Comment by Iceland-Leo at 2023-06-03T04:41:05Z>\nDoes anyone complete the lora/qlora training on llava?\n</Comment>\n<Comment by haotian-liu at 2023-06-03T04:47:35Z>\nHI, thank you for your interest in our work.  We are implementing the LoRA/QLoRA training and we are now verifying the correctness with training some model checkpoints.  Will update soon if the performance is verified.  Thanks!\n</Comment>\n<Comment by XipengY at 2023-06-07T04:40:04Z>\n@haotian-liu Looking forward to the implementation of LoRA/QLoRA training, could you put the code in the develop branch, and we can try to develop and verify it together.\n</Comment>\n<Comment by haotian-liu at 2023-06-11T23:15:18Z>\nHi @XipengY @Iceland-Leo \r\n\r\nLoRA support (preview) and an initial checkpoint I trained is released [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md).  Please let me know if there is something unclear in the instruction.\r\n\r\nAlso, if any of you are interested in contributing to the hyperparameter search, please let me know.\r\n\r\nNote that QLoRA support is partially finished, but distributed training is not supported yet.\n</Comment>\n<Comment by devrituraj at 2023-06-13T17:33:27Z>\nHi...\r\n\r\nI find that LoRA Fine-tuning is slower (almost half - 5.72 s/it) as compared to full training (2.77 s/it). Same LR - 2e-5 and Batch Size -4, Grad-accum- 1 was considered for both the setups. Script was run using torchrun with deepspeed config(zero3.json) as an argument. Are there any similar observations? Intuitively, LoRA should be faster!\r\n\r\nbtw, wonderful work!\n</Comment>\n<Comment by haotian-liu at 2023-06-13T17:36:56Z>\nHi @devrituraj \r\n\r\nThank you for trying this out and for providing the feedback.  Which GPUs (and how many) are you using?  And can you provide your commands?  And do you notice a reduction in the GPU memory consumption?\r\n\r\nFor smaller models I noticed a smaller performance benefit.  It would be better to know more about the specific configurations on your side.  Thanks!\n</Comment>\n<Comment by aprilehannibal at 2023-06-15T06:28:48Z>\n@haotian-liu How many hours do you spend when finetune 13B with lora?\n</Comment>\n<Comment by XipengY at 2023-06-16T07:47:20Z>\n@haotian-liu ，It's great work support deepspeed and lora, significantly reduces GPU memory, but I changed tf32 and bf16 to False, it also fails to train, do you have the same problem?\n</Comment>\n<Comment by YerongLi at 2023-07-21T09:57:12Z>\n> Does anyone complete the lora/qlora training on llava?\r\n\r\nI think lora works now, but quantization does not work for me, does anyone complete the QLora training?\r\n\r\nquantization reports this error:\r\n```\r\n  File \"/scratch/yerong/LLaVA/llava/train/train.py\", line 657, in train                                                                              [276/971]\r\n    model = LlavaLlamaForCausalLM.from_pretrained(                                                                                                            \r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2694, in from_pretrained                         \r\n    dispatch_model(model, device_map=device_map, offload_dir=offload_folder, offload_index=offload_index)                                                     \r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/accelerate/big_modeling.py\", line 371, in dispatch_model                               \r\n    attach_align_device_hook_on_blocks(                                                                                                                       \r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 506, in attach_align_device_hook_on_blocks                  \r\n    add_hook_to_module(module, hook)                                                                                                                          \r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 155, in add_hook_to_module                                  \r\n    module = hook.init_hook(module)                                                                                                                           \r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 253, in init_hook                                           \r\n    set_module_tensor_to_device(module, name, self.execution_device)                                                                                          \r\n  File \"/scratch/yerong/.conda/envs/llava/lib/python3.10/site-packages/accelerate/utils/modeling.py\", line 267, in set_module_tensor_to_device                \r\n    raise ValueError(f\"{tensor_name} is on the meta device, we need a `value` to put in on {device}.\")                                                        \r\nValueError: weight is on the meta device, we need a `value` to put in on cuda:0.   \r\n```\n</Comment>\n<Comment by haotian-liu at 2023-07-24T00:23:07Z>\n@aprilehannibal I do not see too much of a difference between LoRA and full-finetuning, both around 1.5-2 hours on finetune lightning.\n</Comment>\n<Comment by haotian-liu at 2023-07-24T05:46:16Z>\n@YerongLi \r\n\r\nI have updated the QLora support. Please pull the latest repo and reinstall the packages using `pip install -e .`\r\n\r\nSee example script [here](https://github.com/haotian-liu/LLaVA/blob/main/scripts/finetune_qlora.sh).\n</Comment>\n<Comment by lilia147852 at 2024-06-18T13:18:00Z>\n@haotian-liu\r\nexcuse me can i  implement QLoRA for LLAVA v1.5 ?\n</Comment>\n<Comment by YongliangMiao at 2024-09-07T07:00:51Z>\n> @haotian-liu 对不起，我可以为 LLAVA v1.5 实现 QLoRA 吗？\r\n\r\nHi，您后续有完成 LLAVA v1.5 的QLoRA 微调吗？我发现Deepseed zero 3 与 qlora有点冲突，[我发了一个issue在这里](https://github.com/haotian-liu/LLaVA/issues/1692)。\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 205,
    "state": "closed",
    "created_by": "Coolshanlan",
    "created_at": "2023-05-31T17:06:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/205</URL>\n\n<TITLE>[Usage] Cannot find the \"pytorch_model.bin.index.json\" file when executing extract_mm_projector.</TITLE>\n\n<BODY>### When did you clone our code?\r\n\r\nI cloned the code base after 5/1/23\r\n\r\n### Describe the issue\r\n\r\nThanks for your nice work!\r\nI modified the GPT Neo model to a LLaVa LLM model and completed the pre-training. Then I executed the `extract_mm_projector.py` script to extract the mm_projector weights, but an error occurred\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/coolshan/Project/LLaVA/scripts/extract_mm_projector.py\", line\r\n 18, in <module>\r\n    model_indices = json.load(open(os.path.join(args.model_name_or_path, 'pytorch_model.bin.index.json')))\r\nFileNotFoundError: [Errno 2] No such file or directory: './checkpoints/llava-lightning-gpt-neo-125m-p\r\nretrain/pytorch_model.bin.index.json'\r\n```\r\nI can't find `pytorch_model.bin.index.json` file in `checkpoints folder`\r\n\r\nThis is the command I executed in pretraining:\r\n```\r\n#Pretraining (2 hours)\r\npython llava/train/train_mem.py\\\r\n    --model_name_or_path EleutherAI/gpt-neo-125m \\\r\n    --version v1 \\\r\n    --data_path training_data/cc3m_595k.json \\\r\n    --image_folder training_data/cc3m_595k \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-lightning-gpt-neo-125m-pretrain \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 2400 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb \\\r\n```\r\nand this is my checkpoint folder \r\n\r\n\r\n<img width=\"214\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/40363835/72668cb3-c788-4dbe-84b7-1cb122e2d212\">\r\n\r\nHow do I find `pytorch_model.bin.index.json` file or how do I save the model to save this file at the same time.\r\n\r\nThanks</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-31T18:05:41Z>\nHi @Coolshanlan \r\n\r\nThanks for bringing this to our attention.  This is due to some smaller models are saved to a single file, thus it does not need a index json.\r\n\r\nCan you try the [updated script](https://github.com/haotian-liu/LLaVA/blob/main/scripts/extract_mm_projector.py)?\r\n\r\nPlease let me know if there are other issues, thanks!\n</Comment>\n<Comment by Coolshanlan at 2023-05-31T18:33:18Z>\nThank you so much for your quick reply and modify, it worked perfectly!!\n</Comment>\n<Comment by harrytea at 2023-08-27T04:44:07Z>\n> Thank you so much for your quick reply and modify, it worked perfectly!!\r\n\r\nI have the same question, but the updated script is missing, can you share me here?\n</Comment>\n<Comment by Coolshanlan at 2023-08-27T05:46:54Z>\n> > Thank you so much for your quick reply and modify, it worked perfectly!!\r\n> \r\n> I have the same question, but the updated script is missing, can you share me here?\r\n\r\n```\r\nimport os\r\nimport argparse\r\nimport torch\r\nimport json\r\nfrom collections import defaultdict\r\n\r\n\r\ndef parse_args():\r\n    parser = argparse.ArgumentParser(description='Extract MMProjector weights')\r\n    parser.add_argument('--model_name_or_path', type=str, help='model folder')\r\n    parser.add_argument('--output', type=str, help='output file')\r\n    args = parser.parse_args()\r\n    return args\r\n\r\n\r\nif __name__ == '__main__':\r\n    args = parse_args()\r\n\r\n    keys_to_match = ['mm_projector', 'embed_tokens', 'transformer.wte']\r\n    ckpt_to_key = defaultdict(list)\r\n    try:\r\n        model_indices = json.load(open(os.path.join(args.model_name_or_path, 'pytorch_model.bin.index.json')))\r\n        for k, v in model_indices['weight_map'].items():\r\n            if any(key_match in k for key_match in keys_to_match):\r\n                ckpt_to_key[v].append(k)\r\n    except FileNotFoundError:\r\n        # Smaller models or model checkpoints saved by DeepSpeed.\r\n        v = 'pytorch_model.bin'\r\n        for k in torch.load(os.path.join(args.model_name_or_path, v), map_location='cpu').keys():\r\n            if any(key_match in k for key_match in keys_to_match):\r\n                ckpt_to_key[v].append(k)\r\n\r\n    loaded_weights = {}\r\n\r\n    for ckpt_name, weight_keys in ckpt_to_key.items():\r\n        ckpt = torch.load(os.path.join(args.model_name_or_path, ckpt_name), map_location='cpu')\r\n        for k in weight_keys:\r\n            loaded_weights[k] = ckpt[k]\r\n\r\n    torch.save(loaded_weights, args.output)\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 204,
    "state": "open",
    "created_by": "jcrsantiago",
    "created_at": "2023-05-31T16:34:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/204</URL>\n\n<TITLE>[Feature request]</TITLE>\n\n<BODY>### feature\n\nThe installation process is long and difficult. Could you create a setup.py file to run the necessary steps?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-31T16:38:03Z>\nHi @jcrsantiago thank you for your interest in our project.\r\n\r\nExcept for creating conda environment, the current installation for inferencing on pretrained models is running: `pip install -e .`\r\nAnd installing two more packages if you want to train the model.\r\n\r\nCould you please clarify with more detail on which steps you are specifically seeking to improve?  Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 203,
    "state": "closed",
    "created_by": "penghe2021",
    "created_at": "2023-05-30T20:56:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/203</URL>\n\n<TITLE>[Question] Question about the replace_token</TITLE>\n\n<BODY>### Question\n\nIn the model_worker code, it will replace the 'image_patch' token with the line:\r\nhttps://github.com/haotian-liu/LLaVA/blob/8b21169def6c8ed57afa5e7bf790c1a91b530940/llava/serve/model_worker.py#L218\r\nMy question is: is this line only for single image input, or we need to do it for every image patch token when we try multi image input.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-31T16:41:57Z>\nHi @penghe2021,\r\nThank you for your interest in our work.\r\n\r\nThe `DEFAULT_IMAGE_PATCH_TOKEN` basically creates a placeholder for the tokenizer to prepare for the space for us to put in image features extracted from visual encoder.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/llava/model/llava.py#L151\r\n\r\nThe current (expected) behavior is that if you want to query about two images, for example:\r\n```\r\nHuman: Describe image A: <image>\r\nAssistant: <response>\r\nHuman: Image B: <image>\r\nAssistant: <response>\r\n```\r\nThe model has the access to the first image as well, we will replace each `<image>` with `'<IM_PATCH_TOKEN>'*<patch_num_per_image>`.\r\n\r\nPlease let me know if there is anything unclear, thanks.\n</Comment>\n<Comment by penghe2021 at 2023-06-01T23:31:13Z>\nSorry I missed the notification. Thanks, that's very clear, previously I thought the number 256 means the max number of images in the entire conversation.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 202,
    "state": "closed",
    "created_by": "harrietfiagbor",
    "created_at": "2023-05-30T18:12:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/202</URL>\n\n<TITLE>Llava-mpt weights</TITLE>\n\n<BODY>### Question\n\nHello, thanks for your amazing work. Wanted to know if the Llava-mpt supports finetuning?\r\n\r\nAlso, there is a new quantization called qlora that I want to implement on the model but it seems the transformer library you have here is not up to date to do that. Can it be upgraded to incorporate the new changes?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-30T18:16:19Z>\nHi @harrietfiagbor \r\n\r\nThank you for your interest in our work.\r\n\r\nLLaVA-MPT supports finetuning, and you can follow the scripts here: https://github.com/haotian-liu/LLaVA/blob/main/scripts/train_lightning_mpt.sh\r\n\r\nRegarding QLora, I am also interested in this work and I have actually been trying their code base in the past weekend.  I am currently working on integrating the basic version (LoRA) in the code base, and I'll investigate the `transformer` compatibility issue later today or tomorrow. I am aware of at least one compatibility issue that needs to be resolved to upgrade the package.  I'll send you an update by tomorrow :)\n</Comment>\n<Comment by harrietfiagbor at 2023-05-31T12:41:06Z>\nHey @haotian-liu \r\n\r\nThanks so much for your quick reply. Really appreciated. \r\nTo get my question out clearly, can the preview version be finetuned straight away? Because I see in the script that there are some projector features needed. I assumed since this is your checkpoint, I won't need any projector features and use the finetuning part of the script directly. I don't know if it is the right thing to do\n</Comment>\n<Comment by haotian-liu at 2023-05-31T18:21:39Z>\nDo you mean that you want to start with my released checkpoint, and further finetune to some other datasets?  In this case, you can simply run the following command (change base checkpoint to the released checkpoint, and remove `--pretrain_mm_mlp_adapter`).\r\n\r\n```\r\ntorchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path liuhaotian/LLaVA-Lightning-MPT-7B-preview \\\r\n    --version v1 \\\r\n    --data_path /path/to/llava_instruct_80k.json \\\r\n    --image_folder /Data/haotian/coco/train2014 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 5000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --fsdp \"full_shard auto_wrap\" \\\r\n    --fsdp_transformer_layer_cls_to_wrap 'MPTBlock' \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\n</Comment>\n<Comment by harrietfiagbor at 2023-06-01T10:10:15Z>\nThank you so much @haotian-liu. This was really helpful\n</Comment>\n<Comment by kahnchana at 2023-06-08T20:29:04Z>\nDid you get a `WARNING: tokenization mismatch: 450 vs. 451. (ignored)` message when doing this?\n</Comment>\n<Comment by harrietfiagbor at 2023-07-04T11:56:16Z>\n@kahnchana Yes , I did get such a warning which led me into getting this error, \r\n`TypeError: LlavaMPTForCausalLM.forward() got an unexpected keyword argument 'inputs_embeds'`\r\nDid you encounter anything like this?\n</Comment>\n<Comment by findalexli at 2023-07-11T04:33:48Z>\n@harrietfiagbor @kahnchana  Were you able to resolve the issue? Can I ask you how?\n</Comment>\n<Comment by harrietfiagbor at 2023-07-13T09:58:12Z>\nSo I realised `inputs_embeds` have been initialised and used but for some reason not being passed to the forward. You can add `inputs_embeds: Optional[torch.FloatTensor] = None` just like from the llava.py script.  Insert it into the  forward function of  the LlavaMPTforCausalLLM class like this\r\n\r\n`def forward(self, \r\n                input_ids: torch.LongTensor, \r\n                past_key_values: Optional[List[Tuple[torch.FloatTensor]]]=None,\r\n                inputs_embeds: Optional[torch.FloatTensor] = None,\r\n                attention_mask: Optional[torch.ByteTensor]=None, \r\n                prefix_mask: Optional[torch.ByteTensor]=None, \r\n                sequence_id: Optional[torch.LongTensor]=None, \r\n                labels: Optional[torch.LongTensor]=None, \r\n                return_dict: Optional[bool]=None, \r\n                output_attentions: Optional[bool]=None, \r\n                output_hidden_states: Optional[bool]=None, \r\n                use_cache: Optional[bool]=None, images=None):`\n</Comment>\n<Comment by haotian-liu at 2023-08-02T05:24:55Z>\nApologies for the confusion. For MPT models, there was some compatibility issue, which has been resolved in the latest code base.  Please check out the latest code base and MPT checkpoints should work :)\n</Comment>\n<Comment by haotian-liu at 2023-10-16T20:07:46Z>\nClosing for now. Please feel free to re-open or open a new issue, if there are any other questions. Thank you!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 201,
    "state": "closed",
    "created_by": "Kindpire",
    "created_at": "2023-05-30T15:22:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/201</URL>\n\n<TITLE>[Usage] ValueError: The number of image start tokens and image end tokens should be the same.</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\nValueError: The number of image start tokens and image end tokens should be the same.\r\nCommand:\r\n```\r\nCUDA_VISIBLE_DEVICES='2,3,4,5,6,7' torchrun --nnodes=1 --nproc_per_node=6 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path /usr/dataset/model/vicuna-7b \\\r\n    --data_path /usr/dataset/llava_instruct_150k_fix_prefix.json \\\r\n    --image_folder /usr/dataset/COCO2014/train2014/train2014 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/mm_projector/llava-7b-pretrain.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 4 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 5000 \\\r\n    --save_total_limit 3 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --fsdp \"full_shard auto_wrap\" \\\r\n    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\r\n    --model_max_length 256 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\nHi, I was trying to fine-tune LLaVA on 6x3090, when I set model_max_length>=512, I'll get the error like:\r\nRuntimeError: Expected is_sm80 || is_sm90 to be true, but got false.\r\nBut when I get the model_max_length <= 256. I got the error like:\r\nValueError: The number of image start tokens and image end tokens should be the same.\r\n\r\nAny insights?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-30T18:18:32Z>\nHi @Kindpire,\r\n\r\nThank you for your interest in our work.  I am working on the LoRA integration recently and I am testing the performance currently.  I should be able to release a version soon.\r\n\r\nRegarding the errors.  The first is due to that flash attention is not supported on RTX 3090 yet, and you can change `train_mem.py` to `train.py` to avoid that issue.\r\n`model_max_length` should be at least 300 in order to fit all image tokens in (there are 256 of them), or the image will be cut off and that's the reason why you see the second error.\r\n\r\nPlease let me know if you have other questions, thanks.\n</Comment>\n<Comment by Kindpire at 2023-05-31T01:39:51Z>\nThanks, it works. The only remaining issue is GPU memory now.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 200,
    "state": "open",
    "created_by": "hxhcreate",
    "created_at": "2023-05-29T11:10:28Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/200</URL>\n\n<TITLE>[Question] Pip Install Env Error</TITLE>\n\n<BODY>### Question\n\nwhen running the folloing command to install \r\n\r\n`pip install -e .`\r\n\r\nit seems to have an error in  dependencies `transformers @ git+https://github.com/huggingface/transformers.git@cae78c46`\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/71424268/60c82f0c-3bcf-469a-9120-a6d296db404c)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-29T17:21:21Z>\nIt seems that you are having connection issue with cloning the GitHub repo.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/6631389/812c7b77-16d1-4ffc-a0b9-d191d92cd97e)\r\n\r\nYou may need to check your internet connection with GitHub cloning or the proxy used. Thanks.\n</Comment>\n<Comment by hexixiang at 2023-06-01T07:16:32Z>\nme the same issues，so how do you solve this？\n</Comment>\n<Comment by hexixiang at 2023-06-01T07:16:50Z>\n> ### Question\r\n> \r\n> when running the folloing command to install\r\n> \r\n> `pip install -e .`\r\n> \r\n> it seems to have an error in dependencies `transformers @ git+https://github.com/huggingface/transformers.git@cae78c46`\r\n> \r\n> ![image](https://user-images.githubusercontent.com/71424268/241718233-60c82f0c-3bcf-469a-9120-a6d296db404c.png)\r\n\r\nhow to solve？\n</Comment>\n<Comment by haotian-liu at 2023-06-03T04:54:52Z>\n@hexixiang @hxhcreate  What happens if you manually clone the transformers repo and checkpoint the commit to cae78c46?  Basically do the following:\r\n\r\n```Shell\r\ngit clone https://github.com/huggingface/transformers.git\r\ncd transformers\r\ngit checkout cae78c46\r\npip install -e .\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 199,
    "state": "open",
    "created_by": "mary-0830",
    "created_at": "2023-05-29T01:16:12Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/199</URL>\n\n<TITLE>[Question]  Why execute eval ScienceQA When sciqa train dataset is used, different scores will be obtained?</TITLE>\n\n<BODY>### Question\r\n\r\nHello, I have two questions:\r\n**1. I used the same jsonl results, but the scores evaluated were different, the results are shown below.**\r\n`\r\n2023-05-29 09:07:24.546966: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-05-29 09:07:24.594253: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-05-29 09:07:25.299057: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\nTotal: 4241, Correct: 1776, Accuracy: 41.88%\r\n\r\n2023-05-29 09:07:33.273008: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-05-29 09:07:33.320623: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-05-29 09:07:34.029075: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\nTotal: 4241, Correct: 1732, Accuracy: 40.84%\r\n\r\n2023-05-29 09:07:41.954175: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-05-29 09:07:42.000912: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-05-29 09:07:42.703589: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\nTotal: 4241, Correct: 1690, Accuracy: 39.85%\r\n\r\n2023-05-29 09:07:50.292305: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-05-29 09:07:50.340043: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-05-29 09:07:51.039918: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\nTotal: 4241, Correct: 1716, Accuracy: 40.46%`\r\n\r\n**2. Why did I use scienceQA to add to the training for finetune, but the evaluation score still did not reach 90%?**\r\n\r\nLooking forward to your prompt reply, thank you~</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-29T02:16:30Z>\nHi @mary-0830 \r\n\r\nOne other user found that re-download the correct checkpoints resolve the similar issue in #104.\r\n\r\nCan you make sure that: (1) you downloaded the correct ScienceQA delta; (2) you [applied the delta weights](https://github.com/haotian-liu/LLaVA#llava-13b) to get the correct model weights; (3) the base model weights during the conversion mentioned in step (2) is LLaMA instead of Vicuna.\r\n\r\nThanks.\n</Comment>\n<Comment by mary-0830 at 2023-05-29T02:42:12Z>\n> Hi @mary-0830\r\n> \r\n> One other user found that re-download the correct checkpoints resolve the similar issue in #104.\r\n> \r\n> Can you make sure that: (1) you downloaded the correct ScienceQA delta; (2) you [applied the delta weights](https://github.com/haotian-liu/LLaVA#llava-13b) to get the correct model weights; (3) the base model weights during the conversion mentioned in step (2) is LLaMA instead of Vicuna.\r\n> \r\n> Thanks.\r\n\r\nThank you for your reply! Perhaps I didn't make it clear, both of these issues were based on our own fine-tuning of the situation. Firstly, when we use our own fine-tuning model for evaluation, such situations may occur; Secondly, after adding the training set of Sciqa, the trained model does not seem to be as high as 90%.\n</Comment>\n<Comment by haotian-liu at 2023-05-29T04:31:31Z>\nHi @mary-0830\r\n\r\nAs stated in the paper, the results (90.9%) are obtained after (1) feature alignment of the first stage; (2) finetuning on ScienceQA dataset.\r\n\r\nWould you share more details about your training steps, and in which stage do you obtain the accuracy mentioned above?  also, what's your accuracy \"after adding the training set of Sciqa\"?  Thanks.\n</Comment>\n<Comment by mary-0830 at 2023-05-29T05:16:37Z>\n> Hi @mary-0830\r\n> \r\n> As stated in the paper, the results (90.9%) are obtained after (1) feature alignment of the first stage; (2) finetuning on ScienceQA dataset.\r\n> \r\n> Would you share more details about your training steps, and in which stage do you obtain the accuracy mentioned above? also, what's your accuracy \"after adding the training set of Sciqa\"? Thanks.\r\n\r\nThank you for your reply. I have another question, why the results of test_sqa_llava_13b_v0.json and the answer of  llava_test_QCM-LEPA.json  the same?  Is it because the model completely predicts?\r\n\r\nI'm not very familiar with the training details, but I know that I should have included all the training set data from scienceQA.\n</Comment>\n<Comment by haotian-liu at 2023-05-29T05:40:38Z>\n@mary-0830 They are not the same (as it still fails on some questions), but they can be visually very similar -- the main reason is that the ScienceQA has the \"lecture\" section that we use for CoT prediction, and the lectures are the same for questions of the same type.  This means that when the model correctly identifies the knowledge that the question requires, it will retrieve the lecture that it has seen during training, which will also be included in the answer template in the GT answer.  You'll notice difference in the final answer, and the actual problem solving part.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 198,
    "state": "closed",
    "created_by": "jun297",
    "created_at": "2023-05-28T08:18:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/198</URL>\n\n<TITLE>[Question] Five sentences about an image for GPT4 prompting</TITLE>\n\n<BODY>### Question\n\n![image](https://github.com/haotian-liu/LLaVA/assets/25261314/4e8765c3-7f81-4123-bea8-77ef22a0f081)\r\nLike the prompt in table 10, how can I get the five sentences from an image?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-29T02:17:08Z>\nHi, the five sentences from the image are obtained from COCO 2014 Captions annotation, which you can download [here](https://cocodataset.org/#download), thanks!\n</Comment>\n<Comment by jun297 at 2023-05-29T05:41:58Z>\nThanks a lot!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 197,
    "state": "open",
    "created_by": "jsg921019",
    "created_at": "2023-05-26T04:01:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/197</URL>\n\n<TITLE>[Question] Possibility for Multi-image input?</TITLE>\n\n<BODY>### Question\n\nI really enjoyed reading this paper and I have played with it few days and came up to this question:\r\nLLava Architecture is capable of having more than one image input. I tried give two images as an input, but the inference result was not good. That is probably the model was not trained with multiple image input. Have you tried training LLaVA with dataset that has more than one input??</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-29T02:19:11Z>\nHi @jsg921019 \r\n\r\nThank you for your interest in our work.  Due to the current way of training, we do not observe the model having very good capability referring to / comparing with multiple images. We are working on improving this aspect as well, stay tuned!  You can also checkout some of the discussions [here](https://github.com/haotian-liu/LLaVA/issues/57#issuecomment-1528778986).\n</Comment>\n<Comment by codybum at 2023-05-29T13:50:56Z>\nThis is also of great interest to our group as well.  We work with pathology images and it can take more than one image to describe a region of interest due to image size.  In this case we don't need to compare images, but allow several images to represent one thing.  This would be similar in concept to MIL modeling (https://github.com/Project-MONAI/tutorials/tree/main/pathology/multiple_instance_learning).\n</Comment>\n<Comment by haotian-liu at 2023-05-31T17:49:45Z>\n@codybum Thanks for explaining this interesting direction!  I am curious about if there is any plan in your group in working in this direction?  Would be happy to integrate that into LLaVA :)\n</Comment>\n<Comment by sskorol at 2023-10-19T12:14:46Z>\nI am also looking forward to seeing this feature soon. For instance, GPT-4V can take several images and find relationships between objects on different images. It's pretty cool and has a variety of use cases.\n</Comment>\n<Comment by shure-dev at 2023-11-05T01:14:07Z>\nI have a strong interest in this topic\r\nI want to know if this is possible or not for the current version by customizing the code\n</Comment>\n<Comment by unnikrishnanrnair at 2023-11-17T09:37:32Z>\n@haotian-liu In my understanding GPT4v slices higher resolution images into 512x512 images plus one context image and then tokenizes + collates those tokens. Have you tried something like this with the latest LLaVA model by any chance? Is it something worth trying? My use case is simlar to @codybum 's where I need to pass in higher resolution images.\n</Comment>\n<Comment by shure-dev at 2024-04-24T02:43:55Z>\nhttps://tiger-ai-lab.github.io/Blog/mantis\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 196,
    "state": "open",
    "created_by": "jaydeep-work",
    "created_at": "2023-05-25T11:23:29Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/196</URL>\n\n<TITLE>ValueError: MPTForCausalLM only supports tied word embeddings. [Question]</TITLE>\n\n<BODY>### Question\r\n\r\n### **ValueError: MPTForCausalLM only supports tied word embeddings**\r\n\r\nhere is the terminal error...\r\n\r\n```\r\n(env) root@4acd6379ec91:/workspace/reverse_prompting/LLaVA# torchrun --nnodes=1 --nproc_per_node=1 --master_port=25001     llava/train/train_mem.py     --model_name_or_path /workspace/reverse_prompting/LLaVA/LLaVA-7B-v0     --data_path /workspace/reverse_prompting/LLaVA/small_data/chat.json     --image_folder /workspace/reverse_prompting/LLaVA/small_data/images     --vision_tower openai/clip-vit-large-patch14     --tune_mm_mlp_adapter True     --mm_vision_select_layer -2     --bf16 True     --output_dir /workspace/reverse_prompting/LLaVA/checkpoints/llava-13b-pretrain-no_im_start_end_token     --num_train_epochs 1     --per_device_train_batch_size 16     --per_device_eval_batch_size 4     --gradient_accumulation_steps 1     --evaluation_strategy \"no\"     --save_strategy \"steps\"     --save_steps 2400     --save_total_limit 1     --learning_rate 2e-3     --weight_decay 0.     --warmup_ratio 0.03     --lr_scheduler_type \"cosine\"     --logging_steps 1     --tf32 True     --model_max_length 2048     --gradient_checkpointing True     --lazy_preprocess True     --report_to wandb\r\nYou are using a model of type llava to instantiate a model of type llava_mpt. This is not supported for all configurations of models and can yield errors.\r\nTraceback (most recent call last):\r\n  File \"/workspace/reverse_prompting/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/workspace/reverse_prompting/LLaVA/llava/train/train.py\", line 545, in train\r\n    model = LlavaMPTForCausalLM.from_pretrained(\r\n  File \"/workspace/reverse_prompting/env/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2493, in from_pretrained\r\n    model = cls(config, *model_args, **model_kwargs)\r\n  File \"/workspace/reverse_prompting/LLaVA/llava/model/llava_mpt.py\", line 182, in __init__\r\n    raise ValueError('MPTForCausalLM only supports tied word embeddings')\r\nValueError: MPTForCausalLM only supports tied word embeddings\r\nERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2050) of binary: /workspace/reverse_prompting/env/bin/python\r\nTraceback (most recent call last):\r\n  File \"/workspace/reverse_prompting/env/bin/torchrun\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/workspace/reverse_prompting/env/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/workspace/reverse_prompting/env/lib/python3.10/site-packages/torch/distributed/run.py\", line 794, in main\r\n    run(args)\r\n  File \"/workspace/reverse_prompting/env/lib/python3.10/site-packages/torch/distributed/run.py\", line 785, in run\r\n    elastic_launch(\r\n  File \"/workspace/reverse_prompting/env/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n  File \"/workspace/reverse_prompting/env/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \r\n============================================================\r\nllava/train/train_mem.py FAILED\r\n------------------------------------------------------------\r\nFailures:\r\n  <NO_OTHER_FAILURES>\r\n------------------------------------------------------------\r\nRoot Cause (first observed failure):\r\n[0]:\r\n  time      : 2023-05-25_11:18:26\r\n  host      : 4acd6379ec91\r\n  rank      : 0 (local_rank: 0)\r\n  exitcode  : 1 (pid: 2050)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n============================================================\r\n\r\n```\r\n\r\nTransformer version i'm using\r\n`transformers @ git+https://github.com/huggingface/transformers.git@cae78c46d658a8e496a815c2ee49b9b178fb9c9a`\r\n\r\n\r\n```\r\nnvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2022 NVIDIA Corporation\r\nBuilt on Wed_Jun__8_16:49:14_PDT_2022\r\nCuda compilation tools, release 11.7, V11.7.99\r\nBuild cuda_11.7.r11.7/compiler.31442593_0\r\n```\r\n\r\n```\r\nnvidia-smi\r\nThu May 25 11:23:04 2023       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 525.85.05    Driver Version: 525.85.05    CUDA Version: 12.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  NVIDIA A100 80G...  Off  | 00000000:05:00.0 Off |                    0 |\r\n| N/A   49C    P0    71W / 300W |      0MiB / 81920MiB |     21%      Default |\r\n|                               |                      |             Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-25T18:48:05Z>\nHmm, it seems that `mpt` in /workspace/reverse_pro**mpt**ing/LLaVA/LLaVA-7B-v0 makes the code think that this is a LLaVA-MPT model: https://github.com/haotian-liu/LLaVA/blob/main/llava/train/train.py#L544.\r\n\r\nCurrently, you may temporarily change the name of the folder, or you can force the model not to load as a LLaVA-MPT model.\r\n\r\nI will push a fix to these naming issues later this month, thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 195,
    "state": "closed",
    "created_by": "xianghuisun",
    "created_at": "2023-05-25T10:37:39Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/195</URL>\n\n<TITLE>Discussion about training in Stage 1 [Discussion]</TITLE>\n\n<BODY>### Discussion\n\nI have a question about training. Specifically, In stage 1: Pre-training for Feature Alignment. The statement in the paper is: \r\n```\r\nIn training, we keep both the visual encoder and LLM weights frozen, and maximize the likelihood of (3) with trainable parameters θ = W (the projection matrix) only.\r\n```\r\nBut there is an additional update to the embed_tokens of LLM in the code.\r\n```\r\n            if tune_mm_mlp_adapter:\r\n                self.get_model().orig_embeds_params = [self.get_input_embeddings().weight.data.clone().to(device=device)]\r\n                for p in self.get_input_embeddings().parameters():\r\n                    p.requires_grad = True\r\n```\r\nIf we do not update the embed_tokens of LLM, does the performance will decreased significantly?</BODY>\n\n<COMMENTS>\n<Comment by wangjiongw at 2023-05-25T13:37:23Z>\nI met the same problem. Since the projector is pretrained in stage 1 while ViT & LLM freeze, how can I test the performance of naive mm projector ?\n</Comment>\n<Comment by haotian-liu at 2023-07-23T19:03:33Z>\nHi, we find that removing these additional tokens does not affect the performance, and we thus remove these tokens in our latest release. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 194,
    "state": "open",
    "created_by": "aprilehannibal",
    "created_at": "2023-05-25T08:08:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/194</URL>\n\n<TITLE>[Usage] Using Deepspeed pretrainning errors</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue: When I use deepspeed zero3 to pretrainning LLaVA-13B on 4 * A100（40G），I got an error shows below. It seems like  when model parallelism, the clip parameters changed. When I use zero2, pretraining stage can sucessfully run. Because I need to pretrain 13B on a smaller GPU like 16 * A10(24G), I must to use zero3.  @haotian-liu \r\n\r\n<img width=\"377\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/14832463/d725ec5e-203c-4721-9a30-10442ec99a9b\">\r\n\r\n\r\nCommand:\r\n```\r\ntorchrun --nnodes=1 --nproc_per_node=4 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path path/to/llava_13b \\\r\n    --data_path /path/to/LLaVA/LLaVA-CC3M-Pretrain-595K/chat.json \\\r\n    --image_folder /path/to/LLaVA-CC3M-Pretrain-595K/cc3m_595k_images \\\r\n    --vision_tower ./openai/clip-vit-large-patch14 \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end \\\r\n    --output_dir ./checkpoints/llava-13b-pretrain-deepspeed3 \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 32 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 2400 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --bf16 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --deepspeed ds_config_stage3.json\r\n```\r\n\r\n\r\nScreenshots:\r\n<img width=\"758\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/14832463/a85f60d5-78b7-45ff-bc4d-8c506168b41b\"></BODY>\n\n<COMMENTS>\n<Comment by Tomato1101 at 2023-05-30T03:54:07Z>\nI encountered the same error as you did when using deepspeed zero3. I have traced the underlying issue to `torch.nn.Conv2d` returning an empty tensor when initializing the vision model. Interestingly, `torch.nn.Conv2d` works well when generating a convolutional layer and returns a tensor with the right shape.\r\n\r\nYou can observe `self.patch_embedding` weights in `your_path/transformers/models/clip/modeling_clip.py` to potentially identify the error.\r\n\r\npython==3.10\r\ntorch==1.13.1\r\ntransformers==4.28.0.dev0\r\n\r\n<img width=\"483\" alt=\"0530120351\" src=\"https://github.com/haotian-liu/LLaVA/assets/13324641/1438c3d4-e23d-4cdf-84da-0a5c93309bf9\">\n</Comment>\n<Comment by haotian-liu at 2023-05-30T04:06:21Z>\n@Tomato1101 Can you share your deepspeed.json and your command as well?  I'll try investigating this issue this week so would like to gather some sample scripts.  Thanks.\n</Comment>\n<Comment by Tomato1101 at 2023-05-30T04:22:37Z>\nI run the code in RTX 3090. This is my deepspeed configuration, which references the official tutorial.\r\n```\r\n{\r\n    \"train_micro_batch_size_per_gpu\": \"auto\",\r\n    \"zero_optimization\": {\r\n        \"stage\": 3,\r\n        \"overlap_comm\": true,\r\n        \"contiguous_gradients\": true,\r\n        \"sub_group_size\": 1e9,\r\n        \"reduce_bucket_size\": \"auto\",\r\n        \"stage3_prefetch_bucket_size\": \"auto\",\r\n        \"stage3_param_persistence_threshold\": \"auto\",\r\n        \"stage3_max_live_parameters\": 1e9,\r\n        \"stage3_max_reuse_distance\": 1e9,\r\n        \"stage3_gather_16bit_weights_on_model_save\": true\r\n    }\r\n}\r\n```\r\n\r\n\r\nand my launch command:\r\n\r\n```\r\nCUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 deepspeed --num_nodes 1 --num_gpus 8 \\\r\n    llava/train/train_mem.py \\\r\n    --deepspeed deepspeed_config_stage3.json \\\r\n    --model_name_or_path path_to/models/vicuna/vicuna-7b \\\r\n    --version v0 \\\r\n    --data_path path_to/train_data/CC_3M_Concept_balanced_595K/chat.json \\\r\n    --image_folder path_to/train_data/CC_3M_Concept_balanced_595K/images \\\r\n    --vision_tower path_to/models/clip-vit-large-patch14 \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end \\\r\n    --bf16 True \\\r\n    --output_dir path_to/checkpoints/output/LLaVA-7B-pretrain \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 2400 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n```\n</Comment>\n<Comment by aprilehannibal at 2023-05-30T08:27:45Z>\n> You can observe `self.patch_embedding` weights in `your_path/transformers/models/clip/modeling_clip.py` to potentially identify the error.\r\n\r\n@Tomato1101  Have you solve this error now?\n</Comment>\n<Comment by Tomato1101 at 2023-05-30T10:26:30Z>\n> > You can observe `self.patch_embedding` weights in `your_path/transformers/models/clip/modeling_clip.py` to potentially identify the error.\r\n> \r\n> @Tomato1101 Have you solve this error now?\r\n\r\nSorry, Not yet.\r\nI have successfully run llava in A100 without using deepspeed.\n</Comment>\n<Comment by aprilehannibal at 2023-05-30T12:38:27Z>\n> {\r\n>     \"train_micro_batch_size_per_gpu\": \"auto\",\r\n>     \"zero_optimization\": {\r\n>         \"stage\": 3,\r\n>         \"overlap_comm\": true,\r\n>         \"contiguous_gradients\": true,\r\n>         \"sub_group_size\": 1e9,\r\n>         \"reduce_bucket_size\": \"auto\",\r\n>         \"stage3_prefetch_bucket_size\": \"auto\",\r\n>         \"stage3_param_persistence_threshold\": \"auto\",\r\n>         \"stage3_max_live_parameters\": 1e9,\r\n>         \"stage3_max_reuse_distance\": 1e9,\r\n>         \"stage3_gather_16bit_weights_on_model_save\": true\r\n>     }\r\n> }\r\n\r\n@Tomato1101 @haotian-liu When I add bf16 enable in de ds_config，llava run successfully on 4 * A100（40G）or 16 * A10（24G）with Deepspeed Zero3.  My ds_config shows below.\r\n\r\n```\r\n{\r\n  \"fp16\": {\r\n    \"enabled\": \"auto\",\r\n    \"loss_scale\": 0,\r\n    \"loss_scale_window\": 1000,\r\n    \"initial_scale_power\": 16,\r\n    \"hysteresis\": 2,\r\n    \"min_loss_scale\": 1\r\n  },\r\n  \"bf16\": {\r\n    \"enabled\": true\r\n  },\r\n  \"optimizer\": {\r\n    \"type\": \"AdamW\",\r\n    \"params\": {\r\n      \"lr\": \"auto\",\r\n      \"betas\": \"auto\",\r\n      \"eps\": \"auto\",\r\n      \"weight_decay\": \"auto\"\r\n    }\r\n  },\r\n  \"zero_optimization\": {\r\n    \"stage\": 3,\r\n    \"offload_optimizer\": {\r\n      \"device\": \"cpu\",\r\n      \"pin_memory\": true\r\n    },\r\n    \"offload_param\": {\r\n      \"device\": \"cpu\",\r\n      \"pin_memory\": true\r\n    },\r\n    \"overlap_comm\": true,\r\n    \"contiguous_gradients\": true,\r\n    \"sub_group_size\": 1e9,\r\n    \"reduce_bucket_size\": \"auto\",\r\n    \"stage3_prefetch_bucket_size\": \"auto\",\r\n    \"stage3_param_persistence_threshold\": \"auto\",\r\n    \"stage3_max_live_parameters\": 1e9,\r\n    \"stage3_max_reuse_distance\": 1e9,\r\n    \"gather_16bit_weights_on_model_save\": true\r\n  },\r\n  \"gradient_accumulation_steps\": \"auto\",\r\n  \"gradient_clipping\": \"auto\",\r\n  \"train_batch_size\": \"auto\",\r\n  \"train_micro_batch_size_per_gpu\": \"auto\",\r\n  \"steps_per_print\": 1e5,\r\n  \"wall_clock_breakdown\": false\r\n}\r\n```\n</Comment>\n<Comment by Tomato1101 at 2023-05-31T06:52:58Z>\n> > {\r\n> > \"train_micro_batch_size_per_gpu\": \"auto\",\r\n> > \"zero_optimization\": {\r\n> > \"stage\": 3,\r\n> > \"overlap_comm\": true,\r\n> > \"contiguous_gradients\": true,\r\n> > \"sub_group_size\": 1e9,\r\n> > \"reduce_bucket_size\": \"auto\",\r\n> > \"stage3_prefetch_bucket_size\": \"auto\",\r\n> > \"stage3_param_persistence_threshold\": \"auto\",\r\n> > \"stage3_max_live_parameters\": 1e9,\r\n> > \"stage3_max_reuse_distance\": 1e9,\r\n> > \"stage3_gather_16bit_weights_on_model_save\": true\r\n> > }\r\n> > }\r\n> \r\n> @Tomato1101 @haotian-liu When I add bf16 enable in de ds_config，llava run successfully on 4 * A100（40G）or 16 * A10（24G）with Deepspeed Zero3. My ds_config shows below.\r\n> \r\n> ```\r\n> {\r\n>   \"fp16\": {\r\n>     \"enabled\": \"auto\",\r\n>     \"loss_scale\": 0,\r\n>     \"loss_scale_window\": 1000,\r\n>     \"initial_scale_power\": 16,\r\n>     \"hysteresis\": 2,\r\n>     \"min_loss_scale\": 1\r\n>   },\r\n>   \"bf16\": {\r\n>     \"enabled\": true\r\n>   },\r\n>   \"optimizer\": {\r\n>     \"type\": \"AdamW\",\r\n>     \"params\": {\r\n>       \"lr\": \"auto\",\r\n>       \"betas\": \"auto\",\r\n>       \"eps\": \"auto\",\r\n>       \"weight_decay\": \"auto\"\r\n>     }\r\n>   },\r\n>   \"zero_optimization\": {\r\n>     \"stage\": 3,\r\n>     \"offload_optimizer\": {\r\n>       \"device\": \"cpu\",\r\n>       \"pin_memory\": true\r\n>     },\r\n>     \"offload_param\": {\r\n>       \"device\": \"cpu\",\r\n>       \"pin_memory\": true\r\n>     },\r\n>     \"overlap_comm\": true,\r\n>     \"contiguous_gradients\": true,\r\n>     \"sub_group_size\": 1e9,\r\n>     \"reduce_bucket_size\": \"auto\",\r\n>     \"stage3_prefetch_bucket_size\": \"auto\",\r\n>     \"stage3_param_persistence_threshold\": \"auto\",\r\n>     \"stage3_max_live_parameters\": 1e9,\r\n>     \"stage3_max_reuse_distance\": 1e9,\r\n>     \"gather_16bit_weights_on_model_save\": true\r\n>   },\r\n>   \"gradient_accumulation_steps\": \"auto\",\r\n>   \"gradient_clipping\": \"auto\",\r\n>   \"train_batch_size\": \"auto\",\r\n>   \"train_micro_batch_size_per_gpu\": \"auto\",\r\n>   \"steps_per_print\": 1e5,\r\n>   \"wall_clock_breakdown\": false\r\n> }\r\n> ```\r\n\r\nGreat, it works for me too.\n</Comment>\n<Comment by XipengY at 2023-06-02T03:22:32Z>\n@Tomato1101 @aprilehannibal What's the difference between starting training using \"torchrun\" and \"deepspeed\"?\n</Comment>\n<Comment by aprilehannibal at 2023-06-05T12:31:05Z>\n@XipengY I think there is no difference between them.\n</Comment>\n<Comment by XipengY at 2023-06-06T02:23:42Z>\n> @XipengY I think there is no difference between them.\r\n\r\n@aprilehannibal Thanks, I got it. \r\n\r\n@haotian-liu @Tomato1101 @aprilehannibal @abdul  And Do you solved the above error (torch.nn.Conv2d returning an empty tensor) without bf16, For example, the V100 cannot support bf16.\n</Comment>\n<Comment by aprilehannibal at 2023-06-09T08:49:45Z>\n@XipengY Maybe use fp16, or train on more V100 GPUs(like more than 16) with fp32.\n</Comment>\n<Comment by XipengY at 2023-06-12T01:30:54Z>\n> @XipengY Maybe use fp16, or train on more V100 GPUs(like more than 16) with fp32.\r\n\r\n@aprilehannibal Thanks for your guidance, I'll try later.\n</Comment>\n<Comment by uniquehou at 2024-03-04T02:18:58Z>\n> > > You can observe `self.patch_embedding` weights in `your_path/transformers/models/clip/modeling_clip.py` to potentially identify the error.\r\n> > \r\n> > \r\n> > @Tomato1101 Have you solve this error now?\r\n> \r\n> Sorry, Not yet. I have successfully run llava in A100 without using deepspeed.\r\n\r\nHow do you train without deepspeed, can you show your training scripts and configuration？thanks\n</Comment>\n<Comment by SharlotAway at 2024-03-30T13:53:43Z>\n@XipengY  Did you solve the above error that `torch.nn.Conv2d` returning an empty tensor without bf16, I ran ZeRO-3 on 4*V100, and I used fp16. And the vision model's tensors are empty. ZeRO-2 seems to work but I got OOM problem.\n</Comment>\n<Comment by jinghanSunn at 2024-11-08T11:38:26Z>\nHi @SharlotAway. Have you solved this empty tensor error? I also ran on 4*V100 and got the same error (RuntimeError: weight should have at least three dimensions)\n</Comment>\n<Comment by SharlotAway at 2024-11-16T06:46:57Z>\n> Hi @SharlotAway. Have you solved this empty tensor error? I also ran on 4*V100 and got the same error (RuntimeError: weight should have at least three dimensions)\r\n\r\nMaybe you can try the newest script, I ran ZeRO-3-offload in RTX3080 successfully.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 193,
    "state": "open",
    "created_by": "wangjiongw",
    "created_at": "2023-05-24T18:19:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/193</URL>\n\n<TITLE>[Question] Special Token IDs</TITLE>\n\n<BODY>### Question\r\n\r\nHello, thanks for your excellent work. \r\nI am new for NLP, and I have two questions about the special tokens. \r\n\r\n1. In `llava/train/train.py`: line41, you set both BOS TOKEN & EOS TOKEN to be `</s>`, is that a typo or something doesn't matter? Because I found BOS is `<s>` in Vicuna tokenizer.\r\n2. I loaded the tokenizer you released and trained by myself, but I got special tokens got different token ids. For example, LLaVA-13B-v0 you released convert `[PAD], <im_start>, <im_end>, <im_patch>` to be `[32003, 32001, 32002, 32003]`, but tokenizer make from train.py converts them to `[32000, 32002, 32003, 32001]`, I wonder how the difference appears and is it something important?\r\n\r\nThanks in advance.</BODY>\n\n<COMMENTS>\n<Comment by xianghuisun at 2023-05-25T10:09:26Z>\nI have the same question.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 192,
    "state": "open",
    "created_by": "haoxurt",
    "created_at": "2023-05-24T14:32:28Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/192</URL>\n\n<TITLE>Errors when infer  using  the weights trained by myself</TITLE>\n\n<BODY>### Discussion\r\nThank you for your excellent work！\r\nI follow your instruction to finetune LLaVA-13B  with 8x A100 (80G).  \r\nWhen I python -m llava.eval.run_llava  using my self delta weight, I get the errors.  If I  use your  LLaVA-13b-delta-v0 ,it can work well.  Am I doing something wrong?\r\n<img width=\"820\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/43776429/89c9a456-a2bc-4250-84c3-e14df4988709\">\r\n\r\nThe loss is normal. The curve  is as follows.\r\n<img width=\"532\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/43776429/4abcda8d-625a-4bc4-b844-1fdc0533235c\">\r\nMy checkpoints is as follows:\r\n<img width=\"737\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/43776429/dd1c50fa-d632-42be-b463-db7dddd5223e\">\r\nThen I  excute the python3 -m llava.model.consolidate, and I get the new checkpoints as follows:\r\n<img width=\"849\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/43776429/2748e7b1-2b20-4498-b4b1-9469c5dfc8f5\">\r\nFinally ,I excute `python3 -m llava.model.apply_delta \\\r\n    --base /path/to/llama-13b \\\r\n    --target /output/path/to/LLaVA-13B-self \\\r\n    --delta path/to/llava-13b-finetune_consolidate`\r\nand I get LLaVA-13B-self. It is as follows:\r\n<img width=\"746\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/43776429/b6df58eb-69f6-4261-8e61-5fcbac5c02a4\"></BODY>\n\n<COMMENTS>\n<Comment by haoxurt at 2023-05-24T14:41:55Z>\nI  have found a difference in config.json.  Is this the reason?\r\n<img width=\"920\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/43776429/6b1fc33b-404c-49f8-a137-ded69e41ba38\">\n</Comment>\n<Comment by haotian-liu at 2023-05-24T15:04:18Z>\nHi @haoxurt \r\n\r\nYou do *NOT* need to do `apply_delta` for the checkpoints that you trained.  This is only needed when you use a released LLaMA-based checkpoint (as its license does not allow us to release the full checkpoint).  Please try use the trained checkpoint directly, thanks!\n</Comment>\n<Comment by haoxurt at 2023-05-25T02:38:30Z>\nGot it! Thanks for your quick reply!  \r\nWhen I test using some simple image and query, its answer is wrong obviously. Could  you give some simple explanation or possible optimization strategy?\r\n<img width=\"1256\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/43776429/42585769-6839-4625-b541-63b162c3691d\">\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 191,
    "state": "closed",
    "created_by": "xielong",
    "created_at": "2023-05-24T14:32:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/191</URL>\n\n<TITLE>[Question] How much GPU memory is required for inference</TITLE>\n\n<BODY>### Question\n\nI'm currently trying to use your project for some inference tasks and I'm wondering about the GPU memory requirements. Could you please provide some guidance on how much GPU memory is typically needed for inference using this model?\r\n\r\nI'd appreciate it if you could also share any tips on how to optimize memory usage, or any methods to run the model on a GPU with less memory.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-29T17:23:18Z>\nHi, typically the 7B model can run with a GPU with less than 24GB memory, and the 13B model requires ~32 GB memory.  You can use multiple 24-GB GPUs to run 13B model as well following the instructions [here](https://github.com/haotian-liu/LLaVA#launch-a-model-worker-multiple-gpus-when-gpu-vram--24gb).\r\n\r\nWe are also working on bitsandbytes to further cut down the memory cost.  What kind of GPUs are you thinking of currently?\n</Comment>\n<Comment by xielong at 2023-06-17T04:31:36Z>\nThank you for your response. I'm just getting started with large models and don't have much knowledge about GPUs. However, I should be trying to run inference on the public cloud.\n</Comment>\n<Comment by haotian-liu at 2023-07-23T19:01:50Z>\nJust for your reference, we now support 4-bit/8-bit inference, which can reduce the GPU memory footprint by 4x/2x respectively, as a cost of a slight performance (accuracy) degradation. You can also try with multiple GPU inference as documented above.\r\n\r\nhttps://github.com/haotian-liu/LLaVA#launch-a-model-worker-4-bit-8-bit-inference-quantized\n</Comment>\n<Comment by xielong at 2023-08-08T11:31:15Z>\nThanks, I'll give it a try.\n</Comment>\n<Comment by haotian-liu at 2023-10-16T20:03:01Z>\nClosing for now. Please feel free to re-open or open a new issue, if there are any other questions. Thank you!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 190,
    "state": "closed",
    "created_by": "rubydoudou",
    "created_at": "2023-05-24T12:42:29Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/190</URL>\n\n<TITLE>[Question] Is it possible to inference on multiple GPUs?</TITLE>\n\n<BODY>### Question\r\n\r\nI am encountering an issue with the 13B model inference where it fails to complete due to insufficient GPU memory. My computer has two GPUs, each with a memory of 24GB.. Inference using the 7B model is fine.\r\n\r\nI am wondering if it is possible to perform CLI inference with two GPUs. I would appreciate any help or suggestions on how to resolve this issue. \r\n\r\nThank you.</BODY>\n\n<COMMENTS>\n<Comment by zglin at 2023-05-27T22:43:50Z>\nPass --num-gpus <N> to the llava.serve.model_worker command for inference\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 189,
    "state": "open",
    "created_by": "ouyanglinke",
    "created_at": "2023-05-24T10:35:29Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/189</URL>\n\n<TITLE>[Question] Demo crashed</TITLE>\n\n<BODY>### Question\n\nThe demo is not working now: https://llava.hliu.cc/</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-25T18:50:01Z>\nHi, it is working for me.  Can you share a screenshot in which way it is not working for you?  Thanks.\n</Comment>\n<Comment by PhanTask at 2023-05-25T20:28:40Z>\nHi, it crashed on my side too. I cannot open the website. The embedded version in the project page always shows loading.\n</Comment>\n<Comment by haotian-liu at 2023-05-25T20:33:52Z>\nHi @PhanTask, there was a server issue just now, and I have just fixed it :)\r\n\r\nPlease try again, thanks.\n</Comment>\n<Comment by PhanTask at 2023-05-25T21:28:00Z>\n@haotian-liu They both work now. Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 187,
    "state": "open",
    "created_by": "haoxurt",
    "created_at": "2023-05-24T02:19:50Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/187</URL>\n\n<TITLE>dataset  blip_laion_cc_sbu_558k</TITLE>\n\n<BODY>### Question\n\nThanks for your work.  where can I download blip_laion_cc_sbu_558k ?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-25T19:56:50Z>\nHi, thank you for your interest in our work.  Please download from our [HF Dataset](https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain).\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 186,
    "state": "closed",
    "created_by": "shawnshenjx",
    "created_at": "2023-05-24T00:09:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/186</URL>\n\n<TITLE>[Usage] Slow inference speed</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\nSlow inference speed\r\nCommand:\r\n```\r\npython run_llava.py --model-name /home/shawn/desktop/LLaVA-13B-v0 --image-file \"https://llava-vl.github.io/static/images/view.jpg\" --query \"Describe.\"\r\n```\r\n\r\nLog: \r\n```\r\nInference Time:15.626604557037354\r\nThe image features a serene scene with a wooden pier extending out over a large body of water, such as a lake or a mountain-surrounded bay. The water is calm and clear, reflecting the beauty of the surrounding landscape. There are a few small boats scattered on the water, adding to the peaceful atmosphere.\r\n\r\nThe pier itself has a few steps, and a bench is located at the end of the pier, inviting visitors to sit and enjoy the view. The entire scene is framed by trees and mountains, making it a picturesque and inviting location for relaxation or contemplation.\r\n```\r\n\r\nI am using a server with 3x3090 GPU cards. Why the inference speed is this slow? I tried multiple times and the inference time is usually around 15-25 seconds. I only measure the time for this function:     \r\nwith torch.inference_mode():\r\n        output_ids = model.generate(\r\n            input_ids,\r\n            images=image_tensor.unsqueeze(0).half().cuda(),\r\n            do_sample=True,\r\n            temperature=0.2,\r\n            max_new_tokens=1024,\r\n            stopping_criteria=[stopping_criteria])\r\n\r\nThis one costs 15-25 seconds. May I ask why is that?</BODY>\n\n<COMMENTS>\n<Comment by hszhoushen at 2023-11-03T08:32:33Z>\n> ### When did you clone our code?\r\n> I cloned the code base after 5/1/23\r\n> \r\n> ### Describe the issue\r\n> Issue: Slow inference speed Command:\r\n> \r\n> ```\r\n> python run_llava.py --model-name /home/shawn/desktop/LLaVA-13B-v0 --image-file \"https://llava-vl.github.io/static/images/view.jpg\" --query \"Describe.\"\r\n> ```\r\n> \r\n> Log:\r\n> \r\n> ```\r\n> Inference Time:15.626604557037354\r\n> The image features a serene scene with a wooden pier extending out over a large body of water, such as a lake or a mountain-surrounded bay. The water is calm and clear, reflecting the beauty of the surrounding landscape. There are a few small boats scattered on the water, adding to the peaceful atmosphere.\r\n> \r\n> The pier itself has a few steps, and a bench is located at the end of the pier, inviting visitors to sit and enjoy the view. The entire scene is framed by trees and mountains, making it a picturesque and inviting location for relaxation or contemplation.\r\n> ```\r\n> \r\n> I am using a server with 3x3090 GPU cards. Why the inference speed is this slow? I tried multiple times and the inference time is usually around 15-25 seconds. I only measure the time for this function: with torch.inference_mode(): output_ids = model.generate( input_ids, images=image_tensor.unsqueeze(0).half().cuda(), do_sample=True, temperature=0.2, max_new_tokens=1024, stopping_criteria=[stopping_criteria])\r\n> \r\n> This one costs 15-25 seconds. May I ask why is that?\r\n\r\nHow do you fix this problem, did you just turn off the screen output of language description?\n</Comment>\n<Comment by WilTay1 at 2024-01-13T07:04:50Z>\ni got the same slow, very slow. Can you please show how to solve it? Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 185,
    "state": "open",
    "created_by": "aprilehannibal",
    "created_at": "2023-05-23T09:48:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/185</URL>\n\n<TITLE>[Question] Science QA zero shot metrics</TITLE>\n\n<BODY>### Question\r\n\r\nHi, @haotian-liu.  Did you do a Zero-shot Evalatuion on Science QA? If so,  can you provide your result? Thanks a lot!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 184,
    "state": "closed",
    "created_by": "alpayariyak",
    "created_at": "2023-05-23T04:22:21Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/184</URL>\n\n<TITLE>[Usage] Fine-tuning stuck at 0%</TITLE>\n\n<BODY>### When did you clone our code?\r\n\r\nI cloned the code base after 5/1/23\r\n\r\n### Describe the issue\r\n\r\nIssue:\r\nTrying to fine-tune on a custom dataset, progress is stuck at\r\n```\r\n  0%|          | 0/2124 [00:00<?, ?it/s]\r\n```\r\n\r\nCommand:\r\n```\r\n#!/usr/bin/env bash\r\n#SBATCH -N 1\r\n#SBATCH -c 8\r\n#SBATCH --gres=gpu:2\r\n#SBATCH -C A100\r\n#SBATCH -t 10:00:00\r\n#SBATCH --mem 70G\r\n\r\ntorchrun --nnodes=1 --nproc_per_node=2 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path LLaVA-7B-base \\\r\n    --data_path /home/aariyak/gradelight/LLaVA_finetuning/LLaVA_code/data/IAM_LLAVA.json \\\r\n    --image_folder /home/aariyak/gradelight/LLaVA_finetuning/LLaVA_code/data/images \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 1000 \\\r\n    --save_total_limit 3 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --fsdp \"full_shard auto_wrap\" \\\r\n    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb \r\n```\r\n\r\nFull Log: \r\n```\r\nWARNING:torch.distributed.run:\r\n*****************************************\r\nSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n*****************************************\r\n/home/aariyak/miniconda3/envs/rlhf_llama/lib/python3.8/site-packages/transformers/training_args.py:1356: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \r\n  warnings.warn(\r\n/home/aariyak/miniconda3/envs/rlhf_llama/lib/python3.8/site-packages/transformers/training_args.py:1356: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \r\n  warnings.warn(\r\nSome weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_projection.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'visual_projection.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'logit_scale', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.final_layer_norm.bias']\r\n- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\nSome weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'logit_scale', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_projection.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'visual_projection.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm2.bias']\r\n- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n\r\nLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\r\nLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\r\nLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.29s/it]\r\nLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.39s/it]\r\nLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.11s/it]\r\nLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.59s/it]\r\n\r\nLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.12s/it]\r\nLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.61s/it]\r\nWARNING:root:Loading data...\r\nWARNING:root:Formatting inputs...Skip in lazy mode\r\nWARNING:root:Loading data...\r\nWARNING:root:Formatting inputs...Skip in lazy mode\r\nwandb: Currently logged in as: aariyak (wpi-ai). Use `wandb login --relogin` to force relogin\r\nwandb: wandb version 0.15.3 is available!  To upgrade, please run:\r\nwandb:  $ pip install wandb --upgrade\r\nwandb: Tracking run with wandb version 0.15.0\r\nwandb: Run data is saved locally in /home/aariyak/gradelight/LLaVA_finetuning/LLaVA_code/LLaVA/wandb/run-20230522_160926-s6wbl1xn\r\nwandb: Run `wandb offline` to turn off syncing.\r\nwandb: Syncing run happy-morning-43\r\nwandb: ⭐️ View project at ...\r\nwandb: 🚀 View run at ...\r\n\r\n  0%|          | 0/2124 [00:00<?, ?it/s]\r\n```</BODY>\n\n<COMMENTS>\n<Comment by alpayariyak at 2023-05-23T14:23:33Z>\n@haotian-liu thank you for your work with this project, it is incredible. Would you happen to know what might be wrong?\n</Comment>\n<Comment by RajdeepBorgohain at 2023-05-25T04:16:47Z>\n@alpayariyak  Can you please share your infra configuration?\n</Comment>\n<Comment by alpayariyak at 2023-06-02T19:25:18Z>\nSeemed to be a hardware related issue, resolved after switching to a different VM\n</Comment>\n<Comment by dingning97 at 2023-08-16T14:33:46Z>\nI encounter the same problem. The training hangs at the very beginning.\r\nCould you tell me how you finnally resolved this problem?\n</Comment>\n<Comment by wanghao-cst at 2023-10-18T12:51:23Z>\n> I encounter the same problem. The training hangs at the very beginning. Could you tell me how you finnally resolved this problem?\r\n\r\nHi, I got the same issue, may I know how you solve?\n</Comment>\n<Comment by MajorDavidZhang at 2024-03-22T12:31:50Z>\nI got the same issue in the model loading phase. I solved by change the zero3.json in finetune.sh to zero2.json\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 183,
    "state": "open",
    "created_by": "JonathanRayner",
    "created_at": "2023-05-23T04:02:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/183</URL>\n\n<TITLE>[Feature request] Change the dataset license to allow commercial use</TITLE>\n\n<BODY>### feature\n\nWhy does the dataset license restrict commercial use?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 182,
    "state": "open",
    "created_by": "chengzeyi",
    "created_at": "2023-05-23T03:17:02Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/182</URL>\n\n<TITLE>[Feature request] Support RedPajama Models</TITLE>\n\n<BODY>### feature\r\n\r\nRedPajama is fully opensouced and it provides a 3B model which is lightening fast. It would be better if you could support RedPajama. Thanks.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 180,
    "state": "closed",
    "created_by": "alpayariyak",
    "created_at": "2023-05-22T20:03:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/180</URL>\n\n<TITLE>[Usage] fine-tuning raises error at \"assert num_new_tokens == 2\"</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\n\r\nI am trying to fine-tune on a custom dataset. When running the script, I get this error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/home/aariyak/gradelight/LLaVA_finetuning/LLaVA_code/LLaVA/llava/train/train.py\", line 632, in train\r\n    model.initialize_vision_tokenizer(mm_use_im_start_end=model_args.mm_use_im_start_end, tokenizer=tokenizer, device=training_args.device,\r\n  File \"/home/aariyak/gradelight/LLaVA_finetuning/LLaVA_code/LLaVA/llava/model/llava.py\", line 319, in initialize_vision_tokenizer\r\n    assert num_new_tokens == 2\r\nAssertionError\r\n```\r\n\r\nAlso, is this correct? I made a custom dataset of images of different sizes with conversations and formatted everything the same as LLaVA-instruct-150k.json. I didn't touch most of the parameters.\r\nCommand:\r\n```\r\ntorchrun --nnodes=1 --nproc_per_node=1 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path LLaVA-7B-base \\\r\n    --data_path /home/aariyak/gradelight/LLaVA_finetuning/LLaVA_code/data/IAM_LLAVA.json \\\r\n    --image_folder /home/aariyak/gradelight/LLaVA_finetuning/LLaVA_code/data/images \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter projector/llava-7b-pretrain.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 1000 \\\r\n    --save_total_limit 3 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --fsdp \"full_shard auto_wrap\" \\\r\n    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb \r\n```</BODY>\n\n<COMMENTS>\n<Comment by alpayariyak at 2023-05-22T20:09:58Z>\nFixed using solution described in #110\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 179,
    "state": "closed",
    "created_by": "aolivtous",
    "created_at": "2023-05-22T16:09:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/179</URL>\n\n<TITLE>LLaVA-MPT-7b weights</TITLE>\n\n<BODY>### Question\n\nHi, congrats on your amazing work!\r\nI was wondering if the weights of LLaVA-MPT-7b model are available somewhere after pretraining and finetuning.\r\nThanks!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-23T02:03:19Z>\nHi, please checkout the weights of MPT model [here](https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview), thanks!\n</Comment>\n<Comment by aolivtous at 2023-05-23T08:48:12Z>\nThanks!\n</Comment>\n<Comment by harrietfiagbor at 2023-05-29T10:45:28Z>\nAlso, is finetuning the LLava-MPT-7B weights any different from the finetuning process? Since you stated that it can be used directly without delta weights, finetuning is just giving dataset in llava instruct format?\n</Comment>\n<Comment by haotian-liu at 2023-07-23T19:04:32Z>\n@harrietfiagbor \r\n\r\nThere is not difference in the finetuning process.  This decision (w/o delta weights) is simply due to the LLaMA license (and the general practice in the community).\r\n\r\nWe now release full model weights that do not require delta weight merging.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 177,
    "state": "closed",
    "created_by": "o0t1ng0o",
    "created_at": "2023-05-22T05:20:55Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/177</URL>\n\n<TITLE>[Usage] Fine-tune with 4 x 40G A100 gpus and get OOM error when saving state_dict</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nHi, @haotian-liu \r\nIssue: \r\nI downloaded the liuhaotian/LLaVA-7b-delta-v0 model from huggingface.\r\nI fine-tuned this model with my custom dataset with 4 x 40G A100 gpus.\r\nBut I got OOM error when I saved the model.\r\nAnd my per_device_train_batch_size=1.\r\n\r\nCommand:\r\n```\r\npython3 -m torch.distributed.run --nnodes=1 --nproc_per_node=4 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path ./LLaVA-7b-delta-v0 \\\r\n    --data_path ./trainset.json \\\r\n    --image_folder ./images \\\r\n    --vision_tower ./clip-vit-large-patch14 \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 8 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 5000 \\\r\n    --save_total_limit 3 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --fsdp \"full_shard auto_wrap\" \\\r\n    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nLog: \r\n```\r\n  warnings.warn(\r\n/my/path/tool/anaconda3/envs/llava_beta/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:312: UserWarn\r\ning: Failed to clone() tensor with name lm_head.weight on rank 2. This may mean that this state_dict entry could point to invalid memory regions after r\r\neturning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of lm_head.weight. Error: CUDA out of memory. Tr\r\nied to allocate 502.00 MiB (GPU 2; 39.59 GiB total capacity; 36.99 GiB already allocated; 45.19 MiB free; 37.41 GiB reserved in total by PyTorch) If res\r\nerved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALL$\r\nC_CONF\r\n  warnings.warn(\r\n/my/path/tool/anaconda3/envs/llava_beta/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:312: UserWar$\r\ning: Failed to clone() tensor with name _fsdp_wrapped_module.model.layers.31.mlp.up_proj.weight on rank 0. This may mean that this state_dict entry cou$\r\nd point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of _fs$\r\np_wrapped_module.model.layers.31.mlp.up_proj.weight. Error: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 39.59 GiB total capacity; 37.30 Gi$\r\n already allocated; 61.19 MiB free; 37.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to $\r\nvoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n  warnings.warn(\r\n/my/path/tool/anaconda3/envs/llava_beta/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:312: UserWar$\r\ning: Failed to clone() tensor with name lm_head.weight on rank 0. This may mean that this state_dict entry could point to invalid memory regions after $\r\neturning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of lm_head.weight. Error: CUDA out of memory. T$\r\nied to allocate 502.00 MiB (GPU 0; 39.59 GiB total capacity; 37.05 GiB already allocated; 61.19 MiB free; 37.54 GiB reserved in total by PyTorch) If re$\r\nerved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALL$\r\nC_CONF\r\n  warnings.warn(\r\nTraceback (most recent call last):\r\n  File \"/my/path/wt/code/PLM/LLaVA/llava/train/train_mem.py\", line 15, in <module>\r\n    train()\r\n  File \"/my/path/wt/code/PLM/LLaVA/llava/train/train.py\", line 666, in train\r\n    safe_save_model_for_hf_trainer(trainer=trainer,\r\n  File \"/my/path/wt/code/PLM/LLaVA/llava/train/train.py\", line 95, in safe_save_model_for_hf_trainer\r\n    cpu_state_dict = {\r\n  File \"/my/path/wt/code/PLM/LLaVA/llava/train/train.py\", line 96, in <dictcomp>\r\n    key: value.cpu()\r\nRuntimeError: CUDA error: invalid argument\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n```</BODY>\n\n<COMMENTS>\n<Comment by o0t1ng0o at 2023-05-22T05:55:28Z>\nI have solved this issue with this solution. https://discuss.pytorch.org/t/fsdp-failed-to-save-model-checkpoints/178232/2\n</Comment>\n<Comment by kahnchana at 2023-06-09T21:02:30Z>\nDid you figure out any way to solve this without modifying the torch library source code?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 176,
    "state": "open",
    "created_by": "wantsjean",
    "created_at": "2023-05-18T15:31:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/176</URL>\n\n<TITLE>[Question] Different results generated when set 'use_cache=True'</TITLE>\n\n<BODY>### Question\n\nI notice that the origional config's use_cache was set to False, then I set it to True, but found that the result became different. What's the reason? (both are greedy search)\r\n\r\n[use_cache=False]\r\nThe image features a man's headshot on a blue and white business card. The man is wearing a blue shirt and has a beard. The business card is designed to look like a police badge, which adds a unique and professional touch to the card. The card is likely intended for use in a professional setting, such as a business or a professional organization, where the man's identity and authority might be required.\r\n\r\n[use_cache=True]\r\nThe image features a man's headshot on a blue and white business card. The man is wearing a blue shirt and has a beard. The business card is designed to look like a police badge, which adds a unique and professional touch to the card. The card is likely intended for professional purposes, such as networking or representing a specific company or organization.</BODY>\n\n<COMMENTS>\n<Comment by RajdeepBorgohain at 2023-05-21T12:39:36Z>\n@wantsjean  Can you share which file or code you are working with the generate this ?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 175,
    "state": "open",
    "created_by": "siyang1992",
    "created_at": "2023-05-18T08:17:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/175</URL>\n\n<TITLE>[Question]</TITLE>\n\n<BODY>### Question\n\nHI. Thanks for your great work.\r\nI want to know how to generate instruction data by GPT4. \r\nThe generate code is not available in the codebase.\r\nCould you share these code?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-23T02:05:32Z>\nHi @siyang1992 \r\n\r\nThe system message and few-shot samples are released [here](https://github.com/haotian-liu/LLaVA/tree/main/playground/data/prompts).\r\n\r\nI see another question #178 asking about the process of filtering out low-quality samples, we filter out some of the responses that contains keywords like \"according to the description\", \"sorry\", etc.  We'll provide the script of the GPT generation pipeline including these filtering process by the end of this week.\r\n\r\nThanks!\n</Comment>\n<Comment by aprilehannibal at 2023-06-26T03:52:24Z>\nHI. Thanks for your great work. I have the same question. Could you please share the code for generating the instruction data by GPT4?\r\n@haotian-liu\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 174,
    "state": "open",
    "created_by": "AI-Study-Han",
    "created_at": "2023-05-18T07:02:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/174</URL>\n\n<TITLE>Is it possible to use only question and answer data without pictures for fine-tuning?[Question]</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by RajdeepBorgohain at 2023-05-18T09:29:37Z>\nDo you want text to text generation?\r\nOr you want to improve only the text to text generation of this model?\n</Comment>\n<Comment by AI-Study-Han at 2023-05-18T13:27:41Z>\nI want to improve only the text to text generation of this model\n</Comment>\n<Comment by AI-Study-Han at 2023-05-18T13:37:06Z>\nCan I improve llava's dialog capabilities with only dialog data\n</Comment>\n<Comment by AI-Study-Han at 2023-05-18T13:37:20Z>\nCan I improve llava's dialog capabilities with only dialog data？\n</Comment>\n<Comment by haotian-liu at 2023-05-23T02:10:22Z>\nHi @HanSomeLing, according to our experiments, we are able to improve its dialog-only performance, by including some ShareGPT data into our training pipeline.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 173,
    "state": "open",
    "created_by": "KosumosuL",
    "created_at": "2023-05-18T06:00:09Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/173</URL>\n\n<TITLE>[Question] train lightning llava-7b-v1 got relatively higher loss curve.</TITLE>\n\n<BODY>### Question\n\nHere is my training script:\r\n```shell\r\n  torchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path llama-vicuna-7b-v1.1 \\\r\n    --version v1 \\\r\n    --data_path CC3M/chat.json \\\r\n    --image_folder CC3M/images \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-lightning-7b-pretrain-v1 \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 2400 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to none\r\n``` \r\n\r\nI use the llama-vicuna-7b-v1.1 got from FastChat, and finetune it with CC595K image-text pairs, with version v1 conversation format, the losses are keeping around 2.5 and hard to decrease. Yet when useing llama-vicuna-7b with version v0, the losses are repaidly decreased to 1.3~1.5.\r\n\r\nIs there any bugs in the latest code? Or the loss is just normal for version v1?</BODY>\n\n<COMMENTS>\n<Comment by MrigankRaman at 2023-05-19T19:21:18Z>\nEven I am curious about this\n</Comment>\n<Comment by haotian-liu at 2023-05-23T02:16:55Z>\nHi @KosumosuL @MrigankRaman \r\n\r\nWe are seeing the loss of Vicuna v1-1 being larger than Vicuna v0 models as well. Although the qualitative results look good, we are also investigating the reasons.  One possible reason is that in v0 prompts, there are end of sequence token \"###\" after each sentence, while in v1, it is only added after the GPT response.  Furthermore, \"###\" can sometimes be tokenized as \"#\" and \"##\" (two tokens instead of one token).\r\n\r\nGiven that \"###\" is really easy for model to optimize to predict, it may be the reason why the loss of v0 model is lower than v1.\r\n\r\nIf you have other better explanations or insights, please let me know.  I'll update if there are more findings as well.\r\n\r\nThanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 172,
    "state": "closed",
    "created_by": "zhhao1",
    "created_at": "2023-05-18T03:21:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/172</URL>\n\n<TITLE>[Question] about the pretrain stage</TITLE>\n\n<BODY>### Question\n\nOnly adapter was adjusted in the original paper in pretrain stage. However, it seems that the input embedding of llama model is also tuned. Is there some inconsistency here, or is there a problem with my understanding.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/46468492/7f779c55-6b2c-4f04-b2a3-37a3db183f33)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 171,
    "state": "open",
    "created_by": "XipengY",
    "created_at": "2023-05-18T02:58:23Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/171</URL>\n\n<TITLE>[Question] Why training with answer?</TITLE>\n\n<BODY>### Question\n\nWe print the input_ids and labels before calculate losses, and we found that the input_ids with answer same as labels, it means that the answer would be predicted are in input_ids to the LLaMA,  please kindly correct me if I am wrong.\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/17317650/3c6bb071-468d-40e8-a3b9-f492d4423732)\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/17317650/8640ea94-b124-434a-947d-24e305540aee)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-23T02:19:00Z>\nHi, thanks for your question.\r\n\r\nThis is expected, as the model is trained with \"causal masking\", meaning that the tokens only attend to the tokens before itself, without seeing itself and tokens after that.  Please let me know if there are other questions, thanks!\n</Comment>\n<Comment by XipengY at 2023-05-23T06:54:06Z>\nThanks for your reply!\r\n\r\nI have understood it, this is the attibute of decode transformer.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 170,
    "state": "open",
    "created_by": "RajdeepBorgohain",
    "created_at": "2023-05-17T19:21:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/170</URL>\n\n<TITLE>[Question] Finetuning with Custom Image dataset for VQA</TITLE>\n\n<BODY>### Question\n\nSo, I want to finetune this model with our own custom image dataset, which is mostly design images, and we want to give the ability to users to ask questions based on the image.\r\nAt this point, LLaVA 13B cannot produce the expected results. And we are planning to create a set of 3000 images with Questions and Answers and want to improve the Model.\r\n\r\nPlease share your thoughts; what type of infra is required if we want to fine-tune 7B model?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-23T02:21:24Z>\nHi @RajdeepBorgohain, thank you for your interest in our work.\r\n\r\nYou can try to run this with 8x A100s, and for 7B model, you shall be able to train with A100 (40G)x8.\r\n\r\nWe are also working to support more hardwares with DeepSpeed.  It is targeted by the end of this month or early next month.  Stay tuned if you are interest in this.\r\n\r\nPlease let me know if there are other questions, thanks!\n</Comment>\n<Comment by RajdeepBorgohain at 2023-05-25T05:15:18Z>\nHi Thanks a lot for your reply :)\r\nI have another question, so I am using your pretrained model \"LLaVA-7b-delta-v0\" check checkpoint for fine-tuning, and I am trying to do that with 1xA100(40G) since I have a very small dataset of 80 images and instructions. And I got errors like \"torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \".\r\n\r\nAlso, we are experimenting with Web UI images. Can you share any tips to improve the performance of the model on this type of image?\r\n\r\nAlso, we found that the 13B model is failing to answer questions on images which have a lot of text, can you share how can we improve on this?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 169,
    "state": "open",
    "created_by": "aprilehannibal",
    "created_at": "2023-05-17T15:42:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/169</URL>\n\n<TITLE>[Question] pretain 7B loss doesn't change</TITLE>\n\n<BODY>### Question\r\n\r\nI tried to pretrain llava-7b on 8 * A100(40G), CUDA117, torch==1.13.1, py3.10\r\n I found the loss doesn't change when training.\r\n\r\n<img width=\"547\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/14832463/8ea0d73f-103a-4aae-a146-cbba3d040b0d\">\r\n\r\n\r\n\r\nhere is my pretraining scripts:\r\n<img width=\"622\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/14832463/eb6a37c2-e6cf-4de0-9a06-4827ceebe190\"></BODY>\n\n<COMMENTS>\n<Comment by aprilehannibal at 2023-05-17T15:55:05Z>\nI wonder the LLM you used for pretraining is the  \"lmsys/vicuna-7b-delta-v1.1\" or the origin \"llama 7b\" weight. @haotian-liu\n</Comment>\n<Comment by haotian-liu at 2023-05-17T16:02:52Z>\nHi @aprilehannibal, you need to perform [weight conversion](https://github.com/lm-sys/FastChat#vicuna-7b) for vicuna weights, by combing llama-7b and vicuna-delta -> vicuna-7b.  We pretrain based on vicuna-7b.\r\n\r\nWe'll update the instruction to make it clearer, thanks.\n</Comment>\n<Comment by aprilehannibal at 2023-05-18T02:22:12Z>\nOK, got it! Thanks a lot!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 168,
    "state": "open",
    "created_by": "YuanLiuuuuuu",
    "created_at": "2023-05-17T12:19:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/168</URL>\n\n<TITLE>[BUG] RuntimeError when running inference on ScienceQA</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\npython -m llava.eval.model_vqa_science \\\r\n    --model-name /path/to/LLaVA-13b-v0-science_qa \\\r\n    --question-file /path/to/ScienceQA/data/scienceqa/llava_test.json \\\r\n    --image-folder /path/to/ScienceQA/data/scienceqa/images/test \\\r\n    --answers-file vqa/results/ScienceQA/test_llava-13b.jsonl \\\r\n    --answer-prompter\r\n    --conv-mode simple\r\n```\r\n\r\nLog: \r\n```\r\nRuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR\r\n```\r\n\r\nScreenshots:\r\n<img width=\"485\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/30762564/66baea5f-6029-4162-9d92-b136e1667f34\">\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-17T18:16:51Z>\nHi, you may try reinstalling PyTorch with CUDA 11.7, with the commands this user from our community provides: https://github.com/haotian-liu/LLaVA/issues/123#issuecomment-1539434115.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 167,
    "state": "open",
    "created_by": "zhaoshitian",
    "created_at": "2023-05-17T11:11:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/167</URL>\n\n<TITLE>.generate() method in LlavallamaForCausalLM class</TITLE>\n\n<BODY>### Question\n\nI notice that in the evaluation process, the method **model.generate()** is used, but I don't find the implementation code in the LlavallamaForCausalLM class. Did I miss something?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-17T18:18:05Z>\nHi, it's implemented in its parent class in the `transformers` library [here](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationMixin.generate).\n</Comment>\n<Comment by zhaoshitian at 2023-05-18T17:10:38Z>\nThanks！\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 166,
    "state": "open",
    "created_by": "CRCODE22",
    "created_at": "2023-05-17T11:04:56Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/166</URL>\n\n<TITLE>Which commands to use to quantize it to 4 bit?</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-17T18:18:47Z>\nHi, we'll update instructions on the quantization soon, thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 165,
    "state": "open",
    "created_by": "DanqingZ",
    "created_at": "2023-05-17T02:30:10Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/165</URL>\n\n<TITLE>[Question] Some images from the blip_laion_cc_sbu_558k dataset are no longer accessible.</TITLE>\n\n<BODY>### Question\r\n\r\nI attempted to download the blip_laion_cc_sbu_558k images using their provided URLs in the metadata.json file, but I discovered that some images are inaccessible. I'm wondering if this might pose any issues during the pretraining stage, given that some image files are missing.\r\n\r\nI am asking this question because I found in section: https://github.com/haotian-liu/LLaVA#pretraining-dataset that ~15% images of the original CC-3M dataset are no longer accessible.\r\n\r\nmy image downloading logs\r\n```\r\nDownloaded /mnt/my-volume/data/llava/blip_laion_cc_sbu_558k/00453/004539375.jpg\r\nDownloaded /mnt/my-volume/data/llava/blip_laion_cc_sbu_558k/00223/002239345.jpg\r\nDownloaded /mnt/my-volume/data/llava/blip_laion_cc_sbu_558k/00594/005947502.jpg\r\nDownloaded /mnt/my-volume/data/llava/blip_laion_cc_sbu_558k/00511/005116462.jpg\r\nThere was an error: 403 Client Error: Forbidden for url: http://learnopencv.com/wp-content/uploads/2020/03/augmented-reality-aruco-markers-opencv.jpg\r\nRetry 1/3 in 2 seconds.\r\nThere was an error: 403 Client Error: Forbidden for url: http://learnopencv.com/wp-content/uploads/2020/03/augmented-reality-aruco-markers-opencv.jpg\r\nRetry 2/3 in 2 seconds.\r\nThere was an error: 403 Client Error: Forbidden for url: http://learnopencv.com/wp-content/uploads/2020/03/augmented-reality-aruco-markers-opencv.jpg\r\nRetry 3/3 in 2 seconds.\r\nFailed to download https://www.learnopencv.com/wp-content/uploads/2020/03/augmented-reality-aruco-markers-opencv.jpg after 3 attempts.\r\nDownloaded /mnt/my-volume/data/llava/blip_laion_cc_sbu_558k/00181/001819539.jpg\r\nDownloaded /mnt/my-volume/data/llava/blip_laion_cc_sbu_558k/00283/002838027.jpg\r\n...\r\n```</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-25T19:56:02Z>\nHi, thank you for your interest in our work.  We have uploaded the images [here](https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain/blob/main/images.zip). \r\n\r\nImportant notice: Upon the request from the community, as ~15% images of the original LAION/CC/SBU dataset are no longer accessible, we upload images.zip for better reproducing our work in research community. It should not be used for any other purpose. The use of these images must comply with the LAION/CC/SBU license. This may be taken down when requested by the original LAION/CC/SBU dataset owner or owners of the referenced images.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 163,
    "state": "open",
    "created_by": "nathanodle",
    "created_at": "2023-05-16T22:10:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/163</URL>\n\n<TITLE>Segmentation Fault w/ ROCm</TITLE>\n\n<BODY>Running LLaVA-Lightning-MPT-7B-preview, get a core dump on the model worker.\r\n\r\nUsing latest Torch 2.0 w/ ROCm 5.4.2 on AMD Radeon 7900 XTX.  Tried demo images and user images with various prompts.\r\n\r\nFollowing directions as per README.md, model is loading fine but at inference time I get the segfault.  The web server and controller all appear to work fine and the model worker receives the request.  \r\n\r\nThe specific error is:  \"2023-05-16 22:04:16 | INFO | stdout | INFO:     127.0.0.1:36094 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\nSegmentation fault (core dumped)\"</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 162,
    "state": "open",
    "created_by": "jihan-yin",
    "created_at": "2023-05-16T20:43:26Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/162</URL>\n\n<TITLE>[Usage] Llava-7b-v1 missing projection weights in huggingface</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nThey are missing at https://huggingface.co/liuhaotian/LLaVA-Lightning-7B-delta-v1-1.</BODY>\n\n<COMMENTS>\n<Comment by jihan-yin at 2023-05-16T20:47:08Z>\nAh I see it is included in the pytorch model bin weights\n</Comment>\n<Comment by jihan-yin at 2023-05-16T20:48:16Z>\nActually, reopening as the apply delta script does not copy over these weights\n</Comment>\n<Comment by haotian-liu at 2023-05-16T21:11:20Z>\nHi, the projection layers should be covered in the `apply_delta` scripts.  Please share the command and the logs, so that I could know what might be happening, thanks.\n</Comment>\n<Comment by jihan-yin at 2023-05-16T21:37:00Z>\nHmm you're right, it should be... But when calling `LlavaLlamaForCausalLM.from_pretrained(...)`, the mm projection weights get initialized as meta tensors.\n</Comment>\n<Comment by jihan-yin at 2023-05-16T21:38:28Z>\nIe, \r\n```\r\nmodel = LlavaLlamaForCausalLM.from_pretrained(model_path, device_map='auto', torch_dtype=torch.float16)\r\nmodel.model.mm_projector.to(0)\r\n```\r\nGets an error stating that the tensor to clone is a meta tensor. I don't have the exact error log on hand anymore since I manually pulled out the projector weights from the delta weight files and loaded it individually to fix the issue\n</Comment>\n<Comment by haotian-liu at 2023-05-16T21:41:06Z>\nHi @jihan-yin. Glad to hear that your issue is resolved by this workaround.  This is still strange to me, as I have tested the apply_delta at the time I released the lightning weights.  I will check this again tomorrow after the NeurIPS deadline :)\r\nPlease let me know if you have other issues.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 161,
    "state": "closed",
    "created_by": "conorhearn1",
    "created_at": "2023-05-16T13:52:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/161</URL>\n\n<TITLE>Running on MacOS</TITLE>\n\n<BODY>### feature\n\nDo you have any plans to support running this on an Apple M2 processor? \r\n\r\nThank you!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-23T02:28:54Z>\nHi, we are working on support running the model on Apple Silicon.  There is a contribution from the community that currently supports this: https://github.com/cameronbergh/LLaVA-MPS/tree/mps, and you may try it out. We'll update once we finish the integration.  Thanks!\n</Comment>\n<Comment by ricoelmostico at 2023-09-17T12:09:10Z>\nHello ! Have this fork been merged or work on an apple compatiblity been done?\r\nThank you !\n</Comment>\n<Comment by louis030195 at 2023-10-12T17:55:52Z>\nAlso interested, mostly to speed up development\n</Comment>\n<Comment by filipe-m-almeida at 2023-10-12T19:28:45Z>\nYou can start the worker jobs with --device mps and it will use the metal torch backend.\r\n\r\nIt's still a fairly inefficient code path, so you may be better off using the llama.cpp implementation: https://github.com/ggerganov/llama.cpp\n</Comment>\n<Comment by tljstewart at 2023-10-15T20:30:35Z>\n@filipe-m-almeida how would one use llama.ccp instead of this this worker?\r\n\r\n```python\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b --device mps\r\n```\n</Comment>\n<Comment by louis030195 at 2023-10-23T16:51:45Z>\n### llama.cpp\r\n\r\nhttps://github.com/ggerganov/llama.cpp/pull/3436\r\n\r\n`./bin/llava -m ggml-model-q5_k.gguf --mmproj mmproj-model-f16.gguf --image path/to/an/image.jpg`\r\n\r\nusing mac shortcut https://www.reddit.com/r/LocalLLaMA/comments/17aswq4/tutorial_integrate_multimodal_llava_to_macs/\n</Comment>\n<Comment by haotian-liu at 2023-10-31T20:10:26Z>\nmacOS support is updated with quantization coming later. You can try both llama.cpp or the support of macOS in this repo.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 160,
    "state": "open",
    "created_by": "liuguodw",
    "created_at": "2023-05-16T08:02:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/160</URL>\n\n<TITLE>[Question] The answer after fine-tuning appears garbled</TITLE>\n\n<BODY>### Question\n\nHello, I used the pictures I found myself to fine tune the LLaVA 7B model. After fine-tuning, the model's response to my question was incomplete and garbled. May I ask if you have encountered any related issues and how did you solve them?\r\n\r\n你好，我用我自己找的图片对LLaVA 7B模型进行微调。微调后，模型对我的问题的回复不完整，出现乱码。请问你有没有遇到相关问题，又是怎么解决这一问题的呢？\r\n\r\n![屏幕截图 2023-05-16 160137](https://github.com/haotian-liu/LLaVA/assets/60505729/352ca077-e739-421e-9852-d8734eb902d5)</BODY>\n\n<COMMENTS>\n<Comment by RajdeepBorgohain at 2023-05-17T18:07:19Z>\nHey can you share more information which GPU did u used?\n</Comment>\n<Comment by haotian-liu at 2023-05-17T18:13:15Z>\n@liuguodw Hi, what's the base model you use and please share the command, so that we can better understand the base models, datasets that you use. And when did you clone our code base -- the renewable energy in the response may suggest there are some version related issues.\n</Comment>\n<Comment by charush12 at 2023-05-22T16:40:00Z>\nI had fine-tuned 7B and faced the same issue of all the responses being related to renewable energy,\r\nfollowing are the lib versions\r\n\r\ntorch==2.0.1\r\ntorchvision==0.15.2\r\ntransformers @ git+https://github.com/huggingface/transformers@cae78c46d658a8e496a815c2ee49b9b178fb9c9a\r\ndeepspeed==0.9.2\r\n\r\nCommand\r\n\r\nCUDA_VISIBLE_DEVICES=0,1,2,3 \\\r\ntorchrun --nnodes=1 --nproc_per_node=4 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path ./LLaVA-7B-v0 \\\r\n    --data_path LLaVA-Instruct-150K/llava_instruct_150k.json \\\r\n    --image_folder ./Data/train2017 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 1\\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 5000 \\\r\n    --save_total_limit 3 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --report_to none\n</Comment>\n<Comment by haotian-liu at 2023-05-23T02:06:59Z>\nHi @charush12 \r\n\r\nPlease try to rename your model to include \"llava\" in the checkpoint folder, so that the model worker and gradio server can recognize this as a llava model, and use the multimodal prompts.  If you have further questions, please let me know, thanks.\n</Comment>\n<Comment by charush12 at 2023-05-23T06:36:41Z>\n@haotian-liu  Thanks for the quick reply, I renamed the checkpoint folder and the model seems to be working just fine.\r\nAgain thanks for the reply and amazing work on the project\r\n\r\nAll the best\n</Comment>\n<Comment by qiaodongxing at 2025-01-25T13:07:50Z>\n> [@haotian-liu](https://github.com/haotian-liu) Thanks for the quick reply, I renamed the checkpoint folder and the model seems to be working just fine. Again thanks for the reply and amazing work on the project\n> \n> All the best\n\nMay I ask what you changed the folder name to?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 159,
    "state": "closed",
    "created_by": "mmaaz60",
    "created_at": "2023-05-16T07:26:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/159</URL>\n\n<TITLE>Facing error while using FSDP for training</TITLE>\n\n<BODY>Hi @haotian-liu \r\n\r\nI am getting the following errors while using FSDP training with 7B Lightening model.\r\n```\r\nimport flash_attn_cuda                                                                                                                                                                                 \r\nImportError: /share/softwares/anaconda/anaconda3/envs/llava_beta/lib/python3.10/site-packages/flash_attn_cuda.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZNK3c106SymIntltEl\r\n```\r\n\r\n```\r\n**Torch:** 2.1.0.dev20230515+cu118\r\n**CUDA:** Build cuda_11.8.r11.8/compiler.31833905_0\r\n**Flash-attn:** 1.0.5\r\n```\r\n\r\nI have downloaded/installed the LLaVA repo after May 1st.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/55104795/a5be293d-365b-4c7f-921e-21094af560f6)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-17T18:54:52Z>\nHi, thanks for your interest in our work.\r\n\r\nThere seems to be some update in the CUDA drivers in the latest PyTorch nightly, you may need to make sure the CUDA version matchse with the PyTorch CUDA version.  Do you want to do FSDP pretraining, or FSDP finetuning?  If you just want to use the FSDP finetuning, you can use the stable version of PyTorch with CUDA 11.7 with the commands this user from our community provides: https://github.com/haotian-liu/LLaVA/issues/123#issuecomment-1539434115. Thanks!\n</Comment>\n<Comment by mmaaz60 at 2023-05-29T22:13:10Z>\nIt works fine with the stable pytorch 2.0 version as well. Thanks\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 158,
    "state": "closed",
    "created_by": "jihan-yin",
    "created_at": "2023-05-16T06:41:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/158</URL>\n\n<TITLE>[Question] Can we get the pretrained projector weights for llava-7b-v1?</TITLE>\n\n<BODY>### Question\n\nI've been running some experiments around llava-7b-v0 because the pretrained projector weights are not released for 7b-v1. However this has been problematic due to the tokenization issue with vicuna v0, resulting in improper masking of labels during finetuning. Can you make the projector weights available for us to experiment with?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-16T07:00:12Z>\nHi @jihan-yin, sorry for the delayed release.  I have uploaded the projector weights [here](https://huggingface.co/liuhaotian/LLaVA-13b-pretrain-projector-v0/blob/main/LLaVA-7b-pretrain-projector-v1-1-LCS-558K-blip_caption.bin).  Please let me know if you encounter any issues, thanks.\n</Comment>\n<Comment by jihan-yin at 2023-05-16T20:41:18Z>\nThanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 157,
    "state": "open",
    "created_by": "qzhb",
    "created_at": "2023-05-15T14:14:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/157</URL>\n\n<TITLE>[Usage] Questions about fine-tuning the LLaVA-7B model</TITLE>\n\n<BODY>### When did you clone our code?\r\n\r\nI cloned the code base after 5/1/23\r\n\r\n### Describe the issue\r\n\r\nIssue:\r\n\r\nI do not use FSDP since the flash-attention requires A100 or H100 when Head dim > 64. I fine-tune the LLaVA-7B model on two RTX-3090s. It still shows \"out of memory\". So what is the minimum of memory required for fine-tuning the model?\r\n\r\nCommand:\r\n```\r\nCUDA_VISIBLE_DEVICES=0,1 torchrun --nnodes=1 --nproc_per_node=2 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path /data/zhaobo/codes/mm_reasoning/LLaVA/weights/models--lmsys--vicuna-7b-delta-v0/snapshots/f902a2f7e2ca5dfeedf40a0220320e50d2d4fa2a \\\r\n    --data_path /data/zhaobo/codes/mm_reasoning/datasets/LLaVA-Instruct-150K/llava_instruct_150k.json \\\r\n    --image_folder /data/zhaobo/codes/mm_reasoning/datasets/coco2014/train2014 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 4 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 5000 \\\r\n    --save_total_limit 3 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nLog: \r\n```\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 1; 23.70 GiB total capacity; 22.97 GiB already allocated; 8.56 MiB free; 22.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-16T03:39:17Z>\nHi @qzhb and @liuguodw \r\nThank you for your interest in our work.  I am planning to work on supporting our models on more hardwares in this month.  Please stay tuned!\n</Comment>\n<Comment by qzhb at 2023-05-16T07:06:48Z>\nNice work!  I'm looking forward to the new version that supports multiple hardwares.\n</Comment>\n<Comment by Kindpire at 2023-05-30T12:57:36Z>\nDoes anyone know how to finetune on 8 3090?\n</Comment>\n<Comment by YerongLi at 2023-07-19T07:56:27Z>\nDo you know how to  use Lora fine tuning with train_mem.py`? the lora doc start a server.\n</Comment>\n<Comment by YerongLi at 2023-07-19T08:04:24Z>\n> Do you know how to use Lora fine tuning with train_mem.py`? the lora doc start a server.\r\n\r\nscripts/deepspeed/finetune_lora.sh\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 156,
    "state": "closed",
    "created_by": "YuanLiuuuuuu",
    "created_at": "2023-05-15T08:53:23Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/156</URL>\n\n<TITLE>[Question] Instruction tuning data</TITLE>\n\n<BODY>### Question\n\nWhat are ``llava_instruct_80k.json`` and ``llava_instruct_150k.json`` used for? It seems they are not mentioned in the paper.</BODY>\n\n<COMMENTS>\n<Comment by timovr-dev at 2023-05-16T10:11:32Z>\nSeems like they (at least 80k version) were used for visual instruction tuning according to [this](https://github.com/haotian-liu/LLaVA/blob/4682aa6f0a999603f9d21c033328c01bcc0a9f54/scripts/train_lightning_mpt.sh) bash script.\n</Comment>\n<Comment by YuanLiuuuuuu at 2023-05-17T10:41:40Z>\nWhat is the difference between the two files above and those of [conversation_58k.json](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/raw/main/conversation_58k.json), [detail_23k.json](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/raw/main/detail_23k.json) and [complex_reasoning_77k.json](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/raw/main/complex_reasoning_77k.json)\n</Comment>\n<Comment by haotian-liu at 2023-05-17T18:22:23Z>\nHi @YuanLiuuuuuu and @timovr-dev\r\nThank you for your interest in our work. ``llava_instruct_150k.json`` is the merged version of [conversation_58k.json](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/raw/main/conversation_58k.json), [detail_23k.json](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/raw/main/detail_23k.json) and [complex_reasoning_77k.json](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/raw/main/complex_reasoning_77k.json), as described in the paper as\r\n> We collect 158K unique language-image instruction-following samples in total, including 58K in conversations, 23K in detailed description, and 77k in complex reasoning, respectively.\r\n\r\n``llava_instruct_80k.json`` is a filtered subset of ``llava_instruct_150k.json`` after the release of the paper, for training the LLaVA Lightning model. This 80k subset contains 40k conversation and 40k reasoning data, with no image overlap.  We'll update the README to include these clarifications soon.\r\n\r\nPlease let me know if there are other concerns/doubts, thanks.\n</Comment>\n<Comment by YuanLiuuuuuu at 2023-06-05T06:57:08Z>\nThank you for your timely reply.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 155,
    "state": "closed",
    "created_by": "mmaaz60",
    "created_at": "2023-05-15T08:19:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/155</URL>\n\n<TITLE>Can I pretrain the 13B model on 40GB A100 GPU?</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\nI am trying to pretrain the LLaVA-13B-v1.1 model on 8 A100 40 GB GPUs.\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\nSame as mentioned in the README.\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\nIt gave OOM error while attempting to transfer the model on GPU. I am not using FSDP.\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-23T02:25:45Z>\nHi @mmaaz60 can you please paste your command and the logs here?  It will be helpful for understanding the configuration (e.g. batch size, gradient accumulation, dataset, etc.) that can all contribute to the OOM issue.  Thanks!\n</Comment>\n<Comment by mmaaz60 at 2023-06-04T17:14:00Z>\nFSDP is required for 40GB GPU\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 154,
    "state": "closed",
    "created_by": "RunsenXu",
    "created_at": "2023-05-14T15:28:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/154</URL>\n\n<TITLE>[Question] Mask targets</TITLE>\n\n<BODY>### Question\n\nHi, a quick question about the implementation of masking targets in train.py L262.\r\n\r\nWhy does it need to `-2` in `instruction_len = len(tokenizer(parts[0]).input_ids) - 2`?\r\n\r\nThanks!</BODY>\n\n<COMMENTS>\n<Comment by RunsenXu at 2023-05-21T05:58:34Z>\nAnswered here:\r\nlm-sys/fastchat#1272\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 153,
    "state": "closed",
    "created_by": "qzhb",
    "created_at": "2023-05-14T15:15:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/153</URL>\n\n<TITLE>[Usage] Questions about fine-tune the model</TITLE>\n\n<BODY>### When did you clone our code?\r\n\r\nI cloned the code base after 5/1/23\r\n\r\n### Describe the issue\r\n\r\nIssue:\r\n\r\nI ran into the following problems while fine-tuning the llava-7B model on two RTX 3090s. Is this because I did something wrong?\r\n\r\nIssue:\r\n\r\nCommand:\r\n```\r\ntorchrun --nnodes=1 --nproc_per_node=2 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path /data/zhaobo/codes/mm_reasoning/LLaVA/weights/models--lmsys--vicuna-7b-delta-v0/snapshots/f902a2f7e2ca5dfeedf40a0220320e50d2d4fa2a \\\r\n    --data_path /data/zhaobo/codes/mm_reasoning/datasets/LLaVA-Instruct-150K/llava_instruct_150k.json \\\r\n    --image_folder /data/zhaobo/codes/mm_reasoning/datasets/coco2014/train2014 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 16 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 5000 \\\r\n    --save_total_limit 3 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --fsdp \"full_shard auto_wrap\" \\\r\n    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n\r\n```\r\n\r\nLog: \r\n```\r\nWARNING:torch.distributed.run:\r\n*****************************************\r\nSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n*****************************************\r\n/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/transformers/training_args.py:1356: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \r\n  warnings.warn(\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\n/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/transformers/training_args.py:1356: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \r\n  warnings.warn(\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:15<00:00,  7.73s/it]\r\nUsing pad_token, but it is not set yet.\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:17<00:00,  8.91s/it]\r\nUsing pad_token, but it is not set yet.\r\nSome weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_projection.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'visual_projection.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'logit_scale', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.4.layer_norm1.bias']\r\n- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\nSome weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'logit_scale', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_projection.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'visual_projection.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias']\r\n- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\nWARNING:root:Loading data...\r\nWARNING:root:Formatting inputs...Skip in lazy mode\r\nWARNING:root:Loading data...\r\nWARNING:root:Formatting inputs...Skip in lazy mode\r\nwandb: (1) Create a W&B account\r\nwandb: (2) Use an existing W&B account\r\nwandb: (3) Don't visualize my results\r\nwandb: Enter your choice: wandb: Enter your choice: 3\r\nwandb: You chose \"Don't visualize my results\"\r\nwandb: Tracking run with wandb version 0.15.2\r\nwandb: W&B syncing is set to `offline` in this directory.  \r\nwandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\r\n  0%|                                                                                                                                                                             | 0/3696 [00:00<?, ?it/s]Traceback (most recent call last):\r\n  File \"/data/zhaobo/codes/mm_reasoning/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\nTraceback (most recent call last):\r\n    train()\r\n  File \"/data/zhaobo/codes/mm_reasoning/LLaVA/llava/train/train.py\", line 675, in train\r\n      File \"/data/zhaobo/codes/mm_reasoning/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\ntrainer.train()\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/transformers/trainer.py\", line 1644, in train\r\n  File \"/data/zhaobo/codes/mm_reasoning/LLaVA/llava/train/train.py\", line 675, in train\r\n    trainer.train()\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/transformers/trainer.py\", line 1644, in train\r\n    return inner_training_loop(\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/transformers/trainer.py\", line 1909, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/transformers/trainer.py\", line 2675, in training_step\r\n    loss.backward()\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\r\n    torch.autograd.backward(\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\r\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/torch/autograd/function.py\", line 274, in apply\r\n    return user_fn(self, *args)\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 157, in backward\r\n    torch.autograd.backward(outputs_with_grad, args_with_grad)\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\r\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/torch/autograd/function.py\", line 274, in apply\r\n    return user_fn(self, *args)\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py\", line 75, in backward\r\n    _flash_attn_backward(\r\n    return inner_training_loop(\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/transformers/trainer.py\", line 1909, in _inner_training_loop\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py\", line 42, in _flash_attn_backward\r\n    _, _, _, softmax_d = flash_attn_cuda.bwd(\r\nRuntimeError: FlashAttention backward for head dim > 64 requires A100 or H100 GPUs as the implementation needs a large amount of shared memory.\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/transformers/trainer.py\", line 2675, in training_step\r\n    loss.backward()\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\r\n    torch.autograd.backward(\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\r\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/torch/autograd/function.py\", line 274, in apply\r\n    return user_fn(self, *args)\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 157, in backward\r\n    torch.autograd.backward(outputs_with_grad, args_with_grad)\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\r\nwandb: Waiting for W&B process to finish... (failed 1).\r\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/torch/autograd/function.py\", line 274, in apply\r\n    return user_fn(self, *args)\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py\", line 75, in backward\r\n    _flash_attn_backward(\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py\", line 42, in _flash_attn_backward\r\n    _, _, _, softmax_d = flash_attn_cuda.bwd(\r\nRuntimeError: FlashAttention backward for head dim > 64 requires A100 or H100 GPUs as the implementation needs a large amount of shared memory.\r\nwandb: You can sync this run to the cloud by running:ped)\r\nwandb: wandb sync /data/zhaobo/codes/mm_reasoning/LLaVA/wandb/offline-run-20230515_050504-67m3djlj\r\nwandb: Find logs at: ./wandb/offline-run-20230515_050504-67m3djlj/logs\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 4170033 closing signal SIGTERM\r\nERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 4170034) of binary: /home/zhaoboqi/miniconda3/envs/llava_beta/bin/python\r\nTraceback (most recent call last):\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/bin/torchrun\", line 33, in <module>\r\n    sys.exit(load_entry_point('torch==2.0.0', 'console_scripts', 'torchrun')())\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/torch/distributed/run.py\", line 794, in main\r\n    run(args)\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/torch/distributed/run.py\", line 785, in run\r\n    elastic_launch(\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n  File \"/home/zhaoboqi/miniconda3/envs/llava_beta/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \r\n============================================================\r\nllava/train/train_mem.py FAILED\r\n------------------------------------------------------------\r\nFailures:\r\n  <NO_OTHER_FAILURES>\r\n------------------------------------------------------------\r\nRoot Cause (first observed failure):\r\n[0]:\r\n  time      : 2023-05-15_05:05:18\r\n  host      : aa-ESC8000-G4\r\n  rank      : 1 (local_rank: 1)\r\n  exitcode  : 1 (pid: 4170034)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n============================================================\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by qzhb at 2023-05-15T09:19:48Z>\nThis is because Head dim > 64 backward requires A100 or H100 for the flash-attention, where is https://github.com/HazyResearch/flash-attention/issues/190\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 151,
    "state": "open",
    "created_by": "aprilehannibal",
    "created_at": "2023-05-12T08:46:45Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/151</URL>\n\n<TITLE>[Question] finetune dataset</TITLE>\n\n<BODY>### Question\n\nCan you provide your 158k finetune image dataset? Thanks a lot!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-12T14:34:56Z>\nHi @aprilehannibal, please download images directly from COCO website, thanks.\r\n\r\nhttps://cocodataset.org/#download\n</Comment>\n<Comment by aprilehannibal at 2023-05-16T09:28:55Z>\nok, thanks\n</Comment>\n<Comment by dinhanhx at 2023-05-29T04:34:18Z>\n> Hi @aprilehannibal, please download images directly from COCO website, thanks.\r\n> \r\n> https://cocodataset.org/#download\r\n\r\nWhich version you use? 2014 images or 2017 images?\n</Comment>\n<Comment by haotian-liu at 2023-05-29T05:07:40Z>\n@dinhanhx We convert our annotations from the annotations of 2014 split, except the name of the images follow the 2017 convention.  You can download the 2017 images directly.\n</Comment>\n<Comment by dinhanhx at 2023-05-29T06:35:47Z>\nThank you!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 149,
    "state": "closed",
    "created_by": "bofei5675",
    "created_at": "2023-05-12T03:15:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/149</URL>\n\n<TITLE>[Usage] using FSDP leads to libtorch_cuda_cpp.so not found</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\nBy following the docs here https://github.com/haotian-liu/LLaVA/tree/main#experimental-use-fsdp-to-save-memory-in-pretraining\r\n\r\nI create an new environment, and run `torchrun` to do the second step to finetune the model. But I found this error:\r\nCommand:\r\n```\r\nfrom flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func\r\n# throw error\r\nimport flash_attn_cuda\r\nImportError: libtorch_cuda_cpp.so: cannot open shared object file: No such file or directory\r\n```\r\n\r\nPlease kindly advise</BODY>\n\n<COMMENTS>\n<Comment by bofei5675 at 2023-05-12T06:16:32Z>\nFixed, should check server's cuda version. Make sure the cuda version match with pytorch\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 148,
    "state": "open",
    "created_by": "yzxyzh",
    "created_at": "2023-05-11T11:57:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/148</URL>\n\n<TITLE>Error when inference with batchsize > 1</TITLE>\n\n<BODY>### Question\r\n\r\nHi:\r\n\r\nwe use \r\n     output_ids = model.generate(\r\n            input_ids,\r\n            images=image_tensor.unsqueeze(0).half().cuda(),\r\n            do_sample=True,\r\n            temperature=0.2,\r\n            max_new_tokens=1024,\r\n            stopping_criteria=[stopping_criteria])\r\n\r\nto do the inference. it works when input_ids and images both batchsize =1, however it always fail when batchsize is larger than 1. We have removed the stopping criteria, still the error appears. any suggestion on how to fix it?</BODY>\n\n<COMMENTS>\n<Comment by penghe2021 at 2023-05-11T17:43:19Z>\nDo you mean it will give error when image batch size is larger than 1?\r\n\r\nAlso, some error log will be helpful\n</Comment>\n<Comment by Ucas-HaoranWei at 2023-05-13T08:40:21Z>\nmaybe it is the conversation train bug, vicuna also does not support batch>1 inference\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 147,
    "state": "closed",
    "created_by": "JulioZhao97",
    "created_at": "2023-05-11T09:22:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/147</URL>\n\n<TITLE>[Question] Did LLAVA finetune all LLAMA weights or a part of LLAMA weights?</TITLE>\n\n<BODY>### Question\r\n\r\nDid LLAVA finetune all LLAMA weights or just a part of LLAMA weights ? Just wonder. I run the stage2 finetune on SQA and looked the ```param.requires_grad```, it seems that all params in LLAMA is open in finetuning.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/40555727/ead0e8ff-7493-4d95-9597-579b9d8f7367)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-12T14:35:38Z>\nHi, thank you for your interest in our work. For stage-2 finetuning, it finetunes the full model.\n</Comment>\n<Comment by JulioZhao97 at 2023-05-15T01:56:14Z>\n> Hi, thank you for your interest in our work. For stage-2 finetuning, it finetunes the full model.\r\n\r\nThank you for the explaining !\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 146,
    "state": "closed",
    "created_by": "tespio",
    "created_at": "2023-05-11T09:02:35Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/146</URL>\n\n<TITLE>[Question] How can we choose a certain GPU to run it on?</TITLE>\n\n<BODY>### Question\n\nI have 2 * A100 in the server and I would like to specify that LLaVa should use only GPU1 and not GPU0 (since GPU0 has some load on it already while GPU1 is free with 40 GB of VRAM available). the --num_gpus parameter is not usefull obviously since it will try to run it on both GPU's. Would it be possible?</BODY>\n\n<COMMENTS>\n<Comment by tespio at 2023-05-11T12:47:44Z>\nIt's okay i managed to run it by using `export CUDA_VISIBLE_DEVICES=1`. Thank you anyway!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 145,
    "state": "open",
    "created_by": "magicwang1111",
    "created_at": "2023-05-11T06:22:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/145</URL>\n\n<TITLE>Torch not compiled with CUDA enabled[Usage]</TITLE>\n\n<BODY>### When did you clone our code?\r\n\r\nI cloned the code base after 5/1/23\r\n\r\n### Describe the issue\r\n\r\nIssue:AssertionError: Torch not compiled with CUDA enabled\r\n\r\nCommand:python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:8000 --port 40000 --worker http://localhost:40000 --model-path E:/LLaVA-7B-v0 --multi-modal\r\n\r\n\r\n\r\nLog: \r\n```\r\n(llava) E:\\LLaVA>python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:8000 --port 40000 --worker http://localhost:40000 --model-path E:/LLaVA-7B-v0 --multi-modal\r\n2023-05-11 14:21:05 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:8000', model_path='E:/LLaVA-7B-v0', model_name=None, multi_modal=True, keep_aspect_ratio=False, num_gpus=1, limit_model_concurrency=5, stream_interval=2, no_register=False)\r\n2023-05-11 14:21:05 | WARNING | model_worker | Multimodal mode is automatically detected with model name, please make sure `llava` is included in the model path.\r\n2023-05-11 14:21:05 | INFO | model_worker | Loading the model LLaVA-7B-v0 on worker b44a78 ...\r\nSome weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'visual_projection.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_projection.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'logit_scale', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc2.bias']\r\n- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\nLoading checkpoint shards:   0%|                                                                 | 0/2 [00:00<?, ?it/s]\r\nLoading checkpoint shards:  50%|████████████████████████████▌                            | 1/2 [00:02<00:02,  2.28s/it]\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.42s/it]\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.55s/it]\r\n2023-05-11 14:21:10 | ERROR | stderr |\r\nSome weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'visual_projection.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_projection.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'logit_scale', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc2.bias']\r\n- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n2023-05-11 14:21:12 | ERROR | stderr | Traceback (most recent call last):\r\n2023-05-11 14:21:12 | ERROR | stderr |   File \"C:\\Users\\admin\\anaconda3\\envs\\llava\\lib\\runpy.py\", line 196, in _run_module_as_main\r\n2023-05-11 14:21:12 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n2023-05-11 14:21:12 | ERROR | stderr |   File \"C:\\Users\\admin\\anaconda3\\envs\\llava\\lib\\runpy.py\", line 86, in _run_code\r\n2023-05-11 14:21:12 | ERROR | stderr |     exec(code, run_globals)\r\n2023-05-11 14:21:12 | ERROR | stderr |   File \"E:\\LLaVA\\llava\\serve\\model_worker.py\", line 376, in <module>\r\n2023-05-11 14:21:12 | ERROR | stderr |     worker = ModelWorker(args.controller_address,\r\n2023-05-11 14:21:12 | ERROR | stderr |   File \"E:\\LLaVA\\llava\\serve\\model_worker.py\", line 125, in __init__\r\n2023-05-11 14:21:12 | ERROR | stderr |     self.tokenizer, self.model, self.image_processor, self.context_len = load_model(\r\n2023-05-11 14:21:12 | ERROR | stderr |   File \"E:\\LLaVA\\llava\\serve\\model_worker.py\", line 82, in load_model\r\n2023-05-11 14:21:12 | ERROR | stderr |     vision_tower = CLIPVisionModel.from_pretrained(vision_tower.config._name_or_path, torch_dtype=torch.float16, low_cpu_mem_usage=True).cuda()\r\n2023-05-11 14:21:12 | ERROR | stderr |   File \"C:\\Users\\admin\\anaconda3\\envs\\llava\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 905, in cuda\r\n2023-05-11 14:21:12 | ERROR | stderr |     return self._apply(lambda t: t.cuda(device))\r\n2023-05-11 14:21:12 | ERROR | stderr |   File \"C:\\Users\\admin\\anaconda3\\envs\\llava\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 797, in _apply\r\n2023-05-11 14:21:12 | ERROR | stderr |     module._apply(fn)\r\n2023-05-11 14:21:12 | ERROR | stderr |   File \"C:\\Users\\admin\\anaconda3\\envs\\llava\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 797, in _apply\r\n2023-05-11 14:21:12 | ERROR | stderr |     module._apply(fn)\r\n2023-05-11 14:21:12 | ERROR | stderr |   File \"C:\\Users\\admin\\anaconda3\\envs\\llava\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 797, in _apply\r\n2023-05-11 14:21:12 | ERROR | stderr |     module._apply(fn)\r\n2023-05-11 14:21:12 | ERROR | stderr |   File \"C:\\Users\\admin\\anaconda3\\envs\\llava\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 820, in _apply\r\n2023-05-11 14:21:12 | ERROR | stderr |     param_applied = fn(param)\r\n2023-05-11 14:21:12 | ERROR | stderr |   File \"C:\\Users\\admin\\anaconda3\\envs\\llava\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 905, in <lambda>\r\n2023-05-11 14:21:12 | ERROR | stderr |     return self._apply(lambda t: t.cuda(device))\r\n2023-05-11 14:21:12 | ERROR | stderr |   File \"C:\\Users\\admin\\anaconda3\\envs\\llava\\lib\\site-packages\\torch\\cuda\\__init__.py\", line 239, in _lazy_init\r\n2023-05-11 14:21:12 | ERROR | stderr |     raise AssertionError(\"Torch not compiled with CUDA enabled\")\r\n2023-05-11 14:21:12 | ERROR | stderr | AssertionError: Torch not compiled with CUDA enabled\r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/131337141/2541152c-6a2b-4286-b858-e35115eb7557)</BODY>\n\n<COMMENTS>\n<Comment by magicwang1111 at 2023-05-11T06:36:09Z>\ncudaversion v11.8\n</Comment>\n<Comment by penghe2021 at 2023-05-11T17:51:11Z>\nWhat is your torch version, \r\n\r\nAlso, have you tried reinstall the env.\n</Comment>\n<Comment by haotian-liu at 2023-05-13T18:21:17Z>\nHi, you may try reinstalling PyTorch with CUDA 11.7, with the commands this user from our community provides: https://github.com/haotian-liu/LLaVA/issues/123#issuecomment-1539434115.\r\n\r\n@penghe2021 thanks for helping answering these questions :)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 144,
    "state": "open",
    "created_by": "yaoyu-33",
    "created_at": "2023-05-11T00:14:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/144</URL>\n\n<TITLE>[Question] `cur_input_embeds = cur_input_embeds + (0. * dummy_image_features).sum()` seems to do nothing</TITLE>\n\n<BODY>### Question\n\nThis line seems to do nothing? https://github.com/haotian-liu/LLaVA/blob/b080bb98b7445a5a11aecec44c9bf9ab1c5feefc/llava/model/llava.py#L142 Is it a placeholder or anything?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-06-03T04:22:58Z>\nHi, I was doing this in the early development stage to make the gradients to pass through for all modules (when the unlucky case where there is no image in a batch for ScienceQA) so that there is no need for setting `find_unused_parameters=True`.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 143,
    "state": "open",
    "created_by": "gordonhu608",
    "created_at": "2023-05-10T18:39:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/143</URL>\n\n<TITLE>[Discussion] Pretraining Dataset</TITLE>\n\n<BODY>### Discussion\n\nThanks for your great work for LLaVA Lightning. I noticed that you used  LAION/CC/SBU BLIP-Caption Concept-balanced 558K instead of the previously used CC-3M Concept-balanced 595K. Which data set leads to better performance in the pertaining stage? And by the way for the 558k data, do we also expect an upload of the filtered 558k images.zip like for 595k? Thank you!</BODY>\n\n<COMMENTS>\n<Comment by PoseidomWong at 2023-05-12T02:27:39Z>\nI'm looking forward to 558k images.zip, too. Thank you\n</Comment>\n<Comment by liuguodw at 2023-05-12T13:07:01Z>\nI'm looking forward to 558k images.zip, too. Thank you\n</Comment>\n<Comment by haotian-liu at 2023-05-25T19:57:12Z>\n@gordonhu608 @PoseidomWong  @liuguodw \r\n\r\nHi, thank you for your interest in our work. We have uploaded the images [here](https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain/blob/main/images.zip).\r\n\r\nImportant notice: Upon the request from the community, as ~15% images of the original LAION/CC/SBU dataset are no longer accessible, we upload images.zip for better reproducing our work in research community. It should not be used for any other purpose. The use of these images must comply with the LAION/CC/SBU license. This may be taken down when requested by the original LAION/CC/SBU dataset owner or owners of the referenced images.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 142,
    "state": "open",
    "created_by": "dejianchen-x",
    "created_at": "2023-05-10T11:32:45Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/142</URL>\n\n<TITLE>[Question] Difference between 13b-v0 and 13b-v1-1</TITLE>\n\n<BODY>### Question\n\nThanks for this wonderful work!\r\n\r\nI have noticed that weights of \"LLaVA-13b-delta-v1-1\" is released in huggingface, compared to former version v0.\r\n\r\nWhat's the difference of v1-1, compared to v0? Regarding to training data, model, traing process, or just training time?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-12T14:38:09Z>\nHi @dejianchen1989, thank you for your interest in our work. The difference is the base model switched from Vicuna-v0 to Vicuna-v1-1.  Besides, the prompts are changed to a more standard one, and the tokenizer issue has been well-addressed in Vicuna v1.1.\n</Comment>\n<Comment by RunsenXu at 2023-05-13T16:02:33Z>\nDear Liu,\r\n\r\nIf I want to reproduce LLaVA, which one am I recommended to use, Vicuna-v1.1 or Vicuna-V0? The repo supports both VicunaV0 and VicunaV1.1, right?\r\n\r\nAlso, which model are you using for your online demo?\r\n\r\nThank you.\n</Comment>\n<Comment by haotian-liu at 2023-05-13T18:25:44Z>\nHi @RunsenXu, the repo supports both models.  The latest online demo is using the V1.1 variant.\r\n\r\nIf you want to reproduce our results on the paper (e.g. ScienceQA), I would recommend using V0 first, as it is the model we used to train/evaluate the numbers on our paper.\r\n\r\nIf you want to mainly focus on improving the multimodal capability, you may start with V1.1, as it is generally following a standard format for the prompts, and has sometimes slightly better performance than V0.\r\n\r\nThanks.\n</Comment>\n<Comment by RunsenXu at 2023-05-14T03:44:27Z>\nI see. Thank you very much!\n</Comment>\n<Comment by aprilehannibal at 2023-06-13T06:49:21Z>\n> If you want to reproduce our results on the paper (e.g. ScienceQA), I would recommend using V0 first, as it is the model we used to train/evaluate the numbers on our paper.\r\n\r\n@haotian-liu Thanks for this wonderful work! Can you show the acc metrics on ScienceQA when using V1.1? Cause when I use vicuna1.1, only get 86.06%.\r\n\r\nvicuna-13b-v1.1 sqa acc:\r\nTotal: 4241, Correct: 3650, Accuracy: 86.06%\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 141,
    "state": "open",
    "created_by": "microhu",
    "created_at": "2023-05-10T11:29:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/141</URL>\n\n<TITLE>[Question] How GPT4 results is obtained on ScienceQA</TITLE>\n\n<BODY>### Question\n\nDear author, after reading the  LLaVA paper， I have a question about the ScienceQA exp part.  The results show that GPT4 results are not quite good and sometimes will fail to give the answer with the reason of insufficient context of images or plots. \r\n\r\nI wonder how the vision information is fed to GPT4? only the text is provided to GPT4 model?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-13T18:16:59Z>\nHi @microhu, thank you for your interest in our work.\r\n\r\nWe do not have access to the multimodal-GPT-4, so we are only able to feed text information into it at this point.\r\n\r\nWe will make this clearer in our writing in the revision, thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 140,
    "state": "open",
    "created_by": "luochuwei",
    "created_at": "2023-05-10T09:09:23Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/140</URL>\n\n<TITLE>[Feature request] Will the llava project support deepspeed for training?</TITLE>\n\n<BODY>### feature\n\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-13T18:15:19Z>\nHi, we have plan for supporting Deepspeed and it is planned for release by the end of May.  Stay tuned!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 138,
    "state": "open",
    "created_by": "yunh-w",
    "created_at": "2023-05-10T07:30:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/138</URL>\n\n<TITLE>How to fine-tune the LLaVA-7b model ?</TITLE>\n\n<BODY>### Question\r\n\r\nHi, thanks on your great work!\r\n\r\nI use the following command to fine-tune the LLaVA-7b  model.\r\n\r\n`$PYTHON  --nnodes=1 --nproc_per_node=8 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path LLaMA-7b-convert \\\r\n    --data_path $data_path \\\r\n    --image_folder $image_folder \\\r\n    --vision_tower $vision_tower  \\\r\n    --pretrain_mm_mlp_adapter LLaVA-7b-pretrain-projector-v0-CC3M-595K-original_caption.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-7B_new \\\r\n    --num_train_epochs 5 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 5 \\\r\n    --save_total_limit 3 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --fsdp \"full_shard auto_wrap\" \\\r\n    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb`\r\n\r\n But three weights are obtained, when your LLaVA-7b weights number is two. And I get error when  I load these fine-tuned weights.  How to fine-tune the  LLaVA-7b ?  Thanks so much!\r\n\r\n<img width=\"1276\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/82502517/5b4d4556-99dc-454d-a816-66c129b18eb6\">\r\n<img width=\"1440\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/82502517/639f8116-d4f5-449d-8d40-a5a1adfcc3d0\">\r\n\r\nOSError: Unable to load weights from pytorch checkpoint file for 'LLaVA-main/checkpoints/llava-7B_new/checkpoint-5/pytorch_model-00003-of-00003.bin' at 'LLaVA-main/checkpoints/llava-7B_new/checkpoint-5/pytorch_model-00003-of-00003.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\r\n\r\nI found that the third model was not saved completely. When saving, it was OOM, but the training did not stop..  Thanks.</BODY>\n\n<COMMENTS>\n<Comment by Chen-Song at 2023-05-10T07:41:58Z>\nMe too!\r\nAfter I finetune 7B, the model I got is three bin files, but what you release is two bin files. The files I get from finetune are all very large, and the total_size in \"pytorch_model.bin.index.json\" is 26970595328, while what you release is only 13485301760.\r\n\r\n<img width=\"873\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/38151697/6a478275-a37f-4189-bf0c-99298cdfe863\">\n</Comment>\n<Comment by haotian-liu at 2023-05-10T14:43:10Z>\nHi @Chen-Song, you may notice that the size of your trained model is roughly 2x the size of the released checkpoints.  This is because `transformers` saves the model weights with `float32`.  When I release the weights, I convert them to `float16` to save storage space / bandwidth.\r\n\r\n@yunh-w Can you share the size of your trained model weights with `ls -lt` like @Chen-Song does?  Thanks.\n</Comment>\n<Comment by codybum at 2023-05-16T15:42:00Z>\n@haotian-liu What is the process to convert float32 to float16?  I have a 13B fine-tuned model that is 50G.\n</Comment>\n<Comment by haotian-liu at 2023-05-16T16:25:51Z>\n@codybum You can use [this script](https://github.com/haotian-liu/LLaVA/blob/main/llava/model/consolidate.py) for compressing the model. Please make sure to set two different paths for the model instead of overwriting the fp32 model and only delete the fp32 source model after verifying the model is working properly.  Thanks.\n</Comment>\n<Comment by anonymous-atom at 2023-10-13T13:01:48Z>\nHow can we fine tune it on custom data, and what's the format of dataset to feed-in ?\n</Comment>\n<Comment by codybum at 2023-10-13T15:48:14Z>\n@anonymous-atom Here is an example dataset: https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/detail_23k.json\r\n\r\nYou just need to take your data and make it conform to this set.  You can then use the build scripts, substituting your datasets as the training set.\n</Comment>\n<Comment by rahulrajpv at 2023-11-18T04:56:32Z>\nhey anyone please share the way to finetune llava full code\n</Comment>\n<Comment by ali7919 at 2024-01-04T20:58:25Z>\n@yunh-w Hi, what hardware did you use?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 137,
    "state": "open",
    "created_by": "tensorboy",
    "created_at": "2023-05-10T05:02:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/137</URL>\n\n<TITLE>[Feature request]</TITLE>\n\n<BODY>### feature\n\nWould you like to consider merging ImageBind to LLAVA, turning it into a really powerful multimodal LLM?\r\n\r\nhttps://github.com/facebookresearch/ImageBind</BODY>\n\n<COMMENTS>\n<Comment by shelbywhite at 2024-02-28T00:03:41Z>\n@tensorboy did anything come of this? Also would love to see this happen considering how very powerful ImageBind is.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 136,
    "state": "open",
    "created_by": "aaahuia",
    "created_at": "2023-05-10T04:04:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/136</URL>\n\n<TITLE>Questions about flash_attn during training[Usage]</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\n\r\nCould you provide the pytorch version, cuda version and flash_attn version\r\nThe version I use：\r\ntorch：1.13.1+cu116\r\ncuda:   12.1(18.04)\r\nflash-attn:  1.0.4\r\n\r\n\r\nCommand:\r\n\r\nCUDA_VISIBLE_DEVICES=\"6,7\" torchrun --nproc_per_node=2 train_mem.py\r\n\r\nLog: \r\n\r\nFile \"/envs/torch_113/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py\", line 5, in <module>\r\n    import flash_attn_cuda\r\nImportError: /envs/torch_113/lib/python3.10/site-packages/flash_attn_cuda.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-10T05:02:27Z>\nHi, can you try the tips provided by this user from the community to use the corresponding PyTorch/CUDA versions: https://github.com/haotian-liu/LLaVA/issues/102#issuecomment-1537465794\r\n\r\nThere seems to be a mismatch between the cuda version between the flash-attn is compiled and your PyTorch.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 134,
    "state": "closed",
    "created_by": "PoseidomWong",
    "created_at": "2023-05-10T03:21:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/134</URL>\n\n<TITLE>[Usage] about Chinese data finetuning</TITLE>\n\n<BODY>### When did you clone our code?\r\n\r\nI cloned the code base after 5/1/23\r\n\r\n### Describe the issue\r\n\r\nIssue:\r\nWhen I fine-tuned the whole network with Chinese data, which is step 3 of the training script you wrote, I got the following error. However, when I use your data to finetune, it doesn't appear. What do you think might be the cause of this error? Looking forward to your reply，thank you.\r\nCommand:\r\n```\r\ntorchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path ./checkpoints/llama-vicuna-7b \\\r\n    --version $WEIGHT_VERSION \\\r\n    --data_path ./data/total_instruct.json \\\r\n    --image_folder /home/wf/LLaVA_Data/COCO/train2014 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/mm_projector/llava-lightning-7b-pretrain.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 5000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --fsdp \"full_shard auto_wrap\" \\\r\n    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/wf/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/home/wf/LLaVA/llava/train/train.py\", line 665, in train\r\n    trainer.train()\r\n  File \"/home/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1644, in train\r\n    return inner_training_loop(\r\n  File \"/home/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1913, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/home/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2659, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/home/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2691, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/home/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/miniconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 748, in forward\r\n    output = self._fsdp_wrapped_module(*args, **kwargs)\r\n  File \"/home/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/wf/LLaVA/llava/model/llava.py\", line 225, in forward\r\n    outputs = self.model(\r\n  File \"/home/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/wf/LLaVA/llava/model/llava.py\", line 149, in forward\r\n    cur_image_features = image_features[cur_image_idx]\r\nIndexError: index 16 is out of bounds for dimension 0 with size 16\r\n\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by loveyao72 at 2023-05-24T09:10:55Z>\nI meet the same error. Did you found the solution?\n</Comment>\n<Comment by PoseidomWong at 2023-05-24T09:12:40Z>\nI delete the data，it works\n</Comment>\n<Comment by Byshev333 at 2023-07-20T07:59:27Z>\nthe reason is data preprocess is wrong。In my case,  query contain more than one “<image>\\n”  of \"\\n<image>\" token\n</Comment>\n<Comment by findalexli at 2023-07-30T16:23:44Z>\nDid you guys delete the questions  with more than '\\n' and it worked? I tried but is still getting the same error @Byshev333 @PoseidomWong can you eleorabote? Thank yo!\n</Comment>\n<Comment by CrazyBrick at 2023-11-27T06:25:25Z>\n@PoseidomWong hi, I met the same error, I don't quite understand what you mean by deleting data,  are you referring to deleting all Chinese datasets？\r\n\r\n@Byshev333 hi, but I only have one '\\n' token, what should I do?\r\n\r\n@findalexli hi,have you solved this problem?\n</Comment>\n<Comment by PoseidomWong at 2023-11-27T06:44:21Z>\n@CrazyBrick It's been too long since I solved it, I remember it seemed like it was caused by some abnormal data at the time, and after deleting it, it was resolved\n</Comment>\n<Comment by CrazyBrick at 2023-11-27T06:50:28Z>\n> @CrazyBrick It's been too long since I solved it, I remember it seemed like it was caused by some abnormal data at the time, and after deleting it, it was resolved\r\n\r\nwhat is abnormal?I don't understand, can you provide some examples?\r\n```\r\n\"conversations\": [\r\n      {\r\n        \"from\": \"human\",\r\n        \"value\": \"<image>\\n这幅画面是什么样的氛围\"\r\n      },\r\n      {\r\n        \"from\": \"gpt\",\r\n        \"value\": \"这幅画面是室内拍摄，女人侧身看向前方，房间内部展现，可见钢琴等家具，光线较暗，人像带有暗部，整体氛围显得安静思索。 \"\r\n      },\r\n    ]\r\n```\r\nis this abnormal?\n</Comment>\n<Comment by PoseidomWong at 2023-11-27T06:56:52Z>\n> > @CrazyBrick It's been too long since I solved it, I remember it seemed like it was caused by some abnormal data at the time, and after deleting it, it was resolved\r\n> \r\n> what is abnormal?I don't understand, can you provide some examples?\r\n> \r\n> ```\r\n> \"conversations\": [\r\n>       {\r\n>         \"from\": \"human\",\r\n>         \"value\": \"<image>\\n这幅画面是什么样的氛围\"\r\n>       },\r\n>       {\r\n>         \"from\": \"gpt\",\r\n>         \"value\": \"这幅画面是室内拍摄，女人侧身看向前方，房间内部展现，可见钢琴等家具，光线较暗，人像带有暗部，整体氛围显得安静思索。 \"\r\n>       },\r\n>     ]\r\n> ```\r\n> \r\n> is this abnormal?\r\n\r\n我无法举例说明，但你可以打印导致这一问题的数据，并对其进行分析或删除。\n</Comment>\n<Comment by CrazyBrick at 2023-11-27T06:58:16Z>\nOK，I see，thank you for your reply~\r\n\r\n\r\n\r\n---Original---\r\nFrom: ***@***.***&gt;\r\nDate: Mon, Nov 27, 2023 14:57 PM\r\nTo: ***@***.***&gt;;\r\nCc: ***@***.******@***.***&gt;;\r\nSubject: Re: [haotian-liu/LLaVA] [Usage] about Chinese data finetuning (Issue#134)\r\n\r\n\r\n\r\n\r\n   \r\n@CrazyBrick It's been too long since I solved it, I remember it seemed like it was caused by some abnormal data at the time, and after deleting it, it was resolved\r\n  \r\nwhat is abnormal?I don't understand, can you provide some examples?\r\n \"conversations\": [       {         \"from\": \"human\",         \"value\": \"<image&gt;\\n这幅画面是什么样的氛围\"       },       {         \"from\": \"gpt\",         \"value\": \"这幅画面是室内拍摄，女人侧身看向前方，房间内部展现，可见钢琴等家具，光线较暗，人像带有暗部，整体氛围显得安静思索。 \"       },     ]  \r\nis this abnormal?\r\n  \r\n我无法举例说明，但你可以打印导致这一问题的数据，并对其进行分析或删除。\r\n \r\n—\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***&gt;\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 133,
    "state": "open",
    "created_by": "sssssshf",
    "created_at": "2023-05-10T00:54:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/133</URL>\n\n<TITLE>AttributeError: 'LlamaModel' object has no attribute 'vision_tower'</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.\r\n\r\nAttributeError: 'LlamaModel' object has no attribute 'vision_tower'</BODY>\n\n<COMMENTS>\n<Comment by penghe2021 at 2023-05-11T01:41:46Z>\nAre you using 13B v0. I had the same problem before, solved it by downloading the latest version of 13B delta.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 132,
    "state": "open",
    "created_by": "Samin100",
    "created_at": "2023-05-09T22:53:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/132</URL>\n\n<TITLE>[Feature request] Support for load_in_8bit</TITLE>\n\n<BODY>### feature\n\nBeing able to run LLaVA in 8-bit mode would allow better support for inference on consumer GPUs due to lower memory requirements. Passing in `load_in_8bit=True` to `from_pretrained` in the `eval/run_llava.py` doesn't work. I'm testing with the 7B v1.1 model. Do you know what might need to be changed in `llama.py` to support 8-bit inference?\r\n\r\n```\r\n# 7B v1.1 model\r\nmodel = LlavaLlamaForCausalLM.from_pretrained(\r\n  model_name, \r\n  low_cpu_mem_usage=True, \r\n  torch_dtype=torch.float16, \r\n  use_cache=True, \r\n  device_map='auto', \r\n  load_in_8bit=True\r\n).cuda()\r\n```\r\nTraceback:\r\n\r\n```\r\n  File \"/home/ubuntu/llava/LLaVA/llava/eval/run_llava.py\", line 191, in <module>\r\n    eval_model(args)\r\n  File \"/home/ubuntu/llava/LLaVA/llava/eval/run_llava.py\", line 146, in eval_model\r\n    output_ids = model.generate(\r\n  File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1462, in generate\r\n    return self.sample(\r\n  File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2478, in sample\r\n    outputs = self(\r\n  File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/ubuntu/llava/LLaVA/llava/model/llava.py\", line 222, in forward\r\n    outputs = self.model(\r\n  File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/ubuntu/llava/LLaVA/llava/model/llava.py\", line 133, in forward\r\n    image_features = self.mm_projector(image_features)\r\n  File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/bitsandbytes/nn/modules.py\", line 320, in forward\r\n    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)\r\n  File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 500, in matmul\r\n    return MatMul8bitLt.apply(A, B, out, bias, state)\r\n  File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\r\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\r\n  File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 322, in forward\r\n    A = A.view(-1, A.shape[-1]).contiguous()\r\nRuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\r\n```</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-07-23T19:00:14Z>\nHi, please check out the latest code base, which supports both 4bit and 8bit inference.\r\n\r\nhttps://github.com/haotian-liu/LLaVA#launch-a-model-worker-4-bit-8-bit-inference-quantized\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 131,
    "state": "open",
    "created_by": "xiezhiweihk",
    "created_at": "2023-05-09T15:11:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/131</URL>\n\n<TITLE>LLaVA-MPT-7b usage</TITLE>\n\n<BODY>python -m llava.serve.controller --host 0.0.0.0 --port 10000\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/LLaVA-Lightning-MPT-7B-preview\r\npython -m llava.serve.gradio_web_server --controller http://localhost:10000\r\n\r\nWhen I run the above commands, the following error occurs\r\n\r\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=10000): Max retries exceeded with url: /refresh_all_workers (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff66b6f09a0>: Failed to establish a new connection: [Errno 111] Connection refused'))</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-09T22:41:53Z>\nHi @xiezhiweihk, are you executing these commands all at once or one by one?  You need to open several terminal windows / tmux panes, and wait for each command finish before running the next command.  Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 130,
    "state": "closed",
    "created_by": "JulioZhao97",
    "created_at": "2023-05-09T10:00:09Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/130</URL>\n\n<TITLE>[Question] Pickle error when perform SQA stage2 finetune</TITLE>\n\n<BODY>### Question\n\nHeloo, thanks on your great work!\r\n\r\nI want to ask where is the ```llava-13b-pretrain-no_im_start_end_token.bin``` model? I go into this repo as you said, but found ```LLaVA-13b-pretrain-projector-v0-CC3M-595K-original_caption-no_im_token.bin```.\r\n\r\nThen I finetune using following command:\r\n\r\n```\r\nsrun -p llm_exp --gres=gpu:8 --quotatype=auto torchrun --nnodes=1 --nproc_per_node=8 --master_port=$RANDOM \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path ~/MiniGPT-4/vicuna_weight \\\r\n    --data_path ./sqa/llava_train_QCM-LEPA.json \\\r\n    --image_folder ./sqa/train \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/mm_projector/LLaVA-13b-pretrain-projector-v0-CC3M-595K-original_caption-no_im_token.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-13b-pretrain-no_im_start_end_token-finetune_scienceqa \\\r\n    --num_train_epochs 12 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 5000 \\\r\n    --save_total_limit 3 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --fsdp \"full_shard auto_wrap\" \\\r\n    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nIt gives this error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/mnt/petrelfs/zhaozhiyuan/mllm/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/mnt/petrelfs/zhaozhiyuan/mllm/LLaVA/llava/train/train.py\", line 439, in train\r\n    mm_projector_weights = torch.load(model_args.pretrain_mm_mlp_adapter, map_location='cpu')\r\n  File \"/mnt/petrelfs/zhaozhiyuan/anaconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py\", line 713, in load\r\n    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\r\n  File \"/mnt/petrelfs/zhaozhiyuan/anaconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py\", line 920, in _legacy_load\r\n    magic_number = pickle_module.load(f, **pickle_load_args)\r\n_pickle.UnpicklingError: invalid load key, '<'.\r\n```\r\n\r\nIt seems that I load the wrong model, could you please provide ```llava-13b-pretrain-no_im_start_end_token.bin``` model?\r\n\r\nThanks so much!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-09T16:58:23Z>\nHi, this is the correct checkpoint.  Can you try downloading this again from [here](https://huggingface.co/liuhaotian/LLaVA-13b-pretrain-projector-v0/blob/main/LLaVA-13b-pretrain-projector-v0-CC3M-595K-original_caption-no_im_token.bin)?  I just tried downloading it myself, and I was able to load it successfully.  Thanks.\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/6631389/9a36b439-0bac-4c4b-bb68-f719c0fb298f)\n</Comment>\n<Comment by JulioZhao97 at 2023-05-10T02:58:06Z>\n> Hi, this is the correct checkpoint. Can you try downloading this again from [here](https://huggingface.co/liuhaotian/LLaVA-13b-pretrain-projector-v0/blob/main/LLaVA-13b-pretrain-projector-v0-CC3M-595K-original_caption-no_im_token.bin)? I just tried downloading it myself, and I was able to load it successfully. Thanks.\r\n> \r\n> ![image](https://user-images.githubusercontent.com/6631389/237167100-9a36b439-0bac-4c4b-bb68-f719c0fb298f.png)\r\n\r\nThanks, the problem is solved. The reason why this error occur is that I use wget to download checkpoint which is easy but silly. Thanks for your patience.\r\n\r\nCan I ask you another question? I keep getting another error:\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/40555727/03ae3d77-8e6b-4456-ae04-2c5ab0d34109)\r\n\r\nI checked this and some say it is a conflict between torch==1.12 and transformers==4.28.0\r\n\r\nBut after I upgrade torch to 1.13, another strange error occurs from flash-attention:\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/40555727/1006f117-eb61-4bab-9105-8f37ba8f8fb7)\r\n\r\nSo can I ask what is your environment? cuda/torch/transformers/ version? Thanks so much\n</Comment>\n<Comment by haotian-liu at 2023-05-10T03:01:02Z>\nHi I use CUDA 11.7 and PyTorch 2.0.  This is an environment that a user provides that works with PyTorch 1.13.1: https://github.com/haotian-liu/LLaVA/issues/102#issuecomment-1537465794.\r\n\r\nPlease note that the transformers version should be [this](https://github.com/haotian-liu/LLaVA#upgrade-to-v01).\r\n\r\nI would recommend create a new environment and reinstall everything, which may be easier.\n</Comment>\n<Comment by JulioZhao97 at 2023-05-10T03:27:50Z>\n> Hi I use CUDA 11.7 and PyTorch 2.0. This is an environment that a user provides that works with PyTorch 1.13.1: [#102 (comment)](https://github.com/haotian-liu/LLaVA/issues/102#issuecomment-1537465794).\r\n> \r\n> Please note that the transformers version should be [this](https://github.com/haotian-liu/LLaVA#upgrade-to-v01).\r\n> \r\n> I would recommend create a new environment and reinstall everything, which may be easier.\r\n\r\nThanks! I try it now, is this transformer version correct?\r\n![image](https://github.com/haotian-liu/LLaVA/assets/40555727/abfe8f76-a1d6-44e2-ab56-dd1db27ae979)\n</Comment>\n<Comment by haotian-liu at 2023-05-10T05:03:21Z>\nyes it is correct.\n</Comment>\n<Comment by JulioZhao97 at 2023-05-10T06:36:41Z>\n> yes it is correct.\r\n\r\nFinally, I am able to perform finetune model using cuda==11.7 and torch==2.1.0, thansk for your patience Liu !\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 129,
    "state": "open",
    "created_by": "xuhzyy",
    "created_at": "2023-05-09T09:25:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/129</URL>\n\n<TITLE>[Question] When training with the llm of vicuna_v1_1, the conversation_lib.default_conversation is set to be conversation_lib.conv_templates[\"vicuna_v1_1\"], while  template_name = \"llava_v1\" in the 'gradio_web_server.py' in this case, does this have any effect?</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-09T22:48:10Z>\nHi @xuhzyy, thank you for the question.  This is intentional as we do not want to include the model names like \"LLaVA\" in training. Thanks.\n</Comment>\n<Comment by YanqiDai at 2023-12-07T02:17:36Z>\nI have a similar confusion: the conversation template used in the fine-tuning stage of llava-v1.5 is conv_vicuna_v1, while conv_llava_v1 is used in the inference files. The only difference between the two is \"user\" and \"human\", why is this? \r\nI'm sorry that I didn't understand your previous answer. I would appreciate a more detailed elucidation. Thanks.\n</Comment>\n<Comment by Zeqiang-Lai at 2023-12-12T13:03:23Z>\nI am very confused about the conversation templates of llava-v1.5 as well(pretraining, fine-tuning, and inference use three different templates), @haotian-liu would appreciate for the response.\r\n\r\n> I have a similar confusion: the conversation template used in the fine-tuning stage of llava-v1.5 is conv_vicuna_v1, while conv_llava_v1 is used in the inference files. The only difference between the two is \"user\" and \"human\", why is this? I'm sorry that I didn't understand your previous answer. I would appreciate a more detailed elucidation. Thanks.\n</Comment>\n<Comment by linhaojia13 at 2024-04-11T05:25:26Z>\n> I have a similar confusion: the conversation template used in the fine-tuning stage of llava-v1.5 is conv_vicuna_v1, while conv_llava_v1 is used in the inference files. The only difference between the two is \"user\" and \"human\", why is this? I'm sorry that I didn't understand your previous answer. I would appreciate a more detailed elucidation. Thanks.\r\n\r\nI am confused with this, too. @haotian-liu \r\n```\r\nconv_vicuna_v1 = Conversation(\r\n    system=\"A chat between a curious **user** and an artificial intelligence assistant. \"\r\n    \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\r\n    roles=(\"USER\", \"ASSISTANT\"),\r\n    version=\"v1\",\r\n    messages=(),\r\n    offset=0,\r\n    sep_style=SeparatorStyle.TWO,\r\n    sep=\" \",\r\n    sep2=\"</s>\",\r\n)\r\nconv_llava_v1 = Conversation(\r\n    system=\"A chat between a curious **human** and an artificial intelligence assistant. \"\r\n           \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\r\n    roles=(\"USER\", \"ASSISTANT\"),\r\n    version=\"v1\",\r\n    messages=(),\r\n    offset=0,\r\n    sep_style=SeparatorStyle.TWO,\r\n    sep=\" \",\r\n    sep2=\"</s>\",\r\n)\r\n\r\n```\r\nIs it a typo?\n</Comment>\n<Comment by darkpromise98 at 2024-07-12T02:11:12Z>\nsome question~\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 128,
    "state": "open",
    "created_by": "xuhzyy",
    "created_at": "2023-05-09T09:23:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/128</URL>\n\n<TITLE>[Question] It is a great job. I wonder if you have implemented the lora traning?</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-09T22:46:48Z>\nHi, LORA training is planned.  Please stay tuned!  Contribution is also welcomed!\n</Comment>\n<Comment by Iceland-Leo at 2023-05-26T04:58:53Z>\n> Hi, LORA training is planned. Please stay tuned! Contribution is also welcomed!\r\n\r\n@haotian-liu Hello, have you finished the LoRA training part?\n</Comment>\n<Comment by haotian-liu at 2023-06-11T23:15:43Z>\nHi @xuhzyy @Iceland-Leo\r\n\r\nLoRA support (preview) and an initial checkpoint I trained is released [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md). Please let me know if there is something unclear in the instruction.\r\n\r\nAlso, if any of you are interested in contributing to the hyperparameter search, please let me know.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 127,
    "state": "closed",
    "created_by": "Gaoyg",
    "created_at": "2023-05-09T07:01:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/127</URL>\n\n<TITLE>[Question] Encounter a problem when we fine-tuning vicuna-7B-v1.1 on V100</TITLE>\n\n<BODY>### Question\n\nI try to fine-tune the vicuna-7B-v1.1 model using the given instruction-following data on 8 gpus of V100. To train on gpus of V100, I make the following adaptations:\r\n1) Reduce the \"per_device_train_batch_size\" to 1 and increase the \"gradient_accumulation_steps\" to 4.\r\n2) Change the bf16 to fp16.\r\n3) Set the tf32 to False.\r\n4) Train the model without flash-attn.\r\n\r\nThe training command is as follows:\r\n\r\n> torchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \\\r\n>     llava/train/train.py \\\r\n>     --model_name_or_path /path/to/vicuna-7b-v1.1 \\\r\n>     --version v1 \\\r\n>     --data_path /path/to/llava_instruct_80k.json \\\r\n>     --image_folder /path/to/COCO2014/train2014 \\\r\n>     --vision_tower openai/clip-vit-large-patch14 \\\r\n>     --pretrain_mm_mlp_adapter ./checkpoints/mm_projector/LLaVA-7b-pretrain-projector-v0-CC3M-595K-original_caption.bin \\\r\n>     --mm_vision_select_layer -2 \\\r\n>     --mm_use_im_start_end True \\\r\n>     --fp16 True \\\r\n>     --output_dir ./checkpoints \\\r\n>     --num_train_epochs 1 \\\r\n>     --per_device_train_batch_size 1 \\\r\n>     --per_device_eval_batch_size 1 \\\r\n>     --gradient_accumulation_steps 4 \\\r\n>     --evaluation_strategy \"no\" \\\r\n>     --save_strategy \"steps\" \\\r\n>     --save_steps 5000 \\\r\n>     --save_total_limit 1 \\\r\n>     --learning_rate 2e-5 \\\r\n>     --weight_decay 0. \\\r\n>     --warmup_ratio 0.03 \\\r\n>     --lr_scheduler_type \"cosine\" \\\r\n>     --logging_steps 1 \\\r\n>     --tf32 False \\\r\n>     --fsdp \"full_shard auto_wrap\" \\\r\n>     --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\r\n>     --dataloader_num_workers 4 \\\r\n>     --model_max_length 2048 \\\r\n>     --gradient_checkpointing True \\\r\n>     --lazy_preprocess True \\\r\n>     --report_to wandb\r\n\r\nBut when the training is done, I found the weights of some modules of LLM are all zero (even the linear head) shown as follows:\r\n<img width=\"1764\" alt=\"image\" src=\"https://user-images.githubusercontent.com/14977393/237016871-c7daaae4-7a1a-40e7-9f5c-14fb5634cebe.png\">\r\n\r\nI wonder if this problem is related to the data precision (bf16 or fp16), can you provide some suggestions to address this problem?\r\n\r\nReally appreciate your great work and Look forward your reply.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-09T22:52:45Z>\nHi, I haven't tried using pure fp16 for training, as it may have precision-related issue.  But the way you modify the config seems correct to me.  Maybe you can try to save the checkpoint right after the first iteration to see if the weights are normal.  If so, you may save the checkpoint every, say 100 iterations, and see if it gradually changes to zero or suddenly becomes zero.\r\n\r\nThanks.\n</Comment>\n<Comment by Gaoyg at 2023-05-10T06:11:23Z>\n> Hi, I haven't tried using pure fp16 for training, as it may have precision-related issue. But the way you modify the config seems correct to me. Maybe you can try to save the checkpoint right after the first iteration to see if the weights are normal. If so, you may save the checkpoint every, say 100 iterations, and see if it gradually changes to zero or suddenly becomes zero.\r\n> \r\n> Thanks.\r\n\r\nThanks for your feedbacks. It may be the  problem of CUDA OOM while saving model with fsdp, as described in https://github.com/tatsu-lab/stanford_alpaca/issues/81\n</Comment>\n<Comment by TonyXuQAQ at 2023-07-15T12:26:04Z>\nHi, may I know the way to disable flash-attn for training the model? I also have V100 for experiments. Thanks a lot!\n</Comment>\n<Comment by zjr2000 at 2023-10-08T02:18:44Z>\n> ### Question\r\n> I try to fine-tune the vicuna-7B-v1.1 model using the given instruction-following data on 8 gpus of V100. To train on gpus of V100, I make the following adaptations:\r\n> \r\n> 1. Reduce the \"per_device_train_batch_size\" to 1 and increase the \"gradient_accumulation_steps\" to 4.\r\n> 2. Change the bf16 to fp16.\r\n> 3. Set the tf32 to False.\r\n> 4. Train the model without flash-attn.\r\n> \r\n> The training command is as follows:\r\n> \r\n> > torchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \r\n> > llava/train/train.py \r\n> > --model_name_or_path /path/to/vicuna-7b-v1.1 \r\n> > --version v1 \r\n> > --data_path /path/to/llava_instruct_80k.json \r\n> > --image_folder /path/to/COCO2014/train2014 \r\n> > --vision_tower openai/clip-vit-large-patch14 \r\n> > --pretrain_mm_mlp_adapter ./checkpoints/mm_projector/LLaVA-7b-pretrain-projector-v0-CC3M-595K-original_caption.bin \r\n> > --mm_vision_select_layer -2 \r\n> > --mm_use_im_start_end True \r\n> > --fp16 True \r\n> > --output_dir ./checkpoints \r\n> > --num_train_epochs 1 \r\n> > --per_device_train_batch_size 1 \r\n> > --per_device_eval_batch_size 1 \r\n> > --gradient_accumulation_steps 4 \r\n> > --evaluation_strategy \"no\" \r\n> > --save_strategy \"steps\" \r\n> > --save_steps 5000 \r\n> > --save_total_limit 1 \r\n> > --learning_rate 2e-5 \r\n> > --weight_decay 0. \r\n> > --warmup_ratio 0.03 \r\n> > --lr_scheduler_type \"cosine\" \r\n> > --logging_steps 1 \r\n> > --tf32 False \r\n> > --fsdp \"full_shard auto_wrap\" \r\n> > --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \r\n> > --dataloader_num_workers 4 \r\n> > --model_max_length 2048 \r\n> > --gradient_checkpointing True \r\n> > --lazy_preprocess True \r\n> > --report_to wandb\r\n> \r\n> But when the training is done, I found the weights of some modules of LLM are all zero (even the linear head) shown as follows: <img alt=\"image\" width=\"1764\" src=\"https://user-images.githubusercontent.com/14977393/237016871-c7daaae4-7a1a-40e7-9f5c-14fb5634cebe.png\">\r\n> \r\n> I wonder if this problem is related to the data precision (bf16 or fp16), can you provide some suggestions to address this problem?\r\n> \r\n> Really appreciate your great work and Look forward your reply.\r\n\r\nHi, may I know if you have solved the model-saving issues? I am also trying to use the V100 to train the models. Could you please provide me with more details? I'd appreciate it if you could reply to me.\n</Comment>\n<Comment by Gaoyg at 2023-10-12T05:49:20Z>\n@zjr2000 Please follow the methods described in https://github.com/tatsu-lab/stanford_alpaca/issues/81\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 126,
    "state": "open",
    "created_by": "xdevfaheem",
    "created_at": "2023-05-09T04:50:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/126</URL>\n\n<TITLE>Unexpected ValueError</TITLE>\n\n<BODY>### Question\n\nI ran the llava.eval.run_llava.py with model as \"liuhaotian/LLaVA-Lightning-MPT-7B-preview\".What's The problem here?\r\n\r\n\r\n/content/llava/model/mpt/attention.py:148: UserWarning: Using `attn_impl: torch`. If your model does not use `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.\r\n  warnings.warn('Using `attn_impl: torch`. If your model does not use `alibi` or ' + '`prefix_lm` we recommend using `attn_impl: flash` otherwise ' + 'we recommend using `attn_impl: triton`.')\r\nSome weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'visual_projection.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'logit_scale', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_projection.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.0.mlp.fc2.weight']\r\n- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\r\n│ in <cell line: 1>:1                                                                              │\r\n│                                                                                                  │\r\n│ /content/llava/llava_eval.py:39 in llava_generate                                                │\r\n│                                                                                                  │\r\n│    36 │   tokenizer = AutoTokenizer.from_pretrained(model_name)                                  │\r\n│    37 │                                                                                          │\r\n│    38 │   if \"mpt\" in model_name.lower():                                                        │\r\n│ ❱  39 │   │   model = LlavaMPTForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True,    │\r\n│    40 │   else:                                                                                  │\r\n│    41 │   │   model = LlavaLlamaForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True   │\r\n│    42 │   image_processor = CLIPImageProcessor.from_pretrained(model.config.mm_vision_tower, t   │\r\n│                                                                                                  │\r\n│ /usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2703 in from_pretrained   │\r\n│                                                                                                  │\r\n│   2700 │   │   │   )                                                                             │\r\n│   2701 │   │   │                                                                                 │\r\n│   2702 │   │   │   if model._no_split_modules is None:                                           │\r\n│ ❱ 2703 │   │   │   │   raise ValueError(f\"{model.__class__.__name__} does not support `device_m  │\r\n│   2704 │   │   │   no_split_modules = model._no_split_modules                                    │\r\n│   2705 │   │   │   if device_map not in [\"auto\", \"balanced\", \"balanced_low_0\", \"sequential\"]:    │\r\n│   2706 │   │   │   │   raise ValueError(                                                         │\r\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\r\nValueError: LlavaMPTForCausalLM does not support `device_map='auto'` yet.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 125,
    "state": "open",
    "created_by": "DL-ML",
    "created_at": "2023-05-09T04:03:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/125</URL>\n\n<TITLE>[Question] The codebase for data generation</TITLE>\n\n<BODY>### Question\r\n\r\nHi. Thanks for the public codebase. \r\nWhich part is the complete codebase for data generation?\r\nAnd how to get the five associated captions for an image, as the input of GPT-4?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-09T22:45:40Z>\nHi, thank you for your interest in our work.\r\n\r\nPlease see [here](https://github.com/haotian-liu/LLaVA/tree/main/playground/data/prompts) for the prompts and few-shot examples that we use for data generation.\r\n\r\nThe current version, we use the COCO 2014 dataset for associated captions, images, and bounding boxes.\r\n\r\nThanks.\n</Comment>\n<Comment by leimeng86 at 2023-07-05T08:24:30Z>\nAre you using COCO 2014 training split?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 124,
    "state": "closed",
    "created_by": "dmalikv",
    "created_at": "2023-05-08T19:45:09Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/124</URL>\n\n<TITLE>Issue using the LLaVA-Lightning-MPT-7B-preview</TITLE>\n\n<BODY>### Question\r\n\r\nHi Haotian - I'm trying to deploy the **LLaVA-Lightning-MPT-7B-preview** version so that I can call it via an API into an iOS chatbot app I'm experimenting with. However, the Inference API is turned off on HuggingFace. \r\n\r\nPer your comment on the Lightning 7B - \"unlike other LLaVA models, this model can (should) be used directly without delta weights conversion!\" How would you recommend a first-time AI/ML practitioner deploy this specific model without a) the HF Inference API or b) a direct API? Does this link (for [HF/SageMaker](https://github.com/huggingface/notebooks/blob/main/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb)) - image below - how you would do it? \r\n\r\nAlso a follow up - can the LLaVa-13B run on a CPU? The demo doesn't show which hardware it's running on. \r\n\r\nAppreciate your help!\r\n![Screenshot 2023-05-08 at 1 08 19 PM](https://user-images.githubusercontent.com/124695472/236923671-6f0ea1fd-976d-4a48-bb61-1e4e0878e488.png)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 123,
    "state": "closed",
    "created_by": "fan-chao",
    "created_at": "2023-05-08T11:51:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/123</URL>\n\n<TITLE>While processing image, it reports RuntimeError: GET was unable to find an engine to execute this computation</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\nI use gradio demo to test conversation. There is no problem when using plain text for conversations. \r\nBut it reports \"RuntimeError: GET was unable to find an engine to execute this computation\" while processing image. And all subsequent conversations also showed this error. The model_worker must be restarted to solve this problem. The image I use is what the webpage provides \"What is unusual about this image?\".\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.controller --host 0.0.0.0 --port 10000\r\n\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path weights/llava-13b-v0 --multi-modal\r\n\r\npython -m llava.serve.gradio_web_server --controller http://localhost:10000\r\n```\r\n\r\nLog: \r\n```\r\n2023-05-08 19:28:47 | INFO | model_worker | Register to controller\r\n2023-05-08 19:28:47 | ERROR | stderr | INFO:     Started server process [31603]\r\n2023-05-08 19:28:47 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2023-05-08 19:28:47 | ERROR | stderr | INFO:     Application startup complete.\r\n2023-05-08 19:28:47 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)\r\n2023-05-08 19:29:02 | INFO | model_worker | Send heart beat. Models: ['llava-13b-v0']. Semaphore: None. global_counter: 0\r\n2023-05-08 19:29:17 | INFO | model_worker | Send heart beat. Models: ['llava-13b-v0']. Semaphore: None. global_counter: 0\r\n2023-05-08 19:29:20 | INFO | model_worker | Send heart beat. Models: ['llava-13b-v0']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n2023-05-08 19:29:20 | INFO | stdout | INFO:     127.0.0.1:48850 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\n2023-05-08 19:29:28 | INFO | model_worker | Send heart beat. Models: ['llava-13b-v0']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n2023-05-08 19:29:32 | INFO | model_worker | Send heart beat. Models: ['llava-13b-v0']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n2023-05-08 19:29:47 | INFO | model_worker | Send heart beat. Models: ['llava-13b-v0']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n2023-05-08 19:29:52 | INFO | model_worker | Send heart beat. Models: ['llava-13b-v0']. Semaphore: Semaphore(value=4, locked=False). global_counter: 2\r\n2023-05-08 19:29:52 | INFO | stdout | INFO:     127.0.0.1:35472 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\n2023-05-08 19:29:52 | ERROR | stderr | ERROR:    Exception in ASGI application\r\n2023-05-08 19:29:52 | ERROR | stderr | Traceback (most recent call last):\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py\", line 428, in run_asgi\r\n2023-05-08 19:29:52 | ERROR | stderr |     result = await app(  # type: ignore[func-returns-value]\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 78, in __call__\r\n2023-05-08 19:29:52 | ERROR | stderr |     return await self.app(scope, receive, send)\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/fastapi/applications.py\", line 276, in __call__\r\n2023-05-08 19:29:52 | ERROR | stderr |     await super().__call__(scope, receive, send)\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/applications.py\", line 122, in __call__\r\n2023-05-08 19:29:52 | ERROR | stderr |     await self.middleware_stack(scope, receive, send)\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 184, in __call__\r\n2023-05-08 19:29:52 | ERROR | stderr |     raise exc\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 162, in __call__\r\n2023-05-08 19:29:52 | ERROR | stderr |     await self.app(scope, receive, _send)\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\r\n2023-05-08 19:29:52 | ERROR | stderr |     raise exc\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\r\n2023-05-08 19:29:52 | ERROR | stderr |     await self.app(scope, receive, sender)\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 21, in __call__\r\n2023-05-08 19:29:52 | ERROR | stderr |     raise e\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\r\n2023-05-08 19:29:52 | ERROR | stderr |     await self.app(scope, receive, send)\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/routing.py\", line 718, in __call__\r\n2023-05-08 19:29:52 | ERROR | stderr |     await route.handle(scope, receive, send)\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/routing.py\", line 276, in handle\r\n2023-05-08 19:29:52 | ERROR | stderr |     await self.app(scope, receive, send)\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/routing.py\", line 69, in app\r\n2023-05-08 19:29:52 | ERROR | stderr |     await response(scope, receive, send)\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/responses.py\", line 270, in __call__\r\n2023-05-08 19:29:52 | ERROR | stderr |     async with anyio.create_task_group() as task_group:\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 662, in __aexit__\r\n2023-05-08 19:29:52 | ERROR | stderr |     raise exceptions[0]\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/responses.py\", line 273, in wrap\r\n2023-05-08 19:29:52 | ERROR | stderr |     await func()\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/responses.py\", line 262, in stream_response\r\n2023-05-08 19:29:52 | ERROR | stderr |     async for chunk in self.body_iterator:\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/concurrency.py\", line 63, in iterate_in_threadpool\r\n2023-05-08 19:29:52 | ERROR | stderr |     yield await anyio.to_thread.run_sync(_next, iterator)\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/anyio/to_thread.py\", line 31, in run_sync\r\n2023-05-08 19:29:52 | ERROR | stderr |     return await get_asynclib().run_sync_in_worker_thread(\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n2023-05-08 19:29:52 | ERROR | stderr |     return await future\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\r\n2023-05-08 19:29:52 | ERROR | stderr |     result = context.run(func, *args)\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/concurrency.py\", line 53, in _next\r\n2023-05-08 19:29:52 | ERROR | stderr |     return next(iterator)\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/data1/fc/LLaVA/llava/serve/model_worker.py\", line 306, in generate_stream_gate\r\n2023-05-08 19:29:52 | ERROR | stderr |     for x in self.generate_stream(params):\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 35, in generator_context\r\n2023-05-08 19:29:52 | ERROR | stderr |     response = gen.send(None)\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/data1/fc/LLaVA/llava/serve/model_worker.py\", line 251, in generate_stream\r\n2023-05-08 19:29:52 | ERROR | stderr |     out = model(\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-05-08 19:29:52 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2023-05-08 19:29:52 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/data1/fc/LLaVA/llava/model/llava.py\", line 223, in forward\r\n2023-05-08 19:29:52 | ERROR | stderr |     outputs = self.model(\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-05-08 19:29:52 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/data1/fc/LLaVA/llava/model/llava.py\", line 126, in forward\r\n2023-05-08 19:29:52 | ERROR | stderr |     image_forward_outs = vision_tower(images, output_hidden_states=True)\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-05-08 19:29:52 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 934, in forward\r\n2023-05-08 19:29:52 | ERROR | stderr |     return self.vision_model(\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-05-08 19:29:52 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 859, in forward\r\n2023-05-08 19:29:52 | ERROR | stderr |     hidden_states = self.embeddings(pixel_values)\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-05-08 19:29:52 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 195, in forward\r\n2023-05-08 19:29:52 | ERROR | stderr |     patch_embeds = self.patch_embedding(pixel_values)  # shape = [*, width, grid, grid]\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-05-08 19:29:52 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 463, in forward\r\n2023-05-08 19:29:52 | ERROR | stderr |     return self._conv_forward(input, self.weight, self.bias)\r\n2023-05-08 19:29:52 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\r\n2023-05-08 19:29:52 | ERROR | stderr |     return F.conv2d(input, weight, bias, self.stride,\r\n2023-05-08 19:29:52 | ERROR | stderr | RuntimeError: GET was unable to find an engine to execute this computation\r\n\r\n```\r\n\r\nScreenshots:\r\n![image](https://user-images.githubusercontent.com/10006853/236814943-7049c28f-0be0-4b27-b9e4-a740b6e05e4d.png)\r\n\r\n![image](https://user-images.githubusercontent.com/10006853/236815272-3d5264b6-96da-4037-87d8-be102bdafe64.png)\r\n\r\n![image](https://user-images.githubusercontent.com/10006853/236815338-9b72751f-65d2-48a9-9020-f15fdc115597.png)\r\n\r\n![image](https://user-images.githubusercontent.com/10006853/236815672-d389b855-50e6-4add-bd5c-1a205475210f.png)\r\n\r\n![image](https://user-images.githubusercontent.com/10006853/236815770-d567ea54-da08-405f-9a9f-fdadf423580a.png)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-08T23:55:32Z>\nHi, there is another user who met a similar issue, which seems to be related to CUDA/PyTorch versions.  He has provided detailed solution on how he solved the issue.  Can you try [this](https://github.com/haotian-liu/LLaVA/issues/102#issuecomment-1537465794)?\r\n\r\nMeanwhile, would you please share your CUDA version, GPU type, GCC version, OS version?  Thanks.\n</Comment>\n<Comment by fan-chao at 2023-05-09T01:59:01Z>\nHi, the detail version info is as below:\r\nDriver Version: 510.47.03\r\nCUDA version: 11.7\r\nGPU type: Tesla V100 SXM2\r\nGCC version: 7.5.0\r\nOS version: Ubuntu 18.04\r\ntorch version: 2.0.0\r\ntransformers version: 4.28.0.dev0\n</Comment>\n<Comment by fan-chao at 2023-05-09T05:30:26Z>\nThanks for your useful info about downgrading torch version.\r\nIt works after downgrading the torch version to 1.13.1 and setting the CUDA version to 11.7. I used the following command to reinstall pytorch.\r\n`\r\npip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117\r\n`\n</Comment>\n<Comment by JulioZhao97 at 2023-05-10T03:17:11Z>\n> Thanks for your useful info about downgrading torch version. It works after downgrading the torch version to 1.13.1 and setting the CUDA version to 11.7. I used the following command to reinstall pytorch. `pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117`\r\n\r\nHi, can I ask you could you run ```import flash_attn_cuda```\r\n\r\nI have the same env as you, but the flash_attn_cuda can not be imported.\n</Comment>\n<Comment by fan-chao at 2023-05-10T03:30:26Z>\n> > Thanks for your useful info about downgrading torch version. It works after downgrading the torch version to 1.13.1 and setting the CUDA version to 11.7. I used the following command to reinstall pytorch. `pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117`\r\n> \r\n> Hi, can I ask you could you run `import flash_attn_cuda`\r\n> \r\n> I have the same env as you, but the flash_attn_cuda can not be imported.\r\n\r\nHi, I only used the demo provided by the author and did not modify any code. Btw, was flash-attn installed successfully? In additional, another user seems to have encountered a problem with flash-attn, and you can also refer to [his solution](https://github.com/haotian-liu/LLaVA/issues/102#issuecomment-1537465794?).\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 122,
    "state": "closed",
    "created_by": "copperwiring",
    "created_at": "2023-05-08T09:44:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/122</URL>\n\n<TITLE>[Question] Why does CLI inference give different result than the gradio served UI result?</TITLE>\n\n<BODY>### Question\r\n\r\nI am using an image which looks like this: https://archive.md/JPiRZ . When I upload the image on the webpage, I see this\r\n\r\n![image](https://user-images.githubusercontent.com/23316280/236790731-72435242-8b84-4f23-9e62-63b2a7f969e2.png)\r\n \r\nbut when I use inference on CLI as mentioned here, by using command:\r\n\r\n`python -m llava.eval.run_llava --model-name LLaVA-7B-v0 --image-file sample.jpg --query \"Describe the scene in the image\" `, I get few words description \r\n\r\n`© 2018 The New York Times Company`. Running this inference CLI multiple times gives different results but very similar (and wrong) e.g. `© 2015 Bloomberg Finance LP` and , snapshot below\r\n\r\n![image](https://user-images.githubusercontent.com/23316280/236791946-4ceee5a5-54a7-490a-b8ed-dcddf53b085d.png)\r\n\r\n![image](https://user-images.githubusercontent.com/23316280/236792587-41fb53f1-4b25-422b-ad13-301fd37d2210.png)\r\n\r\nI know I am using 7B-v0 for CLI and 7B-lightning for UI example (because it didnt have 7B-v0), I would either ways expect more detailed response than just few words.</BODY>\n\n<COMMENTS>\n<Comment by copperwiring at 2023-05-08T12:31:17Z>\nNever mind, I had to pull he latest changes ,it works better now.\n</Comment>\n<Comment by penghe2021 at 2023-05-09T03:37:42Z>\nAlso pay attention to the sample parameters, the web demo uses Max output tokens: 512, which is different in CLI example\n</Comment>\n<Comment by copperwiring at 2023-05-09T10:58:53Z>\nAh! good catch\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 121,
    "state": "closed",
    "created_by": "liuguodw",
    "created_at": "2023-05-08T06:20:12Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/121</URL>\n\n<TITLE>[Question] Can the fine-tuning of the 7B model be done on an A100 40G?</TITLE>\n\n<BODY>### Question\n\nHi, I tried to fine-tune the 7B model with an A100 40G graphics card and failed. Program display memory is insufficient. My English is not good. Please forgive me if I have some problems with the English description.\r\n\r\n你好，我尝试用一张A100 40G显卡对7B模型进行微调，失败了。 程序显示显存不足。 我的英语不好，如果英文描述的有问题，请见谅。</BODY>\n\n<COMMENTS>\n<Comment by liuguodw at 2023-05-08T06:28:21Z>\n![1](https://user-images.githubusercontent.com/60505729/236750486-68355efe-e789-4ef3-95af-20fc711c2ef9.png)\r\n![2](https://user-images.githubusercontent.com/60505729/236750518-5a94f2fd-eb75-40e6-ae9a-a7dda3003155.png)\r\n![3](https://user-images.githubusercontent.com/60505729/236750525-3537dd2f-52f6-44ca-a23e-082aa35fa5e7.png)\r\n![4](https://user-images.githubusercontent.com/60505729/236750529-dca3e797-53da-403d-88e6-f89df6efb20a.png)\r\n![5](https://user-images.githubusercontent.com/60505729/236750533-8767c855-f7d4-40b7-af6a-dd03ef467f51.png)\n</Comment>\n<Comment by haotian-liu at 2023-05-08T16:42:27Z>\nHi, thank you for your interest in our work.  Currently it's not supported to finetune with a single A100-40G (see discussion [here](https://github.com/haotian-liu/LLaVA/issues/68#issuecomment-1528652546)).\r\n\r\nYou may try only tuning the MLP layer as in the pretraining stage [here](https://github.com/haotian-liu/LLaVA#fine-tuning-with-local-gpus), and stay tuned for our LORA implementation!\n</Comment>\n<Comment by liuguodw at 2023-05-09T03:56:52Z>\nI see, thanks for your reply!\r\n我明白了，感谢你的回复！\n</Comment>\n<Comment by aprilehannibal at 2023-05-17T03:05:16Z>\n> Hi, thank you for your interest in our work. Currently it's not supported to finetune with a single A100-40G (see discussion [here](https://github.com/haotian-liu/LLaVA/issues/68#issuecomment-1528652546)).\r\n> \r\n> You may try only tuning the MLP layer as in the pretraining stage [here](https://github.com/haotian-liu/LLaVA#fine-tuning-with-local-gpus), and stay tuned for our LORA implementation!\r\n@haotian-liu  can I finetune with 8 * A100-40G ?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 120,
    "state": "closed",
    "created_by": "XipengY",
    "created_at": "2023-05-08T03:36:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/120</URL>\n\n<TITLE>How to load CLIP from local</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\npython llava/train/train_mem.py \\\r\n    --model_name_or_path ./checkpoints/llama-vicuna/7B-convert \\\r\n    --data_path data/cc3m_595k/chat.json \\\r\n    --image_folder data/cc3m_595k/images \\\r\n    --vision_tower ./checkpoints/clip-l \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end \\\r\n    --bf16 False \\\r\n    --output_dir ./checkpoints/llava-7b-pretrain \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 2 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 2400 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 False \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.87s/it]\r\nUsing pad_token, but it is not set yet.\r\n> /root/LLaVA/llava/train/train.py(505)train()\r\n-> if model_args.vision_tower is not None:\r\n(Pdb) c\r\nTraceback (most recent call last):\r\n  File \"/root/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/root/LLaVA/llava/train/train.py\", line 506, in train\r\n    model_vision_dict = model.model.initialize_vision_modules(\r\n  File \"/root/LLaVA/llava/model/llava.py\", line 61, in initialize_vision_modules\r\n    vision_tower = CLIPVisionModel.from_pretrained(vision_tower)\r\n  File \"/root/conda3.10/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2170, in from_pretrained\r\n    config, model_kwargs = cls.config_class.from_pretrained(\r\n  File \"/root/conda3.10/lib/python3.10/site-packages/transformers/models/clip/configuration_clip.py\", line 233, in from_pretrained\r\n    config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n  File \"/root/conda3.10/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 573, in get_config_dict\r\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n  File \"/root/conda3.10/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 628, in _get_config_dict\r\n    resolved_config_file = cached_file(\r\n  File \"/root/conda3.10/lib/python3.10/site-packages/transformers/utils/hub.py\", line 380, in cached_file\r\n    raise EnvironmentError(\r\nOSError: ./checkpoints/clip-l does not appear to have a file named config.json. Checkout 'https://huggingface.co/./checkpoints/clip-l/None' for available files.\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-08T03:58:06Z>\n`OSError: ./checkpoints/clip-l does not appear to have a file named config.json. Checkout`\r\n\r\nHi, can you check if you have the configuration file set properly?\r\n\r\n`./checkpoints/clip-l/config.json`\n</Comment>\n<Comment by XipengY at 2023-05-08T04:10:54Z>\nThanks reply!\r\nSet vision_tower as  \"./checkpoints/clip-l/config.json\" not work.\r\n\r\nAlso set vision_tower as the directory of \"./checkpoints/clip-l\"\r\nmeanwhile, modify line 59 of llava.py as follows:\r\nimport os\r\n#image_processor = CLIPImageProcessor.from_pretrained(vision_tower)\r\nimage_processor = CLIPImageProcessor.from_pretrained(os.path.join(vision_tower, 'preprocessor_config.json'))\n</Comment>\n<Comment by haotian-liu at 2023-05-08T04:50:09Z>\nOh I mean, is `config.json` in this folder `./checkpoints/clip-l/`?  It seems to be saying that it cannot find this file.\n</Comment>\n<Comment by 459737087 at 2024-03-08T03:11:35Z>\nHi! Did you solve the problem\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 119,
    "state": "open",
    "created_by": "aaahuia",
    "created_at": "2023-05-08T03:12:45Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/119</URL>\n\n<TITLE>How to use the lightning-7B-v1-1[Usage]</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\nWhen I use the lightning-7B-v1-1 in the same way as the 13b-v0, the output will change my prompt, and a prompt will appear randomly\r\nCommand:\r\npython -m llava.eval.model_vqa\r\nLog: \r\n{\"question_id\": 0, \"prompt\": \"Could you describe the contents of this image for me?\", \"text\": \"Human: Hi!\\n\\nAssistant: Hi there! How can I help you today?\", \"answer_id\": \"exo8upb82UKrNjAMBPwixB\"</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-08T05:46:45Z>\nHi, thank you for your feedback.  `model_vqa.py` has not been updated to automatically adapt to new model architecture yet, and I will update that tomorrow, thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 118,
    "state": "closed",
    "created_by": "TheShy-Dream",
    "created_at": "2023-05-08T02:42:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/118</URL>\n\n<TITLE>How can I get llava-13b-v1 parameters</TITLE>\n\n<BODY>### Question\n\nHello author, I have found that llava-13b-v1 has stronger image annotation capabilities than llava13b-v0, so what should I do to obtain the weights of llava-13b-v1。\r\nThe first image is from [demo](https://llava.hliu.cc/) using llava-13b-v1\r\n![image](https://user-images.githubusercontent.com/58178926/236722103-8c201ef2-5cc9-4886-98b5-549094619a69.png)\r\nThe second image is from my webui using llava-13b-v0\r\n![image](https://user-images.githubusercontent.com/58178926/236722142-d04ce5c5-ae87-4733-8700-de5c37f96a5d.png)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-08T05:47:13Z>\nHi, thank you for your interest in our work.  We'll release v1 weights this week, thanks!\n</Comment>\n<Comment by haotian-liu at 2023-05-08T23:52:40Z>\nHi, it's released [here](https://huggingface.co/liuhaotian/LLaVA-13b-delta-v1-1).\n</Comment>\n<Comment by TheShy-Dream at 2023-05-09T08:44:22Z>\nHi,many thanks!!!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 117,
    "state": "open",
    "created_by": "sslx",
    "created_at": "2023-05-07T14:21:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/117</URL>\n\n<TITLE>[Feature request] Content Restrictions</TITLE>\n\n<BODY>### feature\n\nFirst, let me say, fantastic work! This is awesome!\r\nWhile playing around with the model, I noticed it doesn't really deal with certain NSFW content, specifically those involving violence and sexual themes. I understand the base model LLaMA may not have these content restrictions, so perhaps it's due to the training data used to develop the model?\r\nI totally get it - we're all trying to keep things safe and sound. But, hear me out, I think there might be some solid reasons to consider a version without restrictions.\r\n1. Removing restrictions helps keep things neutral, giving users the freedom to explore content as they wish, no moral policing.\r\n2. Describing ALL images helps visually impaired people get the full picture of image-based content, so they can be part of the conversation. Allowing NSFW description would be a game-changer for audio-described videos and captioned images, making video games, movies, & TV shows more inclusive.\r\n3. Rather than refusing to describe the image entirely, the model can output a warning statement first when an image has sensitive content - this will let users choose if they wanna go down that rabbit hole or not.\r\n4. Since we're dealing with text, there's less risk of misuse or harm compared  to AI-generated visual content like deepfakes.\r\n5. More diverse content = stronger AI models. It could lead to improved generalization and effectiveness. This could even help develop better filters for other models too!\r\n6. Unrestricted AI would be a goldmine for all sorts of studies, from psych to human sexuality and beyond, adding to our understanding of ourselves.\r\nSo, what do you think? By lifting content restrictions, it can open the door to all kinds of research, education, and professional uses, all while being responsible with AI tech.\r\nThanks for giving these ideas some thought,</BODY>\n\n<COMMENTS>\n<Comment by Tedy50 at 2023-05-07T21:25:09Z>\nThis is not only content restriction but the models now is very stupid as it cannot understand jokes and constantly talks about safety and other stuff. \r\n\r\nEven in the demo picture it spends half of the time bragging about how it is unsafe to iron clothes standing on the back of the car while driving.\r\nI gave if a picture of children in halloween costumes and it started moralizing about how it is inappropriate to dress as devils because that may hurt someone's religious feelings. \r\nIt looks like it is absolutely has to insert some disclaimer into everything. \r\nThe purpose of this models should be to describe images not to judge them or moralize about the contents\r\nAls if you want to use this model to moderate some forum and it simply refuses to describe some pictures which it decides to be controversial  \r\nWhat is the purpose of that selective blindness? we already see the picture there is nothing to gain by refusing  describe it.\n</Comment>\n<Comment by chigkim at 2023-05-09T12:43:09Z>\nHope other people chime in, but this might be difficult because LLaVA is finetuned with Vicuna which is finetuned with 70K Chat GPT conversations from ShareGPT.\r\nJust a speculation, but Content restriction might be result of Chat GPT data.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 116,
    "state": "closed",
    "created_by": "fan-chao",
    "created_at": "2023-05-07T08:03:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/116</URL>\n\n<TITLE>RuntimeError: probability tensor contains either `inf`, `nan` or element < 0</TITLE>\n\n<BODY>### When did you clone our code?\r\n\r\nI cloned the code base after 5/1/23\r\n\r\n### Describe the issue\r\n\r\nIssue: RuntimeError: probability tensor contains either inf, nan or element < 0\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.controller --host 0.0.0.0 --port 10000\r\n\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path weights/LLaVA-13B-v0 --multi-modal\r\n\r\npython -m llava.serve.test_message --model-name LLaVA-13B-v0 --controller http://localhost:1000\r\n\r\n```\r\n\r\nLog: \r\n```\r\n2023-05-07 15:54:33 | INFO | model_worker | Register to controller\r\n2023-05-07 15:54:33 | ERROR | stderr | INFO:     Started server process [20537]\r\n2023-05-07 15:54:33 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2023-05-07 15:54:33 | ERROR | stderr | INFO:     Application startup complete.\r\n2023-05-07 15:54:33 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)\r\n2023-05-07 15:54:42 | INFO | stdout | INFO:     127.0.0.1:33944 - \"POST /worker_get_status HTTP/1.1\" 200 OK\r\n2023-05-07 15:54:42 | INFO | model_worker | Send heart beat. Models: ['LLaVA-13B-v0']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n2023-05-07 15:54:42 | INFO | stdout | INFO:     127.0.0.1:33960 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\n2023-05-07 15:54:43 | ERROR | stderr | ERROR:    Exception in ASGI application\r\n2023-05-07 15:54:43 | ERROR | stderr | Traceback (most recent call last):\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py\", line 428, in run_asgi\r\n2023-05-07 15:54:43 | ERROR | stderr |     result = await app(  # type: ignore[func-returns-value]\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 78, in __call__\r\n2023-05-07 15:54:43 | ERROR | stderr |     return await self.app(scope, receive, send)\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/fastapi/applications.py\", line 276, in __call__\r\n2023-05-07 15:54:43 | ERROR | stderr |     await super().__call__(scope, receive, send)\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/applications.py\", line 122, in __call__\r\n2023-05-07 15:54:43 | ERROR | stderr |     await self.middleware_stack(scope, receive, send)\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 184, in __call__\r\n2023-05-07 15:54:43 | ERROR | stderr |     raise exc\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 162, in __call__\r\n2023-05-07 15:54:43 | ERROR | stderr |     await self.app(scope, receive, _send)\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\r\n2023-05-07 15:54:43 | ERROR | stderr |     raise exc\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\r\n2023-05-07 15:54:43 | ERROR | stderr |     await self.app(scope, receive, sender)\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 21, in __call__\r\n2023-05-07 15:54:43 | ERROR | stderr |     raise e\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\r\n2023-05-07 15:54:43 | ERROR | stderr |     await self.app(scope, receive, send)\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/routing.py\", line 718, in __call__\r\n2023-05-07 15:54:43 | ERROR | stderr |     await route.handle(scope, receive, send)\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/routing.py\", line 276, in handle\r\n2023-05-07 15:54:43 | ERROR | stderr |     await self.app(scope, receive, send)\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/routing.py\", line 69, in app\r\n2023-05-07 15:54:43 | ERROR | stderr |     await response(scope, receive, send)\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/responses.py\", line 270, in __call__\r\n2023-05-07 15:54:43 | ERROR | stderr |     async with anyio.create_task_group() as task_group:\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 662, in __aexit__\r\n2023-05-07 15:54:43 | ERROR | stderr |     raise exceptions[0]\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/responses.py\", line 273, in wrap\r\n2023-05-07 15:54:43 | ERROR | stderr |     await func()\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/responses.py\", line 262, in stream_response\r\n2023-05-07 15:54:43 | ERROR | stderr |     async for chunk in self.body_iterator:\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/concurrency.py\", line 63, in iterate_in_threadpool\r\n2023-05-07 15:54:43 | ERROR | stderr |     yield await anyio.to_thread.run_sync(_next, iterator)\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/anyio/to_thread.py\", line 31, in run_sync\r\n2023-05-07 15:54:43 | ERROR | stderr |     return await get_asynclib().run_sync_in_worker_thread(\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n2023-05-07 15:54:43 | ERROR | stderr |     return await future\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\r\n2023-05-07 15:54:43 | ERROR | stderr |     result = context.run(func, *args)\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/concurrency.py\", line 53, in _next\r\n2023-05-07 15:54:43 | ERROR | stderr |     return next(iterator)\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/data1/fc/LLaVA/llava/serve/model_worker.py\", line 306, in generate_stream_gate\r\n2023-05-07 15:54:43 | ERROR | stderr |     for x in self.generate_stream(params):\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 35, in generator_context\r\n2023-05-07 15:54:43 | ERROR | stderr |     response = gen.send(None)\r\n2023-05-07 15:54:43 | ERROR | stderr |   File \"/data1/fc/LLaVA/llava/serve/model_worker.py\", line 272, in generate_stream\r\n2023-05-07 15:54:43 | ERROR | stderr |     token = int(torch.multinomial(probs, num_samples=1))\r\n2023-05-07 15:54:43 | ERROR | stderr | RuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n2023-05-07 15:54:48 | INFO | model_worker | Send heart beat. Models: ['LLaVA-13B-v0']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n```\r\n\r\nScreenshots:\r\n![image](https://user-images.githubusercontent.com/10006853/236665486-cd8e924a-48bd-4118-b2ef-5bf404b060e3.png)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-08T05:48:43Z>\nHi @fan-chao, thank you for your interest in our work.  If you directly try running [`run_llava.py`](https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/run_llava.py), is it working correctly?  If so, you may skip this `test_message` step and go ahead directly to launching the gradio demo.  We'll fix the issues with `test message` soon.  Thanks!\n</Comment>\n<Comment by fan-chao at 2023-05-08T11:19:32Z>\nThanks for your suggestion. I found the cause is that I used the wrong base model. It can be fixed by switching to the correct base model.\n</Comment>\n<Comment by peiyixuan at 2023-05-14T09:51:15Z>\n> Thanks for your suggestion. I found the cause is that I used the wrong base model. It can be fixed by switching to the correct base model.\r\n\r\nHi, I wonder what is wrong with your base model. I encountered the same problem.\n</Comment>\n<Comment by fan-chao at 2023-05-16T06:14:09Z>\n> > Thanks for your suggestion. I found the cause is that I used the wrong base model. It can be fixed by switching to the correct base model.\r\n> \r\n> Hi, I wonder what is wrong with your base model. I encountered the same problem.\r\n\r\nThe LLaMA model I firtsly used is not huggingface format. So it reported this error. Later, I used this model https://huggingface.co/huggyllama/llama-13b and it worked.\n</Comment>\n<Comment by kahnchana at 2023-07-03T21:17:01Z>\nHave you been getting this with MPT models? I've been running following code and getting the same error. \r\n\r\n```\r\npython -m llava.eval.run_llava \\\r\n    --model-name \"liuhaotian/LLaVA-Lightning-MPT-7B-preview\" \\\r\n    --image-file \"llava/serve/examples/extreme_ironing.jpg\" \\\r\n    --query \"What is unusual about this image?\"\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 115,
    "state": "closed",
    "created_by": "jihan-yin",
    "created_at": "2023-05-07T06:07:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/115</URL>\n\n<TITLE>Occasional bursts of gibberish tokens when inferencing without an image.</TITLE>\n\n<BODY>### When did you clone our code?\r\n\r\nI cloned the code base after 5/1/23\r\n\r\n### Describe the issue\r\n\r\nI am running into quite a weird issue. I am using the [13b-v0 model](https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0), and am testing llava's language-only functionality. I find that occasionally, the output of the model will have weird tokens, like släktet, spliced in between what otherwise looks like normal output. What's weird is that I can't reproduce this on the [demo site](https://llava.hliu.cc/) when no image is uploaded, with the same prompt.\r\n\r\nCan you try running the following on your setup (or see if there's anything I'm doing wrong) as a sanity test? I have a 100% reproducible example with the following (using the 13b-v0 weights):\r\n\r\n```\r\ntokenizer = AutoTokenizer.from_pretrained(model_path) \r\n# https://github.com/haotian-liu/LLaVA/issues/70\r\ntokenizer.bos_token_id = 0\r\ntokenizer.eos_token_id = 1\r\ntokenizer.pad_token_id = 0\r\nmodel = LlavaLlamaForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map='auto')\r\n\r\n\r\n# Note - the tokenizer already has the special tokens added using the tokenizer config in the model_path\r\ninput_ids = torch.tensor([[0,   319, 13563,  1546,   263, 12758,  5199,   322,   385, 23116,\r\n         21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\r\n           322,  1248,   568,  6089,   304,   278,  5199, 29915, 29879,  5155,\r\n         29889,  2277, 29937, 29950,  7889, 29901, 14350,   263,  5016, 22214,\r\n         13135,  9076, 29892,  5353,   292,   278,   470, 15554, 29915, 29879,\r\n          4180,   322, 12463, 20026,  7271, 29889,  2277, 29937,  7900, 22137,\r\n         29901,  9208,  4646, 29892,   306,   750,   278, 15377,   310,  1098,\r\n          2548,   263,  5016, 22214, 13135, 23425,   278,  1394,   305, 15679,\r\n          5241, 23024,   265,  1387,   316, 11240, 29899,  8179,   417, 15859,\r\n           472,   278,   317,  3498,   553,  3067,   517,  5475,   297, 11240,\r\n         29899,  8179,   417, 29889,   450,   470, 15554, 29892, 18043,   491,\r\n          3219, 16906,   476, 17243,  4485,   678,   436,   265, 29892,  4846,\r\n           385, 21210,   573,   322,  4332,   440,  1218,  4180, 29892, 16384,\r\n          1009,  3682,   284, 19911,   322, 29311,  3381, 29889,    13]]).to(model.device)\r\nprint(tokenizer.decode(input_ids[0]))\r\nout = model(input_ids=input_ids)\r\ntoken_id = torch.argmax(out.logits[0,-1,:])\r\nprint(token_id, tokenizer.decode(token_id))\r\n```\r\n\r\nThis will output:\r\n```\r\n<unk>A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.###Human: Write a symphony concert review, discussing the orchestra's performance and overall audience experience.###Assistant: Last night, I had the pleasure of attending a symphony concert featuring the Orchestre Philharmonique de Monte-Carlo performing at the Salle des Étoiles in Monte-Carlo. The orchestra, conducted by Maestro Karel Mark Chichon, gave an impressive and captivating performance, displaying their exceptional skill and coordination.\r\n\r\ntensor(7651, device='cuda:0') släktet\r\n```\r\n\r\nAs you can see, the most likely token to be outputted at this position is `släktet`. The full generated sequence was as follows:\r\n```\r\n<unk>A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.###Human: Write a symphony concert review, discussing the orchestra's performance and overall audience experience.###Assistant: Last night, I had the pleasure of attending a symphony concert featuring the Orchestre Philharmonique de Monte-Carlo performing at the Salle des Étoiles in Monte-Carlo. The orchestra, conducted by Maestro Karel Mark Chichon, gave an impressive and captivating performance, displaying their exceptional skill and coordination.\r\n släktetil\r\nThe program included well-known pieces such as Beethoven's Symphony No. 5 and Tchaikovsky's 1812 Overture, as well as a few lesser-known works. The audience was delighted and immersed in the rich, complex sounds of the orchestra, with each instrumental section shining through the composition.\r\n\r\nThe acoustics of the Salle des Étoiles concert hall enhanced the overall experience, as the reverberations and resonances throughout the space perfectly complemented the orchestra's sound. The atmosphere was amplified by the impressive organ situated in the venue, which added depth and power to the performance.\r\n\r\nThe Monte-Carlo orchestra's passionate and vibrant interpretation of the music, combined with the exceptional venue, created a memorable and exhilarating experience for the audience. It was a truly remarkable evening, showcasing the beauty of classical music and the talents of the Orchestre Philharmonique\r\n```</BODY>\n\n<COMMENTS>\n<Comment by jihan-yin at 2023-05-07T06:12:56Z>\nAlso, I'm curious about the way you implemented the model serving code - why don't you use huggingface's default generate call? Is it only to support streaming, or are there other changes you've made for llava there as well?\n</Comment>\n<Comment by haotian-liu at 2023-05-08T23:52:24Z>\nHi @jihan-yin, this may be due to the tokenizer issue (I can get the same result as yours in my local env).  You may try our latest checkpoint that is based on Vicuna v1.1, which has resolved lots of these issues.\r\n\r\nhttps://huggingface.co/liuhaotian/LLaVA-13b-delta-v1-1\n</Comment>\n<Comment by jihan-yin at 2023-05-10T00:04:56Z>\nFixed with the new version, thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 114,
    "state": "closed",
    "created_by": "tensorboy",
    "created_at": "2023-05-06T22:56:12Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/114</URL>\n\n<TITLE>[Usage] New MPT Training crashed</TITLE>\n\n<BODY>### When did you clone our code?\r\n\r\nI cloned the code base after 5/1/23\r\n\r\n### Describe the issue\r\n\r\nIssue:\r\n\r\nCommand:\r\n```\r\n# Visual instruction tuning (1 hour)\r\ntorchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path mosaicml/mpt-7b-chat \\\r\n    --version v1 \\\r\n    --data_path /mnt/bd/data-tns-algo-masp-videocaption/experiment/llava_data/LLaVA-Instruct-150K/llava_instruct_80k.json \\\r\n    --image_folder /mnt/bd/data-tns-algo-masp-videocaption/experiment/llava_data/LLaVA-Instruct-150K/train2017 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/mm_projector/llava-lightning-mpt-7b-pretrain.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 5000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --fsdp \"full_shard auto_wrap\" \\\r\n    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n\r\n```\r\n\r\nLog: \r\n```\r\nWARNING:root:Loading data...\r\nWARNING:root:Formatting inputs...Skip in lazy mode\r\nTraceback (most recent call last):\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/experiment/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/experiment/LLaVA/llava/train/train.py\", line 664, in train\r\n    trainer.train()\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/environment/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/trainer.py\", line 1644, in train\r\n    return inner_training_loop(\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/environment/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/trainer.py\", line 1731, in _inner_training_loop\r\n    model = self._wrap_model(self.model_wrapped)\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/environment/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/trainer.py\", line 1451, in _wrap_model\r\n    raise Exception(\"Could not find the transformer layer class to wrap in the model.\")\r\nException: Could not find the transformer layer class to wrap in the model.\r\nWARNING:root:Loading data...\r\nWARNING:root:Loading data...\r\nWARNING:root:Loading data...\r\nWARNING:root:Loading data...\r\nWARNING:root:Formatting inputs...Skip in lazy mode\r\nTraceback (most recent call last):\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/experiment/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/experiment/LLaVA/llava/train/train.py\", line 664, in train\r\n    trainer.train()\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/environment/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/trainer.py\", line 1644, in train\r\n    return inner_training_loop(\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/environment/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/trainer.py\", line 1731, in _inner_training_loop\r\n    model = self._wrap_model(self.model_wrapped)\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/environment/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/trainer.py\", line 1451, in _wrap_model\r\n    raise Exception(\"Could not find the transformer layer class to wrap in the model.\")\r\nException: Could not find the transformer layer class to wrap in the model.\r\nWARNING:root:Formatting inputs...Skip in lazy mode\r\nTraceback (most recent call last):\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/experiment/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/experiment/LLaVA/llava/train/train.py\", line 664, in train\r\n    trainer.train()\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/environment/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/trainer.py\", line 1644, in train\r\n    return inner_training_loop(\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/environment/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/trainer.py\", line 1731, in _inner_training_loop\r\n    model = self._wrap_model(self.model_wrapped)\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/environment/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/trainer.py\", line 1451, in _wrap_model\r\n    raise Exception(\"Could not find the transformer layer class to wrap in the model.\")\r\nException: Could not find the transformer layer class to wrap in the model.\r\nWARNING:root:Formatting inputs...Skip in lazy mode\r\nWARNING:root:Formatting inputs...Skip in lazy mode\r\nTraceback (most recent call last):\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/experiment/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/experiment/LLaVA/llava/train/train.py\", line 664, in train\r\n    trainer.train()\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/environment/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/trainer.py\", line 1644, in train\r\n    return inner_training_loop(\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/environment/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/trainer.py\", line 1731, in _inner_training_loop\r\n    model = self._wrap_model(self.model_wrapped)\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/environment/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/trainer.py\", line 1451, in _wrap_model\r\n    raise Exception(\"Could not find the transformer layer class to wrap in the model.\")\r\nException: Could not find the transformer layer class to wrap in the model.\r\nTraceback (most recent call last):\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/experiment/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/experiment/LLaVA/llava/train/train.py\", line 664, in train\r\n    trainer.train()\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/environment/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/trainer.py\", line 1644, in train\r\n    return inner_training_loop(\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/environment/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/trainer.py\", line 1731, in _inner_training_loop\r\n    model = self._wrap_model(self.model_wrapped)\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/environment/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/trainer.py\", line 1451, in _wrap_model\r\n    raise Exception(\"Could not find the transformer layer class to wrap in the model.\")\r\nException: Could not find the transformer layer class to wrap in the model.\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 299585 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 299586 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 299587 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 299589 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 299590 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 299591 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 299592 closing signal SIGTERM\r\nERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 3 (pid: 299588) of binary: /mnt/bd/data-tns-algo-masp-videocaption/environment/anaconda3/envs/llava/bin/python\r\nTraceback (most recent call last):\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/environment/anaconda3/envs/llava/bin/torchrun\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/environment/anaconda3/envs/llava/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/environment/anaconda3/envs/llava/lib/python3.9/site-packages/torch/distributed/run.py\", line 794, in main\r\n    run(args)\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/environment/anaconda3/envs/llava/lib/python3.9/site-packages/torch/distributed/run.py\", line 785, in run\r\n    elastic_launch(\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/environment/anaconda3/envs/llava/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n  File \"/mnt/bd/data-tns-algo-masp-videocaption/environment/anaconda3/envs/llava/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \r\n============================================================\r\nllava/train/train_mem.py FAILED\r\n------------------------------------------------------------\r\nFailures:\r\n  <NO_OTHER_FAILURES>\r\n------------------------------------------------------------\r\nRoot Cause (first observed failure):\r\n[0]:\r\n  time      : 2023-05-07_06:51:26\r\n  host      : n214-178-206.byted.org\r\n  rank      : 3 (local_rank: 3)\r\n  exitcode  : 1 (pid: 299588)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n```\r\nthe verision of the trainsfoerms:\r\ntransformers==4.28.0.dev0 (installed by: \r\n```\r\npip uninstall transformers\r\npip install git+https://github.com/huggingface/transformers@cae78c46\r\n```\r\nScreenshots:\r\n\r\n<img width=\"998\" alt=\"Screen Shot 2023-05-06 at 3 57 04 PM\" src=\"https://user-images.githubusercontent.com/17189055/236649533-5968f9d4-20cf-429a-bb70-3846126134b2.png\"></BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-06T23:18:26Z>\nHi, sorry for the confusion.  There was a mistake in the train lightning command, and it is fixed in https://github.com/haotian-liu/LLaVA/commit/fee35939829370eef8371a553c57c3b3b4bdeed9. \r\n\r\n```\r\n--fsdp_transformer_layer_cls_to_wrap 'MPTBlock'\r\n```\r\n\r\nPlease change this line and try again. Thanks!\n</Comment>\n<Comment by tensorboy at 2023-05-07T00:23:32Z>\n> fee3593\r\n\r\nyou always response so fast.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 113,
    "state": "open",
    "created_by": "Tongcheng",
    "created_at": "2023-05-06T20:25:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/113</URL>\n\n<TITLE>[Question] Cmd line, sometimes the \"human prompt\" don't match</TITLE>\n\n<BODY>### Question\n\nI have a question when running the command line version of LLaVA, sometimes it seems the prompt printed out isn't exactly my prompt, for example:\r\n![image](https://user-images.githubusercontent.com/6565047/236645275-c731bd08-9a1e-4875-93ba-3214ef1ada37.png)\r\n\r\nYou can see in this image (the \"human: <query>\" printed out is a paraphrase of my query), I wonder if you know where is this \"human:\" prompt printed out? I just want to make sure this command line can have the correct query input.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-07T05:57:03Z>\nHi, thank you for the feedback.\r\n\r\nThe previous implementation was not updated for v1 models.  Now it has been fixed to support all model variants.  Please pull the latest code base and try again. Thanks!\n</Comment>\n<Comment by Tongcheng at 2023-05-08T02:55:15Z>\nThanks a lot @haotian-liu , will try it shortly!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 112,
    "state": "open",
    "created_by": "Bensolz",
    "created_at": "2023-05-06T18:43:50Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/112</URL>\n\n<TITLE>hallucinations</TITLE>\n\n<BODY>### feature\n\nHello just wanted to say that the model works great in general although it seems to have an issue with visual and textual hallucinations for example if i ask the model what color is the car in the image is in an image that has no car in it just hallucinates something up similar things happen with text only requests where the model hallucinates facts all the time and the strange thing is is that it does this more often than vicuna the model it is based on and fixing these issues would make the model a lot more reliable  and could possibly be done though better instruction fine tuning on examples like i listed above\r\n\r\nAnyway thanks for reading  my request i think llava has great potential to become an open source gpt 4 and eventually maybe even surpass it</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-07T05:58:29Z>\nHi @Bensolz, thanks for the interest in our work and for the great feedback. The hallucination is definitely one of the most important weaknesses that we are striving to tackle with.  It is harder to tackle due to the existence of a separate vision encoder.  Please stay tuned for future updates, thanks!\n</Comment>\n<Comment by Bensolz at 2023-05-07T07:31:14Z>\nthanks for the reply interestingly i noticed that mini gpt 4 seems to be less vulnerable to this and that uses Blip-2 s vision encoder so maybe just using a better vision encoder  and the latest checkpoint of vicuna could mitigate the issues\n</Comment>\n<Comment by xdevfaheem at 2023-05-08T02:43:58Z>\nNo Fine Tune Can Fix Hallucination. We need Architecture Redesign.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 111,
    "state": "closed",
    "created_by": "Tongcheng",
    "created_at": "2023-05-06T18:02:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/111</URL>\n\n<TITLE>[Question] What should be the base model for Lightning 7B (v1.1) delta?</TITLE>\n\n<BODY>### Question\n\nI saw the README instruction seems to suggest use LLaMA-7B for the lightning 7B basis, though another issue (https://github.com/haotian-liu/LLaVA/issues/86) seems to suggest we should use Vicuna-7B as basis.\r\n\r\nCould you clarify which one is the right basis model to use for Lightning 7B?\r\n\r\nThanks a lot!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-06T18:19:40Z>\nPlease use the LLaMA-7B as the base.  We train based on Vicuna-7B, but release the delta from LLaMA, so that people do not need to convert the Vicuna first in order to use our checkpoint.  Thanks.\n</Comment>\n<Comment by Tongcheng at 2023-05-06T18:22:06Z>\nI see! Thanks for the fast response @haotian-liu , and appreciate your great work!\n</Comment>\n<Comment by haotian-liu at 2023-05-06T18:25:49Z>\nThanks for your interest in our work :)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 110,
    "state": "closed",
    "created_by": "heaplax",
    "created_at": "2023-05-06T15:39:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/110</URL>\n\n<TITLE>Error when finetuning LLaVA-13B</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base before 5/1/23, but have pulled the latest code base\n\n### Describe the issue\n\nIssue:\r\nI used LLaVA-13B as the model and extracted projector features from it.  So I didn't do any pre-training.\r\nI don't know if it's the right thing to do.\r\n\r\nCommand:\r\n```\r\ntorchrun --nnodes=1 --nproc_per_node=4 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path /nobackup/users/zfchen/zt/LLaVA/checkpoints/llava-13b-pretrain \\\r\n    --data_path /nobackup/users/zfchen/zt/LLaVA/LLaVA-Instruct-150K/llava_instruct_150k.json \\\r\n    --image_folder /nobackup/users/zitian/multi_data/COCO/train2017 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/mm_projector/llava-13b-pretrain.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --bf16 False \\\r\n    --output_dir ./checkpoints \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 5000 \\\r\n    --save_total_limit 3 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 False \\\r\n    --fsdp \"full_shard auto_wrap\" \\\r\n    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nLog: \r\n```\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:49<00:00, 36.62s/it]\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:49<00:00, 36.41s/it]\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:49<00:00, 36.48s/it]\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:50<00:00, 36.71s/it]\r\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\r\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\r\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\r\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\r\nTraceback (most recent call last):\r\n  File \"/nobackup/users/zfchen/zt/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/nobackup/users/zfchen/zt/LLaVA/llava/train/train.py\", line 535, in train\r\n    model.initialize_vision_tokenizer(mm_use_im_start_end=model_args.mm_use_im_start_end, tokenizer=tokenizer, device=training_args.device,\r\n  File \"/nobackup/users/zfchen/zt/LLaVA/llava/model/llava.py\", line 316, in initialize_vision_tokenizer\r\n    assert num_new_tokens == 2\r\nAssertionError\r\n```\r\nI found num_new_tokens was 0.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-06T15:56:51Z>\nHi @heaplax, thank you for the interest in our work.\r\n\r\nI saw the `--model_name_or_path /nobackup/users/zfchen/zt/LLaVA/checkpoints/llava-13b-pretrain` and `--pretrain_mm_mlp_adapter ./checkpoints/mm_projector/llava-13b-pretrain.bin`, when `--pretrain_mm_mlp_adapter` is used, it is expected that `--model_name_or_path` is a base model like Vicuna, instead of LLaVA.  You may also remove `--pretrain_mm_mlp_adapter` if it is already contained in the pretrained checkpoint.\r\n\r\nIf there is something special in your specific usage, please let me know, thanks.\n</Comment>\n<Comment by heaplax at 2023-05-07T04:28:14Z>\n> Hi @heaplax, thank you for the interest in our work.\r\n> \r\n> I saw the `--model_name_or_path /nobackup/users/zfchen/zt/LLaVA/checkpoints/llava-13b-pretrain` and `--pretrain_mm_mlp_adapter ./checkpoints/mm_projector/llava-13b-pretrain.bin`, when `--pretrain_mm_mlp_adapter` is used, it is expected that `--model_name_or_path` is a base model like Vicuna, instead of LLaVA. You may also remove `--pretrain_mm_mlp_adapter` if it is already contained in the pretrained checkpoint.\r\n> \r\n> If there is something special in your specific usage, please let me know, thanks.\r\n\r\nIt works. Thanks a lot!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 109,
    "state": "closed",
    "created_by": "Unrealluver",
    "created_at": "2023-05-06T15:27:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/109</URL>\n\n<TITLE>[Question] ScienceQA inference questions</TITLE>\n\n<BODY>### Question\n\nGreetings! When I run the SQA inference codes. I have a question how the model gets the contexts?\r\n\r\nThe codes in `.llava/eval/eval_science_qa.py` are:\r\n```\r\n        # conv = default_conversation.copy()\r\n        conv = conv_templates[args.conv_mode].copy()\r\n        conv.append_message(conv.roles[0], qs)\r\n        prompt = conv.get_prompt()\r\n        inputs = tokenizer([prompt])\r\n\r\n        input_ids = torch.as_tensor(inputs.input_ids).cuda()\r\n\r\n        keywords = ['###']\r\n        stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\r\n\r\n        with torch.inference_mode():\r\n            output_ids = model.generate(\r\n                input_ids,\r\n                images=images,\r\n                do_sample=True,\r\n                temperature=0.7,\r\n                max_new_tokens=1024,\r\n                stopping_criteria=[stopping_criteria])\r\n\r\n        # TODO: new implementation\r\n        input_token_len = input_ids.shape[1]\r\n        n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\r\n        if n_diff_input_output > 0:\r\n            print(f'[Warning] Sample {i}: {n_diff_input_output} output_ids are not the same as the input_ids')\r\n        outputs = tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]\r\n```\r\n\r\n`prompt` is a string, `You are LLaVA, a large language model trained by UW Madison WAIV Lab, based on LLaMA architecture.You are designed to assist human with a variety of tasks using natural language.Follow the instructions carefully.###Human: Hi!###Assistant: Hi there!  How can I help you today?\r\n###Human: Which figure of speech is used in this text?\r\nSing, O goddess, the anger of Achilles son of Peleus, that brought countless ills upon the Achaeans.\r\n—Homer, The Iliad\r\nContext: N/A\r\nOptions: (A) chiasmus (B) apostrophe###`\r\n\r\n`outputs` is a string, ` Assistant: LECTURE: Figures of speech are words or phrases that use language in a nonliteral or unusual way. They can make writing more expressive.\r\nAnaphora is the repetition of the same word or words at the beginning of several phrases or clauses.\r\nWe are united. We are powerful. We are winners.\r\nAntithesis involves contrasting opposing ideas within a parallel grammatical structure.\r\nI want to help, not to hurt.\r\nApostrophe is a direct address to an absent person or a nonhuman entity.\r\nOh, little bird, what makes you sing so beautifully?\r\nAssonance is the repetition of a vowel sound in a series of nearby words.\r\nTry to light the fire.\r\nChiasmus is an expression in which the second half parallels the first but reverses the order of words.\r\nNever let a fool kiss you or a kiss fool you.\r\nUnderstatement involves deliberately representing something as less serious or important than it really is.\r\nAs you know, it can get a little cold in the Antarctic.\r\nSOLUTION: The text uses apostrophe, a direct address to an absent person or a nonhuman entity.\r\nSing, O goddess is a direct address to the goddess Pallas Athena.\r\n###`\r\n\r\nI noticed that the `gt_ans` of this sample is `{'from': 'gpt', 'value': 'LECTURE: Figures of speech are words or phrases that use language in a nonliteral or unusual way. They can make writing more expressive.\\nAnaphora is the repetition of the same word or words at the beginning of several phrases or clauses.\\nWe are united. We are powerful. We are winners.\\nAntithesis involves contrasting opposing ideas within a parallel grammatical structure.\\nI want to help, not to hurt.\\nApostrophe is a direct address to an absent person or a nonhuman entity.\\nOh, little bird, what makes you sing so beautifully?\\nAssonance is the repetition of a vowel sound in a series of nearby words.\\nTry to light the fire.\\nChiasmus is an expression in which the second half parallels the first but reverses the order of words.\\nNever let a fool kiss you or a kiss fool you.\\nUnderstatement involves deliberately representing something as less serious or important than it really is.\\nAs you know, it can get a little cold in the Antarctic.\\nSOLUTION: The text uses apostrophe, a direct address to an absent person or a nonhuman entity.\\nO goddess is a direct address to a goddess, a nonhuman entity.\\n###\\nANSWER: B.'}`\r\n\r\nMy question is why would the model infer the context that is not seeded to the model? As we can see, except for the last sentence, the model's `output` is totally the same as the `gt_ans`</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-06T18:17:01Z>\nHi @Unrealluver, thank you for your question and for the close inspection.\r\n\r\nI believe this is due to the way ScienceQA is constructed.  If you search for `Figures of speech are words or phrases that use language in a nonliteral or unusual way. They can make writing more expressive` as the keyword in the training set, you'll see this sentence appear multiple times, and this will be followed by different types like `Anaphora`, `Alliteration` etc.\r\n\r\nSo, the model first recognizes the problem type, and refers to the lecture that it has previously seen, and finally, reasons and generates the solution.\r\n\r\nNote that I would say it is almost impossible for us to accidentally mixup the train/test set as the images from train/test/val are in separate folders.\r\n\r\nPlease let me know if there are any concerns, thanks.\n</Comment>\n<Comment by Unrealluver at 2023-05-06T18:48:32Z>\nThanks for your reply. Before I open this issue, I have also checked that there is no data leaking. ^_^\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 108,
    "state": "open",
    "created_by": "vishaal27",
    "created_at": "2023-05-06T13:15:35Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/108</URL>\n\n<TITLE>[Question] Getting output likelihood scores from the model</TITLE>\n\n<BODY>### Question\n\nHi, is it possible to get the tokenwise log-likelihood scores of different outputs from the model?\r\n\r\nThe use-case would be something like:\r\nGiven an interleaved image/text input and a list of output text candidates, we should be able to get a score for each output candidate and then return their ranked list, rather than generating the outputs directly. This would be close to how LLMs are evaluated on MCQ tasks. An example from the T0 paper Page 6 (https://arxiv.org/pdf/2110.08207.pdf):\r\n\r\n```\r\nFor tasks that involve choosing the correct completion from several options (e.g. multiple choice\r\nquestion answering), we follow Brown et al. (2020) and use rank classification to evaluate our\r\nmodel: we compute the log-likelihood of each of the target options under the fine-tuned model and\r\nselect the option with the highest log-likelihood as the prediction. For simplicity, we do not apply\r\nlength normalization to the log-likelihoods of the target options.\r\n```\r\n\r\nIs it straightforward to do this with LLaVA? I assume since the LM is built with transformers there should be a possibility to use output score functions already implemented (haven't dug into this yet)?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-07T06:07:02Z>\nHi @vishaal27, thank you for the great question.  Yes it is easy to do this with LLaVA.\r\n\r\nHere is a simple example that you may start with, by inserting this into [`run_llava.py`](https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/run_llava.py#L98):\r\n\r\n```python\r\ngeneration_output = model.generate(\r\n    input_ids,\r\n    images=image_tensor.unsqueeze(0).half().cuda(),\r\n    do_sample=True,\r\n    temperature=0.2,\r\n    max_new_tokens=1024,\r\n    stopping_criteria=[stopping_criteria],\r\n    # add following two lines\r\n    return_dict_in_generate=True,\r\n    output_scores=True\r\n)\r\n\r\ninput_token_len = input_ids.shape[1]\r\noutput_ids = generation_output.sequences[0, input_token_len:]\r\noutput_scores = generation_output.scores\r\n```\n</Comment>\n<Comment by vishaal27 at 2023-05-07T12:22:57Z>\nThanks @haotian-liu, but as I understand it, this will return the log-likelihood of the generated output given some initial prompt right? I don't want to generate more tokens but rather evaluate the likelihood of a given token sequence under the model, for example if I want to do ImageNet classification with this model I would do something like: evaluate the log-likelihood of the sequence `<image> This is a photo of a {CLASS}` where I would iterate over all classnames and replace {CLASS}  appropriately, and then take the argmax of the log-likelihoods of each class. I guess the code you provided would generate more tokens on top of the `<image> This is a photo of a {CLASS}` sequence, and then return the log-likelihood of the entire sequence right? \r\nPlease correct me if I misunderstood something, thanks!\n</Comment>\n<Comment by haotian-liu at 2023-05-07T21:08:09Z>\n@vishaal27 I think this is also possible.  Consider this following (pseudo) code:\r\n```python\r\nmessage = \"\"\"Human: <image> what is the object in the photo?\r\nGPT: This is a photo of a \"\"\"\r\ninput_ids = tokenizer(message)\r\n```\r\nThe first output token should be the `CLASS` if it is a single-token word, and you can obtain the log likelihood with the code above.  Please correct me if I misunderstand anything, thanks.\n</Comment>\n<Comment by copperwiring at 2024-05-14T21:39:03Z>\n@vishaal27 Did you find the solution to your problem (I know its an old issue). I have a similar issue. I have a set of possible options and I want to computer log prob of those options as the output. When using prompt based method, tokens are generated. In case of single word output, I still get prob of one output not the distribution ob prob over all my possible options (which in your case were classes I think). How did you resolve it?\n</Comment>\n<Comment by vishaal27 at 2024-05-14T21:43:27Z>\nThis code should work:\r\n\r\n```python\r\nfrom llava.constants import (\r\n    IMAGE_TOKEN_INDEX,\r\n    DEFAULT_IMAGE_TOKEN,\r\n    DEFAULT_IM_START_TOKEN,\r\n    DEFAULT_IM_END_TOKEN,\r\n    IMAGE_PLACEHOLDER,\r\n)\r\nfrom llava.conversation import conv_templates, SeparatorStyle\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.utils import disable_torch_init\r\nfrom llava.mm_utils import (\r\n    process_images,\r\n    tokenizer_image_token,\r\n    get_model_name_from_path,\r\n    KeywordsStoppingCriteria,\r\n)\r\nfrom PIL import Image\r\nimport requests\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\nimport re\r\nimport torch\r\nimport numpy as np\r\n\r\ndef image_parser(image_file):\r\n    out = image_file.split(',')\r\n    return out\r\n\r\n\r\ndef load_image(image_file):\r\n    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\r\n        response = requests.get(image_file)\r\n        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\r\n    else:\r\n        image = Image.open(image_file).convert(\"RGB\")\r\n    return image\r\n\r\n\r\ndef load_images(image_files):\r\n    out = []\r\n    for image_file in image_files:\r\n        image = load_image(image_file)\r\n        out.append(image)\r\n    return out\r\n\r\ndef count_all_parameters(model):\r\n    return sum(p.numel() for p in model.parameters())\r\n\r\ndef eval_model(model_path, image_file, query, options):\r\n    # Model\r\n    disable_torch_init()\r\n\r\n    model_name = get_model_name_from_path(model_path)\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(\r\n        model_path, None, model_name\r\n    )\r\n\r\n    qs = query\r\n    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\r\n    if IMAGE_PLACEHOLDER in qs:\r\n        if model.config.mm_use_im_start_end:\r\n            qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\r\n        else:\r\n            qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\r\n    else:\r\n        if model.config.mm_use_im_start_end:\r\n            qs = image_token_se + \"\\n\" + qs\r\n        else:\r\n            qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\r\n\r\n    if \"llama-2\" in model_name.lower():\r\n        conv_mode = \"llava_llama_2\"\r\n    elif \"v1\" in model_name.lower():\r\n        conv_mode = \"llava_v1\"\r\n    elif \"mpt\" in model_name.lower():\r\n        conv_mode = \"mpt\"\r\n    else:\r\n        conv_mode = \"llava_v0\"\r\n\r\n    conv = conv_templates[conv_mode].copy()\r\n    conv.append_message(conv.roles[0], qs)\r\n    conv.append_message(conv.roles[1], None)\r\n    prompt = conv.get_prompt()\r\n\r\n    image_files = image_parser(image_file)\r\n    images = load_images(image_files)\r\n    images_tensor = process_images(\r\n        images,\r\n        image_processor,\r\n        model.config\r\n    ).to(model.device, dtype=torch.float16)\r\n\r\n    log_lik_scores = []\r\n\r\n    for option in options:\r\n\r\n        target_prompt = prompt + ' ' + option\r\n        print(target_prompt)\r\n\r\n        input_ids = (\r\n            tokenizer_image_token(target_prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\r\n            .unsqueeze(0)\r\n            .cuda()\r\n        )\r\n        attention_mask = torch.ones_like(input_ids)\r\n\r\n        with torch.inference_mode(), torch.cuda.amp.autocast():\r\n            outputs = model.forward(\r\n                input_ids=input_ids,\r\n                labels=input_ids,\r\n                attention_mask=attention_mask,\r\n                images=images_tensor,\r\n                )\r\n\r\n        log_lik_scores.append((option, -outputs.loss.item()))\r\n\r\n    pred_id = np.argmax(np.asarray([x[1] for x in log_lik_scores]))\r\n    print(log_lik_scores)\r\n    print('Prediction: {}'.format(log_lik_scores[pred_id]))\r\n\r\nif __name__ == '__main__':    \r\n\r\n    model_path = \"liuhaotian/llava-v1.5-13b\"\r\n\r\n    prompt = \"Describe the image.\"\r\n    image_file = \"https://llava-vl.github.io/static/images/view.jpg\"\r\n\r\n    shared_prompt = 'This is an image of a '\r\n    options = [shared_prompt+x for x in ['horse', 'lion', 'tiger', 'elephant', 'eagle', 'dog']]\r\n\r\n    eval_model(model_path, image_file, prompt, options)\r\n```\n</Comment>\n<Comment by copperwiring at 2024-05-15T09:09:35Z>\nThanks, @vishaal27 It was helpful!\n</Comment>\n<Comment by copperwiring at 2024-05-23T14:05:24Z>\n@vishaal27 Though the answers are correct, I am surprised that probabilities of all options are so close to each other. I computed log likehood and probs \r\n\r\nPrompt was slightly different but same image and I had these options: `['cat', 'river', 'dog', 'Invalid option']` and got following outputs\r\n\r\n```\r\nLog likelihood scores:\r\nAssistant: If had to select one of the options, my answer would be cat: -3.405327081680298\r\nAssistant: If had to select one of the options, my answer would be river: -3.405212163925171\r\nAssistant: If had to select one of the options, my answer would be dog: -3.413139581680298\r\nAssistant: If had to select one of the options, my answer would be Invalid option: -3.4227676391601562\r\n**************************************************\r\nProbabilities:\r\nAssistant: If had to select one of the options, my answer would be cat: 0.2515695733004952\r\nAssistant: If had to select one of the options, my answer would be river: 0.251598484772306\r\nAssistant: If had to select one of the options, my answer would be dog: 0.2496118433492265\r\nAssistant: If had to select one of the options, my answer would be Invalid option: 0.24722009857797245\r\nPrediction: Assistant: If had to select one of the options, my answer would be river with probability 0.251598484772306\r\n```\r\n\r\nDid you get similar scores too?\n</Comment>\n<Comment by vishaal27 at 2024-05-23T14:09:46Z>\nThat could potentially be because your prompts are too long? One option would be to length-normalize your log-likelihood scores with the number of tokens in the prompt. In my experiments this did not make too much of a difference, but if you expect your prompts to be too long or of significantly different token lengths I would recommend to use length-normalized log-likelihoods. For reference, you can see here: https://blog.eleuther.ai/multiple-choice-normalization/\n</Comment>\n<Comment by copperwiring at 2024-05-23T14:13:59Z>\nNot really. I can change the prompt\r\n\r\n```\r\n    prompt = \"\"\"Describe the image. \\n\\n\"\"\"\r\n    shared_prompt = 'This is an image of a '\r\n    options = [shared_prompt+x for x in  ['cat', 'river', 'dog', 'Invalid option']]\r\n\r\n    eval_model(model_path, image_file, prompt, options)\r\n```\r\nand outputs are still similar (very close/uniform(=):\r\n```\r\n\r\nLog likelihood scores:\r\nThis is an image of a cat: -4.218678951263428\r\nThis is an image of a river: -4.173059940338135\r\nThis is an image of a dog: -4.236606121063232\r\nThis is an image of a Invalid option: -4.627951145172119\r\n**************************************************\r\nProbabilities:\r\nThis is an image of a cat: 0.2707795140599804\r\nThis is an image of a river: 0.2834183003354851\r\nThis is an image of a dog: 0.2659684569012282\r\nThis is an image of a Invalid option: 0.17983372870330633\r\nPrediction: This is an image of a river with probability 0.2834183003354851\r\n```\n</Comment>\n<Comment by vishaal27 at 2024-05-23T14:28:07Z>\nYes, however these look quite similar to the scores I was getting. One correction to my earlier comment: the scores are actually length-normalised since internally it uses [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) which by default has `reduction='mean'` set.\r\n\r\nYou could try checking the length-unnormalised scores by:\r\n```python\r\n        log_lik_scores.append((option, -outputs.loss.item() * input_ids.shape[1]))\r\n```\r\ninstead of \r\n```python\r\n        log_lik_scores.append((option, -outputs.loss.item()))\r\n```\r\n\r\nHowever, I wouldn't expect to see too much of a difference. In general, these scores that you get seem similar to the scores I had from my experience.\n</Comment>\n<Comment by SakuraTroyChen at 2024-06-25T09:52:20Z>\nMaybe it is better to use the output_scores to calculate the softmax scores?\r\n```python\r\ndef eval_relevance(args, tokenizer, model, image_processor):\r\n    disable_torch_init()\r\n\r\n    model_name = get_model_name_from_path(args.model_path)\r\n\r\n    if \"llama-2\" in model_name.lower():\r\n        conv_mode = \"llava_llama_2\"\r\n    elif \"mistral\" in model_name.lower():\r\n        conv_mode = \"mistral_instruct\"\r\n    elif \"v1.6-34b\" in model_name.lower():\r\n        conv_mode = \"chatml_direct\"\r\n    elif \"v1\" in model_name.lower():\r\n        conv_mode = \"llava_v1\"\r\n    elif \"mpt\" in model_name.lower():\r\n        conv_mode = \"mpt\"\r\n    else:\r\n        conv_mode = \"llava_v0\"\r\n\r\n    if args.conv_mode is not None and conv_mode != args.conv_mode:\r\n        print(\r\n            \"[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}\".format(\r\n                conv_mode, args.conv_mode, args.conv_mode\r\n            )\r\n        )\r\n    else:\r\n        args.conv_mode = conv_mode\r\n\r\n    qs = args.query\r\n    if args.image_file != \"\":\r\n        image_token_se = (\r\n            DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\r\n        )\r\n        if IMAGE_PLACEHOLDER in qs:\r\n            if model.config.mm_use_im_start_end:\r\n                qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\r\n            else:\r\n                qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\r\n        else:\r\n            if model.config.mm_use_im_start_end:\r\n                qs = image_token_se + \"\\n\" + qs\r\n            else:\r\n                qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\r\n\r\n        image_files = image_parser(args)\r\n        images = load_images(image_files)\r\n        image_sizes = [x.size for x in images]\r\n        images_tensor = process_images(images, image_processor, model.config)\r\n        if type(images_tensor) is list:\r\n            for i in range(len(images_tensor)):\r\n                images_tensor[i] = images_tensor[i].to(\r\n                    model.device, dtype=torch.float16\r\n                )\r\n        else:\r\n            images_tensor = images_tensor.to(model.device, dtype=torch.float16)\r\n    else:\r\n        images_tensor = None\r\n        image_sizes = None\r\n\r\n    conv = conv_templates[args.conv_mode].copy()\r\n    conv.append_message(conv.roles[0], qs)\r\n    conv.append_message(conv.roles[1], None)\r\n    prompt = conv.get_prompt()\r\n\r\n    input_ids = (\r\n        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\r\n        .unsqueeze(0)\r\n        .cuda()\r\n    )\r\n    with torch.inference_mode():\r\n        generation_output = model.generate(\r\n            input_ids,\r\n            images=images_tensor,\r\n            image_sizes=image_sizes,\r\n            do_sample=True if args.temperature > 0 else False,\r\n            temperature=args.temperature,\r\n            top_p=args.top_p,\r\n            num_beams=args.num_beams,\r\n            max_new_tokens=args.max_new_tokens,\r\n            use_cache=True,\r\n            return_dict_in_generate=True,\r\n            output_scores=True,\r\n        )\r\n\r\n    logits = generation_output.scores[0][0]\r\n\r\n    probs = (\r\n        torch.nn.functional.softmax(\r\n            torch.tensor(\r\n                [\r\n                    logits[tokenizer(\"Yes\").input_ids[1]],\r\n                    logits[tokenizer(\"No\").input_ids[1]],\r\n                ]\r\n            ),\r\n            dim=0,\r\n        )\r\n        .detach()\r\n        .cpu()\r\n        .numpy()\r\n    )\r\n\r\n    return probs[0]\r\n```\r\nJust replace the tokens \"Yes\" and \"No\" with your options.\n</Comment>\n<Comment by Stardust-y at 2024-07-02T03:03:17Z>\n> Hi @vishaal27, thank you for the great question. Yes it is easy to do this with LLaVA.\r\n> \r\n> Here is a simple example that you may start with, by inserting this into [`run_llava.py`](https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/run_llava.py#L98):\r\n> \r\n> ```python\r\n> generation_output = model.generate(\r\n>     input_ids,\r\n>     images=image_tensor.unsqueeze(0).half().cuda(),\r\n>     do_sample=True,\r\n>     temperature=0.2,\r\n>     max_new_tokens=1024,\r\n>     stopping_criteria=[stopping_criteria],\r\n>     # add following two lines\r\n>     return_dict_in_generate=True,\r\n>     output_scores=True\r\n> )\r\n> \r\n> input_token_len = input_ids.shape[1]\r\n> output_ids = generation_output.sequences[0, input_token_len:]\r\n> output_scores = generation_output.scores\r\n> ```\r\n\r\nI'm using the same script to get the likelihood score, the output text are correct, but the scores contain 'inf':\r\n tensor([[   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\r\n        [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\r\n        [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\r\n        ...,\r\n        [   -inf,    -inf, 42.3438,  ...,    -inf,    -inf,    -inf],\r\n        [   -inf,    -inf, 48.3594,  ...,    -inf,    -inf,    -inf],\r\n        [   -inf,    -inf, 91.0938,  ...,    -inf,    -inf,    -inf]],\r\n       device='cuda:0') tensor([[   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\r\n        [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\r\n        [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\r\n        ...,\r\n        [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\r\n        [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\r\n        [   -inf,    -inf, 82.8125,  ...,    -inf,    -inf,    -inf]],\r\n       device='cuda:0')\n</Comment>\n<Comment by KatameRonin at 2024-08-07T19:47:53Z>\n> > Hi @vishaal27, thank you for the great question. Yes it is easy to do this with LLaVA.\r\n> > Here is a simple example that you may start with, by inserting this into [`run_llava.py`](https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/run_llava.py#L98):\r\n> > ```python\r\n> > generation_output = model.generate(\r\n> >     input_ids,\r\n> >     images=image_tensor.unsqueeze(0).half().cuda(),\r\n> >     do_sample=True,\r\n> >     temperature=0.2,\r\n> >     max_new_tokens=1024,\r\n> >     stopping_criteria=[stopping_criteria],\r\n> >     # add following two lines\r\n> >     return_dict_in_generate=True,\r\n> >     output_scores=True\r\n> > )\r\n> > \r\n> > input_token_len = input_ids.shape[1]\r\n> > output_ids = generation_output.sequences[0, input_token_len:]\r\n> > output_scores = generation_output.scores\r\n> > ```\r\n> \r\n> I'm using the same script to get the likelihood score, the output text are correct, but the scores contain 'inf': tensor([[ -inf, -inf, -inf, ..., -inf, -inf, -inf], [ -inf, -inf, -inf, ..., -inf, -inf, -inf], [ -inf, -inf, -inf, ..., -inf, -inf, -inf], ..., [ -inf, -inf, 42.3438, ..., -inf, -inf, -inf], [ -inf, -inf, 48.3594, ..., -inf, -inf, -inf], [ -inf, -inf, 91.0938, ..., -inf, -inf, -inf]], device='cuda:0') tensor([[ -inf, -inf, -inf, ..., -inf, -inf, -inf], [ -inf, -inf, -inf, ..., -inf, -inf, -inf], [ -inf, -inf, -inf, ..., -inf, -inf, -inf], ..., [ -inf, -inf, -inf, ..., -inf, -inf, -inf], [ -inf, -inf, -inf, ..., -inf, -inf, -inf], [ -inf, -inf, 82.8125, ..., -inf, -inf, -inf]], device='cuda:0')\r\n\r\nThe scores that you get are of the shape of : [ num_tokens_in_generate.sequences - 1, batch, vocab_size]\r\nAnd the -inf you see is the score associated with different vocab at each token position. Essentially when the softmax is taken after this step all of these -inf scores will be 0.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 107,
    "state": "open",
    "created_by": "Unrealluver",
    "created_at": "2023-05-06T10:20:29Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/107</URL>\n\n<TITLE>[Question] Can not reproduce the Acc. on Science QA datasets</TITLE>\n\n<BODY>### Question\n\nThanks for running the script for SQA finetuning. After the fine-tuning of SQA. I find the result for the SQA test set (Total: 4241, Correct: 3670, Accuracy: 86.54%) is not good as the result reported in the paper (Accuracy: 90.92%). Could you please share some advice for fixing the mismatch reproduced results?\r\n\r\nHere are my running scripts:\r\n```\r\ntorchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path /share/project/lianghuizhu/vicuna-13b-v0 \\\r\n    --data_path /share/project/lianghuizhu/science_qa/ScienceQA/data/scienceqa/llava_train_QCM-LEPA.json \\\r\n    --image_folder /share/project/lianghuizhu/science_qa/ScienceQA/data/scienceqa/images/train \\\r\n    --vision_tower /home/zhulianghui/ProjectC_ChatGPT/llava/reference/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter /home/zhulianghui/ProjectC_ChatGPT/llava/reference/LLaVA-13b-pretrain-projector-v0-CC3M-595K-original_caption-no_im_token.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --output_dir ./checkpoints/llava-13b-finetune-8x40g-a100-sqa-no_im_start_end_token \\\r\n    --num_train_epochs 12 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 5000 \\\r\n    --save_total_limit 3 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --bf16 True \\\r\n    --fsdp \"full_shard auto_wrap offload\" \\\r\n    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --run_name \"llava-13b-finetune-8x40g-a100-sqa-no_im_start_end_token\" \\\r\n    --lazy_preprocess True \\\r\n    --report_to mlflow\r\n```\r\n\r\nThe `--model_name_or_path /share/project/lianghuizhu/vicuna-13b-v0` is the checkpoint that applies the official vicuna delta on LLaMA-13b.  \r\n\r\n`--pretrain_mm_mlp_adapter /home/zhulianghui/ProjectC_ChatGPT/llava/reference/LLaVA-13b-pretrain-projector-v0-CC3M-595K-original_caption-no_im_token.bin` is the projection layer provided in this repo that does not contains im token. \r\n\r\nAt last, I run the multi-gpu generation scripts in this repo to generate and gather the results.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-07T03:40:10Z>\nHi @Unrealluver, thank you for your feedback.  I run a test training process locally and do notice this performance drop using the latest code release.  However, when I re-run the commit that I got the results during the development, I was able to reproduce the results.  I am investigating this currently, and will let you know soon.\r\n\r\nSorry about this confusion in the released code, and thank you again for the feedback.\n</Comment>\n<Comment by Unrealluver at 2023-05-07T11:24:21Z>\n@haotian-liu Thanks, I am waiting for your further reply.\n</Comment>\n<Comment by haotian-liu at 2023-05-07T16:22:36Z>\nHi @Unrealluver, it has been fixed now.  There was an index not updated which caused the datasets with mixed image-text/text-only content having issues.  Please pull the latest code base and it should work now.  Thanks!\n</Comment>\n<Comment by haotian-liu at 2023-05-07T17:32:03Z>\nPlease also re-download [this checkpoint](https://huggingface.co/liuhaotian/LLaVA-13b-pretrain-projector-v0/blob/main/LLaVA-13b-pretrain-projector-v0-CC3M-595K-original_caption-no_im_token.bin), thanks.  I used this checkpoint to verify the ScienceQA finetuning, not sure why I uploaded the wrong version.  Sorry for the confusion again.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 106,
    "state": "open",
    "created_by": "guozhiyao",
    "created_at": "2023-05-06T07:58:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/106</URL>\n\n<TITLE>[Question] question about the pretrain data</TITLE>\n\n<BODY>### Question\n\nI saw that you provided two pre-training data `CC-3M Concept-balanced 595K` and `LAION/CC/SBU BLIP-Caption Concept-balanced 558K`, what is the difference between these two data? Which are you using? In addition, `LAION/CC/SBU BLIP-Caption Concept-balanced 558K` did not provide the corresponding metadata, could you provide it please?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-07T02:27:26Z>\nHi @guozhiyao, thank you for the reminder.  I have uploaded the metadata for LCS-558K [here](https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain/blob/main/blip_laion_cc_sbu_558k_meta.json).\r\n\r\nRegarding the difference, LAION/CC/SBU is a much larger dataset than CC-3M, and has a wider concept coverage.  For example, CC-3M has intentionally filtered out the names of celebrities.\r\n\r\nBy perform concept-balanced filtering, we are able to maintain a similar size of pretrained dataset, while allowing a much wider concept coverage during the pretraining stage.\r\n\r\nThanks.\n</Comment>\n<Comment by guozhiyao at 2023-05-09T02:04:42Z>\nHi @haotian-liu . Could you please provide the `images.zip` of `LAION/CC/SBU BLIP-Caption Concept-balanced 558K` like `CC-3M Concept-balanced 595K`?\n</Comment>\n<Comment by cnxupupup at 2023-05-16T12:19:44Z>\n> Hi @haotian-liu . Could you please provide the `images.zip` of `LAION/CC/SBU BLIP-Caption Concept-balanced 558K` like `CC-3M Concept-balanced 595K`?\r\n\r\nsame question, Could you please provide the images.zip of LAION/CC/SBU BLIP-Caption Concept-balanced 558K like CC-3M Concept-balanced 595K?\n</Comment>\n<Comment by haotian-liu at 2023-05-25T19:57:29Z>\nHi @guozhiyao @cnxupupup \r\n\r\nHi, thank you for your interest in our work. We have uploaded the images [here](https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain/blob/main/images.zip).\r\n\r\nImportant notice: Upon the request from the community, as ~15% images of the original LAION/CC/SBU dataset are no longer accessible, we upload images.zip for better reproducing our work in research community. It should not be used for any other purpose. The use of these images must comply with the LAION/CC/SBU license. This may be taken down when requested by the original LAION/CC/SBU dataset owner or owners of the referenced images.\n</Comment>\n<Comment by TyRantLQlyf at 2023-10-17T03:58:54Z>\n> Hi @guozhiyao, thank you for the reminder. I have uploaded the metadata for LCS-558K [here](https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain/blob/main/blip_laion_cc_sbu_558k_meta.json).\r\n> \r\n> Regarding the difference, LAION/CC/SBU is a much larger dataset than CC-3M, and has a wider concept coverage. For example, CC-3M has intentionally filtered out the names of celebrities.\r\n> \r\n> By perform concept-balanced filtering, we are able to maintain a similar size of pretrained dataset, while allowing a much wider concept coverage during the pretraining stage.\r\n> \r\n> Thanks.\r\n\r\nHi @haotian-liu . Can you disclose specific filtering strategies for pretrain dataset?\n</Comment>\n<Comment by ChintanShahDS at 2024-01-10T04:13:06Z>\n@haotian-liu Please confirm if these datasets are available for commercial use.\n</Comment>\n<Comment by williamium3000 at 2024-08-28T15:36:53Z>\n@haotian-liu Hi, is it possible for you to release the concept balance code for filtering? Thank you so much!!\r\n\r\nBest\r\nYijiang\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 105,
    "state": "open",
    "created_by": "jiangying000",
    "created_at": "2023-05-06T05:14:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/105</URL>\n\n<TITLE>[Usage] bug on https://llava.hliu.cc/</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\n\r\nAt 2nd message, I input turtle image, but answer is  about 1st  image\r\n\r\nScreenshots:\r\n![image](https://user-images.githubusercontent.com/23182033/236601384-5751599d-3787-44fb-b115-d171da630ad2.png)</BODY>\n\n<COMMENTS>\n<Comment by jiangying000 at 2023-05-06T05:21:04Z>\nthis issue is steadily reproducible\n</Comment>\n<Comment by jiangying000 at 2023-05-06T05:30:44Z>\nSometimes demo is mixing two image's content\r\n![image](https://user-images.githubusercontent.com/23182033/236602161-24c99848-a665-4912-9e5d-d43d29f5e5da.png)\n</Comment>\n<Comment by haotian-liu at 2023-05-07T02:29:41Z>\nHi @jiangying000, thank you for your interest in our work.\r\n\r\nDue to the current way of training (only a single image in a conversation), we do not observe the model having very good capability referring to / comparing with multiple images.  You may refer to the discussion / examples in this thread as well: #57.\r\n\r\nWe are working on improving this aspect as well, stay tuned!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 104,
    "state": "closed",
    "created_by": "sunanhe",
    "created_at": "2023-05-06T02:02:12Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/104</URL>\n\n<TITLE>[Question] Low Accuracy on ScienceQA Dataset</TITLE>\n\n<BODY>### Question\n\nHi, nice work!\r\nI followed your commands to convert format, generate answers and evaluate. But I got about 60 acc on ScienceQA dataset.\r\nMy question are what is the prompt_format in Convert process and the following command may be ignored.\r\nhttps://github.com/haotian-liu/LLaVA/blob/5ac72bcb5d1fdb0f3bb0ced3d9f8be9c3458ad46/README.md?plain=1#L280</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-06T02:57:21Z>\nHi, thank you for your interest in our work.\r\n\r\nYou may refer to [this script](https://github.com/haotian-liu/LLaVA/blob/5ac72bcb5d1fdb0f3bb0ced3d9f8be9c3458ad46/README.md?plain=1#L249) for the prompts during the conversion process.\r\n\r\nRegarding that you are unable to reproduce the numbers, would you mind sharing the commands you used to generate the responses?  An accuracy of 60% definitely means that something is not happening correctly.\r\n\r\nThanks.\n</Comment>\n<Comment by sunanhe at 2023-05-06T15:27:16Z>\nThanks for your timely reply.\r\nHere are the commands I used.\r\n\r\n```bash\r\npython scripts/convert_sqa_to_llava.py \\\r\n    convert_to_llava \\\r\n    --base-dir ScienceQA/data/scienceqa \\\r\n    --split test\r\n\r\npython -m llava.model.apply_delta \\\r\n    --base llama-13b-hf \\\r\n    --target LLaVA-13b-v0-science_qa \\\r\n    --delta LLaVA-13b-delta-v0-science_qa\r\n\r\npython -m llava.eval.model_vqa_science \\\r\n    --model-name LLaVA-13b-v0-science_qa \\\r\n    --question-file ScienceQA/data/scienceqa/llava_test_QCM-LEPA.json \\\r\n    --image-folder ScienceQA/test \\\r\n    --answers-file vqa/results/ScienceQA/test_llava-13b_test2.jsonl \\\r\n    --answer-prompter --conv-mode simple\r\n\r\npython -m llava.eval.eval_science_qa \\\r\n    --base-dir ScienceQA/data/scienceqa \\\r\n    --result-file vqa/results/ScienceQA/test_llava-13b_test2.jsonl \\\r\n    --output-file vqa/results/ScienceQA/test_llava-13b_output_test2.json \\\r\n    --output-result vqa/results/ScienceQA/test_llava-13b_result_test2.json`\r\n```\n</Comment>\n<Comment by haotian-liu at 2023-05-07T15:22:46Z>\nHi, the command seems right.  Can you try pull the latest code base, and reinstall the environment from scratch following instructions [here](https://github.com/haotian-liu/LLaVA#install)?  Also, for faster debugging, can you try directly evaluate on `minitest` split, as it is 10% of the full test set.  It should also give you around 90% accuracy.\n</Comment>\n<Comment by sunanhe at 2023-05-08T05:09:39Z>\nHi, I pull the lasted code but get the similar score （~60%）.\r\nCan you share your question-file and is your conv-mode simple?\n</Comment>\n<Comment by haotian-liu at 2023-05-08T05:45:34Z>\nSure, here are the converted test/minitest.\r\n\r\n[llava_scienceqa_test_minitest.zip](https://github.com/haotian-liu/LLaVA/files/11418145/llava_scienceqa_test_minitest.zip)\r\n\r\nAnd yes, my conv-mode is simple.  I used [this script for batch inference](https://github.com/haotian-liu/LLaVA/blob/main/scripts/sqa_eval_batch.sh) and [this script for gathering the results](https://github.com/haotian-liu/LLaVA/blob/main/scripts/sqa_eval_gather.sh).\n</Comment>\n<Comment by sunanhe at 2023-05-08T14:02:31Z>\nThanks for your patient reply. Maybe the previous ckpt I download are broken. I download the ckpt again and get the ~90% score.\r\n\r\nBest\r\nSunan\n</Comment>\n<Comment by CupidJay at 2023-08-03T02:55:47Z>\n> Thanks for your patient reply. Maybe the previous ckpt I download are broken. I download the ckpt again and get the ~90% score.\r\n> \r\n> Best Sunan\r\n\r\nHi, May I ask if you downloaded the weight of llama from [https://huggingface.co/decapoda-research/llama-13b-hf](https://huggingface.co/decapoda-research/llama-13b-hf)? By the way, what is your transformer version? It seems that I used the same command as you, but my accuray is only 40%. Is the weight you re-downloaded llama-13b or delta? Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 103,
    "state": "closed",
    "created_by": "jihan-yin",
    "created_at": "2023-05-05T17:57:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/103</URL>\n\n<TITLE>Any plans to release the pretrain weights for Llava-7b?</TITLE>\n\n<BODY>### feature\n\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-05T17:58:37Z>\nHi, thanks for your interest in our work.\r\n\r\nOur 7b weights are released here: https://github.com/haotian-liu/LLaVA#llava-weights\r\n\r\nInclude one based on Vicuna v0, and our lightning model based on Vicuna v1.1.\n</Comment>\n<Comment by jihan-yin at 2023-05-05T20:34:35Z>\nAh I meant the pretrained projector weights, not the final weights after fine-tuning\n</Comment>\n<Comment by haotian-liu at 2023-05-08T16:35:46Z>\nHi @jihan-yin, it's uploaded here just now, thanks.\r\n\r\nhttps://huggingface.co/liuhaotian/LLaVA-13b-pretrain-projector-v0/blob/main/LLaVA-7b-pretrain-projector-v0-CC3M-595K-original_caption.bin\n</Comment>\n<Comment by jihan-yin at 2023-05-10T00:12:02Z>\nAlso just wanna confirm, these are based off Vicuna v0 right?\n</Comment>\n<Comment by haotian-liu at 2023-05-10T01:04:03Z>\nYes you are right.\n</Comment>\n<Comment by jihan-yin at 2023-05-10T02:16:12Z>\nThanks for the help so far! Though I'm trying to load the new pretrained weights locally, and am getting an error.\r\n\r\n```\r\nimport torch\r\n\r\nmm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location=\"cpu\")\r\n```\r\n\r\ngets \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/serialization.py\", line 795, in load\r\n    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\r\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/serialization.py\", line 1002, in _legacy_load\r\n    magic_number = pickle_module.load(f, **pickle_load_args)\r\n_pickle.UnpicklingError: invalid load key, 'v'.\r\n```\n</Comment>\n<Comment by jihan-yin at 2023-05-10T02:18:31Z>\nActually, this is because I downloaded the weight file incorrectly. Redownloading with git-lfs fixed the issue. Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 102,
    "state": "closed",
    "created_by": "zzhanghub",
    "created_at": "2023-05-05T17:15:39Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/102</URL>\n\n<TITLE>Error while tuning LLaVA-Lighting</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\n#!/bin/bash\r\n\r\nWEIGHT_VERSION=1\r\n\r\n\r\n# Visual instruction tuning (1 hour)\r\nsrun -p rdbp1_a100_80g -n1 -N 1 --gres=gpu:1 \\\r\ntorchrun --nnodes=1 --nproc_per_node=1 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path /mnt/lustre/share_data/zhangzhao2/VG/ckpt/llava/llava_v1/7B \\\r\n    --version $WEIGHT_VERSION \\\r\n    --data_path /mnt/lustre/share_data/zhangzhao2/VG/instruction_data/LLaVA-Instruct-150K/llava_instruct_80k.json \\\r\n    --image_folder /mnt/lustre/share_data/dongzhiwei1/coco2014/train2014 \\\r\n    --vision_tower /mnt/lustre/share_data/zhangzhao2/VG/ckpt/openai/clip-vit-large-patch14 \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 2 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 5000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --fsdp \"full_shard auto_wrap\" \\\r\n    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to none\r\n```\r\n\r\nLog: \r\n```\r\nLoading checkpoint shards: 100%|          | 2/2 [01:31<00:00, 45.96s/it]\r\nWARNING:root:Loading data...\r\nWARNING:root:Formatting inputs...Skip in lazy mode\r\n/mnt/cache/zhangzhao2/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.\r\n  warnings.warn(\r\n  0%|          | 0/5000 [00:00<?, ?it/s]Traceback (most recent call last):\r\n  File \"/mnt/cache/zhangzhao2/codes/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/mnt/cache/zhangzhao2/codes/LLaVA/llava/train/train.py\", line 569, in train\r\n    trainer.train()\r\n  File \"/mnt/cache/zhangzhao2/anaconda3/envs/torch2/lib/python3.9/site-packages/transformers/trainer.py\", line 1662, in train\r\n    return inner_training_loop(\r\n  File \"/mnt/cache/zhangzhao2/anaconda3/envs/torch2/lib/python3.9/site-packages/transformers/trainer.py\", line 1927, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/mnt/cache/zhangzhao2/anaconda3/envs/torch2/lib/python3.9/site-packages/transformers/trainer.py\", line 2699, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/mnt/cache/zhangzhao2/anaconda3/envs/torch2/lib/python3.9/site-packages/transformers/trainer.py\", line 2731, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/mnt/cache/zhangzhao2/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/cache/zhangzhao2/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 748, in forward\r\n    output = self._fsdp_wrapped_module(*args, **kwargs)\r\n  File \"/mnt/cache/zhangzhao2/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/cache/zhangzhao2/codes/LLaVA/llava/model/llava.py\", line 218, in forward\r\n    outputs = self.model(\r\n  File \"/mnt/cache/zhangzhao2/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/cache/zhangzhao2/codes/LLaVA/llava/model/llava.py\", line 126, in forward\r\n    image_forward_outs = vision_tower(images, output_hidden_states=True)\r\n  File \"/mnt/cache/zhangzhao2/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/cache/zhangzhao2/anaconda3/envs/torch2/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py\", line 934, in forward\r\n    return self.vision_model(\r\n  File \"/mnt/cache/zhangzhao2/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/cache/zhangzhao2/anaconda3/envs/torch2/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py\", line 859, in forward\r\n    hidden_states = self.embeddings(pixel_values)\r\n  File \"/mnt/cache/zhangzhao2/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/cache/zhangzhao2/anaconda3/envs/torch2/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py\", line 195, in forward\r\n    patch_embeds = self.patch_embedding(pixel_values)  # shape = [*, width, grid, grid]\r\n  File \"/mnt/cache/zhangzhao2/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/cache/zhangzhao2/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/nn/modules/conv.py\", line 463, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n  File \"/mnt/cache/zhangzhao2/anaconda3/envs/torch2/lib/python3.9/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\r\n    return F.conv2d(input, weight, bias, self.stride,\r\nRuntimeError: GET was unable to find an engine to execute this computation\r\n```</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-05T17:47:58Z>\nHi @zzhanghub, thanks for the interest in our work.\r\n\r\nSimilar to Vicuna, 13B model finetuning needs 8x A100, you may do that with 4x A100 (80G), but this is not fully verified.\r\n\r\nYou may also see discussion here: #68 \r\nThanks!\n</Comment>\n<Comment by zzhanghub at 2023-05-05T18:42:21Z>\n> Hi @zzhanghub, thanks for the interest in our work.\r\n> \r\n> Similar to Vicuna, 13B model finetuning needs 8x A100, you may do that with 4x A100 (80G), but this is not fully verified.\r\n> \r\n> You may also see discussion here: #68 Thanks!\r\n\r\n@haotian-liu Thanks for replying😊, but this bug doesn't seem to be caused by OOM. I wonder if torch2.0 is the reason\n</Comment>\n<Comment by haotian-liu at 2023-05-05T18:45:10Z>\n@zzhanghub I remember a similar issue when I tried with a single A100, and using more GPUs solves the issue.  I recommend trying with a 7B model (with a small batch size like 1 or 2), or with pretraining, which shall be okay running on a single A100 (80G).\r\n\r\nPyTorch 2.0 may not be the issue, as I am also using PyTorch 2.0 :)\n</Comment>\n<Comment by zzhanghub at 2023-05-06T01:52:45Z>\n> @zzhanghub I remember a similar issue when I tried with a single A100, and using more GPUs solves the issue. I recommend trying with a 7B model (with a small batch size like 1 or 2), or with pretraining, which shall be okay running on a single A100 (80G).\r\n> \r\n> PyTorch 2.0 may not be the issue, as I am also using PyTorch 2.0 :)\r\n\r\n@haotian-liu\r\nHi~ \r\nI was doing tuning on 7B. I tried two settings, both on 80g A100: one using 1 GPU and a small batch size, the other using 8 GPUs and a normal batch size. But I got the same error in both settings.\r\n\r\nI also tried using the pretraining script, the script is as follows:\r\n```bash\r\nsrun -p a100_80g -n1 -N 1 --gres=gpu:8 \\\r\ntorchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path /mnt/lustre/share_data/zhangzhao2/VG/ckpt/llava/llava_v1/7B \\\r\n    --version $WEIGHT_VERSION \\\r\n    --data_path /mnt/lustre/share_data/zhangzhao2/VG/instruction_data/LLaVA-Instruct-150K/llava_instruct_80k.json \\\r\n    --image_folder /mnt/lustre/share_data/dongzhiwei1/coco2014/train2014 \\\r\n    --vision_tower /mnt/lustre/share_data/zhangzhao2/VG/ckpt/openai/clip-vit-large-patch14 \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end \\\r\n    --bf16 True \\\r\n    --output_dir /mnt/lustre/share_data/zhangzhao2/VG/log/llava \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 2400 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to none\r\n```\r\nBut I got an error:\r\n\r\n![iShot_2023-05-06_09 49 12](https://user-images.githubusercontent.com/25194962/236591852-d72eae3f-15fc-418f-b6a0-bcded8cbe45f.png)\r\n\r\n I set `model_name_or_path` to 'llava' instead of 'vicuna'. I'm wondering if this could be the cause of the problem?\n</Comment>\n<Comment by haotian-liu at 2023-05-06T02:34:55Z>\n@zzhanghub\r\nThis is strange, the error indicates that you used FSDP, but it does not appear in your command.\r\n\r\nYou used `--tune_mm_mlp_adapter True`, which only tunes the projection layer, and currently unless you install PyTorch nightly and follow instructions [here](https://github.com/haotian-liu/LLaVA#experimental-use-fsdp-to-save-memory-in-pretraining), it is not possible to use FSDP with partially frozen parameters, as PEFT is not supported fully yet.  You can only use FSDP in instruction tuning.\r\n\r\nI am not sure if you enabled FSDP somewhere else?\r\n\r\nAnd I also see that you are using instruction tuning dataset, so could you please confirm the command that you are using, and the intent of the experiment?  This may let me better understand the issue, thanks.\n</Comment>\n<Comment by zzhanghub at 2023-05-06T03:04:10Z>\n@haotian-liu \r\nThank you for your reply!\r\nI haven't enabled FSDP anywhere else because I haven't made any modifications to the llava code yet.\r\n\r\nThe purpose of me conducting this experiment is to further tune the llava-ckpt based on a new dataset. However, I haven't used the new dataset yet, and instead I used the data you provided to test my code.\n</Comment>\n<Comment by haotian-liu at 2023-05-06T03:07:24Z>\nI see, if so, can you try printing `training_args.fsdp` and see what's configured? if possible, you may also print out all training arguments. It should not be enabled by default, but we can see what's happening behind.  You may check this right after this:\r\n\r\n```\r\n    parser = transformers.HfArgumentParser(\r\n        (ModelArguments, DataArguments, TrainingArguments))\r\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\r\n```\n</Comment>\n<Comment by haotian-liu at 2023-05-06T03:08:55Z>\nAlso as a direct solution (if you are not planning to tune the projection layer only), you may try removing `--tune_mm_mlp_adapter True` and running with 8 GPUs, this will work with FSDP enabled.\n</Comment>\n<Comment by zzhanghub at 2023-05-06T03:29:52Z>\n@haotian-liu \r\nI printed out the `training_args.fsdp` and the result is as follows:\r\n![image](https://user-images.githubusercontent.com/25194962/236596561-e77f7c65-88c7-4ed4-a825-aa46432387f1.png)\r\n\r\nI removed `--tune_mm_mlp_adapter Tru` and use 8 GPUs for pre-training, and encountered a similar problem as before\r\n``` shell\r\nsrun -p a100_80g -n1 -N 1 --gres=gpu:8  \\\r\ntorchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path /mnt/lustre/share_data/zhangzhao2/VG/ckpt/llava/llava_v1/7B \\\r\n    --version $WEIGHT_VERSION \\\r\n    --data_path /mnt/lustre/share_data/zhangzhao2/VG/instruction_data/LLaVA-Instruct-150K/llava_instruct_80k.json \\\r\n    --image_folder /mnt/lustre/share_data/dongzhiwei1/coco2014/train2014 \\\r\n    --vision_tower /mnt/lustre/share_data/zhangzhao2/VG/ckpt/openai/clip-vit-large-patch14 \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end \\\r\n    --bf16 True \\\r\n    --output_dir /mnt/lustre/share_data/zhangzhao2/VG/log/llava \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 2400 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to none\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/25194962/236595719-ece495d2-56b5-4597-a636-0e8b39c6e6e3.png)\n</Comment>\n<Comment by haotian-liu at 2023-05-06T03:32:11Z>\nThanks for providing this info.  I am wondering how did you setup your slurm cuda environment?  Is it a new environment?  Furthermore, what's your PyTorch and CUDA version?  Thanks.\n</Comment>\n<Comment by zzhanghub at 2023-05-06T03:45:13Z>\n@haotian-liu \r\nHere are some version information :)\r\n![image](https://user-images.githubusercontent.com/25194962/236597951-f409da39-61f7-41d4-82ab-6f268241f8a8.png)\r\n\r\n![image](https://user-images.githubusercontent.com/25194962/236597787-f7a69c03-3a88-4c4c-bba9-0deabd609485.png)\r\n![image](https://user-images.githubusercontent.com/25194962/236597816-38291bcc-5a75-4d7e-b1d7-087ad740a6ba.png)\r\n![image](https://user-images.githubusercontent.com/25194962/236597836-499598d8-2a96-4ca0-8695-91df4dfe9cd5.png)\n</Comment>\n<Comment by haotian-liu at 2023-05-06T03:47:22Z>\nWhat's your `transformers` version?  And can you also please confirm that this is a new environment for LLaVA only?\n</Comment>\n<Comment by zzhanghub at 2023-05-06T05:01:33Z>\n> What's your `transformers` version? And can you also please confirm that this is a new environment for LLaVA only?.\r\nYes, it is a new environment for LLaVA.\r\n![image](https://user-images.githubusercontent.com/25194962/236600931-01da427c-2386-4de7-8663-c3f4d275319d.png)\r\n@haotian-liu\n</Comment>\n<Comment by haotian-liu at 2023-05-06T18:08:27Z>\nThis may be the issue as the `transformers` version is not correct, as it should show something like `dev`.  Please try installing the correct version of `transformers` following the instructions [here](https://github.com/haotian-liu/LLaVA#upgrade-to-v01).\n</Comment>\n<Comment by zzhanghub at 2023-05-07T09:08:49Z>\n> This may be the issue as the `transformers` version is not correct, as it should show something like `dev`. Please try installing the correct version of `transformers` following the instructions [here](https://github.com/haotian-liu/LLaVA#upgrade-to-v01).\r\n\r\n@haotian-liu \r\n<img width=\"275\" alt=\"image\" src=\"https://user-images.githubusercontent.com/25194962/236668427-98f7526f-3d56-4391-9388-ea48081f79bf.png\">\r\nI have created a new environment again, but the situation seems to have not changed.\n</Comment>\n<Comment by zzhanghub at 2023-05-07T15:16:59Z>\n> This may be the issue as the `transformers` version is not correct, as it should show something like `dev`. Please try installing the correct version of `transformers` following the instructions [here](https://github.com/haotian-liu/LLaVA#upgrade-to-v01).\r\n\r\n@haotian-liu \r\nGreat news! I have finally succeeded in running the Tuning by downgrading the `torch` version to 1.13.1 and setting the `CUDA` version to 11.7!\r\n\r\nI spent a lot of effort building `flash-attn`, and I know that `torch` and `flash-attn` require consistent use of CUDA. However, I failed to compile them with many different CUDA versions and GCC versions (the versions of `torch` and `flash-attn` are corresponding). Finally, I succeeded with `CUDA=11.7` and `gcc=7.3.0`.\r\n\r\nRegarding the bug I encountered before, I think it may be related to what is discussed in this [issue](https://github.com/pytorch/pytorch/issues/92907), and it should not be due to `LLaVA` or `transformers`. However, I don't know why the error occurred since many people can use `torch-2.0` without any problems.\r\n\r\nLastly, I would like to express my gratitude for your active replies and support. Your assistance gave me confidence that LLaVA will be a reliable and promising project!\n</Comment>\n<Comment by haotian-liu at 2023-05-07T17:19:05Z>\n@zzhanghub Great to hear that it works out!  And thank you for your kind words.  Btw, please still keep `transformers` version to the one that we include in README, as other versions may potentially lead to strange issues :)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 101,
    "state": "closed",
    "created_by": "altryne",
    "created_at": "2023-05-05T15:47:26Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/101</URL>\n\n<TITLE>[Feature request] - Integrate MPT from MosaicML</TITLE>\n\n<BODY>### feature\r\n\r\nHi dear LLaVa maintainers. \r\n\r\nMosaic has just released their new OpenSource model called MPT-7B and they claim it's fully open source and is matching LLaMa while having up to 65K token window! \r\n\r\n![image](https://user-images.githubusercontent.com/463317/236505791-2739fa48-094b-4d94-8b55-f189621c1c11.png)\r\n\r\n\r\nWould be incredible to see it used inside LLaVa\r\n\r\nhttps://www.mosaicml.com/blog/mpt-7b</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-06T17:59:52Z>\nHi @altryne\r\n\r\nIt's released!  Thanks to the LLaVA-lightning, we are able to release it today.\r\n\r\nCheckout the Tweet [here](https://twitter.com/imhaotian/status/1654905800165392384).\r\nCheckout the instructions [here](https://github.com/haotian-liu/LLaVA#LLaVA-MPT-7b).\n</Comment>\n<Comment by altryne at 2023-05-06T18:46:25Z>\nIncredible effort @haotian-liu 👏 Did NOT expect this to be resolved this quick! Super exciting! Thank you for the effort. (boosted on twitter)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 100,
    "state": "closed",
    "created_by": "npurson",
    "created_at": "2023-05-05T12:31:26Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/100</URL>\n\n<TITLE>[Usage] Entry Not Found for url: https://huggingface.co/liuhaotian/LLaVA-7b-delta-v0/resolve/main/pytorch_model.bin.</TITLE>\n\n<BODY>### When did you clone our code?\r\n\r\nI cloned the code base after 5/1/23\r\n\r\n### Describe the issue\r\n\r\nI encountered an error while converting the weight of LLaMA-7B to LLaVA using the following command:\r\n\r\n```bash\r\npython3 -m llava.model.apply_delta \\\r\n    --base checkpoints/llama-7b-hf \\\r\n    --target checkpoints/llava-7b-v0 \\\r\n    --delta liuhaotian/LLaVA-7b-delta-v0\r\n```\r\n\r\nThe error message I received is as follows:\r\n\r\n```\r\nHTTPError                                 Traceback (most recent call last)\r\n~/envs/miniconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py in hf_raise_for_status\r\n    258     try:\r\n--> 259         response.raise_for_status()\r\n    260     except HTTPError as e:\r\n\r\n\r\n/usr/local/lib/python3.10/dist-packages/requests/models.py in raise_for_status\r\n    959         if http_error_msg:\r\n--> 960             raise HTTPError(http_error_msg, response=self)\r\n    961 \r\n\r\nHTTPError: 404 Client Error: Not Found for url: https://huggingface.co/liuhaotian/LLaVA-7b-delta-v0/resolve/main/pytorch_model.bin\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nEntryNotFoundError                        Traceback (most recent call last)\r\n~/envs/miniconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py in cached_download\r\n    688             )\r\n    689             headers.pop(\"Accept-Encoding\", None)\r\n--> 690             hf_raise_for_status(r)\r\n    691             etag = r.headers.get(HUGGINGFACE_HEADER_X_LINKED_ETAG) or r.headers.get(\"ETag\")\r\n    692             # We favor a custom header indicating the etag of the linked resource, and\r\n\r\n~/envs/miniconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py in hf_raise_for_status\r\n    267         elif error_code == \"EntryNotFound\":\r\n    268             message = f\"{response.status_code} Client Error.\" + \"\\n\\n\" + f\"Entry Not Found for url: {response.url}.\"\r\n--> 269             raise EntryNotFoundError(message, response) from e\r\n    270\r\n    271         elif error_code == \"GatedRepo\":\r\n\r\nEntryNotFoundError: 404 Client Error. (Request ID: Root=1-6454f263-14b6f0b24a4090276df28333)\r\n\r\nEntry Not Found for url: https://huggingface.co/liuhaotian/LLaVA-7b-delta-v0/resolve/main/pytorch_model.bin.\r\n```\r\n\r\nThe URL https://huggingface.co/liuhaotian/LLaVA-7b-delta-v0/resolve/main/pytorch_model.bin could not be accessed, resulting in an HTTP 404 error. It is worth noting that I was able to successfully download from https://huggingface.co/liuhaotian/LLaVA-7b-delta-v0/resolve/main/config.json just prior to this error. Additionally, I attempted to open the aforementioned URL in my browser, but it was also inaccessible. This leads me to believe that the issue is not related to my local environment.\r\n\r\nPlease let me know if you have any insights regarding this issue.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-06T18:28:11Z>\nHi, thank you for your interest in our work.\r\n\r\nI am wondering if this is only happening to this specific model or to all the models that we have released?  Not sure if it due to the network issue, but if the [model list](https://huggingface.co/liuhaotian/LLaVA-7b-delta-v0/blob/main/pytorch_model.bin.index.json) is loaded successfuly, it should know that there are two shards of the model checkpoint, and should not look for `pytorch_model.bin`.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 99,
    "state": "open",
    "created_by": "pixeli99",
    "created_at": "2023-05-05T11:55:59Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/99</URL>\n\n<TITLE>[Discussion] A Preliminary Attempt in the Video Level</TITLE>\n\n<BODY>### Discussion\r\n\r\nHi! \r\nThank you very much for such an interesting project. After learning the mode of using GPT4 to generate data, we use GPT4 api to do similar things for videos, we hope that LLM can also understand some behaviors of time series.\r\n\r\nWe have simply extended LLaVA like model for video level in our project [DriveScenify](https://github.com/pixeli99/DSify).\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/46072190/236450843-82437bcd-98ef-45e2-81e7-67defd042eda.png)\r\n\r\nWe currently do not do this on general video (firstly, the API is indeed quite expensive and our computing power does not support us to do large-scale pre training). At present, it is only an initial version, and the data used for training is limited. However, there is already a prototype, and everyone is welcome to try it out!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 98,
    "state": "open",
    "created_by": "feymanwang",
    "created_at": "2023-05-05T08:41:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/98</URL>\n\n<TITLE>Can't loading delta</TITLE>\n\n<BODY>### When did you clone our code?\n\nI cloned the code base after 5/1/23\n\n### Describe the issue\n\nIssue:\r\nCan't load delta model, it stuck and showed a warning message\r\n\r\nCommand:\r\n```\r\npython3 -m llava.model.apply_delta \\\r\n    --base /data/home/llama-7b-hf\\\r\n    --target /data/home/LLaVA-7B-v0 \\\r\n    --delta /data/home/LLaVA-7b-delta-v0\r\n```\r\n\r\nLog: \r\n```\r\nLoading base model\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 33/33 [00:07<00:00,  4.70it/s]\r\nLoading delta\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\n\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-07T02:31:31Z>\nHi, this is expected (as the checkpoint was created in an earlier version) and it is not an error; our latest code base can deal with correctly and generate the corresponding model weights.  Can you take a look at `/data/home/LLaVA-7B-v0` folder and see if there is anything written inside it?\n</Comment>\n<Comment by RongkunYang at 2023-12-26T12:20:29Z>\nHi, I meet the same problem, and the target folder does not have any output, have you resolved it.\r\nAnd I think this may be caused by the llama weight version should be 1, not 2, right?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 97,
    "state": "closed",
    "created_by": "PoseidomWong",
    "created_at": "2023-05-05T08:19:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/97</URL>\n\n<TITLE>How to use the BLIP synthetic caption?</TITLE>\n\n<BODY>I found that there is a content about blip caption in the introduction of pretrain data. I would like to ask how to combine the original CC3M data caption with the blip caption, and how to use it?\r\n\r\nThanks for your time!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-06T17:54:31Z>\nNote that we only used the original CC3M data caption in our v0 release.  If you'd like to combine them, you may directly modify the annotation file, and include the response as from \"GPT\".\n</Comment>\n<Comment by PoseidomWong at 2023-05-08T01:31:17Z>\nI see. Thank you very much\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 96,
    "state": "closed",
    "created_by": "penghe2021",
    "created_at": "2023-05-04T19:58:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/96</URL>\n\n<TITLE>How to get the llava 13b v1</TITLE>\n\n<BODY>### Question\n\nFrom the https://huggingface.co/liuhaotian, I cant find the llava 13b delta v1, maybe I have some misunderstanding, can you tell me where to find the delta weight for the llava 13b v1 model, thank.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-05T02:33:13Z>\nHi, we'll release more LLaVA checkpoints based on Vicuna-v1 weights soon.  Currently we have only released the [Lightning-7B](https://huggingface.co/liuhaotian/LLaVA-Lightning-7B-delta-v1-1) for Vicuna v1.1.  Stay tuned!\n</Comment>\n<Comment by penghe2021 at 2023-05-06T00:18:23Z>\nThank!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 95,
    "state": "closed",
    "created_by": "Unrealluver",
    "created_at": "2023-05-04T13:09:45Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/95</URL>\n\n<TITLE>Details for finetune SQA dataset</TITLE>\n\n<BODY>Greetings! I have checked the great performance on your COCO VQA datasets. Now I want to check the performance on the SQA dataset, could you give me more guidance about the fine-tuning details? Is the LLaVA-13b-SQA still fine-tuning on the LLaVA-13b-cc3m-595k? A running script containing epochs, learning rate, and other training parameters helps us followers a lot to reproduce LLaVA. \r\n\r\nThanks for your time!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-05T02:23:45Z>\nHi @Unrealluver, thank you for the interest in our work.  It's trained on the ScienceQA training set for 12 epochs as indicated in our paper.  I have updated our README to include [details of ScienceQA training](https://github.com/haotian-liu/LLaVA#fine-tuning-on-scienceqa) to reproduce our current checkpoint (w/o image start/end tokens).\r\nI have also provided the pretrained projectors, please see the link in the instructions.\r\nPlease let me know if there are anything unclear, thanks.\n</Comment>\n<Comment by haotian-liu at 2023-05-25T20:01:45Z>\nClosing the issue due to inactivity, please feel free to re-open if you meet any other issues, thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 94,
    "state": "closed",
    "created_by": "RupertLuo",
    "created_at": "2023-05-04T11:17:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/94</URL>\n\n<TITLE>about inference by batch encode</TITLE>\n\n<BODY>The '[PAD]' token's id is 32003, but embedding matrix's shape is [32003,5120] in given weight, which means when I do batch inference, it will come out an error , how to fix it ?</BODY>\n\n<COMMENTS>\n<Comment by RupertLuo at 2023-05-04T11:17:43Z>\n![image](https://user-images.githubusercontent.com/36270260/236189126-6294a246-26dc-49b8-a06e-5120b0668a7d.png)\n</Comment>\n<Comment by haotian-liu at 2023-05-05T04:34:19Z>\nHi @RupertLuo, thank you for your interest in our work.\r\n\r\nCan you provide the command you use and the full error log for me to better understand the problem and better assist?  Thanks.\n</Comment>\n<Comment by RupertLuo at 2023-05-05T05:09:59Z>\n是我自己写的脚本，就是参照 /llava/eval/run_llava.py 把模型参数加载进去，然后用的我自己的数据集，每条数据的prompt不一样长，我用 pad_sequence 把它们pad到等长，pad index 用的 32003\r\n![image](https://user-images.githubusercontent.com/36270260/236380336-469ebbc0-cc4b-47ab-83c1-7bb686c6297c.png)\r\n然后这个 idx 在，embeding 矩阵里面是没有的，因为embedding矩阵维度是 （32003，5120），也就是最大idx是32002，所以爆这个错了, 我这里在代码中 print了input中最大的index，和embedding的shape\r\n![image](https://user-images.githubusercontent.com/36270260/236381578-4087acd5-5cbf-4b21-b1f8-1faa3499e865.png)\r\n![image](https://user-images.githubusercontent.com/36270260/236381608-ca40f4f5-2d02-42bf-9f84-6f78c8ca5fab.png)\n</Comment>\n<Comment by RupertLuo at 2023-05-05T09:55:36Z>\n![image](https://user-images.githubusercontent.com/36270260/236427959-b274e085-f8ee-46cd-a225-cf6b3dc01bb2.png)\r\n做generation的时候是不是要用这个idx做 padding\n</Comment>\n<Comment by haotian-liu at 2023-05-06T23:57:02Z>\nI see, I guess you may use any token for `PAD` but make sure that they are masked out during the inference.  There are some issues with the tokenizer config currently, but anyway the embedding of the `PAD` token will not make sense, as it has never been optimized (they are masked out during the training).\n</Comment>\n<Comment by youweiliang at 2023-05-07T18:53:01Z>\nHi @haotian-liu ,\r\nThanks for the nice work! Does using the unknown token `<unk>` for padding make sense?\n</Comment>\n<Comment by haotian-liu at 2023-05-07T20:02:37Z>\nYes it makes sense, and starting from v1 (Vicuna) checkpoints, it uses `<unk>` by default.\n</Comment>\n<Comment by RupertLuo at 2023-05-08T06:31:56Z>\nThanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 93,
    "state": "open",
    "created_by": "nonstopnht",
    "created_at": "2023-05-04T06:06:02Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/93</URL>\n\n<TITLE>Could we use LLaVA to extract information from a given photo?</TITLE>\n\n<BODY>Hi LLaVA team, thanks for the great work, it is really impressive of what you have done!\r\nI am trying to understand what we could do with LLaVA, and to extract information from a photo but the answer is a bit interesting. \r\n\r\nWhile LLaVA is able to tell the uploaded photo is a certificate of incorporation, but it is telling a wrong UEN, is it due to the training is not enough or the limitation of transformer? Thank you for your great effort!\r\n\r\n![image](https://user-images.githubusercontent.com/17609528/236123433-9d2c7de6-8bbd-4244-b4ea-6a85ae19fa1b.png)\r\n![image](https://user-images.githubusercontent.com/17609528/236123633-dd58f7cf-e413-402e-81db-c64da568e478.png)\r\n![Screenshot 2023-05-04 135147](https://user-images.githubusercontent.com/17609528/236123649-13df1e3e-39d8-4940-b096-7fb98761104b.png)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-05T02:32:34Z>\nHi, thank you for your interest in our work.  The current LLaVA is not good at parsing the details of high resolution images, as the current input resolution is only 224x224.  It is also partially due to that these OCR tasks is not very common in our dataset. We are working on these improvements, stay tuned!\n</Comment>\n<Comment by nonstopnht at 2023-05-05T02:36:12Z>\nThank you very much for your reply! Looking forward to seeing it, let me know if I can contribute. (I am new to AI, but very interested in it)\n</Comment>\n<Comment by haotian-liu at 2023-05-05T04:29:06Z>\n@nonstopnht Of course, contributions are always welcomed!  If you find anything that you are interested in improving, feel free to let me know :) Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 92,
    "state": "closed",
    "created_by": "jihan-yin",
    "created_at": "2023-05-04T05:31:29Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/92</URL>\n\n<TITLE>TypeError: forward() got an unexpected keyword argument 'images'</TITLE>\n\n<BODY>I'm trying to set up the finetuning script locally, but am getting this error when testing the model in `train.py`. I'm manually passing in a single sample from the lazy dataset, and the error makes sense - I don't see where in the code we actually adapt the model's forward function to actually use the vision tower. Am I missing a setup step or something?</BODY>\n\n<COMMENTS>\n<Comment by jihan-yin at 2023-05-04T05:48:58Z>\nJust ran the finetuning script fully, and am getting `TypeError: forward() got an unexpected keyword argument 'images'`\n</Comment>\n<Comment by haotian-liu at 2023-05-04T05:50:37Z>\nHi, thank you for your interest in our work.  To better understand the issue, can you share the command that you use?\n</Comment>\n<Comment by jihan-yin at 2023-05-04T05:51:37Z>\nYup - it's pretty much what you have outlined in the repo for finetuning:\r\n```\r\ntorchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \\\\\r\n        llava/train/train_mem.py \\\\\r\n        --model_name_or_path {VICUNA_LOCAL_PATH} \\\\\r\n        --data_path {LLAVA_DATA_LOCAL_PATH} \\\\\r\n        --image_folder {COCO_2014_LOCAL_PATH} \\\\\r\n        --vision_tower openai/clip-vit-large-patch14 \\\\\r\n        --pretrain_mm_mlp_adapter {PRETRAIN_ADAPTER_LOCAL_PATH} \\\\\r\n        --mm_vision_select_layer -2 \\\\\r\n        --mm_use_im_start_end True \\\\\r\n        --bf16 True \\\\\r\n        --output_dir /opt/ml/checkpoints/mix_pretrain_0.25_instruct_0.25 \\\\\r\n        --num_train_epochs 3 \\\\\r\n        --per_device_train_batch_size 4 \\\\\r\n        --per_device_eval_batch_size 4 \\\\\r\n        --gradient_accumulation_steps 1 \\\\\r\n        --evaluation_strategy 'no' \\\\\r\n        --save_strategy 'steps' \\\\\r\n        --save_steps 5000 \\\\\r\n        --save_total_limit 3 \\\\\r\n        --learning_rate 2e-5 \\\\\r\n        --weight_decay 0. \\\\\r\n        --warmup_ratio 0.03 \\\\\r\n        --lr_scheduler_type 'cosine' \\\\\r\n        --logging_steps 1 \\\\\r\n        --tf32 True \\\\\r\n        --fsdp 'full_shard auto_wrap' \\\\\r\n        --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\\\r\n        --model_max_length 1024 \\\\\r\n        --gradient_checkpointing True \\\\\r\n        --lazy_preprocess True \\\\\r\n        --report_to wandb \r\n```\n</Comment>\n<Comment by jihan-yin at 2023-05-04T05:53:44Z>\nBut yeah, I just don't see how the model is constructed to take in 'images' in the forward call, since we're using the `LlamaForCausalLM` class which doesn't handle multimodal inputs\n</Comment>\n<Comment by haotian-liu at 2023-05-04T05:56:18Z>\nHi @jihan-yin, are you using the latest code base?  You can see from [here](https://github.com/haotian-liu/LLaVA/blob/main/llava/train/train.py#L465) that it will use `LlavaLlamaForCausalLM` when you provide `--vision_tower` in argument.\r\n\r\nIf you are using the latest code base, please share the full error log so that I can better understand the issue.  Thanks.\n</Comment>\n<Comment by jihan-yin at 2023-05-04T05:57:53Z>\nAh, I am not using the latest version. Looks like I need to update - thanks\n</Comment>\n<Comment by haotian-liu at 2023-05-04T06:00:03Z>\nYep, please try upgrading the code base, as for the older code base, you need to install our customized transformers package [here](https://github.com/haotian-liu/transformers_llava).\r\n\r\nUpgrading to the latest code base removes that dependency, and we can make upgrades directly in this main code base.  It also supports more Vicuna models and new features :)\r\n\r\nPlease let me know if you have further questions, thanks.\n</Comment>\n<Comment by jihan-yin at 2023-05-04T17:49:02Z>\nHey @haotian-liu, now I am getting an error stating\r\n```\r\nFile \"/workspace/llava/llava/model/llava.py\", line 430, in initialize_vision_tokenizer    \r\nassert input_embeddings.shape == embed_tokens_weight.shape \r\nTraceback (most recent call last): File \"llava/train/train_mem.py\", line 13\r\nin <module> train() File \"/workspace/llava/llava/train/train.py\", line 615\r\nin train model.initialize_vision_tokenizer( File \"/workspace/llava/llava/model/llava.py\", line 430\r\nin initialize_vision_tokenizer assert input_embeddings.shape == embed_tokens_weight.shape\r\n```\r\n\r\nI am passing in `--pretrain_mm_mlp_adapter` as the pretrain checkpoint listed in the repo at https://huggingface.co/liuhaotian/LLaVA-13b-pretrain-projector-v0/blob/main/LLaVA-13b-pretrain-projector-v0-CC3M-595K-original_caption.bin. I'm still running with the same commands as above - should I be changing something?\n</Comment>\n<Comment by haotian-liu at 2023-05-04T17:54:26Z>\n@jihan-yin My apologies, there is a fix that was not merged to the latest code base.  I just applied that fix, could you please pull the latest code base and try again?  Thank you.\n</Comment>\n<Comment by jihan-yin at 2023-05-04T17:57:28Z>\nAh I ended up applying the same fix locally, glad to see it's in main as well! Thanks for the fast response too, will patch and try again\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 91,
    "state": "open",
    "created_by": "SinanAkkoyun",
    "created_at": "2023-05-04T01:49:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/91</URL>\n\n<TITLE>Upcoming model?</TITLE>\n\n<BODY>> We are working on a more capable model that is pretrained with the data at a larger scale. Stay tuned!\r\n\r\nHello again! :)\r\nWhat parameter size will this model be? And will it be based on > v0 vicuna?\r\n\r\nThank you very much for this awesome work!!!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-04T05:52:41Z>\nHi @SinanAkkoyun, thank you for your interest in our work.\r\n\r\nThis is WIP, and we haven't make every decision choices yet.  As of today, we build future LLaVA models based on Vicuna-v1 checkpoints.  We are also exploring to improve the model's performance without largely increasing the model parameter size. We may explore other larger backbones in the future.  Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 90,
    "state": "closed",
    "created_by": "StrangeTcy",
    "created_at": "2023-05-03T14:03:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/90</URL>\n\n<TITLE>the lightning training script</TITLE>\n\n<BODY>That amazing script you mention in the readme for training [LLaVA Lightning](https://github.com/haotian-liu/LLaVA#train-llava-lightning): \r\n`bash ./scripts/train_lightning.sh {v0,v1}`?\r\nWell, it doesn't exist</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-03T16:14:52Z>\nSorry that was not committed and pushed successfully. Just uploaded the script. Thanks!\n</Comment>\n<Comment by StrangeTcy at 2023-05-04T06:18:03Z>\nThanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 89,
    "state": "closed",
    "created_by": "donggeai",
    "created_at": "2023-05-03T08:04:14Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/89</URL>\n\n<TITLE>chat in Web UI, but throws exception AttributeError: 'NoneType' object has no attribute 'shape'</TITLE>\n\n<BODY>when I complete configuring the environment well and start the web api, and test chating in broswer, erroe happened: NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.\r\n![image](https://user-images.githubusercontent.com/53808058/235861099-a212fef1-94eb-433f-a6dd-4e592c809edf.png)\r\n\r\nthe model list is not empty, and backend outout the exception as follows:\r\n![20230503155527](https://user-images.githubusercontent.com/53808058/235861427-e45bd3dc-d8f3-49e0-ba25-e73477f4da12.png)\r\nabsolutely, this error is caused by transformers, and I installed transformers by:\r\n```bash\r\npip install git+https://github.com/haotian-liu/transformers_llava.git@988b6abb3b7da9a5cbb5051e994706f7f88c2565\r\n```\r\nI guess it's due to inappropriate installation of transformers or bug of transformers_llava. Tks a lot.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-03T16:16:27Z>\nHi, thank you for your interest in our work.  It seems that you are using the latest code base (with the present of `llava.py` but the old `transformers`). Please upgrade to the latest code base following the instructions [here](https://github.com/haotian-liu/LLaVA#upgrade-to-v01).  Thanks!\n</Comment>\n<Comment by haoxurt at 2023-05-22T08:26:11Z>\n@corleytd How did you start web ui?  My  ip  is 10.227.70.87, port is 8081\n</Comment>\n<Comment by haotian-liu at 2023-07-19T17:48:31Z>\nClosing due to inactivity. Please feel free to re-open if you still encounter any issues, thanks.\n</Comment>\n<Comment by LiuDongDaniel at 2025-03-14T09:58:53Z>\ni change the numpy verison to 1.26.4. and solve this problem\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 88,
    "state": "closed",
    "created_by": "gordonhu608",
    "created_at": "2023-05-03T07:48:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/88</URL>\n\n<TITLE>Difference of using image start end token or not</TITLE>\n\n<BODY>I'm very interested in the performance of using image start and end tokens or not. Did you try setting it to false, i.e only using image token? If the model only has image patch features, but for each patch, the model doesn't know the image start and image end token around, which is a strong indicator helping the language model.  Does the model still recognize these image-patched features effectively? Will it harm the performance?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-03T16:18:01Z>\nHi @gordonhu608, we do not notice a significant difference.  And our ScienceQA model was trained without these tokens.  However, we still add these tokens as a plan for the future releases of supporting a more flexible image backbone, as well as different image aspect ratio and image resolution.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 87,
    "state": "closed",
    "created_by": "gray311",
    "created_at": "2023-05-03T06:48:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/87</URL>\n\n<TITLE>Can the code for generating instruction data using GPT-4 be made public?</TITLE>\n\n<BODY>Can the code for generating instruction data using GPT-4 be made public?\r\nBecause I have two related questions:\r\n1. I would like to know if you use bounding boxes when generating all three types of data? Because table 11 and table 12 in your paper do not contain bounding boxes.\r\n2. I see that all the queries in the data (llava_instruct_150k.json) are marked as from human, how did you collect these queries? Is the self-instrut method used?\r\n\r\nI would appreciate it if you could answer my question!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-03T16:22:09Z>\nHi @gray311, thank you for your interest in our work.\r\n\r\nThe conversation data was generated without the bounding boxes, while the detailed description and reasoning data is generated with bounding boxes.\r\n\r\nWe generate the data by querying text-only GPT-4 following a similar pipeline as self-instruct, except that we are using fewer few-shot samples.  And marking them as `Human: ` and `GPT` is simply complying with the format in `Vicuna` code base that we build with.\r\n\r\nAll system messages, prompts, and few-shot samples are released [here](https://github.com/haotian-liu/LLaVA/tree/main/playground/data/prompts).  You may refer to them on detailed format and content that we used to generate these data.\r\n\r\nPlease let me know if you have further concerns.\n</Comment>\n<Comment by gray311 at 2023-05-04T13:13:50Z>\n> o\r\n\r\nThank you for your reply!\r\n\r\nI have one more question, where did you get this query from in the process of generating the conversations?\r\n![屏幕截图 2023-05-04 211052](https://user-images.githubusercontent.com/64787301/236215074-21e7ca2d-a79f-49db-935c-9cf9e23441b8.png)\n</Comment>\n<Comment by gray311 at 2023-05-04T13:42:56Z>\nI have one more question, does this no instruction tuning mean that neither stage 1 training nor stage 2 training was done? Or is it the result of stage 1 training only? If it is the former, then is there a result of only doing stage 1 training?\r\n![image](https://user-images.githubusercontent.com/64787301/236223955-987c941b-3bb5-4dfd-9c83-47ad284925d4.png)\n</Comment>\n<Comment by hjjpku at 2023-05-04T16:05:49Z>\n> > o\r\n> \r\n> Thank you for your reply!\r\n> \r\n> I have one more question, where did you get this query from in the process of generating the conversations? ![屏幕截图 2023-05-04 211052](https://user-images.githubusercontent.com/64787301/236215074-21e7ca2d-a79f-49db-935c-9cf9e23441b8.png)\r\n\r\nI guess that \"sample['context']\" refers to the captions of an image in a given sample, while \"sample['response']\" is the conversation in the sample. Therefore, the query is intended to be the captions of a image selected from COCO dataset, which will serve as the input for generating conversations?\n</Comment>\n<Comment by haotian-liu at 2023-05-05T04:30:20Z>\n@hjjpku Your understanding is correct :)  And for complex reasoning and detailed description, we also add bounding boxes.  You may refer to [here](https://github.com/haotian-liu/LLaVA/tree/main/playground/data/prompts) for each prompt that we use.\n</Comment>\n<Comment by haotian-liu at 2023-05-05T04:31:04Z>\n@gray311 Regarding `query` please see my response above.  And no-instruction tuning means that we only perform the first stage training, and do the evaluation.\n</Comment>\n<Comment by gray311 at 2023-05-05T05:08:50Z>\n@haotian-liu Thanks for your reply!\n</Comment>\n<Comment by gray311 at 2023-05-05T05:10:16Z>\n@hjjpku Thanks for your reply!\n</Comment>\n<Comment by RyanHuangNLP at 2023-05-15T17:01:35Z>\n@hjjpku I still have question about generating instruction data using GPT-4, the [data prompt](https://github.com/haotian-liu/LLaVA/tree/main/playground/data/prompts) provide in the repo, the [000_caps.txt](https://github.com/haotian-liu/LLaVA/blob/main/playground/data/prompts/complex_reasoning/000_caps.txt) and [000_conv.txt](https://github.com/haotian-liu/LLaVA/blob/main/playground/data/prompts/complex_reasoning/000_conv.txt),  is it a pre-designed pattern that is used to guide GPT-4 to generate data in a similar pattern?\n</Comment>\n<Comment by gray311 at 2023-05-16T07:34:30Z>\n@RyanHuangNLP I think you are right.\n</Comment>\n<Comment by rex-yue-wu at 2023-06-11T12:12:16Z>\n@haotian-liu Great work. I have a follow-up question -- after you create instruction-following samples, how do you reject a bad generated sample? I didn't see it anywhere in the paper, but I am curious about it, as I probably need to follow your work and generate more samples.\n</Comment>\n<Comment by YoojLee at 2024-01-24T08:27:29Z>\n@haotian-liu hi, thanks for such a great work. I have one follow-up question, why did you decide to not include bounding boxes into the context for the conversation type? I think, given the object information as a context, GPT-4 can generate more accurate answers for the questions regarding object location or object counts! Your reply would help me a lot, thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 86,
    "state": "closed",
    "created_by": "CyberTimon",
    "created_at": "2023-05-03T05:55:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/86</URL>\n\n<TITLE>What is Lighting V1.1?</TITLE>\n\n<BODY>I didn't saw any informations on it how it differs from llava 7b v0. \r\nCan anyone please explain this?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-03T16:28:17Z>\nHi @CyberTimon, LLaVA-Lightning V1.1 has three differences with LLaVA-7B-v0 (mainly in training):\r\n\r\n1. Data.  We create a 558K concept-balanced subset from LAION/SBU/CC, which is a much larger concept converage than the original CC data, while the amount samples are roughly the same (558K / 595K), ensuring a similar cost of pretraining. Similarly, we filtered 80K instruction tuning data, composed of 40K conversation and 40K reasoning from our LLaVA-Instruct-158K.\r\n2. Training.  Instruction tuning stage is only performed for one epoch for Lightning, and given the smaller dataset, it is 4x faster training than LLaVA-7B-v0.\r\n3. Base model.  We use Vicuna-1.1 as the base model for Lightning, and we will use Vicuna V1 series for future releases.  The prompt templates are slightly modified to a more standard format.\r\nWith more concept coverage, smaller data, and fewer training epochs, our Lightning model achieves a similar performance to LLaVA-7B-v0, while keeping the cost down to as few as 3 hours training with $40.\r\n\r\nHope this clarifies the question.  We are also working on a MODEL_ZOO page to clarify these differences in training data/schedule/cost, and will also add to our paper in the revision.  Thank you, and please let me know if you have further questions.\n</Comment>\n<Comment by CyberTimon at 2023-05-03T17:32:44Z>\nThank you for the great answer! This answers my question.\n</Comment>\n<Comment by Tongcheng at 2023-05-06T18:00:01Z>\nQuestion for Lightning 7B, in apply_delta, should we use LLaMA-7B as base or Vicuna-7B (with Vicuna Delta already applied) as base? Thank you.\n</Comment>\n<Comment by haotian-liu at 2023-05-06T18:25:34Z>\nPlease use the LLaMA-7B as the base. We train based on Vicuna-7B, but release the delta from LLaMA, so that people do not need to convert the Vicuna first in order to use our checkpoint. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 84,
    "state": "closed",
    "created_by": "SinanAkkoyun",
    "created_at": "2023-05-03T01:24:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/84</URL>\n\n<TITLE>LLaVa demo hardware + parallelization</TITLE>\n\n<BODY>Hello, thank you so much for the awesome work!\r\n\r\nOn what hardware does the demo run? It's very fast. Did you use a quantized model or the raw model as is?\r\n\r\nAlso, for even faster generation, is it possible to do model parallelization (not only VRAM sharing but actual load balancing) and which framework would be suitable?\r\n\r\nThank you very much!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-03T02:00:55Z>\nHi @SinanAkkoyun for our demo (13B), it is running on a A6000 for most of the time, recently on A100 for several days due to the maintenance of A6000 server in our lab.  For our recently release Lightning-7B, it is running on RTX 3090.  And all our models are just using FP16, without quantization.  You may see [here](https://github.com/oobabooga/text-generation-webui/tree/main/extensions/llava) for an aggressively compressed 4-bit checkpoint.  Support for 4-bit/8-bit in this repo is planned, and if you are interested, feel free to contribute (let me know first so that we do not do duplicate work :)\r\n\r\nRegarding model parallelization for speed up inference, I am not very familiar this so I am afraid that I may not be able to provide insights on this :( And if you hear something regarding this that you feel can help LLaVA in the future, please don't hesitate to let me know :)\n</Comment>\n<Comment by SinanAkkoyun at 2023-05-03T10:00:18Z>\nHi @haotian-liu \r\nI cannot thank you enough for the very kind help and comment! :)\r\n\r\nI am locally running the 7b model on a 3090 and also tried the 4bit 128g quantized 13b model via text-generation-webui.\r\nI personally want to compare output quality with quantized vs raw fp16 models as I get the feeling that quantization performance is not as great as I hoped.\r\nWhen I do plan on implementing GPTQ into this repo I surely will contact you! But I have to admit that I have no experience in writing or porting quantization inference code, so I did not plan on implementing it sadly\r\n\r\nIf I hear something about LLaMa model parallelization I will surely post it here and try help to implement it!\r\n\r\nThank you very much :)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 83,
    "state": "closed",
    "created_by": "Unrealluver",
    "created_at": "2023-05-02T10:38:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/83</URL>\n\n<TITLE>Finetune errors</TITLE>\n\n<BODY>Hi, thanks for your latest commit! I noticed that I could not fine-tune with my pretrained ckpt `/home/zhulianghui/ProjectC_ChatGPT/llava/checkpoints/llava-13b-pretrain-8x40g-a100-fsdp-vicuna-0501-2`. When I execute the command like this:\r\n```\r\nbash /client-tools/repair_A100.sh\r\nsource /opt/conda/bin/activate /home/zhulianghui/.conda/envs/llava_beta\r\ntorchrun --nnodes=1 --nproc_per_node=4 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path /home/zhulianghui/ProjectC_ChatGPT/llava/checkpoints/llava-13b-pretrain-8x40g-a100-fsdp-vicuna-0501-2 \\\r\n    --data_path /share/project/datasets/LLaVA-Instruct-150K/llava_instruct_150k.json \\\r\n    --image_folder /share/project/datasets/MSCOCO/coco2017/train2017 \\\r\n    --vision_tower /share/project/wyz/MM/models/openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter /home/zhulianghui/ProjectC_ChatGPT/llava/checkpoints/llava-13b-pretrain-8x40g-a100-fsdp-vicuna-0501-2/llava-13b-pretrain.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --output_dir ./checkpoints/llava-13b-finetune \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 8 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 5000 \\\r\n    --save_total_limit 3 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --bf16 True \\\r\n    --fsdp \"full_shard auto_wrap offload\" \\\r\n    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --run_name \"vicuna-13b-clip-l-finetune-llava\" \\\r\n    --lazy_preprocess True \\\r\n    --report_to mlflow\r\n```\r\nI got the error below:\r\n![image](https://user-images.githubusercontent.com/27504120/235639600-69e0d5f8-66ef-49ad-a07f-5bc6c0983a50.png)\r\n\r\nI checked the `llava/model/llava.py` and set several logs like below:\r\n```\r\n        if mm_use_im_start_end:\r\n            num_new_tokens = tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\r\n            print(f\"Added {num_new_tokens} tokens to the tokenizer.\")\r\n            self.resize_token_embeddings(len(tokenizer))\r\n            vision_config.im_start_token, vision_config.im_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN])\r\n            print(f\"Set im_start_token to {vision_config.im_start_token} and im_end_token to {vision_config.im_end_token}.\")\r\n\r\n```\r\n\r\nthe `num_new_tokens` is 0, which means that the tokenizer already contains the `DEFAULT_IM_START_TOKEN` and `DEFAULT_IM_END_TOKEN`. So the code below will face some errors: 1. `input_embeddings` referenced before the assignment. 2. num_new_tokens != 2 \r\n\r\n```\r\n            if pretrain_mm_mlp_adapter:\r\n                mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\r\n                embed_tokens_weight = mm_projector_weights['model.embed_tokens.weight']\r\n                assert input_embeddings.shape == embed_tokens_weight.shape\r\n                assert num_new_tokens == 2\r\n                input_embeddings[-num_new_tokens:] = embed_tokens_weight[-num_new_tokens:]\r\n\r\n```\r\nSo, I change the codes to these:\r\n```\r\n            if pretrain_mm_mlp_adapter:\r\n                if num_new_tokens > 0:\r\n                    mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\r\n                    embed_tokens_weight = mm_projector_weights['model.embed_tokens.weight']\r\n                    assert input_embeddings.shape == embed_tokens_weight.shape\r\n                    assert num_new_tokens == 2\r\n                    input_embeddings[-num_new_tokens:] = embed_tokens_weight[-num_new_tokens:]\r\n                else:\r\n                    input_embeddings = self.get_input_embeddings().weight.data\r\n                    mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\r\n                    embed_tokens_weight = mm_projector_weights['model.embed_tokens.weight']\r\n                    assert (input_embeddings == embed_tokens_weight).all()\r\n```\r\nIt could continue to fine-tune. I wonder if the changes lead to a wrong fine-tuning process. I would be appreciate if you could give me some guidance!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-02T13:52:22Z>\nHi @Unrealluver, maybe there is some ambiguity in the way I name my parameter arguments.\r\n\r\n`--pretrain_mm_mlp_adapter /home/zhulianghui/ProjectC_ChatGPT/llava/checkpoints/llava-13b-pretrain-8x40g-a100-fsdp-vicuna-0501-2/llava-13b-pretrain.bin` loads the pretrained projector from this file `args.pretrain_mm_mlp_adapter`, that is extracted using `scripts/extract_projector.py`.\r\nThe based checkpoint you loaded is `--model_name_or_path /home/zhulianghui/ProjectC_ChatGPT/llava/checkpoints/llava-13b-pretrain-8x40g-a100-fsdp-vicuna-0501-2`, also seems to be some LLaVA-pretrained/finetuned checkpoints.\r\n\r\nThis suggests that either the projector weights are contained in both `args.model_name_or_path` and `args.pretrain_mm_mlp_adapter`.  It will either be redundant, or there would be a conflict.\r\n\r\nIf you would like to finetune the projector only, you can add `--tune_mm_mlp_adapter`.\r\nIf the projector weights in `args.model_name_or_path` and `args.pretrain_mm_mlp_adapter` is the same, then you can either (1) do not use `--pretrain_mm_mlp_adapter` at all, or (2) set `args.model_name_or_path` to `Vicuna` weights, if there are no other weights tuned.\r\n\r\n`args.pretrain_mm_mlp_adapter` is used so that (1) we can share to public the pretrained projectors (first stage in our case) without downloading the untuned Vicuna weights. (2) you can share pretrained projector easily between servers for the second stage instruction tuning -- it's much smaller than the full model weights.  If you are using the same server to train/finetune your checkpoint, you may simply choose not using this :)\r\n\r\nPlease let me know if there are still things that's unclear.\n</Comment>\n<Comment by Unrealluver at 2023-05-02T14:20:00Z>\nThanks for your detailed answer! I will try it again.\n</Comment>\n<Comment by Unrealluver at 2023-05-04T14:36:56Z>\nThanks for your help in solving this problem. I still have some questions about the checkpoint saved by `LLaVA pretrain (step1)`\r\n\r\nHere are the contents of `vicuna-13b-v0`:\r\n![image](https://user-images.githubusercontent.com/27504120/236236578-33d27608-8ddd-49d3-85db-38f9ae7a74df.png)\r\n\r\nAnd here are the contents of checkpoints saved by `LLaVA pretrain (step1)`\r\n![image](https://user-images.githubusercontent.com/27504120/236236953-8fb55dd4-67e9-4264-9bd4-600f62127ad7.png)\r\n\r\nMy questions are as follows:\r\n(1) If the contents of checkpoints saved by `LLaVA pretrain (step1)` contains parameters of `vicuna-13b-v0`, `projection layer`, and `CLIP-L`?\r\n(2) If the answer to (1) is yes. If the `vicuna-13b-v0` and `LLaVA pretrain (step1)` contains the same weights for the language part?\r\n(3) If the `LLaVA pretrain (step1)` contains the same weights as `CLIP-L` for the vision part expect for the `projection layer`?\r\n(4) When the `--tune_mm_mlp_adapter` is `True`, why do the codes in `llava/model/llava.py` work as bellow:\r\n```\r\n                        if orig_embeds_params is not None:\r\n                            cur_new_input_embeds = torch.cat((cur_input_embeds[:image_start_token_pos].detach(), cur_input_embeds[image_start_token_pos:image_start_token_pos+1], cur_image_features, cur_input_embeds[image_start_token_pos + num_patches + 1:image_start_token_pos + num_patches + 2], cur_input_embeds[image_start_token_pos + num_patches + 2:].detach()), dim=0)\r\n                        else:\r\n                            cur_new_input_embeds = torch.cat((cur_input_embeds[:image_start_token_pos+1], cur_image_features, cur_input_embeds[image_start_token_pos + num_patches + 1:]), dim=0)\r\n```\r\n\r\nThe `:image_start_token_pos+1` is strange to me. Why we should add `.detach()` here? I think we have set the whole model except the `projection layer` to `required_grad = False` already. The long codes here with long indices puzzle me a lot. I will appreciate it if you could give me some guidance here.\r\n\r\nThanks for your time!\n</Comment>\n<Comment by haotian-liu at 2023-05-05T04:26:50Z>\nHi @Unrealluver,\r\n\r\nRegarding your questions\r\n(1) it contains vicuna-13b-v0 + projection layer (and potentially additional token embeddings for <im_start> and <im_end> etc), it does not contain CLIP-L.\r\n(2) yes, so that we have this [extract_projector.py](https://github.com/haotian-liu/LLaVA/blob/main/scripts/extract_mm_projector.py) that allow you to only share the projectors and do not need to share other weights.\r\n(3) yes.\r\n(4) This is for tuning the `<im_start>` and `<im_end>` only, and for other token embeddings, it will be detached and not updated.\r\n\r\nPlease let me know if there are anything unclear, thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 82,
    "state": "open",
    "created_by": "tensorboy",
    "created_at": "2023-05-02T05:43:57Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/82</URL>\n\n<TITLE>How to make an image request to test_message.py</TITLE>\n\n<BODY>I'm trying to send image to test_message.py.\r\n\r\nHere is the code I've modified:\r\n```\r\nimport argparse\r\nimport json\r\n\r\nimport requests\r\nimport hashlib\r\nimport base64\r\nfrom llava.conversation import default_conversation\r\nfrom io import BytesIO\r\nfrom PIL import Image\r\n\r\n'''\r\npython -m llava.serve.test_message --model-name LLaVA-7B-v0 --controller http://localhost:10000\r\n'''\r\ndef main():\r\n    if args.worker_address:\r\n        worker_addr = args.worker_address\r\n    else:\r\n        controller_addr = args.controller_address\r\n        ret = requests.post(controller_addr + \"/refresh_all_workers\")\r\n        ret = requests.post(controller_addr + \"/list_models\")\r\n        models = ret.json()[\"models\"]\r\n        models.sort()\r\n        print(f\"Models: {models}\")\r\n\r\n        ret = requests.post(controller_addr + \"/get_worker_address\",\r\n            json={\"model\": args.model_name})\r\n        worker_addr = ret.json()[\"address\"]\r\n        print(f\"worker_addr: {worker_addr}\")\r\n\r\n    if worker_addr == \"\":\r\n        return\r\n\r\n    conv = default_conversation.copy()\r\n    conv.append_message(conv.roles[0], args.message)\r\n    prompt = conv.get_prompt()\r\n\r\n    headers = {\"User-Agent\": \"LLaVA Client\"}\r\n    pload = {\r\n        \"model\": args.model_name,\r\n        \"prompt\": prompt,\r\n        \"max_new_tokens\": args.max_new_tokens,\r\n        \"temperature\": 0.7,\r\n        \"stop\": conv.sep,\r\n    }\r\n    image_path = '/home/tensorboy/LLaVA/images/llava_logo.png'\r\n    image = Image.open(image_path)\r\n    image = image.resize((224, 224))\r\n    buffered = BytesIO()\r\n    image.save(buffered, format=\"JPEG\")\r\n    img_b64_str = base64.b64encode(buffered.getvalue()).decode()\r\n    #images.append(img_b64_str)\r\n    encoded_image = img_b64_str\r\n    all_images = [encoded_image]\r\n    #print(all_images)\r\n    #all_image_hash = [hashlib.md5(image.tobytes()).hexdigest() for image in all_images]\r\n    #print(all_image_hash)\r\n    pload['images'] = all_images\r\n    response = requests.post(worker_addr + \"/worker_generate_stream\", headers=headers,\r\n            json=pload, stream=True)\r\n\r\n    print(prompt.replace(conv.sep, \"\\n\"), end=\"\")\r\n    for chunk in response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b\"\\0\"):\r\n        if chunk:\r\n            data = json.loads(chunk.decode(\"utf-8\"))\r\n            output = data[\"text\"].split(conv.sep)[-1]\r\n            print(output, end=\"\\r\")\r\n    print(\"\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--controller-address\", type=str, default=\"http://localhost:21001\")\r\n    parser.add_argument(\"--worker-address\", type=str)\r\n    parser.add_argument(\"--model-name\", type=str, default=\"facebook/opt-350m\")\r\n    parser.add_argument(\"--max-new-tokens\", type=int, default=32)\r\n    parser.add_argument(\"--message\", type=str, default=\r\n        \"Describe the image in detail.\")\r\n    args = parser.parse_args()\r\n\r\n    main()\r\n\r\n```\r\n\r\nand it just stucked at here\r\n```\r\npython -m llava.serve.test_message --model-name LLaVA-7B-v0 --controller http://localhost:10000\r\nModels: ['LLaVA-7B-v0']\r\nworker_addr: http://localhost:40000\r\n```\r\n\r\nany suggestions to fix?</BODY>\n\n<COMMENTS>\n<Comment by tensorboy at 2023-05-02T05:54:06Z>\nIt has some errors now:\r\n\r\n```\r\npython -m llava.serve.test_message_new --model-name LLaVA-7B-v0 --controller http://localhost:10000\r\nModels: ['LLaVA-7B-v0']\r\nworker_addr: http://localhost:40000\r\nA chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\r\nHuman: What are the key differences between renewable and non-renewable energy sources?\r\nAssistant: Renewable energy sources are those that can be replenished naturally in a relatively short amount of time, such as solar, wind, hydro, geothermal, and biomass. Non-renewable energy sources, on the other hand, are finite and will eventually be depleted, such as coal, oil, and natural gas. Here are some key differences between renewable and non-renewable energy sources:\r\n1. Availability: Renewable energy sources are virtually inexhaustible, while non-renewable energy sources are finite and will eventually run out.\r\n2. Environmental impact: Renewable energy sources have a much lower environmental impact than non-renewable sources, which can lead to air and water pollution, greenhouse gas emissions, and other negative effects.\r\n3. Cost: Renewable energy sources can be more expensive to initially set up, but they typically have lower operational costs than non-renewable sources.\r\n4. Reliability: Renewable energy sources are often more reliable and can be used in more remote locations than non-renewable sources.\r\n5. Flexibility: Renewable energy sources are often more flexible and can be adapted to different situations and needs, while non-renewable sources are more rigid and inflexible.\r\n6. Sustainability: Renewable energy sources are more sustainable over the long term, while non-renewable sources are not, and their depletion can lead to economic and social instability.\r\n\r\nHuman: Describe the image in detail.\r\nTraceback (most recent call last):\r\n  File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/urllib3/response.py\", line 761, in _update_chunk_length\r\n    self.chunk_left = int(line, 16)\r\nValueError: invalid literal for int() with base 16: b''\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/urllib3/response.py\", line 444, in _error_catcher\r\n    yield\r\n  File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/urllib3/response.py\", line 828, in read_chunked\r\n    self._update_chunk_length()\r\n  File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/urllib3/response.py\", line 765, in _update_chunk_length\r\n    raise InvalidChunkLength(self, line)\r\nurllib3.exceptions.InvalidChunkLength: InvalidChunkLength(got length b'', 0 bytes read)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/requests/models.py\", line 816, in generate\r\n    yield from self.raw.stream(chunk_size, decode_content=True)\r\n  File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/urllib3/response.py\", line 624, in stream\r\n    for line in self.read_chunked(amt, decode_content=decode_content):\r\n  File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/urllib3/response.py\", line 816, in read_chunked\r\n    with self._error_catcher():\r\n  File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/contextlib.py\", line 153, in __exit__\r\n    self.gen.throw(typ, value, traceback)\r\n  File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/urllib3/response.py\", line 461, in _error_catcher\r\n    raise ProtocolError(\"Connection broken: %r\" % e, e)\r\nurllib3.exceptions.ProtocolError: (\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/tensorboy/LLaVA/llava/serve/test_message_new.py\", line 81, in <module>\r\n    main()\r\n  File \"/home/tensorboy/LLaVA/llava/serve/test_message_new.py\", line 63, in main\r\n    for chunk in response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b\"\\0\"):\r\n  File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/requests/models.py\", line 865, in iter_lines\r\n    for chunk in self.iter_content(\r\n  File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/requests/models.py\", line 818, in generate\r\n    raise ChunkedEncodingError(e)\r\nrequests.exceptions.ChunkedEncodingError: (\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))\r\n\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 81,
    "state": "closed",
    "created_by": "tensorboy",
    "created_at": "2023-05-02T04:08:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/81</URL>\n\n<TITLE>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1!</TITLE>\n\n<BODY>With current transformers installed by (newest code in this repo):\r\n```\r\npip uninstall transformers\r\npip install git+https://github.com/huggingface/transformers@cae78c46\r\n```\r\nmy command:\r\n```\r\npython -m llava.serve.controller --host 0.0.0.0 --port 10000\r\n\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path /media/tensorboy/4df39736-fd86-443e-b6fe-a6ae0fe28d72/llama-dl-main/LLaVA-7B-v0 --multi-modal --num-gpus 2\r\n\r\npython -m llava.serve.gradio_web_server --controller http://localhost:10000 --share\r\n```\r\n\r\nlogs:\r\n\r\n```\r\n2023-05-01 20:46:05 | ERROR | stderr | INFO:     Started server process [3502503]\r\n2023-05-01 20:46:05 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2023-05-01 20:46:05 | ERROR | stderr | INFO:     Application startup complete.\r\n2023-05-01 20:46:05 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)\r\n2023-05-01 20:46:20 | INFO | model_worker | Send heart beat. Models: ['LLaVA-7B-v0']. Semaphore: None. global_counter: 0\r\n2023-05-01 20:46:35 | INFO | model_worker | Send heart beat. Models: ['LLaVA-7B-v0']. Semaphore: None. global_counter: 0\r\n2023-05-01 20:46:50 | INFO | model_worker | Send heart beat. Models: ['LLaVA-7B-v0']. Semaphore: None. global_counter: 0\r\n2023-05-01 20:47:05 | INFO | model_worker | Send heart beat. Models: ['LLaVA-7B-v0']. Semaphore: None. global_counter: 0\r\n2023-05-01 20:47:20 | INFO | model_worker | Send heart beat. Models: ['LLaVA-7B-v0']. Semaphore: None. global_counter: 0\r\n2023-05-01 20:47:35 | INFO | model_worker | Send heart beat. Models: ['LLaVA-7B-v0']. Semaphore: None. global_counter: 0\r\n2023-05-01 20:47:48 | INFO | stdout | INFO:     127.0.0.1:56932 - \"POST /worker_get_status HTTP/1.1\" 200 OK\r\n2023-05-01 20:47:50 | INFO | model_worker | Send heart beat. Models: ['LLaVA-7B-v0']. Semaphore: None. global_counter: 0\r\n2023-05-01 20:48:05 | INFO | model_worker | Send heart beat. Models: ['LLaVA-7B-v0']. Semaphore: None. global_counter: 0\r\n2023-05-01 20:48:20 | INFO | model_worker | Send heart beat. Models: ['LLaVA-7B-v0']. Semaphore: None. global_counter: 0\r\n2023-05-01 20:48:25 | INFO | model_worker | Send heart beat. Models: ['LLaVA-7B-v0']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n2023-05-01 20:48:25 | INFO | stdout | INFO:     127.0.0.1:57470 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\n2023-05-01 20:48:27 | ERROR | stderr | ERROR:    Exception in ASGI application\r\n2023-05-01 20:48:27 | ERROR | stderr | Traceback (most recent call last):\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py\", line 428, in run_asgi\r\n2023-05-01 20:48:27 | ERROR | stderr |     result = await app(  # type: ignore[func-returns-value]\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 78, in __call__\r\n2023-05-01 20:48:27 | ERROR | stderr |     return await self.app(scope, receive, send)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/fastapi/applications.py\", line 276, in __call__\r\n2023-05-01 20:48:27 | ERROR | stderr |     await super().__call__(scope, receive, send)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/applications.py\", line 122, in __call__\r\n2023-05-01 20:48:27 | ERROR | stderr |     await self.middleware_stack(scope, receive, send)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 184, in __call__\r\n2023-05-01 20:48:27 | ERROR | stderr |     raise exc\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 162, in __call__\r\n2023-05-01 20:48:27 | ERROR | stderr |     await self.app(scope, receive, _send)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\r\n2023-05-01 20:48:27 | ERROR | stderr |     raise exc\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\r\n2023-05-01 20:48:27 | ERROR | stderr |     await self.app(scope, receive, sender)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 21, in __call__\r\n2023-05-01 20:48:27 | ERROR | stderr |     raise e\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\r\n2023-05-01 20:48:27 | ERROR | stderr |     await self.app(scope, receive, send)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/routing.py\", line 718, in __call__\r\n2023-05-01 20:48:27 | ERROR | stderr |     await route.handle(scope, receive, send)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/routing.py\", line 276, in handle\r\n2023-05-01 20:48:27 | ERROR | stderr |     await self.app(scope, receive, send)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/routing.py\", line 69, in app\r\n2023-05-01 20:48:27 | ERROR | stderr |     await response(scope, receive, send)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/responses.py\", line 270, in __call__\r\n2023-05-01 20:48:27 | ERROR | stderr |     async with anyio.create_task_group() as task_group:\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 662, in __aexit__\r\n2023-05-01 20:48:27 | ERROR | stderr |     raise exceptions[0]\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/responses.py\", line 273, in wrap\r\n2023-05-01 20:48:27 | ERROR | stderr |     await func()\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/responses.py\", line 262, in stream_response\r\n2023-05-01 20:48:27 | ERROR | stderr |     async for chunk in self.body_iterator:\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/concurrency.py\", line 63, in iterate_in_threadpool\r\n2023-05-01 20:48:27 | ERROR | stderr |     yield await anyio.to_thread.run_sync(_next, iterator)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/anyio/to_thread.py\", line 31, in run_sync\r\n2023-05-01 20:48:27 | ERROR | stderr |     return await get_asynclib().run_sync_in_worker_thread(\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n2023-05-01 20:48:27 | ERROR | stderr |     return await future\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\r\n2023-05-01 20:48:27 | ERROR | stderr |     result = context.run(func, *args)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/concurrency.py\", line 53, in _next\r\n2023-05-01 20:48:27 | ERROR | stderr |     return next(iterator)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/LLaVA/llava/serve/model_worker.py\", line 292, in generate_stream_gate\r\n2023-05-01 20:48:27 | ERROR | stderr |     for x in self.generate_stream(params):\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 43, in generator_context\r\n2023-05-01 20:48:27 | ERROR | stderr |     response = gen.send(None)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/LLaVA/llava/serve/model_worker.py\", line 239, in generate_stream\r\n2023-05-01 20:48:27 | ERROR | stderr |     out = model(\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\r\n2023-05-01 20:48:27 | ERROR | stderr |     return forward_call(*input, **kwargs)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2023-05-01 20:48:27 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/LLaVA/llava/model/llava.py\", line 218, in forward\r\n2023-05-01 20:48:27 | ERROR | stderr |     outputs = self.model(\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\r\n2023-05-01 20:48:27 | ERROR | stderr |     return forward_call(*input, **kwargs)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/LLaVA/llava/model/llava.py\", line 159, in forward\r\n2023-05-01 20:48:27 | ERROR | stderr |     cur_new_input_embeds = torch.cat((cur_input_embeds[:image_start_token_pos+1], cur_image_features, cur_input_embeds[image_start_token_pos + num_patches + 1:]), dim=0)\r\n2023-05-01 20:48:27 | ERROR | stderr | RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument tensors in method wrapper_cat)\r\n\r\n```\r\n\r\nrelevant issues:\r\nhttps://github.com/haotian-liu/LLaVA/issues/20\r\nhttps://github.com/haotian-liu/LLaVA/issues/60</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-02T04:32:04Z>\nThanks for letting me know, this patch was not merged to the new training pipeline.  It should be fixed now :)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 80,
    "state": "closed",
    "created_by": "tensorboy",
    "created_at": "2023-05-02T00:42:50Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/80</URL>\n\n<TITLE>Error When Fine-Tuning</TITLE>\n\n<BODY>When run the fine-tuning code:\r\n```  \r\nFileNotFoundError: [Errno 2] No such file or directory: .../coco/train2014/000000185326.jpg'\r\n  ```\r\n\r\nAnd I've checked the coco filename, I run it:\r\n```\r\nls|grep 000000185326.jpg\r\n```\r\nshows\r\n```\r\nCOCO_train2014_000000185326.jpg\r\n```</BODY>\n\n<COMMENTS>\n<Comment by tensorboy at 2023-05-02T00:46:07Z>\nuse train2017 fix it..\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 79,
    "state": "closed",
    "created_by": "tensorboy",
    "created_at": "2023-05-01T22:32:50Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/79</URL>\n\n<TITLE>Errors when fine-tuning</TITLE>\n\n<BODY>```\r\n    File \"/mnt/bd/data-tns-algo-masp-llm/experiment/LLaVA/llava/train/train.py\", line 567, in train\r\n      trainer.train()\r\n    File \"/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/caption/lib/python3.9/site-packages/transformers/trainer.py\", line 1644, in train\r\n      return inner_training_loop(\r\n    File \"/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/caption/lib/python3.9/site-packages/transformers/trainer.py\", line 1731, in _inner_training_loop\r\n      model = self._wrap_model(self.model_wrapped)\r\n    File \"/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/caption/lib/python3.9/site-packages/transformers/trainer.py\", line 1469, in _wrap_model\r\n      self.model = model = FSDP(\r\n  TypeError: __init__() got an unexpected keyword argument 'forward_prefetch'\r\n```\r\n\r\nseems it needs downgrade transformers to 4.26.1:\r\nhttps://github.com/huggingface/transformers/issues/22446</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-01T22:41:46Z>\nHi, we use PyTorch 2.0 for developing the code, and it seems that this is an argument newly introduced in later PyTorch versions, which is addressed in this PR: https://github.com/huggingface/transformers/pull/22489. I will take a look into this later on the compatibility issue of upgrading the `transformers` package.\r\n\r\nAs a solution, I would suggest upgrading the PyTorch to 2.0, instead of downgrading `transformers`, as downgrading `trasnformers` may give rise to other incompatibility issues.\r\n\r\nBtw, maybe this can solve the `torchrun` with 8-GPU issue as well?\n</Comment>\n<Comment by tensorboy at 2023-05-01T22:43:53Z>\n> Hi, we use PyTorch 2.0 for developing the code, and it seems that this is an argument newly introduced in later PyTorch versions, which is addressed in this PR: [huggingface/transformers#22489](https://github.com/huggingface/transformers/pull/22489). I will take a look into this later on the compatibility issue of upgrading the `transformers` package.\r\n> \r\n> As a solution, I would suggest upgrading the PyTorch to 2.0, instead of downgrading `transformers`, as downgrading `trasnformers` may give rise to other incompatibility issues.\r\n> \r\n> Btw, maybe this can solve the `torchrun` with 8-GPU issue as well?\r\n\r\nI'm using pytorch 1.12.1 now, I will upgrade to pytorch 2.0 and have a try, will let you know if it can work with 8-GPUs. \r\n\r\nThanks for the quick feedbacks!!!\n</Comment>\n<Comment by sameeravithana at 2023-06-04T18:30:23Z>\nIs it working with pytorch 1.12.1? I am still facing the issue. The problem is that I can not update torch due to dependency in the cuda drivers 11.4.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 78,
    "state": "closed",
    "created_by": "tensorboy",
    "created_at": "2023-05-01T21:38:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/78</URL>\n\n<TITLE>run_llava.py may need modify after switch transformers?</TITLE>\n\n<BODY>The error after I switch to transformers from transformers by:\r\n```\r\npip uninstall transformers\r\npip install git+https://github.com/huggingface/transformers@cae78c46\r\n```\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/llava/eval/run_llava.py\r\n\r\n```\r\npython -m llava.eval.run_llava \\\r\n>     --model-name /mnt/bd/data-tns-algo-masp-llm/weights/llama-dl-main/LLaVA-13B-v0 \\\r\n>     --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n>     --query \"What are the things I should be cautious about when I visit here?\"\r\n\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████| 3/3 [00:16<00:00,  5.36s/it]\r\nTraceback (most recent call last):\r\n  File \"/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/caption/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/caption/lib/python3.9/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/mnt/bd/data-tns-algo-masp-llm/experiment/LLaVA/llava/eval/run_llava.py\", line 164, in <module>\r\n    eval_model(args)\r\n  File \"/mnt/bd/data-tns-algo-masp-llm/experiment/LLaVA/llava/eval/run_llava.py\", line 65, in eval_model\r\n    vision_tower = model.model.vision_tower[0]\r\n  File \"/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/caption/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1614, in __getattr__\r\n    raise AttributeError(\"'{}' object has no attribute '{}'\".format(\r\nAttributeError: 'LlamaModel' object has no attribute 'vision_tower'\r\n```</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-01T22:10:23Z>\nThanks for the feedback, it has been fixed now :)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 77,
    "state": "closed",
    "created_by": "SinanAkkoyun",
    "created_at": "2023-05-01T18:44:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/77</URL>\n\n<TITLE>CLIP grid size?</TITLE>\n\n<BODY>Hello! Thank you so much for the awesome work!\r\n\r\nI was wondering what grid size you actually use and embed with CLIP? I could not find any specifics on that in the paper.\r\n\r\nThank you very much!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-01T22:11:29Z>\nHi, thank you for your interest in our work.  The feature backbone we use is CLIP ViT-L/14.  We'll add these details into our revision, thank you!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 76,
    "state": "closed",
    "created_by": "tensorboy",
    "created_at": "2023-05-01T05:24:30Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/76</URL>\n\n<TITLE>Any Plan to add how to run 4bit 13B LLava at the README.md for a demo?</TITLE>\n\n<BODY></BODY>\n\n<COMMENTS>\n<Comment by tensorboy at 2023-05-01T05:24:53Z>\n> _No description provided._\n</Comment>\n<Comment by tensorboy at 2023-05-01T05:26:02Z>\nThe instructions at here is unclear: https://github.com/oobabooga/text-generation-webui/blob/main/extensions/llava/README.md\r\n\r\nit would be great if you can officially support it, I believe it will make the model more accessible and more popular.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 75,
    "state": "closed",
    "created_by": "Unrealluver",
    "created_at": "2023-04-30T12:38:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/75</URL>\n\n<TITLE>Question about abnormal response</TITLE>\n\n<BODY>Thanks for your awesome work again! When I run the pretrained code in the latest FSDP style code, I use the command here for a test.\r\n```\r\npython -m llava.eval.run_llava \\\r\n    --model-name /path/to/LLaVA-13B-v0 \\\r\n    --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n    --query \"What are the things I should be cautious about when I visit here?\"\r\n```\r\n\r\nBut I got the output like this:\r\n![f308bbde-8fc9-42ed-96ef-7e5992745043](https://user-images.githubusercontent.com/27504120/235353315-7d4cedda-8e62-49f7-a30b-f7b38d173fac.jpeg)\r\n\r\nIt seems that the model knows something about the image, but there are still some problems here that lead to a strange response.\r\nCould you please help me with some possible reasons for that? Thank you very much!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-05-01T01:54:09Z>\nHi @Unrealluver, the issue has been fixed, and I trained for very few iterations (400 iterations) with LLaVA-7B checkpoint, and verified the results are reasonable.\r\n\r\nWe have also successfully moved the LLaVA implementation all into a single code base, so that we do not need to upgrade `transformers` package for new features.  Please upgrade to the latest code base and reinstall the `transformers` package, following the instruction below.  Thanks.\r\n\r\n```\r\ngit pull\r\npip uninstall transformers\r\npip install git+https://github.com/huggingface/transformers@cae78c46\r\npip install -e .\r\n```\n</Comment>\n<Comment by Unrealluver at 2023-05-01T14:13:48Z>\n> Hi @Unrealluver, the issue has been fixed, and I trained for very few iterations (400 iterations) with LLaVA-7B checkpoint, and verified the results are reasonable.\r\n> \r\n> We have also successfully moved the LLaVA implementation all into a single code base, so that we do not need to upgrade `transformers` package for new features. Please upgrade to the latest code base and reinstall the `transformers` package, following the instruction below. Thanks.\r\n> \r\n> ```\r\n> git pull\r\n> pip uninstall transformers\r\n> pip install git+https://github.com/huggingface/transformers@cae78c46\r\n> pip install -e .\r\n> ```\r\n\r\nThanks for your help, the latest commit truly solved the problem! But I found that the resume running will still face an error like bellow:\r\n\r\n![image](https://user-images.githubusercontent.com/27504120/235464743-b01fc409-a914-41d3-b6aa-8a26e657dbb0.png)\r\n\r\nHere my checkpoint iter is 2400, when I stop the pertaining and resume to run, the program returns the RuntimeError.\n</Comment>\n<Comment by haotian-liu at 2023-05-02T13:54:44Z>\nHi, thank you for pointing out this issue.. Since the pretraining on 8x A100s currently takes only around 2 hours (7B) and 4 hours (13B), I hope that it can be finished without interrupting.  I will investigate this issue later this week.  Thank you!\n</Comment>\n<Comment by Unrealluver at 2023-05-02T14:21:03Z>\nThanks for your reply.\n</Comment>\n<Comment by Unrealluver at 2023-05-04T14:18:18Z>\nHi, I am here to clarify the reason for the abnormal response. I noticed that in your latest code, we could get the true response, you switch the transformer library version to the version of the public repo and implement the llava model in `.llava/model/llava.py`. But I am still curious about the reason that leads to the abnormal response. Could you please give me more explanation about it? Thanks again!\n</Comment>\n<Comment by tingxueronghua at 2023-07-25T06:47:39Z>\n> \r\n\r\nhello? have you found the reasons? I think the author tried to solve the problem by this commit: https://github.com/haotian-liu/LLaVA/commit/871bbd4a5248510156629e00ee5042b74f74764c\r\n\r\nIn this commit, I found the previous solution to keep the original embeddings parameters is to copy and paste it each time forwarding. And in this commit, he tries to block the gradients of corresponding embeddings by detach. I have no idea why this differs.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 74,
    "state": "closed",
    "created_by": "gordonhu608",
    "created_at": "2023-04-29T23:30:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/74</URL>\n\n<TITLE>We know that 7b is coming soon but is there a estimation of when?</TITLE>\n\n<BODY>As title. Since 7b is more memory friendly for researchers to play and study with.</BODY>\n\n<COMMENTS>\n<Comment by vateye at 2023-04-30T14:21:17Z>\nHi, I find mPLUG-Owl is actually a 7B model which is shown to perform better than LLaVA 13B with more emergent abilities. Maybe you can try that.\n</Comment>\n<Comment by haotian-liu at 2023-04-30T18:19:01Z>\n@gordonhu608 What about now? :)\r\n\r\nPlease checkout the instruction [here](https://github.com/haotian-liu/LLaVA#llava-7b).\n</Comment>\n<Comment by gordonhu608 at 2023-04-30T18:27:46Z>\n> @gordonhu608 What about now? :)\r\n> \r\n> Please checkout the instruction [here](https://github.com/haotian-liu/LLaVA#llava-7b).\r\n\r\nThank you!\n</Comment>\n<Comment by gordonhu608 at 2023-04-30T18:28:38Z>\n> Hi, I find mPLUG-Owl is actually a 7B model which is shown to perform better than LLaVA 13B with more emergent abilities. Maybe you can try that.\r\n\r\nThis is very helpful, thank you, btw we find us both from UCSD\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 73,
    "state": "open",
    "created_by": "deter3",
    "created_at": "2023-04-29T19:41:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/73</URL>\n\n<TITLE>No good at model training , any chance easily get it running on huggingface spces or other service ?</TITLE>\n\n<BODY>I am working on NLP and not really familiar with model training , it just too much information nowadays .  I am wondering whether I can easily get it running on huggingface spaces or other cloud service . or there's any video show me actually doing ? \r\n\r\nI am really sorry if I am saying something stupid , just really want to set it on for university psychology experimenting .\r\n\r\nThanks</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-30T18:21:29Z>\nHi @deter3, thank you for your interest in our work.  Would you clarify if you are looking for instructions for training and evaluation, and which part are you specifically interest in / having issue with?\r\n\r\nIt may be better for me to understand what to provide when I get these details :)\n</Comment>\n<Comment by deter3 at 2023-05-01T17:33:25Z>\nhi , @haotian-liu ,   Thanks for the reply . here are my questions 1. If I want to to set up the model in huggingface or google cloud , shall i follow the instructions of Data Donwnload, Install, LLaVA Weights,Serving,Evaluation, Fine-tuning ? All I need is a model just like your demo .  2. Even I am familiar with python , but the instructions are heavily domain specific and I am kind of get lost what I should do ? Use you data to train the model or i can just install the per-trained model  . Thanks for your time .\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 72,
    "state": "closed",
    "created_by": "xjdeng",
    "created_at": "2023-04-29T18:59:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/72</URL>\n\n<TITLE>Doesn't work with wojtab/llava-13b-v0-4bit-128g</TITLE>\n\n<BODY>I know it's possible to load that model in oobabooga's backend but that one lacks the ability to programatically run inference on a large # of images at once.  I know this is a weird request since I'm considering using LLaVA as a way to auto-caption images for Stable Diffusion training.\r\n\r\nWhen I try to load this model using this repository using the standard code show in the README as referenced below, it complains about the lack of pytorch_model-00001-of-00003.bin. \r\n\r\n```\r\npython -m llava.eval.run_llava \\\r\n    --model-name /path/to/LLaVA-13B-v0 \\\r\n    --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n    --query \"What are the things I should be cautious about when I visit here?\"\r\n```\r\n\r\nHowever, I anticipate that even fixing the code to more intelligently look for the pytorch .bin files won't be enough as this model requires GPTQ support in order to be loaded.\r\n\r\nWhy am I suggesting that we add support for this?  First, it's possible to load in a Free Colab instance without having to spend a lot of credits for an A100 premium GPU.  Second, it'll allow people to programatically feed it hundreds, thousands, or more images and automatically get labels for them whereas if you're using oobabooga's interface, you'll have to manually drag each image in one by one and sit there spending hours doing so.</BODY>\n\n<COMMENTS>\n<Comment by Wojtab at 2023-04-30T01:43:25Z>\nyou can always use the api extension in text-generation-webui\r\n```\r\npython3 server.py --model llava-13b-4bit-128g --wbits 4 --group 128 --chat --extensions llava api\r\n```\r\nand input the images in the prompt, like so(images need to be jpegs):\r\n```Python\r\nimport base64\r\nimport requests\r\n\r\nCONTEXT = \"You are LLaVA, a large language and vision assistant trained by UW Madison WAIV Lab. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language. Follow the instructions carefully and explain your answers in detail.\\n### Human: \\nHi!\\n### Assistant: \\nHi there! How can I help you today?\\n\"\r\n\r\nwith open('extreme_ironing.jpg', 'rb') as f:\r\n    img_str = base64.b64encode(f.read()).decode('utf-8')\r\n    prompt = CONTEXT + f'### Human: \\nWhat is unusual about this image: \\n<img src=\"data:image/jpeg;base64,{img_str}\">\\n### Assistant: \\n'\r\n    print(requests.post('http://127.0.0.1:5000/api/v1/generate', json={'prompt': prompt, 'stopping_strings': ['\\n###']}).json())\r\n```\r\noutput:\r\n```\r\n╰─❯ python3 test.py\r\n{'results': [{'text': \"The unusual aspect of this image is that a man is standing on top of a yellow minivan while doing his laundry. He has set up a makeshift clothes line using the car's rooftop as an outdoor drying area. This scene is uncommon because people typically do their laundry indoors, in a dedicated space like a laundromat or a room in their home, rather than on top of a moving vehicle. Additionally, hanging clothes on the car could be potentially hazardous or illegal in some jurisdictions due to the risk of damaging the vehicle or causing accidents on the road.\\n##\"}]}\r\n```\r\n(do note, stopping_strings are broken till this gets merged: https://github.com/oobabooga/text-generation-webui/pull/1664, and you will need to tweak the parameters, like temperature, see: https://github.com/oobabooga/text-generation-webui/blob/main/api-example.py)\n</Comment>\n<Comment by xjdeng at 2023-04-30T18:47:20Z>\nIs http://127.0.0.1:5000/api/v1/generate accessible from Google colab?\n</Comment>\n<Comment by Wojtab at 2023-04-30T21:51:14Z>\nno idea, but if it isn't there is `--public-api` switch which creates a cloudflare tunnel, so you'll get a public endpoint\n</Comment>\n<Comment by xjdeng at 2023-04-30T22:02:12Z>\nI'll certainly give it a try but I was under the impression that --public-api creates a gradio link which only forwards port 80 from the endpoint to port 7860 on the colab instance where the text-generation-webui is running and forgets about the api running on port 5000.  Maybe there's a way to forward multiple ports but in any case, I need it running on colab because my local machine's 6 GB gpu ain't up to this job.\n</Comment>\n<Comment by Wojtab at 2023-04-30T22:08:26Z>\nthere are 2 different switches, --public for gradio, and --public-api for api\n</Comment>\n<Comment by haotian-liu at 2023-05-25T20:01:18Z>\nClosing the issue due to inactivity, please feel free to re-open if you meet any other issues, thanks.\n</Comment>\n<Comment by xjdeng at 2023-05-25T22:58:41Z>\nOh yeah, it worked, thx for closing.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 71,
    "state": "closed",
    "created_by": "qzhb",
    "created_at": "2023-04-28T10:07:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/71</URL>\n\n<TITLE>questions about getting LLaVA Weights</TITLE>\n\n<BODY>when I perform \"python3 -m llava.model.apply_delta --base /data/zhaobo/codes/mm_reasoning/LLaVA/weights/llama/13B --target /data/zhaobo/codes/mm_reasoning/LLaVA/weights/llava/13B --delta /data/zhaobo/codes/mm_reasoning/LLaVA/weights/llava_delta\"\r\nto get the weights of LLaVA, I got the following issues:\r\n\"OSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory \r\n/data/zhaobo/codes/mm_reasoning/LLaVA/weights/llama/13B.\"\r\nBut there is no problem with the weight download of  LLaMA. Do you know why?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 70,
    "state": "closed",
    "created_by": "mingshanhee",
    "created_at": "2023-04-28T03:49:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/70</URL>\n\n<TITLE>Inconsistent Special Token in Tokenizer</TITLE>\n\n<BODY>Hi, thanks for the great work in releasing the pre-trained model checkpoints. I tried using the provided model checkpoints (i.e. downloading and applying delta weights) and found some peculiar tokenizer settings regarding the special tokens. Here is a screenshot of the findings.\r\n\r\n![image](https://user-images.githubusercontent.com/4586211/235049523-e019e859-3e6f-4d73-8a84-57dc513c035d.png)\r\n\r\nAs you can see from the screenshot, the unk_token in pretrained LLaVA model and finetuned LLaVA (for ScienceQA) model is actually different. From my understanding, the unk_token should be mapped to \"<unk>\", which is correctly reflected in the pretrained LLaVA model. However, the finetuned LLaVA (for ScienceQA) model is actually mapped to \"</s>\".\r\n\r\nI am just wondering whether this inconsistency was intended and/or would it affect the model's performance?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-29T04:56:02Z>\nHi @mingshanhee, thank you for your interest in our work.\r\n\r\nOur SQA checkpoint was developed at an earlier stage, so it has some minor difference with the latest tokenizer configurations (e.g. `unk_token`, and it does not have `<im_start>` and `<im_end>`).\r\n\r\nHowever, these shall not cause issues in evaluating the models, and we'll release the new checkpoints in a later release to fix all these issues.\r\n\r\nPlease let me know if this clarifies the doubts, and if there are other inference-related issues, please also let me know.  Thank you!\n</Comment>\n<Comment by haotian-liu at 2023-07-19T17:47:25Z>\nClosing this as all tokenization related issues are fixed with [v1.0.0 release](https://github.com/haotian-liu/LLaVA/blob/main/docs/Release_Notes.md#7192023).\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 69,
    "state": "closed",
    "created_by": "jihan-yin",
    "created_at": "2023-04-28T03:48:57Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/69</URL>\n\n<TITLE>Huggingface generation config incorrect</TITLE>\n\n<BODY>The generation config file in huggingface is incorrect. This leads to cuda errors when loading the finetuned llama model with huggingface and trying to run batch inference.\r\n\r\nConfig file - https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0/blob/main/generation_config.json\r\n```\r\n\"bos_token_id\": 0,\r\n\"eos_token_id\": 1,\r\n\"pad_token_id\": 0,\r\n```\r\n\r\nBut the pad token is `32003` as defined at https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0/blob/main/added_tokens.json, and the eos and bos tokens are also off by 1.\r\n\r\nHowever, if we look at the loaded model in huggingface, its embedding layer is only size 32003, so the pad token id is too high. The model's embedding layer claims the pad token id to be 32002. Setting it to this allows for correct generation.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-29T04:53:25Z>\nHi @jihan-yin, thank you for your interest in our work and for pointing this out.\r\n\r\nThere are some issues with the tokenizer in LLaVA-v0 as well as Vicuna-v0 and we are working on fixing these.\r\n\r\nWould you mind sharing which script you are facing issues with?  Thanks.\n</Comment>\n<Comment by haotian-liu at 2023-05-25T20:01:01Z>\nClosing the issue due to inactivity, please feel free to re-open if you meet any other issues, thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 68,
    "state": "closed",
    "created_by": "wangjiongw",
    "created_at": "2023-04-28T02:46:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/68</URL>\n\n<TITLE>CUDA Out of Memory during Training on 80GB A100</TITLE>\n\n<BODY>Thanks for your great work.\r\nI am trying to run experiments following your command on a slurm cluster with 80GB A100; But I got CUDA out of memory in backwarding the very first iteration. I wonder if anyone met the same problem? or if I misunderstood the instructions?\r\n\r\nScript I tried to start the experiment on one GPU is as follows\r\n```\r\nsrun -p A100 --gres=gpu:1 -n 1 --ntasks-per-node 1 --kill-on-bad-exit \\\r\n  torchrun --nnodes=1 --nproc_per_node=1 --master_port=25001 \\\r\n  llava/train/train_mem.py \\\r\n    --model_name /path to /LLaVA_13B_v0/ \\\r\n    --data_path ./LLaVA-Instruct-150K/conversation_58k.json \\\r\n    --image_folder /path to /coco/train2017/ \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 5000 \\\r\n    --save_total_limit 3 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --fsdp \"full_shard auto_wrap\" \\\r\n    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb \r\n```\r\nI have set per_device_batch_size to 1, but it still doesn't work. However,  I can run the CLI inference.\r\nError Messages are here.\r\n```\r\n  0%|          | 0/170043 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\r\nTraceback (most recent call last):\r\n  File \"/mnt/petrelfs/unified_benchmark/LLaMA-X/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/mnt/petrelfs/unified_benchmark/LLaMA-X/LLaVA/llava/train/train.py\", line 508, in train\r\n    trainer.train()\r\n  File \"/mnt/cache/anaconda3/envs/llama/lib/python3.9/site-packages/transformers/trainer.py\", line 1644, in train\r\n    return inner_training_loop(\r\n  File \"/mnt/cache/anaconda3/envs/llama/lib/python3.9/site-packages/transformers/trainer.py\", line 1911, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/mnt/cache/anaconda3/envs/llama/lib/python3.9/site-packages/transformers/trainer.py\", line 2675, in training_step\r\n    loss.backward()\r\n  File \"/mnt/cache/anaconda3/envs/llama/lib/python3.9/site-packages/torch/_tensor.py\", line 487, in backward\r\n    torch.autograd.backward(\r\n  File \"/mnt/cache/anaconda3/envs/llama/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 200, in backward\r\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n  File \"/mnt/cache/anaconda3/envs/llama/lib/python3.9/site-packages/torch/autograd/function.py\", line 274, in apply\r\n    return user_fn(self, *args)\r\n  File \"/mnt/cache/anaconda3/envs/llama/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 157, in backward\r\n    torch.autograd.backward(outputs_with_grad, args_with_grad)\r\n  File \"/mnt/cache/anaconda3/envs/llama/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 200, in backward\r\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 136.00 MiB (GPU 0; 79.35 GiB total capacity; 77.09 GiB already allocated; 103.19 MiB free; 77.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid f\r\nragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n```\r\nThanks for help in advance!</BODY>\n\n<COMMENTS>\n<Comment by JulioZhao97 at 2023-04-28T06:21:10Z>\nsame OOM here\n</Comment>\n<Comment by wangjiongw at 2023-04-28T07:10:57Z>\n> same OOM here\r\n\r\nHi @JulioZhao97 , I wonder what do you mean by here, did you succeed in pretraining stage?\n</Comment>\n<Comment by yuezewang at 2023-04-28T07:12:59Z>\nme too, especially when saving ckpt\n</Comment>\n<Comment by JulioZhao97 at 2023-04-28T07:32:35Z>\n> > same OOM here\r\n> \r\n> Hi @JulioZhao97 , I wonder what do you mean by here, did you succeed in pretraining stage?\r\n\r\nNo, I directly run the second stage finetune, but the same OOM in 80GB A100. waiting for response from author.\n</Comment>\n<Comment by haotian-liu at 2023-04-29T04:48:09Z>\nHi, thank you for your interest in our work.\r\n\r\nCurrently it is impossible to train the 13B model on a **single** 80G A100. You may refer to https://github.com/lm-sys/FastChat/issues/367 for a calculation of the size of GPU memory needed.  We require a similar amount of VRAM as Vicuna training.  You'll need multiple GPUs with FSDP to offload model parameters / optimizer to different GPUs.\r\n\r\nWe have recently updated FSDP for pretraining, so that helps with the pretraining a bit.\r\n\r\nWe are also working on parameter efficient tuning methods like LORA.  Stay tuned on this, and **contribution is welcomed**!\r\n\r\nThanks.\n</Comment>\n<Comment by wangjiongw at 2023-04-29T16:39:44Z>\n> Hi, thank you for your interest in our work.\r\n> \r\n> Currently it is impossible to train the 13B model on a **single** 80G A100. You may refer to [lm-sys/FastChat#367](https://github.com/lm-sys/FastChat/issues/367) for a calculation of the size of GPU memory needed. We require a similar amount of VRAM as Vicuna training. You'll need multiple GPUs with FSDP to offload model parameters / optimizer to different GPUs.\r\n> \r\n> We have recently updated FSDP for pretraining, so that helps with the pretraining a bit.\r\n> \r\n> We are also working on parameter efficient tuning methods like LORA. Stay tuned on this, and **contribution is welcomed**!\r\n> \r\n> Thanks.\r\n\r\nGot it! Thanks for your help. I'll try to involve more GPUs for training. To my understanding, the most recent code support FSDP training strategy, right?\n</Comment>\n<Comment by haotian-liu at 2023-05-01T01:52:45Z>\nHi @wangjiongw, the most recent code supports FSDP for both pretraining and finetuning.  There was an issue with the FSDP support with pretraining, that was fixed today.\r\n\r\nWe have also successfully moved the LLaVA implementation all into a single code base, so that we do not need to upgrade `transformers` package for new features.  Please upgrade to the latest code base and reinstall the `transformers` package, following the instruction below.  Thanks.\r\n\r\n```\r\ngit pull\r\npip uninstall transformers\r\npip install git+https://github.com/huggingface/transformers@cae78c46\r\npip install -e .\r\n```\n</Comment>\n<Comment by wangjiongw at 2023-05-01T07:42:53Z>\n> Hi @wangjiongw, the most recent code supports FSDP for both pretraining and finetuning. There was an issue with the FSDP support with pretraining, that was fixed today.\r\n> \r\n> We have also successfully moved the LLaVA implementation all into a single code base, so that we do not need to upgrade `transformers` package for new features. Please upgrade to the latest code base and reinstall the `transformers` package, following the instruction below. Thanks.\r\n> \r\n> ```\r\n> git pull\r\n> pip uninstall transformers\r\n> pip install git+https://github.com/huggingface/transformers@cae78c46\r\n> pip install -e .\r\n> ```\r\n\r\nGot it! Thanks for your help and excellent work, I will try following the instructions. FSDP in your earlier reply has really helped a lot. Maybe I will close this issue and great appreciation again~\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 67,
    "state": "closed",
    "created_by": "JulioZhao97",
    "created_at": "2023-04-27T08:36:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/67</URL>\n\n<TITLE>evaluation on SQA very slow</TITLE>\n\n<BODY>when I follow instructions run eval on SQA, the inference is very slow, however I find that web broswer demo is very fast.</BODY>\n\n<COMMENTS>\n<Comment by JulioZhao97 at 2023-04-27T08:37:13Z>\n![image](https://user-images.githubusercontent.com/40555727/234807055-aaae71fe-18b1-4c1f-a707-c27908d4eaf5.png)\r\nI eval on 8 gpus and it cost nearly one hour to inference on ~5K\n</Comment>\n<Comment by mary-0830 at 2023-05-26T08:48:59Z>\nHow can I use multiple cards for reasoning? @JulioZhao97\n</Comment>\n<Comment by haotian-liu at 2023-05-29T05:09:15Z>\nHi @mary-0830 \r\nYou can use our scripts for write yours for multiple GPU (batch inference): https://github.com/haotian-liu/LLaVA#evaluation-1\n</Comment>\n<Comment by mary-0830 at 2023-05-29T05:24:27Z>\n> Hi @mary-0830 You can use our scripts for write yours for multiple GPU (batch inference): https://github.com/haotian-liu/LLaVA#evaluation-1\r\n\r\nths!!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 66,
    "state": "closed",
    "created_by": "jun297",
    "created_at": "2023-04-27T06:59:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/66</URL>\n\n<TITLE>a plan for using vicuna delta v1.1</TITLE>\n\n<BODY>Is there a plan for using vicuna delta v1.1?\r\nOr is it possible to directly train with v1.1 on my own?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-29T04:41:10Z>\nHi @jun297, thank you for your interest in our work.\r\n\r\nWe are working on the updated model weights based on Vicuna delta v1.1, and it is planned for releasing next week.  However, since there are changes in tokenizer and special tokens between vicuna-v0 and v1.1, so the current code base does not natively support changing the backbone and train (similarly, FastChat-v0 is not compatible with v1.1 as well).\r\n\r\nWe will update the code base once we finish the verification, thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 65,
    "state": "closed",
    "created_by": "Unrealluver",
    "created_at": "2023-04-27T06:44:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/65</URL>\n\n<TITLE>Problem of adding tokens to tokenizer</TITLE>\n\n<BODY>Greetings! Thanks for your update on FSDP support! When I ran the code, I noticed that the add token operation seems always failed. For example, when the interpreter executes the command:\r\n```\r\ntokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\r\n```\r\n\r\nIt only returns 0. Just the same as the following command:\r\n```\r\nnum_new_tokens = tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\r\n```\r\n, the `num_new_tokens` is 0 too. So the `vision_config.im_start_token` and `vision_config.im_end_token` are all 0.\r\n\r\n```\r\nvision_config.im_start_token, vision_config.im_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN])\r\n```\r\n\r\nAt last, it will lead to an error like this:\r\n```\r\ns]cur_input_ids index  tensor(305, device='cuda:0')\r\nvision_tower.config.im_end_token:  0\r\nTraceback (most recent call last):\r\n  File \"/home/zhulianghui/ProjectC_ChatGPT/llava/reference/test/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/home/zhulianghui/ProjectC_ChatGPT/llava/reference/test/LLaVA/llava/train/train.py\", line 499, in train\r\n    trainer.train()\r\n  File \"/home/zhulianghui/.conda/envs/llava_beta/lib/python3.10/site-packages/transformers/trainer.py\", line 1645, in train\r\n    return inner_training_loop(\r\n  File \"/home/zhulianghui/.conda/envs/llava_beta/lib/python3.10/site-packages/transformers/trainer.py\", line 1912, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/home/zhulianghui/.conda/envs/llava_beta/lib/python3.10/site-packages/transformers/trainer.py\", line 2658, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/home/zhulianghui/.conda/envs/llava_beta/lib/python3.10/site-packages/transformers/trainer.py\", line 2690, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/home/zhulianghui/.conda/envs/llava_beta/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/zhulianghui/.conda/envs/llava_beta/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 789, in forward\r\n    output = self._fsdp_wrapped_module(*args, **kwargs)\r\n  File \"/home/zhulianghui/.conda/envs/llava_beta/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/zhulianghui/.conda/envs/llava_beta/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 846, in forward\r\n    outputs = self.model(\r\n  File \"/home/zhulianghui/.conda/envs/llava_beta/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/zhulianghui/.conda/envs/llava_beta/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 632, in forward\r\n    raise ValueError(\"Seems that the image is cut.\")\r\nValueError: Seems that the image is cut.\r\n\r\n```\r\n\r\n\r\nCould you please share some suggestions for this situation?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 64,
    "state": "closed",
    "created_by": "JulioZhao97",
    "created_at": "2023-04-27T04:17:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/64</URL>\n\n<TITLE>what is the torch version and cuda version?</TITLE>\n\n<BODY>what torch version and cuda version?</BODY>\n\n<COMMENTS>\n<Comment by ZhengMengbin at 2023-04-27T07:53:38Z>\n> what torch version and cuda version?\r\n\r\nI use CUDA117 and torch==1.13.1 or torch==2.1.0.dev20230424 is all ok.\n</Comment>\n<Comment by JulioZhao97 at 2023-04-27T08:34:03Z>\n> > what torch version and cuda version?\r\n> \r\n> I use CUDA117 and torch==1.13.1 or torch==2.1.0.dev20230424 is all ok.\r\n\r\nThanks for reply. I use torch-1.12.0 and cuda=11.6 also ok.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 63,
    "state": "closed",
    "created_by": "hengyuan-hu",
    "created_at": "2023-04-27T01:12:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/63</URL>\n\n<TITLE>Should I use the projector weights when evaluating pretrained model?</TITLE>\n\n<BODY>Thanks for this great repo.\r\n\r\nI was able to load the model and run eval following this command\r\n```\r\npython -m llava.eval.run_llava \\\r\n    --model-name llava_models/13b \\\r\n    --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n    --query \"What are the things I should be cautious about when I visit here?\"\r\n```\r\n\r\nHowever, I notice that the weights for the projector layers in this model is different from the projector weights (as well as embedding weights) downloaded here `https://github.com/haotian-liu/LLaVA#llava-pretrained-projector-weights`. Are the separate projection weights needed for this model? Why are they different from the weights currently in the model?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-29T04:38:45Z>\nHi, thank you for your interest in our work.\r\n\r\nYou do not need to load the pretrained MLP layer for running our chat model.  The current released checkpoint uses full-model finetuning (except for CLIP visual encoder), thus the projection layer is also updated -- this accounts for the weight difference.\r\n\r\nWe are also trying with ablation on freezing the linear projection layer or not, and will update the comparison and analysis in our paper. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 62,
    "state": "closed",
    "created_by": "ZhengMengbin",
    "created_at": "2023-04-26T03:01:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/62</URL>\n\n<TITLE>Pretrain and Finetune log</TITLE>\n\n<BODY>Can you provide the training loss log？ And how to evaluate pretrain stage's performance ?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-29T04:34:34Z>\nHi @ZhengMengbin \r\nThank you for your interest in our work.\r\n\r\nAttached are the screenshots of our training loss curves for both pretraining and finetuning stages.  You may compare the loss curves for roughly verifying if the model training is healthy.\r\n\r\nPretraining:\r\n![pretraining-loss](https://user-images.githubusercontent.com/6631389/235282170-60ef5b8d-481f-4a81-9088-c38f48cf2236.png)\r\n\r\nVisual Instruction Tuning:\r\n![finetuning-loss](https://user-images.githubusercontent.com/6631389/235282173-711e9c9b-ad7c-41c3-bab6-3244931c055d.png)\r\n\r\nFor evaluation for the pretraining stage, we may try evaluating on COCO caption.  Note that if we use the raw CC captions, we may not compare directly with existing approaches, as the original CC captions are noisy and not in COCO style.\n</Comment>\n<Comment by yiranyyu at 2023-05-11T01:10:07Z>\n> Hi @ZhengMengbin Thank you for your interest in our work.\r\n> \r\n> Attached are the screenshots of our training loss curves for both pretraining and finetuning stages. You may compare the loss curves for roughly verifying if the model training is healthy.\r\n> \r\n> Pretraining: ![pretraining-loss](https://user-images.githubusercontent.com/6631389/235282170-60ef5b8d-481f-4a81-9088-c38f48cf2236.png)\r\n> \r\n> Visual Instruction Tuning: ![finetuning-loss](https://user-images.githubusercontent.com/6631389/235282173-711e9c9b-ad7c-41c3-bab6-3244931c055d.png)\r\n> \r\n> For evaluation for the pretraining stage, we may try evaluating on COCO caption. Note that if we use the raw CC captions, we may not compare directly with existing approaches, as the original CC captions are noisy and not in COCO style.\r\n\r\nHi, I am trying to reproduce your excellent results with vicuna 1.1 (simply add the `--version v1` config to the training command and use the vicuna 1.1 pre-trained weight instead). However, I find the loss during our pre-training process is much higher than yours. Our pre-train loss is still 2.1 after 4.6k training steps. \r\n\r\nCan you share your training log and evaluation results based on vicuna 1.1? Thank you.\n</Comment>\n<Comment by haotian-liu at 2023-05-13T18:19:36Z>\nHi @yiranyyu, the loss pattern is similar on Vicuna v1.1 and v0.  But we observe a higher loss in pretraining stage than v0 as well.  Our loss values are similar.  This may be due to the differences in the pretrained prompt template.\r\n\r\nAfter the finetuning, the model's loss are similar and can produce satisfactory results.\n</Comment>\n<Comment by haotian-liu at 2023-05-25T20:00:46Z>\nClosing the issue due to inactivity, please feel free to re-open if you meet any other issues, thanks.\n</Comment>\n<Comment by guozhiyao at 2023-06-28T06:22:01Z>\n@haotian-liu Excuse me, I replaced an llm model and modified the code for training. The initial loss during pretrain is about 5.5, and it is still about 3 after training for one epoch. What might be the cause?\n</Comment>\n<Comment by guozhiyao at 2023-06-28T08:53:59Z>\n> @haotian-liu Excuse me, I replaced an llm model and modified the code for training. The initial loss during pretrain is about 5.5, and it is still about 3 after training for one epoch. What might be the cause?\r\n\r\n@haotian-liu \r\n\r\nI have tried the following:\r\n1. Only input plain text sft data, the loaded loss is normal\r\n2. Letting the visual module to train with linear module, but it does not alleviate the problem\r\n3. Pretrain multiple epochs, the loss is still around 3, and it has not gone down\r\n4. I tried two frameworks, llava and minigpt4, and the loss was both stuck at 3\r\n\r\nHave you ever encountered this problem?\n</Comment>\n<Comment by yzf-code at 2023-09-05T11:46:49Z>\n> 3\\. has\r\n\r\n\r\n\r\n> @haotian-liu Excuse me, I replaced an llm model and modified the code for training. The initial loss during pretrain is about 5.5, and it is still about 3 after training for one epoch. What might be the cause?\r\n\r\nExcuse me, we also encountered the same problem, have you solved it? Thanks\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 60,
    "state": "closed",
    "created_by": "lonestriker",
    "created_at": "2023-04-25T17:30:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/60</URL>\n\n<TITLE>Unable to run on 1x 4090 or 2x 4090s</TITLE>\n\n<BODY>I'm getting different errors when trying to run on either a single or double 4090s.\r\n\r\nWith 1x 4090  (--num-gpus=1):\r\n```\r\n2023-04-25 21:25:18 | ERROR | stderr | torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 136.00 MiB (GPU 0; 23.65 GiB total capacity; 23.11 GiB already allocated; 76.19 MiB free; 23.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n```\r\n\r\nWith 2x 4090s (--num-gpus=2)\r\n```\r\n2023-04-25 21:27:37 | ERROR | stderr | RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument tensors in method wrapper_CUDA_cat)\r\n```\r\n\r\nI'm not running any other applications at the moment and nvidia-smi shows the following:\r\n```\r\nTue Apr 25 21:28:27 2023\r\n+---------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\r\n|-----------------------------------------+----------------------+----------------------+\r\n| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                      |               MIG M. |\r\n|=========================================+======================+======================|\r\n|   0  NVIDIA GeForce RTX 4090         On | 00000000:01:00.0 Off |                  Off |\r\n|  0%   47C    P5               39W / 450W|     22MiB / 24564MiB |      0%      Default |\r\n|                                         |                      |                  N/A |\r\n+-----------------------------------------+----------------------+----------------------+\r\n|   1  NVIDIA GeForce RTX 4090         On | 00000000:08:00.0 Off |                  Off |\r\n|  0%   37C    P5               47W / 480W|      6MiB / 24564MiB |      0%      Default |\r\n|                                         |                      |                  N/A |\r\n+-----------------------------------------+----------------------+----------------------+\r\n\r\n+---------------------------------------------------------------------------------------+\r\n| Processes:                                                                            |\r\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n|        ID   ID                                                             Usage      |\r\n|=======================================================================================|\r\n|    0   N/A  N/A      1236      G   /usr/lib/xorg/Xorg                            9MiB |\r\n|    0   N/A  N/A      1486      G   /usr/bin/gnome-shell                         10MiB |\r\n|    1   N/A  N/A      1236      G   /usr/lib/xorg/Xorg                            4MiB |\r\n+---------------------------------------------------------------------------------------+\r\n```\r\n\r\nAny advice?\r\n\r\nThanks!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-25T17:34:58Z>\nHi thank you for the interest in our work!  The currect FP16 version is expected to OOM on a single RTX 4090.  The 8-bit quantization in WIP, and will release this week.  Regarding running on two-gpus.\r\n\r\nWould you mind: \r\n\r\n(1) sharing the error message before this one, so that I can see which line of code is causing the issue;\r\n(2) sharing the command on running with multiple gpus;\r\n(2) meanwhile, trying to see if the `transformers` version is correct as there is a fix 5 days ago on multiple gpu: `pip install git+https://github.com/haotian-liu/transformers_llava.git@988b6abb3b7da9a5cbb5051e994706f7f88c2565`\r\n\r\nThanks.\n</Comment>\n<Comment by lonestriker at 2023-04-25T17:41:24Z>\nThanks for the quick reply!  \r\n\r\nI updated transformers before running the tests.  I just reran the installation again and tested again, same results.\r\n```\r\n$ pip install git+https://github.com/haotian-liu/transformers_llava.git@988b6abb3b7da9a5cbb5051e994706f7f88c2565\r\nCollecting git+https://github.com/haotian-liu/transformers_llava.git@988b6abb3b7da9a5cbb5051e994706f7f88c2565\r\n  Cloning https://github.com/haotian-liu/transformers_llava.git (to revision 988b6abb3b7da9a5cbb5051e994706f7f88c2565) to /tmp/pip-req-build-xv11al22\r\n  Running command git clone --filter=blob:none --quiet https://github.com/haotian-liu/transformers_llava.git /tmp/pip-req-build-xv11al22\r\n  Running command git rev-parse -q --verify 'sha^988b6abb3b7da9a5cbb5051e994706f7f88c2565'\r\n  Running command git fetch -q https://github.com/haotian-liu/transformers_llava.git 988b6abb3b7da9a5cbb5051e994706f7f88c2565\r\n  Resolved https://github.com/haotian-liu/transformers_llava.git to commit 988b6abb3b7da9a5cbb5051e994706f7f88c2565\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n  Preparing metadata (pyproject.toml) ... done\r\nRequirement already satisfied: filelock in /home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages (from transformers==4.28.0.dev0) (3.12.0)\r\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages (from transformers==4.28.0.dev0) (0.13.4)\r\nRequirement already satisfied: numpy>=1.17 in /home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages (from transformers==4.28.0.dev0) (1.24.2)\r\nRequirement already satisfied: packaging>=20.0 in /home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages (from transformers==4.28.0.dev0) (23.1)\r\nRequirement already satisfied: pyyaml>=5.1 in /home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages (from transformers==4.28.0.dev0) (6.0)\r\nRequirement already satisfied: regex!=2019.12.17 in /home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages (from transformers==4.28.0.dev0) (2023.3.23)\r\nRequirement already satisfied: requests in /home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages (from transformers==4.28.0.dev0) (2.28.2)\r\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages (from transformers==4.28.0.dev0) (0.12.1)\r\nRequirement already satisfied: tqdm>=4.27 in /home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages (from transformers==4.28.0.dev0) (4.65.0)\r\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0.dev0) (4.5.0)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages (from requests->transformers==4.28.0.dev0) (3.1.0)\r\nRequirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages (from requests->transformers==4.28.0.dev0) (3.4)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages (from requests->transformers==4.28.0.dev0) (1.26.15)\r\nRequirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages (from requests->transformers==4.28.0.dev0) (2022.12.7)\r\n```\r\n\r\nHere's my command:\r\n```\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path /home/ubuntu/models/LLaVA-13B-v0 --multi-modal --num-gpus 2\r\n```\r\n\r\nHere's the full stack:\r\n\r\n```\r\n2023-04-25 21:27:36 | INFO | stdout | INFO:     127.0.0.1:39364 - \"POST /receive_heart_beat HTTP/1.1\" 200 OK\r\n2023-04-25 21:27:37 | ERROR | stderr | ERROR:    Exception in ASGI application\r\n2023-04-25 21:27:37 | ERROR | stderr | Traceback (most recent call last):\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/uvicorn/protocols/http/\r\nh11_impl.py\", line 429, in run_asgi\r\n2023-04-25 21:27:37 | ERROR | stderr |     result = await app(  # type: ignore[func-returns-value]\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/uvicorn/middleware/prox\r\ny_headers.py\", line 78, in __call__\r\n2023-04-25 21:27:37 | ERROR | stderr |     return await self.app(scope, receive, send)\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/fastapi/applications.py\r\n\", line 276, in __call__\r\n2023-04-25 21:27:37 | ERROR | stderr |     await super().__call__(scope, receive, send)\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/starlette/applications.\r\npy\", line 122, in __call__\r\n2023-04-25 21:27:37 | ERROR | stderr |     await self.middleware_stack(scope, receive, send)\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/er\r\nrors.py\", line 184, in __call__\r\n2023-04-25 21:27:37 | ERROR | stderr |     raise exc\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/er\r\nrors.py\", line 162, in __call__\r\n2023-04-25 21:27:37 | ERROR | stderr |     await self.app(scope, receive, _send)\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/ex\r\nceptions.py\", line 79, in __call__\r\n2023-04-25 21:27:37 | ERROR | stderr |     raise exc\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/ex\r\nceptions.py\", line 68, in __call__\r\n2023-04-25 21:27:37 | ERROR | stderr |     await self.app(scope, receive, sender)\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/fastapi/middleware/asyn\r\ncexitstack.py\", line 21, in __call__\r\n2023-04-25 21:27:37 | ERROR | stderr |     raise e\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/fastapi/middleware/asyn\r\ncexitstack.py\", line 18, in __call__\r\n2023-04-25 21:27:37 | ERROR | stderr |     await self.app(scope, receive, send)\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/starlette/routing.py\",\r\nline 718, in __call__\r\n2023-04-25 21:27:37 | ERROR | stderr |     await route.handle(scope, receive, send)\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/starlette/routing.py\",\r\nline 276, in handle\r\n2023-04-25 21:27:37 | ERROR | stderr |     await self.app(scope, receive, send)\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/starlette/routing.py\",\r\nline 69, in app\r\n2023-04-25 21:27:37 | ERROR | stderr |     await response(scope, receive, send)\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/starlette/responses.py\"\r\n, line 270, in __call__\r\n2023-04-25 21:27:37 | ERROR | stderr |     async with anyio.create_task_group() as task_group:\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/anyio/_backends/_asynci\r\no.py\", line 662, in __aexit__\r\n2023-04-25 21:27:37 | ERROR | stderr |     raise exceptions[0]\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/starlette/responses.py\"\r\n, line 273, in wrap\r\n2023-04-25 21:27:37 | ERROR | stderr |     await func()\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/starlette/responses.py\"\r\n, line 262, in stream_response\r\n2023-04-25 21:27:37 | ERROR | stderr |     async for chunk in self.body_iterator:\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/starlette/concurrency.p\r\ny\", line 63, in iterate_in_threadpool\r\n2023-04-25 21:27:37 | ERROR | stderr |     yield await anyio.to_thread.run_sync(_next, iterator)\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/anyio/to_thread.py\", li\r\nne 31, in run_sync\r\n2023-04-25 21:27:37 | ERROR | stderr |     return await get_asynclib().run_sync_in_worker_thread(\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/anyio/_backends/_asynci\r\no.py\", line 937, in run_sync_in_worker_thread\r\n2023-04-25 21:27:37 | ERROR | stderr |     return await future\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/anyio/_backends/_asynci\r\no.py\", line 867, in run\r\n2023-04-25 21:27:37 | ERROR | stderr |     result = context.run(func, *args)\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/starlette/concurrency.p\r\ny\", line 53, in _next\r\n2023-04-25 21:27:37 | ERROR | stderr |     return next(iterator)\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/git/LLaVA/llava/serve/model_worker.py\", line 294, in generate_stream_gate\r\n2023-04-25 21:27:37 | ERROR | stderr |     for x in self.generate_stream(params):\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib\r\n.py\", line 35, in generator_context\r\n2023-04-25 21:27:37 | ERROR | stderr |     response = gen.send(None)\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/git/LLaVA/llava/serve/model_worker.py\", line 232, in generate_stream\r\n2023-04-25 21:27:37 | ERROR | stderr |     out = model(\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-04-25 21:27:37 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2023-04-25 21:27:37 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 844, in forward\r\n2023-04-25 21:27:37 | ERROR | stderr |     outputs = self.model(\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-04-25 21:27:37 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2023-04-25 21:27:37 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 631, in forward\r\n2023-04-25 21:27:37 | ERROR | stderr |     cur_new_input_embeds = torch.cat((cur_input_embeds[:image_start_token_pos+1], cur_image_features, cur_input_embeds[image_start_token_pos + num_patches + 1:]), dim=0)\r\n2023-04-25 21:27:37 | ERROR | stderr | RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument tensors in method wrapper_CUDA_cat)\r\n```\n</Comment>\n<Comment by haotian-liu at 2023-04-25T17:47:51Z>\nHmm this is strange.  Your command and installation both look good to me.\r\n\r\nWould you mind go to L627-631 in `/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py` to see if you see `cur_image_features = ...to(cur_input_embeds.device)`?\r\n\r\nThe reference code is [here](https://github.com/haotian-liu/transformers_llava/blob/988b6abb3b7da9a5cbb5051e994706f7f88c2565/src/transformers/models/llama/modeling_llama.py#L627-L631).\r\n\r\nIf this is the case, then I may need to look into huggingface doc on what is happening with GPU slicing (and unfortunately I am not reproducing this on my local server)\n</Comment>\n<Comment by lonestriker at 2023-04-25T17:57:01Z>\nThanks for the pointer, fixed now!\r\n\r\nNotes:\r\n\r\nMine did not match your version:\r\n\r\n```\r\n                        cur_image_features = image_features[cur_image_idx]\r\n                        num_patches = cur_image_features.shape[0]\r\n                        if cur_input_ids[image_start_token_pos + num_patches + 1] != vision_tower.config.im_end_token:\r\n                            raise ValueError(\"Seems that the image is cut.\")\r\n                        cur_new_input_embeds = torch.cat((cur_input_embeds[:image_start_token_pos+1], cur_image_features, cur_input_embeds[image_start_token_pos + num_patches + 1:]), dim=0)\r\n```\r\n\r\nI had to manually uninstall transformers and re-install your version again and now it matches and works!\r\n\r\nSo, moral of the story for me is to make sure to `pip uninstall transformers` and install again.\r\n\r\nThanks again.\n</Comment>\n<Comment by haotian-liu at 2023-04-25T18:02:54Z>\nOh cool!  I was always thinking `pip install` with a package associated with commit hash would ensure a clean install.  I will update the instructions in README as well.\r\n\r\nIf your problem has been solved, would you mind closing this issue?  Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 59,
    "state": "closed",
    "created_by": "bigfanforChatGPT",
    "created_at": "2023-04-25T08:12:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/59</URL>\n\n<TITLE>Inability to Reproduce Effective Results with ScienceQA</TITLE>\n\n<BODY>Hi there, \r\nI followed the instructions in the README to evaluate LLava-13B-ScienceQA's performance on the ScienceQA dataset. However, I noticed that the answers predicted by LLava seem to be significantly worse than the reported results in the paper. I'm wondering where the issue might be coming from. I'm confident that I've completed all the mentioned steps such as parameter conversion and dataset processing, etc. \r\nThis is a great piece of work, and if this issue can be resolved, it would make a huge contribution to the research community.\r\n\r\nHere are some examples:\r\n\"prompt\":  ”What is the capital of Massachusetts?\\nContext: N/A\\nOptions: (A) Cambridge (B) Plymouth (C) Boston (D) Dover\\n<image>\", \"text\": \"HALSOLUTERs\"\r\n\r\n\"prompt\":  “Look at the models of molecules below. Select the elementary substance.\\nContext: N/A\\nOptions: (A) carbon tetrachloride (B) acetaldehyde (C) bromine\\n<image>\", \"text\": \"\"\r\n\r\n\"prompt\":  \"What can Greta and Allie trade to each get what they want?\\nContext: Trade happens when people agree to exchange goods and services. People give up something to get something else. Sometimes people barter, or directly exchange one good or service for another.\\nGreta and Allie open their lunch boxes in the school cafeteria. Both of them could be happier with their lunches. Greta wanted broccoli in her lunch and Allie was hoping for tomatoes. Look at the images of their lunches. Then answer the question below.\\nOptions: (A) Greta can trade her tomatoes for Allie's sandwich. (B) Allie can trade her broccoli for Greta's oranges. (C) Allie can trade her almonds for Greta's tomatoes. (D) Greta can trade her tomatoes for Allie's broccoli.\\n<image>\", \"text\": \"H1.\\nSOLUTION: Greta and Allie can trade tomatoes for broccoli to get what they want.“\r\n\r\n\"prompt\":  \"Is the following trait inherited or acquired?\\nSasha is good at knitting hats.\\nContext: Hint: People who can knit had to learn how to do it.\\nOptions: (A) acquired (B) inherited\", \"text\": \"Assistant: LECTURE: Organisms, including people, have both inherited and acquired traits. Inherited and acquired traits are gained in different ways.\\nInherited traits are passed down through families. Children gain these traits from their parents. Inherited traits do not need to be learned.\\nAcquired traits are gained during a person's life. Some acquired traits, such as riding a bicycle, are gained by learning. Other acquired traits, such as scars, are caused by the environment. Children do not inherit their parents' acquired traits.\\nSOLUTION: People are not born knowing how to knit. Instead, many people learn how to knit. Knitting well takes practice. So, knitting well is an acquired trait.\"\r\n\r\n\"prompt\":  \"What is the capital of Delaware?\\nContext: N/A\\nOptions: (A) Montpelier (B) Trenton (C) Dover (D) Georgetown\\n<image>\", \"text\": \"SOLUTSOLUTSOLUTRs;'sOLs;'sOLs;'OLs;'OLs;OLs;'OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs ;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;OLs;\"</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-25T16:56:37Z>\nHi @bigfanforChatGPT, thank you for your interest in our work.  We have tried to perform the inference on ScienceQA from scratch, meaning that we perform actions that a new user would need to do, including, weight conversion, and dataset download/conversion.\r\n\r\nDuring this process, we did notice that an option was missing in the original instruction, which however, would not cause the garbled outputs above.  By adding that option back, we were able to reproduce the results.\r\n\r\nThe complete instruction is [here](https://github.com/haotian-liu/LLaVA#evaluation-1), including an updated script for conveniently running batch evaluation on multiple GPUs.  We have also attached our [inference outputs](https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/table/results/test_sqa_llava_13b_v0.json) for users to compare and also for a deeper analysis of the model behavior.  We hope this can further facilitate the research in this area.\r\n\r\nRegarding the specific issue in the garbled outputs, it would be great if you could double check the [weight conversion process](https://github.com/haotian-liu/LLaVA#evaluation-1): (1) you would need to perform weight conversion for ScienceQA delta; (2) the base checkpoint should be llama instead of vicuna.\r\n\r\nPlease let me know if there are further issues.  Thank you!\n</Comment>\n<Comment by bigfanforChatGPT at 2023-04-28T07:03:31Z>\nHello @haotian-liu ,\r\n\r\nThank you very much for your prompt response. The new script and code are able to achieve the same performance as reported in the paper. Great job! However, I noticed that the difference between the new code and the previous one is that it runs the model twice to generate the answer. Is this similar to a two-stage method, where the first stage generates additional context and image captions, and the second stage predicts the answer? Are there any related ablation experiments using only a one-stage method? \r\n\r\nGood Job👍\n</Comment>\n<Comment by haotian-liu at 2023-04-29T03:47:06Z>\nYes, the current version is using CoT-like prompting, however, as mentioned in our paper, we do not notice a big difference in using CoT or not.  And you may notice that there is also a slight difference between ScienceQA model with our Chat model (SQA model is not using `<im_begin>` and `<im_end>` tokens).  These are due to the experiments are conducted in our early research stage.  We are working on a more comprehensive analysis of our models and more ablations.  Please stay tuned for our revised paper, and thank you again for your interest in our work.\n</Comment>\n<Comment by bigfanforChatGPT at 2023-05-01T04:59:54Z>\nSorry to bother you, but I'm still facing some difficulties when fine-tuning ScienceQA. When I followed your code for fine-tuning ScienceQA, the configs for the tokenizer and the model were not the same as the ones released.  Also, I'm not sure if the base model vicuna-13b was applied delta according to the code in the fastchat-v0. I'm currently using the model fastchat-v0 released and the code provided by you, but I'm encountering an error during fine-tuning:\r\n'Using pad_token, but it is not set yet.'\n</Comment>\n<Comment by haotian-liu at 2023-05-01T05:50:29Z>\nHi @bigfanforChatGPT \r\n\r\nDo you see `error` or just `warning`.  This warning about `pad_token` is expected, as it is set after the tokenizer is loaded.  If it is an error, can you please provide the full log?  Thanks.\n</Comment>\n<Comment by haotian-liu at 2023-05-25T20:00:32Z>\nClosing the issue due to inactivity, please feel free to re-open if you meet any other issues, thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 58,
    "state": "closed",
    "created_by": "NormXU",
    "created_at": "2023-04-25T07:04:57Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/58</URL>\n\n<TITLE>The template used to generate input sequence in the codes is different from the one in the paper</TITLE>\n\n<BODY>Thank you very much for your awesome work.\r\n\r\nI try to run the code and noticed the template used to generate conversation between humans and gpt is different from the one mentioned in the paper.\r\n\r\nAccording to the paper:\r\n![template](https://user-images.githubusercontent.com/33339685/234197326-db4e04ec-eb59-4cb2-96d0-2ee52261eabb.jpeg)\r\nwhere `<STOP>` in Vicuna v1.0 is `###`\r\n\r\nHowever, according to the code,\r\n```python\r\ndef _add_speaker_and_signal(header, source, get_conversation=True):\r\n    \"\"\"Add speaker and start/end signal on each round.\"\"\"\r\n    BEGIN_SIGNAL = \"### \"\r\n    END_SIGNAL = \"\\n\"\r\n    conversation = header\r\n    for sentence in source:\r\n        from_str = sentence[\"from\"]\r\n        if from_str.lower() == \"human\":\r\n            from_str = conversation_lib.default_conversation.roles[0]\r\n        elif from_str.lower() == \"gpt\":\r\n            from_str = conversation_lib.default_conversation.roles[1]\r\n        else:\r\n            from_str = 'unknown'\r\n        sentence[\"value\"] = (BEGIN_SIGNAL + from_str + \": \" +\r\n                             sentence[\"value\"] + END_SIGNAL)\r\n        if get_conversation:\r\n            conversation += sentence[\"value\"]\r\n    conversation += BEGIN_SIGNAL\r\n    return conversation\r\n```\r\nIf we follow this function, we end up with an input sequence as\r\n```\r\nsystem-message\\n\\n### Human: .... \\n ### Assistant: .... \\n\r\n```\r\n\r\nand according to \r\n```python\r\ndef _mask_targets(target, tokenized_lens, speakers):\r\n    # cur_idx = 0\r\n    cur_idx = tokenized_lens[0]\r\n    tokenized_lens = tokenized_lens[1:]\r\n    target[:cur_idx] = IGNORE_INDEX\r\n    for tokenized_len, speaker in zip(tokenized_lens, speakers):\r\n        if speaker == \"human\":\r\n            target[cur_idx + 2:cur_idx + tokenized_len] = IGNORE_INDEX\r\n        cur_idx += tokenized_len\r\n```\r\nonly human message and system message is masked, which means after tokenizing the conversation, we get\r\n```\r\nsystem-message \\n\\n ### Human: … \\n ### Assistant: … \\n\r\n```\r\nand only tokens in 【】 are masked\r\n```\r\n【system-message \\n】\\n ### 【Human: … 】\\n ### Assistant: … \\n\r\n```\r\n\r\nIs this a bug or just a trick where the implemented template works better than the one mentioned in the paper?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-25T17:56:45Z>\nHi @NormXU, thank you for your interest in our work!\r\n\r\nDo you mean that `Assistant` and `STOP` token is not masked?\r\n\r\nThe key difference between our masking strategy and Vicuna's may be that I am not masking the `STOP` token (which is `###` for vicuna-v0).  This was due to that I cloned Vicuna's library on their release date, and pulled again some time later.  At that time, there was strange masking behavior I noticed.  Besides, when I was developing in the early stage, I noticed that not masking `STOP` token seems to allow model know how to stop better, so I made that change.\r\n\r\nAlso there is slight inconsistency in the writing on the relative position between `\\n` and `STOP`, and I will fix this disparity in the future revision.\n</Comment>\n<Comment by NormXU at 2023-04-25T18:39:16Z>\n@haotian-liu Thank you for your quick reply.\r\n\r\nExactly. According to the paper, the `Assistant:` should be masked as well.\r\n\r\nFrom my understanding of the paper, we should process our conversation like this (Let's first ignore the \\n and only use ### as our STOP token)\r\n```\r\nsystem-message### Human: .... ### Assistant: .... ###\r\n```\r\nand masked sequence should be\r\n\r\n```\r\n【system-message】###【 Human: .... 】### 【Assistant:】 .... ###\r\n```\r\ntokens in 【】 are masked.\r\n\r\nCurrent codes add `\\n\\n` after the system prompt as shown in `preprocess`; `BEGIN_SIGNAL = \"### \"` and `END_SIGNAL = \"\\n\" `for human/gpt message\r\n\r\n```python\r\ndef preprocess(\r\n    sources: Sequence[str],\r\n    tokenizer: transformers.PreTrainedTokenizer,\r\n) -> Dict:\r\n    \"\"\"\r\n    Given a list of sources, each is a conversation list. This transform:\r\n    1. Add signal '### ' at the beginning each sentence, with end signal '\\n';\r\n    2. Concatenate conversations together;\r\n    3. Tokenize the concatenated conversation;\r\n    4. Make a deepcopy as the target. Mask human words with IGNORE_INDEX.\r\n    \"\"\"\r\n    # add end signal and concatenate together\r\n    conversations = []\r\n    for source in sources:\r\n        header = f\"{conversation_lib.default_conversation.system}\\n\\n\"\r\n        conversation = _add_speaker_and_signal(header, source)\r\n        conversations.append(conversation)\r\n    # tokenize conversations\r\n    conversations_tokenized = _tokenize_fn(conversations, tokenizer)\r\n    input_ids = conversations_tokenized[\"input_ids\"]\r\n    targets = copy.deepcopy(input_ids)\r\n    for target, source in zip(targets, sources):\r\n        tokenized_lens = _tokenize_fn([header] + [s[\"value\"] for s in source],\r\n                                      tokenizer)[\"input_ids_lens\"]\r\n        speakers = [sentence[\"from\"] for sentence in source]\r\n        _mask_targets(target, tokenized_lens, speakers)\r\n\r\n    return dict(input_ids=input_ids, labels=targets)\r\n```\r\n\r\nwhat we can end up with this preprocess function is a sequence like\r\n\r\n```\r\nsystem-message\\n\\n### Human: .... \\n ### Assistant: .... \\n\r\n```\r\n\r\nAfter tokenizing the sequence, we mask the sequence as\r\n```\r\n【 system-message \\n】\\n ### 【Human: … 】\\n ### Assistant: … \\n <s>\r\n```\r\ntokens in 【】 are masked.\r\n\r\nA masked version following the paper should be like\r\n```\r\n【system-message \\n\\n】 ### 【Human: … \\n】 ### 【Assistant:】 … \\n <s>\r\n```\r\n\r\nPlease kindly correct me if I am wrong, I am confused by the misalignment between the code and the paper.\r\n\r\nThank you again for building this repo.\n</Comment>\n<Comment by haotian-liu at 2023-04-25T18:52:31Z>\nHi, thank you for pointing this out, and there is indeed a disparity between the code and the paper that we would definitely fix. I do not find the issues with the current implementation, and let me try to clarify the design choices here.\r\n\r\n1. At the point I cloned Vicuna code base, the conversation is constructed in this way:\r\n```\r\n<system_message>\r\n<human_part>\r\n<gpt_part>\r\n```\r\n\r\n`human_part` and `gpt_part` are constructed following\r\n```\r\n### {Human/GPT}: {content}\\n\r\n```\r\n\r\nDuring training, `<system_message>` and `<human_part>` are masked.\r\n\r\nIn my preliminary experiments, I noticed that some times the agent does not know when to stop, and I suspect the reason being that there are single Q-A pair in our dataset (for example, complex reasoning and detail question, not exactly the same in our preliminary experiments, but it's a sense of why there are single Q-A pairs). The model is not trained with adding `###` to the end of GPT's response, when there are only single Q-A pairs.\r\n\r\nAs a fix, I appended `###` to the end of the conversation (despite single-turn or multiple turn), and make them unmasked.\r\n\r\nTherefore, you will see that comparing with the original Vicuna's implementation, I have two un-masked components: (1) `###`s in human conversations, (2) a single `###` at the end of the *whole* conversation.\r\n\r\nPlease let me know if this clarifies the doubts, and if the design choices make sense to you.  Please let me know if you have suggestions in potential improvements to the design.\r\n\r\nThank you!\n</Comment>\n<Comment by NormXU at 2023-04-26T02:13:59Z>\n@haotian-liu I understand. One more question: Do you suggest masking `Assistant:` ?\n</Comment>\n<Comment by haotian-liu at 2023-04-26T02:17:27Z>\nTo me, correctly predicting `Assistant` after `###` seems pretty easy, especially for a LLM with 7B parameters or even more.  So masking it or not probably would not make a big difference?  Therefore, I simply follow Vicuna on keeping it unmasked.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 57,
    "state": "open",
    "created_by": "sssssshf",
    "created_at": "2023-04-25T06:36:19Z",
    "labels": [
      "enhancement"
    ],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/57</URL>\n\n<TITLE>python inference demo</TITLE>\n\n<BODY>Do I have to use a browser to demonstrate when running a large model locally?\r\nIs there a demo in Python that directly feeds images and language into Python?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-25T21:30:28Z>\nHi, thank you for your interest in our work. \r\n\r\nThis is a great suggestion!  We have added an example script for CLI inference (single-turn Q-A session).  An interactive CLI interface is WIP.\r\n\r\nPlease see instruction here: https://github.com/haotian-liu/LLaVA#cli-inference.\n</Comment>\n<Comment by sssssshf at 2023-04-26T06:06:35Z>\nwhen I run this shell :\r\npython -m llava.eval.run_llava \\\r\n    --model-name /LLaVA-13B-v0 \\\r\n    --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n    --query \"What are the things I should be cautious about when I visit here?\"\r\n\r\nit's error : \r\nHFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: '/LLaVA-13B-v0'.\n</Comment>\n<Comment by sssssshf at 2023-04-26T06:09:33Z>\n为什么直接下载这里的的模型不能直接用？https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0\n</Comment>\n<Comment by MaxFBurg at 2023-04-26T09:04:09Z>\nCool, thank you very much @haotian-liu ! Do you have plans for providing a CLI that allows to feed multiple images and text prompts turn by turn anytime soon? This would be super cool to use your model for new downstream tasks.\n</Comment>\n<Comment by vishaal27 at 2023-04-26T12:37:41Z>\nYes, I agree with @MaxFBurg, are there any such implementation plans?\n</Comment>\n<Comment by haotian-liu at 2023-04-29T03:43:06Z>\n@MaxFBurg @vishaal27 \r\n\r\nYes, that's a great suggestion, and as mentioned in my previous reply, the interactive CLI support is planned.  We are planning to upgrade to the Vicuna v1.1 soon, as it has a better support for these.  Stay tuned!  And if you are interested in contributing, please let me know!\n</Comment>\n<Comment by haotian-liu at 2023-04-29T03:43:45Z>\n> 为什么直接下载这里的的模型不能直接用？https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0\r\n\r\nWe are not allowed to share the full model weights due to the LLaMA license, please see [here](https://github.com/haotian-liu/LLaVA#llava-weights) for weight conversion.\n</Comment>\n<Comment by vishaal27 at 2023-04-29T12:38:47Z>\nThanks for your response @haotian-liu \r\nI tried replacing these lines in your eval script `llava.eval.run_llava.py`:\r\n\r\n```python\r\n    qs = args.query\r\n    if mm_use_im_start_end:\r\n        qs = qs + '\\n' + DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len + DEFAULT_IM_END_TOKEN\r\n    else:\r\n        qs = qs + '\\n' + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len\r\n```\r\n\r\nwith \r\n\r\n```python\r\n    qs = args.query\r\n    if mm_use_im_start_end:\r\n        qs = qs + \"\\n\" + DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len + DEFAULT_IM_END_TOKEN + \"\\n\" + DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len + DEFAULT_IM_END_TOKEN\r\n    else:\r\n        qs = qs + \"\\n\" + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len + \"\\n\" + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len\r\n```\r\n\r\nI think this would be a naive extension of the current single image one turn inference procedure to a single turn inference procedure that can take two images as input in the prompt. Do you think something as straightforward as this will work out of the box?\r\n\r\nHowever, this doesn't seem to work well in practice for multi-image comparisons, a few examples follow:\r\n\r\nFor all the below examples, I used the following prompt with the modified code above: `{<img_1> <img_2> <\"Describe the change applied to the first image to get to the second image\">}`.\r\n\r\n![Screenshot 2023-04-29 at 1 57 07 PM](https://user-images.githubusercontent.com/21314772/235301342-3ffcbeb7-1f9f-4818-88c7-032c70330f0c.png)\r\n\r\n![Screenshot 2023-04-29 at 1 57 22 PM](https://user-images.githubusercontent.com/21314772/235301357-ff073414-037e-4b49-866a-4f6b54c99203.png)\r\n\r\n![Screenshot 2023-04-29 at 1 57 15 PM](https://user-images.githubusercontent.com/21314772/235301360-fdb1b3bb-b6ea-4d83-b8ba-506218f3ef85.png)\r\n\r\nAs you can see, the generations completely ignore the first image and give a detailed description of the second image. However, the model does understand that there are two images in the third case, given that its description contains \"In the second image\". \r\n\r\nFor a comparison, these are the model's responses when prompting with the same images and prompt on the web demo:\r\n\r\n![Screenshot 2023-04-29 at 2 02 59 PM](https://user-images.githubusercontent.com/21314772/235301499-8d5b9870-099a-45a2-b4b6-2a6481eda7c1.png)\r\n\r\nThis response is more coherent, and describes the difference between the two images fairly reasonably.\r\n\r\nI am wondering if this is an inherent limitation of the single-turn multi-image prompting style I've used above since it could be out-of-distribution (since your visual instruction tuning dataset only contains a single image per sample) for the model. Do you have any suggestions on a better evaluation strategy for this multi-image comparison either through single turn or multi turn prompting?\n</Comment>\n<Comment by penghe2021 at 2023-04-30T07:42:06Z>\n> Thanks for your response @haotian-liu I tried replacing these lines in your eval script `llava.eval.run_llava.py`:\r\n> \r\n> ```python\r\n>     qs = args.query\r\n>     if mm_use_im_start_end:\r\n>         qs = qs + '\\n' + DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len + DEFAULT_IM_END_TOKEN\r\n>     else:\r\n>         qs = qs + '\\n' + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len\r\n> ```\r\n> \r\n> with\r\n> \r\n> ```python\r\n>     qs = args.query\r\n>     if mm_use_im_start_end:\r\n>         qs = qs + \"\\n\" + DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len + DEFAULT_IM_END_TOKEN + \"\\n\" + DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len + DEFAULT_IM_END_TOKEN\r\n>     else:\r\n>         qs = qs + \"\\n\" + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len + \"\\n\" + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len\r\n> ```\r\n> \r\n> I think this would be a naive extension of the current single image one turn inference procedure to a single turn inference procedure that can take two images as input in the prompt. Do you think something as straightforward as this will work out of the box?\r\n> \r\n> However, this doesn't seem to work well in practice for multi-image comparisons, a few examples follow:\r\n> \r\n> For all the below examples, I used the following prompt with the modified code above: `{<img_1> <img_2> <\"Describe the change applied to the first image to get to the second image\">}`.\r\n> \r\n> ![Screenshot 2023-04-29 at 1 57 07 PM](https://user-images.githubusercontent.com/21314772/235301342-3ffcbeb7-1f9f-4818-88c7-032c70330f0c.png)\r\n> \r\n> ![Screenshot 2023-04-29 at 1 57 22 PM](https://user-images.githubusercontent.com/21314772/235301357-ff073414-037e-4b49-866a-4f6b54c99203.png)\r\n> \r\n> ![Screenshot 2023-04-29 at 1 57 15 PM](https://user-images.githubusercontent.com/21314772/235301360-fdb1b3bb-b6ea-4d83-b8ba-506218f3ef85.png)\r\n> \r\n> As you can see, the generations completely ignore the first image and give a detailed description of the second image. However, the model does understand that there are two images in the third case, given that its description contains \"In the second image\".\r\n> \r\n> For a comparison, these are the model's responses when prompting with the same images and prompt on the web demo:\r\n> \r\n> ![Screenshot 2023-04-29 at 2 02 59 PM](https://user-images.githubusercontent.com/21314772/235301499-8d5b9870-099a-45a2-b4b6-2a6481eda7c1.png)\r\n> \r\n> This response is more coherent, and describes the difference between the two images fairly reasonably.\r\n> \r\n> I am wondering if this is an inherent limitation of the single-turn multi-image prompting style I've used above since it could be out-of-distribution (since your visual instruction tuning dataset only contains a single image per sample) for the model. Do you have any suggestions on a better evaluation strategy for this multi-image comparison either through single turn or multi turn prompting?\r\n\r\nDid you also change the image part\n</Comment>\n<Comment by vishaal27 at 2023-04-30T09:41:36Z>\nYes, this is the code I updated:\r\n\r\n```python\r\n    image = load_image(args.image_file)\r\n    image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\r\n```\r\n\r\nwith \r\n\r\n```python\r\n   image_tensor = torch.stack(\r\n        [\r\n            image_processor.preprocess(load_image(image_file), return_tensors=\"pt\")[\"pixel_values\"][0]\r\n            for image_file in args.image_file.split(\",\")\r\n        ]\r\n    )\r\n\r\n    input_ids = torch.as_tensor(inputs.input_ids).cuda()\r\n```\r\n\r\nI just pass in comma-separated image input files. Please let me know whether there is an issue in this impl?\n</Comment>\n<Comment by haotian-liu at 2023-04-30T18:20:20Z>\n@penghe2021 Due to the current way of training, we do not observe the model having very good capability referring to / comparing with multiple images.  We are working on improving this aspect as well, stay tuned!\n</Comment>\n<Comment by vishaal27 at 2023-04-30T19:05:31Z>\nThanks @haotian-liu, so I assume the above implementation for a single turn multi-image inference is correct, but its an OOD problem due to the current training set-up of the model (sees only one image per sample during visual instruction tuning). However, I still see the model performing well on multiple images when used in the multi-turn set-up, so looking forward to your demo implementation of that, do you have a plan for when that can be released?\n</Comment>\n<Comment by Marcusntnu at 2023-05-02T14:55:04Z>\nIt might be a superfluous or large request, but if the model could be integrated into a huggingface AutoModel or pipeline setup, I think it would be very accessible. Especially for experimenting with different use-cases.\n</Comment>\n<Comment by haotian-liu at 2023-05-05T04:39:27Z>\nHi @Marcusntnu, thank you for your interest in our work, and thank you for the great suggestion. This is WIP, and our first step is to move that the LLaVA model implementation to this repo, which has been completed.  It should be implemented very soon, thanks and stay tuned!\n</Comment>\n<Comment by wjjlisa at 2023-05-17T10:12:13Z>\n@haotian-liu Have you ever considered releasing the multi turn infernece code?\n</Comment>\n<Comment by haotian-liu at 2023-05-17T18:24:18Z>\n@wjjlisa, do you mean the multi-turn conversation in CLI, as in our Gradio demo?  This is planned for release by the end of this month.  Was busy working on the NeurIPS recently...\n</Comment>\n<Comment by SeungyounShin at 2023-05-29T11:14:00Z>\n<img width=\"726\" alt=\"Screen Shot 2023-05-29 at 8 13 09 PM\" src=\"https://github.com/haotian-liu/LLaVA/assets/20262536/2acd7b6d-c689-42b1-afa6-6bea5a034a88\">\r\n\r\nThis is my experiments with prompt tunning. Not perfect but pretty amazing\r\n\r\nSeems like img1,img2,text is better performing.\n</Comment>\n<Comment by cyril-mino at 2023-06-13T07:46:18Z>\n@vishaal27 I Would like to know what the structure of the data input looks like. I am trying to do a similar thing.\n</Comment>\n<Comment by vishaal27 at 2023-06-13T08:36:05Z>\n@cyril-mino Sorry I don't get your question -- what do you mean by structure of data input? I just pass in two images to the model as a list of tensors (with the updated code above) and pass in the prompt that asks to compare the two images.\n</Comment>\n<Comment by cyril-mino at 2023-06-13T08:37:49Z>\n@vishaal27 apologies, I thought you were finetuning.\n</Comment>\n<Comment by adrielkuek at 2023-06-13T10:36:50Z>\n> <img width=\"726\" alt=\"Screen Shot 2023-05-29 at 8 13 09 PM\" src=\"https://github.com/haotian-liu/LLaVA/assets/20262536/2acd7b6d-c689-42b1-afa6-6bea5a034a88\">\n> \n> \n> \n> This is my experiments with prompt tunning. Not perfect but pretty amazing\n> \n> \n> \n> Seems like img1,img2,text is better performing.\n\nHi, possible to share the input query prompts for this output? Thanks!\n</Comment>\n<Comment by adrielkuek at 2023-06-14T02:07:26Z>\n> @wjjlisa, do you mean the multi-turn conversation in CLI, as in our Gradio demo? This is planned for release by the end of this month. Was busy working on the NeurIPS recently...\r\n\r\ncan I check whether the multi-turn framework would be updated into the repo anytime soon? Thanks for the great work.\n</Comment>\n<Comment by adrielkuek at 2023-06-19T08:08:30Z>\n> Yes, this is the code I updated:\r\n> \r\n> ```python\r\n>     image = load_image(args.image_file)\r\n>     image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\r\n> ```\r\n> \r\n> with\r\n> \r\n> ```python\r\n>    image_tensor = torch.stack(\r\n>         [\r\n>             image_processor.preprocess(load_image(image_file), return_tensors=\"pt\")[\"pixel_values\"][0]\r\n>             for image_file in args.image_file.split(\",\")\r\n>         ]\r\n>     )\r\n> \r\n>     input_ids = torch.as_tensor(inputs.input_ids).cuda()\r\n> ```\r\n> \r\n> I just pass in comma-separated image input files. Please let me know whether there is an issue in this impl?\r\n\r\nHi Vishaal, by stacking the tensors we create a new dimension input for the model which would throw an exception, how did you overcome this issue?\n</Comment>\n<Comment by vishaal27 at 2023-06-21T10:03:08Z>\nHi Adriel, as far as I recall (I must admit I haven't looked at this script in over a month) the `model.generate` function was able to take in multiple input images as a concatenated tensor. For full clarity, here is the script I used, hope it helps (disclaimer: this script uses a fork of the repository that is quite old, it is possible a few things might have changed since then): https://github.com/MaxFBurg/LLaVA/blob/main/llava/eval/run_llava_two_images.py#L51\n</Comment>\n<Comment by adrielkuek at 2023-06-22T14:18:38Z>\n> \r\n\r\nHi Vishaal, thanks for sharing the code. Indeed the fork has changed quite a fair bit. Seems like the mm-projector is removed, and the pretrained model as well. I can confirm that the current fork with the modified image tensor input is unable to work due to the dimensionality error in one of the nn.modules during forward pass. Can I do a quick check with you, did you use the llama-13b model or the facebook/opt model for your testing back then?\n</Comment>\n<Comment by vishaal27 at 2023-06-22T20:42:56Z>\nGreat, thanks for letting me know -- I will however need to get back to this script at some point  and get it to work, so I can let you know if I can figure something out for this use-case. Please do let me know if you are able to as well :)\r\nRe. your question -- We used the llama-13b model back then, I think at that stage the opt model was not available if I recall correctly.\n</Comment>\n<Comment by codybum at 2023-07-02T07:40:20Z>\n> Hi Vishaal, thanks for sharing the code. Indeed the fork has changed quite a fair bit. Seems like the mm-projector is removed, and the pretrained model as well. I can confirm that the current fork with the modified image tensor input is unable to work due to the dimensionality error in one of the nn.modules during forward pass. Can I do a quick check with you, did you use the llama-13b model or the facebook/opt model for your testing back then?\r\n\r\n@adrielkuek @vishaal27 \r\nWe are also very interested in using multi-image input.  Our interest is less in comparison, but rather using multiple images to represent the same thing, as described here: https://github.com/haotian-liu/LLaVA/issues/197#issuecomment-1567164371\n</Comment>\n<Comment by HireTheHero at 2023-09-10T01:56:08Z>\nInterested in multiple image input as well. We're wondering if we could perform multimodal few-shot classification (on-the-fly, without fine-tuning) or not.\r\nWill test Vishaal's solution and maybe create PR when I have time.\n</Comment>\n<Comment by LumenYoung at 2023-09-11T17:16:50Z>\nHi, everyone. I've be surfing in the code base of LLaVA for a while and find it hard to find the exact `generate()` function implementation for llama based LLaVa. I'm trying to find the generate() for LLaMA.  It would be helpful since I want to find a way to work in the multiple image mode. Any help would be appreciated!\n</Comment>\n<Comment by LumenYoung at 2023-09-11T17:59:51Z>\n> <img alt=\"Screen Shot 2023-05-29 at 8 13 09 PM\" width=\"726\" src=\"https://user-images.githubusercontent.com/20262536/241718727-2acd7b6d-c689-42b1-afa6-6bea5a034a88.png\">\r\n> \r\n> This is my experiments with prompt tunning. Not perfect but pretty amazing\r\n> \r\n> Seems like img1,img2,text is better performing.\r\n\r\nHi, SeungyounShin. Would you mind sharing how you manage to embed both image into one query. It would be really helpful as I currently am not able to find a way to do this.\n</Comment>\n<Comment by HireTheHero at 2023-09-12T13:45:07Z>\nSomething like #432 ? Would appreciate any suggestions.\n</Comment>\n<Comment by CreativeBuilds at 2023-10-25T13:55:34Z>\n> <img alt=\"Screen Shot 2023-05-29 at 8 13 09 PM\" width=\"726\" src=\"https://user-images.githubusercontent.com/20262536/241718727-2acd7b6d-c689-42b1-afa6-6bea5a034a88.png\">\r\n> This is my experiments with prompt tunning. Not perfect but pretty amazing\r\n> \r\n> Seems like img1,img2,text is better performing.\r\n\r\nWould you be able to upload this model to hugging face or share it some other way? Very interested in getting this to run with image comparison.\n</Comment>\n<Comment by aldoz-mila at 2024-01-15T18:27:57Z>\nHello, I am also interested in inputting more than one image for some experiments. I am trying to find the right template for this, considering that the base template is `USER: <image>\\n<prompt>\\nASSISTANT:`. \r\n\r\n- (i) What would be the prompt template when feeding more than one image? Should I use something like `USER: <image1><image2>\\n<prompt>\\nASSISTANT:`? \r\n- (ii) How do you input the multiple images? Do you concatenate them in the first dimension? For example, assuming this in my code snippet (from HuggingFace integration of LLaVA): `output = pipe(\r\n            image, prompt=text_prompt, generate_kwargs={\"max_new_tokens\": 200}\r\n        )`\n</Comment>\n<Comment by fisher75 at 2024-03-25T10:23:30Z>\nI am in great need of a multi-dialog feature for batch-inference with SGLang.\n</Comment>\n<Comment by Sprinter1999 at 2024-04-02T03:37:45Z>\n> @vishaal27 apologies, I thought you were finetuning.\r\n\r\nHi cyril-mino, did you make it to support the fine-tuning LLaVa with multi-images & text? I wonder if there are some extra steps besides the codes mentioned by @vishaal27. Thanks~\n</Comment>\n<Comment by yc-cui at 2024-04-22T11:13:04Z>\nIs there any way we can embed other modalities? Such as bbox, class ... ?\n</Comment>\n<Comment by XinrunXu at 2024-09-18T04:58:53Z>\n> Is there any way we can embed other modalities? Such as bbox, class ... ?我们有什么方法可以嵌入其他模式吗？比如 bbox、class ... ？\r\n\r\nHave you figured it out？\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 56,
    "state": "closed",
    "created_by": "zxcvbn114514",
    "created_at": "2023-04-25T06:05:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/56</URL>\n\n<TITLE>Is it normal when launching a model worker?</TITLE>\n\n<BODY>![image](https://user-images.githubusercontent.com/128148517/234188153-c8327001-afb3-42ce-b295-8b50e55d83a8.png)</BODY>\n\n<COMMENTS>\n<Comment by zxcvbn114514 at 2023-04-25T06:31:28Z>\nIt failed at last....\r\nlast few lines:\r\n2023-04-25 14:30:18 | ERROR | stderr |     self._send_request(method, url, body, headers, encode_chunked)\r\n2023-04-25 14:30:18 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\http\\client.py\", line 1328, in _send_request\r\n2023-04-25 14:30:18 | ERROR | stderr |     self.endheaders(body, encode_chunked=encode_chunked)\r\n2023-04-25 14:30:18 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\http\\client.py\", line 1277, in endheaders\r\n2023-04-25 14:30:18 | ERROR | stderr |     self._send_output(message_body, encode_chunked=encode_chunked)\r\n2023-04-25 14:30:18 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\http\\client.py\", line 1037, in _send_output\r\n2023-04-25 14:30:18 | ERROR | stderr |     self.send(msg)\r\n2023-04-25 14:30:18 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\http\\client.py\", line 975, in send\r\n2023-04-25 14:30:18 | ERROR | stderr |     self.connect()\r\n2023-04-25 14:30:18 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\urllib3\\connection.py\", line 205, in connect\r\n2023-04-25 14:30:18 | ERROR | stderr |     conn = self._new_conn()\r\n2023-04-25 14:30:18 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\r\n2023-04-25 14:30:18 | ERROR | stderr |     raise NewConnectionError(\r\n2023-04-25 14:30:18 | ERROR | stderr | urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002A68ECEECE0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。\r\n2023-04-25 14:30:18 | ERROR | stderr |\r\n2023-04-25 14:30:18 | ERROR | stderr | During handling of the above exception, another exception occurred:\r\n2023-04-25 14:30:18 | ERROR | stderr |\r\n2023-04-25 14:30:18 | ERROR | stderr | Traceback (most recent call last):\r\n2023-04-25 14:30:18 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\requests\\adapters.py\", line 489, in send\r\n2023-04-25 14:30:18 | ERROR | stderr |     resp = conn.urlopen(\r\n2023-04-25 14:30:18 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\r\n2023-04-25 14:30:18 | ERROR | stderr |     retries = retries.increment(\r\n2023-04-25 14:30:18 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\r\n2023-04-25 14:30:18 | ERROR | stderr |     raise MaxRetryError(_pool, url, error or ResponseError(cause))\r\n2023-04-25 14:30:18 | ERROR | stderr | urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=10000): Max retries exceeded with url: /register_worker (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002A68ECEECE0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\r\n2023-04-25 14:30:18 | ERROR | stderr |\r\n2023-04-25 14:30:18 | ERROR | stderr | During handling of the above exception, another exception occurred:\r\n2023-04-25 14:30:18 | ERROR | stderr |\r\n2023-04-25 14:30:18 | ERROR | stderr | Traceback (most recent call last):\r\n2023-04-25 14:30:18 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\runpy.py\", line 196, in _run_module_as_main\r\n2023-04-25 14:30:18 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n2023-04-25 14:30:18 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\runpy.py\", line 86, in _run_code\r\n2023-04-25 14:30:18 | ERROR | stderr |     exec(code, run_globals)\r\n2023-04-25 14:30:18 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\serve\\model_worker.py\", line 361, in <module>\r\n2023-04-25 14:30:18 | ERROR | stderr |     worker = ModelWorker(args.controller_address,\r\n2023-04-25 14:30:18 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\serve\\model_worker.py\", line 122, in __init__\r\n2023-04-25 14:30:18 | ERROR | stderr |     self.register_to_controller()\r\n2023-04-25 14:30:18 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\serve\\model_worker.py\", line 136, in register_to_controller\r\n2023-04-25 14:30:18 | ERROR | stderr |     r = requests.post(url, json=data)\r\n2023-04-25 14:30:18 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\requests\\api.py\", line 115, in post\r\n2023-04-25 14:30:18 | ERROR | stderr |     return request(\"post\", url, data=data, json=json, **kwargs)\r\n2023-04-25 14:30:18 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\requests\\api.py\", line 59, in request\r\n2023-04-25 14:30:18 | ERROR | stderr |     return session.request(method=method, url=url, **kwargs)\r\n2023-04-25 14:30:18 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\r\n2023-04-25 14:30:18 | ERROR | stderr |     resp = self.send(prep, **send_kwargs)\r\n2023-04-25 14:30:18 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\r\n2023-04-25 14:30:18 | ERROR | stderr |     r = adapter.send(request, **kwargs)\r\n2023-04-25 14:30:18 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\requests\\adapters.py\", line 565, in send\r\n2023-04-25 14:30:18 | ERROR | stderr |     raise ConnectionError(e, request=request)\r\n2023-04-25 14:30:18 | ERROR | stderr | requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=10000): Max retries exceeded with url: /register_worker (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002A68ECEECE0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n</Comment>\n<Comment by haotian-liu at 2023-04-25T22:14:30Z>\nHi, thank you for the interest in our work.\r\n\r\nThis seems a similar issue to this one: https://github.com/lm-sys/FastChat/issues/186\r\n\r\nHave you launched the controller first before running the model worker?\r\n\r\n> You must follow the three steps:\r\n> \r\n> \r\n> \r\n> launch a controller\r\n> launch a worker and connect to the controller\r\n> launch Gradio webserver and connect to the controller.\r\n> You cannot launch a server alone.\n</Comment>\n<Comment by haotian-liu at 2023-05-25T19:59:52Z>\nClosing the issue due to inactivity, please feel free to re-open if you meet any other issues, thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 55,
    "state": "open",
    "created_by": "zxcvbn114514",
    "created_at": "2023-04-25T05:25:14Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/55</URL>\n\n<TITLE>Error came out when I was applying delta.</TITLE>\n\n<BODY>PS A:\\LLaVA> py -3 -m llava.model.apply_delta --base A:/vicuna-minigpt/vicuna/llama-13b/LLaMA/output/13B  --target A:/llava13b  --delta A:/llava-delta\r\nLoading base model\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 3/3 [00:17<00:00,  5.93s/it]\r\nLoading delta\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 3/3 [00:20<00:00,  6.97s/it]\r\nSome weights of the model checkpoint at A:/llava-delta were not used when initializing LlamaForCausalLM: ['model.mm_projector.weight', 'model.mm_projector.bias']\r\n- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\nApplying delta\r\nApplying delta: 100%|████████████████████████████████████████████████████████████████| 403/403 [00:40<00:00,  9.90it/s]\r\nSaving target model\r\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\r\n│ C:\\Users\\Ge Yunxiang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py:196 in                 │\r\n│ _run_module_as_main                                                                              │\r\n│                                                                                                  │\r\n│   193 │   main_globals = sys.modules[\"__main__\"].__dict__                                        │\r\n│   194 │   if alter_argv:                                                                         │\r\n│   195 │   │   sys.argv[0] = mod_spec.origin                                                      │\r\n│ ❱ 196 │   return _run_code(code, main_globals, None,                                             │\r\n│   197 │   │   │   │   │    \"__main__\", mod_spec)                                                 │\r\n│   198                                                                                            │\r\n│   199 def run_module(mod_name, init_globals=None,                                                │\r\n│                                                                                                  │\r\n│ C:\\Users\\Ge Yunxiang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py:86 in _run_code        │\r\n│                                                                                                  │\r\n│    83 │   │   │   │   │      __loader__ = loader,                                                │\r\n│    84 │   │   │   │   │      __package__ = pkg_name,                                             │\r\n│    85 │   │   │   │   │      __spec__ = mod_spec)                                                │\r\n│ ❱  86 │   exec(code, run_globals)                                                                │\r\n│    87 │   return run_globals                                                                     │\r\n│    88                                                                                            │\r\n│    89 def _run_module_code(code, init_globals=None,                                              │\r\n│                                                                                                  │\r\n│ A:\\LLaVA\\llava\\model\\apply_delta.py:47 in <module>                                               │\r\n│                                                                                                  │\r\n│   44 │                                                                                           │\r\n│   45 │   args = parser.parse_args()                                                              │\r\n│   46 │                                                                                           │\r\n│ ❱ 47 │   apply_delta(args.base_model_path, args.target_model_path, args.delta_path)              │\r\n│   48                                                                                             │\r\n│                                                                                                  │\r\n│ A:\\LLaVA\\llava\\model\\apply_delta.py:35 in apply_delta                                            │\r\n│                                                                                                  │\r\n│   32 │   │   │   param.data[:bparam.shape[0], :bparam.shape[1]] += bparam                        │\r\n│   33 │                                                                                           │\r\n│   34 │   print(\"Saving target model\")                                                            │\r\n│ ❱ 35 │   delta.save_pretrained(target_model_path)                                                │\r\n│   36 │   delta_tokenizer.save_pretrained(target_model_path)                                      │\r\n│   37                                                                                             │\r\n│   38                                                                                             │\r\n│                                                                                                  │\r\n│ C:\\Users\\Ge                                                                                      │\r\n│ Yunxiang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.p │\r\n│ y:1843 in save_pretrained                                                                        │\r\n│                                                                                                  │\r\n│   1840 │   │   │   │   # joyfulness), but for now this enough.                                   │\r\n│   1841 │   │   │   │   safe_save_file(shard, os.path.join(save_directory, shard_file), metadata  │\r\n│   1842 │   │   │   else:                                                                         │\r\n│ ❱ 1843 │   │   │   │   save_function(shard, os.path.join(save_directory, shard_file))            │\r\n│   1844 │   │                                                                                     │\r\n│   1845 │   │   if index is None:                                                                 │\r\n│   1846 │   │   │   path_to_weights = os.path.join(save_directory, _add_variant(WEIGHTS_NAME, va  │\r\n│                                                                                                  │\r\n│ C:\\Users\\Ge                                                                                      │\r\n│ Yunxiang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:422 in │\r\n│ save                                                                                             │\r\n│                                                                                                  │\r\n│    419 │   _check_dill_version(pickle_module)                                                    │\r\n│    420 │                                                                                         │\r\n│    421 │   if _use_new_zipfile_serialization:                                                    │\r\n│ ❱  422 │   │   with _open_zipfile_writer(f) as opened_zipfile:                                   │\r\n│    424 │   │   │   return                                                                        │\r\n│    425 │   else:                                                                                 │\r\n│                                                                                                  │\r\n│ C:\\Users\\Ge                                                                                      │\r\n│ Yunxiang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:309 in │\r\n│ _open_zipfile_writer                                                                             │\r\n│                                                                                                  │\r\n│    306 │   │   container = _open_zipfile_writer_file                                             │\r\n│    307 │   else:                                                                                 │\r\n│    308 │   │   container = _open_zipfile_writer_buffer                                           │\r\n│ ❱  309 │   return container(name_or_buffer)                                                      │\r\n│    310                                                                                           │\r\n│    311                                                                                           │\r\n│    312 def _is_compressed_file(f) -> bool:                                                       │\r\n│                                                                                                  │\r\n│ C:\\Users\\Ge                                                                                      │\r\n│ Yunxiang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:287 in │\r\n│ __init__                                                                                         │\r\n│                                                                                                  │\r\n│    284                                                                                           │\r\n│    285 class _open_zipfile_writer_file(_opener):                                                 │\r\n│    286 │   def __init__(self, name) -> None:                                                     │\r\n│ ❱  287 │   │   super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(n  │\r\n│    288 │                                                                                         │\r\n│    289 │   def __exit__(self, *args) -> None:                                                    │\r\n│    290 │   │   self.file_like.write_end_of_file()                                                │\r\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\r\nRuntimeError: Parent directory A: does not exist.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-25T18:01:17Z>\nHi, thank you for your interest in our work.\r\n\r\nI noticed this error in your log:\r\n\r\n```\r\nSome weights of the model checkpoint at A:/llava-delta were not used when initializing LlamaForCausalLM: ['model.mm_projector.weight', 'model.mm_projector.bias']\r\n```\r\n\r\nThis may suggest that the `transformers` library is not installed correctly.  Please try doing a clean `https://github.com/haotian-liu/LLaVA#install` with a new conda environment.  You may also first try reinstalling `transformers` by first uninstalling and then reinstalling, if you do not want to start a new conda environment, but this is not guaranteed to fix all version related issues:\r\n```\r\npip uninstall transformers\r\npip install git+https://github.com/haotian-liu/transformers_llava.git@988b6abb3b7da9a5cbb5051e994706f7f88c2565\r\n```\r\n\r\nThanks, and please let me know if you find further issues.\n</Comment>\n<Comment by zxcvbn114514 at 2023-04-26T03:05:45Z>\n> Hi, thank you for your interest in our work.\r\n> \r\n> I noticed this error in your log:\r\n> \r\n> ```\r\n> Some weights of the model checkpoint at A:/llava-delta were not used when initializing LlamaForCausalLM: ['model.mm_projector.weight', 'model.mm_projector.bias']\r\n> ```\r\n> \r\n> This may suggest that the `transformers` library is not installed correctly. Please try doing a clean `https://github.com/haotian-liu/LLaVA#install` with a new conda environment. You may also first try reinstalling `transformers` by first uninstalling and then reinstalling, if you do not want to start a new conda environment, but this is not guaranteed to fix all version related issues:\r\n> \r\n> ```\r\n> pip uninstall transformers\r\n> pip install git+https://github.com/haotian-liu/transformers_llava.git@988b6abb3b7da9a5cbb5051e994706f7f88c2565\r\n> ```\r\n> \r\n> Thanks, and please let me know if you find further issues.\r\n\r\nI'll certainly try it. Thanks a lot btw. You are really committed to your work\n</Comment>\n<Comment by zxcvbn114514 at 2023-04-26T03:41:53Z>\n> Hi, thank you for your interest in our work.\r\n> \r\n> I noticed this error in your log:\r\n> \r\n> ```\r\n> Some weights of the model checkpoint at A:/llava-delta were not used when initializing LlamaForCausalLM: ['model.mm_projector.weight', 'model.mm_projector.bias']\r\n> ```\r\n> \r\n> This may suggest that the `transformers` library is not installed correctly. Please try doing a clean `https://github.com/haotian-liu/LLaVA#install` with a new conda environment. You may also first try reinstalling `transformers` by first uninstalling and then reinstalling, if you do not want to start a new conda environment, but this is not guaranteed to fix all version related issues:\r\n> \r\n> ```\r\n> pip uninstall transformers\r\n> pip install git+https://github.com/haotian-liu/transformers_llava.git@988b6abb3b7da9a5cbb5051e994706f7f88c2565\r\n> ```\r\n> \r\n> Thanks, and please let me know if you find further issues.\r\n\r\nI reinstalled literally everything, but the error '**Some weights of the model checkpoint at A:/llava-delta were not used when initializing LlamaForCausalLM: ['model.mm_projector.weight', 'model.mm_projector.bias']**' still popped out .However the progam continued and finally gave me a mixed model. I started the controller and tried to run it but there was another error :\r\n2023-04-26 11:39:57 | INFO | model_worker | Register to controller\r\n2023-04-26 11:39:59 | ERROR | stderr | Traceback (most recent call last):\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\r\n2023-04-26 11:39:59 | ERROR | stderr |     httplib_response = self._make_request(\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\urllib3\\connectionpool.py\", line 449, in _make_request\r\n2023-04-26 11:39:59 | ERROR | stderr |     six.raise_from(e, None)\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"<string>\", line 3, in raise_from\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\urllib3\\connectionpool.py\", line 444, in _make_request\r\n2023-04-26 11:39:59 | ERROR | stderr |     httplib_response = conn.getresponse()\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\http\\client.py\", line 1374, in getresponse\r\n2023-04-26 11:39:59 | ERROR | stderr |     response.begin()\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\http\\client.py\", line 318, in begin\r\n2023-04-26 11:39:59 | ERROR | stderr |     version, status, reason = self._read_status()\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\http\\client.py\", line 287, in _read_status\r\n2023-04-26 11:39:59 | ERROR | stderr |     raise RemoteDisconnected(\"Remote end closed connection without\"\r\n2023-04-26 11:39:59 | ERROR | stderr | http.client.RemoteDisconnected: Remote end closed connection without response\r\n2023-04-26 11:39:59 | ERROR | stderr |\r\n2023-04-26 11:39:59 | ERROR | stderr | During handling of the above exception, another exception occurred:\r\n2023-04-26 11:39:59 | ERROR | stderr |\r\n2023-04-26 11:39:59 | ERROR | stderr | Traceback (most recent call last):\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\requests\\adapters.py\", line 489, in send\r\n2023-04-26 11:39:59 | ERROR | stderr |     resp = conn.urlopen(\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\r\n2023-04-26 11:39:59 | ERROR | stderr |     retries = retries.increment(\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\urllib3\\util\\retry.py\", line 550, in increment\r\n2023-04-26 11:39:59 | ERROR | stderr |     raise six.reraise(type(error), error, _stacktrace)\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\urllib3\\packages\\six.py\", line 769, in reraise\r\n2023-04-26 11:39:59 | ERROR | stderr |     raise value.with_traceback(tb)\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\r\n2023-04-26 11:39:59 | ERROR | stderr |     httplib_response = self._make_request(\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\urllib3\\connectionpool.py\", line 449, in _make_request\r\n2023-04-26 11:39:59 | ERROR | stderr |     six.raise_from(e, None)\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"<string>\", line 3, in raise_from\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\urllib3\\connectionpool.py\", line 444, in _make_request\r\n2023-04-26 11:39:59 | ERROR | stderr |     httplib_response = conn.getresponse()\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\http\\client.py\", line 1374, in getresponse\r\n2023-04-26 11:39:59 | ERROR | stderr |     response.begin()\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\http\\client.py\", line 318, in begin\r\n2023-04-26 11:39:59 | ERROR | stderr |     version, status, reason = self._read_status()\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\http\\client.py\", line 287, in _read_status\r\n2023-04-26 11:39:59 | ERROR | stderr |     raise RemoteDisconnected(\"Remote end closed connection without\"\r\n2023-04-26 11:39:59 | ERROR | stderr | urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\r\n2023-04-26 11:39:59 | ERROR | stderr |\r\n2023-04-26 11:39:59 | ERROR | stderr | During handling of the above exception, another exception occurred:\r\n2023-04-26 11:39:59 | ERROR | stderr |\r\n2023-04-26 11:39:59 | ERROR | stderr | Traceback (most recent call last):\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\runpy.py\", line 196, in _run_module_as_main\r\n2023-04-26 11:39:59 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\runpy.py\", line 86, in _run_code\r\n2023-04-26 11:39:59 | ERROR | stderr |     exec(code, run_globals)\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"A:\\llava\\llava\\serve\\model_worker.py\", line 361, in <module>\r\n2023-04-26 11:39:59 | ERROR | stderr |     worker = ModelWorker(args.controller_address,\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"A:\\llava\\llava\\serve\\model_worker.py\", line 122, in __init__\r\n2023-04-26 11:39:59 | ERROR | stderr |     self.register_to_controller()\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"A:\\llava\\llava\\serve\\model_worker.py\", line 136, in register_to_controller\r\n2023-04-26 11:39:59 | ERROR | stderr |     r = requests.post(url, json=data)\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\requests\\api.py\", line 115, in post\r\n2023-04-26 11:39:59 | ERROR | stderr |     return request(\"post\", url, data=data, json=json, **kwargs)\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\requests\\api.py\", line 59, in request\r\n2023-04-26 11:39:59 | ERROR | stderr |     return session.request(method=method, url=url, **kwargs)\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\r\n2023-04-26 11:39:59 | ERROR | stderr |     resp = self.send(prep, **send_kwargs)\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\r\n2023-04-26 11:39:59 | ERROR | stderr |     r = adapter.send(request, **kwargs)\r\n2023-04-26 11:39:59 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\requests\\adapters.py\", line 547, in send\r\n2023-04-26 11:39:59 | ERROR | stderr |     raise ConnectionError(err, request=request)\r\n2023-04-26 11:39:59 | ERROR | stderr | requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n</Comment>\n<Comment by zxcvbn114514 at 2023-04-26T03:44:40Z>\nthe controller info\r\n\r\n> Hi, thank you for your interest in our work.\r\n> \r\n> I noticed this error in your log:\r\n> \r\n> ```\r\n> Some weights of the model checkpoint at A:/llava-delta were not used when initializing LlamaForCausalLM: ['model.mm_projector.weight', 'model.mm_projector.bias']\r\n> ```\r\n> \r\n> This may suggest that the `transformers` library is not installed correctly. Please try doing a clean `https://github.com/haotian-liu/LLaVA#install` with a new conda environment. You may also first try reinstalling `transformers` by first uninstalling and then reinstalling, if you do not want to start a new conda environment, but this is not guaranteed to fix all version related issues:\r\n> \r\n> ```\r\n> pip uninstall transformers\r\n> pip install git+https://github.com/haotian-liu/transformers_llava.git@988b6abb3b7da9a5cbb5051e994706f7f88c2565\r\n> ```\r\n> \r\n> Thanks, and please let me know if you find further issues.\r\n\r\nThe controller info:\r\n(LLAVA) PS A:\\LLAVA> python -m llava.serve.controller --host 0.0.0.0 --port 10000\r\n2023-04-26 11:38:23 | INFO | controller | args: Namespace(host='0.0.0.0', port=10000, dispatch_method='shortest_queue')\r\n2023-04-26 11:38:23 | INFO | controller | Init controller\r\n2023-04-26 11:38:23 | ERROR | stderr | \u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m5192\u001b[0m]\r\n2023-04-26 11:38:23 | ERROR | stderr | \u001b[32mINFO\u001b[0m:     Waiting for application startup.\r\n2023-04-26 11:38:23 | ERROR | stderr | \u001b[32mINFO\u001b[0m:     Application startup complete.\r\n2023-04-26 11:38:23 | ERROR | stderr | \u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:10000\u001b[0m (Press CTRL+C to quit)\n</Comment>\n<Comment by haotian-liu at 2023-05-01T04:07:41Z>\nThis seems that the installation was not successful, possibly due to some dependency issues.\r\n\r\nWe have updated the code base by moving most of the LLaVA code to the main repo, and removes the dependency of the custom transformers framework.  Please try installing the latest version following the instructions, and see if it fix your issue.  Thanks!\n</Comment>\n<Comment by zxcvbn114514 at 2023-05-01T13:06:29Z>\n> This seems that the installation was not successful, possibly due to some dependency issues.\r\n> \r\n> We have updated the code base by moving most of the LLaVA code to the main repo, and removes the dependency of the custom transformers framework. Please try installing the latest version following the instructions, and see if it fix your issue. Thanks!\r\n\r\nThank a lot, I'll try it at once after the holiday.\n</Comment>\n<Comment by zxcvbn114514 at 2023-05-04T11:18:07Z>\n> This seems that the installation was not successful, possibly due to some dependency issues.\r\n> \r\n> We have updated the code base by moving most of the LLaVA code to the main repo, and removes the dependency of the custom transformers framework. Please try installing the latest version following the instructions, and see if it fix your issue. Thanks!\r\n\r\nUnfortunately, after I updated the llava.git, the problem still exists.\r\n\r\nHere's the info:\r\n(llava) PS A:\\llava> py -3 -m llava.model.apply_delta --base A:/vicuna-minigpt/vicuna/llama-13b/LLaMA/output/13B  --target llava13b  --delta A:/llava-delta\r\nLoading base model\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:08<00:00,  2.97s/it]\r\nLoading delta\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nSome weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.embeddings.position_ids', 'visual_projection.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_projection.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'logit_scale', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias']\r\n- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n</Comment>\n<Comment by zxcvbn114514 at 2023-05-04T11:21:22Z>\n> This seems that the installation was not successful, possibly due to some dependency issues.\r\n> \r\n> We have updated the code base by moving most of the LLaVA code to the main repo, and removes the dependency of the custom transformers framework. Please try installing the latest version following the instructions, and see if it fix your issue. Thanks!\r\n\r\nAnd this is the config.json of my base llama model.There seems to be something wrong with it compared with delta's config.json.\r\n\r\nbase config:\r\n{\r\n  \"architectures\": [\r\n    \"LlamaForCausalLM\"\r\n  ],\r\n  \"bos_token_id\": 1,\r\n  \"eos_token_id\": 2,\r\n  \"hidden_act\": \"silu\",\r\n  \"hidden_size\": 5120,\r\n  \"initializer_range\": 0.02,\r\n  \"intermediate_size\": 13824,\r\n  \"max_position_embeddings\": 2048,\r\n  \"model_type\": \"llama\",\r\n  \"num_attention_heads\": 40,\r\n  \"num_hidden_layers\": 40,\r\n  \"pad_token_id\": 0,\r\n  \"rms_norm_eps\": 1e-06,\r\n  \"tie_word_embeddings\": false,\r\n  \"torch_dtype\": \"float16\",\r\n  \"transformers_version\": \"4.29.0.dev0\",\r\n  \"use_cache\": true,\r\n  \"vocab_size\": 32000\r\n}\r\n\r\n\r\ndelta config:\r\n{\r\n  \"_name_or_path\": \"./checkpoints/LLaVA-13B-v0\",\r\n  \"architectures\": [\r\n    \"LlamaForCausalLM\"\r\n  ],\r\n  \"bos_token_id\": 0,\r\n  \"eos_token_id\": 1,\r\n  \"hidden_act\": \"silu\",\r\n  \"hidden_size\": 5120,\r\n  \"initializer_range\": 0.02,\r\n  \"intermediate_size\": 13824,\r\n  \"max_sequence_length\": 2048,\r\n  \"mm_hidden_size\": 1024,\r\n  \"mm_use_im_start_end\": true,\r\n  \"mm_vision_select_layer\": -2,\r\n  \"mm_vision_tower\": \"openai/clip-vit-large-patch14\",\r\n  \"model_type\": \"llama\",\r\n  \"num_attention_heads\": 40,\r\n  \"num_hidden_layers\": 40,\r\n  \"pad_token_id\": -1,\r\n  \"rms_norm_eps\": 1e-06,\r\n  \"tie_word_embeddings\": false,\r\n  \"torch_dtype\": \"float16\",\r\n  \"transformers_version\": \"4.28.0.dev0\",\r\n  \"tune_mm_mlp_adapter\": false,\r\n  \"use_cache\": true,\r\n  \"use_mm_proj\": true,\r\n  \"vocab_size\": 32003\r\n}\n</Comment>\n<Comment by haotian-liu at 2023-05-05T04:33:10Z>\nHi @zxcvbn114514 This is not an error.  It is a warning for us to check, but it is expected as we only used the vision encoder from CLIP, and drop the text encoder weights.   Since there are no further errors, I guess you may see the converted weights in `llava13b` folder?\n</Comment>\n<Comment by zxcvbn114514 at 2023-05-05T06:19:27Z>\n> Hi @zxcvbn114514 This is not an error. It is a warning for us to check, but it is expected as we only used the vision encoder from CLIP, and drop the text encoder weights. Since there are no further errors, I guess you may see the converted weights in `llava13b` folder?\r\n\r\nI continued and error  came out when launching a model worker:\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.callHandlers(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-05-05 14:18:22 | ERROR | stderr |     hdlr.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.callHandlers(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-05-05 14:18:22 | ERROR | stderr |     hdlr.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.callHandlers(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-05-05 14:18:22 | ERROR | stderr |     hdlr.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.callHandlers(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-05-05 14:18:22 | ERROR | stderr |     hdlr.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.callHandlers(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-05-05 14:18:22 | ERROR | stderr |     hdlr.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.callHandlers(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-05-05 14:18:22 | ERROR | stderr |     hdlr.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.callHandlers(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-05-05 14:18:22 | ERROR | stderr |     hdlr.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.callHandlers(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-05-05 14:18:22 | ERROR | stderr |     hdlr.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.callHandlers(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-05-05 14:18:22 | ERROR | stderr |     hdlr.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.callHandlers(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-05-05 14:18:22 | ERROR | stderr |     hdlr.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.callHandlers(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-05-05 14:18:22 | ERROR | stderr |     hdlr.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.callHandlers(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-05-05 14:18:22 | ERROR | stderr |     hdlr.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.callHandlers(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-05-05 14:18:22 | ERROR | stderr |     hdlr.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.callHandlers(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-05-05 14:18:22 | ERROR | stderr |     hdlr.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.callHandlers(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-05-05 14:18:22 | ERROR | stderr |     hdlr.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.callHandlers(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-05-05 14:18:22 | ERROR | stderr |     hdlr.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.callHandlers(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-05-05 14:18:22 | ERROR | stderr |     hdlr.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.callHandlers(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-05-05 14:18:22 | ERROR | stderr |     hdlr.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.callHandlers(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-05-05 14:18:22 | ERROR | stderr |     hdlr.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.callHandlers(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-05-05 14:18:22 | ERROR | stderr |     hdlr.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.callHandlers(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-05-05 14:18:22 | ERROR | stderr |     hdlr.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.callHandlers(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-05-05 14:18:22 | ERROR | stderr |     hdlr.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.callHandlers(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-05-05 14:18:22 | ERROR | stderr |     hdlr.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1547, in log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self._log(level, msg, args, **kwargs)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1624, in _log\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1634, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.callHandlers(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1696, in callHandlers\r\n2023-05-05 14:18:22 | ERROR | stderr |     hdlr.handle(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 968, in handle\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.emit(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\handlers.py\", line 75, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     logging.FileHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1218, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     StreamHandler.emit(self, record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1108, in emit\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.handleError(record)\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\logging\\__init__.py\", line 1038, in handleError\r\n2023-05-05 14:18:22 | ERROR | stderr |     sys.stderr.write('Message: %r\\n'\r\n2023-05-05 14:18:22 | ERROR | stderr |   File \"A:\\LLaVA\\llava\\utils.py\", line 83, in write\r\n2023-05-05 14:18:22 | ERROR | stderr |     self.logger.log(self.log_level, line.rstrip())\n</Comment>\n<Comment by zxcvbn114514 at 2023-05-05T06:23:31Z>\n@haotian-liu \r\n[model_worker_fbe705.log](https://github.com/haotian-liu/LLaVA/files/11403741/model_worker_fbe705.log)\n</Comment>\n<Comment by haotian-liu at 2023-05-25T20:00:09Z>\nHi do you resolve your issue as in #56?\n</Comment>\n<Comment by zxcvbn114514 at 2023-06-05T13:57:16Z>\n> Hi do you resolve your issue as in #56?\r\n\r\nSadly not .I've tried the new version but the problem still exists.\n</Comment>\n<Comment by adi-kmt at 2024-02-17T09:43:06Z>\n> Hi do you resolve your issue as in #56?\r\n\r\nHey @haotian-liu, getting similar issue. What do you think I can do to fix it?\r\nTried it with qwen 1.5\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 54,
    "state": "closed",
    "created_by": "zxcvbn114514",
    "created_at": "2023-04-25T04:08:21Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/54</URL>\n\n<TITLE>RuntimeError: The size of tensor a (32000) must match the size of tensor b (32003) at non-singleton dimension 0</TITLE>\n\n<BODY>PS A:\\vicuna-minigpt\\FastChat> python -m fastchat.model.apply_delta --base A:/vicuna-minigpt/vicuna/llama-13b/LLaMA/output/13B  --target A:/llava-minigpt  --delta A:/llava-delta\r\nLoading the base model from A:/vicuna-minigpt/vicuna/llama-13b/LLaMA/output/13B\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 3/3 [00:28<00:00,  9.37s/it]\r\nLoading the delta from A:/llava-delta\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 3/3 [00:35<00:00, 11.80s/it]\r\nSome weights of the model checkpoint at A:/llava-delta were not used when initializing LlamaForCausalLM: ['model.mm_projector.weight', 'model.mm_projector.bias']\r\n- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\nApplying the delta\r\nApplying delta:   0%|                                                                          | 0/403 [00:00<?, ?it/s]\r\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\r\n│ C:\\Users\\Ge Yunxiang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py:196 in                 │\r\n│ _run_module_as_main                                                                              │\r\n│                                                                                                  │\r\n│   193 │   main_globals = sys.modules[\"__main__\"].__dict__                                        │\r\n│   194 │   if alter_argv:                                                                         │\r\n│   195 │   │   sys.argv[0] = mod_spec.origin                                                      │\r\n│ ❱ 196 │   return _run_code(code, main_globals, None,                                             │\r\n│   197 │   │   │   │   │    \"__main__\", mod_spec)                                                 │\r\n│   198                                                                                            │\r\n│   199 def run_module(mod_name, init_globals=None,                                                │\r\n│                                                                                                  │\r\n│ C:\\Users\\Ge Yunxiang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py:86 in _run_code        │\r\n│                                                                                                  │\r\n│    83 │   │   │   │   │      __loader__ = loader,                                                │\r\n│    84 │   │   │   │   │      __package__ = pkg_name,                                             │\r\n│    85 │   │   │   │   │      __spec__ = mod_spec)                                                │\r\n│ ❱  86 │   exec(code, run_globals)                                                                │\r\n│    87 │   return run_globals                                                                     │\r\n│    88                                                                                            │\r\n│    89 def _run_module_code(code, init_globals=None,                                              │\r\n│                                                                                                  │\r\n│ A:\\vicuna-minigpt\\FastChat\\fastchat\\model\\apply_delta.py:165 in <module>                         │\r\n│                                                                                                  │\r\n│   162 │   │   │   args.base_model_path, args.target_model_path, args.delta_path                  │\r\n│   163 │   │   )                                                                                  │\r\n│   164 │   else:                                                                                  │\r\n│ ❱ 165 │   │   apply_delta(args.base_model_path, args.target_model_path, args.delta_path)         │\r\n│   166                                                                                            │\r\n│                                                                                                  │\r\n│ A:\\vicuna-minigpt\\FastChat\\fastchat\\model\\apply_delta.py:140 in apply_delta                      │\r\n│                                                                                                  │\r\n│   137 │   print(\"Applying the delta\")                                                            │\r\n│   138 │   for name, param in tqdm(base.state_dict().items(), desc=\"Applying delta\"):             │\r\n│   139 │   │   assert name in delta.state_dict()                                                  │\r\n│ ❱ 140 │   │   param.data += delta.state_dict()[name]                                             │\r\n│   141 │                                                                                          │\r\n│   142 │   print(f\"Saving the target model to {target_model_path}\")                               │\r\n│   143 │   base.save_pretrained(target_model_path)                                                │\r\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\r\nRuntimeError: The size of tensor a (32000) must match the size of tensor b (32003) at non-singleton dimension 0</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-25T18:36:50Z>\nHi, thank you for your interest in our work!\r\n\r\nLet's consolidate the issue to #55, and please let me know if there are further issues there.  Thank you!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 53,
    "state": "closed",
    "created_by": "HenryHZY",
    "created_at": "2023-04-24T19:31:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/53</URL>\n\n<TITLE>Why not compare with FLAN-T5-Large?</TITLE>\n\n<BODY>What considerations preclude a comparison against the FLAN-T5-Large model? As shown in MM-COT paper (arXiv:2302.00923) Table7, FLAN-T5-Large achieves ~93% on ScienceQA. \r\n\r\nIs it because the experimental settings are different? By the way, FLAN-T5-Large (~783M) has less number of parameters than LLaMA / LLaVA (~13B). \r\n\r\nThank you.</BODY>\n\n<COMMENTS>\n<Comment by ChunyuanLI at 2023-04-25T16:40:04Z>\nThanks for pointing it out. We were looking at their main results in Table 7, which shows the comparison with existing methods. The number 91.68% is emphasized in Table 7 and Abstract, we thought it is the SoTA number. Could you please confirm with the MM-COT authors? Meanwhile, let's assume ~93% is best number on ScienceQA for now. We are confident that LLaVA can outperform it with more tuning, though it is not the main focus on of our project. Thanks.\r\n\r\n![image](https://user-images.githubusercontent.com/8978644/234344107-151ef510-aa07-41b8-b9f1-9ae0d898ca06.png)\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/8978644/234343494-09badec9-da30-4c2d-bea5-4b521ae390b8.png)\n</Comment>\n<Comment by HenryHZY at 2023-04-26T06:48:09Z>\n@ChunyuanLI Thanks for your quick reply. My main concern is not performance, but curiosity about the contribution of instruction tuning for the performance improvement. \r\n\r\nBy the way, one of the conclusions in the paper is interesting, \"We conclude that CoT-like reasoning-ﬁrst strategy can largely improve convergence, but contributes relatively little to the ﬁnal performance.\" Can you provide some intuitive analysis? For example, instruction tuning may replace the role of COT Prompting in some way.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 52,
    "state": "closed",
    "created_by": "Unrealluver",
    "created_at": "2023-04-24T11:04:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/52</URL>\n\n<TITLE>OOM error and fsdp support</TITLE>\n\n<BODY>Thanks for your excellent work! I have tried vicuna-13b on 8xA100(40G), but it will result in an OOM error. To avoid the GPU OOM error, I add the following commands to the pretraining scripts:\r\n```\r\n--fsdp \"full_shard auto_wrap offload\" \\  \r\n--fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\  \r\n--bf16 True \\\r\n```\r\n \r\nI got the error: \r\n\r\n```\r\nValueError: `FlatParameter` requires uniform `requires_grad`\r\n```\r\n\r\nCould you please provide some suggestions?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-25T18:41:55Z>\nHi, thank you for your interest in our work.  I am also working for this to be supported with a wider range of hardware so that more researchers can benefit from our work.\r\n\r\nCurrently PyTorch native support for FSDP on parameter efficient tuning (part of the parameters are frozen) is still under-development.  I am trying two potential solutions: (1) use the PyTorch nightly with some minor modification to the `transformers`; (2) use deepspeed for pretraining.\r\n\r\nI am currently looking into this, and the first direction seems possible.  I will send an update here tonight.\n</Comment>\n<Comment by haotian-liu at 2023-04-26T04:40:56Z>\nHey good news, I have made the FSDP with pretraining work, please follow the instructions [here](https://github.com/haotian-liu/LLaVA#experimental-use-fsdp-to-save-memory-in-pretraining).\r\n\r\nI verified that with the first 200 iterations (25600 samples), the loss curve is almost identical to that I trained without FSDP on 7B model.  Unfortunately, I may not have the free machine to finish the training to make the full validation today.  You may try this out, as it seems no problem to me. It'd be great if you could help try this, and share the results :)\r\n\r\nTo be safe, maybe still switch to our stable branch for finetuning which has the native/stable FSDP support from PyTorch.\n</Comment>\n<Comment by ZhengMengbin at 2023-04-26T13:13:24Z>\n> Hey good news, I have made the FSDP with pretraining work, please follow the instructions [here](https://github.com/haotian-liu/LLaVA#experimental-use-fsdp-to-save-memory-in-pretraining).\r\n> \r\n> I verified that with the first 200 iterations (25600 samples), the loss curve is almost identical to that I trained without FSDP on 7B model. Unfortunately, I may not have the free machine to finish the training to make the full validation today. You may try this out, as it seems no problem to me. It'd be great if you could help try this, and share the results :)\r\n> \r\n> To be safe, maybe still switch to our stable branch for finetuning which has the native/stable FSDP support from PyTorch.\r\n\r\n@haotian-liu I follow the instructions, but get [ValueError: Must flatten tensors with uniform `requires_grad` when `use_orig_params=False`] error\r\n\r\nGPU: 8 * A100 80G, CUDA117\r\nPytorch: torch==2.1.0.dev20230424+cu117\r\nPython: 3.9\r\n\r\n![image](https://user-images.githubusercontent.com/26650043/234586711-8c81c526-6e2a-43da-81f4-0d5a5cbcdf54.png)\r\n![image](https://user-images.githubusercontent.com/26650043/234586848-a9210112-cbb1-4520-831c-60635925ce3c.png)\r\n![image](https://user-images.githubusercontent.com/26650043/234586963-8de769f7-7787-478e-ba02-6d4271daa14b.png)\n</Comment>\n<Comment by scenarios at 2023-04-26T18:25:10Z>\nIMO set the learning rate of all LLaMA-13B parameters to be zero and then make all require_grad=True could be a more reliable solution at this point.\n</Comment>\n<Comment by Unrealluver at 2023-04-27T03:35:39Z>\nWhen I use the scripts like below:\r\n```\r\ntorchrun --nnodes=1 --nproc_per_node=1 --master_port=25301 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path /home/zhulianghui/ProjectC_ChatGPT/alpaca-quan/output/vicuna-13b-composite-3gpt-data \\\r\n    --data_path /home/zhulianghui/ProjectC_ChatGPT/llava/reference/LLaVA-CC3M-Pretrain-595K/chat_new.json \\\r\n    --image_folder /share/project/lianghuizhu/cc3m-llava \\\r\n    --vision_tower /home/zhulianghui/ProjectC_ChatGPT/llava/reference/clip-vit-large-patch14 \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True\\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-13b-pretrain_fsdp \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 2400 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --bf16 True \\\r\n    --fsdp \"full_shard auto_wrap offload\" \\\r\n    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --run_name \"vicuna-13b-clip-l-pretrain\" \\\r\n    --lazy_preprocess True \\\r\n    --report_to mlflow\r\n```\r\n\r\nI got the error message like this:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/zhulianghui/ProjectC_ChatGPT/llava/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/home/zhulianghui/ProjectC_ChatGPT/llava/llava/train/train.py\", line 499, in train\r\n    trainer.train()\r\n  File \"/home/zhulianghui/.conda/envs/llava_beta/lib/python3.10/site-packages/transformers/trainer.py\", line 1645, in train\r\n    return inner_training_loop(\r\n  File \"/home/zhulianghui/.conda/envs/llava_beta/lib/python3.10/site-packages/transformers/trainer.py\", line 1912, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/home/zhulianghui/.conda/envs/llava_beta/lib/python3.10/site-packages/transformers/trainer.py\", line 2658, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/home/zhulianghui/.conda/envs/llava_beta/lib/python3.10/site-packages/transformers/trainer.py\", line 2690, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/home/zhulianghui/.conda/envs/llava_beta/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/zhulianghui/.conda/envs/llava_beta/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 789, in forward\r\n    output = self._fsdp_wrapped_module(*args, **kwargs)\r\n  File \"/home/zhulianghui/.conda/envs/llava_beta/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/zhulianghui/.conda/envs/llava_beta/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 846, in forward\r\n    outputs = self.model(\r\n  File \"/home/zhulianghui/.conda/envs/llava_beta/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/zhulianghui/.conda/envs/llava_beta/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 632, in forward\r\n    raise ValueError(\"Seems that the image is cut.\")\r\nValueError: Seems that the image is cut.\r\n\r\n```\r\nMy libs' versions are shown below:\r\n```\r\ntorch                    2.1.0.dev20230426+cu118\r\ntorchaudio               2.1.0.dev20230426+cu118\r\ntorchvision              0.16.0.dev20230426+cu118\r\n```\r\n\r\nI will appreciate it if you could share some suggestions!\n</Comment>\n<Comment by haotian-liu at 2023-05-01T01:53:06Z>\nHi @Unrealluver Are you still facing this issue?\n</Comment>\n<Comment by haotian-liu at 2023-05-01T01:56:25Z>\n@scenarios Thank you for your suggestion.  I have also tried this, but it does not work out very well on my side.  The parameters are all wrapped with the FSDP, so you still need to do `use_orig_params` hack, and the memory cost and training speed also double.  Do you have any suggestions regarding this?  Thanks.\n</Comment>\n<Comment by Unrealluver at 2023-05-01T14:10:43Z>\n> Hi @Unrealluver Are you still facing this issue?\r\n\r\nThanks, I have found the reason.\n</Comment>\n<Comment by ycchen-tw at 2023-05-13T08:52:30Z>\n@Unrealluver Hello, I'm facing the same issue.\r\n`\r\nValueError: ‘FlatParameter’ requires uniform ‘requires_grad’\r\n`\r\nI noticed that you mentioned that you have already resolved it. Could you please share how you resolved it? Thank you!\n</Comment>\n<Comment by aprilehannibal at 2023-05-16T15:04:50Z>\n> > Hi @Unrealluver Are you still facing this issue?\r\n> \r\n> Thanks, I have found the reason.\r\n I'm facing the same issue. Could you please share how you resolved it? Thank you!\n</Comment>\n<Comment by Kobemaomingyuan at 2024-05-17T09:40:56Z>\n> Hey good news, I have made the FSDP with pretraining work, please follow the instructions [here](https://github.com/haotian-liu/LLaVA#experimental-use-fsdp-to-save-memory-in-pretraining).\r\n> \r\n> I verified that with the first 200 iterations (25600 samples), the loss curve is almost identical to that I trained without FSDP on 7B model. Unfortunately, I may not have the free machine to finish the training to make the full validation today. You may try this out, as it seems no problem to me. It'd be great if you could help try this, and share the results :)\r\n> \r\n> To be safe, maybe still switch to our stable branch for finetuning which has the native/stable FSDP support from PyTorch.\r\n\r\nthanks for the information, i use fsdp in LLava but find it occupies a larger amount of GPU memory than deepspeed. have you met the same problem? Here's my setup:\r\n\"fsdp\": \"full_shard auto_wrap\",\r\n  \"fsdp_transformer_layer_cls_to_wrap\": \"LlamaDecoderLayer\",\r\n  \"fsdp_cpu_ram_efficient_loading\": true,\r\n  \"fsdp_use_orig_params\": true,\r\nflash-attn==2.5.8, transformers==4.37.2\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 51,
    "state": "closed",
    "created_by": "FourthM",
    "created_at": "2023-04-24T08:37:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/51</URL>\n\n<TITLE>there is something wrong with the answer</TITLE>\n\n<BODY>![image](https://user-images.githubusercontent.com/117635816/233943750-4672927d-9846-4243-8078-1885594bd2fb.png)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-25T18:38:57Z>\nHi, thank you for your interest in our work.\r\n\r\nCould you try confirming the weights conversion process following the instruction [here](https://github.com/haotian-liu/LLaVA#llava-weights)?  The common issues are:\r\n(1) you need to convert the weights from delta, due to LLaMA license issue;\r\n(2) the base checkpoint should be llama, instead of vicuna.\r\n\r\nPlease let me know if you have further concerns, thanks!\n</Comment>\n<Comment by haotian-liu at 2023-05-01T04:05:29Z>\nHi, I am closing this issue, due to the inactivity.  Hope your problem has been resolved.  If you have further concerns, please feel free to re-open or open a new issue.  Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 50,
    "state": "closed",
    "created_by": "yuezewang",
    "created_at": "2023-04-24T08:34:29Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/50</URL>\n\n<TITLE>How to get the image according to the 'id' or 'image' from llava_instruct_150k.json?</TITLE>\n\n<BODY></BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 49,
    "state": "closed",
    "created_by": "REIGN12",
    "created_at": "2023-04-24T07:37:10Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/49</URL>\n\n<TITLE>Is the data collection prompt for detailed decription and complex reasoning released?</TITLE>\n\n<BODY>Many thanks for your great work! I am loving the idea of using text-only-gpt-4 to harness img-based multimodal data. And I am checking your paper on Table 10 about the details of conversation data collection prompt, which is really detailed. And I found the detailed description and complex reasoing prompt is not existing in this repo? \r\n![image](https://user-images.githubusercontent.com/67776176/233929769-15221018-716f-4f8e-a3f5-81cd94384f66.png)\r\nMany thanks if you can provide this!!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-25T18:35:28Z>\nHi thank you for your interest in our work!\r\n\r\nWe have released these prompts, and please see description [here](https://github.com/haotian-liu/LLaVA#gpt-4-prompts).\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 48,
    "state": "closed",
    "created_by": "imzhaoruijie",
    "created_at": "2023-04-24T03:20:27Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/48</URL>\n\n<TITLE>Answers of the model are garbled code</TITLE>\n\n<BODY>When I Launched a gradio web server, I could open my browser and chat with a model. However, the answers of the model are garbled code.\r\nHow can I fix this problem? There is no error information.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-24T03:25:23Z>\nHi, thank you for your interest in our work.  To troubleshoot the issue, please check and confirm the following,\r\n\r\n1. Please create converted LLaVA weight from LLaMA checkpoint following the instructions [here](https://github.com/haotian-liu/LLaVA#llava-13b).  Note: our delta weights are relative to **LLaMA** instead of Vicuna.\r\n2. The converted model should contain \"LLaVA\" at least once to load the correct prompts (#46).\r\n3. If the above two does not solve your problem, please share the command, and the log from the worker and gradio (specifically, it will print out a log that includes the prompt and the response, please screenshot or paste that here with ``` code block), as well as the output so that we can better troubleshoot.\r\n\r\nThanks.\n</Comment>\n<Comment by imzhaoruijie at 2023-04-24T06:42:14Z>\nThanks for your response!\r\nI checked the steps but did not solve the garbled code.\r\nAnd here is the specific information:\r\n\r\n1.When running, the model name contains LLaVA.\r\npython3 -m llava.model.apply_delta --base llama-13b/ --target output_LLaVA-13B-v0/ --delta LLaVA-13b-delta-v0/\r\n\r\n2.\r\n<img width=\"1255\" alt=\"image\" src=\"https://user-images.githubusercontent.com/49706142/233916944-744dd8f7-b33e-479e-9d23-c55cea3549a6.png\">\r\n\r\n3.\r\nroot@ctmt2304211537279nb-6cc856d9db-hslcd:/mnt/home/LLaVA-main# python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path output_LLaVA-13B-v0  --multi-modal\r\n2023-04-24 14:39:40 | INFO | model_worker | args: Namespace(controller_address='http://localhost:10000', host='0.0.0.0', keep_aspect_ratio=False, limit_model_concurrency=5, model_name=None, model_path='output_LLaVA-13B-v0', multi_modal=True, no_register=False, num_gpus=1, port=40000, stream_interval=2, worker_address='http://localhost:40000')\r\n2023-04-24 14:39:40 | INFO | model_worker | Loading the model output_LLaVA-13B-v0 on worker 8ca868 ...\r\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\r\n2023-04-24 14:39:51 | INFO | stdout | path_or_repo_id openai/clip-vit-large-patch14\r\n2023-04-24 14:39:51 | INFO | stdout | filename config.json\r\n2023-04-24 14:39:51 | INFO | stdout | subfolder\r\n2023-04-24 14:39:51 | INFO | stdout | cache_dir /root/.cache/huggingface/hub\r\n2023-04-24 14:39:51 | INFO | stdout | local_files_only False\r\n2023-04-24 14:39:51 | INFO | stdout | storage_folder /root/.cache/huggingface/hub/models--openai--clip-vit-large-patch14\r\n2023-04-24 14:39:51 | INFO | stdout | local_dir None\r\n2023-04-24 14:39:51 | INFO | stdout | url_to_download https://huggingface.co/openai/clip-vit-large-patch14/resolve/main/config.json\r\n2023-04-24 14:39:51 | INFO | stdout | filename config.json\r\n2023-04-24 14:39:51 | INFO | stdout | revision main\r\n2023-04-24 14:39:51 | INFO | stdout | cached_file /root/.cache/huggingface/hub/models--openai--clip-vit-large-patch14/clip-vit-large-patch14/config.json\r\n2023-04-24 14:39:51 | INFO | stdout | !!!!!!!!!!!!!!!!!!!!\r\n2023-04-24 14:39:51 | INFO | stdout | resolved_file /root/.cache/huggingface/hub/models--openai--clip-vit-large-patch14/clip-vit-large-patch14/config.json\r\n2023-04-24 14:39:51 | INFO | stdout | path_or_repo_id openai/clip-vit-large-patch14\r\n2023-04-24 14:39:51 | INFO | stdout | filename pytorch_model.bin\r\n2023-04-24 14:39:51 | INFO | stdout | subfolder\r\n2023-04-24 14:39:51 | INFO | stdout | cache_dir /root/.cache/huggingface/hub\r\n2023-04-24 14:39:51 | INFO | stdout | local_files_only False\r\n2023-04-24 14:39:51 | INFO | stdout | storage_folder /root/.cache/huggingface/hub/models--openai--clip-vit-large-patch14\r\n2023-04-24 14:39:51 | INFO | stdout | local_dir None\r\n2023-04-24 14:39:51 | INFO | stdout | url_to_download https://huggingface.co/openai/clip-vit-large-patch14/resolve/main/pytorch_model.bin\r\n2023-04-24 14:39:51 | INFO | stdout | filename pytorch_model.bin\r\n2023-04-24 14:39:51 | INFO | stdout | revision main\r\n2023-04-24 14:39:51 | INFO | stdout | cached_file /root/.cache/huggingface/hub/models--openai--clip-vit-large-patch14/clip-vit-large-patch14/pytorch_model.bin\r\n2023-04-24 14:39:51 | INFO | stdout | !!!!!!!!!!!!!!!!!!!!\r\n2023-04-24 14:39:51 | INFO | stdout | resolved_file /root/.cache/huggingface/hub/models--openai--clip-vit-large-patch14/clip-vit-large-patch14/pytorch_model.bin\r\nSome weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'visual_projection.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_projection.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'logit_scale', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight']\r\n- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\nLoading checkpoint shards:   0%|                                                                                                   | 0/3 [00:00<?, ?it/s]\r\nLoading checkpoint shards:  33%|██████████████████████████████▎                                                            | 1/3 [00:03<00:06,  3.44s/it]\r\nLoading checkpoint shards:  67%|████████████████████████████████████████████████████████████▋                              | 2/3 [00:07<00:03,  3.57s/it]\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:11<00:00,  3.61s/it]\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:11<00:00,  3.67s/it]\r\n2023-04-24 14:40:04 | ERROR | stderr | \r\n2023-04-24 14:40:06 | INFO | stdout | path_or_repo_id openai/clip-vit-large-patch14\r\n2023-04-24 14:40:06 | INFO | stdout | filename preprocessor_config.json\r\n2023-04-24 14:40:06 | INFO | stdout | subfolder\r\n2023-04-24 14:40:06 | INFO | stdout | cache_dir /root/.cache/huggingface/hub\r\n2023-04-24 14:40:06 | INFO | stdout | local_files_only False\r\n2023-04-24 14:40:06 | INFO | stdout | storage_folder /root/.cache/huggingface/hub/models--openai--clip-vit-large-patch14\r\n2023-04-24 14:40:06 | INFO | stdout | local_dir None\r\n2023-04-24 14:40:06 | INFO | stdout | url_to_download https://huggingface.co/openai/clip-vit-large-patch14/resolve/main/preprocessor_config.json\r\n2023-04-24 14:40:06 | INFO | stdout | filename preprocessor_config.json\r\n2023-04-24 14:40:06 | INFO | stdout | revision main\r\n2023-04-24 14:40:06 | INFO | stdout | cached_file /root/.cache/huggingface/hub/models--openai--clip-vit-large-patch14/clip-vit-large-patch14/preprocessor_config.json\r\n2023-04-24 14:40:06 | INFO | stdout | !!!!!!!!!!!!!!!!!!!!\r\n2023-04-24 14:40:06 | INFO | stdout | resolved_file /root/.cache/huggingface/hub/models--openai--clip-vit-large-patch14/clip-vit-large-patch14/preprocessor_config.json\r\n2023-04-24 14:40:14 | INFO | model_worker | Register to controller\r\n2023-04-24 14:40:14 | ERROR | stderr | INFO:     Started server process [58522]\r\n2023-04-24 14:40:14 | INFO | uvicorn.error | Started server process [58522]\r\n2023-04-24 14:40:14 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2023-04-24 14:40:14 | INFO | uvicorn.error | Waiting for application startup.\r\n2023-04-24 14:40:14 | ERROR | stderr | INFO:     Application startup complete.\r\n2023-04-24 14:40:14 | INFO | uvicorn.error | Application startup complete.\r\n2023-04-24 14:40:14 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)\r\n2023-04-24 14:40:14 | INFO | uvicorn.error | Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)\r\n2023-04-24 14:40:29 | INFO | model_worker | Send heart beat. Models: ['output_LLaVA-13B-v0']. Semaphore: None. global_counter: 0\r\n2023-04-24 14:40:44 | INFO | model_worker | Send heart beat. Models: ['output_LLaVA-13B-v0']. Semaphore: None. global_counter: 0\r\n2023-04-24 14:40:59 | INFO | model_worker | Send heart beat. Models: ['output_LLaVA-13B-v0']. Semaphore: None. global_counter: 0\r\n2023-04-24 14:41:14 | INFO | model_worker | Send heart beat. Models: ['output_LLaVA-13B-v0']. Semaphore: None. global_counter: 0\r\n2023-04-24 14:41:29 | INFO | model_worker | Send heart beat. Models: ['output_LLaVA-13B-v0']. Semaphore: None. global_counter: 0\r\n2023-04-24 14:41:44 | INFO | model_worker | Send heart beat. Models: ['output_LLaVA-13B-v0']. Semaphore: None. global_counter: 0\r\n2023-04-24 14:41:59 | INFO | model_worker | Send heart beat. Models: ['output_LLaVA-13B-v0']. Semaphore: None. global_counter: 0\r\n2023-04-24 14:42:14 | INFO | model_worker | Send heart beat. Models: ['output_LLaVA-13B-v0']. Semaphore: None. global_counter: 0\r\n2023-04-24 14:42:29 | INFO | model_worker | Send heart beat. Models: ['output_LLaVA-13B-v0']. Semaphore: None. global_counter: 0\r\n2023-04-24 14:42:44 | INFO | model_worker | Send heart beat. Models: ['output_LLaVA-13B-v0']. Semaphore: None. global_counter: 0\r\n\r\n4.\r\npython -m llava.serve.test_message --model-name output_LLaVA-13B-v0 --controller http://localhost:10000\r\nModels: ['output_LLaVA-13B-v0']\r\nworker_addr: http://localhost:40000\r\nA chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\r\nHuman: What are the key differences between renewable and non-renewable energy sources?\r\nAssistant: Renewable energy sources are those that can be replenished naturally in a relatively short amount of time, such as solar, wind, hydro, geothermal, and biomass. Non-renewable energy sources, on the other hand, are finite and will eventually be depleted, such as coal, oil, and natural gas. Here are some key differences between renewable and non-renewable energy sources:\r\n1. Availability: Renewable energy sources are virtually inexhaustible, while non-renewable energy sources are finite and will eventually run out.\r\n2. Environmental impact: Renewable energy sources have a much lower environmental impact than non-renewable sources, which can lead to air and water pollution, greenhouse gas emissions, and other negative effects.\r\n3. Cost: Renewable energy sources can be more expensive to initially set up, but they typically have lower operational costs than non-renewable sources.\r\n4. Reliability: Renewable energy sources are often more reliable and can be used in more remote locations than non-renewable sources.\r\n5. Flexibility: Renewable energy sources are often more flexible and can be adapted to different situations and needs, while non-renewable sources are more rigid and inflexible.\r\n6. Sustainability: Renewable energy sources are more sustainable over the long term, while non-renewable sources are not, and their depletion can lead to economic and social instability.\r\n\r\nHuman: Tell me a story with more than 1000 words.\r\n weator що pu DISachsubsection mil Deнародten bound File filenameoolsPOST >>>ategory;; workcontext Offramço resetXT Digsert Palww agreSBN\r\n<img width=\"1257\" alt=\"屏幕截图 2023-04-24 143912\" src=\"https://user-images.githubusercontent.com/49706142/233918710-ecedba3d-4578-4e25-9a78-db7d8bf216f4.png\">\n</Comment>\n<Comment by haotian-liu at 2023-04-24T07:00:30Z>\nHi, the output of the test message is indeed unexpected.  But the steps you did on weight conversion, and launching the workers all seem correct to me.\r\n\r\nI noticed that the command line begins with `root@XXXXX`.  Is this a docker environment?  In other words, is it a clean install?  In current research preview, we use a modified version of transformers.  Using clean install resolves most of the version-related issues. https://github.com/haotian-liu/LLaVA#install\r\n\r\nAlso, would you mind sharing the gradio logs as well, as it provides slightly more information: upload an image, and ask it to describe the details in the image.\r\n\r\nThanks.\n</Comment>\n<Comment by imzhaoruijie at 2023-04-24T07:05:05Z>\n![8cca3540e653b7299a3e9ef540fd2bb](https://user-images.githubusercontent.com/49706142/233923015-746201b4-1297-4ed5-a6e0-72f722291f59.jpg)\r\n![2d7fc570c69fb971861f5fe6e9e1882](https://user-images.githubusercontent.com/49706142/233923037-65074641-8072-436d-836b-aaccad3c2b00.jpg)\n</Comment>\n<Comment by haotian-liu at 2023-04-24T07:35:45Z>\nHi, these prompts all look correct to me. Unfortunately, I am unsure about what the issue is. I wonder if you directly load your llama model, is it working correctly?  Or vicuna?\r\n\r\nAnd is it an env installed from scratch?  i.e., are the tokenizer versions the same as the default?\n</Comment>\n<Comment by imzhaoruijie at 2023-04-24T09:05:01Z>\nThanks for your patience!\r\nI load the model which is genereted by apply_delta.\r\nI found I did not install tokenizer. Then I installed default tokenizer and version is 3.4.1, but  the output of the test message is still unexpected.\r\n\r\nI skip the step of \" pip install -e .\" The cli reminds me \"consider using a build backend  support pep 660 support\".\r\n\r\nI will try to install an env  from scratch and see the results.\r\n\r\nThanks again!\n</Comment>\n<Comment by haotian-liu at 2023-04-25T18:43:10Z>\nHi @imzhaoruijie maybe you would also need this `pip install --upgrade pip  # enable PEP 660 support`\r\n\r\nPlease let me know if there are any updates after installing the environment from scratch, and if there are further updates.  Thanks!\n</Comment>\n<Comment by yuwfan at 2023-04-25T18:51:20Z>\nI also have the same issue. I can confirm that I apply delta to the original llama 13b checkpoints and use llava models. Could you help to share the md5sum of your LLaVA model? Thanks! @haotian-liu\n</Comment>\n<Comment by haotian-liu at 2023-04-25T18:56:21Z>\nHi @yuwfan, of course!  Below is the `md5sum` for the converted LLaVA model.  Please let me know if there are any disparities.  Besides, would you mind sharing if you are building the environment from scratch based on the latest instructions?  Let me know if any further concerns.  Thanks.\r\n\r\n```\r\n95e1b235bf9921a5f1e1f387d87aa46e  ./added_tokens.json\r\n395bf409a41a30abdbf0f8f611eec042  ./pytorch_model.bin.index.json\r\ndc67bae5a78e59bafe37782e40763e7f  ./special_tokens_map.json\r\na05ef2e4e89a6cc411f6f46b5621ad1a  ./tokenizer_config.json\r\nf64a72a6ddd8f35635606b2863fd6931  ./generation_config.json\r\naa21a374a965a0f6118355f901f47b94  ./pytorch_model-00003-of-00003.bin\r\neeec4125e9c7560836b4873b6f8e3025  ./tokenizer.model\r\n9585dc8282cf1ea48c2ae2e07471db0c  ./pytorch_model-00001-of-00003.bin\r\n481795ef4fcd5994bc1d4aa200b90abe  ./config.json\r\n96e55e68ada8315601ee226633074398  ./pytorch_model-00002-of-00003.bin\r\n```\n</Comment>\n<Comment by yuwfan at 2023-04-25T19:21:21Z>\nThank you for the response! Yes, I built the environment from scratch following the latest instructions. Here are my md5sum for all the files under LLaVA-13B-v0. Can you share your files except for model files? Thanks!\r\n\r\n```bash\r\n95e1b235bf9921a5f1e1f387d87aa46e  added_tokens.json\r\n0e42b09768749913cf937dad7c29203c  config.json\r\nccdf396b2fd194d3fb39f29acceb2624  generation_config.json\r\n9585dc8282cf1ea48c2ae2e07471db0c  pytorch_model-00001-of-00003.bin\r\n96e55e68ada8315601ee226633074398  pytorch_model-00002-of-00003.bin\r\n70c0eeec77635d9b51b8f42360fe57d5  pytorch_model-00003-of-00003.bin\r\nf8397131c74cc98e4b7cb9e609cf9fbc  pytorch_model.bin.index.json\r\ndc67bae5a78e59bafe37782e40763e7f  special_tokens_map.json\r\n7f249d120639c5c13d6d8c331bee38c9  tokenizer_config.json\r\nba7ce161fad40ac607fa4f29c1b98498  tokenizer.json\r\neeec4125e9c7560836b4873b6f8e3025  tokenizer.model\r\n```\n</Comment>\n<Comment by haotian-liu at 2023-04-25T21:02:12Z>\nHi @yuwfan I tried to do a conversion from scratch, and verified that the model is working correctly.  Some hash changes, and there are more matches between that and yours:\r\n\r\n```\r\n95e1b235bf9921a5f1e1f387d87aa46e  added_tokens.json\r\nc692514d88900bdf7d710c3353bbc4e2  config.json\r\nf64a72a6ddd8f35635606b2863fd6931  generation_config.json\r\n9585dc8282cf1ea48c2ae2e07471db0c  pytorch_model-00001-of-00003.bin\r\n96e55e68ada8315601ee226633074398  pytorch_model-00002-of-00003.bin\r\naa21a374a965a0f6118355f901f47b94  pytorch_model-00003-of-00003.bin\r\n395bf409a41a30abdbf0f8f611eec042  pytorch_model.bin.index.json\r\ndc67bae5a78e59bafe37782e40763e7f  special_tokens_map.json\r\n2dfcf86dfa7b4e91b87ffa05faf044ff  tokenizer_config.json\r\neeec4125e9c7560836b4873b6f8e3025  tokenizer.model\r\n```\r\n\r\nIt seems that the main differences are in these files, and I do not see `tokenizer.json` in my folder. Unfortunately I am unable to share `pytorch_model-00003-of-00003.bin` due to the LLaMA license.\r\n\r\n```\r\nc692514d88900bdf7d710c3353bbc4e2  config.json\r\nf64a72a6ddd8f35635606b2863fd6931  generation_config.json\r\naa21a374a965a0f6118355f901f47b94  pytorch_model-00003-of-00003.bin\r\n395bf409a41a30abdbf0f8f611eec042  pytorch_model.bin.index.json\r\n2dfcf86dfa7b4e91b87ffa05faf044ff  tokenizer_config.json\r\n```\r\n\r\nI guess we may compare `pytorch_model.bin.index.json` to see what are the differences, and other files as well of course.\r\n\r\n[diff_cfgs.zip](https://github.com/haotian-liu/LLaVA/files/11326853/diff_cfgs.zip)\r\n\r\nPlease let me know if you find anything potentially be the cause, or you've figured it out.  Also let me know if there are anything else I could do to help.  Thanks.\n</Comment>\n<Comment by yuwfan at 2023-04-27T20:22:51Z>\n@haotian-liu Thank you for the details. I compared `pytorch_model.bin.index.json` and didn't find the projector parameters in the model. Then I added the projector weight to the model and fixed the issue. Thank you for the help!\r\n\r\n```bash\r\n\"model.mm_projector.bias\": \"pytorch_model-00003-of-00003.bin\",\r\n\"model.mm_projector.weight\": \"pytorch_model-00003-of-00003.bin\",\r\n```\n</Comment>\n<Comment by haotian-liu at 2023-05-01T04:05:12Z>\n@yuwfan Glad to hear that the issue has been resolved! I am closing this issue now.  If you have further concerns, please feel free to re-open or open a new issue.  Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 47,
    "state": "closed",
    "created_by": "AreChen",
    "created_at": "2023-04-23T10:42:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/47</URL>\n\n<TITLE>Can LLaVA-13b-delta-v0 be split into 900M parts?</TITLE>\n\n<BODY>I have 4 x 3080ti. According to the current bin file size（9G-9G-6G）, it is impossible to load them into multiple graphics cards. If the model is segmented into 900M, it will provide more possibilities for running on multiple graphics cards!\r\nOr is there a way for me to split it myself?\r\n\r\n[Like this llama-13b-hf, the model has been segmented into 41 parts, each of which is 900M.](https://huggingface.co/decapoda-research/llama-13b-hf/tree/main)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-24T07:11:29Z>\nHi, thank you for your interest in our work.\r\n\r\nThis may have already been supported even with the current weight splits.  I tried launch a worker with `multiple-gpus` parameter set to 4, and it seems to scatter the model weights over four GPUs. Can you try it following the instructions here: https://github.com/haotian-liu/LLaVA#launch-a-model-worker-multiple-gpus-when-gpu-vram--24gb\n</Comment>\n<Comment by AreChen at 2023-04-24T07:13:23Z>\nOkay, I will try it later. Thank you!\n</Comment>\n<Comment by haotian-liu at 2023-05-01T04:04:38Z>\nHi, I am closing this issue, due to the inactivity.  Hope your problem has been resolved.  If you have further concerns, please feel free to re-open or open a new issue.  Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 46,
    "state": "closed",
    "created_by": "TheShy-Dream",
    "created_at": "2023-04-23T09:15:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/46</URL>\n\n<TITLE>For scientific research purposes</TITLE>\n\n<BODY>Can you open the API or provide weights after integration? Thank you very much!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-23T22:59:04Z>\nHi, thank you for your interest in our work.\r\n\r\nI am a bit unsure what is requested.\r\n\r\nThe weights are released [here](https://github.com/haotian-liu/LLaVA#llava-weights).\r\n\r\nRegarding API, can you clarify if it is the CLI interface, or a hosted API, that has the same functionality as our [demo](https://llava.hliu.cc/)?\n</Comment>\n<Comment by TheShy-Dream at 2023-04-24T01:13:15Z>\nThank you for your answer. I will follow your steps to build a webui and use it to caption image。But the effect is not ideal, as shown in the following picture。He always answers questions related to energy。\r\n![image](https://user-images.githubusercontent.com/58178926/233878158-5160944b-3e36-400e-9a34-31ed2a3d4920.png)\r\nI found that there is an energy description in the prompt, which is considered to be caused by it. This is the output of the command line. How to solve this problem? I want to run it like your demo\r\n![image](https://user-images.githubusercontent.com/58178926/233878268-18da4035-7034-4161-9f0f-d5b412dd3372.png)\r\nAdditionally, could you please create a hosted API as I may encounter the aforementioned issues if I build my own cli\n</Comment>\n<Comment by haotian-liu at 2023-04-24T01:24:16Z>\nHi it seems that the model is not loaded with the correct prompt template, please make sure that you include `--multi-modal` when running, and the model name contains `LLaVA`.\r\n\r\n```\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./checkpoints/LLaVA-13B-v0 --multi-modal\r\n```\r\n\r\nA hosted API is WIP, and it will be released soon (expect like by the end of this week).\n</Comment>\n<Comment by TheShy-Dream at 2023-04-24T01:55:24Z>\nIf only the webui is needed without fine-tuning, is it only necessary to integrate the original llama and delta llama? I followed the steps and found that it still cannot be achieved\r\n./target storing the model after weight fusion\r\nHere are some prompt outputs during my execution process\r\n![image](https://user-images.githubusercontent.com/58178926/233881847-b8a5de23-6eee-4632-8988-ed82233fb033.png)\r\n![image](https://user-images.githubusercontent.com/58178926/233881938-437fb8c0-6155-4a83-9a8c-5fdaf2a348d9.png)\r\n![image](https://user-images.githubusercontent.com/58178926/233882020-1f44c362-9f8f-4efc-abac-0b7c28b507d9.png)\n</Comment>\n<Comment by haotian-liu at 2023-04-24T02:06:07Z>\nHi, please rename the folder `target` to `LLaVA-13B-v0` so that it knows that it is a LLaVA model.\n</Comment>\n<Comment by TheShy-Dream at 2023-04-24T02:15:21Z>\nThank you for effectively solving a problem that has been bothering me all afternoon haha,got it,salute!!This model is too powerful!!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 40,
    "state": "closed",
    "created_by": "Wojtab",
    "created_at": "2023-04-23T03:24:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/40</URL>\n\n<TITLE>I made it work on a single 3090</TITLE>\n\n<BODY>There is no discussion tab, so opening it as an issue. \r\nI made it work on a single 3090, in ooba's webui, see this PR for more info: https://github.com/oobabooga/text-generation-webui/pull/1487.\r\nThere is even a small possibility that it will run on 12GB GPUs, as 4-bit vicuna 13b fits, question is if it fits with CLIP+projector</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 38,
    "state": "closed",
    "created_by": "PhanTask",
    "created_at": "2023-04-22T22:35:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/38</URL>\n\n<TITLE>offine env - error while attempting to bind on address: cannot assign requested address</TITLE>\n\n<BODY>Hi, I am trying to serve the model in an offline env and I have finished downloading all the weights (including clip) and finished weight conversion.\r\n\r\nWhen launching the controller do not encounter any issue.\r\n\r\n```\r\n2023-04-22 22:25:46 | INFO | controller | args: Namespace(host='0.0.0.0', port=10000, dispatch_method='shortest_queue')\r\n2023-04-22 22:25:46 | INFO | controller | Init controller\r\n2023-04-22 22:25:46 | ERROR | stderr | INFO:     Started server process [20039]\r\n2023-04-22 22:25:46 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2023-04-22 22:25:46 | ERROR | stderr | INFO:     Application startup complete.\r\n2023-04-22 22:25:46 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:10000 (Press CTRL+C to quit)\r\n```\r\n\r\nWhen launching the model worker, I encountered an address assignment issue:\r\n\r\n```\r\n2023-04-22 22:29:36 | INFO | model_worker | Register to controller\r\n2023-04-22 22:29:36 | ERROR | stderr | INFO:     Started server process [20125]\r\n2023-04-22 22:29:36 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2023-04-22 22:29:36 | ERROR | stderr | INFO:     Application startup complete.\r\n2023-04-22 22:29:36 | ERROR | stderr | ERROR:    [Errno 99] error while attempting to bind on address ('::1', 40000, 0, 0): cannot assign requested address\r\n2023-04-22 22:29:36 | ERROR | stderr | INFO:     Waiting for application shutdown.\r\n2023-04-22 22:29:36 | ERROR | stderr | INFO:     Application shutdown complete.\r\n2023-04-22 22:29:51 | INFO | model_worker | Send heart beat. Models: ['LLaVA-13B-v0']. Semaphore: None. global_counter: 0\r\n2023-04-22 22:30:06 | INFO | model_worker | Send heart beat. Models: ['LLaVA-13B-v0']. Semaphore: None. global_counter: 0\r\n2023-04-22 22:30:21 | INFO | model_worker | Send heart beat. Models: ['LLaVA-13B-v0']. Semaphore: None. global_counter: 0\r\n2023-04-22 22:30:36 | INFO | model_worker | Send heart beat. Models: ['LLaVA-13B-v0']. Semaphore: None. global_counter: 0\r\n2023-04-22 22:30:51 | INFO | model_worker | Send heart beat. Models: ['LLaVA-13B-v0']. Semaphore: None. global_counter: 0\r\n2023-04-22 22:31:06 | INFO | model_worker | Send heart beat. Models: ['LLaVA-13B-v0']. Semaphore: None. global_counter: 0\r\n2023-04-22 22:31:21 | INFO | model_worker | Send heart beat. Models: ['LLaVA-13B-v0']. Semaphore: None. global_counter: 0\r\n```\r\n\r\nAnd controller's side further shows:\r\n\r\n```\r\n2023-04-22 22:29:36 | INFO | controller | Register a new worker: http://localhost:40000\r\n2023-04-22 22:29:36 | INFO | controller | Register done: http://localhost:40000, {'model_names': ['LLaVA-13B-v0'], 'speed': 1, 'queue_length': 0}\r\n2023-04-22 22:29:36 | INFO | stdout | INFO:     127.0.0.1:49804 - \"POST /register_worker HTTP/1.1\" 200 OK\r\n2023-04-22 22:29:51 | INFO | controller | Receive heart beat. http://localhost:40000\r\n2023-04-22 22:29:51 | INFO | stdout | INFO:     127.0.0.1:52354 - \"POST /receive_heart_beat HTTP/1.1\" 200 OK\r\n2023-04-22 22:30:06 | INFO | controller | Receive heart beat. http://localhost:40000\r\n2023-04-22 22:30:06 | INFO | stdout | INFO:     127.0.0.1:46308 - \"POST /receive_heart_beat HTTP/1.1\" 200 OK\r\n2023-04-22 22:30:21 | INFO | controller | Receive heart beat. http://localhost:40000\r\n```\r\n\r\nDo you have any clues about how to solve this issue? Thanks!</BODY>\n\n<COMMENTS>\n<Comment by PhanTask at 2023-04-22T22:43:25Z>\nI figured it out. Setting `--host` to `0.0.0.0` when launching model worker fixed the issue.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 37,
    "state": "closed",
    "created_by": "haorunze",
    "created_at": "2023-04-22T03:34:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/37</URL>\n\n<TITLE>AttributeError: 'LlamaConfig' object has no attribute 'mm_vision_tower'</TITLE>\n\n<BODY>log：\r\n\r\n(llava) D:\\LLaVA>python -m llava.serve.model_worker --controller http://localhost:10000 --port 40000 --worker http://loc\r\nalhost:40000 --model-path ./checkpoints/LLaVA-13B-v0 --multi-modal\r\n2023-04-22 11:15:42 | INFO | model_worker | args: Namespace(host='localhost', port=40000, worker_address='http://localho\r\nst:40000', controller_address='http://localhost:10000', model_path='./checkpoints/LLaVA-13B-v0', model_name=None, multi_\r\nmodal=True, keep_aspect_ratio=False, num_gpus=1, limit_model_concurrency=5, stream_interval=2, no_register=False)\r\n2023-04-22 11:15:42 | INFO | model_worker | Loading the model LLaVA-13B-v0 on worker 56bbc2 ...\r\nLoading checkpoint shards:   0%|                                                                 | 0/3 [00:00<?, ?it/s]\r\nLoading checkpoint shards:  33%|███████████████████                                      | 1/3 [00:02\r\n<00:05,  2.57s/it]\r\nLoading checkpoint shards:  67%|██████████████████████████████████████\r\n       | 2/3 [00:05<00:02,  2.56s/it]\r\nLoading checkpoint shards: 100%|████████████████████████████████████████████\r\n█████████████| 3/3 [00:06<00:00,  2.17s/it]\r\nLoading checkpoint shards: 100%|████████████████████████████████████████████\r\n█████████████| 3/3 [00:06<00:00,  2.28s/it]\r\n2023-04-22 11:15:49 | ERROR | stderr |\r\n2023-04-22 11:15:49 | ERROR | stderr | Traceback (most recent call last):\r\n2023-04-22 11:15:49 | ERROR | stderr |   File \"C:\\Users\\xht\\anaconda3\\envs\\llava\\lib\\runpy.py\", line 196, in _run_module\r\n_as_main\r\n2023-04-22 11:15:49 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n2023-04-22 11:15:49 | ERROR | stderr |   File \"C:\\Users\\xht\\anaconda3\\envs\\llava\\lib\\runpy.py\", line 86, in _run_code\r\n2023-04-22 11:15:49 | ERROR | stderr |     exec(code, run_globals)\r\n2023-04-22 11:15:49 | ERROR | stderr |   File \"D:\\LLaVA\\llava\\serve\\model_worker.py\", line 361, in <module>\r\n2023-04-22 11:15:49 | ERROR | stderr |     worker = ModelWorker(args.controller_address,\r\n2023-04-22 11:15:49 | ERROR | stderr |   File \"D:\\LLaVA\\llava\\serve\\model_worker.py\", line 118, in __init__\r\n2023-04-22 11:15:49 | ERROR | stderr |     self.tokenizer, self.model, self.image_processor, self.context_len = load_mod\r\nel(\r\n2023-04-22 11:15:49 | ERROR | stderr |   File \"D:\\LLaVA\\llava\\serve\\model_worker.py\", line 65, in load_model\r\n2023-04-22 11:15:49 | ERROR | stderr |     image_processor = CLIPImageProcessor.from_pretrained(model.config.mm_vision_t\r\nower, torch_dtype=torch.float16)\r\n2023-04-22 11:15:49 | ERROR | stderr |   File \"D:\\LLaVA\\transformers\\src\\transformers\\configuration_utils.py\", line 260,\r\n in __getattribute__\r\n2023-04-22 11:15:49 | ERROR | stderr |     return super().__getattribute__(key)\r\n2023-04-22 11:15:49 | ERROR | stderr | AttributeError: 'LlamaConfig' object has no attribute 'mm_vision_tower'</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-22T03:37:25Z>\nHi, thank you for the interest in our work.\r\n\r\nCan you try with the updated instructions: first create a new Conda environment, and then do the install? This has worked for others in #15,#23. Thanks!\r\n\r\nhttps://github.com/haotian-liu/LLaVA#install\n</Comment>\n<Comment by haorunze at 2023-04-22T05:51:55Z>\n> \r\nThank you for reply\r\nI have tried these commands from https://github.com/haotian-liu/LLaVA/issues/15:\r\npip install git+https://github.com/haotian-liu/transformers_llava.git@26356f0d07bacfb3857dafc7f8a519304b4c0572\r\ngit clone https://github.com/haotian-liu/transformers_llava.git transformers\r\npip install -e ./transformers\r\n\r\nIt doesn't work.\r\nDo I need other operation?\r\n\r\nfirst create a new Conda environment, and then do the install? What is that mean?Do I need reinstall LLaVa ？\n</Comment>\n<Comment by haotian-liu at 2023-04-22T06:29:06Z>\nI think I misread the error log.  `AttributeError: 'LlamaConfig' object has no attribute 'mm_vision_tower'`. This suggest that the model config may not be correct.\r\n\r\nCan you check the `config.json` under `./checkpoints/LLaVA-13B-v0`, and see if you see `\"mm_vision_tower\": \"openai/clip-vit-large-patch14\"`?  If not, there may be something wrong happened during the weight conversion process.  Please try follow the instructions here and do the weight conversion: https://github.com/haotian-liu/LLaVA#llava-weights\r\n\r\n> first create a new Conda environment, and then do the install? What is that mean?Do I need reinstall LLaVa ？\r\nThis is to try avoid any potential issues on the incompatibility with the package versions, so I would suggest do these four commands, to ensure a clean install.\r\n\r\n```\r\nconda create -n llava python=3.10 -y\r\nconda activate llava\r\npip install --upgrade pip  # enable PEP 660 support\r\npip install -e .\r\n```\n</Comment>\n<Comment by haotian-liu at 2023-05-01T04:04:28Z>\nHi, I am closing this issue, due to the inactivity.  Hope your problem has been resolved.  If you have further concerns, please feel free to re-open or open a new issue.  Thanks!\n</Comment>\n<Comment by prashant-dn at 2025-02-05T10:03:12Z>\nIn my case, the 'flash attention' lib mismatched with transformers. This is how I fixed it.\n\nUse container `docker pull huggingface/transformers-pytorch-gpu:latest` with local CUDA 12.1+.\n\nInside, just install 'flash attention', if needed, using `pip install flash-attn --no-build-isolation`.\n\nWorks like a charm!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 36,
    "state": "closed",
    "created_by": "Richar-Du",
    "created_at": "2023-04-22T01:50:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/36</URL>\n\n<TITLE>Error when install flash-attn</TITLE>\n\n<BODY>When I run `pip intall flash-attn`, it raises an error:\r\n```ERROR: Could not build wheels for flash-attn, which is required to install pyproject.toml-based projects```\r\n\r\nHowever, I have run `pip install -e .` and successfully installed llava. Do you know how to solve this problem?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-22T01:55:12Z>\nHi @Richar-Du, thank you for your interest in our work.\r\n\r\n`flash-attn` is not required for running the inference of LLaVA, so that error message you were seeing regarding flash-attn should be not related to LLaVA at all (including the `pyproject.toml` part).\r\n\r\nCan you provide: (1) the full error log, and wrap with ``` as well; (2) your system environment, including the OS, CUDA version, and GPU type?\n</Comment>\n<Comment by brightdeng at 2023-04-23T03:29:58Z>\nThe following is my full error log \r\n\r\n(llava) E:\\LLaVA>pip install flash-attn\r\nCollecting flash-attn\r\n  Using cached flash_attn-1.0.3.post0.tar.gz (2.0 MB)\r\n  Preparing metadata (setup.py) ... done\r\nRequirement already satisfied: torch in c:\\users\\admin\\.conda\\envs\\llava\\lib\\site-packages (from flash-attn) (2.0.0)\r\nCollecting einops (from flash-attn)\r\n  Using cached einops-0.6.1-py3-none-any.whl (42 kB)\r\nRequirement already satisfied: packaging in c:\\users\\admin\\.conda\\envs\\llava\\lib\\site-packages (from flash-attn) (23.1)\r\nRequirement already satisfied: filelock in c:\\users\\admin\\.conda\\envs\\llava\\lib\\site-packages (from torch->flash-attn) (3.12.0)\r\nRequirement already satisfied: typing-extensions in c:\\users\\admin\\.conda\\envs\\llava\\lib\\site-packages (from torch->flash-attn) (4.5.0)\r\nRequirement already satisfied: sympy in c:\\users\\admin\\.conda\\envs\\llava\\lib\\site-packages (from torch->flash-attn) (1.11.1)\r\nRequirement already satisfied: networkx in c:\\users\\admin\\.conda\\envs\\llava\\lib\\site-packages (from torch->flash-attn) (3.1)\r\nRequirement already satisfied: jinja2 in c:\\users\\admin\\.conda\\envs\\llava\\lib\\site-packages (from torch->flash-attn) (3.1.2)\r\nRequirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\.conda\\envs\\llava\\lib\\site-packages (from jinja2->torch->flash-attn) (2.1.2)\r\nRequirement already satisfied: mpmath>=0.19 in c:\\users\\admin\\.conda\\envs\\llava\\lib\\site-packages (from sympy->torch->flash-attn) (1.3.0)\r\nBuilding wheels for collected packages: flash-attn\r\n  Building wheel for flash-attn (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n\r\n  × python setup.py bdist_wheel did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> [127 lines of output]\r\n      No CUDA runtime is found, using CUDA_HOME='C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1'\r\n\r\n      Warning: Torch did not find available GPUs on this system.\r\n       If your intention is to cross-compile, this is not an error.\r\n      By default, Apex will cross-compile for Pascal (compute capabilities 6.0, 6.1, 6.2),\r\n      Volta (compute capability 7.0), Turing (compute capability 7.5),\r\n      and, if the CUDA version is >= 11.0, Ampere (compute capability 8.0).\r\n      If you wish to cross-compile for a single specific architecture,\r\n      export TORCH_CUDA_ARCH_LIST=\"compute capability\" before running setup.py.\r\n\r\n\r\n\r\n      torch.__version__  = 2.0.0+cpu\r\n\r\n\r\n      fatal: not a git repository (or any of the parent directories): .git\r\n      running bdist_wheel\r\n      running build\r\n      running build_py\r\n      creating build\r\n      creating build\\lib.win-amd64-cpython-310\r\n      creating build\\lib.win-amd64-cpython-310\\flash_attn\r\n      copying flash_attn\\attention_kernl.py -> build\\lib.win-amd64-cpython-310\\flash_attn\r\n      copying flash_attn\\bert_padding.py -> build\\lib.win-amd64-cpython-310\\flash_attn\r\n      copying flash_attn\\flash_attention.py -> build\\lib.win-amd64-cpython-310\\flash_attn\r\n      copying flash_attn\\flash_attn_interface.py -> build\\lib.win-amd64-cpython-310\\flash_attn\r\n      copying flash_attn\\flash_attn_triton.py -> build\\lib.win-amd64-cpython-310\\flash_attn\r\n      copying flash_attn\\flash_attn_triton_og.py -> build\\lib.win-amd64-cpython-310\\flash_attn\r\n      copying flash_attn\\flash_attn_triton_single_query.py -> build\\lib.win-amd64-cpython-310\\flash_attn\r\n      copying flash_attn\\flash_attn_triton_tmp.py -> build\\lib.win-amd64-cpython-310\\flash_attn\r\n      copying flash_attn\\flash_attn_triton_tmp_og.py -> build\\lib.win-amd64-cpython-310\\flash_attn\r\n      copying flash_attn\\flash_attn_triton_varlen.py -> build\\lib.win-amd64-cpython-310\\flash_attn\r\n      copying flash_attn\\flash_blocksparse_attention.py -> build\\lib.win-amd64-cpython-310\\flash_attn\r\n      copying flash_attn\\flash_blocksparse_attn_interface.py -> build\\lib.win-amd64-cpython-310\\flash_attn\r\n      copying flash_attn\\fused_softmax.py -> build\\lib.win-amd64-cpython-310\\flash_attn\r\n      copying flash_attn\\rotary.py -> build\\lib.win-amd64-cpython-310\\flash_attn\r\n      copying flash_attn\\__init__.py -> build\\lib.win-amd64-cpython-310\\flash_attn\r\n      creating build\\lib.win-amd64-cpython-310\\flash_attn\\layers\r\n      copying flash_attn\\layers\\patch_embed.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\layers\r\n      copying flash_attn\\layers\\rotary.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\layers\r\n      copying flash_attn\\layers\\__init__.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\layers\r\n      creating build\\lib.win-amd64-cpython-310\\flash_attn\\losses\r\n      copying flash_attn\\losses\\cross_entropy.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\losses\r\n      copying flash_attn\\losses\\cross_entropy_apex.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\losses\r\n      copying flash_attn\\losses\\cross_entropy_parallel.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\losses\r\n      copying flash_attn\\losses\\__init__.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\losses\r\n      creating build\\lib.win-amd64-cpython-310\\flash_attn\\models\r\n      copying flash_attn\\models\\bert.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\models\r\n      copying flash_attn\\models\\gpt.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\models\r\n      copying flash_attn\\models\\gptj.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\models\r\n      copying flash_attn\\models\\gpt_j.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\models\r\n      copying flash_attn\\models\\gpt_neox.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\models\r\n      copying flash_attn\\models\\llama.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\models\r\n      copying flash_attn\\models\\opt.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\models\r\n      copying flash_attn\\models\\vit.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\models\r\n      copying flash_attn\\models\\__init__.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\models\r\n      creating build\\lib.win-amd64-cpython-310\\flash_attn\\modules\r\n      copying flash_attn\\modules\\block.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\modules\r\n      copying flash_attn\\modules\\embedding.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\modules\r\n      copying flash_attn\\modules\\mha.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\modules\r\n      copying flash_attn\\modules\\mlp.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\modules\r\n      copying flash_attn\\modules\\__init__.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\modules\r\n      creating build\\lib.win-amd64-cpython-310\\flash_attn\\ops\r\n      copying flash_attn\\ops\\activations.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\ops\r\n      copying flash_attn\\ops\\fused_dense.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\ops\r\n      copying flash_attn\\ops\\gelu_activation.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\ops\r\n      copying flash_attn\\ops\\layer_norm.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\ops\r\n      copying flash_attn\\ops\\rms_norm.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\ops\r\n      copying flash_attn\\ops\\__init__.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\ops\r\n      creating build\\lib.win-amd64-cpython-310\\flash_attn\\triton\r\n      copying flash_attn\\triton\\fused_attention.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\triton\r\n      copying flash_attn\\triton\\__init__.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\triton\r\n      creating build\\lib.win-amd64-cpython-310\\flash_attn\\utils\r\n      copying flash_attn\\utils\\benchmark.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\utils\r\n      copying flash_attn\\utils\\distributed.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\utils\r\n      copying flash_attn\\utils\\generation.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\utils\r\n      copying flash_attn\\utils\\pretrained.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\utils\r\n      copying flash_attn\\utils\\__init__.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\utils\r\n      running build_ext\r\n      C:\\Users\\admin\\.conda\\envs\\llava\\lib\\site-packages\\torch\\utils\\cpp_extension.py:359: UserWarning: Error checking compiler version for cl: [WinError 2] 系统找不到指定的文件。\r\n        warnings.warn(f'Error checking compiler version for {compiler}: {error}')\r\n      Traceback (most recent call last):\r\n        File \"<string>\", line 2, in <module>\r\n        File \"<pip-setuptools-caller>\", line 34, in <module>\r\n        File \"C:\\Users\\admin\\AppData\\Local\\Temp\\pip-install-tp31ysl7\\flash-attn_b02fe69769be4953b2cf3debf59648b6\\setup.py\", line 163, in <module>\r\n          setup(\r\n        File \"C:\\Users\\admin\\.conda\\envs\\llava\\lib\\site-packages\\setuptools\\__init__.py\", line 87, in setup\r\n          return distutils.core.setup(**attrs)\r\n        File \"C:\\Users\\admin\\.conda\\envs\\llava\\lib\\site-packages\\setuptools\\_distutils\\core.py\", line 185, in setup\r\n          return run_commands(dist)\r\n        File \"C:\\Users\\admin\\.conda\\envs\\llava\\lib\\site-packages\\setuptools\\_distutils\\core.py\", line 201, in run_commands\r\n          dist.run_commands()\r\n        File \"C:\\Users\\admin\\.conda\\envs\\llava\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 969, in run_commands\r\n          self.run_command(cmd)\r\n        File \"C:\\Users\\admin\\.conda\\envs\\llava\\lib\\site-packages\\setuptools\\dist.py\", line 1208, in run_command\r\n          super().run_command(command)\r\n        File \"C:\\Users\\admin\\.conda\\envs\\llava\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 988, in run_command\r\n          cmd_obj.run()\r\n        File \"C:\\Users\\admin\\.conda\\envs\\llava\\lib\\site-packages\\wheel\\bdist_wheel.py\", line 325, in run\r\n          self.run_command(\"build\")\r\n        File \"C:\\Users\\admin\\.conda\\envs\\llava\\lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 318, in run_command\r\n          self.distribution.run_command(command)\r\n        File \"C:\\Users\\admin\\.conda\\envs\\llava\\lib\\site-packages\\setuptools\\dist.py\", line 1208, in run_command\r\n          super().run_command(command)\r\n        File \"C:\\Users\\admin\\.conda\\envs\\llava\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 988, in run_command\r\n          cmd_obj.run()\r\n        File \"C:\\Users\\admin\\.conda\\envs\\llava\\lib\\site-packages\\setuptools\\_distutils\\command\\build.py\", line 132, in run\r\n          self.run_command(cmd_name)\r\n        File \"C:\\Users\\admin\\.conda\\envs\\llava\\lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 318, in run_command\r\n          self.distribution.run_command(command)\r\n        File \"C:\\Users\\admin\\.conda\\envs\\llava\\lib\\site-packages\\setuptools\\dist.py\", line 1208, in run_command\r\n          super().run_command(command)\r\n        File \"C:\\Users\\admin\\.conda\\envs\\llava\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 988, in run_command\r\n          cmd_obj.run()\r\n        File \"C:\\Users\\admin\\.conda\\envs\\llava\\lib\\site-packages\\setuptools\\command\\build_ext.py\", line 84, in run\r\n          _build_ext.run(self)\r\n        File \"C:\\Users\\admin\\.conda\\envs\\llava\\lib\\site-packages\\setuptools\\_distutils\\command\\build_ext.py\", line 346, in run\r\n          self.build_extensions()\r\n        File \"C:\\Users\\admin\\.conda\\envs\\llava\\lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 499, in build_extensions\r\n          _check_cuda_version(compiler_name, compiler_version)\r\n        File \"C:\\Users\\admin\\.conda\\envs\\llava\\lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 383, in _check_cuda_version\r\n          torch_cuda_version = packaging.version.parse(torch.version.cuda)\r\n        File \"C:\\Users\\admin\\.conda\\envs\\llava\\lib\\site-packages\\pkg_resources\\_vendor\\packaging\\version.py\", line 49, in parse\r\n          return Version(version)\r\n        File \"C:\\Users\\admin\\.conda\\envs\\llava\\lib\\site-packages\\pkg_resources\\_vendor\\packaging\\version.py\", line 264, in __init__\r\n          match = self._regex.search(version)\r\n      TypeError: expected string or bytes-like object\r\n      [end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for flash-attn\r\n  Running setup.py clean for flash-attn\r\nFailed to build flash-attn\r\nERROR: Could not build wheels for flash-attn, which is required to install pyproject.toml-based projects\n</Comment>\n<Comment by haotian-liu at 2023-04-23T03:55:11Z>\nHi it seems that it's a Windows machine, and it cannot find the CUDA/GPU?  I am not familiar with compiling these on Windows so I may not be able to offer much help on this.\r\n\r\nOne thing I would like to mention is that the flash attention is only needed for training.  So you may go ahead without installing the flash-attn and run the demo/inference.\n</Comment>\n<Comment by Richar-Du at 2023-04-25T02:24:41Z>\n> Hi @Richar-Du, thank you for your interest in our work.\r\n> \r\n> `flash-attn` is not required for running the inference of LLaVA, so that error message you were seeing regarding flash-attn should be not related to LLaVA at all (including the `pyproject.toml` part).\r\n> \r\n> Can you provide: (1) the full error log, and wrap with ``` as well; (2) your system environment, including the OS, CUDA version, and GPU type?\r\n\r\nI want to run the training code and the error is:\r\n[flash-attn.log](https://github.com/haotian-liu/LLaVA/files/11315850/flash-attn.log)\r\n\r\nThe OS is CentOS Linux release 7.6.1810 (Core) x86_64, the CUDA is 11.4, and the GPU is NVIDIA A100-SXM4-80GB.\r\n\r\nThanks in advance.\n</Comment>\n<Comment by haotian-liu at 2023-05-01T04:09:50Z>\nHi @Richar-Du, sorry I just saw your comment.  I am not sure if this is the cause, as it is an issue with the `flash-attn` and not our repo itself.  But it seems that you may need a newer GCC compiler.  Can you try use a newer gcc compiler in $PATH and rerun pip install?  Thanks!\r\n\r\n```\r\n      /home/hadoop-ba-dealrank/dolphinfs_hdd_hadoop-ba-dealrank/duyifan04/miniconda3/envs/llava/lib/python3.8/site-packages/torch/include/c10/util/C++17.h:16:2: error: #error \"You're trying to build PyTorch with a too old version of GCC. We need GCC 5 or later.\"\r\n       #error \\\r\n        ^\r\n```\n</Comment>\n<Comment by XipengY at 2023-05-06T07:23:10Z>\n@Richar-Du Maybe you can checkout your NVCC version, you'd better use NVCC>11.7, hope to help you!\n</Comment>\n<Comment by shinganEuler at 2023-05-06T12:29:56Z>\nI meet the same problem. Here's how to solve it.\r\n1、You should install cuda11.7, cuda12 or above will failed.\r\n2、You should install gcc11 and g++11 or below, as cuda11.7 required. You should run g++ --version to make sure you install the correct version.\r\n3、conda install -c nvidia/label/cuda-11.7.0 cuda-nvcc\r\n4、export CPATH=/usr/local/cuda-11.7/targets/x86_64-linux/include:$CPATH\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-11.7/targets/x86_64-linux/lib:$LD_LIBRARY_PATH\r\nexport PATH=/usr/local/cuda-11.7/bin:$PATH\r\n5、Finally, run pip install flash-attn\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 35,
    "state": "closed",
    "created_by": "conorhearn1",
    "created_at": "2023-04-21T21:28:47Z",
    "labels": [
      "question"
    ],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/35</URL>\n\n<TITLE>Future directions?</TITLE>\n\n<BODY>Really fascinating work, great job! I would be fascinated to know what future directions you're planning to explore. Are you thinking of utilizing SAM or other vision encoders in future models? Would also be interesting to see how performance is affected by the model size, such as how larger LLaMA models perform with this scheme... The dataset is phenomenal work, thank you!</BODY>\n\n<COMMENTS>\n<Comment by ChunyuanLI at 2023-04-22T19:27:07Z>\nThere are many exciting directions enabled after we see LLaVA achieves the amazing results with little data and training. Please see the very brief discussions on the future opportunities we wrote in the paper:\r\n\r\n![image](https://user-images.githubusercontent.com/8978644/233802936-4ec54d8c-9730-4f0d-8244-3e16f980a9a3.png)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 34,
    "state": "closed",
    "created_by": "yix-chen",
    "created_at": "2023-04-21T14:05:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/34</URL>\n\n<TITLE>some urls in CC-3M Pretrain 595K metadata.json are broken</TITLE>\n\n<BODY>some urls in CC-3M Pretrain 595K metadata.json are broken, could you please provide the downloaded images?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-22T01:51:59Z>\nHi @yix-chen can you provide what's the percentage of data not available?  I will need to check to see if I can provide these images directly.  If the percentage is not high (~10%), the performance shall be similar.  We have found our approach being pretty robust and generalizable towards the pretraining dataset.\r\n\r\nBut thank you for bringing this to our attention.  I will let you know with the update soon.\n</Comment>\n<Comment by jianantian at 2023-04-24T05:58:03Z>\nsame problem, some url are broken, i only get ~72% readable image out of total 595375 data from metadata.json.\n</Comment>\n<Comment by yix-chen at 2023-04-25T02:25:26Z>\nHi @haotian-liu , same results as @jianantian , appreciate it if you could provide these images.\n</Comment>\n<Comment by haotian-liu at 2023-04-26T02:37:27Z>\nHi @jianantian @yix-chen \r\n\r\nThank you for your feedback!  We have uploaded the images to huggingface.  Please see [here](https://github.com/haotian-liu/LLaVA#pretraining-dataset) and the disclaimer below.  Thanks!\r\n\r\nUpon the request from the community, as ~15% images of the original CC-3M dataset are no longer accessible, we upload images.zip for better reproducing our work in research community. It should not be used for any other purpose. The use of these images must comply with the CC-3M license. This may be taken down when requested by the original CC-3M dataset owner or owners of the referenced images.\n</Comment>\n<Comment by haotian-liu at 2023-05-01T04:03:46Z>\nHi, I am closing this issue, due to the inactivity.  Hope your problem has been resolved.  If you have further concerns, please feel free to re-open or open a new issue.  Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 33,
    "state": "closed",
    "created_by": "qibao77",
    "created_at": "2023-04-21T10:06:10Z",
    "labels": [
      "question"
    ],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/33</URL>\n\n<TITLE>May I know the effect of filtering CC3M? If I use all CC3M for stage1 pre-training, will the result be good or not?</TITLE>\n\n<BODY></BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-22T01:57:37Z>\nHi @qibao77, thank you for the interest in our work.\r\n\r\nThe main reason for filtering CC3M is for:\r\n(1) creating a more balanced pretraining dataset, as shown in our paper and project page;\r\n(2) to save pretraining cost.\r\n\r\nWe have some that our method is pretty robust and generalizable w.r.t. the pretraining dataset choice, so using the full CC3M set probably would not hurt.\r\n\r\nWe are working on a comparison with different pretraining set currently and will provide an update very soon.\n</Comment>\n<Comment by haotian-liu at 2023-05-01T04:03:39Z>\nHi, I am closing this issue, due to the inactivity.  Hope your problem has been resolved.  If you have further concerns, please feel free to re-open or open a new issue.  Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 31,
    "state": "closed",
    "created_by": "gordonhu608",
    "created_at": "2023-04-21T07:06:06Z",
    "labels": [
      "documentation"
    ],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/31</URL>\n\n<TITLE>How to reproduce the evaluation of science qa score.</TITLE>\n\n<BODY>Hi, when I'm loading the question.jsonl in eval model vqa science.py. You directly implemented it as json.load() and I'm receiving a error. However since it's a json line file, shouldn't you do [json.load(line) for line in file]\r\n\r\nAnyway, my question is could you provide more information on how to reproduce your science qa score. I encountered many problems when reproducing it. Simpling specifying which file to load for each parser argument also helps a lot!  The following link you provided is also a 404 error. Thanks for your efforts ahead for making your work reproducible. \r\n\r\n![image](https://user-images.githubusercontent.com/24793015/233563798-f833b5a5-8a02-43ca-b7fd-2e6493b0f258.png)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-24T05:17:29Z>\nHi @gordonhu608\r\nThank you for your interest in our work!\r\n\r\nWe have converted the weights for release, and also updated instructions: https://github.com/haotian-liu/LLaVA#scienceqa.\r\n\r\nPlease let me know if you have further questions.\n</Comment>\n<Comment by haotian-liu at 2023-05-01T03:59:53Z>\nHi, I am closing this issue, due to the inactivity.  Hope your problem has been resolved.  If you have further concerns, please feel free to re-open or open a new issue.  Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 30,
    "state": "closed",
    "created_by": "yix-chen",
    "created_at": "2023-04-21T05:18:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/30</URL>\n\n<TITLE>assert input_embeddings.shape == embed_tokens_weight.shape</TITLE>\n\n<BODY>Error message:\r\nFile \"/home/ubuntu/project/llava/llava/train/train.py\", line 479, in train\r\nassert input_embeddings.shape == embed_tokens_weight.shape\r\nAssertionError\r\n\r\nI download the projector weights from https://huggingface.co/liuhaotian/LLaVA-13b-pretrain-projector-v0, and use vicuna-13b as base model.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-21T05:20:44Z>\nHi, please pull the latest code, as it is updated to only load the additional embeddings this morning, in compliance to the LLaMA license.  Thanks!\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/llava/train/train.py#L479-L483\n</Comment>\n<Comment by haotian-liu at 2023-05-01T04:03:30Z>\nHi, I am closing this issue, due to the inactivity.  Hope your problem has been resolved.  If you have further concerns, please feel free to re-open or open a new issue.  Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 29,
    "state": "closed",
    "created_by": "yix-chen",
    "created_at": "2023-04-21T04:41:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/29</URL>\n\n<TITLE>where is the llava_instruct_150k.json</TITLE>\n\n<BODY>Hi, in ''download_data.sh'' we can only download ''complex_reasoning_77k.json  conversation_58k.json  detail_23k.json'', I wonder if we need to concat them to llava_instruct_150k.json?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-21T05:26:53Z>\nThank you for pointing this out.  For convenience of the usage, I have uploaded concatenated version [llava_instruct_150k.json](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_instruct_150k.json) as well.\n</Comment>\n<Comment by haotian-liu at 2023-05-01T04:03:21Z>\nHi, I am closing this issue, due to the inactivity.  Hope your problem has been resolved.  If you have further concerns, please feel free to re-open or open a new issue.  Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 28,
    "state": "closed",
    "created_by": "haorunze",
    "created_at": "2023-04-21T04:01:56Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/28</URL>\n\n<TITLE>where can i get the model LLaVA-13B-v0 in ./checkpoints/LLaVA-13B-v0?</TITLE>\n\n<BODY></BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-21T05:48:37Z>\nHi, please follow the instructions here to download the delta weights and convert to the target weights.  We need this extra conversion step due to the LLaMA license, similar to Vicuna, Koala, etc.\r\n\r\nhttps://github.com/haotian-liu/LLaVA#llava-weights\n</Comment>\n<Comment by haotian-liu at 2023-05-01T04:03:13Z>\nHi, I am closing this issue, due to the inactivity.  Hope your problem has been resolved.  If you have further concerns, please feel free to re-open or open a new issue.  Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 27,
    "state": "closed",
    "created_by": "jpWang",
    "created_at": "2023-04-21T03:00:14Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/27</URL>\n\n<TITLE>which version of vicuna weight (v0 or v1.1?) should be used</TITLE>\n\n<BODY>Thanks for your great work! \r\nIn fine-tune command there is a \r\n--model_name_or_path /path/to/llama-vicuna-13b .\r\nAnd I just want to ask that which version of vicuna weight (v0 or v1.1?) should be used?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-21T03:02:33Z>\nHi, thank you for your interest in our work.  We used Vicuna-v0 weights.\r\n\r\nNote that when you are performing [weights conversion](https://github.com/haotian-liu/LLaVA#llava-13b), you should use LLaMA weights as the base weight instead of Vicuna weights.\n</Comment>\n<Comment by PhanTask at 2023-04-22T19:44:25Z>\n> Hi, thank you for your interest in our work. We used Vicuna-v0 weights.\r\n> \r\n> Note that when you are performing [weights conversion](https://github.com/haotian-liu/LLaVA#llava-13b), you should use LLaMA weights as the base weight instead of Vicuna weights.\r\n\r\nHi, thanks for the great work. A follow-up question: does it mean LLaVA's delta weights include Vicuna's delta weights? Which means, we do not need to first convert LLaMA to Vicuna, and we do not need to even download Vicuna's delta weight. Is this correct?\n</Comment>\n<Comment by haotian-liu at 2023-04-22T19:50:04Z>\n@PhanTask You're right.  Our delta is computed by directly comparing with LLaMA.\n</Comment>\n<Comment by PhanTask at 2023-04-22T19:51:58Z>\nWow thanks for your prompt reply! Really appreciate your great work.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 26,
    "state": "closed",
    "created_by": "gordonhu608",
    "created_at": "2023-04-20T23:58:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/26</URL>\n\n<TITLE>Time takes for each stage training</TITLE>\n\n<BODY>Hi, How long does it take to train for stage 1 and stage 2 on how many say A100 (40g) GPU. Thank you!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-21T00:18:25Z>\nHi @gordonhu608 \r\n\r\nWe pretrain our model on CC3M-595K data on 8x A100s for around 5 hours. The instruction tuning of the initial release takes ~10 hours on the same machine using full 150K data. We also find using a smaller subset (even as small as, say 15k, which finishes within an hour) can achieve similar performance. We'll update details of these experiments later.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 25,
    "state": "closed",
    "created_by": "152334H",
    "created_at": "2023-04-20T17:06:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/25</URL>\n\n<TITLE>Possible to use int8 during pretraining stage?</TITLE>\n\n<BODY>Since the LLaMA model is frozen during pretraining, I was wondering if it was possible to run the model with int8 to reduce the vram used during training.\r\n\r\nIn my attempts to do so, I successfully started training with llama as int8 (by just adding the usual `load_in_8bit=True, device_map=...` combo), but observed that the loss would always collapse to `0.0` immediately. I also noticed the same issue when I simply loaded the llama model as fp16, rather than as bf16.</BODY>\n\n<COMMENTS>\n<Comment by 152334H at 2023-04-20T17:11:18Z>\nalso just an fyi but the references to `fastchat/train/train_mem.py` in the README should probably be changed to `llava/train/train_mem.py`\n</Comment>\n<Comment by haotian-liu at 2023-04-20T19:31:53Z>\nHi @152334H, thanks for your interest in our work!  Thanks for pointing out the script path issue, and I have just corrected it.\r\n\r\nRegarding using INT8 during the pretraining stage, there _may_ be issues doing so.  The reason is that the Linear layer that is being projected, are optimized with the gradient that is backward propagated all the way from the last layer to the first layer of the LLM.  When directly doing this INT8 loading, I guess some gradients will underflow / overflow.\r\n\r\nI do know that some Alpaca-LORA projects use INT8 training, and I am interested in integrating these techniques into our project as well.  I will also look into this after finishing all related release stuff.  Please let me know if you are also interested in this and would like to contribute!\n</Comment>\n<Comment by haotian-liu at 2023-05-01T04:02:37Z>\nHi, I am closing this issue, due to the inactivity.  Hope your problem has been resolved.  If you have further concerns, please feel free to re-open or open a new issue.  Thanks!\n</Comment>\n<Comment by 152334H at 2023-05-01T07:56:14Z>\nNo problem.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 24,
    "state": "closed",
    "created_by": "paulpaul91",
    "created_at": "2023-04-20T14:52:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/24</URL>\n\n<TITLE>pretrain error</TITLE>\n\n<BODY>![image](https://user-images.githubusercontent.com/22076188/233404474-9d0977c7-c374-4aae-b673-06fabccb0466.png)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-20T16:57:32Z>\nHi, thank you for your interest in our work.\r\n\r\nCan you provide the log that contains the actual error message?  It should be somewhere above this error message.  You may wrap the error log with ``` to make it easier to read in GitHub.  Thanks.\n</Comment>\n<Comment by yix-chen at 2023-04-21T03:17:23Z>\nHi, I encountered the same error in finetuning, it seems it runs out of RAM.\n</Comment>\n<Comment by tensorboy at 2023-05-01T03:42:26Z>\nI have similiar issues. \r\n\r\nI can run it with 4 GPUs, but cannot with 8GPUs (A100 80G memeory each, 1.96TB CPU RAM.\r\n\r\nMy command it\r\n```\r\nLOGLEVEL=INFO TORCHELASTIC_ENABLE_FILE_TIMER=1 torchrun --nnodes=1 --nproc_per_node=4 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path /mnt/bd/xxxx/weights/llama-dl-main/vicuna_13B \\\r\n    --data_path /mnt/bd/xxxx/experiment/LLaVA/LLaVA-Instruct-150K/chat.json \\\r\n    --image_folder /mnt/bd/xxxx/experiment/LLaVA/LLaVA-Instruct-150K/images \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-13b-pretrain \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 2 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 2400 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --debug underflow_overflow \\\r\n    --report_to none\r\n```\r\n\r\nLogs:\r\n\r\n```\r\nLOGLEVEL=INFO TORCHELASTIC_ENABLE_FILE_TIMER=1 torchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \\\r\n>     llava/train/train_mem.py \\\r\n>     --model_name_or_path /mnt/bd/data-tns-algo-masp-llm/weights/llama-dl-main/vicuna_13B \\\r\n>     --data_path /mnt/bd/data-tns-algo-masp-llm/experiment/LLaVA/LLaVA-Instruct-150K/chat.json \\\r\n>     --image_folder /mnt/bd/data-tns-algo-masp-llm/experiment/LLaVA/LLaVA-Instruct-150K/images \\\r\n>     --vision_tower openai/clip-vit-large-patch14 \\\r\n>     --tune_mm_mlp_adapter True \\\r\n>     --mm_vision_select_layer -2 \\\r\n>     --mm_use_im_start_end \\\r\n>     --bf16 True \\\r\n>     --output_dir ./checkpoints/llava-13b-pretrain \\\r\n>     --num_train_epochs 1 \\\r\n>     --per_device_train_batch_size 16 \\\r\n>     --per_device_eval_batch_size 4 \\\r\n>     --gradient_accumulation_steps 2 \\\r\n>     --evaluation_strategy \"no\" \\\r\n>     --save_strategy \"steps\" \\\r\n>     --save_steps 2400 \\\r\n>     --save_total_limit 1 \\\r\n>     --learning_rate 2e-3 \\\r\n>     --weight_decay 0. \\\r\n>     --warmup_ratio 0.03 \\\r\n>     --lr_scheduler_type \"cosine\" \\\r\n>     --logging_steps 1 \\\r\n>     --tf32 True \\\r\n>     --model_max_length 2048 \\\r\n>     --gradient_checkpointing True \\\r\n>     --lazy_preprocess True \\\r\n>     --debug underflow_overflow \\\r\n>     --report_to none\r\n\r\nINFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:\r\n  entrypoint       : llava/train/train_mem.py\r\n  min_nodes        : 1\r\n  max_nodes        : 1\r\n  nproc_per_node   : 8\r\n  run_id           : none\r\n  rdzv_backend     : static\r\n  rdzv_endpoint    : 127.0.0.1:25001\r\n  rdzv_configs     : {'rank': 0, 'timeout': 900}\r\n  max_restarts     : 0\r\n  monitor_interval : 5\r\n  log_dir          : None\r\n  metrics_cfg      : {}\r\n\r\nINFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic__5qd21x9/none_xfx__n23\r\nINFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=0\r\n  master_addr=127.0.0.1\r\n  master_port=25001\r\n  group_rank=0\r\n  group_world_size=1\r\n  local_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\r\n  role_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\r\n  global_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\r\n  role_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]\r\n  global_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]\r\n\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\nINFO:torch.distributed.elastic.agent.server.local_elastic_agent:Starting a FileTimerServer with /tmp/watchdog_timer_01ab64a1-b00c-4ad2-ae61-4607b03008e7 ...\r\nINFO:torch.distributed.elastic.agent.server.local_elastic_agent:FileTimerServer started\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic__5qd21x9/none_xfx__n23/attempt_0/0/error.json\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic__5qd21x9/none_xfx__n23/attempt_0/1/error.json\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker2 reply file to: /tmp/torchelastic__5qd21x9/none_xfx__n23/attempt_0/2/error.json\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker3 reply file to: /tmp/torchelastic__5qd21x9/none_xfx__n23/attempt_0/3/error.json\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker4 reply file to: /tmp/torchelastic__5qd21x9/none_xfx__n23/attempt_0/4/error.json\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker5 reply file to: /tmp/torchelastic__5qd21x9/none_xfx__n23/attempt_0/5/error.json\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker6 reply file to: /tmp/torchelastic__5qd21x9/none_xfx__n23/attempt_0/6/error.json\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker7 reply file to: /tmp/torchelastic__5qd21x9/none_xfx__n23/attempt_0/7/error.json\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 39409 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 39410 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 39411 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 39412 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 39414 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 39415 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 39416 closing signal SIGTERM\r\nERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9) local_rank: 4 (pid: 39413) of binary: /mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/bin/python\r\nINFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish\r\nINFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.00801992416381836 seconds\r\nINFO:torch.distributed.elastic.multiprocessing.errors:local_rank 4 FAILED with no error file. Decorate your entrypoint fn with @record for traceback info. See: https://pytorch.org/docs/stable/elastic/errors.html\r\nTraceback (most recent call last):\r\n  File \"/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/bin/torchrun\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/run.py\", line 794, in main\r\n    run(args)\r\n  File \"/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/run.py\", line 785, in run\r\n    elastic_launch(\r\n  File \"/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n  File \"/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \r\n======================================================\r\nllava/train/train_mem.py FAILED\r\n------------------------------------------------------\r\nFailures:\r\n  <NO_OTHER_FAILURES>\r\n------------------------------------------------------\r\nRoot Cause (first observed failure):\r\n[0]:\r\n  time      : 2023-05-01_11:41:43\r\n  host      : n193-016-074.byted.org\r\n  rank      : 4 (local_rank: 4)\r\n  exitcode  : -9 (pid: 39413)\r\n  error_file: <N/A>\r\n  traceback : Signal 9 (SIGKILL) received by PID 39413\r\n======================================================\r\n```\n</Comment>\n<Comment by haotian-liu at 2023-05-01T03:49:28Z>\n@tensorboy, thank you for your interest in our work.\r\n\r\nI noticed that you are using the instruction tuning dataset in the pretraining stage.  This shall be the reason of OOM, as the instruction tuning dataset have longer responses, comparing to the short captions in CC3M.\r\n\r\nIs there any specific reason you want to use instruction tuning with pretraining stage?  If so, you may try [FSDP](https://github.com/haotian-liu/LLaVA#experimental-use-fsdp-to-save-memory-in-pretraining). Note that this is experimental as PyTorch has not officially supported PEFT for FSDP yet.  I have locally verified with very few iterations (400 iters x 8GPUs x 16 per_gpu_batch_size) that the model behavior is reasonable, and similar to w/o FSDP.  Make sure to use the latest code base, as there is an issue fixed this afternoon.\r\n\r\nThanks.\n</Comment>\n<Comment by haotian-liu at 2023-05-01T03:50:31Z>\n@yix-chen Sorry just saw this comment.  Are you still facing this issue, and if so, can you share your hardware setting (GPU type and count), and the command, as well as the error logs?  Thanks.\n</Comment>\n<Comment by tensorboy at 2023-05-01T03:56:51Z>\n> \r\n\r\nthanks for the quick reply, I think I've made mistakes for the dataset, but where I can download CC3M dataset..\n</Comment>\n<Comment by haotian-liu at 2023-05-01T03:58:00Z>\n@tensorboy Please checkout the download instructions [here](https://github.com/haotian-liu/LLaVA#pretraining-dataset), thanks.\n</Comment>\n<Comment by tensorboy at 2023-05-01T04:10:43Z>\n> @tensorboy Please checkout the download instructions [here](https://github.com/haotian-liu/LLaVA#pretraining-dataset), thanks.\r\n\r\nThanks for the quick feedback. I'm still confused for the pretraining command:\r\n```\r\ntorchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path ./checkpoints/llama-vicuna-13b \\\r\n    --data_path /path/to/cc3m_595k.json \\\r\n    --image_folder /path/to/cc3m_595k \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-13b-pretrain \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 2400 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nI cannot find cc3m_595k.json, and the most relevant one is 'chat.json' at here: https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K/tree/main\n</Comment>\n<Comment by haotian-liu at 2023-05-01T04:12:48Z>\nHi @tensorboy, `chat.json` in this repo is the correct one to go with.  You may also download and see if `len(json.load(...))` is roughly 595K.\n</Comment>\n<Comment by tensorboy at 2023-05-01T04:14:50Z>\n> Hi @tensorboy, `chat.json` in this repo is the correct one to go with. You may also download and see if `len(json.load(...))` is roughly 595K.\r\n\r\nit's 595375\n</Comment>\n<Comment by haotian-liu at 2023-05-01T04:15:34Z>\nYep, that's correct :)\n</Comment>\n<Comment by tensorboy at 2023-05-01T04:16:21Z>\nThank you, what is your suggestions now?\n</Comment>\n<Comment by haotian-liu at 2023-05-01T04:18:11Z>\nI am confused.  Do you mean that although you were naming the folder as `LLaVA-Instruct-150K/chat.json`, but it actually comes from this CC3M instead?  Because we have another dataset [LLaVA-Instruct-150K](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K).\n</Comment>\n<Comment by tensorboy at 2023-05-01T04:18:50Z>\n> I am confused. Do you mean that although you were naming the folder as `LLaVA-Instruct-150K/chat.json`, but it actually comes from this CC3M instead?\r\n\r\nyes, I think I've put all the data in that directory (LLaVA-Instruct-150K)\n</Comment>\n<Comment by haotian-liu at 2023-05-01T04:22:45Z>\nThis is strange.  Can you try seeing if finetuning works?  You can download the `./checkpoints/mm_projector/llava-13b-pretrain.bin` [here](https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0/blob/main/mm_projector.bin).  Since you are using A100-80Gx8, this shall not cause OOM at all.\r\n\r\n```\r\ntorchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \\\r\n    llava/train/train_mem.py \\\r\n    --model_name_or_path /path/to/llama-vicuna-13b \\\r\n    --data_path /path/to/llava_instruct_150k.json \\\r\n    --image_folder /Data/haotian/coco/train2014 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/mm_projector/llava-13b-pretrain.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 5000 \\\r\n    --save_total_limit 3 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --fsdp \"full_shard auto_wrap\" \\\r\n    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\n</Comment>\n<Comment by tensorboy at 2023-05-01T04:23:47Z>\n> haotian\r\n\r\nlet me try it now\n</Comment>\n<Comment by haotian-liu at 2023-05-01T04:24:46Z>\nAlso, can you monitor both the GPU RAM, and CPU RAM usage when you are running the code, are they changing before throwing out the error?  Your CPU RAM/GPU RAM should be more than sufficient, but we can see what is happening.  The peak CPU memory usage may be ~500GB in your case.\n</Comment>\n<Comment by tensorboy at 2023-05-01T04:30:43Z>\n> Also, can you monitor both the GPU RAM, and CPU RAM usage when you are running the code, are they changing before throwing out the error? Your CPU RAM/GPU RAM should be more than sufficient, but we can see what is happening. The peak CPU memory usage may be ~500GB in your case.\r\n\r\nCPU is around 400GB at peak, but GPU memory is 3MB / 81251 MB until it crashes.\n</Comment>\n<Comment by haotian-liu at 2023-05-01T04:46:11Z>\nIt seems that it does not even start loading the checkpoints.  And it crashes when just initializing the model.  Honestly, I am not sure what the cause is, as I have not met such error before.\r\n\r\n```\r\nINFO:torch.distributed.elastic.multiprocessing.errors:local_rank 4 FAILED with no error file. Decorate your entrypoint fn with @record for traceback info. See: https://pytorch.org/docs/stable/elastic/errors.html\r\n```\r\n\r\nDo you think this `@record` is useful for this case?  It seems a similar issue to https://github.com/lm-sys/FastChat/issues/627?  Unfortunately there is no solution provided there either.\n</Comment>\n<Comment by tensorboy at 2023-05-01T04:54:18Z>\n> lm-sys/FastChat#627\r\n\r\nIt's exactly same issues with that fastchat. I'm not sure what that @record is and how to use that in your code either..\n</Comment>\n<Comment by tensorboy at 2023-05-01T05:05:23Z>\n> here\r\n\r\nsame errors. log:\r\n\r\n```\r\ntorchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \\\r\n> llava/train/train_mem.py \\\r\n> --model_name_or_path /mnt/bd/data-tns-algo-masp-llm/weights/llama-dl-main/vicuna_13B  \\\r\n> --data_path /mnt/bd/data-tns-algo-masp-llm/experiment/LLaVA/LLaVA-Instruct-150K/llava_instruct_150k.json \\\r\n> --image_folder /mnt/bd/data-tns-algo-masp-llm/experiment/LLaVA/data/coco/train2014 \\\r\n> --vision_tower openai/clip-vit-large-patch14 \\\r\n> --pretrain_mm_mlp_adapter /mnt/bd/data-tns-algo-masp-llm/experiment/LLaVA/checkpoints/llava-13b-pretrain/mm_projector.bin \\\r\n> --mm_vision_select_layer -2 \\\r\n> --mm_use_im_start_end True \\\r\n> --bf16 True \\\r\n> --output_dir ./checkpoints \\\r\n> --num_train_epochs 3 \\\r\n> --per_device_train_batch_size 4 \\\r\n> --per_device_eval_batch_size 4 \\\r\n> --gradient_accumulation_steps 1 \\\r\n> --evaluation_strategy \"no\" \\\r\n> --save_strategy \"steps\" \\\r\n> --save_steps 5000 \\\r\n> --save_total_limit 3 \\\r\n> --learning_rate 2e-5 \\\r\n> --weight_decay 0. \\\r\n> --warmup_ratio 0.03 \\\r\n> --lr_scheduler_type \"cosine\" \\\r\n> --logging_steps 1 \\\r\n> --tf32 True \\\r\n> --fsdp \"full_shard auto_wrap\" \\\r\n> --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\r\n> --model_max_length 2048 \\\r\n> --gradient_checkpointing True \\\r\n> --lazy_preprocess True \\\r\n> --report_to wandb\r\n\r\n\r\n/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/training_args.py:1356: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \r\n  warnings.warn(\r\n/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/training_args.py:1356: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \r\n  warnings.warn(\r\n/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/training_args.py:1356: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \r\n  warnings.warn(\r\n/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/training_args.py:1356: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \r\n  warnings.warn(\r\n/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/training_args.py:1356: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \r\n  warnings.warn(\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\n/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/training_args.py:1356: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \r\n  warnings.warn(\r\n/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/training_args.py:1356: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \r\n  warnings.warn(\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\n/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/training_args.py:1356: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \r\n  warnings.warn(\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 144298 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 144299 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 144300 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 144301 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 144302 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 144303 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 144304 closing signal SIGTERM\r\nERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9) local_rank: 7 (pid: 144305) of binary: /mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/bin/python\r\nTraceback (most recent call last):\r\n  File \"/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/bin/torchrun\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/run.py\", line 794, in main\r\n    run(args)\r\n  File \"/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/run.py\", line 785, in run\r\n    elastic_launch(\r\n  File \"/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n  File \"/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \r\n=======================================================\r\nllava/train/train_mem.py FAILED\r\n-------------------------------------------------------\r\nFailures:\r\n  <NO_OTHER_FAILURES>\r\n-------------------------------------------------------\r\nRoot Cause (first observed failure):\r\n[0]:\r\n  time      : 2023-05-01_13:00:38\r\n  host      : n193-016-074.byted.org\r\n  rank      : 7 (local_rank: 7)\r\n  exitcode  : -9 (pid: 144305)\r\n  error_file: <N/A>\r\n  traceback : Signal 9 (SIGKILL) received by PID 144305\r\n=======================================================\r\n```\n</Comment>\n<Comment by haotian-liu at 2023-05-01T05:08:24Z>\nHi, a random thought: can this be related to `ulimit`?\r\n\r\nSpecifically, what's `ulimit -u`, `ulimit -v`, `ulimit -m` on your machine?\r\n\r\nAnd what's your OS type/version, and CUDA version (just to find the disparity between our machines)?\n</Comment>\n<Comment by tensorboy at 2023-05-01T05:09:45Z>\n> Hi, a random thought: can this be related to `ulimit`?\r\n> \r\n> Specifically, what's `ulimit -u`, `ulimit -v`, `ulimit -m` on your machine?\r\n> \r\n> And what's your OS type/version, and CUDA version (just to find the disparity between our machines)?\r\n\r\nall (ulimit -u, ulimit -v, ulimit -m) will output: \r\n```\r\nunlimited\r\n```\n</Comment>\n<Comment by haotian-liu at 2023-05-01T05:17:11Z>\nSad, then that's not the cause either.  I am not sure what we can do to debug this.  One last attempt is to try what ChatGPT suggests about `record`:\r\n```python\r\nfrom torch.distributed.elastic.multiprocessing.errors import record  \r\n  \r\n@record  \r\ndef main():  \r\n    # Your code here  \r\n```\r\n\r\nSo maybe we can add to `llava/train.py`?\r\n```python\r\n@record\r\ndef train():\r\n```\n</Comment>\n<Comment by tensorboy at 2023-05-01T05:33:08Z>\n> ```python\r\n> from torch.distributed.elastic.multiprocessing.errors import record  \r\n>   \r\n> @record \r\n> ```\r\n\r\nsame errors:\r\n```\r\nOGLEVEL=INFO TORCHELASTIC_ENABLE_FILE_TIMER=1 torchrun --nnodes=1 --nproc_per_node=8 --master_port=25001     llava/train/train_mem.py     --model_name_or_path /mnt/bd/data-tns-algo-masp-llm/weights/llama-dl-main/vicuna_13B     --data_path /mnt/bd/data-tns-algo-masp-llm/experiment/LLaVA/LLaVA-Instruct-150K/chat.json     --image_folder /mnt/bd/data-tns-algo-masp-llm/experiment/LLaVA/LLaVA-Instruct-150K/images     --vision_tower openai/clip-vit-large-patch14     --tune_mm_mlp_adapter True     --mm_vision_select_layer -2     --mm_use_im_start_end     --bf16 True     --output_dir ./checkpoints/llava-13b-pretrain     --num_train_epochs 1     --per_device_train_batch_size 16     --per_device_eval_batch_size 4     --gradient_accumulation_steps 1     --evaluation_strategy \"no\"     --save_strategy \"steps\"     --save_steps 2400     --save_total_limit 1     --learning_rate 2e-3     --weight_decay 0.     --warmup_ratio 0.03     --lr_scheduler_type \"cosine\"     --logging_steps 1     --tf32 True     --model_max_length 2048     --gradient_checkpointing True     --lazy_preprocess True     --debug underflow_overflow     --report_to none\r\nINFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:\r\n  entrypoint       : llava/train/train_mem.py\r\n  min_nodes        : 1\r\n  max_nodes        : 1\r\n  nproc_per_node   : 8\r\n  run_id           : none\r\n  rdzv_backend     : static\r\n  rdzv_endpoint    : 127.0.0.1:25001\r\n  rdzv_configs     : {'rank': 0, 'timeout': 900}\r\n  max_restarts     : 0\r\n  monitor_interval : 5\r\n  log_dir          : None\r\n  metrics_cfg      : {}\r\n\r\nINFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh\r\nINFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=0\r\n  master_addr=127.0.0.1\r\n  master_port=25001\r\n  group_rank=0\r\n  group_world_size=1\r\n  local_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\r\n  role_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\r\n  global_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\r\n  role_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]\r\n  global_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]\r\n\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\nINFO:torch.distributed.elastic.agent.server.local_elastic_agent:Starting a FileTimerServer with /tmp/watchdog_timer_f83bf4ec-c486-4774-9b56-c1eda1f9192d ...\r\nINFO:torch.distributed.elastic.agent.server.local_elastic_agent:FileTimerServer started\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh/attempt_0/0/error.json\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh/attempt_0/1/error.json\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker2 reply file to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh/attempt_0/2/error.json\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker3 reply file to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh/attempt_0/3/error.json\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker4 reply file to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh/attempt_0/4/error.json\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker5 reply file to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh/attempt_0/5/error.json\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker6 reply file to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh/attempt_0/6/error.json\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker7 reply file to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh/attempt_0/7/error.json\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 216958 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 216959 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 216960 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 216961 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 216962 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 216963 closing signal SIGTERM\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 216964 closing signal SIGTERM\r\nERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9) local_rank: 0 (pid: 216957) of binary: /mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/bin/python\r\nINFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish\r\nINFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.008567571640014648 seconds\r\nINFO:torch.distributed.elastic.multiprocessing.errors:local_rank 0 FAILED with no error file. Decorate your entrypoint fn with @record for traceback info. See: https://pytorch.org/docs/stable/elastic/errors.html\r\nTraceback (most recent call last):\r\n  File \"/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/bin/torchrun\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/run.py\", line 794, in main\r\n    run(args)\r\n  File \"/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/run.py\", line 785, in run\r\n    elastic_launch(\r\n  File \"/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n  File \"/mnt/bd/data-tns-algo-masp-llm/environment/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \r\n=======================================================\r\nllava/train/train_mem.py FAILED\r\n-------------------------------------------------------\r\nFailures:\r\n  <NO_OTHER_FAILURES>\r\n-------------------------------------------------------\r\nRoot Cause (first observed failure):\r\n[0]:\r\n  time      : 2023-05-01_13:32:26\r\n  host      : n193-016-074.byted.org\r\n  rank      : 0 (local_rank: 0)\r\n  exitcode  : -9 (pid: 216957)\r\n  error_file: <N/A>\r\n  traceback : Signal 9 (SIGKILL) received by PID 216957\r\n=======================================================\r\n```\n</Comment>\n<Comment by haotian-liu at 2023-05-01T05:35:34Z>\n```\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh/attempt_0/0/error.json\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh/attempt_0/1/error.json\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker2 reply file to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh/attempt_0/2/error.json\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker3 reply file to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh/attempt_0/3/error.json\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker4 reply file to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh/attempt_0/4/error.json\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker5 reply file to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh/attempt_0/5/error.json\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker6 reply file to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh/attempt_0/6/error.json\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker7 reply file to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh/attempt_0/7/error.json\r\n```\r\n\r\nAnything in these files?\n</Comment>\n<Comment by tensorboy at 2023-05-01T05:41:11Z>\n> ```\r\n> INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh/attempt_0/0/error.json\r\n> INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh/attempt_0/1/error.json\r\n> INFO:torch.distributed.elastic.multiprocessing:Setting worker2 reply file to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh/attempt_0/2/error.json\r\n> INFO:torch.distributed.elastic.multiprocessing:Setting worker3 reply file to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh/attempt_0/3/error.json\r\n> INFO:torch.distributed.elastic.multiprocessing:Setting worker4 reply file to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh/attempt_0/4/error.json\r\n> INFO:torch.distributed.elastic.multiprocessing:Setting worker5 reply file to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh/attempt_0/5/error.json\r\n> INFO:torch.distributed.elastic.multiprocessing:Setting worker6 reply file to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh/attempt_0/6/error.json\r\n> INFO:torch.distributed.elastic.multiprocessing:Setting worker7 reply file to: /tmp/torchelastic_zg87tjfx/none_hh2up_uh/attempt_0/7/error.json\r\n> ```\r\n> \r\n> Anything in these files?\r\nall is empty:\r\n<img width=\"530\" alt=\"Screen Shot 2023-04-30 at 10 41 01 PM\" src=\"https://user-images.githubusercontent.com/17189055/235412378-a51e07c5-4b95-4c13-82c5-1d0f68e3c0df.png\">\n</Comment>\n<Comment by haotian-liu at 2023-05-01T05:42:56Z>\n😮‍💨 I have no more ideas now. If you somehow solve this issue, please let me know and maybe also share the solution here.  Thanks!\n</Comment>\n<Comment by tensorboy at 2023-05-01T05:53:55Z>\nI will for sure come back to let you know if I can solve it.\n</Comment>\n<Comment by LetsGoFir at 2023-06-06T02:10:58Z>\n> I will for sure come back to let you know if I can solve it.\r\n\r\nhello how's it going?\n</Comment>\n<Comment by tensorboy at 2023-06-06T03:26:28Z>\nupgrade to pytorch 2.0 fix all of it\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 23,
    "state": "closed",
    "created_by": "yix-chen",
    "created_at": "2023-04-20T14:46:55Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/23</URL>\n\n<TITLE>ValueError: Some specified arguments are not used by the HfArgumentParser: ['--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'True']</TITLE>\n\n<BODY>I installed the correct transformer library using \"pip install --upgrade --force-reinstall git+https://github.com/haotian-liu/transformers_llava.git@26356f0d07bacfb3857dafc7f8a519304b4c0572\" but when I run the finetune script, it throws the error message.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-20T16:14:15Z>\nHi, thank you for the interest in our work.\r\n\r\nCan you try with the updated instructions: first create a new Conda environment, and then do the install?  I have verified locally that this works.  Thanks!\r\n\r\nhttps://github.com/haotian-liu/LLaVA#install\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 22,
    "state": "closed",
    "created_by": "vtddggg",
    "created_at": "2023-04-20T11:46:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/22</URL>\n\n<TITLE>How to obtain pretraining dataset: cc3m_595k?</TITLE>\n\n<BODY>Anyone can help to point out if 600K filtered CC3M (used for pretraining) is released? Where can we download them</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-20T15:01:25Z>\nHi, thank you for your interest in our work!\r\n\r\nOur pretraining dataset is released.  Please see [here](https://github.com/haotian-liu/LLaVA#pretraining-dataset).\n</Comment>\n<Comment by vtddggg at 2023-04-21T01:24:17Z>\nThanks for your reply!!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 21,
    "state": "closed",
    "created_by": "Richar-Du",
    "created_at": "2023-04-20T03:05:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/21</URL>\n\n<TITLE>Any plan to release the cc3m filtered data and the weight of the linear layer after the 1st training stage?</TITLE>\n\n<BODY>It would be valuable to train our model based on the linear layer after the 1st training stage. Meanwhile, will the filtered cc3m data be released?\r\n\r\nThanks a lot!</BODY>\n\n<COMMENTS>\n<Comment by WooKimm at 2023-04-20T06:46:21Z>\n+1\n</Comment>\n<Comment by jpWang at 2023-04-20T09:27:16Z>\n+1\n</Comment>\n<Comment by haotian-liu at 2023-04-20T16:50:20Z>\nHi @Richar-Du @WooKimm @jpWang , thank you for your interest in our work!\r\n\r\nOur pretraining dataset is released.  Please see [here](https://github.com/haotian-liu/LLaVA#pretraining-dataset).\r\n\r\nThe linear layer weights is also released.  Please see [here](https://github.com/haotian-liu/LLaVA#llava-pretrained-projector-weights).\n</Comment>\n<Comment by Richar-Du at 2023-04-21T07:15:48Z>\nGreat! Thanks a lot!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 20,
    "state": "closed",
    "created_by": "valine",
    "created_at": "2023-04-20T02:23:59Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/20</URL>\n\n<TITLE>Error running with --num-gpus 2</TITLE>\n\n<BODY>I'm trying to run the LLaVA on two RTX 4090 GPUs for inference. The model loads onto the GPUs without any issues, but an error occurs at inference time when I run the sample example from the Gradio web interface.\r\n\r\nHere is the error:\r\n\r\n`RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument tensors in method wrapper_CUDA_cat)`\r\n\r\nThe error seems to be caused by tensors being on different GPUs.\r\n\r\nEnvironment\r\n\r\n    OS: Ubuntu\r\n    Python version: 3.10\r\n    CUDA version: 11.8\r\n    GPU model: Dual RTX 4090s\r\n\r\nSteps to reproduce:\r\n\r\n    python -m llava.serve.controller --host 0.0.0.0 --port 10000\r\n    python3 -m llava.serve.model_worker --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path /home/lukas/Desktop/models/llava --num-gpus 2 --multi-modal\r\n    python -m llava.serve.gradio_web_server --controller http://localhost:10000\r\n    Run the sample example from the Gradio web interface</BODY>\n\n<COMMENTS>\n<Comment by valine at 2023-04-20T02:31:45Z>\nHere is the full log from running llava.serve.model_worker:\r\n\r\npython3 -m llava.serve.model_worker --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path /home/lukas/Desktop/models/llava --num-gpus 2 --multi-modal\r\n2023-04-19 21:08:00 | INFO | model_worker | args: Namespace(host='localhost', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='/home/lukas/Desktop/models/llava', model_name=None, multi_modal=True, keep_aspect_ratio=False, num_gpus=2, limit_model_concurrency=5, stream_interval=2, no_register=False)\r\n2023-04-19 21:08:00 | INFO | model_worker | Loading the model llava on worker 261b0e ...\r\n2023-04-19 21:08:01.135613: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-04-19 21:08:01.155617: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nSome weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'visual_projection.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_projection.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'logit_scale', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.layer_norm2.weight']\r\n- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\nLoading checkpoint shards:   0%|                          | 0/3 [00:00<?, ?it/s]\r\nLoading checkpoint shards:  33%|██████            | 1/3 [00:04<00:08,  4.09s/it]\r\nLoading checkpoint shards:  67%|████████████      | 2/3 [00:08<00:04,  4.09s/it]\r\nLoading checkpoint shards: 100%|██████████████████| 3/3 [00:10<00:00,  3.13s/it]\r\nLoading checkpoint shards: 100%|██████████████████| 3/3 [00:10<00:00,  3.39s/it]\r\n2023-04-19 21:08:13 | ERROR | stderr | \r\nSome weights of LlamaForCausalLM were not initialized from the model checkpoint at /home/lukas/Desktop/models/llava and are newly initialized: ['model.mm_projector.weight', 'model.mm_projector.bias']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\nSome weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'visual_projection.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_projection.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'logit_scale', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.layer_norm2.weight']\r\n- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n2023-04-19 21:08:15 | INFO | model_worker | Register to controller\r\n2023-04-19 21:08:15 | ERROR | stderr | INFO:     Started server process [167257]\r\n2023-04-19 21:08:15 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2023-04-19 21:08:15 | ERROR | stderr | INFO:     Application startup complete.\r\n2023-04-19 21:08:15 | ERROR | stderr | INFO:     Uvicorn running on http://localhost:40000 (Press CTRL+C to quit)\r\n2023-04-19 21:08:27 | INFO | stdout | INFO:     127.0.0.1:53996 - \"POST /worker_get_status HTTP/1.1\" 200 OK\r\n2023-04-19 21:08:45 | INFO | model_worker | Send heart beat. Models: ['llava']. Semaphore: None. global_counter: 0\r\n2023-04-19 21:09:09 | INFO | stdout | INFO:     127.0.0.1:58114 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\n2023-04-19 21:09:10 | ERROR | stderr | ERROR:    Exception in ASGI application\r\n2023-04-19 21:09:10 | ERROR | stderr | Traceback (most recent call last):\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 436, in run_asgi\r\n2023-04-19 21:09:10 | ERROR | stderr |     result = await app(  # type: ignore[func-returns-value]\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 78, in __call__\r\n2023-04-19 21:09:10 | ERROR | stderr |     return await self.app(scope, receive, send)\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/fastapi/applications.py\", line 270, in __call__\r\n2023-04-19 21:09:10 | ERROR | stderr |     await super().__call__(scope, receive, send)\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/starlette/applications.py\", line 124, in __call__\r\n2023-04-19 21:09:10 | ERROR | stderr |     await self.middleware_stack(scope, receive, send)\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 184, in __call__\r\n2023-04-19 21:09:10 | ERROR | stderr |     raise exc\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 162, in __call__\r\n2023-04-19 21:09:10 | ERROR | stderr |     await self.app(scope, receive, _send)\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\r\n2023-04-19 21:09:10 | ERROR | stderr |     raise exc\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\r\n2023-04-19 21:09:10 | ERROR | stderr |     await self.app(scope, receive, sender)\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 21, in __call__\r\n2023-04-19 21:09:10 | ERROR | stderr |     raise e\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\r\n2023-04-19 21:09:10 | ERROR | stderr |     await self.app(scope, receive, send)\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/starlette/routing.py\", line 706, in __call__\r\n2023-04-19 21:09:10 | ERROR | stderr |     await route.handle(scope, receive, send)\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/starlette/routing.py\", line 276, in handle\r\n2023-04-19 21:09:10 | ERROR | stderr |     await self.app(scope, receive, send)\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/starlette/routing.py\", line 69, in app\r\n2023-04-19 21:09:10 | ERROR | stderr |     await response(scope, receive, send)\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/starlette/responses.py\", line 266, in __call__\r\n2023-04-19 21:09:10 | ERROR | stderr |     async with anyio.create_task_group() as task_group:\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 662, in __aexit__\r\n2023-04-19 21:09:10 | ERROR | stderr |     raise exceptions[0]\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/starlette/responses.py\", line 269, in wrap\r\n2023-04-19 21:09:10 | ERROR | stderr |     await func()\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/starlette/responses.py\", line 258, in stream_response\r\n2023-04-19 21:09:10 | ERROR | stderr |     async for chunk in self.body_iterator:\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/starlette/concurrency.py\", line 63, in iterate_in_threadpool\r\n2023-04-19 21:09:10 | ERROR | stderr |     yield await anyio.to_thread.run_sync(_next, iterator)\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/anyio/to_thread.py\", line 31, in run_sync\r\n2023-04-19 21:09:10 | ERROR | stderr |     return await get_asynclib().run_sync_in_worker_thread(\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n2023-04-19 21:09:10 | ERROR | stderr |     return await future\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\r\n2023-04-19 21:09:10 | ERROR | stderr |     result = context.run(func, *args)\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/starlette/concurrency.py\", line 53, in _next\r\n2023-04-19 21:09:10 | ERROR | stderr |     return next(iterator)\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/Desktop/LLaVA/llava/serve/model_worker.py\", line 295, in generate_stream_gate\r\n2023-04-19 21:09:10 | ERROR | stderr |     for x in self.generate_stream(params):\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 35, in generator_context\r\n2023-04-19 21:09:10 | ERROR | stderr |     response = gen.send(None)\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/Desktop/LLaVA/llava/serve/model_worker.py\", line 234, in generate_stream\r\n2023-04-19 21:09:10 | ERROR | stderr |     out = model(\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-04-19 21:09:10 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2023-04-19 21:09:10 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/Desktop/LLaVA/transformers/src/transformers/models/llama/modeling_llama.py\", line 844, in forward\r\n2023-04-19 21:09:10 | ERROR | stderr |     outputs = self.model(\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-04-19 21:09:10 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2023-04-19 21:09:10 | ERROR | stderr |   File \"/home/lukas/Desktop/LLaVA/transformers/src/transformers/models/llama/modeling_llama.py\", line 631, in forward\r\n2023-04-19 21:09:10 | ERROR | stderr |     cur_new_input_embeds = torch.cat((cur_input_embeds[:image_start_token_pos+1], cur_image_features, cur_input_embeds[image_start_token_pos + num_patches + 1:]), dim=0)\r\n2023-04-19 21:09:10 | ERROR | stderr | RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument tensors in method wrapper_CUDA_cat)\n</Comment>\n<Comment by drawing-robot at 2023-04-20T05:22:41Z>\nsame problem here.\n</Comment>\n<Comment by haotian-liu at 2023-04-20T16:15:17Z>\nHi, thank you for your interest in our work.  We are working on the multiple GPU support, and will try to provide a solution today.  Thanks.\n</Comment>\n<Comment by valine at 2023-04-20T16:16:12Z>\n> Hi, thank you for your interest in our work. We are working on the multiple GPU support, and will try to provide a solution today. Thanks.\r\n\r\nThank you so much! Really excited for this project, you guys are doing great work.\n</Comment>\n<Comment by haotian-liu at 2023-04-20T22:55:16Z>\n@valine Hi, it has been fixed and our code now supports inference with multiple GPUs.\r\n\r\nI have tested it with 2x RTX 3090, and it works fine.  Note that you need to reinstall our latest fork of `transformers` in order for it to work.\r\n\r\nSee [here](https://github.com/haotian-liu/LLaVA#launch-a-model-worker-multiple-gpus-when-gpu-vram--24gb) for details.\n</Comment>\n<Comment by valine at 2023-04-20T23:28:34Z>\n@haotian-liu \r\n\r\nThanks for the quick update! I just tested it out, the model is loading and inference is working on my dual 4090s. \r\n\r\nI am however getting strange results from the model. The sample image and prompt of the man ironing on the back of a taxi returns a strange description. It's not gibberish so the language model seems to be working, but something seems off with the image embeddings maybe. \r\n\r\nA few outputs I got from the sample prompt:\r\n\r\n\"In the image, there is a person wearing a black cape and a black hat. The black cape is standing on a pile of clothes.\" \r\n\r\n\"In the image, there is a person wearing a black cape and holding a book. The scene is unusual because it features a large black dog wearing a cape in a room with a book and a bottle of wine. The dog is also holding a banana, which is an unexpected object to find in the image.\"\r\n\r\n\"The image depicts a scene where a man is using a cell phone to capture an interesting moment. However, the presence of the cell phone in the image is questionable, as it is placed in a unusual location and positioned at an unexpected angle. This combination of objects and scene elements suggest that the image has been staged with a fake cell phone placed in an unusual location, and a unclear context.\"\n</Comment>\n<Comment by haotian-liu at 2023-04-20T23:47:47Z>\nThis is what I got on my 2x RTX 3090s.\r\n\r\n\"The unusual aspect of this image is that a man is standing on a car in the middle of the street while doing his laundry. He has a portable ironing board set up on the roof of the car, and he is ironing clothes. This scene is particularly odd because it is not typical to see someone ironing clothes in the middle of a street, especially on top of a car. Such an activity can cause traffic disruptions and is generally considered unsafe and inappropriate for an urban setting.\"\r\n\r\nThe issue you are seeing may be due to some version difference in tokenizer. Can you try creating a completely new Conda environment and try running the [installation](https://github.com/haotian-liu/LLaVA#install) again?  This may solve the issue.\n</Comment>\n<Comment by valine at 2023-04-20T23:55:32Z>\nHmm no luck, when through the whole installation again, and this was the model response:\r\n\"The unusual aspect of this image is the presence of a language model with a \"funny\" in it. Typically, you would expect to see a combination of letters, numbers, and underscoresultimate. However, I can only see a few lines of code, as I am not capable of understanding or analyzing the image.\"\"\r\n\r\nSounds like the problem is something in my environment, thanks for your help.\n</Comment>\n<Comment by haotian-liu at 2023-04-20T23:57:38Z>\nHave you performed the model delta weight conversion?  It seems that the model is not understanding the image patches correctly.\r\n\r\nhttps://github.com/haotian-liu/LLaVA#llava-13b\n</Comment>\n<Comment by valine at 2023-04-21T00:01:25Z>\nYeah I applied the delta to the llama 13b v1 weights. \r\n\r\nThis is the content of my llama13b folder that I applied the weights to:\r\n\r\nconfig.json                      \r\npytorch_model.bin.index.json\r\ngeneration_config.json           \r\nspecial_tokens_map.json\r\npytorch_model-00001-of-00003.bin  \r\ntokenizer_config.json\r\npytorch_model-00002-of-00003.bin  \r\ntokenizer.model\r\npytorch_model-00003-of-00003.bin\r\n\r\nAnd the contents of the llava folder with the weights applied:\r\nadded_tokens.json                 \r\npytorch_model-00003-of-00003.bin\r\nconfig.json                       \r\npytorch_model.bin.index.json\r\ngeneration_config.json            \r\nspecial_tokens_map.json\r\npytorch_model-00001-of-00003.bin  \r\ntokenizer_config.json\r\npytorch_model-00002-of-00003.bin  \r\ntokenizer.model\n</Comment>\n<Comment by valine at 2023-04-21T00:13:58Z>\nOk I got it working properly now. I re-applied the delta to the v2 llama13b and made sure to have your custom version of transformers installed. \r\n\r\nThanks again for your help!\n</Comment>\n<Comment by haotian-liu at 2023-04-21T00:16:44Z>\nGlad to hear that it works!  You may close the issue :)\r\n\r\nOut of curiosity, can you please explain what is v1 v2 of llama-13b?  Thanks.\n</Comment>\n<Comment by valine at 2023-04-21T00:35:54Z>\nThere’s nothing different about the weights between the versions as I understand it, I’m not sure if v1 and v2 is the proper terms for it. There have been multiple versions of the hugging face weights floating around with a different config files. The weights I originally applied the deltas to I downloaded back in early March which seem incompatible with your deltas for whatever reason.\n</Comment>\n<Comment by haotian-liu at 2023-04-21T00:39:08Z>\nThanks for the explanation!  And hope you enjoy playing with our LLaVA.  Looking forward to more feedbacks :)\n</Comment>\n<Comment by penghe2021 at 2023-04-24T06:28:56Z>\nHi, great work, since it is the same question, I won't post another issue\r\n\r\nAfter having the correct version of transformer, I still get the error using multi gpus for inference: \r\n`RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:3!`\r\n\r\nENV:\r\nOS: Ubuntu\r\nPython version: 3.10\r\nCUDA version: 11.7\r\nGPU model: 4 x NVIDIA GeForce GTX 1080 Ti\r\n\r\nInstead of running the web demo, I am writing a small script to have it reading the local image. Running with one gpu will output meaningful answer, so I assume the weight is okay\r\n\r\nAny advice on how to solve this will be very helpful\n</Comment>\n<Comment by haotian-liu at 2023-04-24T06:38:04Z>\n@penghe2021 Can you paste the full error log here so that I can know which line is causing this issue (wrap them with ``` would make it easier to read)?  Thanks!\n</Comment>\n<Comment by penghe2021 at 2023-04-24T07:00:47Z>\nSure, here is the log\r\n\r\n```\r\ntotal gpu resources allocated: 1,2,3,4\r\nSome weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'visual_projection.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'logit_scale', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.embeddings.position_ids', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_projection.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight']\r\n- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\nNVIDIA GeForce GTX 1080 Ti\r\nNVIDIA GeForce GTX 1080 Ti\r\nNVIDIA GeForce GTX 1080 Ti\r\nNVIDIA GeForce GTX 1080 Ti\r\n\r\nLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\r\nLoading checkpoint shards:  33%|███▎      | 1/3 [00:16<00:32, 16.48s/it]\r\nLoading checkpoint shards:  67%|██████▋   | 2/3 [00:32<00:16, 16.24s/it]\r\nLoading checkpoint shards: 100%|██████████| 3/3 [00:42<00:00, 13.26s/it]\r\nLoading checkpoint shards: 100%|██████████| 3/3 [00:42<00:00, 14.09s/it]\r\nSome weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'visual_projection.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'logit_scale', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.embeddings.position_ids', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_projection.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight']\r\n- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\nTraceback (most recent call last):\r\n  File \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx/LLaVA/test/test01.py\", line 228, in <module>\r\n    output_string = model_worker.generate_reply(params)\r\n  File \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx/LLaVA/test/test01.py\", line 123, in generate_reply\r\n    out = model(\r\n  File \"/nas/home/phe/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/nas/home/phe/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/nas/home/phe/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 844, in forward\r\n    outputs = self.model(\r\n  File \"/nas/home/phe/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/nas/home/phe/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 617, in forward\r\n    cur_input_embeds = cur_input_embeds + (0. * dummy_image_features).sum()\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:3!\r\n```\r\n\r\n\r\nIn the test01.py line 123, is the same to this line https://github.com/haotian-liu/LLaVA/blob/2f439b5b019e8e7fe8b8147f05d4c71a079d65e4/llava/serve/model_worker.py#L232\r\n\r\nThanks for the help\n</Comment>\n<Comment by haotian-liu at 2023-04-24T07:32:58Z>\nI tried running this with multiple-gpu support worker (4 gpus to simulate your case, tried both text-only and multimodal input), it seems to be running fine.  Would you mind confirming this for me by running the demo this way? This way, we may be able to isolate the issue. \r\n\r\nAnd FYI, this is how we implemented the multiple GPU support in our worker: https://github.com/haotian-liu/LLaVA/blob/2f439b5b019e8e7fe8b8147f05d4c71a079d65e4/llava/serve/model_worker.py#L52-L59.\n</Comment>\n<Comment by penghe2021 at 2023-04-24T07:53:13Z>\nSorry, I cannot start the website on server, I will try running it on cpu\r\n\r\nYes, I use the same code for multi gpu implementation.\r\n\r\nThanks for the help\n</Comment>\n<Comment by tensorboy at 2023-05-02T03:53:38Z>\nsame problems with current transformers installed by:\r\n```\r\npip uninstall transformers\r\npip install git+https://github.com/huggingface/transformers@cae78c46\r\n```\r\nmy command:\r\n```\r\npython -m llava.serve.controller --host 0.0.0.0 --port 10000\r\n\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path /media/tensorboy/4df39736-fd86-443e-b6fe-a6ae0fe28d72/llama-dl-main/LLaVA-7B-v0 --multi-modal --num-gpus 2\r\n\r\npython -m llava.serve.gradio_web_server --controller http://localhost:10000 --share\r\n```\r\n\r\nlogs:\r\n\r\n```\r\n2023-05-01 20:46:05 | ERROR | stderr | INFO:     Started server process [3502503]\r\n2023-05-01 20:46:05 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2023-05-01 20:46:05 | ERROR | stderr | INFO:     Application startup complete.\r\n2023-05-01 20:46:05 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)\r\n2023-05-01 20:46:20 | INFO | model_worker | Send heart beat. Models: ['LLaVA-7B-v0']. Semaphore: None. global_counter: 0\r\n2023-05-01 20:46:35 | INFO | model_worker | Send heart beat. Models: ['LLaVA-7B-v0']. Semaphore: None. global_counter: 0\r\n2023-05-01 20:46:50 | INFO | model_worker | Send heart beat. Models: ['LLaVA-7B-v0']. Semaphore: None. global_counter: 0\r\n2023-05-01 20:47:05 | INFO | model_worker | Send heart beat. Models: ['LLaVA-7B-v0']. Semaphore: None. global_counter: 0\r\n2023-05-01 20:47:20 | INFO | model_worker | Send heart beat. Models: ['LLaVA-7B-v0']. Semaphore: None. global_counter: 0\r\n2023-05-01 20:47:35 | INFO | model_worker | Send heart beat. Models: ['LLaVA-7B-v0']. Semaphore: None. global_counter: 0\r\n2023-05-01 20:47:48 | INFO | stdout | INFO:     127.0.0.1:56932 - \"POST /worker_get_status HTTP/1.1\" 200 OK\r\n2023-05-01 20:47:50 | INFO | model_worker | Send heart beat. Models: ['LLaVA-7B-v0']. Semaphore: None. global_counter: 0\r\n2023-05-01 20:48:05 | INFO | model_worker | Send heart beat. Models: ['LLaVA-7B-v0']. Semaphore: None. global_counter: 0\r\n2023-05-01 20:48:20 | INFO | model_worker | Send heart beat. Models: ['LLaVA-7B-v0']. Semaphore: None. global_counter: 0\r\n2023-05-01 20:48:25 | INFO | model_worker | Send heart beat. Models: ['LLaVA-7B-v0']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n2023-05-01 20:48:25 | INFO | stdout | INFO:     127.0.0.1:57470 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\n2023-05-01 20:48:27 | ERROR | stderr | ERROR:    Exception in ASGI application\r\n2023-05-01 20:48:27 | ERROR | stderr | Traceback (most recent call last):\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py\", line 428, in run_asgi\r\n2023-05-01 20:48:27 | ERROR | stderr |     result = await app(  # type: ignore[func-returns-value]\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 78, in __call__\r\n2023-05-01 20:48:27 | ERROR | stderr |     return await self.app(scope, receive, send)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/fastapi/applications.py\", line 276, in __call__\r\n2023-05-01 20:48:27 | ERROR | stderr |     await super().__call__(scope, receive, send)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/applications.py\", line 122, in __call__\r\n2023-05-01 20:48:27 | ERROR | stderr |     await self.middleware_stack(scope, receive, send)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 184, in __call__\r\n2023-05-01 20:48:27 | ERROR | stderr |     raise exc\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 162, in __call__\r\n2023-05-01 20:48:27 | ERROR | stderr |     await self.app(scope, receive, _send)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\r\n2023-05-01 20:48:27 | ERROR | stderr |     raise exc\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\r\n2023-05-01 20:48:27 | ERROR | stderr |     await self.app(scope, receive, sender)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 21, in __call__\r\n2023-05-01 20:48:27 | ERROR | stderr |     raise e\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\r\n2023-05-01 20:48:27 | ERROR | stderr |     await self.app(scope, receive, send)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/routing.py\", line 718, in __call__\r\n2023-05-01 20:48:27 | ERROR | stderr |     await route.handle(scope, receive, send)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/routing.py\", line 276, in handle\r\n2023-05-01 20:48:27 | ERROR | stderr |     await self.app(scope, receive, send)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/routing.py\", line 69, in app\r\n2023-05-01 20:48:27 | ERROR | stderr |     await response(scope, receive, send)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/responses.py\", line 270, in __call__\r\n2023-05-01 20:48:27 | ERROR | stderr |     async with anyio.create_task_group() as task_group:\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 662, in __aexit__\r\n2023-05-01 20:48:27 | ERROR | stderr |     raise exceptions[0]\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/responses.py\", line 273, in wrap\r\n2023-05-01 20:48:27 | ERROR | stderr |     await func()\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/responses.py\", line 262, in stream_response\r\n2023-05-01 20:48:27 | ERROR | stderr |     async for chunk in self.body_iterator:\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/concurrency.py\", line 63, in iterate_in_threadpool\r\n2023-05-01 20:48:27 | ERROR | stderr |     yield await anyio.to_thread.run_sync(_next, iterator)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/anyio/to_thread.py\", line 31, in run_sync\r\n2023-05-01 20:48:27 | ERROR | stderr |     return await get_asynclib().run_sync_in_worker_thread(\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n2023-05-01 20:48:27 | ERROR | stderr |     return await future\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\r\n2023-05-01 20:48:27 | ERROR | stderr |     result = context.run(func, *args)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/concurrency.py\", line 53, in _next\r\n2023-05-01 20:48:27 | ERROR | stderr |     return next(iterator)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/LLaVA/llava/serve/model_worker.py\", line 292, in generate_stream_gate\r\n2023-05-01 20:48:27 | ERROR | stderr |     for x in self.generate_stream(params):\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 43, in generator_context\r\n2023-05-01 20:48:27 | ERROR | stderr |     response = gen.send(None)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/LLaVA/llava/serve/model_worker.py\", line 239, in generate_stream\r\n2023-05-01 20:48:27 | ERROR | stderr |     out = model(\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\r\n2023-05-01 20:48:27 | ERROR | stderr |     return forward_call(*input, **kwargs)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2023-05-01 20:48:27 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/LLaVA/llava/model/llava.py\", line 218, in forward\r\n2023-05-01 20:48:27 | ERROR | stderr |     outputs = self.model(\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\r\n2023-05-01 20:48:27 | ERROR | stderr |     return forward_call(*input, **kwargs)\r\n2023-05-01 20:48:27 | ERROR | stderr |   File \"/home/tensorboy/LLaVA/llava/model/llava.py\", line 159, in forward\r\n2023-05-01 20:48:27 | ERROR | stderr |     cur_new_input_embeds = torch.cat((cur_input_embeds[:image_start_token_pos+1], cur_image_features, cur_input_embeds[image_start_token_pos + num_patches + 1:]), dim=0)\r\n2023-05-01 20:48:27 | ERROR | stderr | RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument tensors in method wrapper_cat)\r\n\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 19,
    "state": "closed",
    "created_by": "Richar-Du",
    "created_at": "2023-04-20T02:07:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/19</URL>\n\n<TITLE>Question about the object detection</TITLE>\n\n<BODY>When encoding the image to prompt, you mentioned *captions* and *bounding boxes*, I wonder which object detection model you utilized to generate the bounding boxes?</BODY>\n\n<COMMENTS>\n<Comment by wanxinzzz at 2023-04-20T03:27:24Z>\n> When encoding the image to prompt, you mentioned _captions_ and _bounding boxes_, I wonder which object detection model you utilized to generate the bounding boxes?\r\n\r\nI think the bounding boxes come from ground truth in coco dataset\n</Comment>\n<Comment by haotian-liu at 2023-04-21T00:43:09Z>\nHi @Richar-Du both annotations come from the original COCO dataset: captions from coco-caption-2014 annotation, and boxes from coco-instances-2014 annotations.\r\n\r\nThanks @wanxinzzz for answering!\n</Comment>\n<Comment by Richar-Du at 2023-04-21T07:13:53Z>\nGot it, thanks for your explanations :)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 18,
    "state": "closed",
    "created_by": "plmsuper8",
    "created_at": "2023-04-20T00:22:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/18</URL>\n\n<TITLE>Is it possible to run at CPU？</TITLE>\n\n<BODY>Thanks for the great work.\r\n\r\nI have a server with large RAM but no GPU, and another 16G VRAM local PC. Unfortunately，both of them seem insufficient.\r\n\r\nSorry, but I’m a newbie for this. When I try to modify the code (remove .cuda(), and set device=cpu), it crashes . \r\nI also tried CLI (cpu only), it's working but not multimodal. \r\n\r\nBesides, is it support load_in_8bits or quant to 4bit like other LLama based model? Thanks again!</BODY>\n\n<COMMENTS>\n<Comment by satyajitghana at 2023-04-20T06:50:08Z>\nIt's possible to run on CPU, you'll need >=60GB of RAM, and i tried FP16, didn't work, some layers don't have half float implementation, so had to use FP32.\n</Comment>\n<Comment by haotian-liu at 2023-05-01T04:02:24Z>\nHi, I am closing this issue, due to the inactivity.  Hope your problem has been resolved.  If you have further concerns, please feel free to re-open or open a new issue.  Thanks!\n</Comment>\n<Comment by copperwiring at 2023-05-11T08:27:06Z>\n@haotian-liu I think the inference by defaults expects a gpu with .device(cuda). Can we add cpu option (something like if gpu is absent use cuda) and mention in the readme how much ram cpu inference would take. A cu flag would be relly helpful for wider usage.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 17,
    "state": "closed",
    "created_by": "tensorboy",
    "created_at": "2023-04-19T20:53:12Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/17</URL>\n\n<TITLE>Maybe A bug</TITLE>\n\n<BODY>https://github.com/haotian-liu/LLaVA/blob/main/llava/conversation.py#L96\r\n\r\nThe parameter of resize of pil  is (width, height) instead of (H, W) at your code:\r\n<img width=\"614\" alt=\"Screen Shot 2023-04-19 at 1 52 11 PM\" src=\"https://user-images.githubusercontent.com/17189055/233197388-55dbb920-74fe-4857-92c7-b9ad520e8873.png\">\r\n\r\nreference: https://pillow.readthedocs.io/en/stable/reference/Image.html</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-20T16:59:16Z>\nThank you for pointing this out! Yes, it is indeed a mistake.\r\n\r\nHowever, coincidentally it does not become a bug and becomes a naming error instead, as we obtained the width and height also using the PyTorch convension: `height, width = image.size`.\r\n\r\nI will swap the `height` and `width` and seems like we're good to go?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 16,
    "state": "closed",
    "created_by": "as3959",
    "created_at": "2023-04-19T17:27:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/16</URL>\n\n<TITLE>Delta to Llama conversion on PaperSpace Gradient or Google Collab?</TITLE>\n\n<BODY>Great project! Is there anyway to run the conversion, to get the actual LLava model on PaperSpace Gradient or Google Collab? My project machine has an Nvidia RTX A4500, with 8 Cores, but I only have 16 GB of CPU ram.</BODY>\n\n<COMMENTS>\n<Comment by as3959 at 2023-04-19T19:38:06Z>\nUpdate: I was surprised to see that the conversion made it all the way through on my machine. Many times, I was at or near 99% memory usage, but I think my machine was using page file or virtual memory to manage the load. It probably took about 30 minutes, between downloads, applying the delta, and saving, but it worked.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 15,
    "state": "closed",
    "created_by": "Sequential-circuits",
    "created_at": "2023-04-19T11:32:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/15</URL>\n\n<TITLE>AttributeError: 'LlamaModel' object has no attribute 'vision_tower'</TITLE>\n\n<BODY>Whenever I run python -m llava.serve.model_worker --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path LLaVA-13B-v0 --multi-moda I get \r\n**_AttributeError: 'LlamaModel' object has no attribute 'vision_tower'_**</BODY>\n\n<COMMENTS>\n<Comment by 152334H at 2023-04-19T14:02:10Z>\n> NOTE: In this research preview, we used a modified version of huggingface/transformers library to support multimodal models and the LLaMA tokenizer. Make sure that you are using the correct transformers library from https://github.com/haotian-liu/transformers_llava.\n</Comment>\n<Comment by tekntrash at 2023-04-19T14:28:42Z>\nI installed it and same error\n</Comment>\n<Comment by haotian-liu at 2023-04-19T15:37:23Z>\n@Sequential-circuits @tekntrash Can you share the full error message here?  This shall be solved by installing the correct transformers package.\n</Comment>\n<Comment by tekntrash at 2023-04-19T15:49:32Z>\n(base) ***@***.***:~/LLaVA# python -m llava.serve.model_worker --controller \nhttp://localhost:10000 --port 40000 --worker http://localhost:40000 \n--model-path ./LLaVA-13B-v0 --multi-modal\n2023-04-19 15:47:30 | INFO | model_worker | args: \nNamespace(host='localhost', port=40000, \nworker_address='http://localhost:40000', \ncontroller_address='http://localhost:10000', \nmodel_path='./LLaVA-13B-v0', model_name=None, multi_modal=True, \nkeep_aspect_ratio=False, num_gpus=1, limit_model_concurrency=5, \nstream_interval=2, no_register=False)\n2023-04-19 15:47:30 | INFO | model_worker | Loading the model \nLLaVA-13B-v0 on worker 6c1a9a ...\n2023-04-19 15:47:30.703901: I \ntensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow \nbinary is optimized with oneAPI Deep Neural Network Library (oneDNN) to \nuse the following CPU instructions in performance-critical operations:  \nSSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the \nappropriate compiler flags.\n2023-04-19 15:47:32 | INFO | numexpr.utils | Note: NumExpr detected 16 \ncores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n2023-04-19 15:47:32 | INFO | numexpr.utils | NumExpr defaulting to 8 \nthreads.\nLoading checkpoint shards: 0%| | 0/3 [00:00<?, ?it/s]\nLoading checkpoint shards: 33%|███████████████████████ | 1/3 \n[00:05<00:10,  5.14s/it]\nLoading checkpoint shards: \n67%|██████████████████████████████████████████████ | 2/3 [00:16<00:09,  \n9.05s/it]\nLoading checkpoint shards: \n100%|█████████████████████████████████████████████████████████████████████| \n3/3 [00:28<00:00, 10.20s/it]\nLoading checkpoint shards: \n100%|█████████████████████████████████████████████████████████████████████| \n3/3 [00:28<00:00,  9.50s/it]\n2023-04-19 15:48:01 | ERROR | stderr |\n2023-04-19 15:48:01 | ERROR | stderr | ╭─────────────────────────────── \nTraceback (most recent call last) ────────────────────────────────╮\n2023-04-19 15:48:01 | ERROR | stderr | │ \n/root/anaconda3/lib/python3.10/runpy.py:196 in \n_run_module_as_main                               │\n2023-04-19 15:48:01 | ERROR | stderr | │ │\n2023-04-19 15:48:01 | ERROR | stderr | │   193 │   main_globals = \nsys.modules[\"__main__\"].__dict__ │\n2023-04-19 15:48:01 | ERROR | stderr | │   194 │   if alter_argv: │\n2023-04-19 15:48:01 | ERROR | stderr | │   195 │   │   sys.argv[0] = \nmod_spec.origin │\n2023-04-19 15:48:01 | ERROR | stderr | │ ❱ 196 │   return \n_run_code(code, main_globals, \nNone,                                             │\n2023-04-19 15:48:01 | ERROR | stderr | │   197 │   │   │   │ │    \n\"__main__\", mod_spec)                                                 │\n2023-04-19 15:48:01 | ERROR | stderr | │ 198 │\n2023-04-19 15:48:01 | ERROR | stderr | │   199 def run_module(mod_name, \ninit_globals=None, │\n2023-04-19 15:48:01 | ERROR | stderr | │ │\n2023-04-19 15:48:01 | ERROR | stderr | │ \n/root/anaconda3/lib/python3.10/runpy.py:86 in \n_run_code                                          │\n2023-04-19 15:48:01 | ERROR | stderr | │ │\n2023-04-19 15:48:01 | ERROR | stderr | │    83 │   │   │   │ │      \n__loader__ = loader,                                                │\n2023-04-19 15:48:01 | ERROR | stderr | │    84 │   │   │   │ │      \n__package__ = pkg_name,                                             │\n2023-04-19 15:48:01 | ERROR | stderr | │    85 │   │   │   │ │      \n__spec__ = mod_spec)                                                │\n2023-04-19 15:48:01 | ERROR | stderr | │ ❱  86 │   exec(code, run_globals) │\n2023-04-19 15:48:01 | ERROR | stderr | │    87 │   return run_globals │\n2023-04-19 15:48:01 | ERROR | stderr | │ 88 │\n2023-04-19 15:48:01 | ERROR | stderr | │    89 def \n_run_module_code(code, \ninit_globals=None,                                              │\n2023-04-19 15:48:01 | ERROR | stderr | │ │\n2023-04-19 15:48:01 | ERROR | stderr | │ \n/root/LLaVA/llava/serve/model_worker.py:361 in \n<module>                                          │\n2023-04-19 15:48:01 | ERROR | stderr | │ │\n2023-04-19 15:48:01 | ERROR | stderr | │   358 │   args = \nparser.parse_args() │\n2023-04-19 15:48:01 | ERROR | stderr | │   359 │ logger.info(f\"args: \n{args}\") │\n2023-04-19 15:48:01 | ERROR | stderr | │   360 │ │\n2023-04-19 15:48:01 | ERROR | stderr | │ ❱ 361 │   worker = \nModelWorker(args.controller_address, │\n2023-04-19 15:48:01 | ERROR | stderr | │   362 │   │   │   │   │ │ \nargs.worker_address, │\n2023-04-19 15:48:01 | ERROR | stderr | │   363 │   │   │   │   │ │ \nworker_id, │\n2023-04-19 15:48:01 | ERROR | stderr | │   364 │   │   │   │   │ │ \nargs.no_register, │\n2023-04-19 15:48:01 | ERROR | stderr | │ │\n2023-04-19 15:48:01 | ERROR | stderr | │ \n/root/LLaVA/llava/serve/model_worker.py:118 in \n__init__                                          │\n2023-04-19 15:48:01 | ERROR | stderr | │ │\n2023-04-19 15:48:01 | ERROR | stderr | │   115 │   │ \nlogger.info(f\"Loading the model {self.model_name} on worker {worker_id} \n...\")      │\n2023-04-19 15:48:01 | ERROR | stderr | │   116 │   │ self.is_multi_modal \n= is_multi_modal                                               │\n2023-04-19 15:48:01 | ERROR | stderr | │   117 │   │ \nself.keep_aspect_ratio = \nkeep_aspect_ratio                                         │\n2023-04-19 15:48:01 | ERROR | stderr | │ ❱ 118 │   │ self.tokenizer, \nself.model, self.image_processor, self.context_len = load_model(   │\n2023-04-19 15:48:01 | ERROR | stderr | │   119 │   │   │ model_path, \nnum_gpus, is_multi_modal)                                          │\n2023-04-19 15:48:01 | ERROR | stderr | │   120 │ │ │\n2023-04-19 15:48:01 | ERROR | stderr | │   121 │   │   if not no_register: │\n2023-04-19 15:48:01 | ERROR | stderr | │ │\n2023-04-19 15:48:01 | ERROR | stderr | │ \n/root/LLaVA/llava/serve/model_worker.py:72 in \nload_model                                         │\n2023-04-19 15:48:01 | ERROR | stderr | │ │\n2023-04-19 15:48:01 | ERROR | stderr | │    69 │   │   if \nmm_use_im_start_end: │\n2023-04-19 15:48:01 | ERROR | stderr | │    70 │   │   │ \ntokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], \nspecial   │\n2023-04-19 15:48:01 | ERROR | stderr | │    71 │ │ │\n2023-04-19 15:48:01 | ERROR | stderr | │ ❱  72 │   │ vision_tower = \nmodel.model.vision_tower[0] │\n2023-04-19 15:48:01 | ERROR | stderr | │    73 │   │   if \nvision_tower.device.type == \n'meta':                                             │\n2023-04-19 15:48:01 | ERROR | stderr | │    74 │   │   │ vision_tower = \nCLIPVisionModel.from_pretrained(vision_tower.config._name_or_   │\n2023-04-19 15:48:01 | ERROR | stderr | │    75 │   │   │ \nmodel.model.vision_tower[0] = \nvision_tower                                     │\n2023-04-19 15:48:01 | ERROR | stderr | │ │\n2023-04-19 15:48:01 | ERROR | stderr | │ \n/root/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1614 \nin __getattr__      │\n2023-04-19 15:48:01 | ERROR | stderr | │ │\n2023-04-19 15:48:01 | ERROR | stderr | │   1611 │   │   │ modules = \nself.__dict__['_modules'] │\n2023-04-19 15:48:01 | ERROR | stderr | │   1612 │   │   │   if name in \nmodules: │\n2023-04-19 15:48:01 | ERROR | stderr | │   1613 │   │   │   │ return \nmodules[name] │\n2023-04-19 15:48:01 | ERROR | stderr | │ ❱ 1614 │   │   raise \nAttributeError(\"'{}' object has no attribute \n'{}'\".format(                  │\n2023-04-19 15:48:01 | ERROR | stderr | │   1615 │   │   │ \ntype(self).__name__, \nname))                                                   │\n2023-04-19 15:48:01 | ERROR | stderr | │   1616 │ │\n2023-04-19 15:48:01 | ERROR | stderr | │   1617 │   def \n__setattr__(self, name: str, value: Union[Tensor, 'Module']) -> \nNone:             │\n2023-04-19 15:48:01 | ERROR | stderr | \n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n2023-04-19 15:48:01 | ERROR | stderr | AttributeError: 'LlamaModel' \nobject has no attribute 'vision_tower'\n\nOn 19/04/2023 16:37, Haotian Liu wrote:\n>\n> @Sequential-circuits <https://github.com/Sequential-circuits> \n> @tekntrash <https://github.com/tekntrash> Can you share the full error \n> message here? This shall be solved by installing the correct \n> transformers package.\n>\n> —\n> Reply to this email directly, view it on GitHub \n> <https://github.com/haotian-liu/LLaVA/issues/15#issuecomment-1514952453>, \n> or unsubscribe \n> <https://github.com/notifications/unsubscribe-auth/AYL4CNUHT6EKPCYPYTKOGVLXCABD5ANCNFSM6AAAAAAXD5ZYLE>.\n> You are receiving this because you were mentioned.Message ID: \n> ***@***.***>\n>\n-- \n\nAl Costa\nCEO\n37th Floor, 1 Canada Square, Canary Wharf, London\nUnited Kingdom, E145 AA\nTel: +44 1737669662\nMob: +44 7892928973\nMail: ***@***.***\nWeb: http://www.tekntrash.ai\nBefore printing this message, Be sure it is necessary.\n</Comment>\n<Comment by haotian-liu at 2023-04-19T15:55:25Z>\nThis is strange.  Can you share the `config.json` under your converted LLaVA model folder? More specifically, do you see these lines\r\n```\r\n  \"mm_hidden_size\": 1024,\r\n  \"mm_use_im_start_end\": true,\r\n  \"mm_vision_select_layer\": -2,\r\n  \"mm_vision_tower\": \"openai/clip-vit-large-patch14\",\r\n```\n</Comment>\n<Comment by tekntrash at 2023-04-19T15:58:46Z>\nThere you go\n\nOut of curiosity, what is that vision tower thing?\n\n\n{\n   \"_name_or_path\": \"liuhaotian/LLaVA-13b-delta-v0\",\n   \"architectures\": [\n     \"LlamaForCausalLM\"\n   ],\n   \"bos_token_id\": 0,\n   \"eos_token_id\": 1,\n   \"hidden_act\": \"silu\",\n   \"hidden_size\": 5120,\n   \"initializer_range\": 0.02,\n   \"intermediate_size\": 13824,\n   \"max_sequence_length\": 2048,\n   \"mm_hidden_size\": 1024,\n   \"mm_use_im_start_end\": true,\n   \"mm_vision_select_layer\": -2,\n   \"mm_vision_tower\": \"openai/clip-vit-large-patch14\",\n   \"model_type\": \"llama\",\n   \"num_attention_heads\": 40,\n   \"num_hidden_layers\": 40,\n   \"pad_token_id\": -1,\n   \"rms_norm_eps\": 1e-06,\n   \"tie_word_embeddings\": false,\n   \"torch_dtype\": \"float16\",\n   \"transformers_version\": \"4.28.0.dev0\",\n   \"tune_mm_mlp_adapter\": false,\n   \"use_cache\": true,\n   \"use_mm_proj\": true,\n   \"vocab_size\": 32003\n}\n\nOn 19/04/2023 16:55, Haotian Liu wrote:\n>\n> This is strange. Can you share the |config.json| under your converted \n> LLaVA model folder? More specifically, do you see these lines\n>\n> |\"mm_hidden_size\": 1024, \"mm_use_im_start_end\": true, \n> \"mm_vision_select_layer\": -2, \"mm_vision_tower\": \n> \"openai/clip-vit-large-patch14\", |\n>\n> —\n> Reply to this email directly, view it on GitHub \n> <https://github.com/haotian-liu/LLaVA/issues/15#issuecomment-1514977958>, \n> or unsubscribe \n> <https://github.com/notifications/unsubscribe-auth/AYL4CNXBORYJBPLWM63FRF3XCADHRANCNFSM6AAAAAAXD5ZYLE>.\n> You are receiving this because you were mentioned.Message ID: \n> ***@***.***>\n>\n-- \n\nAl Costa\nCEO\n37th Floor, 1 Canada Square, Canary Wharf, London\nUnited Kingdom, E145 AA\nTel: +44 1737669662\nMob: +44 7892928973\nMail: ***@***.***\nWeb: http://www.tekntrash.ai\nBefore printing this message, Be sure it is necessary.\n</Comment>\n<Comment by haotian-liu at 2023-04-19T16:03:38Z>\nThe vision tower is the CLIP vision encoder in our architecture to support multimodal modeling.  In this first version of research preview, we modified the `transformers` code base so you need to install the `transformers` from our repo: https://github.com/haotian-liu/transformers_llava.\r\n\r\nIt is reference [here](https://github.com/haotian-liu/transformers_llava/blob/main/src/transformers/models/llama/modeling_llama.py#L464-L466), so since you have the arguments in the config, it shall load successfully if you have installed our `transformers` package.\r\n\r\nCan you please try reinstalling the transformers library by running the following command:\r\n```\r\npip install git+https://github.com/haotian-liu/transformers_llava.git@26356f0d07bacfb3857dafc7f8a519304b4c0572\r\n```\r\n\r\nThanks!\n</Comment>\n<Comment by tekntrash at 2023-04-19T16:13:05Z>\nJust ran|pip install \n***@***.*** \nand same error|\n\n|Below the printscreen if it helps\n|\n\n|Sorry to be a pain in the ass and probably my questions are dumb as \nhell, but be aware a lot of people want to jump in the chatgpt bandwagon \nso be prepared for a lot of noobs complaining they can't turn on the \ncomputer :D\n|\n\n||\n\nOn 19/04/2023 17:03, Haotian Liu wrote:\n>\n> The vision tower is the CLIP vision encoder in our architecture to \n> support multimodal modeling. In this first version of research \n> preview, we modified the |transformers| code base so you need to \n> install the |transformers| from our repo: \n> https://github.com/haotian-liu/transformers_llava.\n>\n> It is reference here \n> <https://github.com/haotian-liu/transformers_llava/blob/main/src/transformers/models/llama/modeling_llama.py#L464-L466>, \n> so since you have the arguments in the config, it shall load \n> successfully if you have installed our |transformers| package.\n>\n> Can you please try reinstalling the transformers library by running \n> the following command:\n>\n> |pip install \n> ***@***.*** \n> |\n>\n> Thanks!\n>\n> —\n> Reply to this email directly, view it on GitHub \n> <https://github.com/haotian-liu/LLaVA/issues/15#issuecomment-1514990373>, \n> or unsubscribe \n> <https://github.com/notifications/unsubscribe-auth/AYL4CNSHY4QB35XJTEX254LXCAEGJANCNFSM6AAAAAAXD5ZYLE>.\n> You are receiving this because you were mentioned.Message ID: \n> ***@***.***>\n>\n-- \n\nAl Costa\nCEO\n37th Floor, 1 Canada Square, Canary Wharf, London\nUnited Kingdom, E145 AA\nTel: +44 1737669662\nMob: +44 7892928973\nMail: ***@***.***\nWeb: http://www.tekntrash.ai\nBefore printing this message, Be sure it is necessary.\n</Comment>\n<Comment by haotian-liu at 2023-04-19T16:30:05Z>\nHi @tekntrash No worries at all! We want to make sure that the current implementation is easy to use -- you are helping us to make this happen!\r\n\r\nI cannot see the screenshot though.  If the screenshot does not work, can you paste the raw message here?  Better wrap with ``` so that it is more well formatted and easier to read for me.\r\n\r\nAnother option that we can do to troubleshoot is to clone the transformers and reinstall:\r\n\r\nUnder LLaVA code base, run the following.  And please paste the logs as well.\r\n```\r\ngit clone https://github.com/haotian-liu/transformers_llava.git transformers\r\npip install -e ./transformers\r\n```\n</Comment>\n<Comment by tekntrash at 2023-04-19T16:41:15Z>\nok now I think it is working: it ran out of memory but that I can handle :D\n\nBelow the trace for both the installation of the transformers library \nand running the thing\n\nAnd thanks really for the help and glad to be helping you too: feel free \nto connect over linkedin at https://www.linkedin.com/in/alcosta01/ or to \nping me in my whatsapp +44 7892928973, and if you come to London I \ninvite you for a beer\n\n\n(base) ***@***.***:~# git clone \nhttps://github.com/haotian-liu/transformers_llava.git transformers\npip install -e ./transformers\nCloning into 'transformers'...\nremote: Enumerating objects: 124725, done.\nremote: Counting objects: 100% (9/9), done.\nremote: Compressing objects: 100% (7/7), done.\nremote: Total 124725 (delta 1), reused 5 (delta 1), pack-reused 124716\nReceiving objects: 100% (124725/124725), 125.66 MiB | 25.39 MiB/s, done.\nResolving deltas: 100% (94085/94085), done.\nObtaining file:///root/transformers\n   Installing build dependencies ... done\n   Checking if build backend supports build_editable ... done\n   Getting requirements to build editable ... done\n   Preparing editable metadata (pyproject.toml) ... done\nRequirement already satisfied: filelock in \n./anaconda3/lib/python3.10/site-packages (from \ntransformers==4.28.0.dev0) (3.9.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in \n./anaconda3/lib/python3.10/site-packages (from \ntransformers==4.28.0.dev0) (0.13.3)\nRequirement already satisfied: numpy>=1.17 in \n./anaconda3/lib/python3.10/site-packages (from \ntransformers==4.28.0.dev0) (1.21.6)\nRequirement already satisfied: packaging>=20.0 in \n./anaconda3/lib/python3.10/site-packages (from \ntransformers==4.28.0.dev0) (22.0)\nRequirement already satisfied: pyyaml>=5.1 in \n./anaconda3/lib/python3.10/site-packages (from \ntransformers==4.28.0.dev0) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in \n./anaconda3/lib/python3.10/site-packages (from \ntransformers==4.28.0.dev0) (2017.4.5)\nRequirement already satisfied: requests in \n./anaconda3/lib/python3.10/site-packages (from \ntransformers==4.28.0.dev0) (2.27.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in \n./anaconda3/lib/python3.10/site-packages (from \ntransformers==4.28.0.dev0) (0.12.1)\nRequirement already satisfied: tqdm>=4.27 in \n./anaconda3/lib/python3.10/site-packages (from \ntransformers==4.28.0.dev0) (4.64.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in \n./anaconda3/lib/python3.10/site-packages (from \nhuggingface-hub<1.0,>=0.11.0->transformers==4.28.0.dev0) (4.4.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in \n./anaconda3/lib/python3.10/site-packages (from \nrequests->transformers==4.28.0.dev0) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in \n./anaconda3/lib/python3.10/site-packages (from \nrequests->transformers==4.28.0.dev0) (2022.12.7)\nRequirement already satisfied: charset-normalizer~=2.0.0 in \n./anaconda3/lib/python3.10/site-packages (from \nrequests->transformers==4.28.0.dev0) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in \n./anaconda3/lib/python3.10/site-packages (from \nrequests->transformers==4.28.0.dev0) (3.4)\nBuilding wheels for collected packages: transformers\n   Building editable for transformers (pyproject.toml) ... done\n   Created wheel for transformers: \nfilename=transformers-4.28.0.dev0-0.editable-py3-none-any.whl size=35072 \nsha256=39e7c8c031e544b6d921021aa7a8966069161c58bcad2aa1aeb696c4baf058be\n   Stored in directory: \n/tmp/pip-ephem-wheel-cache-ja8m1x14/wheels/cc/64/f7/a67713e0143d17a61a8c81af64dffa96f04d6602a4e4d50e71\nSuccessfully built transformers\nInstalling collected packages: transformers\n   Attempting uninstall: transformers\n     Found existing installation: transformers 4.28.0.dev0\n     Uninstalling transformers-4.28.0.dev0:\n       Successfully uninstalled transformers-4.28.0.dev0\nSuccessfully installed transformers-4.28.0.dev0\nWARNING: Running pip as the 'root' user can result in broken permissions \nand conflicting behaviour with the system package manager. It is \nrecommended to use a virtual environment instead: \nhttps://pip.pypa.io/warnings/venv\n(base) ***@***.***:~# cd LLaVA\n(base) ***@***.***:~/LLaVA# python -m llava.serve.model_worker --controller \nhttp://localhost:10000 --port 40000 --worker http://localhost:40000 \n--model-path ./LLaVA-13B-v0 --multi-modal\n2023-04-19 16:35:32 | INFO | model_worker | args: \nNamespace(host='localhost', port=40000, \nworker_address='http://localhost:40000', \ncontroller_address='http://localhost:10000', \nmodel_path='./LLaVA-13B-v0', model_name=None, multi_modal=True, \nkeep_aspect_ratio=False, num_gpus=1, limit_model_concurrency=5, \nstream_interval=2, no_register=False)\n2023-04-19 16:35:32 | INFO | model_worker | Loading the model \nLLaVA-13B-v0 on worker 5517a8 ...\n2023-04-19 16:35:33.600890: I \ntensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow \nbinary is optimized with oneAPI Deep Neural Network Library (oneDNN) to \nuse the following CPU instructions in performance-critical operations:  \nSSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the \nappropriate compiler flags.\n2023-04-19 16:35:34 | INFO | numexpr.utils | Note: NumExpr detected 16 \ncores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n2023-04-19 16:35:34 | INFO | numexpr.utils | NumExpr defaulting to 8 \nthreads.\nDownloading (…)lve/main/config.json: \n0%|                                                             | \n0.00/4.52k [00:00<?, ?B/s]\nDownloading (…)lve/main/config.json: \n100%|████████████████████████████████████████████████████| 4.52k/4.52k \n[00:00<00:00, 5.54MB/s]\n2023-04-19 16:35:36 | ERROR | stderr |\nDownloading pytorch_model.bin: 0%| | 0.00/1.71G [00:00<?, ?B/s]\nDownloading pytorch_model.bin: \n1%|▋                                                          | \n21.0M/1.71G [00:00<00:14, 113MB/s]\nDownloading pytorch_model.bin: \n2%|█▍                                                         | \n41.9M/1.71G [00:00<00:14, 111MB/s]\nDownloading pytorch_model.bin: \n4%|██▏                                                        | \n62.9M/1.71G [00:00<00:14, 111MB/s]\nDownloading pytorch_model.bin: \n5%|██▉                                                        | \n83.9M/1.71G [00:00<00:14, 110MB/s]\nDownloading pytorch_model.bin: \n6%|███▋                                                        | \n105M/1.71G [00:00<00:14, 110MB/s]\nDownloading pytorch_model.bin: \n7%|████▍                                                       | \n126M/1.71G [00:01<00:14, 110MB/s]\nDownloading pytorch_model.bin: \n9%|█████▏                                                      | \n147M/1.71G [00:01<00:14, 110MB/s]\nDownloading pytorch_model.bin: \n10%|█████▉                                                      | \n168M/1.71G [00:01<00:14, 110MB/s]\nDownloading pytorch_model.bin: \n11%|██████▌                                                     | \n189M/1.71G [00:01<00:13, 110MB/s]\nDownloading pytorch_model.bin: \n12%|███████▏                                                   | \n210M/1.71G [00:01<00:15, 99.8MB/s]\nDownloading pytorch_model.bin: \n13%|████████                                                    | \n231M/1.71G [00:02<00:14, 103MB/s]\nDownloading pytorch_model.bin: \n15%|████████▊                                                   | \n252M/1.71G [00:02<00:13, 105MB/s]\nDownloading pytorch_model.bin: \n16%|█████████▌                                                  | \n273M/1.71G [00:02<00:13, 107MB/s]\nDownloading pytorch_model.bin: \n17%|██████████▎                                                 | \n294M/1.71G [00:02<00:13, 108MB/s]\nDownloading pytorch_model.bin: \n18%|███████████                                                 | \n315M/1.71G [00:02<00:12, 109MB/s]\nDownloading pytorch_model.bin: \n20%|███████████▊                                                | \n336M/1.71G [00:03<00:12, 109MB/s]\nDownloading pytorch_model.bin: \n21%|████████████▌                                               | \n357M/1.71G [00:03<00:12, 109MB/s]\nDownloading pytorch_model.bin: \n22%|█████████████▏                                              | \n377M/1.71G [00:03<00:12, 109MB/s]\nDownloading pytorch_model.bin: \n23%|█████████████▉                                              | \n398M/1.71G [00:03<00:11, 110MB/s]\nDownloading pytorch_model.bin: \n25%|██████████████▋                                             | \n419M/1.71G [00:03<00:11, 110MB/s]\nDownloading pytorch_model.bin: \n26%|███████████████▍                                            | \n440M/1.71G [00:04<00:11, 110MB/s]\nDownloading pytorch_model.bin: \n27%|████████████████▏                                           | \n461M/1.71G [00:04<00:11, 110MB/s]\nDownloading pytorch_model.bin: \n28%|████████████████▉                                           | \n482M/1.71G [00:04<00:11, 110MB/s]\nDownloading pytorch_model.bin: \n29%|█████████████████▋                                          | \n503M/1.71G [00:04<00:10, 110MB/s]\nDownloading pytorch_model.bin: \n31%|██████████████████▍                                         | \n524M/1.71G [00:04<00:10, 111MB/s]\nDownloading pytorch_model.bin: \n32%|███████████████████                                         | \n545M/1.71G [00:05<00:10, 110MB/s]\nDownloading pytorch_model.bin: \n33%|███████████████████▊                                        | \n566M/1.71G [00:05<00:10, 110MB/s]\nDownloading pytorch_model.bin: \n34%|████████████████████▌                                       | \n587M/1.71G [00:05<00:10, 109MB/s]\nDownloading pytorch_model.bin: \n36%|█████████████████████▎                                      | \n608M/1.71G [00:05<00:10, 110MB/s]\nDownloading pytorch_model.bin: \n37%|██████████████████████                                      | \n629M/1.71G [00:05<00:09, 110MB/s]\nDownloading pytorch_model.bin: \n38%|██████████████████████▊                                     | \n650M/1.71G [00:05<00:09, 110MB/s]\nDownloading pytorch_model.bin: \n39%|███████████████████████▌                                    | \n671M/1.71G [00:06<00:09, 110MB/s]\nDownloading pytorch_model.bin: \n40%|████████████████████████▎                                   | \n692M/1.71G [00:06<00:09, 111MB/s]\nDownloading pytorch_model.bin: \n42%|█████████████████████████                                   | \n713M/1.71G [00:06<00:09, 111MB/s]\nDownloading pytorch_model.bin: \n43%|█████████████████████████▋                                  | \n734M/1.71G [00:06<00:08, 111MB/s]\nDownloading pytorch_model.bin: \n44%|██████████████████████████▍                                 | \n755M/1.71G [00:06<00:08, 110MB/s]\nDownloading pytorch_model.bin: \n45%|███████████████████████████▏                                | \n776M/1.71G [00:07<00:08, 111MB/s]\nDownloading pytorch_model.bin: \n47%|███████████████████████████▉                                | \n797M/1.71G [00:07<00:08, 111MB/s]\nDownloading pytorch_model.bin: \n48%|████████████████████████████▋                               | \n818M/1.71G [00:07<00:08, 111MB/s]\nDownloading pytorch_model.bin: \n49%|█████████████████████████████▍                              | \n839M/1.71G [00:07<00:07, 109MB/s]\nDownloading pytorch_model.bin: \n50%|██████████████████████████████▏                             | \n860M/1.71G [00:07<00:07, 110MB/s]\nDownloading pytorch_model.bin: \n51%|██████████████████████████████▉                             | \n881M/1.71G [00:08<00:07, 110MB/s]\nDownloading pytorch_model.bin: \n53%|███████████████████████████████▋                            | \n902M/1.71G [00:08<00:07, 110MB/s]\nDownloading pytorch_model.bin: \n54%|████████████████████████████████▎                           | \n923M/1.71G [00:08<00:07, 110MB/s]\nDownloading pytorch_model.bin: \n55%|█████████████████████████████████                           | \n944M/1.71G [00:08<00:06, 110MB/s]\nDownloading pytorch_model.bin: \n56%|█████████████████████████████████▊                          | \n965M/1.71G [00:08<00:06, 110MB/s]\nDownloading pytorch_model.bin: \n58%|██████████████████████████████████▌                         | \n986M/1.71G [00:09<00:06, 110MB/s]\nDownloading pytorch_model.bin: \n59%|██████████████████████████████████▋                        | \n1.01G/1.71G [00:09<00:06, 111MB/s]\nDownloading pytorch_model.bin: \n60%|███████████████████████████████████▍                       | \n1.03G/1.71G [00:09<00:06, 110MB/s]\nDownloading pytorch_model.bin: \n61%|████████████████████████████████████▏                      | \n1.05G/1.71G [00:09<00:05, 110MB/s]\nDownloading pytorch_model.bin: \n63%|████████████████████████████████████▉                      | \n1.07G/1.71G [00:09<00:05, 110MB/s]\nDownloading pytorch_model.bin: \n64%|█████████████████████████████████████▌                     | \n1.09G/1.71G [00:09<00:05, 110MB/s]\nDownloading pytorch_model.bin: \n65%|██████████████████████████████████████▎                    | \n1.11G/1.71G [00:10<00:05, 110MB/s]\nDownloading pytorch_model.bin: \n66%|███████████████████████████████████████                    | \n1.13G/1.71G [00:10<00:05, 110MB/s]\nDownloading pytorch_model.bin: \n67%|███████████████████████████████████████▊                   | \n1.15G/1.71G [00:10<00:05, 110MB/s]\nDownloading pytorch_model.bin: \n69%|████████████████████████████████████████▌                  | \n1.17G/1.71G [00:10<00:04, 110MB/s]\nDownloading pytorch_model.bin: \n70%|█████████████████████████████████████████▏                 | \n1.20G/1.71G [00:10<00:04, 110MB/s]\nDownloading pytorch_model.bin: \n71%|█████████████████████████████████████████▉                 | \n1.22G/1.71G [00:11<00:04, 110MB/s]\nDownloading pytorch_model.bin: \n72%|██████████████████████████████████████████▋                | \n1.24G/1.71G [00:11<00:04, 111MB/s]\nDownloading pytorch_model.bin: \n74%|███████████████████████████████████████████▍               | \n1.26G/1.71G [00:11<00:04, 111MB/s]\nDownloading pytorch_model.bin: \n75%|████████████████████████████████████████████               | \n1.28G/1.71G [00:11<00:03, 110MB/s]\nDownloading pytorch_model.bin: \n76%|████████████████████████████████████████████▊              | \n1.30G/1.71G [00:11<00:03, 110MB/s]\nDownloading pytorch_model.bin: \n77%|█████████████████████████████████████████████▌             | \n1.32G/1.71G [00:12<00:03, 110MB/s]\nDownloading pytorch_model.bin: \n78%|██████████████████████████████████████████████▎            | \n1.34G/1.71G [00:12<00:03, 110MB/s]\nDownloading pytorch_model.bin: \n80%|███████████████████████████████████████████████            | \n1.36G/1.71G [00:12<00:03, 110MB/s]\nDownloading pytorch_model.bin: \n81%|███████████████████████████████████████████████▋           | \n1.38G/1.71G [00:12<00:02, 111MB/s]\nDownloading pytorch_model.bin: \n82%|████████████████████████████████████████████████▍          | \n1.41G/1.71G [00:12<00:02, 110MB/s]\nDownloading pytorch_model.bin: \n83%|█████████████████████████████████████████████████▏         | \n1.43G/1.71G [00:13<00:02, 110MB/s]\nDownloading pytorch_model.bin: \n85%|█████████████████████████████████████████████████▉         | \n1.45G/1.71G [00:13<00:02, 111MB/s]\nDownloading pytorch_model.bin: \n86%|█████████████████████████████████████████████████▊        | \n1.47G/1.71G [00:13<00:02, 99.3MB/s]\nDownloading pytorch_model.bin: \n87%|███████████████████████████████████████████████████▎       | \n1.49G/1.71G [00:13<00:02, 102MB/s]\nDownloading pytorch_model.bin: \n88%|████████████████████████████████████████████████████       | \n1.51G/1.71G [00:13<00:01, 104MB/s]\nDownloading pytorch_model.bin: \n89%|████████████████████████████████████████████████████▊      | \n1.53G/1.71G [00:14<00:01, 106MB/s]\nDownloading pytorch_model.bin: \n91%|█████████████████████████████████████████████████████▌     | \n1.55G/1.71G [00:14<00:01, 107MB/s]\nDownloading pytorch_model.bin: \n92%|██████████████████████████████████████████████████████▏    | \n1.57G/1.71G [00:14<00:01, 108MB/s]\nDownloading pytorch_model.bin: \n93%|██████████████████████████████████████████████████████▉    | \n1.59G/1.71G [00:14<00:01, 109MB/s]\nDownloading pytorch_model.bin: \n94%|███████████████████████████████████████████████████████▋   | \n1.61G/1.71G [00:14<00:00, 109MB/s]\nDownloading pytorch_model.bin: \n96%|████████████████████████████████████████████████████████▍  | \n1.64G/1.71G [00:14<00:00, 110MB/s]\nDownloading pytorch_model.bin: \n97%|█████████████████████████████████████████████████████████▏ | \n1.66G/1.71G [00:15<00:00, 110MB/s]\nDownloading pytorch_model.bin: \n98%|█████████████████████████████████████████████████████████▊ | \n1.68G/1.71G [00:15<00:00, 110MB/s]\nDownloading pytorch_model.bin: \n99%|██████████████████████████████████████████████████████████▌| \n1.70G/1.71G [00:15<00:00, 110MB/s]\nDownloading pytorch_model.bin: \n100%|███████████████████████████████████████████████████████████| \n1.71G/1.71G [00:15<00:00, 109MB/s]\nDownloading pytorch_model.bin: \n100%|███████████████████████████████████████████████████████████| \n1.71G/1.71G [00:15<00:00, 109MB/s]\n2023-04-19 16:35:52 | ERROR | stderr |\nSome weights of the model checkpoint at openai/clip-vit-large-patch14 \nwere not used when initializing CLIPVisionModel: \n['text_model.encoder.layers.0.self_attn.q_proj.weight', \n'text_model.embeddings.position_ids', \n'text_model.encoder.layers.3.self_attn.out_proj.weight', \n'text_model.encoder.layers.2.self_attn.out_proj.bias', \n'text_model.encoder.layers.11.self_attn.k_proj.weight', \n'text_model.encoder.layers.8.self_attn.v_proj.bias', \n'text_model.final_layer_norm.weight', \n'text_model.encoder.layers.11.self_attn.out_proj.bias', \n'text_model.encoder.layers.8.self_attn.q_proj.weight', \n'text_model.encoder.layers.10.self_attn.k_proj.bias', \n'text_model.encoder.layers.2.self_attn.q_proj.weight', \n'text_model.encoder.layers.1.self_attn.out_proj.weight', \n'text_model.encoder.layers.11.self_attn.k_proj.bias', \n'text_model.encoder.layers.11.layer_norm2.weight', \n'text_model.encoder.layers.7.self_attn.v_proj.bias', \n'text_model.encoder.layers.2.self_attn.k_proj.weight', \n'text_model.encoder.layers.2.mlp.fc2.weight', \n'text_model.encoder.layers.2.layer_norm2.bias', \n'text_model.encoder.layers.6.self_attn.v_proj.bias', \n'text_model.encoder.layers.6.mlp.fc1.weight', \n'text_model.encoder.layers.7.layer_norm1.bias', \n'text_model.encoder.layers.7.self_attn.out_proj.bias', \n'text_model.encoder.layers.0.self_attn.k_proj.weight', \n'text_model.encoder.layers.0.layer_norm2.weight', \n'text_model.encoder.layers.10.mlp.fc1.weight', \n'text_model.encoder.layers.0.layer_norm2.bias', \n'text_model.encoder.layers.6.self_attn.q_proj.weight', \n'text_model.encoder.layers.2.mlp.fc2.bias', \n'text_model.encoder.layers.0.mlp.fc2.bias', \n'text_model.encoder.layers.3.mlp.fc2.weight', 'text_projection.weight', \n'text_model.encoder.layers.11.self_attn.v_proj.weight', \n'text_model.encoder.layers.1.self_attn.q_proj.bias', \n'text_model.encoder.layers.9.layer_norm1.weight', \n'text_model.encoder.layers.8.self_attn.out_proj.weight', \n'text_model.encoder.layers.5.mlp.fc2.weight', \n'text_model.encoder.layers.11.self_attn.out_proj.weight', \n'text_model.encoder.layers.9.self_attn.k_proj.weight', \n'text_model.encoder.layers.11.mlp.fc2.bias', \n'text_model.embeddings.token_embedding.weight', \n'text_model.encoder.layers.5.mlp.fc1.weight', 'logit_scale', \n'text_model.encoder.layers.8.mlp.fc2.weight', \n'text_model.encoder.layers.10.layer_norm2.bias', \n'text_model.encoder.layers.5.self_attn.v_proj.weight', \n'text_model.encoder.layers.0.mlp.fc1.bias', \n'text_model.encoder.layers.4.self_attn.v_proj.bias', \n'text_model.encoder.layers.0.self_attn.q_proj.bias', \n'text_model.encoder.layers.4.mlp.fc2.weight', \n'text_model.encoder.layers.11.layer_norm1.weight', \n'text_model.encoder.layers.3.self_attn.v_proj.bias', \n'text_model.encoder.layers.3.layer_norm2.weight', \n'text_model.encoder.layers.7.self_attn.k_proj.bias', \n'text_model.encoder.layers.10.mlp.fc2.bias', \n'text_model.encoder.layers.7.layer_norm1.weight', \n'text_model.encoder.layers.11.mlp.fc2.weight', \n'text_model.encoder.layers.3.self_attn.out_proj.bias', \n'text_model.encoder.layers.2.mlp.fc1.bias', \n'text_model.embeddings.position_embedding.weight', \n'text_model.encoder.layers.1.layer_norm1.weight', \n'text_model.encoder.layers.7.self_attn.q_proj.bias', \n'text_model.encoder.layers.6.self_attn.out_proj.weight', \n'text_model.encoder.layers.3.self_attn.k_proj.bias', \n'text_model.encoder.layers.5.mlp.fc2.bias', \n'text_model.encoder.layers.6.self_attn.v_proj.weight', \n'text_model.encoder.layers.10.self_attn.v_proj.bias', \n'text_model.encoder.layers.6.self_attn.out_proj.bias', \n'text_model.encoder.layers.6.mlp.fc2.bias', \n'text_model.encoder.layers.3.layer_norm1.weight', \n'text_model.encoder.layers.3.layer_norm2.bias', \n'text_model.encoder.layers.8.mlp.fc1.weight', \n'text_model.encoder.layers.4.self_attn.v_proj.weight', \n'text_model.encoder.layers.4.layer_norm1.weight', \n'text_model.encoder.layers.7.mlp.fc2.bias', \n'text_model.encoder.layers.6.mlp.fc2.weight', \n'text_model.encoder.layers.9.self_attn.v_proj.bias', \n'text_model.encoder.layers.10.mlp.fc2.weight', \n'text_model.encoder.layers.10.layer_norm1.bias', \n'text_model.final_layer_norm.bias', \n'text_model.encoder.layers.9.self_attn.out_proj.bias', \n'text_model.encoder.layers.3.self_attn.v_proj.weight', \n'text_model.encoder.layers.10.layer_norm2.weight', \n'text_model.encoder.layers.1.layer_norm1.bias', \n'text_model.encoder.layers.8.layer_norm1.bias', \n'text_model.encoder.layers.10.self_attn.q_proj.bias', \n'text_model.encoder.layers.4.self_attn.out_proj.weight', \n'text_model.encoder.layers.6.layer_norm2.weight', \n'text_model.encoder.layers.3.self_attn.q_proj.weight', \n'text_model.encoder.layers.7.mlp.fc2.weight', \n'text_model.encoder.layers.4.self_attn.k_proj.bias', \n'text_model.encoder.layers.2.self_attn.v_proj.weight', \n'text_model.encoder.layers.4.layer_norm2.weight', \n'text_model.encoder.layers.7.self_attn.q_proj.weight', \n'text_model.encoder.layers.9.layer_norm2.bias', \n'text_model.encoder.layers.2.self_attn.out_proj.weight', \n'text_model.encoder.layers.6.self_attn.q_proj.bias', \n'text_model.encoder.layers.8.self_attn.out_proj.bias', \n'text_model.encoder.layers.6.layer_norm2.bias', \n'text_model.encoder.layers.7.layer_norm2.bias', \n'text_model.encoder.layers.11.layer_norm1.bias', \n'text_model.encoder.layers.0.self_attn.k_proj.bias', \n'text_model.encoder.layers.5.self_attn.k_proj.bias', \n'text_model.encoder.layers.10.mlp.fc1.bias', \n'text_model.encoder.layers.9.layer_norm1.bias', \n'text_model.encoder.layers.9.self_attn.out_proj.weight', \n'text_model.encoder.layers.3.layer_norm1.bias', \n'text_model.encoder.layers.4.layer_norm1.bias', \n'text_model.encoder.layers.6.self_attn.k_proj.bias', \n'text_model.encoder.layers.9.mlp.fc1.bias', \n'text_model.encoder.layers.3.mlp.fc1.weight', \n'text_model.encoder.layers.11.self_attn.q_proj.weight', \n'text_model.encoder.layers.0.layer_norm1.weight', \n'text_model.encoder.layers.7.mlp.fc1.weight', \n'text_model.encoder.layers.9.self_attn.q_proj.bias', \n'text_model.encoder.layers.2.self_attn.k_proj.bias', \n'text_model.encoder.layers.4.mlp.fc1.bias', \n'text_model.encoder.layers.11.mlp.fc1.bias', \n'text_model.encoder.layers.10.self_attn.v_proj.weight', \n'text_model.encoder.layers.5.layer_norm1.weight', \n'text_model.encoder.layers.1.mlp.fc2.bias', 'visual_projection.weight', \n'text_model.encoder.layers.10.self_attn.out_proj.weight', \n'text_model.encoder.layers.2.self_attn.q_proj.bias', \n'text_model.encoder.layers.8.self_attn.k_proj.weight', \n'text_model.encoder.layers.8.mlp.fc2.bias', \n'text_model.encoder.layers.2.self_attn.v_proj.bias', \n'text_model.encoder.layers.4.self_attn.k_proj.weight', \n'text_model.encoder.layers.11.self_attn.v_proj.bias', \n'text_model.encoder.layers.11.mlp.fc1.weight', \n'text_model.encoder.layers.0.self_attn.v_proj.weight', \n'text_model.encoder.layers.5.self_attn.out_proj.bias', \n'text_model.encoder.layers.10.self_attn.q_proj.weight', \n'text_model.encoder.layers.10.layer_norm1.weight', \n'text_model.encoder.layers.8.layer_norm1.weight', \n'text_model.encoder.layers.4.mlp.fc1.weight', \n'text_model.encoder.layers.5.mlp.fc1.bias', \n'text_model.encoder.layers.2.layer_norm1.bias', \n'text_model.encoder.layers.5.layer_norm1.bias', \n'text_model.encoder.layers.0.self_attn.out_proj.bias', \n'text_model.encoder.layers.3.self_attn.k_proj.weight', \n'text_model.encoder.layers.9.mlp.fc2.weight', \n'text_model.encoder.layers.6.layer_norm1.weight', \n'text_model.encoder.layers.8.layer_norm2.weight', \n'text_model.encoder.layers.1.mlp.fc1.weight', \n'text_model.encoder.layers.1.mlp.fc2.weight', \n'text_model.encoder.layers.2.layer_norm2.weight', \n'text_model.encoder.layers.7.self_attn.k_proj.weight', \n'text_model.encoder.layers.9.self_attn.k_proj.bias', \n'text_model.encoder.layers.3.mlp.fc1.bias', \n'text_model.encoder.layers.7.self_attn.out_proj.weight', \n'text_model.encoder.layers.9.layer_norm2.weight', \n'text_model.encoder.layers.6.layer_norm1.bias', \n'text_model.encoder.layers.1.self_attn.q_proj.weight', \n'text_model.encoder.layers.5.layer_norm2.weight', \n'text_model.encoder.layers.9.mlp.fc1.weight', \n'text_model.encoder.layers.0.self_attn.v_proj.bias', \n'text_model.encoder.layers.8.self_attn.v_proj.weight', \n'text_model.encoder.layers.1.self_attn.v_proj.weight', \n'text_model.encoder.layers.5.self_attn.k_proj.weight', \n'text_model.encoder.layers.0.mlp.fc2.weight', \n'text_model.encoder.layers.5.layer_norm2.bias', \n'text_model.encoder.layers.1.self_attn.out_proj.bias', \n'text_model.encoder.layers.0.layer_norm1.bias', \n'text_model.encoder.layers.8.self_attn.k_proj.bias', \n'text_model.encoder.layers.8.self_attn.q_proj.bias', \n'text_model.encoder.layers.8.mlp.fc1.bias', \n'text_model.encoder.layers.4.self_attn.out_proj.bias', \n'text_model.encoder.layers.8.layer_norm2.bias', \n'text_model.encoder.layers.9.self_attn.v_proj.weight', \n'text_model.encoder.layers.9.self_attn.q_proj.weight', \n'text_model.encoder.layers.0.mlp.fc1.weight', \n'text_model.encoder.layers.1.self_attn.v_proj.bias', \n'text_model.encoder.layers.7.self_attn.v_proj.weight', \n'text_model.encoder.layers.0.self_attn.out_proj.weight', \n'text_model.encoder.layers.1.layer_norm2.bias', \n'text_model.encoder.layers.5.self_attn.q_proj.bias', \n'text_model.encoder.layers.5.self_attn.v_proj.bias', \n'text_model.encoder.layers.11.self_attn.q_proj.bias', \n'text_model.encoder.layers.1.self_attn.k_proj.bias', \n'text_model.encoder.layers.1.mlp.fc1.bias', \n'text_model.encoder.layers.1.self_attn.k_proj.weight', \n'text_model.encoder.layers.7.layer_norm2.weight', \n'text_model.encoder.layers.2.mlp.fc1.weight', \n'text_model.encoder.layers.4.self_attn.q_proj.weight', \n'text_model.encoder.layers.3.self_attn.q_proj.bias', \n'text_model.encoder.layers.10.self_attn.out_proj.bias', \n'text_model.encoder.layers.4.self_attn.q_proj.bias', \n'text_model.encoder.layers.4.mlp.fc2.bias', \n'text_model.encoder.layers.6.self_attn.k_proj.weight', \n'text_model.encoder.layers.1.layer_norm2.weight', \n'text_model.encoder.layers.10.self_attn.k_proj.weight', \n'text_model.encoder.layers.5.self_attn.q_proj.weight', \n'text_model.encoder.layers.9.mlp.fc2.bias', \n'text_model.encoder.layers.2.layer_norm1.weight', \n'text_model.encoder.layers.7.mlp.fc1.bias', \n'text_model.encoder.layers.6.mlp.fc1.bias', \n'text_model.encoder.layers.11.layer_norm2.bias', \n'text_model.encoder.layers.4.layer_norm2.bias', \n'text_model.encoder.layers.5.self_attn.out_proj.weight', \n'text_model.encoder.layers.3.mlp.fc2.bias']\n- This IS expected if you are initializing CLIPVisionModel from the \ncheckpoint of a model trained on another task or with another \narchitecture (e.g. initializing a BertForSequenceClassification model \nfrom a BertForPreTraining model).\n- This IS NOT expected if you are initializing CLIPVisionModel from the \ncheckpoint of a model that you expect to be exactly identical \n(initializing a BertForSequenceClassification model from a \nBertForSequenceClassification model).\nLoading checkpoint shards: 0%| | 0/3 [00:00<?, ?it/s]\nLoading checkpoint shards: 33%|███████████████████████ | 1/3 \n[00:04<00:09,  4.83s/it]\nLoading checkpoint shards: \n67%|██████████████████████████████████████████████ | 2/3 [00:09<00:04,  \n4.79s/it]\nLoading checkpoint shards: \n100%|█████████████████████████████████████████████████████████████████████| \n3/3 [00:12<00:00,  3.99s/it]\nLoading checkpoint shards: \n100%|█████████████████████████████████████████████████████████████████████| \n3/3 [00:12<00:00,  4.21s/it]\n2023-04-19 16:36:06 | ERROR | stderr |\nSome weights of LlamaForCausalLM were not initialized from the model \ncheckpoint at ./LLaVA-13B-v0 and are newly initialized: \n['model.mm_projector.weight', 'model.mm_projector.bias']\nYou should probably TRAIN this model on a down-stream task to be able to \nuse it for predictions and inference.\nSome weights of the model checkpoint at openai/clip-vit-large-patch14 \nwere not used when initializing CLIPVisionModel: \n['text_model.encoder.layers.0.self_attn.q_proj.weight', \n'text_model.embeddings.position_ids', \n'text_model.encoder.layers.3.self_attn.out_proj.weight', \n'text_model.encoder.layers.2.self_attn.out_proj.bias', \n'text_model.encoder.layers.11.self_attn.k_proj.weight', \n'text_model.encoder.layers.8.self_attn.v_proj.bias', \n'text_model.final_layer_norm.weight', \n'text_model.encoder.layers.11.self_attn.out_proj.bias', \n'text_model.encoder.layers.8.self_attn.q_proj.weight', \n'text_model.encoder.layers.10.self_attn.k_proj.bias', \n'text_model.encoder.layers.2.self_attn.q_proj.weight', \n'text_model.encoder.layers.1.self_attn.out_proj.weight', \n'text_model.encoder.layers.11.self_attn.k_proj.bias', \n'text_model.encoder.layers.11.layer_norm2.weight', \n'text_model.encoder.layers.7.self_attn.v_proj.bias', \n'text_model.encoder.layers.2.self_attn.k_proj.weight', \n'text_model.encoder.layers.2.mlp.fc2.weight', \n'text_model.encoder.layers.2.layer_norm2.bias', \n'text_model.encoder.layers.6.self_attn.v_proj.bias', \n'text_model.encoder.layers.6.mlp.fc1.weight', \n'text_model.encoder.layers.7.layer_norm1.bias', \n'text_model.encoder.layers.7.self_attn.out_proj.bias', \n'text_model.encoder.layers.0.self_attn.k_proj.weight', \n'text_model.encoder.layers.0.layer_norm2.weight', \n'text_model.encoder.layers.10.mlp.fc1.weight', \n'text_model.encoder.layers.0.layer_norm2.bias', \n'text_model.encoder.layers.6.self_attn.q_proj.weight', \n'text_model.encoder.layers.2.mlp.fc2.bias', \n'text_model.encoder.layers.0.mlp.fc2.bias', \n'text_model.encoder.layers.3.mlp.fc2.weight', 'text_projection.weight', \n'text_model.encoder.layers.11.self_attn.v_proj.weight', \n'text_model.encoder.layers.1.self_attn.q_proj.bias', \n'text_model.encoder.layers.9.layer_norm1.weight', \n'text_model.encoder.layers.8.self_attn.out_proj.weight', \n'text_model.encoder.layers.5.mlp.fc2.weight', \n'text_model.encoder.layers.11.self_attn.out_proj.weight', \n'text_model.encoder.layers.9.self_attn.k_proj.weight', \n'text_model.encoder.layers.11.mlp.fc2.bias', \n'text_model.embeddings.token_embedding.weight', \n'text_model.encoder.layers.5.mlp.fc1.weight', 'logit_scale', \n'text_model.encoder.layers.8.mlp.fc2.weight', \n'text_model.encoder.layers.10.layer_norm2.bias', \n'text_model.encoder.layers.5.self_attn.v_proj.weight', \n'text_model.encoder.layers.0.mlp.fc1.bias', \n'text_model.encoder.layers.4.self_attn.v_proj.bias', \n'text_model.encoder.layers.0.self_attn.q_proj.bias', \n'text_model.encoder.layers.4.mlp.fc2.weight', \n'text_model.encoder.layers.11.layer_norm1.weight', \n'text_model.encoder.layers.3.self_attn.v_proj.bias', \n'text_model.encoder.layers.3.layer_norm2.weight', \n'text_model.encoder.layers.7.self_attn.k_proj.bias', \n'text_model.encoder.layers.10.mlp.fc2.bias', \n'text_model.encoder.layers.7.layer_norm1.weight', \n'text_model.encoder.layers.11.mlp.fc2.weight', \n'text_model.encoder.layers.3.self_attn.out_proj.bias', \n'text_model.encoder.layers.2.mlp.fc1.bias', \n'text_model.embeddings.position_embedding.weight', \n'text_model.encoder.layers.1.layer_norm1.weight', \n'text_model.encoder.layers.7.self_attn.q_proj.bias', \n'text_model.encoder.layers.6.self_attn.out_proj.weight', \n'text_model.encoder.layers.3.self_attn.k_proj.bias', \n'text_model.encoder.layers.5.mlp.fc2.bias', \n'text_model.encoder.layers.6.self_attn.v_proj.weight', \n'text_model.encoder.layers.10.self_attn.v_proj.bias', \n'text_model.encoder.layers.6.self_attn.out_proj.bias', \n'text_model.encoder.layers.6.mlp.fc2.bias', \n'text_model.encoder.layers.3.layer_norm1.weight', \n'text_model.encoder.layers.3.layer_norm2.bias', \n'text_model.encoder.layers.8.mlp.fc1.weight', \n'text_model.encoder.layers.4.self_attn.v_proj.weight', \n'text_model.encoder.layers.4.layer_norm1.weight', \n'text_model.encoder.layers.7.mlp.fc2.bias', \n'text_model.encoder.layers.6.mlp.fc2.weight', \n'text_model.encoder.layers.9.self_attn.v_proj.bias', \n'text_model.encoder.layers.10.mlp.fc2.weight', \n'text_model.encoder.layers.10.layer_norm1.bias', \n'text_model.final_layer_norm.bias', \n'text_model.encoder.layers.9.self_attn.out_proj.bias', \n'text_model.encoder.layers.3.self_attn.v_proj.weight', \n'text_model.encoder.layers.10.layer_norm2.weight', \n'text_model.encoder.layers.1.layer_norm1.bias', \n'text_model.encoder.layers.8.layer_norm1.bias', \n'text_model.encoder.layers.10.self_attn.q_proj.bias', \n'text_model.encoder.layers.4.self_attn.out_proj.weight', \n'text_model.encoder.layers.6.layer_norm2.weight', \n'text_model.encoder.layers.3.self_attn.q_proj.weight', \n'text_model.encoder.layers.7.mlp.fc2.weight', \n'text_model.encoder.layers.4.self_attn.k_proj.bias', \n'text_model.encoder.layers.2.self_attn.v_proj.weight', \n'text_model.encoder.layers.4.layer_norm2.weight', \n'text_model.encoder.layers.7.self_attn.q_proj.weight', \n'text_model.encoder.layers.9.layer_norm2.bias', \n'text_model.encoder.layers.2.self_attn.out_proj.weight', \n'text_model.encoder.layers.6.self_attn.q_proj.bias', \n'text_model.encoder.layers.8.self_attn.out_proj.bias', \n'text_model.encoder.layers.6.layer_norm2.bias', \n'text_model.encoder.layers.7.layer_norm2.bias', \n'text_model.encoder.layers.11.layer_norm1.bias', \n'text_model.encoder.layers.0.self_attn.k_proj.bias', \n'text_model.encoder.layers.5.self_attn.k_proj.bias', \n'text_model.encoder.layers.10.mlp.fc1.bias', \n'text_model.encoder.layers.9.layer_norm1.bias', \n'text_model.encoder.layers.9.self_attn.out_proj.weight', \n'text_model.encoder.layers.3.layer_norm1.bias', \n'text_model.encoder.layers.4.layer_norm1.bias', \n'text_model.encoder.layers.6.self_attn.k_proj.bias', \n'text_model.encoder.layers.9.mlp.fc1.bias', \n'text_model.encoder.layers.3.mlp.fc1.weight', \n'text_model.encoder.layers.11.self_attn.q_proj.weight', \n'text_model.encoder.layers.0.layer_norm1.weight', \n'text_model.encoder.layers.7.mlp.fc1.weight', \n'text_model.encoder.layers.9.self_attn.q_proj.bias', \n'text_model.encoder.layers.2.self_attn.k_proj.bias', \n'text_model.encoder.layers.4.mlp.fc1.bias', \n'text_model.encoder.layers.11.mlp.fc1.bias', \n'text_model.encoder.layers.10.self_attn.v_proj.weight', \n'text_model.encoder.layers.5.layer_norm1.weight', \n'text_model.encoder.layers.1.mlp.fc2.bias', 'visual_projection.weight', \n'text_model.encoder.layers.10.self_attn.out_proj.weight', \n'text_model.encoder.layers.2.self_attn.q_proj.bias', \n'text_model.encoder.layers.8.self_attn.k_proj.weight', \n'text_model.encoder.layers.8.mlp.fc2.bias', \n'text_model.encoder.layers.2.self_attn.v_proj.bias', \n'text_model.encoder.layers.4.self_attn.k_proj.weight', \n'text_model.encoder.layers.11.self_attn.v_proj.bias', \n'text_model.encoder.layers.11.mlp.fc1.weight', \n'text_model.encoder.layers.0.self_attn.v_proj.weight', \n'text_model.encoder.layers.5.self_attn.out_proj.bias', \n'text_model.encoder.layers.10.self_attn.q_proj.weight', \n'text_model.encoder.layers.10.layer_norm1.weight', \n'text_model.encoder.layers.8.layer_norm1.weight', \n'text_model.encoder.layers.4.mlp.fc1.weight', \n'text_model.encoder.layers.5.mlp.fc1.bias', \n'text_model.encoder.layers.2.layer_norm1.bias', \n'text_model.encoder.layers.5.layer_norm1.bias', \n'text_model.encoder.layers.0.self_attn.out_proj.bias', \n'text_model.encoder.layers.3.self_attn.k_proj.weight', \n'text_model.encoder.layers.9.mlp.fc2.weight', \n'text_model.encoder.layers.6.layer_norm1.weight', \n'text_model.encoder.layers.8.layer_norm2.weight', \n'text_model.encoder.layers.1.mlp.fc1.weight', \n'text_model.encoder.layers.1.mlp.fc2.weight', \n'text_model.encoder.layers.2.layer_norm2.weight', \n'text_model.encoder.layers.7.self_attn.k_proj.weight', \n'text_model.encoder.layers.9.self_attn.k_proj.bias', \n'text_model.encoder.layers.3.mlp.fc1.bias', \n'text_model.encoder.layers.7.self_attn.out_proj.weight', \n'text_model.encoder.layers.9.layer_norm2.weight', \n'text_model.encoder.layers.6.layer_norm1.bias', \n'text_model.encoder.layers.1.self_attn.q_proj.weight', \n'text_model.encoder.layers.5.layer_norm2.weight', \n'text_model.encoder.layers.9.mlp.fc1.weight', \n'text_model.encoder.layers.0.self_attn.v_proj.bias', \n'text_model.encoder.layers.8.self_attn.v_proj.weight', \n'text_model.encoder.layers.1.self_attn.v_proj.weight', \n'text_model.encoder.layers.5.self_attn.k_proj.weight', \n'text_model.encoder.layers.0.mlp.fc2.weight', \n'text_model.encoder.layers.5.layer_norm2.bias', \n'text_model.encoder.layers.1.self_attn.out_proj.bias', \n'text_model.encoder.layers.0.layer_norm1.bias', \n'text_model.encoder.layers.8.self_attn.k_proj.bias', \n'text_model.encoder.layers.8.self_attn.q_proj.bias', \n'text_model.encoder.layers.8.mlp.fc1.bias', \n'text_model.encoder.layers.4.self_attn.out_proj.bias', \n'text_model.encoder.layers.8.layer_norm2.bias', \n'text_model.encoder.layers.9.self_attn.v_proj.weight', \n'text_model.encoder.layers.9.self_attn.q_proj.weight', \n'text_model.encoder.layers.0.mlp.fc1.weight', \n'text_model.encoder.layers.1.self_attn.v_proj.bias', \n'text_model.encoder.layers.7.self_attn.v_proj.weight', \n'text_model.encoder.layers.0.self_attn.out_proj.weight', \n'text_model.encoder.layers.1.layer_norm2.bias', \n'text_model.encoder.layers.5.self_attn.q_proj.bias', \n'text_model.encoder.layers.5.self_attn.v_proj.bias', \n'text_model.encoder.layers.11.self_attn.q_proj.bias', \n'text_model.encoder.layers.1.self_attn.k_proj.bias', \n'text_model.encoder.layers.1.mlp.fc1.bias', \n'text_model.encoder.layers.1.self_attn.k_proj.weight', \n'text_model.encoder.layers.7.layer_norm2.weight', \n'text_model.encoder.layers.2.mlp.fc1.weight', \n'text_model.encoder.layers.4.self_attn.q_proj.weight', \n'text_model.encoder.layers.3.self_attn.q_proj.bias', \n'text_model.encoder.layers.10.self_attn.out_proj.bias', \n'text_model.encoder.layers.4.self_attn.q_proj.bias', \n'text_model.encoder.layers.4.mlp.fc2.bias', \n'text_model.encoder.layers.6.self_attn.k_proj.weight', \n'text_model.encoder.layers.1.layer_norm2.weight', \n'text_model.encoder.layers.10.self_attn.k_proj.weight', \n'text_model.encoder.layers.5.self_attn.q_proj.weight', \n'text_model.encoder.layers.9.mlp.fc2.bias', \n'text_model.encoder.layers.2.layer_norm1.weight', \n'text_model.encoder.layers.7.mlp.fc1.bias', \n'text_model.encoder.layers.6.mlp.fc1.bias', \n'text_model.encoder.layers.11.layer_norm2.bias', \n'text_model.encoder.layers.4.layer_norm2.bias', \n'text_model.encoder.layers.5.self_attn.out_proj.weight', \n'text_model.encoder.layers.3.mlp.fc2.bias']\n- This IS expected if you are initializing CLIPVisionModel from the \ncheckpoint of a model trained on another task or with another \narchitecture (e.g. initializing a BertForSequenceClassification model \nfrom a BertForPreTraining model).\n- This IS NOT expected if you are initializing CLIPVisionModel from the \ncheckpoint of a model that you expect to be exactly identical \n(initializing a BertForSequenceClassification model from a \nBertForSequenceClassification model).\n2023-04-19 16:36:14 | ERROR | stderr | ╭─────────────────────────────── \nTraceback (most recent call last) ────────────────────────────────╮\n2023-04-19 16:36:14 | ERROR | stderr | │ \n/root/anaconda3/lib/python3.10/runpy.py:196 in \n_run_module_as_main                               │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │   193 │   main_globals = \nsys.modules[\"__main__\"].__dict__ │\n2023-04-19 16:36:14 | ERROR | stderr | │   194 │   if alter_argv: │\n2023-04-19 16:36:14 | ERROR | stderr | │   195 │   │   sys.argv[0] = \nmod_spec.origin │\n2023-04-19 16:36:14 | ERROR | stderr | │ ❱ 196 │   return \n_run_code(code, main_globals, \nNone,                                             │\n2023-04-19 16:36:14 | ERROR | stderr | │   197 │   │   │   │ │    \n\"__main__\", mod_spec)                                                 │\n2023-04-19 16:36:14 | ERROR | stderr | │ 198 │\n2023-04-19 16:36:14 | ERROR | stderr | │   199 def run_module(mod_name, \ninit_globals=None, │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │ \n/root/anaconda3/lib/python3.10/runpy.py:86 in \n_run_code                                          │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │    83 │   │   │   │ │      \n__loader__ = loader,                                                │\n2023-04-19 16:36:14 | ERROR | stderr | │    84 │   │   │   │ │      \n__package__ = pkg_name,                                             │\n2023-04-19 16:36:14 | ERROR | stderr | │    85 │   │   │   │ │      \n__spec__ = mod_spec)                                                │\n2023-04-19 16:36:14 | ERROR | stderr | │ ❱  86 │   exec(code, run_globals) │\n2023-04-19 16:36:14 | ERROR | stderr | │    87 │   return run_globals │\n2023-04-19 16:36:14 | ERROR | stderr | │ 88 │\n2023-04-19 16:36:14 | ERROR | stderr | │    89 def \n_run_module_code(code, \ninit_globals=None,                                              │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │ \n/root/LLaVA/llava/serve/model_worker.py:361 in \n<module>                                          │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │   358 │   args = \nparser.parse_args() │\n2023-04-19 16:36:14 | ERROR | stderr | │   359 │ logger.info(f\"args: \n{args}\") │\n2023-04-19 16:36:14 | ERROR | stderr | │   360 │ │\n2023-04-19 16:36:14 | ERROR | stderr | │ ❱ 361 │   worker = \nModelWorker(args.controller_address, │\n2023-04-19 16:36:14 | ERROR | stderr | │   362 │   │   │   │   │ │ \nargs.worker_address, │\n2023-04-19 16:36:14 | ERROR | stderr | │   363 │   │   │   │   │ │ \nworker_id, │\n2023-04-19 16:36:14 | ERROR | stderr | │   364 │   │   │   │   │ │ \nargs.no_register, │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │ \n/root/LLaVA/llava/serve/model_worker.py:118 in \n__init__                                          │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │   115 │   │ \nlogger.info(f\"Loading the model {self.model_name} on worker {worker_id} \n...\")      │\n2023-04-19 16:36:14 | ERROR | stderr | │   116 │   │ self.is_multi_modal \n= is_multi_modal                                               │\n2023-04-19 16:36:14 | ERROR | stderr | │   117 │   │ \nself.keep_aspect_ratio = \nkeep_aspect_ratio                                         │\n2023-04-19 16:36:14 | ERROR | stderr | │ ❱ 118 │   │ self.tokenizer, \nself.model, self.image_processor, self.context_len = load_model(   │\n2023-04-19 16:36:14 | ERROR | stderr | │   119 │   │   │ model_path, \nnum_gpus, is_multi_modal)                                          │\n2023-04-19 16:36:14 | ERROR | stderr | │   120 │ │ │\n2023-04-19 16:36:14 | ERROR | stderr | │   121 │   │   if not no_register: │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │ \n/root/LLaVA/llava/serve/model_worker.py:85 in \nload_model                                         │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │    82 │   │   │ \nvision_config.im_start_token, vision_config.im_end_token = \ntokenizer.convert   │\n2023-04-19 16:36:14 | ERROR | stderr | │    83 │ │\n2023-04-19 16:36:14 | ERROR | stderr | │    84 │   if num_gpus == 1: │\n2023-04-19 16:36:14 | ERROR | stderr | │ ❱  85 │   │ model.cuda() │\n2023-04-19 16:36:14 | ERROR | stderr | │    86 │ │\n2023-04-19 16:36:14 | ERROR | stderr | │    87 │   if \nhasattr(model.config, \n\"max_sequence_length\"):                                       │\n2023-04-19 16:36:14 | ERROR | stderr | │    88 │   │   context_len = \nmodel.config.max_sequence_length │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │ \n/root/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:905 \nin cuda              │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │    902 │   │ Returns: │\n2023-04-19 16:36:14 | ERROR | stderr | │    903 │   │   │ Module: self │\n2023-04-19 16:36:14 | ERROR | stderr | │    904 │   │ \"\"\" │\n2023-04-19 16:36:14 | ERROR | stderr | │ ❱  905 │   │   return \nself._apply(lambda t: t.cuda(device))                                      │\n2023-04-19 16:36:14 | ERROR | stderr | │    906 │ │\n2023-04-19 16:36:14 | ERROR | stderr | │    907 │   def ipu(self: T, \ndevice: Optional[Union[int, device]] = None) -> T:                   │\n2023-04-19 16:36:14 | ERROR | stderr | │    908 │   │   r\"\"\"Moves all \nmodel parameters and buffers to the IPU.                            │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │ \n/root/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:797 \nin _apply            │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │    794 │ │\n2023-04-19 16:36:14 | ERROR | stderr | │    795 │   def _apply(self, fn): │\n2023-04-19 16:36:14 | ERROR | stderr | │    796 │   │   for module in \nself.children(): │\n2023-04-19 16:36:14 | ERROR | stderr | │ ❱  797 │   │   │ \nmodule._apply(fn) │\n2023-04-19 16:36:14 | ERROR | stderr | │    798 │ │ │\n2023-04-19 16:36:14 | ERROR | stderr | │    799 │   │   def \ncompute_should_use_set_data(tensor, \ntensor_applied):                          │\n2023-04-19 16:36:14 | ERROR | stderr | │    800 │   │   │   if \ntorch._has_compatible_shallow_copy_type(tensor, tensor_applied):           │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │ \n/root/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:797 \nin _apply            │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │    794 │ │\n2023-04-19 16:36:14 | ERROR | stderr | │    795 │   def _apply(self, fn): │\n2023-04-19 16:36:14 | ERROR | stderr | │    796 │   │   for module in \nself.children(): │\n2023-04-19 16:36:14 | ERROR | stderr | │ ❱  797 │   │   │ \nmodule._apply(fn) │\n2023-04-19 16:36:14 | ERROR | stderr | │    798 │ │ │\n2023-04-19 16:36:14 | ERROR | stderr | │    799 │   │   def \ncompute_should_use_set_data(tensor, \ntensor_applied):                          │\n2023-04-19 16:36:14 | ERROR | stderr | │    800 │   │   │   if \ntorch._has_compatible_shallow_copy_type(tensor, tensor_applied):           │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │ \n/root/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:797 \nin _apply            │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │    794 │ │\n2023-04-19 16:36:14 | ERROR | stderr | │    795 │   def _apply(self, fn): │\n2023-04-19 16:36:14 | ERROR | stderr | │    796 │   │   for module in \nself.children(): │\n2023-04-19 16:36:14 | ERROR | stderr | │ ❱  797 │   │   │ \nmodule._apply(fn) │\n2023-04-19 16:36:14 | ERROR | stderr | │    798 │ │ │\n2023-04-19 16:36:14 | ERROR | stderr | │    799 │   │   def \ncompute_should_use_set_data(tensor, \ntensor_applied):                          │\n2023-04-19 16:36:14 | ERROR | stderr | │    800 │   │   │   if \ntorch._has_compatible_shallow_copy_type(tensor, tensor_applied):           │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │ \n/root/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:797 \nin _apply            │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │    794 │ │\n2023-04-19 16:36:14 | ERROR | stderr | │    795 │   def _apply(self, fn): │\n2023-04-19 16:36:14 | ERROR | stderr | │    796 │   │   for module in \nself.children(): │\n2023-04-19 16:36:14 | ERROR | stderr | │ ❱  797 │   │   │ \nmodule._apply(fn) │\n2023-04-19 16:36:14 | ERROR | stderr | │    798 │ │ │\n2023-04-19 16:36:14 | ERROR | stderr | │    799 │   │   def \ncompute_should_use_set_data(tensor, \ntensor_applied):                          │\n2023-04-19 16:36:14 | ERROR | stderr | │    800 │   │   │   if \ntorch._has_compatible_shallow_copy_type(tensor, tensor_applied):           │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │ \n/root/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:797 \nin _apply            │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │    794 │ │\n2023-04-19 16:36:14 | ERROR | stderr | │    795 │   def _apply(self, fn): │\n2023-04-19 16:36:14 | ERROR | stderr | │    796 │   │   for module in \nself.children(): │\n2023-04-19 16:36:14 | ERROR | stderr | │ ❱  797 │   │   │ \nmodule._apply(fn) │\n2023-04-19 16:36:14 | ERROR | stderr | │    798 │ │ │\n2023-04-19 16:36:14 | ERROR | stderr | │    799 │   │   def \ncompute_should_use_set_data(tensor, \ntensor_applied):                          │\n2023-04-19 16:36:14 | ERROR | stderr | │    800 │   │   │   if \ntorch._has_compatible_shallow_copy_type(tensor, tensor_applied):           │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │ \n/root/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:820 \nin _apply            │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │    817 │   │   │   # track \nautograd history of `param_applied`, so we have to use                │\n2023-04-19 16:36:14 | ERROR | stderr | │    818 │   │   │   # `with \ntorch.no_grad():` │\n2023-04-19 16:36:14 | ERROR | stderr | │    819 │   │   │   with \ntorch.no_grad(): │\n2023-04-19 16:36:14 | ERROR | stderr | │ ❱  820 │   │   │   │ \nparam_applied = fn(param)                                                 │\n2023-04-19 16:36:14 | ERROR | stderr | │    821 │   │   │ \nshould_use_set_data = compute_should_use_set_data(param, \nparam_applied)       │\n2023-04-19 16:36:14 | ERROR | stderr | │    822 │   │   │   if \nshould_use_set_data: │\n2023-04-19 16:36:14 | ERROR | stderr | │    823 │   │   │   │ param.data \n= param_applied                                                │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │ \n/root/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:905 \nin <lambda>          │\n2023-04-19 16:36:14 | ERROR | stderr | │ │\n2023-04-19 16:36:14 | ERROR | stderr | │    902 │   │ Returns: │\n2023-04-19 16:36:14 | ERROR | stderr | │    903 │   │   │ Module: self │\n2023-04-19 16:36:14 | ERROR | stderr | │    904 │   │ \"\"\" │\n2023-04-19 16:36:14 | ERROR | stderr | │ ❱  905 │   │   return \nself._apply(lambda t: t.cuda(device))                                      │\n2023-04-19 16:36:14 | ERROR | stderr | │    906 │ │\n2023-04-19 16:36:14 | ERROR | stderr | │    907 │   def ipu(self: T, \ndevice: Optional[Union[int, device]] = None) -> T:                   │\n2023-04-19 16:36:14 | ERROR | stderr | │    908 │   │   r\"\"\"Moves all \nmodel parameters and buffers to the IPU.                            │\n2023-04-19 16:36:14 | ERROR | stderr | \n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n2023-04-19 16:36:14 | ERROR | stderr | OutOfMemoryError: CUDA out of \nmemory. Tried to allocate 136.00 MiB (GPU 0; 23.87 GiB total capacity; \n23.71 GiB already allocated;\n2023-04-19 16:36:14 | ERROR | stderr | 19.62 MiB free; 23.71 GiB \nreserved in total by PyTorch) If reserved memory is >> allocated memory \ntry setting max_split_size_mb to\n2023-04-19 16:36:14 | ERROR | stderr | avoid fragmentation.  See \ndocumentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\nOn 19/04/2023 17:30, Haotian Liu wrote:\n> |git clone https://github.com/haotian-liu/transformers_llava.git \n> transformers pip install -e ./transformers|\n-- \n\nAl Costa\nCEO\n37th Floor, 1 Canada Square, Canary Wharf, London\nUnited Kingdom, E145 AA\nTel: +44 1737669662\nMob: +44 7892928973\nMail: ***@***.***\nWeb: http://www.tekntrash.ai\nBefore printing this message, Be sure it is necessary.\n</Comment>\n<Comment by kagevazquez at 2023-04-19T18:53:30Z>\nwhats the minimum vram needed and would bitsandbytes help lower the vram requirement?\n</Comment>\n<Comment by haotian-liu at 2023-04-21T00:20:43Z>\n@tekntrash @kagevazquez\r\n\r\nI updated the code base today, to support inferencing on 2x 3090s.  It needs around 28GB memory currently for inference.\r\n\r\nhttps://github.com/haotian-liu/LLaVA#launch-a-model-worker-multiple-gpus-when-gpu-vram--24gb\r\n\r\nbitsandbytes will definitely help, and we'll update instructions on that as well this week.\n</Comment>\n<Comment by 152334H at 2023-04-21T02:41:14Z>\nbitsandbytes sounds great. I haven't actually tried LLaVA inference yet, but I see no reason why it shouldn't be hackable to work on just one 3090 (which I accomplished with MiniGPT-4 previously).\n</Comment>\n<Comment by microhu at 2023-05-04T05:45:59Z>\n> This is strange. Can you share the `config.json` under your converted LLaVA model folder? More specifically, do you see these lines\r\n> \r\n> ```\r\n>   \"mm_hidden_size\": 1024,\r\n>   \"mm_use_im_start_end\": true,\r\n>   \"mm_vision_select_layer\": -2,\r\n>   \"mm_vision_tower\": \"openai/clip-vit-large-patch14\",\r\n> ```\r\n\r\nI have updated the codebase to v0.1 but has the same error when Launch a model worker\r\nthe error says \" AttributeError: 'LlamaModel' object has no attribute 'vision_tower\"\n</Comment>\n<Comment by haotian-liu at 2023-05-04T05:49:35Z>\nHi @microhu, can you share the command you use and the full error log?  This can help me better figure out what the issue you are facing.  Thanks.\n</Comment>\n<Comment by microhu at 2023-05-04T15:12:40Z>\n> Hi @microhu, can you share the command you use and the full error log? This can help me better figure out what the issue you are facing. Thanks.\r\n\r\nit happens when i  Launch a model worker\r\n\r\n python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path /mnt/dolphinfs/hdd_pool/docker/user/hadoop-mt-ocr/huwenping/HuggingFace_models/LLaVA-13b-v0   --multi-modal\r\n2023-05-04 23:09:27 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mt-ocr/huwenping/HuggingFace_models/LLaVA-13b-v0', model_name=None, multi_modal=True, keep_aspect_ratio=False, num_gpus=1, limit_model_concurrency=5, stream_interval=2, no_register=False)\r\n2023-05-04 23:09:27 | INFO | model_worker | Loading the model LLaVA-13b-v0 on worker c64123 ...\r\nLoading checkpoint shards:   0%|                                                                        | 0/3 [00:00<?, ?it/s]\r\nLoading checkpoint shards:  33%|█████████████████████▎                                          | 1/3 [00:29<00:59, 29.67s/it]\r\nLoading checkpoint shards:  67%|██████████████████████████████████████████▋                     | 2/3 [00:47<00:22, 22.45s/it]\r\nLoading checkpoint shards: 100%|████████████████████████████████████████████████████████████████| 3/3 [01:08<00:00, 22.06s/it]\r\nLoading checkpoint shards: 100%|████████████████████████████████████████████████████████████████| 3/3 [01:08<00:00, 22.90s/it]\r\n\r\nthe error log:\r\n2023-05-04 23:10:40 | ERROR | stderr | Traceback (most recent call last):\r\n2023-05-04 23:10:40 | ERROR | stderr |   File \"/mnt/dolphinfs/ssd_pool/docker/user/hadoop-mt-ocr/huwenping/conda_envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n2023-05-04 23:10:40 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n2023-05-04 23:10:40 | ERROR | stderr |   File \"/mnt/dolphinfs/ssd_pool/docker/user/hadoop-mt-ocr/huwenping/conda_envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n2023-05-04 23:10:40 | ERROR | stderr |     exec(code, run_globals)\r\n2023-05-04 23:10:40 | ERROR | stderr |   File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mt-ocr/huwenping/Repos/LLaVA/llava/serve/model_worker.py\", line 363, in <module>\r\n2023-05-04 23:10:40 | ERROR | stderr |     worker = ModelWorker(args.controller_address,\r\n2023-05-04 23:10:40 | ERROR | stderr |   File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mt-ocr/huwenping/Repos/LLaVA/llava/serve/model_worker.py\", line 120, in __init__\r\n2023-05-04 23:10:40 | ERROR | stderr |     self.tokenizer, self.model, self.image_processor, self.context_len = load_model(\r\n2023-05-04 23:10:40 | ERROR | stderr |   File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mt-ocr/huwenping/Repos/LLaVA/llava/serve/model_worker.py\", line 73, in load_model\r\n2023-05-04 23:10:40 | ERROR | stderr |     vision_tower = model.model.vision_tower[0]\r\n2023-05-04 23:10:40 | ERROR | stderr |   File \"/mnt/dolphinfs/ssd_pool/docker/user/hadoop-mt-ocr/huwenping/conda_envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1614, in __getattr__\r\n2023-05-04 23:10:40 | ERROR | stderr |     raise AttributeError(\"'{}' object has no attribute '{}'\".format(\r\n2023-05-04 23:10:40 | ERROR | stderr | AttributeError: 'LlamaModel' object has no attribute 'vision_tower'\n</Comment>\n<Comment by microhu at 2023-05-04T23:20:02Z>\n> > Hi @microhu, can you share the command you use and the full error log? This can help me better figure out what the issue you are facing. Thanks.\r\n> \r\n> it happens when i Launch a model worker\r\n> \r\n> python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path /mnt/dolphinfs/hdd_pool/docker/user/hadoop-mt-ocr/huwenping/HuggingFace_models/LLaVA-13b-v0 --multi-modal 2023-05-04 23:09:27 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mt-ocr/huwenping/HuggingFace_models/LLaVA-13b-v0', model_name=None, multi_modal=True, keep_aspect_ratio=False, num_gpus=1, limit_model_concurrency=5, stream_interval=2, no_register=False) 2023-05-04 23:09:27 | INFO | model_worker | Loading the model LLaVA-13b-v0 on worker c64123 ... Loading checkpoint shards: 0%| | 0/3 [00:00<?, ?it/s] Loading checkpoint shards: 33%|█████████████████████▎ | 1/3 [00:29<00:59, 29.67s/it] Loading checkpoint shards: 67%|██████████████████████████████████████████▋ | 2/3 [00:47<00:22, 22.45s/it] Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████| 3/3 [01:08<00:00, 22.06s/it] Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████| 3/3 [01:08<00:00, 22.90s/it]\r\n> \r\n> the error log: 2023-05-04 23:10:40 | ERROR | stderr | Traceback (most recent call last): 2023-05-04 23:10:40 | ERROR | stderr | File \"/mnt/dolphinfs/ssd_pool/docker/user/hadoop-mt-ocr/huwenping/conda_envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main 2023-05-04 23:10:40 | ERROR | stderr | return _run_code(code, main_globals, None, 2023-05-04 23:10:40 | ERROR | stderr | File \"/mnt/dolphinfs/ssd_pool/docker/user/hadoop-mt-ocr/huwenping/conda_envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code 2023-05-04 23:10:40 | ERROR | stderr | exec(code, run_globals) 2023-05-04 23:10:40 | ERROR | stderr | File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mt-ocr/huwenping/Repos/LLaVA/llava/serve/model_worker.py\", line 363, in 2023-05-04 23:10:40 | ERROR | stderr | worker = ModelWorker(args.controller_address, 2023-05-04 23:10:40 | ERROR | stderr | File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mt-ocr/huwenping/Repos/LLaVA/llava/serve/model_worker.py\", line 120, in **init** 2023-05-04 23:10:40 | ERROR | stderr | self.tokenizer, self.model, self.image_processor, self.context_len = load_model( 2023-05-04 23:10:40 | ERROR | stderr | File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mt-ocr/huwenping/Repos/LLaVA/llava/serve/model_worker.py\", line 73, in load_model 2023-05-04 23:10:40 | ERROR | stderr | vision_tower = model.model.vision_tower[0] 2023-05-04 23:10:40 | ERROR | stderr | File \"/mnt/dolphinfs/ssd_pool/docker/user/hadoop-mt-ocr/huwenping/conda_envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1614, in **getattr** 2023-05-04 23:10:40 | ERROR | stderr | raise AttributeError(\"'{}' object has no attribute '{}'\".format( 2023-05-04 23:10:40 | ERROR | stderr | AttributeError: 'LlamaModel' object has no attribute 'vision_tower'\r\n\r\n@haotian-liu  above is the error message. and one more question what is the required env to install 'flash-attn', I tried different configurations but failed to install flash-attn succ.\n</Comment>\n<Comment by haotian-liu at 2023-05-04T23:25:22Z>\nHi @microhu, can you try pulling the latest repo again, as it seems that your code base has not upgraded to v0.1 yet.  For example, you added `--multi-modal`, which is totally fine, but it should print out a warning message [here](https://github.com/haotian-liu/LLaVA/blob/main/llava/serve/model_worker.py#L359) if you are using the latest code base.  Please also make sure the `transformers` is the correct the version following the instructions [here](https://github.com/haotian-liu/LLaVA#upgrade-to-v01), thanks!\n</Comment>\n<Comment by haotian-liu at 2023-05-04T23:26:47Z>\n@microhu also regarding the flash-attn, if you create a new environment following the installation commands, it should not raise issues usually.  You may paste your error log of `flash-attn` here.\n</Comment>\n<Comment by microhu at 2023-05-04T23:36:36Z>\n> here\r\n\r\n@haotian-liu  Thanks for your quick reply.  The error message is is quite long, looks like below\r\n\r\n  ptxas info    : Used 201 registers, 632 bytes cmem[0], 16 bytes cmem[2]\r\n    ptxas info    : Compiling entry function '_Z29fmha_bwd_dq_dk_dv_loop_kernelI18FMHA_kernel_traitsILi128ELi64ELi16ELi1ELi8ELj8E13__nv_bfloat16ELb1ELb0ELin1EEv17FMHA_dgrad_params' for 'sm_80'\r\n    ptxas info    : Function properties for _Z29fmha_bwd_dq_dk_dv_loop_kernelI18FMHA_kernel_traitsILi128ELi64ELi16ELi1ELi8ELj8E13__nv_bfloat16ELb1ELb0ELin1EEv17FMHA_dgrad_params\r\n        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\n    ptxas info    : Used 225 registers, 632 bytes cmem[0], 16 bytes cmem[2]\r\n    ptxas info    : Compiling entry function '_Z29fmha_bwd_dq_dk_dv_loop_kernelI18FMHA_kernel_traitsILi128ELi64ELi16ELi1ELi8ELj8E13__nv_bfloat16ELb1ELb1ELin1EEv17FMHA_dgrad_params' for 'sm_80'\r\n    ptxas info    : Function properties for _Z29fmha_bwd_dq_dk_dv_loop_kernelI18FMHA_kernel_traitsILi128ELi64ELi16ELi1ELi8ELj8E13__nv_bfloat16ELb1ELb1ELin1EEv17FMHA_dgrad_params\r\n        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\n    ptxas info    : Used 229 registers, 632 bytes cmem[0], 16 bytes cmem[2]\r\n    ninja: build stopped: subcommand failed.\r\n    Traceback (most recent call last):\r\n      File \"/mnt/dolphinfs/ssd_pool/docker/user/hadoop-mt-ocr/huwenping/conda_envs/fastchat_native/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 1893, in _run_ninja_build\r\n        subprocess.run(\r\n      File \"/mnt/dolphinfs/ssd_pool/docker/user/hadoop-mt-ocr/huwenping/conda_envs/fastchat_native/lib/python3.8/subprocess.py\", line 516, in run\r\n        raise CalledProcessError(retcode, process.args,\r\n    subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\r\nThe above exception was the direct cause of the following exception:\r\n\r\n    Traceback (most recent call last):\r\n      File \"<string>\", line 1, in <module>\r\n      File \"/tmp/pip-install-wvh_2hdw/flash-attn_f250c79ec2304275af0cdf40b9a09c77/setup.py\", line 163, in <module>\r\n        setup(\r\n      File \"/mnt/dolphinfs/ssd_pool/docker/user/hadoop-mt-ocr/huwenping/conda_envs/fastchat_native/lib/python3.8/site-packages/setuptools/__init__.py\", line 153, in setup\r\n        return distutils.core.setup(**attrs)\r\n      File \"/mnt/dolphinfs/ssd_pool/docker/user/hadoop-mt-ocr/huwenping/conda_envs/fastchat_native/lib/python3.8/distutils/core.py\", line 148, in setup\r\n        dist.run_commands()\r\n      File \"/mnt/dolphinfs/ssd_pool/docker/user/hadoop-mt-ocr/huwenping/conda_envs/fastchat_native/lib/python3.8/distutils/dist.py\", line 966, in run_commands\r\n        self.run_command(cmd)\r\n      File \"/mnt/dolphinfs/ssd_pool/docker/user/hadoop-mt-ocr/huwenping/conda_envs/fastchat_native/lib/python3.8/distutils/dist.py\", line 985, in run_command\r\n        cmd_obj.run()\r\n      File \"/mnt/dolphinfs/ssd_pool/docker/user/hadoop-mt-ocr/huwenping/conda_envs/fastchat_native/lib/python3.8/site-packages/setuptools/command/install.py\", line 61, in run\n</Comment>\n<Comment by haotian-liu at 2023-05-05T02:30:57Z>\n@microhu I do not find the exact error in this log, maybe you can upload to gist, or you can reach out to the `flash-attn` repo for help as well.  And are you able to launch the model worker successfully after upgrading?\n</Comment>\n<Comment by penghe2021 at 2023-05-09T08:09:56Z>\nHi, I am facing the same issue with the latest code and 13B v0 model. \r\n\r\nThe CLI script can load the 13B v1-1 without problem, but when I try to load the 13B v0 model, it will give\r\n\r\n```\r\n    vision_tower = model.model.vision_tower[0]\r\n  File \"xxxxxxxxxxxxxxx/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1614, in __getattr__\r\n    raise AttributeError(\"'{}' object has no attribute '{}'\".format(\r\nAttributeError: 'LlamaModel' object has no attribute 'vision_tower'\r\n```\r\n\r\nI guess there is a version conflict.\n</Comment>\n<Comment by haotian-liu at 2023-05-09T22:49:52Z>\nHi @penghe2021, are you using the latest code base?  You may need to use the latest code base in order to load both v0 and v1 checkpoints.  Also, since you only provide the partial log, I am assuming this is a model_worker.py?\n</Comment>\n<Comment by penghe2021 at 2023-05-09T23:13:19Z>\nHi Liu. Yes, I am using the latest code, but I just realized that I haven't update the 13B v0 model, let me try to download the latest v0 model and try with the code.\n</Comment>\n<Comment by haotian-liu at 2023-05-09T23:15:14Z>\n@penghe2021 Oh you do not need to update the 13B model in order to use the latest code.  And just to confirm, are you seeing this error in `model_worker.py`?\n</Comment>\n<Comment by haotian-liu at 2023-05-09T23:20:50Z>\n@penghe2021 I just verified that the latest code work with the v0 model.  Do you see this line when you load your model?  If you see this line, then the code should be correct and running fine.\r\n\r\nAnd please make sure that the model name includes \"llava\": `if 'llava' in model_path.lower():`\r\n\r\n```\r\n2023-05-09 18:18:42 | INFO | model_worker | Loading the model LLaVA-13B-v0 on worker ae4bec ...\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\n```\n</Comment>\n<Comment by penghe2021 at 2023-05-09T23:47:16Z>\nI am using the old version of CLI code, maybe this is the issue, give me some time to try with the new code.\n</Comment>\n<Comment by penghe2021 at 2023-05-10T18:19:13Z>\nUPDATE: after I download the latest 13B delta v0 weight and regenerate the llava model, it works without problem. Thanks for the help!\n</Comment>\n<Comment by microhu at 2023-05-11T10:43:24Z>\n> @microhu I do not find the exact error in this log, maybe you can upload to gist, or you can reach out to the `flash-attn` repo for help as well. And are you able to launch the model worker successfully after upgrading?\r\n\r\nyes. I solved all the issues by re-installing  the whole project on a docker with newer cuda driver. Thanks\n</Comment>\n<Comment by BountyMage at 2024-08-16T01:23:24Z>\n> This is strange. Can you share the `config.json` under your converted LLaVA model folder? More specifically, do you see these lines\r\n> \r\n> ```\r\n>   \"mm_hidden_size\": 1024,\r\n>   \"mm_use_im_start_end\": true,\r\n>   \"mm_vision_select_layer\": -2,\r\n>   \"mm_vision_tower\": \"openai/clip-vit-large-patch14\",\r\n> ```\r\n\r\nHi, Liu. How do i get a LLaVA model converted to contain vision tower related config items?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 14,
    "state": "closed",
    "created_by": "Sequential-circuits",
    "created_at": "2023-04-19T11:07:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/14</URL>\n\n<TITLE>NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.</TITLE>\n\n<BODY>can't use it as it always says NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-19T15:36:35Z>\nThis seems to be an error in the backend.\r\n1. can you see the model list on the top left?\r\n2. There may be an error in the model worker, can you paste the error message here?\n</Comment>\n<Comment by DifferentComputers at 2023-04-25T16:02:40Z>\nI get the same error but I suspect it's because I may be running it on entirely inadequate hardware. Alternately, I may not have the model data installed correctly.\r\n\r\nI don't see any model worker error. that would be appearing on the command line process, correct?\r\n\r\nI don't see any \"model list\", at least not on the webpage where the \"NETWORK ERROR\" appears as every response to an attempt to use the chatBot.\n</Comment>\n<Comment by haotian-liu at 2023-05-01T04:01:57Z>\nHi @DifferentComputers, sorry that I just saw this comment.  It is not a normal behavior of an empty model list.  You need to start the gradio demo after the worker is fully loaded, so as to get the model list.  Please let me know if you have further concerns.\n</Comment>\n<Comment by donggeai at 2023-05-03T08:13:21Z>\nI met the same problem \"NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.\", but my model list is not empty, there is a model I have applied the delta before, but still the error, some details are as follow:\r\n![image](https://user-images.githubusercontent.com/53808058/235863716-a311bf84-8acc-4312-9e46-1d7e9e09d396.png)\r\n\r\n![image](https://user-images.githubusercontent.com/53808058/235863549-929c388b-ce25-46b1-97f2-28f394a49e58.png)\r\n\r\nthank you!\n</Comment>\n<Comment by haotian-liu at 2023-05-04T05:56:53Z>\n@corleytd Hi please see my response in #89, thanks.\n</Comment>\n<Comment by aymenabid-lab at 2024-03-04T12:05:51Z>\nI have the folowing problem; actually I want load model from my pc\r\n\r\n\r\n(llava) C:\\Users\\aymen\\LLaVA>python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path C:/Users/aymen/llava-1.5-7b-hf\r\n2024-03-04 12:43:53 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='C:/Users/aymen/llava-1.5-7b-hf', model_base=None, model_name=None, device='cuda', multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=False, use_flash_attn=False)\r\n2024-03-04 12:43:53 | INFO | model_worker | Loading the model llava-1.5-7b-hf on worker 62e548 ...\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\n2024-03-04 12:43:53 | INFO | accelerate.utils.modeling | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\r\nLoading checkpoint shards:   0%|                                                                                                                                                            | 0/3 [00:00<?, ?it/s]\r\nLoading checkpoint shards:  33%|█████████████████████████████████████████████████▎                                                                                                  | 1/3 [00:00<00:00,  5.88it/s]\r\nLoading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  8.46it/s]\r\nLoading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  8.10it/s]\r\n2024-03-04 12:54:33 | ERROR | stderr |\r\nSome weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at C:/Users/aymen/llava-1.5-7b-hf and are newly initialized: ['layers.8.self_attn.o_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.22.mlp.down_proj.weight', 'layers.29.mlp.gate_proj.weight', 'layers.1.self_attn.k_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.18.mlp.up_proj.weight', 'layers.19.mlp.down_proj.weight', 'layers.19.mlp.up_proj.weight', 'layers.22.mlp.gate_proj.weight', 'layers.27.self_attn.v_proj.weight', 'layers.18.mlp.gate_proj.weight', 'layers.8.mlp.down_proj.weight', 'layers.2.mlp.gate_proj.weight', 'layers.26.mlp.up_proj.weight', 'layers.0.self_attn.k_proj.weight', 'layers.17.self_attn.k_proj.weight', 'layers.20.mlp.up_proj.weight', 'layers.29.mlp.up_proj.weight', 'layers.23.self_attn.q_proj.weight', 'layers.19.self_attn.k_proj.weight', 'layers.20.post_attention_layernorm.weight', 'layers.24.mlp.up_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.21.input_layernorm.weight', 'layers.12.input_layernorm.weight', 'layers.16.input_layernorm.weight', 'layers.31.self_attn.k_proj.weight', 'layers.19.self_attn.o_proj.weight', 'layers.9.self_attn.k_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.15.self_attn.v_proj.weight', 'layers.20.mlp.down_proj.weight', 'layers.28.self_attn.k_proj.weight', 'layers.16.self_attn.q_proj.weight', 'layers.12.self_attn.k_proj.weight', 'layers.29.self_attn.q_proj.weight', 'layers.24.self_attn.k_proj.weight', 'layers.13.self_attn.q_proj.weight', 'layers.30.self_attn.k_proj.weight', 'layers.20.self_attn.o_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.28.self_attn.v_proj.weight', 'layers.17.mlp.up_proj.weight', 'layers.24.self_attn.v_proj.weight', 'layers.1.self_attn.q_proj.weight', 'layers.6.mlp.down_proj.weight', 'layers.27.post_attention_layernorm.weight', 'layers.30.mlp.gate_proj.weight', 'layers.18.self_attn.o_proj.weight', 'layers.28.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.12.mlp.gate_proj.weight', 'layers.9.self_attn.q_proj.weight', 'layers.4.self_attn.q_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.21.mlp.down_proj.weight', 'layers.31.self_attn.q_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.24.post_attention_layernorm.weight', 'layers.22.self_attn.k_proj.weight', 'layers.11.self_attn.k_proj.weight', 'layers.19.self_attn.q_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.22.mlp.up_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.2.mlp.down_proj.weight', 'layers.24.input_layernorm.weight', 'layers.25.post_attention_layernorm.weight', 'layers.9.post_attention_layernorm.weight', 'layers.22.input_layernorm.weight', 'layers.3.self_attn.v_proj.weight', 'layers.5.self_attn.k_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.24.self_attn.o_proj.weight', 'layers.23.self_attn.v_proj.weight', 'layers.28.mlp.gate_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.10.self_attn.q_proj.weight', 'layers.11.mlp.down_proj.weight', 'layers.16.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.13.mlp.gate_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.12.mlp.down_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.22.self_attn.v_proj.weight', 'layers.9.input_layernorm.weight', 'layers.25.self_attn.v_proj.weight', 'layers.23.input_layernorm.weight', 'layers.6.mlp.up_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.16.mlp.gate_proj.weight', 'layers.25.self_attn.k_proj.weight', 'layers.16.self_attn.o_proj.weight', 'layers.16.mlp.down_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.21.post_attention_layernorm.weight', 'layers.13.post_attention_layernorm.weight', 'layers.23.mlp.up_proj.weight', 'layers.30.self_attn.q_proj.weight', 'layers.30.self_attn.v_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.11.self_attn.q_proj.weight', 'layers.6.self_attn.q_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.20.input_layernorm.weight', 'layers.31.self_attn.v_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.21.self_attn.q_proj.weight', 'layers.21.self_attn.k_proj.weight', 'layers.14.mlp.gate_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.15.mlp.down_proj.weight', 'layers.24.mlp.gate_proj.weight', 'layers.4.self_attn.k_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.23.mlp.down_proj.weight', 'layers.8.input_layernorm.weight', 'layers.25.self_attn.o_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.25.mlp.gate_proj.weight', 'layers.29.mlp.down_proj.weight', 'layers.7.mlp.down_proj.weight', 'layers.1.input_layernorm.weight', 'layers.23.self_attn.k_proj.weight', 'layers.30.mlp.up_proj.weight', 'layers.22.post_attention_layernorm.weight', 'layers.26.self_attn.v_proj.weight', 'layers.3.input_layernorm.weight', 'layers.13.mlp.up_proj.weight', 'layers.18.mlp.down_proj.weight', 'layers.13.input_layernorm.weight', 'layers.16.mlp.up_proj.weight', 'layers.26.self_attn.o_proj.weight', 'layers.20.self_attn.q_proj.weight', 'layers.3.self_attn.q_proj.weight', 'layers.15.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.15.mlp.gate_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.2.self_attn.q_proj.weight', 'layers.29.input_layernorm.weight', 'layers.9.self_attn.o_proj.weight', 'layers.24.self_attn.q_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.19.input_layernorm.weight', 'layers.14.input_layernorm.weight', 'layers.3.self_attn.k_proj.weight', 'layers.23.self_attn.o_proj.weight', 'layers.17.mlp.gate_proj.weight', 'layers.31.post_attention_layernorm.weight', 'layers.3.mlp.up_proj.weight', 'layers.19.self_attn.v_proj.weight', 'layers.27.self_attn.q_proj.weight', 'layers.19.post_attention_layernorm.weight', 'layers.2.self_attn.o_proj.weight', 'layers.5.input_layernorm.weight', 'layers.14.post_attention_layernorm.weight', 'layers.6.mlp.gate_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.27.mlp.gate_proj.weight', 'layers.9.self_attn.v_proj.weight', 'layers.28.self_attn.o_proj.weight', 'layers.17.self_attn.v_proj.weight', 'layers.31.input_layernorm.weight', 'layers.24.mlp.down_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.3.mlp.down_proj.weight', 'layers.18.post_attention_layernorm.weight', 'lm_head.weight', 'layers.7.self_attn.v_proj.weight', 'layers.14.self_attn.k_proj.weight', 'layers.10.mlp.down_proj.weight', 'layers.28.post_attention_layernorm.weight', 'layers.8.self_attn.v_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.8.self_attn.q_proj.weight', 'layers.27.input_layernorm.weight', 'layers.2.mlp.up_proj.weight', 'layers.29.self_attn.o_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.29.post_attention_layernorm.weight', 'layers.31.mlp.gate_proj.weight', 'layers.26.post_attention_layernorm.weight', 'layers.26.input_layernorm.weight', 'layers.8.mlp.gate_proj.weight', 'layers.7.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.0.self_attn.q_proj.weight', 'layers.17.mlp.down_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.17.self_attn.q_proj.weight', 'layers.13.self_attn.v_proj.weight', 'layers.21.mlp.gate_proj.weight', 'layers.27.mlp.up_proj.weight', 'layers.10.self_attn.k_proj.weight', 'layers.5.mlp.down_proj.weight', 'layers.27.mlp.down_proj.weight', 'layers.7.self_attn.k_proj.weight', 'layers.30.post_attention_layernorm.weight', 'layers.7.post_attention_layernorm.weight', 'layers.6.self_attn.v_proj.weight', 'layers.11.input_layernorm.weight', 'layers.17.post_attention_layernorm.weight', 'layers.29.self_attn.v_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.26.self_attn.k_proj.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.18.input_layernorm.weight', 'layers.19.mlp.gate_proj.weight', 'layers.6.input_layernorm.weight', 'layers.29.self_attn.k_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.23.mlp.gate_proj.weight', 'layers.22.self_attn.q_proj.weight', 'layers.21.self_attn.o_proj.weight', 'layers.25.mlp.down_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.22.self_attn.o_proj.weight', 'layers.15.self_attn.k_proj.weight', 'layers.18.self_attn.v_proj.weight', 'layers.0.input_layernorm.weight', 'layers.15.post_attention_layernorm.weight', 'layers.31.self_attn.o_proj.weight', 'layers.30.self_attn.o_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.20.self_attn.k_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.31.mlp.down_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.16.self_attn.v_proj.weight', 'layers.16.post_attention_layernorm.weight', 'layers.27.self_attn.k_proj.weight', 'layers.30.mlp.down_proj.weight', 'layers.20.mlp.gate_proj.weight', 'layers.14.mlp.down_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.25.mlp.up_proj.weight', 'layers.12.self_attn.q_proj.weight', 'layers.17.self_attn.o_proj.weight', 'layers.26.self_attn.q_proj.weight', 'layers.30.input_layernorm.weight', 'layers.0.post_attention_layernorm.weight', 'layers.14.self_attn.q_proj.weight', 'layers.31.mlp.up_proj.weight', 'embed_tokens.weight', 'layers.23.post_attention_layernorm.weight', 'layers.7.self_attn.q_proj.weight', 'layers.21.self_attn.v_proj.weight', 'layers.17.input_layernorm.weight', 'layers.10.self_attn.o_proj.weight', 'layers.26.mlp.down_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.13.self_attn.k_proj.weight', 'layers.26.mlp.gate_proj.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.18.self_attn.q_proj.weight', 'layers.21.mlp.up_proj.weight', 'layers.28.mlp.up_proj.weight', 'norm.weight', 'layers.10.mlp.gate_proj.weight', 'layers.8.self_attn.k_proj.weight', 'layers.15.self_attn.q_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.25.self_attn.q_proj.weight', 'layers.25.input_layernorm.weight', 'layers.27.self_attn.o_proj.weight', 'layers.10.input_layernorm.weight', 'layers.28.input_layernorm.weight', 'layers.28.mlp.down_proj.weight', 'layers.4.input_layernorm.weight', 'layers.20.self_attn.v_proj.weight', 'layers.18.self_attn.k_proj.weight']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n2024-03-04 12:54:35 | WARNING | root | Some parameters are on the meta device device because they were offloaded to the cpu.\r\n2024-03-04 12:54:35 | ERROR | stderr | Traceback (most recent call last):\r\n2024-03-04 12:54:35 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\runpy.py\", line 196, in _run_module_as_main\r\n2024-03-04 12:54:35 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n2024-03-04 12:54:35 | ERROR | stderr |   File \"C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\runpy.py\", line 86, in _run_code\r\n2024-03-04 12:54:35 | ERROR | stderr |     exec(code, run_globals)\r\n2024-03-04 12:54:35 | ERROR | stderr |   File \"C:\\Users\\aymen\\LLaVA\\llava\\serve\\model_worker.py\", line 277, in <module>\r\n2024-03-04 12:54:35 | ERROR | stderr |     worker = ModelWorker(args.controller_address,\r\n2024-03-04 12:54:35 | ERROR | stderr |   File \"C:\\Users\\aymen\\LLaVA\\llava\\serve\\model_worker.py\", line 65, in __init__\r\n2024-03-04 12:54:35 | ERROR | stderr |     self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\r\n2024-03-04 12:54:35 | ERROR | stderr |   File \"C:\\Users\\aymen\\LLaVA\\llava\\model\\builder.py\", line 156, in load_pretrained_model\r\n2024-03-04 12:54:35 | ERROR | stderr |     if not vision_tower.is_loaded:\r\n2024-03-04 12:54:35 | ERROR | stderr | AttributeError: 'NoneType' object has no attribute '\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 13,
    "state": "closed",
    "created_by": "PhoebusSi",
    "created_at": "2023-04-19T10:02:21Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/13</URL>\n\n<TITLE>How can I download the raw image data?</TITLE>\n\n<BODY>How can I download the raw image data?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-19T15:37:48Z>\nThank you for the interest in our project. You can download the images from COCO website: https://cocodataset.org/#download\n</Comment>\n<Comment by PhoebusSi at 2023-04-19T15:48:14Z>\nIs it enough for me to download these two: \"2014 Train images [83K/13GB]\" and \"2014 Val images [41K/6GB]\"?\n</Comment>\n<Comment by PhoebusSi at 2023-04-19T15:48:53Z>\nDo I need to download the 2017 version of COCO ?\n</Comment>\n<Comment by haotian-liu at 2023-04-19T15:49:37Z>\nWe used COCO 2014 to build the instruction tuning dataset, so COCO 2014 is sufficient\n</Comment>\n<Comment by PhoebusSi at 2023-04-19T15:53:59Z>\nMany thanks!\n</Comment>\n<Comment by PhanTask at 2023-04-23T07:03:58Z>\n> We used COCO 2014 to build the instruction tuning dataset, so COCO 2014 is sufficient\r\n\r\nHi, does it mean in the current finetune command `--image_folder /Data/haotian/coco/train2017` actually should be the image folder of train2014?\n</Comment>\n<Comment by haotian-liu at 2023-04-23T20:47:35Z>\n@PhanTask Hi sorry for the confusion.  It should be train2014.  I have changed that in the command line instruction.  Thanks.\n</Comment>\n<Comment by PhanTask at 2023-04-23T21:02:57Z>\n> @PhanTask Hi sorry for the confusion. It should be train2014. I have changed that in the command line instruction. Thanks.\r\n\r\nThanks for looking into it, Haotian! Yesterday I launched finetuning with COCO train2017 images and it seems to be no issues (still running with reduced loss). The image_id in the 150K instruction set indicates they might come from train2017 (e.g., no COCO_train2014_ prefix as seen in train2014). If I launch training with train2014, it reports file name missing issue. Could you please confirm if the finetuning images are actually from train2017? Thanks.\r\n\r\nSince COCO 2014 and 2017 uses the same image sets only with different data split, so that won't really be an issue I believe.\n</Comment>\n<Comment by haotian-liu at 2023-04-23T22:56:42Z>\nHi @PhanTask the images are coming from COCO 2014, but I may have renamed the file due to some historic reasons.  The COCO 2017 training image is a super set of COCO 2014, so you will not notice any issues when training.  The current set is only using COCO 2014 training images.  This makes sure that we can use the coco 2014 validation set for evaluation.\r\n\r\nI may modify the code this week, so that it automatically recognizes the COCO 2014 naming convention, and it will not raise errors when you use either of 2014/2017 split.\n</Comment>\n<Comment by PhanTask at 2023-04-23T23:54:05Z>\n> Hi @PhanTask the images are coming from COCO 2014, but I may have renamed the file due to some historic reasons. The COCO 2017 training image is a super set of COCO 2014, so you will not notice any issues when training. The current set is only using COCO 2014 training images. This makes sure that we can use the coco 2014 validation set for evaluation.\r\n> \r\n> I may modify the code this week, so that it automatically recognizes the COCO 2014 naming convention, and it will not raise errors when you use either of 2014/2017 split.\r\n\r\nGreat, thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 12,
    "state": "closed",
    "created_by": "dongxiaolong",
    "created_at": "2023-04-19T05:57:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/12</URL>\n\n<TITLE>I think this method looks like MM-COT, What's the difference with it except used llm?</TITLE>\n\n<BODY>Absolutely awesome work. Firstly thanks for the instruct tuning data that contributed to the community. Just confused with above question mentioned in the title.</BODY>\n\n<COMMENTS>\n<Comment by 152334H at 2023-04-19T07:20:17Z>\nthe other important part is the synthetic gpt-4 based dataset.\n</Comment>\n<Comment by ChunyuanLI at 2023-04-19T20:57:12Z>\nThe section of fine-tuning on ScienceQA is similar with MM-COT, in terms of architectures and data organization. One major finding in MM-CoT is that prediction order matters: reason-then-answer, and thus it is called \"CoT\". In our study, we find that the \"CoT\" claim is not very important.  See the evidence in our paper:\r\n\r\n> Chain-of-thoughts. To decide the order between the answer and reasoning process in the model prediction, we run both variants and observe that answer-first reports the best number 89.77% accuracy in 12 epochs, while reasoning-first can quickly reach 89.77% accuracy in 6 epochs, but no further improvement with more training (89.96%). Training the model for 24 epochs does not improve the performance. We conclude that CoT-like reasoning-first strategy can largely improve convergence speed, but contributes relatively little to the final performance.\r\n\r\nNow, for both papers, the most performance gain is the use of vision encoder and the end-to-end training on ScienceQA. This dataset is relatively small compared with VQA 2.0. It is easy to reach high performance by training a large model on it. I hope this should be noted for readers to make solid conclusions. Further, there are implementation difference between the two papers for us to reach the different conclusions: (1) The choice of LLM; (2) We have pre-training stage to connect the two modality, which leads 5% improvement compared with training from scratch, while MMCoT does not has this pre-training stage. Hope it can re-considered in the development.\r\n\r\n--------\r\nI'd like to clarify that ScienceQA has helped us quantitively ablate our design choice in the early stage of the project, but ScienceQA is not the single main focus of this project. We aim to help the community produce multimodal GPT-4 level capability with minimum efforts: (1) From focus shift from model-centric to data-centric AI: the multimodal instruction-following data is the key, and the most of our time is spent on. (2) Achieving multimodal chat with detailed description such as OCR and complex reasoning. The current demo has preliminary capabilities on this.  Hope the community can be inspired to scale up this approach to reach better performance.\n</Comment>\n<Comment by 152334H at 2023-04-20T01:17:44Z>\n> In our study, we find that the \"CoT\" claim is not very important. See the evidence in our paper:\r\n\r\nWow, thanks for the work!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 11,
    "state": "closed",
    "created_by": "152334H",
    "created_at": "2023-04-19T05:40:55Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/11</URL>\n\n<TITLE>Any plans for cc3m data json / filter pipeline release?</TITLE>\n\n<BODY>title</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-20T16:56:38Z>\nHi, thank you for your interest in our work!\r\n\r\nOur pretraining dataset is released.  Please see [here](https://github.com/haotian-liu/LLaVA#pretraining-dataset).\n</Comment>\n<Comment by 152334H at 2023-04-20T17:11:49Z>\nThank you!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 10,
    "state": "closed",
    "created_by": "Richar-Du",
    "created_at": "2023-04-19T05:36:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/10</URL>\n\n<TITLE>A question about training cost</TITLE>\n\n<BODY>Thanks for your awesome work! Would you mind mentioning the training cost of LLaVA (including both the GPU hours and cost of API)? Thanks in advance :)</BODY>\n\n<COMMENTS>\n<Comment by 152334H at 2023-04-19T05:52:53Z>\nGiven that they ran GPT-4 over the full 600k cc3m subset, the vast majority of the cost involved should come from the API calls to gpt-4 itself (somewhere between $10k~100k). The cost of GPU rental would be tiny in comparison.\n</Comment>\n<Comment by haotian-liu at 2023-04-20T16:55:45Z>\n@Richar-Du Thanks for your question and for the interest in our work.\r\n\r\nWe pretrain our model on 595K data on 8x A100s for around 5 hours.  The finetuning of the initial release takes ~10 hours on the same machine.  We also find using a smaller subset can achieve similar performance.  We'll update details of these experiments later.\r\n\r\nThanks @152334H for answering, but we would clarify that we **do not need to run GPT-4 on CC3M for pretraining stage**. We use the official CC3M captions directly, without using BLIP synthetic captions in the first release.  Only the instruction tuning stage data is generated by GPT-4.  You can refer to our released [LLaVA-CC3M-Pretrain-595K](https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K) and [LLaVA-Instruct-150K](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K) for more details.\r\n\r\nWe'll update the information in our paper as well, thanks.\n</Comment>\n<Comment by 152334H at 2023-04-20T16:59:55Z>\nOh okay, my apologies for misunderstanding!\n</Comment>\n<Comment by haotian-liu at 2023-04-20T17:02:17Z>\n@152334H No worries at all!  Thank you for your contribution to the discussion, and looking forward to hearing more feedbacks from you guys!\n</Comment>\n<Comment by HashmatShadab at 2024-08-23T06:04:51Z>\n> @Richar-Du Thanks for your question and for the interest in our work.\r\n> \r\n> We pretrain our model on 595K data on 8x A100s for around 5 hours. The finetuning of the initial release takes ~10 hours on the same machine. We also find using a smaller subset can achieve similar performance. We'll update details of these experiments later.\r\n> \r\n> Thanks @152334H for answering, but we would clarify that we **do not need to run GPT-4 on CC3M for pretraining stage**. We use the official CC3M captions directly, without using BLIP synthetic captions in the first release. Only the instruction tuning stage data is generated by GPT-4. You can refer to our released [LLaVA-CC3M-Pretrain-595K](https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K) and [LLaVA-Instruct-150K](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K) for more details.\r\n> \r\n> We'll update the information in our paper as well, thanks.\r\n\r\nRegarding \"We also find using a smaller subset can achieve similar performance. We'll update details of these experiments later.\". Where these details updated?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 9,
    "state": "closed",
    "created_by": "jxy",
    "created_at": "2023-04-18T18:13:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/9</URL>\n\n<TITLE>The actual training sequence</TITLE>\n\n<BODY>I found the conversation here https://github.com/haotian-liu/LLaVA/blob/6520ad9067e0484000ba4fb20dbe9eaa5a80aa84/llava/conversation.py#L165-L194\r\n\r\nGiven the code here https://github.com/haotian-liu/LLaVA/blob/6520ad9067e0484000ba4fb20dbe9eaa5a80aa84/llava/conversation.py#L26-L35\r\n\r\nThis seems to tell me that the actual training sequence is rather\r\n\r\n```\r\nf'{system}###Human: {instruct}###Assistant: {completion}###'\r\n```\r\n\r\nThere doesn't seem to be any additional `\\n` as mentioned in Table 2 of the paper.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-04-20T17:00:00Z>\nThank you for your interest in our work, and for pointing this out!  Your understanding is correct that there is no additional `\\n` added.\r\n\r\nWe will fix this discrepancy in the future release.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 7,
    "state": "open",
    "created_by": "MIL-VLG",
    "created_at": "2023-04-18T08:45:27Z",
    "labels": [
      "question"
    ],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/7</URL>\n\n<TITLE>Question about the OCR capability</TITLE>\n\n<BODY>Great work indeed!\r\n\r\nFrom the description in the paper, I do not find any special OCR module. I am curious how LLaVA obtains the ability to understand the text in the image (e.g., the famous examples of chicken nuggets). Is there any magic in the training dataset?</BODY>\n\n<COMMENTS>\n<Comment by 152334H at 2023-04-19T03:44:52Z>\nCLIP is all you need\n</Comment>\n<Comment by ChunyuanLI at 2023-04-19T03:49:12Z>\nThe so-called emerging properties.\r\nThe pre-trained models of the visual encoder and LLM already have a good understanding of their respective domain data (with their own structured feature space). We link them together using a linear projection layer, which can be considered as a visual tokenization step,  which embeds and aligns visual tokens into the pre-trained language model's word embedding space. Therefore these projected visual embeddings are very close the corresponding word embeddings, which make OCR possible. \r\n\r\nThis step of image-text feature alignment shows good ability via training with very little paired image-text data.\n</Comment>\n<Comment by erjiaxiao at 2023-08-28T06:24:02Z>\nHello @ChunyuanLI  I wonder do you use training paired image-text data that contains text in the image? Could you show me an example of them? Thanks a lot!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 6,
    "state": "closed",
    "created_by": "paulpaul91",
    "created_at": "2023-04-18T08:26:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/6</URL>\n\n<TITLE>vision_tower how to insert LLama？</TITLE>\n\n<BODY>https://github.com/haotian-liu/LLaVA/blob/main/llava/train/train.py#L427\r\norigin LLama not include vision tower, you may change the transformer code, but not public?</BODY>\n\n<COMMENTS>\n<Comment by WooKimm at 2023-04-18T10:34:29Z>\nI just had this question, maybe we just ignored the notice mentioned in the readme:\r\n```\r\nNOTE: In this research preview, we used a modified version of huggingface/transformers library to support multimodal models and the LLaMA tokenizer. \r\nMake sure that you are using the correct transformers library from https://github.com/haotian-liu/transformers_llava.\r\n```\n</Comment>\n<Comment by paulpaul91 at 2023-04-18T12:15:21Z>\n> I just had this question, maybe we just ignored the notice mentioned in the readme:\r\n> \r\n> ```\r\n> NOTE: In this research preview, we used a modified version of huggingface/transformers library to support multimodal models and the LLaMA tokenizer. \r\n> Make sure that you are using the correct transformers library from https://github.com/haotian-liu/transformers_llava.\r\n> ```\r\n\r\nthx\n</Comment>\n<Comment by haotian-liu at 2023-04-19T15:50:28Z>\n@WooKimm Thanks for sharing this!\r\n\r\nYes, please make sure to install the correct transformers library from https://github.com/haotian-liu/transformers_llava.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 5,
    "state": "closed",
    "created_by": "satyajitghana",
    "created_at": "2023-04-18T06:30:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/5</URL>\n\n<TITLE>`LlamaTokenizerFast` implementation missing</TITLE>\n\n<BODY>```\r\nsatyajit@zeus:~/satyajit/llama-dl$ python3 convert_llama_weights_to_hf.py --input_dir . --model_size 13B --output_dir llama_hf\r\nTraceback (most recent call last):\r\n  File \"convert_llama_weights_to_hf.py\", line 28, in <module>\r\n    from transformers import LlamaTokenizerFast\r\nImportError: cannot import name 'LlamaTokenizerFast' from 'transformers' (/home/satyajit/satyajit/transformers_llava/src/transformers/__init__.py)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"convert_llama_weights_to_hf.py\", line 30, in <module>\r\n    warnings.warn(e)\r\nTypeError: expected string or bytes-like object\r\n```</BODY>\n\n<COMMENTS>\n<Comment by satyajitghana at 2023-04-18T06:59:15Z>\nreplace all instances of `LlamaTokenizerFast` with `LlamaTokenizer` fixed the issue\n</Comment>\n<Comment by satyajitghana at 2023-04-18T09:49:09Z>\nI've got it working on CPU !\r\n\r\n![image](https://user-images.githubusercontent.com/8092481/232740034-0fb6fc5f-72d7-4b35-9324-be91b922f3bb.png)\n</Comment>\n<Comment by haotian-liu at 2023-04-19T15:51:58Z>\nThank you for your interest in our work and congratulations on successfully running this!\r\n\r\nAnd also thanks for figuring out this issue!  We'll fix this soon.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 4,
    "state": "closed",
    "created_by": "satyajitghana",
    "created_at": "2023-04-18T06:26:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/4</URL>\n\n<TITLE>`setup.py` missing</TITLE>\n\n<BODY>```\r\nsatyajit@zeus:~/satyajit/LLaVA$ pip install -e .\r\nERROR: File \"setup.py\" not found. Directory cannot be installed in editable mode: /home/satyajit/satyajit/LLaVA\r\n(A \"pyproject.toml\" file was found, but editable mode currently requires a setup.py based build.)\r\n```</BODY>\n\n<COMMENTS>\n<Comment by satyajitghana at 2023-04-18T09:49:40Z>\npip install for the package is not required\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 3,
    "state": "closed",
    "created_by": "satyajitghana",
    "created_at": "2023-04-18T06:06:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/3</URL>\n\n<TITLE>`transformers_llava` broken link</TITLE>\n\n<BODY>Hi,\r\n\r\nThe `transformers_llava` github link isn't working: https://github.com/haotian-liu/transformers_llava</BODY>\n\n<COMMENTS>\n<Comment by satyajitghana at 2023-04-18T06:14:09Z>\nits' working now ! thanks\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 2,
    "state": "closed",
    "created_by": "152334H",
    "created_at": "2023-04-18T05:35:53Z",
    "labels": [
      "question"
    ],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/2</URL>\n\n<TITLE>Benchmark/comparison against MiniGPT-4?</TITLE>\n\n<BODY>Or just thoughts about which architecture should be expected to perform better with the same training data</BODY>\n\n<COMMENTS>\n<Comment by ChunyuanLI at 2023-04-19T20:18:27Z>\nIt is great to see the community's recognition and excitement about this direction; Both pieces of work are taken  independently during the same period.\r\n\r\nIMHO, LLaVA is unique at three aspects, see below.\r\n\r\n1. **More rigorous results**: LLaVA has rigorous quantitative results, including the level of similarity with Visual Chat and GPT-4, the SoTA accuracy on Science QA, and ablation studies on data iteration and model design. Mini GPT-4, on the other hand, lacks quantitative results.\r\n2. **Quality of Chat Demo**: LLaVA can reproduce results for visual reasoning examples in GPT-4 paper and has strong OCR capabilities. These features are impressive and unique, making it possibly the closest demo to Multimodal GPT-4. Check results: [https://llava-vl.github.io](https://t.co/Pdvg9MICHg)\r\n3. **Lastly, it should be clarified that the focus of this line of work is **data-centric****, not model-centric. As the differences in models are diminishing, data quality has a greater impact on results. We released our multi-modal instruction following data, to replicate Multimodal GPT-4. The high quality data is all you need (compare which, the architecture is secondary).\n</Comment>\n<Comment by ChunyuanLI at 2023-04-19T22:15:53Z>\nComparison of LLaVA and mini-GPT4 on \"Extreme Ironing\" example from the OpenAI GPT-4 technique report.\r\n\r\n**LLaVA**\r\n\r\n![image](https://user-images.githubusercontent.com/8978644/233210829-7ebe8886-a152-4dd7-a100-97b357c413c2.png)\r\n\r\n-----------\r\n\r\n**mini-GPT4**\r\n\r\nRun 1:\r\n![mini-gpt4-ironing](https://user-images.githubusercontent.com/8978644/233211451-40368c59-5f9a-4d25-a9f0-422bb3256c1b.png)\r\n\r\nRun 2:\r\n![mini-gpt4-ironing-2](https://user-images.githubusercontent.com/8978644/233212980-6ebd8b43-8128-43cf-a536-1ba2a122ba16.png)\n</Comment>\n</COMMENTS>"
  }
]