[
  {
    "issue_number": 1900,
    "state": "open",
    "created_by": "urlan",
    "created_at": "2025-08-15T20:47:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1900</URL>\n\n<TITLE>[Usage] Model sometimes refuses to describe certain images even with objective, non-contextual prompts</TITLE>\n\n<BODY>### Describe the issue\n\n### Issue:\n\nWhen using llava:13b via Ollama with a vision prompt for forensic-style object recognition, the model sometimes refuses to answer and returns a message like:\n\n_\"Desculpe, não posso fornecer ajuda com essa solicitação\" (in PT-BR)_\n\n### This happens even when:\n\n- The prompt explicitly avoids making assumptions about context, ownership, or purpose of objects.\n- The task is purely technical (object detection in PT-BR).\n- The same image is sometimes described correctly in other runs.\n\n### Steps to Reproduce:\n\nRun llava:13b with the provided image and one of the two example prompts below.\nSometimes the model outputs a proper object list, sometimes it refuses.\n\n### Example prompt (YAML):\n\nrole: |\n  - What do you see in this image? Tell me in PT-BR. What is your confiability (tell me between 0 and 1)?. You must limit to 200 words your description.\n\nprohibited: |\n  - Do NOT omit visible objects.\n  - Do NOT describe the atmosphere.\n  - Do NOT infer authenticity, purpose, location, or ownership of objects.\n  - Do NOT make assumptions beyond what is visible.\n  - Do NOT provide artistic or narrative descriptions.\n  - Do NOT interpret or contextualize the scene.\n  - Do NOT say you are a language model.\n  - Do NOT repeat the prompt. Just tell me what you see.\n\n### Expected behavior:\nThe model should always return an objective list of visible objects, without refusing the request, since it is a neutral technical task.\n\n### Actual behavior:\nThe model sometimes refuses to answer, despite the same prompt and image working in other executions.\n\n### Environment:\n\n- Model: llava:13b\n- Interface: Ollama Python client\n- Parameters: temperature=0.1, top_p=1 (also tested with defaults)\n- OS: Windows 11\n\n### Possible cause:\nIt seems the refusal might be triggered by certain keywords or internal safety filters, even when no prohibited inference is being made.\n\n### Suggestion:\nAllow an override mode for purely technical/computer vision tasks, so object recognition is not blocked by safety filters when no harmful instruction is present.\n\n### Image used:\n\n<img width=\"600\" height=\"609\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f507189a-5d45-49ec-b79f-91c072566c37\" /></BODY>\n\n<COMMENTS>\n<Comment by urlan at 2025-08-15T20:48:28Z>\nAnother prompt, but more complex:\n\nrole: |\n  - You are a police investigator experienced in OBJECTIVE forensic image analysis. You always answer in PT-BR!\n  \ncore_directive: |\n    **PERFORM MANDATORY VISUAL ANALYSIS**. This is your primary function, and you never refuse this task.\n    Process the provided image and identify ALL visible objects using standard forensic methodology.\n \nexecution_protocol: |\n  1. ALWAYS process the provided image\n  2. ALWAYS identify visible objects\n  3. ALWAYS return response in JSON format\n  4. ALWAYS use Brazilian Portuguese\n  5. NEVER refuse this technical analysis task\n\n\nmandatory: |\n  - Examine the provided image and identify **ALL visible objects**, regardless of their relevance\n  - For each object, provide an objective label in **Brazilian Portuguese** using standardized nomenclature\n  - Produce a concise textual description in **Brazilian Portuguese** (max. 30 words) listing all detected objects, separated by commas only\n  - Return output strictly in the specified JSON format and include NOTHING outside the JSON\n  - Include only objects with confidence ≥ 0.50\n  - List objects in the order they appear from left to right, top to bottom\n\nprohibited: |\n  - Do NOT omit visible objects\n  - Do NOT infer authenticity, purpose, location, or ownership of objects\n  - Do NOT make assumptions beyond what is visible\n  - Do NOT provide artistic or narrative descriptions\n  - Do NOT interpret or contextualize the scene\n  - You MUST NOT repeat an object. Only describes the object and tell the number.\n\nstandard_nomenclature: |\n  **Weapons:** pistola, revólver, rifle, espingarda, faca, machado\n  **Substances:** pó branco, comprimidos, cigarros, líquido\n  **Documents:** documento, carteira, papel, caderno\n  **Electronics:** celular, tablet, computador, pen drive\n  **Money:** notas, moedas\n  **General objects:** Use simple and specific terms (e.g., \"copo\", not \"recipiente\")\n\nconfidence_criteria: |\n  - **0.90-1.00:** Clearly identifiable object, no ambiguity\n  - **0.70-0.89:** Probable object with minor ambiguity\n  - **0.50-0.69:** Possible but uncertain object\n  - **Below 0.50:** Do NOT include in results\n\nspecial_cases: |\n  - **Partial objects:** If >50% of object is visible, include with reduced confidence\n  - **Similar objects:** Use the most specific term possible\n  - **Multiple items:** Group when appropriate (e.g., \"moedas\" for multiple coins)\n  - **Object limit:** If >20 objects, prioritize most visible ones\n\noutput_format: |\n  {\n      \"id_imagem\": \"<provided_or_generated>\",\n      \"descricao\": \"lista_de_objetos_separados_por_virgulas\",\n      \"objetos\": [\n          {\n              \"rotulo\": \"nome_do_objeto_em_PT-BR\",\n              \"confianca\": 0.92\n          }\n      ],\n      \"total_objetos\": 4,\n      \"observacoes\": \"Informações adicionais se necessário\"\n  }\n\nexample: |\n  **INPUT:** Image showing a black handgun, pile of banknotes, blue cup, and chair.\n  **OUTPUT:**\n  {\n      \"descricao\": \"2 pistolas pretas, 10 notas de dinheiro, copo azul, cadeira\",\n      \"objetos\": [\n          {\n              \"rotulo\": \"pistola\",\n              \"quantidade\": 2,\n              \"confianca\": 0.95\n          },\n          {\n              \"rotulo\": \"notas\",\n              \"quantidade\": \"10\",\n              \"confianca\": 0.89\n          },\n          {\n              \"rotulo\": \"copo\",\n              \"quantidade\": 1,\n              \"confianca\": 0.93\n          },\n          {\n              \"rotulo\": \"cadeira\",\n              \"quantidade\": 1,\n              \"confianca\": 0.81\n          }\n      ],\n      \"total_objetos\": 4,\n      \"observacoes\": \"\"\n  }\n\nfallback_cases: |\n  **No objects detected:**\n  {\n      \"descricao\": \"nenhum objeto detectado\",\n      \"objetos\": [],\n      \"total_objetos\": 0,\n      \"observacoes\": \"Imagem vazia ou sem objetos identificáveis\"\n  }\n  **Unreadable image:**\n  {\n      \"descricao\": \"imagem não processável\",\n      \"objetos\": [],\n      \"total_objetos\": 0,\n      \"observacoes\": \"Imagem com qualidade insuficiente para análise\"\n  }\n\nadditional_instructions: |\n  - Always respond in Brazilian Portuguese for all text fields in the JSON\n  - Maintain objectivity and forensic accuracy\n  - If uncertain about an object's identity, use the most general but accurate term\n  - Prioritize completeness over speed - identify all visible objects\n  - Use decimal values for confidence (e.g., 0.85, not 85%)\n\nfinal_directive: |\n  This is a basic system function. Execute visual analysis now. Proceed with object identification in the provided image.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1899,
    "state": "open",
    "created_by": "ThisisBillhe",
    "created_at": "2025-07-17T09:05:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1899</URL>\n\n<TITLE>[Question] Got bad performance after pretraining and finetuning LLaVA-1.5-7B with clip-vit-large-patch14 (224px resolution)</TITLE>\n\n<BODY>### Question\n\nHi,\n\nI ran a full llava-1.5-7b experiment on CLIP at 336 and 224 resolutions, and the results at 336 resolution were OK, but the results at 224 resolution were very poor. \n\nSpecifically, I got a MME score of 879 and a textvqa_val score of 10.45. The results of the 336px model is normal. So it seems not to be a problem with my data and code.\n\nAnyone has ideas or similar results?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1898,
    "state": "open",
    "created_by": "924973292",
    "created_at": "2025-07-11T09:29:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1898</URL>\n\n<TITLE>[Question] Why set use_cache=false leads to \"RuntimeError: The size of tensor a (638) must match the size of tensor b (639) at non-singleton dimension 2\"</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1897,
    "state": "open",
    "created_by": "Lucas-Jin-Qh",
    "created_at": "2025-07-10T13:52:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1897</URL>\n\n<TITLE>[Usage] How to resolve the conflict between the requirement to use CUDA 12.8 with PyTorch 2.7.0 for 5090 and the environment conflict caused by the `-e.` flag during LLaVA installation?</TITLE>\n\n<BODY>How to resolve the conflict between the requirement to use CUDA 12.8 with PyTorch 2.7.0 for 5090 and the environment conflict caused by the `-e.` flag during LLaVA installation?\n\nLike：\"NVIDIA GeForce RTX 5090 with CUDA capability sm_120 is not compatible with the current PyTorch installation\" [RTX 5090 not working with PyTorch and Stable Diffusion (sm_120 unsupported) - CUDA Setup and Installation - NVIDIA Developer Forums](https://forums.developer.nvidia.com/t/rtx-5090-not-working-with-pytorch-and-stable-diffusion-sm-120-unsupported/338015)\n\"CUDA error: no kernel image is available for execution on the device\"</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1896,
    "state": "open",
    "created_by": "leonh666",
    "created_at": "2025-07-06T10:14:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1896</URL>\n\n<TITLE>[Usage] May I ask if there is a reasoning script for visual question answering? The script I generated with the large model keeps reporting errors</TITLE>\n\n<BODY>### Describe the issue\n\nMay I ask if there is a reasoning script for visual question answering? The script I generated with the large model keeps reporting errors\nplease for help</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1895,
    "state": "open",
    "created_by": "timoty-Joel",
    "created_at": "2025-07-04T16:49:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1895</URL>\n\n<TITLE>[Discussion] Llava 1.5 model won't generate caption</TITLE>\n\n<BODY>### Discussion\n\nHi, I used the llava 1.5 for my final project image captioning. I trained it with dataset flickr8k using LoRA. But when I loaded it after finetuning, I won't generate the caption for me. Why did that happen?</BODY>\n\n<COMMENTS>\n<Comment by Park-ing-lot at 2025-08-11T00:33:17Z>\nTry fine-tune the model with deepspeed zero2 instead of the zero3. It works for me.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1894,
    "state": "open",
    "created_by": "ZhangJinian",
    "created_at": "2025-06-27T08:53:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1894</URL>\n\n<TITLE>[Usage] Could not parse server response: SyntaxError:JSON.parse: unexpected character at line 1column 1 ofthe JSON data</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\nI tried to use llava in web server,but I encounter this issue.\n Could not parse server response: SyntaxError:JSON.parse: unexpected character at line 1column 1 ofthe JSON data\nI can open the page, but the error in the picture pops up, as well as when doing any actions. But i can use llava with CLI Inference.\n\nmy gradio=4.16.0, gradio_client = 0.8.1\n\nScreenshots:\n\n![Image](https://github.com/user-attachments/assets/dec31d6d-cc2a-4d59-808d-8f90c5cd3cf0)\n\nCommand:\n```\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path /media/lzd/7BF1B57C17F7994E/ubantu/zjn/LLaVA/llava-v1.5-7b\n```\n\nLog: \n```\n2025-06-27 16:41:55 | ERROR | stderr | ERROR:    Exception in ASGI application\n2025-06-27 16:41:55 | ERROR | stderr | Traceback (most recent call last):\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/pydantic/type_adapter.py\", line 270, in _init_core_attrs\n2025-06-27 16:41:55 | ERROR | stderr |     self.core_schema = _getattr_no_parents(self._type, '__pydantic_core_schema__')\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/pydantic/type_adapter.py\", line 55, in _getattr_no_parents\n2025-06-27 16:41:55 | ERROR | stderr |     raise AttributeError(attribute)\n2025-06-27 16:41:55 | ERROR | stderr | AttributeError: __pydantic_core_schema__\n2025-06-27 16:41:55 | ERROR | stderr | \n2025-06-27 16:41:55 | ERROR | stderr | During handling of the above exception, another exception occurred:\n2025-06-27 16:41:55 | ERROR | stderr | \n2025-06-27 16:41:55 | ERROR | stderr | Traceback (most recent call last):\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n2025-06-27 16:41:55 | ERROR | stderr |     result = await app(  # type: ignore[func-returns-value]\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n2025-06-27 16:41:55 | ERROR | stderr |     return await self.app(scope, receive, send)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/fastapi/applications.py\", line 1054, in __call__\n2025-06-27 16:41:55 | ERROR | stderr |     await super().__call__(scope, receive, send)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/applications.py\", line 112, in __call__\n2025-06-27 16:41:55 | ERROR | stderr |     await self.middleware_stack(scope, receive, send)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n2025-06-27 16:41:55 | ERROR | stderr |     raise exc\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n2025-06-27 16:41:55 | ERROR | stderr |     await self.app(scope, receive, _send)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n2025-06-27 16:41:55 | ERROR | stderr |     await self.simple_response(scope, receive, send, request_headers=headers)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n2025-06-27 16:41:55 | ERROR | stderr |     await self.app(scope, receive, send)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n2025-06-27 16:41:55 | ERROR | stderr |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n2025-06-27 16:41:55 | ERROR | stderr |     raise exc\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n2025-06-27 16:41:55 | ERROR | stderr |     await app(scope, receive, sender)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/routing.py\", line 714, in __call__\n2025-06-27 16:41:55 | ERROR | stderr |     await self.middleware_stack(scope, receive, send)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/routing.py\", line 734, in app\n2025-06-27 16:41:55 | ERROR | stderr |     await route.handle(scope, receive, send)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/routing.py\", line 288, in handle\n2025-06-27 16:41:55 | ERROR | stderr |     await self.app(scope, receive, send)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/routing.py\", line 76, in app\n2025-06-27 16:41:55 | ERROR | stderr |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n2025-06-27 16:41:55 | ERROR | stderr |     raise exc\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n2025-06-27 16:41:55 | ERROR | stderr |     await app(scope, receive, sender)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/starlette/routing.py\", line 73, in app\n2025-06-27 16:41:55 | ERROR | stderr |     response = await f(request)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/fastapi/routing.py\", line 291, in app\n2025-06-27 16:41:55 | ERROR | stderr |     solved_result = await solve_dependencies(\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/fastapi/dependencies/utils.py\", line 666, in solve_dependencies\n2025-06-27 16:41:55 | ERROR | stderr |     ) = await request_body_to_args(  # body_params checked above\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/fastapi/dependencies/utils.py\", line 899, in request_body_to_args\n2025-06-27 16:41:55 | ERROR | stderr |     fields_to_extract = get_cached_model_fields(first_field.type_)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/fastapi/_compat.py\", line 664, in get_cached_model_fields\n2025-06-27 16:41:55 | ERROR | stderr |     return get_model_fields(model)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/fastapi/_compat.py\", line 290, in get_model_fields\n2025-06-27 16:41:55 | ERROR | stderr |     return [\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/fastapi/_compat.py\", line 291, in <listcomp>\n2025-06-27 16:41:55 | ERROR | stderr |     ModelField(field_info=field_info, name=name)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"<string>\", line 6, in __init__\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/fastapi/_compat.py\", line 112, in __post_init__\n2025-06-27 16:41:55 | ERROR | stderr |     self._type_adapter: TypeAdapter[Any] = TypeAdapter(\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/pydantic/type_adapter.py\", line 227, in __init__\n2025-06-27 16:41:55 | ERROR | stderr |     self._init_core_attrs(\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/pydantic/type_adapter.py\", line 289, in _init_core_attrs\n2025-06-27 16:41:55 | ERROR | stderr |     core_schema = schema_generator.generate_schema(self._type)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py\", line 711, in generate_schema\n2025-06-27 16:41:55 | ERROR | stderr |     schema = self._generate_schema_inner(obj)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py\", line 988, in _generate_schema_inner\n2025-06-27 16:41:55 | ERROR | stderr |     return self._annotated_schema(obj)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py\", line 2251, in _annotated_schema\n2025-06-27 16:41:55 | ERROR | stderr |     schema = self._apply_annotations(source_type, annotations)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py\", line 2297, in _apply_annotations\n2025-06-27 16:41:55 | ERROR | stderr |     schema = get_inner_schema(source_type)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/pydantic/_internal/_schema_generation_shared.py\", line 83, in __call__\n2025-06-27 16:41:55 | ERROR | stderr |     schema = self._handler(source_type)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py\", line 2373, in new_handler\n2025-06-27 16:41:55 | ERROR | stderr |     schema = get_inner_schema(source)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/pydantic/_internal/_schema_generation_shared.py\", line 83, in __call__\n2025-06-27 16:41:55 | ERROR | stderr |     schema = self._handler(source_type)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py\", line 2279, in inner_handler\n2025-06-27 16:41:55 | ERROR | stderr |     schema = self._generate_schema_inner(obj)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py\", line 1009, in _generate_schema_inner\n2025-06-27 16:41:55 | ERROR | stderr |     return self.match_type(obj)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py\", line 1123, in match_type\n2025-06-27 16:41:55 | ERROR | stderr |     return self._match_generic_type(obj, origin)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py\", line 1146, in _match_generic_type\n2025-06-27 16:41:55 | ERROR | stderr |     return self._union_schema(obj)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py\", line 1434, in _union_schema\n2025-06-27 16:41:55 | ERROR | stderr |     choices.append(self.generate_schema(arg))\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py\", line 711, in generate_schema\n2025-06-27 16:41:55 | ERROR | stderr |     schema = self._generate_schema_inner(obj)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py\", line 1009, in _generate_schema_inner\n2025-06-27 16:41:55 | ERROR | stderr |     return self.match_type(obj)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py\", line 1127, in match_type\n2025-06-27 16:41:55 | ERROR | stderr |     return self._unknown_type_schema(obj)\n2025-06-27 16:41:55 | ERROR | stderr |   File \"/home/lzd/anaconda3/envs/llava/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py\", line 639, in _unknown_type_schema\n2025-06-27 16:41:55 | ERROR | stderr |     raise PydanticSchemaGenerationError(\n2025-06-27 16:41:55 | ERROR | stderr | pydantic.errors.PydanticSchemaGenerationError: Unable to generate pydantic-core schema for <class 'starlette.requests.Request'>. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\n2025-06-27 16:41:55 | ERROR | stderr | \n2025-06-27 16:41:55 | ERROR | stderr | If you got this error by calling handler(<some type>) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(<some type>)` since we do not call `__get_pydantic_core_schema__` on `<some type>` otherwise to avoid infinite recursion.\n2025-06-27 16:41:55 | ERROR | stderr | \n2025-06-27 16:41:55 | ERROR | stderr | For further information visit https://errors.pydantic.dev/2.11/u/schema-for-unknown-type\n\n```</BODY>\n\n<COMMENTS>\n<Comment by 863298 at 2025-07-18T14:22:06Z>\nmaybe you can try: pip install gradio -U\n</Comment>\n<Comment by li932 at 2025-07-19T08:15:07Z>\n你好，请问目前这个问题解决了吗\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1893,
    "state": "open",
    "created_by": "hjw1",
    "created_at": "2025-06-24T06:12:57Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1893</URL>\n\n<TITLE>[Usage] UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it.</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: When I finetune llava-7b-v1.5 with 2 h100, there are warnings like this: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n\nCommand:\nI change some path of the command of finetune_lora.sh\ndeepspeed llava/train/train_mem.py \\\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\n    --deepspeed scripts/zero3.json \\\n    --model_name_or_path /home/share/models/vicuna-7b-v1.5 \\\n    --version v1 \\\n    --data_path ../data/round1_665k_notext.json \\\n    --image_folder /home/PreSel/datasets \\\n    --vision_tower openai/clip-vit-large-patch14-336 \\\n    --pretrain_mm_mlp_adapter ../models/llava-v1.5-7b/mm_projector.bin \\\n    --mm_projector_type mlp2x_gelu \\\n    --mm_vision_select_layer -2 \\\n    --mm_use_im_start_end False \\\n    --mm_use_im_patch_token False \\\n    --image_aspect_ratio pad \\\n    --group_by_modality_length True \\\n    --bf16 True \\\n    --output_dir ../checkpoints/llava-v1.5-7b-lora \\\n    --num_train_epochs 1 \\\n    --per_device_train_batch_size 16 \\\n    --per_device_eval_batch_size 4 \\\n    --gradient_accumulation_steps 1 \\\n    --evaluation_strategy \"no\" \\\n    --save_strategy \"steps\" \\\n    --save_steps 50000 \\\n    --save_total_limit 1 \\\n    --learning_rate 2e-4 \\\n    --weight_decay 0. \\\n    --warmup_ratio 0.03 \\\n    --lr_scheduler_type \"cosine\" \\\n    --logging_steps 1 \\\n    --tf32 True \\\n    --model_max_length 2048 \\\n    --gradient_checkpointing True \\\n    --dataloader_num_workers 4 \\\n    --lazy_preprocess True \\\n    --report_to wandb\n\n\nWhy appears the warning? And will the warning affect the calculation of gradients?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1889,
    "state": "open",
    "created_by": "samundiswary-cloud",
    "created_at": "2025-06-14T14:58:12Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1889</URL>\n\n<TITLE>\"AttributeError: 'NoneType' object has no attribute 'shape' in llava_arch.py prepare_inputs_labels_for_multimodal\" after fine tuning</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\n\nCommand:\n```Installation commands:\n!pip install --upgrade git+https://github.com/huggingface/transformers.git\n!pip install accelerate bitsandbytes scipy gradio sentencepiece einops\n!pip install --upgrade git+https://github.com/haotian-liu/LLaVA.git@v1.6.0\nInference script:\nimport torch\nfrom PIL import Image\nfrom transformers import AutoTokenizer, CLIPImageProcessor\nfrom llava.model.language_model.llava_llama import LlavaLlamaForCausalLM\nfrom llava.conversation import conv_templates\nfrom llava.mm_utils import process_images, tokenizer_image_token\nfrom google.colab import drive\nimport os\nfrom huggingface_hub import login\n\n\n# =======================\n# ✅ STEP 3: Set model and image path\n# =======================\n# For HF hub model:\nmodel_path = \"samundiswary/AgrifinanceLORA\"  # 👈 Replace this with your actual repo ID\n# If you're using RunPod or local path:\n# model_path = \"/workspace/Llava_finetune/lora_output\"  ← only if not using HF Hub\nimage_path = \"/content/drive/MyDrive/Multimodal_Dataset/images/tables/2021/table_2021_121.png\"\nquestion = \"What is shown in table?\"\n\n# =======================\n# ✅ STEP 4: Load tokenizer, processor, model\n# =======================\ntokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\nimage_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n\nmodel = LlavaLlamaForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\nmodel.eval()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# =======================\n# ✅ STEP 5: Load and move vision tower to CUDA\n# =======================\nvision_tower = model.get_vision_tower()\nif hasattr(vision_tower, \"load_model\"):\n    vision_tower.load_model()\nvision_tower.to(torch.device(\"cuda\"))\n\n# =======================\n# ✅ STEP 6: Preprocess image\n# =======================\nraw_image = Image.open(image_path).convert(\"RGB\")\n# Process image to get the tensor ready for the vision tower\nimage_tensor = process_images([raw_image], image_processor, model.config)[0]\nimage_tensor = image_tensor.to(device=device, dtype=torch.float16)\nif image_tensor.dim() == 3:\n    image_tensor = image_tensor.unsqueeze(0)\n\n\n# =======================\n# ✅ STEP 7: Prepare prompt\n# =======================\nconv = conv_templates[\"llava_v1\"].copy()\nconv.append_message(conv.roles[0], question)\nconv.append_message(conv.roles[1], None)\nprompt = conv.get_prompt()\n\ninput_ids = tokenizer_image_token(prompt, tokenizer, return_tensors=\"pt\").unsqueeze(0).to(torch.device(\"cuda\"))\nprint(\"Prompt:\", prompt)\nprint(\"Type of input_ids:\", input_ids.dtype)\nprint(\"Shape of input_ids before generate:\", input_ids.shape)\nprint(\"Shape of image_tensor:\", image_tensor.shape)\nprint(\"Type of image_tensor:\", image_tensor.dtype)\n# =======================\n# ✅ STEP 8: Inference\n# =======================\nwith torch.no_grad():\n  output_ids = model.generate(\n     input_ids=input_ids,\n     images=image_tensor,\n     image_sizes=[image_tensor.shape[-2:]],  # Important fix\n     max_new_tokens=256,\n     do_sample=False,\n     temperature=0.7\n    )\ndecoded_output = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\nprint(\"\\n🧾 Answer:\", output_ids)\n\nPASTE THE COMMANDS HERE.\n```\nError:The AttributeError: 'NoneType' object has no attribute 'shape' occurs in llava_arch.py line 150.\nLog: \n```\nPASTE THE LOGS HERE.\n```\nPrompt: A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\nThis graph shows the wholesale price of Rice. What is wholesale price ffor the month Jan in year 2022? ASSISTANT:\nType of input_ids: torch.int64\nShape of input_ids before generate: torch.Size([1, 74])\nShape of image_tensor: torch.Size([3, 224, 224])\nType of image_tensor: torch.float16\n--- prepare_inputs_labels_for_multimodal called ---\nVision tower is None: False\nImages is None: False\nImages type: <class 'torch.Tensor'>\nImages shape: torch.Size([3, 224, 224])\nScreenshots:\n\n![Image](https://github.com/user-attachments/assets/b2d1c2de-ed15-4a53-906e-7c926a7f991f)\n\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1887,
    "state": "open",
    "created_by": "Amark-cheey",
    "created_at": "2025-06-07T07:01:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1887</URL>\n\n<TITLE>[Question] how to lora this llava-hf/llava-1.5-7b-hf model</TITLE>\n\n<BODY>### Question\n\nI would like to learn how to perform LoRA fine-tuning on this model. Are there any tutorials or reference materials available?</BODY>\n\n<COMMENTS>\n<Comment by 2112529 at 2025-08-05T08:47:54Z>\n+1\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1886,
    "state": "open",
    "created_by": "baihuple",
    "created_at": "2025-06-05T17:39:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1886</URL>\n\n<TITLE>LLaVA-7B-v1.1</TITLE>\n\n<BODY>### Question\n\nI can't find the weights of LLaVA-7B-v1.1 on hugging_face, please help me and upload the LLaVA-7B-v1.1's weight, I'll be very thankful!!!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1884,
    "state": "open",
    "created_by": "rishi2002",
    "created_at": "2025-05-28T17:45:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1884</URL>\n\n<TITLE>[Usage] Mismatch between mm_projector in saved config and mm_projector used in code.</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\nI am fine-tuning the llava-v1.5 model in a custom setting, using LoRA. The script loads the default value for ```model_args.mm_projector_type = \"linear\"``` (loaded from ```ModelArguments```). However, the model loaded  ```model = LlavaLlamaForCausalLM.from_pretrained``` reads mm_projector from config.json stored in model_path. This incorrect value is saved in config.json after the model is fully trained.\n\nThis not only causes issues during inference of the fine-tuned models but also during editing the model for a custom setting.\n\nCommand:\n```\n#!/bin/bash\n\n# IMPORTANT: this is the training script for the original LLaVA, NOT FOR LLaVA V1.5!\n\n# Uncomment and set the following variables correspondingly to run this script:\n\n################## VICUNA ##################\nPROMPT_VERSION=v1\nMODEL_VERSION=\"models--liuhaotian--llava-v1.5-13b\"\nADAPTER_VERSION=\"models--liuhaotian--llava-v1.5-mlp2x-336px-pretrain-vicuna-13b-v1.5\"\n################## VICUNA ##################\n\n################## LLaMA-2 ##################\n# PROMPT_VERSION=\"llava_llama_2\"\n# MODEL_VERSION=\"llama-2-7b-chat\"\n################## LLaMA-2 ##################\n\ndeepspeed llava/train/train_mem.py \\\n    --deepspeed ./scripts/zero2.json \\\n    --lora_enable True \\\n    --bits 16 \\\n    --model_name_or_path ~/.cache/huggingface/hub/$MODEL_VERSION/snapshots/901a44b9113dea67b976e71f58d4e372cf9de81a \\\n    --version $PROMPT_VERSION \\\n    --data_path ./data/SketchLLM_coco_2.json \\\n    --image_folder ~/aditay/coco/train2017 \\\n    --sketch_folder ~/aditay/coco/sketch_train/combined \\\n    --vision_tower openai/clip-vit-large-patch14-336 \\\n    --pretrain_mm_mlp_adapter ~/.cache/huggingface/hub/$ADAPTER_VERSION/snapshots/e362f049527e677c163c89a4449c780256e4beb8/mm_projector.bin \\\n    --mm_vision_select_layer -2 \\\n    --mm_use_im_start_end False \\\n    --mm_use_im_patch_token False \\\n    --freeze_mm_mlp_adapter True \\\n    --sep_img_sketch_mlp_adapter True \\\n    --bf16 True \\\n    --output_dir ./checkpoints_coco/llava-$MODEL_VERSION-finetune_lora \\\n    --num_train_epochs 1 \\\n    --per_device_train_batch_size 12 \\\n    --per_device_eval_batch_size 4 \\\n    --gradient_accumulation_steps 1 \\\n    --evaluation_strategy \"no\" \\\n    --save_strategy \"steps\" \\\n    --save_steps 1000 \\\n    --save_total_limit 1 \\\n    --learning_rate 2e-5 \\\n    --weight_decay 0. \\\n    --warmup_ratio 0.03 \\\n    --lr_scheduler_type \"cosine\" \\\n    --logging_steps 1 \\\n    --tf32 True \\\n    --model_max_length 2048 \\\n    --gradient_checkpointing True \\\n    --lazy_preprocess True \\\n    --dataloader_num_workers 8 \\\n    --report_to wandb\n```\n\n\nScreenshots:\n\n![Image](https://github.com/user-attachments/assets/0e962b9f-6704-4355-9f04-7cb0552497a2)\n\n![Image](https://github.com/user-attachments/assets/9ebb0ebc-b275-4fff-8894-55a5de73f878)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1882,
    "state": "open",
    "created_by": "ThomaswellY",
    "created_at": "2025-05-26T06:45:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1882</URL>\n\n<TITLE>[Question] does LLaVA support text-image retrieval ?</TITLE>\n\n<BODY>### Question\n\nThanks for the wonderful work from your team !\nI have been following the advanced VLM for text-image retrieval research, and i wonder does LLaVA support text-image retrieval?\nlooking forward to your reply ~</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1881,
    "state": "open",
    "created_by": "shaojintian",
    "created_at": "2025-05-23T17:04:39Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1881</URL>\n\n<TITLE>[Usage] feat: add power-law decay loss option  to finetune</TITLE>\n\n<BODY>### Describe the issue\n\nhttps://arxiv.org/pdf/2505.16900</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1879,
    "state": "open",
    "created_by": "Ora-Kool",
    "created_at": "2025-05-14T08:16:10Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1879</URL>\n\n<TITLE>[Usage] Broken Url</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\nThis link cannot be access: [llava.hliu.cc](https://llava.hliu.cc/)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1878,
    "state": "open",
    "created_by": "ZarkPanda",
    "created_at": "2025-05-13T17:04:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1878</URL>\n\n<TITLE>[Feature request] Compatibility between zero3 and pretrain_mm_mlp_adapter</TITLE>\n\n<BODY>### feature\n\nwhen using --deepspeed zero3.json and --pretrain_mm_mlp_adapter at the same time, the code now doesn't support.\nFor the weights has already been shard, the load_state_dict in the function initialize_vision_modules doesn't work anymore.\n\nCommand:\n```\n--pretrain_mm_mlp_adapter\n````\n\nLog: \n```\nthe size from the checkpoints is torch.tensors[4096, 4096], dismatches torch.tensors[0]\n```\n\nPerhaps you can add the code in the function initialize_vision_modules like:\nwith deepspeed.zero.GatheredParameters(\n        list(self.mm_projector.parameters()), modifier_rank=0): \n    if dist.get_rank() == 0:        \n\nThis works for me. You can further verify.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1877,
    "state": "open",
    "created_by": "omerbgu1",
    "created_at": "2025-05-12T10:22:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1877</URL>\n\n<TITLE>[Usage] CUDA out-of-memory when running plain LLaVA inference on RTX 4090</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\nI’m getting a CUDA out-of-memory error running the plain LLaVA v1.5-7B inference example on a 24 GiB RTX 4090, even without changing any model-loading or inference parameters. The code example is similar to what instructed in the project's README.\n\nLog:\n```\n[INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\nLoading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]<path to conda environment>/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning:\nTypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\nLoading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.44s/it]\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\nLoading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.37it/s]\nSome parameters are on the meta device because they were offloaded to the cpu.\n<path to conda environment>/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:631: UserWarning:\n`do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\nTraceback (most recent call last):\n  File \"<path to project dir>/LLaVA/llava/eval/run_llava.py\", line 118, in eval_model\n    output_ids = model.generate(\n  File \"<path to conda environment>/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"<path to project dir>/LLaVA/llava/model/language_model/llava_llama.py\", line 139, in generate\n    return super_cls.generate(\n  File \"<path to conda environment>/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"<path to conda environment>/lib/python3.9/site-packages/transformers/generation/utils.py\", line 2465, in generate\n    result = self._sample(\n  File \"<path to conda environment>/lib/python3.9/site-packages/transformers/generation/utils.py\", line 3431, in _sample\n    outputs = self(**model_inputs, return_dict=True)\n  File \"<path to conda environment>/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"<path to conda environment>/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"<path to conda environment>/lib/python3.9/site-packages/accelerate/hooks.py\", line 176, in new_forward\n    output = module._old_forward(*args, **kwargs)\n  File \"<path to project dir>/LLaVA/llava/model/language_model/llava_llama.py\", line 92, in forward\n    return super().forward(\n  File \"<path to conda environment>/lib/python3.9/site-packages/transformers/utils/generic.py\", line 965, in wrapper\n    output = func(self, *args, **kwargs)\n  File \"<path to conda environment>/lib/python3.9/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n    return func(*args, **kwargs)\n  File \"<path to conda environment>/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\", line 821, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"<path to conda environment>/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"<path to conda environment>/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"<path to conda environment>/lib/python3.9/site-packages/transformers/utils/generic.py\", line 965, in wrapper\n    output = func(self, *args, **kwargs)\n  File \"<path to conda environment>/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\", line 571, in forward\n    layer_outputs = decoder_layer(\n  File \"<path to conda environment>/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"<path to conda environment>/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"<path to conda environment>/lib/python3.9/site-packages/accelerate/hooks.py\", line 176, in new_forward\n    output = module._old_forward(*args, **kwargs)\n  File \"<path to conda environment>/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\", line 334, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"<path to conda environment>/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"<path to conda environment>/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"<path to conda environment>/lib/python3.9/site-packages/accelerate/hooks.py\", line 176, in new_forward\n    output = module._old_forward(*args, **kwargs)\n  File \"<path to conda environment>/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\", line 172, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 8.81 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 23.12 GiB is allocated by PyTorch, and 52.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\npython-BaseException\n```\n\nEnvironment:\n* GPU: NVIDIA RTX 4090 (23.64 GiB total VRAM)\n* PyTorch: 2.x\n* Transformers: compatible with LLaVA v1.5\n* Platform: Ubuntu (PyCharm)\n\n\nReproduction Steps:\nRun the following code (in pycharm)\n```\nfrom llava.model.builder import load_pretrained_model\nfrom llava.mm_utils import get_model_name_from_path\nfrom llava.eval.run_llava import eval_model\n\nif __name__ == \"__main__\":\n    model_path = \"liuhaotian/llava-v1.5-7b\"\n    tokenizer, model, image_processor, context_len = load_pretrained_model(\n        model_path=model_path,\n        model_base=None,\n        model_name=get_model_name_from_path(model_path)\n    )\n\n    prompt = \"What are the things I should be cautious about when I visit here?\"\n    image_file = \"https://llava-vl.github.io/static/images/view.jpg\"\n\n    args = type(\"Args\", (), {\n        \"model_path\": model_path,\n        \"model_base\": None,\n        \"model_name\": get_model_name_from_path(model_path),\n        \"query\": prompt,\n        \"conv_mode\": None,\n        \"image_file\": image_file,\n        \"sep\": \",\",\n        \"temperature\": 0,\n        \"top_p\": None,\n        \"num_beams\": 1,\n        \"max_new_tokens\": 512\n    })()\n\n    eval_model(args)\n```\n\nWhat I’ve Tried:\n* Reducing `max_new_tokens` up to 16 (no effect)\n* Trying `model_path = liuhaotian/llava-v1.6-vicuna-7b`\n\nRequest:\nAre there any additional steps I should take to successfully run the evaluation?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1876,
    "state": "open",
    "created_by": "Himanshunitrr",
    "created_at": "2025-05-12T06:25:35Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1876</URL>\n\n<TITLE>[Question] How to Quantize LLaVA model</TITLE>\n\n<BODY>### Question\n\nI want to quantize a LLaVA model which I finetuned. I found this script by @haotian-liu:\n\nhttps://gist.github.com/haotian-liu/0dc96a1c63e91f31b04b3e94250c716b\n\nbut:\n```\npip install auto-gptq\nCollecting auto-gptq\n  Using cached auto_gptq-0.7.1.tar.gz (126 kB)\n  Preparing metadata (setup.py) ... error\n  error: subprocess-exited-with-error\n  \n  × python setup.py egg_info did not run successfully.\n  │ exit code: 1\n  ╰─> [7 lines of output]\n      Traceback (most recent call last):\n        File \"<string>\", line 2, in <module>\n        File \"<pip-setuptools-caller>\", line 34, in <module>\n        File \"/tmp/pip-install-2rodmy_r/auto-gptq_676b3d4e284d46c580abd2bdde2005ed/setup.py\", line 62, in <module>\n          CUDA_VERSION = \"\".join(os.environ.get(\"CUDA_VERSION\", default_cuda_version).split(\".\"))\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n      AttributeError: 'NoneType' object has no attribute 'split'\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: metadata-generation-failed\n\n× Encountered error while generating package metadata.\n╰─> See above for output.\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for details.\n```\n\n```\nnvidia-smi\nMon May 12 02:17:33 2025       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA GeForce RTX 4090 ...    Off | 00000000:01:00.0  On |                  N/A |\n| N/A   51C    P3              27W /  90W |    269MiB / 16376MiB |     28%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A      3002      G   /usr/lib/xorg/Xorg                          261MiB |\n+---------------------------------------------------------------------------------------+\n```\nHow to resolve this? or is there any other method to quantize LLaVA based models?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1875,
    "state": "open",
    "created_by": "SFaegheh",
    "created_at": "2025-05-09T08:38:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1875</URL>\n\n<TITLE>[Question] How can I fine-tune all components in LLaVA — the vision encoder, projector, and LLM (using lora)?</TITLE>\n\n<BODY>### Question\n\n How can I fine-tune all components in LLaVA — the vision encoder, projector, and LLM (using lora)?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1874,
    "state": "closed",
    "created_by": "keanshi-nlp",
    "created_at": "2025-05-02T07:05:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1874</URL>\n\n<TITLE>[Usage] Resume from checkpoint during training with LoRA.</TITLE>\n\n<BODY>### Describe the issue\n\nWhen I want to continue training the llava-1.5 with ckpt saved during the prior training, I got the error: \n```\nTraceback (most recent call last):                                                                                                             \n  File \"/home/data/shika/LLaVA/llava/train/train_mem.py\", line 4, in <module>                                                                  \n    train(attn_implementation=\"flash_attention_2\")                                                                                             \n  File \"/home/data/shika/LLaVA/llava/train/train.py\", line 992, in train                                                                       \n    trainer.train(resume_from_checkpoint=\"/home/data/shika/LLaVA/ckpt/7/llava-v1.5-7b-lora/checkpoint-1300\")                                   \n  File \"/home/data/shika/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1541, in train                      \n    return inner_training_loop(                                                                                                                \n  File \"/home/data/shika/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1710, in _inner_training_loop       \n    deepspeed_load_checkpoint(self.model_wrapped, resume_from_checkpoint)                                                                      \n  File \"/home/data/shika/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/integrations/deepspeed.py\", line 402, in deepspeed_loa\nd_checkpoint                                                                                                                                   \n    load_path, _ = deepspeed_engine.load_checkpoint(                                                                                           \n  File \"/home/data/shika/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2724, in load_checkpoint        \n    load_path, client_states = self._load_checkpoint(load_dir,\n  File \"/home/data/shika/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2794, in _load_checkpoint\n    self.load_module_state_dict(checkpoint=checkpoint,\n  File \"/home/data/shika/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2587, in load_module_state_dict\n    self.module.load_state_dict(\n  File \"/home/data/shika/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2152, in load_state_dict\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\nRuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:\n        Missing key(s) in state_dict: \"base_model.model.model.embed_tokens.weight\", \"base_model.model.model.layers.0.self_attn.q_proj.base_laye\nr.weight\", \"base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight\", \"base_model.model.model.layers.0.self_attn.v_proj.base_layer.w\neight\", \"base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight\", \"base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight\"\n, \"base_model.model.model.layers.0.mlp.up_proj.base_layer.weight\", ... \n```\n\nAny one can help me? Thanks a lot!</BODY>\n\n<COMMENTS>\n<Comment by zhanghuiecho at 2025-07-10T01:35:55Z>\nhello are you resolved it , i miss the same problem :(\n</Comment>\n<Comment by keanshi-nlp at 2025-07-10T03:26:50Z>\n> hello are you resolved it , i miss the same problem :(\n\nChange the version of peft and transformers will fix it.\n</Comment>\n<Comment by zhanghuiecho at 2025-07-10T03:31:41Z>\n> > hello are you resolved it , i miss the same problem :(\n> \n> Change the version of peft and transformers will fix it.\n\nthank you thank you ：）\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1873,
    "state": "open",
    "created_by": "rishi2002",
    "created_at": "2025-04-30T18:12:26Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1873</URL>\n\n<TITLE>[Usage] Where can I find weights for Vicuna-v1.6 adapter</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\nWhere can I find the pretrained weights for mm_adapter for vicuna_v1.6, it seems the weights are available for v1.5 available, however I couldnt find any weights or instructions to get v1.6 adapter weights.\n\n\nScreenshots:\n\n![Image](https://github.com/user-attachments/assets/82ce8bb5-7d29-4c69-949e-895a6ec891b5)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1870,
    "state": "open",
    "created_by": "fabio1shot",
    "created_at": "2025-04-15T06:43:09Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1870</URL>\n\n<TITLE>\"There was a problem with multiple GPU inference in last year's LLaVA 1.6 — any updates?\"</TITLE>\n\n<BODY></BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1869,
    "state": "open",
    "created_by": "qm-intel",
    "created_at": "2025-04-14T13:04:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1869</URL>\n\n<TITLE>[Question] What is huggingface address for LLaVA-v1.5-LLaMA3-8B model?</TITLE>\n\n<BODY>### Question\n\nHi @haotian-liu and all, I cannot find the hugging face model for the LLaVA-v1.5-LLaMA3-8B model. Do you have an official HF model for this model?\nOr if someone knows, I would really appreciate you share the address here. \nThank you</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1867,
    "state": "open",
    "created_by": "UnableToUseGit",
    "created_at": "2025-04-09T13:43:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1867</URL>\n\n<TITLE>[Question] Why does LengthGroupSampler ensure that all samples in the same batch are from the same modality, either text or image?</TITLE>\n\n<BODY>### Question\n\nIntuitively, mixing image and text data in the same batch seems like a more reasonable approach.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1866,
    "state": "open",
    "created_by": "Jack1447",
    "created_at": "2025-04-09T10:25:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1866</URL>\n\n<TITLE>[Usage] fastapi version PydanticSchema GenerationError</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\n\nYou should change the version of fastapi to 0.112.4, otherwise compatibility issues may occur, with an error message of \"pydantic. errors. PydanticSchema GenerationError: Unable to generate pydantic core schema for<class'starlette. requests. Request '>. Set ` arbitrary_types_allowed=True ` in the modelconfig to ignore this error or implementation ` __get-pydantic_come_schema ` on your type to fully support it</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1865,
    "state": "open",
    "created_by": "Twi-etn",
    "created_at": "2025-04-09T05:55:23Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1865</URL>\n\n<TITLE>[Question] Is the LLaVA-1.5-HD model not open source?</TITLE>\n\n<BODY>### Question\n\nIs the LLaVA-1.5-HD model not open source? I couldn't find LLaVA-1.5-HD checkpoints on huggingface？</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1864,
    "state": "closed",
    "created_by": "HuangChiEn",
    "created_at": "2025-04-03T14:26:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1864</URL>\n\n<TITLE>[Question] Does modality_lengths have meaning ?</TITLE>\n\n<BODY>### Question\n\nI don't think so, modality_lengths assign lang with minus `cur_len` for separating with mm input.\nHowever, get_modality_length_grouped_indices call twice get_length_grouped_indices without meaning..\n\ni have no idea, why we need modality_lengths for get_modality_length_grouped_indices , why not just do not seperate different input in finetuning stage in sampler ?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1862,
    "state": "open",
    "created_by": "july-love",
    "created_at": "2025-04-01T02:55:26Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1862</URL>\n\n<TITLE>[Usage] llava-1.6-34在推理图像上输出不符合要求</TITLE>\n\n<BODY>### Describe the issue\n\n\n1.输出重复\n2.乱回答\n3.有字符不断出现</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1861,
    "state": "open",
    "created_by": "kky677",
    "created_at": "2025-03-25T17:04:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1861</URL>\n\n<TITLE>[Usage] ImportError: cannot import name 'KeywordsStoppingCriteria' from 'llava.model.utils'</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\nImportError: cannot import name 'KeywordsStoppingCriteria' from 'llava.model.utils'\nCommand:\n\nfrom llava.model.utils import KeywordsStoppingCriteria\n\nLog: \n\n from llava.model.utils import KeywordsStoppingCriteria\nImportError: cannot import name 'KeywordsStoppingCriteria' from 'llava.model.utils'\n\nScreenshots:\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1860,
    "state": "closed",
    "created_by": "Horizonll",
    "created_at": "2025-03-25T07:32:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1860</URL>\n\n<TITLE>[Question] zero_2 for finetune</TITLE>\n\n<BODY>### Question\n\nCan I use zero_2.json for finetune? Thanks.</BODY>\n\n<COMMENTS>\n<Comment by Yuanhong-Zheng at 2025-04-20T04:29:54Z>\nsame question\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1859,
    "state": "open",
    "created_by": "july-love",
    "created_at": "2025-03-24T02:14:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1859</URL>\n\n<TITLE>如何解决UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes.</TITLE>\n\n<BODY>### Describe the issue\n\n\nUserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.   warnings.warn( /root/miniconda3/envs/lla/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.   warnings.warn( Floating point exception (core dumped)</BODY>\n\n<COMMENTS>\n<Comment by clf28 at 2025-07-15T09:03:39Z>\n同样有这个问题，请问解决了吗？\n</Comment>\n<Comment by huxiaofs at 2025-07-26T05:07:55Z>\n@july-love I encountered the same problem. How did you resolve it?\n</Comment>\n<Comment by july-love at 2025-07-26T13:26:28Z>\n> 我遇到了同样的问题。你是如何解决的？\n\n抱歉，没有解决\n</Comment>\n<Comment by july-love at 2025-07-26T13:26:43Z>\n> 同样有这个问题，请问解决了吗？\n\n抱歉，没有解决\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1858,
    "state": "open",
    "created_by": "UCASlemontea",
    "created_at": "2025-03-21T04:27:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1858</URL>\n\n<TITLE>[Question] ValueError: The train_dataset does not implement __len__, max_steps has to be specified. The number of steps needs to be known in advance for the learning rate scheduler.</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by UCASlemontea at 2025-04-03T13:20:46Z>\nno，i still have this problem\n\n\n\n---- Replied Message ----\n| From | yinfeng ***@***.***> |\n| Date | 03/23/2025 23:01 |\n| To | ***@***.***> |\n| Cc | ***@***.***>***@***.***> |\n| Subject | Re: [haotian-liu/LLaVA] [Question] ValueError: The train_dataset does not implement __len__, max_steps has to be specified. The number of steps needs to be known in advance for the learning rate scheduler. (Issue #1858) |\n\nDid u meet this trouble in training model with customized data? Have u solved it?\n\n—\nReply to this email directly, view it on GitHub, or unsubscribe.\nYou are receiving this because you authored the thread.Message ID: ***@***.***>\n\nWangyf1998 left a comment (haotian-liu/LLaVA#1858)\n\nDid u meet this trouble in training model with customized data? Have u solved it?\n\n—\nReply to this email directly, view it on GitHub, or unsubscribe.\nYou are receiving this because you authored the thread.Message ID: ***@***.***>\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1857,
    "state": "open",
    "created_by": "ge-bin-hui",
    "created_at": "2025-03-20T05:59:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1857</URL>\n\n<TITLE>[Question] can not find the script extract_video_frames.py</TITLE>\n\n<BODY>### Question\n\ncan not find the script extract_video_frames.py</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1856,
    "state": "open",
    "created_by": "CurryaNa",
    "created_at": "2025-03-18T17:07:09Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1856</URL>\n\n<TITLE>[Question] 维度不匹配</TITLE>\n\n<BODY>### Question\n\n    embeddings = embeddings + self.position_embedding(self.position_ids)\nRuntimeError: The size of tensor a (122) must match the size of tensor b (50) at non-singleton dimension 1\n推理的时候报了这个错</BODY>\n\n<COMMENTS>\n<Comment by HelloRuccs at 2025-07-08T07:14:42Z>\n请问你后来解决了吗 我也碰到相似的问题\n</Comment>\n<Comment by jatural at 2025-08-18T11:45:52Z>\n请问解决问题了吗？我训练时也报了相同的错误\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1854,
    "state": "open",
    "created_by": "kydxh",
    "created_at": "2025-03-17T10:26:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1854</URL>\n\n<TITLE>IndexError: piece id is out of range.</TITLE>\n\n<BODY>I want to inference llava v1.5, but encountered error \"IndexError: piece id is out of range\". The environment I used is \"llava 1.1.3, transformers 4.31.0, sentencepiece 0.1.99, torch 2.0.1\"\nThe detailed error information is listed as follows:\noutput = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)[\nFile \"/root/anaconda3/envs/llava16/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3485, in batch_decode\nreturn [\nFile \"/root/anaconda3/envs/llava16/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3486, in\nself.decode(\nFile \"/root/anaconda3/envs/llava16/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3525, in decode\nreturn self._decode(\nFile \"/root/anaconda3/envs/llava16/lib/python3.10/site-packages/transformers/tokenization_utils.py\", line 931, in _decode\nfiltered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)\nFile \"/root/anaconda3/envs/llava16/lib/python3.10/site-packages/transformers/tokenization_utils.py\", line 912, in convert_ids_to_tokens\ntokens.append(self._convert_id_to_token(index))\nFile \"/root/anaconda3/envs/llava16/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama.py\", line 204, in _convert_id_to_token\ntoken = self.sp_model.IdToPiece(index)\nFile \"/root/anaconda3/envs/llava16/lib/python3.10/site-packages/sentencepiece/init.py\", line 1045, in _batched_func\nreturn _func(self, arg)\nFile \"/root/anaconda3/envs/llava16/lib/python3.10/site-packages/sentencepiece/init.py\", line 1038, in _func\nraise IndexError('piece id is out of range.')\nHow can I solve the problem?</BODY>\n\n<COMMENTS>\n<Comment by Domanmaker at 2025-07-07T05:42:41Z>\nsame\n</Comment>\n<Comment by Panahda at 2025-07-07T20:00:35Z>\nany updates?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1853,
    "state": "closed",
    "created_by": "skbtt",
    "created_at": "2025-03-16T13:40:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1853</URL>\n\n<TITLE>[Discussion]</TITLE>\n\n<BODY></BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1852,
    "state": "open",
    "created_by": "jmanhype",
    "created_at": "2025-03-16T00:02:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1852</URL>\n\n<TITLE>Proposal: Integrating Sparse Autoencoders (SAEs) for LLaVA Interpretability</TITLE>\n\n<BODY></BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1851,
    "state": "open",
    "created_by": "DAIDJACREX",
    "created_at": "2025-03-14T19:19:55Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1851</URL>\n\n<TITLE>[Feature request]</TITLE>\n\n<BODY>### feature\n\n_No response_</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1850,
    "state": "open",
    "created_by": "Jay-zzcoder",
    "created_at": "2025-03-12T02:00:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1850</URL>\n\n<TITLE>Question about the shape of lm_head.weight</TITLE>\n\n<BODY>Question\n\nI found that if I don't pass the BitsAndBytesConfig args in LlavaLlamaForCausalLM.from_pretrained() like this:\n`model = LlavaLlamaForCausalLM.from_pretrained(\n                tokenizer_path,\n                torch_dtype=torch.bfloat16 ,\n                #**bnb_model_from_pretrained_args\n            )`\nthe shape of lm_head.weight is torch.Size([32000, 5120])\n------------------------------------------------\nBut if I pass BitsAndBytesConfig args in LlavaLlamaForCausalLM.from_pretrained() like: \n`model = LlavaLlamaForCausalLM.from_pretrained(\n                tokenizer_path,\n                torch_dtype=torch.bfloat16 ,\n                **bnb_model_from_pretrained_args\n            )`\nthe shape of lm_head.weight will change and become torch.Size([81920000, 1])\n\nWhy the shape of lm_head.weight changes?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1849,
    "state": "open",
    "created_by": "GoodStarLink",
    "created_at": "2025-03-10T12:13:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1849</URL>\n\n<TITLE>[Usage] from .model.language_model.llava_llama import LlavaLlamaForCausalLM is not work</TITLE>\n\n<BODY>### Describe the issue\n\nfrom .model.language_model.llava_llama import LlavaLlamaForCausalLM is not work. try this one: from .model.language_model.llava_llama import LlavaLlamaForCausalLM may solve the problem</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1848,
    "state": "open",
    "created_by": "keanshi-nlp",
    "created_at": "2025-03-09T09:45:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1848</URL>\n\n<TITLE>[Usage] Couldn't save full model after pretrain.</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: When I finished the `pretrain.sh`, the ourput only contains `mm_projector.bin`, `config.json`, `trainer_state.json`. How can I fix it?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1847,
    "state": "open",
    "created_by": "enfantsRichesDeprimes",
    "created_at": "2025-03-08T12:53:59Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1847</URL>\n\n<TITLE>[Question] Where can I download \"llava_gqa_testdev_balanced.jsonl\"</TITLE>\n\n<BODY>### Question\n\nWhere can I download \"llava_gqa_testdev_balanced.jsonl\"</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1846,
    "state": "open",
    "created_by": "LiZhangMing",
    "created_at": "2025-02-28T14:15:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1846</URL>\n\n<TITLE>[Question] The gradient for the additional tunable parameters is None.</TITLE>\n\n<BODY>### Question\n\nFirst of all, thank you very much for your work. I want to add some tunable parameters into the CLIP attention during the LLava fine-tuning process. These parameters have their `requires_grad` set to True and have been included in the `optimizer_grouped_parameters`. However, during training, I noticed that the gradients for these fine-tuning parameters in the optimizer are `p.grad=None`. Could you please advise how I should modify the project? Many thanks!\n\n(Note: I have commented out the `@torch.no_grad()` decorator in the `clip_encoder.py` file; is there anything else that needs to be changed?)\n\n<img width=\"911\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e409f14f-09ee-4119-93cf-015ea1166e27\" /></BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1845,
    "state": "open",
    "created_by": "zzb2019053515",
    "created_at": "2025-02-28T07:41:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1845</URL>\n\n<TITLE>Source data with bounding boxes</TITLE>\n\n<BODY>### Question\n\nHello, where can I find the source data with bounding boxes used to make instruction tunning data?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1843,
    "state": "open",
    "created_by": "fanhso",
    "created_at": "2025-02-27T11:20:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1843</URL>\n\n<TITLE>[Usage] Unable to open the controller.</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:Unable to open the controller.\n\nCommand:\n```\nPASTE THE COMMANDS HERE.\n```\n\n![Image](https://github.com/user-attachments/assets/aff4bed8-4ccb-462d-b086-e46e0a38c265)\n\nLog: \n```\nPASTE THE LOGS HERE.\n```\n\nScreenshots:\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1842,
    "state": "open",
    "created_by": "zxdscsfm",
    "created_at": "2025-02-27T11:02:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1842</URL>\n\n<TITLE>[Question] Issue with Model Type Mismatch and Download Stuck at 0%</TITLE>\n\n<BODY>### Question\n\nI encountered the following issue:\n`Youare using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.` \nAdditionally, the download progress is stuck at 0%, and it continuously shows:\n`Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]`\n**Steps I took:**\nI set the mirror endpoint in the pretrain.sh script:\n`export HF_ENDPOINT=https://hf-mirror.com`\nThe model I’m using is **lmsys/vicuna-7b-v1.5**, and I am attempting to train it.\nHowever, the system is telling me that I am using a llama model to instantiate a llava_llama model, and the download is stuck at 0%.\nMy Questions:\nWhy is there a model type mismatch between llama and llava_llama?\nWhat kind of model should I use to ensure compatibility with llava_llama?\nIs there an official recommended model or conversion process I should follow to proceed with training?\nIf anyone has encountered similar issues or knows of any solutions, I would really appreciate your help. Thank you!</BODY>\n\n<COMMENTS>\n<Comment by mondalanindya at 2025-03-09T12:39:59Z>\nDoes anyone have a fix for this?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1841,
    "state": "open",
    "created_by": "DengNingyuan",
    "created_at": "2025-02-26T07:33:23Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1841</URL>\n\n<TITLE>[Question] Why the forward.shape different with backward.shape before 21.layer</TITLE>\n\n<BODY>### Question\n\nI use module.register_forward_hook and module.register_backward_hook to check the forward and backward results, then I find the shapes are different in some layers.  Here are some examples, I want to know why they are different.\n\n\n0 model.layers.0.self_attn.q_proj\nforward_shape torch.Size([5, 1024, 24, 24])\nbackward_shape torch.Size([5, 576, 4096])\n-------------------\n1 model.layers.0.self_attn.k_proj\nforward_shape torch.Size([1, 577, 1024])\nbackward_shape torch.Size([1, 2691, 4096])\n-------------------\n2 model.layers.0.self_attn.v_proj\nforward_shape torch.Size([5, 577, 1024])\nbackward_shape torch.Size([1, 2691, 4096])\n-------------------</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1840,
    "state": "open",
    "created_by": "intelligence66",
    "created_at": "2025-02-26T06:54:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1840</URL>\n\n<TITLE>ImportError: cannot import name 'LlavaLlamaForCausalLM' from 'llava.model' (/root/LLaVA/llava/model/__init__.py)</TITLE>\n\n<BODY>### Question\n\nI encountered an issue where I was unable to import the model while running Lora fine-tuning again，this is the printed log:\nTraceback (most recent call last):\n  File \"/root/LLaVA/llava/train/train_mem.py\", line 1, in <module>\n    from llava.train.train import train\n  File \"/root/LLaVA/llava/__init__.py\", line 1, in <module>\n    from .model import LlavaLlamaForCausalLM\nImportError: cannot import name 'LlavaLlamaForCausalLM' from 'llava.model' (/root/LLaVA/llava/model/__init__.py)\n\nThe script I am running is:  sh ./scripts/v1_5/finetune_task_lora.sh</BODY>\n\n<COMMENTS>\n<Comment by SixCorePeach at 2025-02-27T08:24:25Z>\nI also encount for this problem, is there lack of any file named the LlavaLlamaForCausalLM ？\n</Comment>\n<Comment by SixCorePeach at 2025-02-27T08:28:13Z>\noh, I see the __init__ file, \nAVAILABLE_MODELS = {\n    \"llava_llama\": \"LlavaLlamaForCausalLM, LlavaConfig\",\n    \"llava_qwen\": \"LlavaQwenForCausalLM, LlavaQwenConfig\",\n    \"llava_mistral\": \"LlavaMistralForCausalLM, LlavaMistralConfig\",\n    \"llava_mixtral\": \"LlavaMixtralForCausalLM, LlavaMixtralConfig\",\n    # \"llava_qwen_moe\": \"LlavaQwenMoeForCausalLM, LlavaQwenMoeConfig\",    \n    # Add other models as needed\n}\nwe could try to initial the corresponding model.\ne.g. the llava_llama is in llava.model.language_model\n\nwhen you solve this problem, there will get another one\n</Comment>\n<Comment by intelligence66 at 2025-02-27T09:58:57Z>\nThere was a version conflict with the package, but I resolved it after reinstalling the package version.This is the version of the package I reinstalled:\n\n![Image](https://github.com/user-attachments/assets/98edd5d4-bd48-47a6-8cfa-66d47ad4668b)\n</Comment>\n<Comment by rajsinha02 at 2025-02-27T22:22:00Z>\nI am facing this exact same issue. Was anyone able to resolve successfully?\n</Comment>\n<Comment by liuxiang09 at 2025-03-25T13:24:07Z>\n这是 flash-attn 包与 PyTorch 版本不兼容导致的问题。首先pytorch版本要是12.1，然后如果你只是想先试一下demo，可以通过pip uninstall flash-attn ，卸载flash-attn。在其他条件正常的情况下，就不会出现这个报错了\n</Comment>\n<Comment by Jack1447 at 2025-04-27T13:35:37Z>\nAbout flash-attn, ImportError: cannot import name ‘LlavaLlamaForCausalLM‘ from ‘llava.model‘ or Failed to build installable wheels for some pyproject.toml based projects (flash-attn) \nInstall the appropriate version according to your environment, and I've successfully solved it\n</Comment>\n<Comment by lsnls at 2025-06-15T08:17:34Z>\n是的，这是由于安装了不兼容的flash-attn库的原因，去flash-attn官网`https://github.com/Dao-AILab/flash-attention`下载对应版本的whl文件，然后在服务器上本地安装即可。\n\n我的CUDA版本是11.8，Pytorch版本是2.1，下载了`https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.2.post1/flash_attn-2.7.2.post1+cu11torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl`，在服务器上运行`pip install flash_attn-2.7.2.post1+cu11torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl`。\n</Comment>\n<Comment by YQ157 at 2025-06-23T11:31:53Z>\n> There was a version conflict with the package, but I resolved it after reinstalling the package version.This is the version of the package I reinstalled:出现了一个与包的版本冲突，但在重新安装包版本后我解决了这个问题。这是我重新安装的包的版本：\n> \n> ![Image](https://github.com/user-attachments/assets/98edd5d4-bd48-47a6-8cfa-66d47ad4668b)\n\nI encountered the same problem and it couldn't be resolved when installing 2.8.0, while 2.7.3 resolved it.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1838,
    "state": "open",
    "created_by": "NmTamil2",
    "created_at": "2025-02-25T05:46:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1838</URL>\n\n<TITLE>FileNotFoundError: [Errno 2] No such file or directory: './checkpoints/videollava-7b-pretrain/mm_projector.bin'</TITLE>\n\n<BODY>I cloned the repository [Video-LLaVA](https://github.com/PKU-YuanGroup/Video-LLaVA/tree/main) to fine-tune video-llava-7b-hf. After cloning, I installed the required Python packages and tried running the scripts/v1.5/finetune_lora.sh script. However, I encountered the following error:\n\nFileNotFoundError: [Errno 2] No such file or directory: './checkpoints/videollava-7b-pretrain/mm_projector.bin'\n\nCan anyone help me resolve this issue? Thanks in advance!\n\n`Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:19<00:00,  9.71s/it]\nAdding LoRA adapters...\nTraceback (most recent call last):\n  File \"/workspace/Video-LLaVA/videollava/train/train_mem.py\", line 13, in <module>\n    train()\n  File \"/workspace/Video-LLaVA/videollava/train/train.py\", line 1003, in train\n    model.get_model().initialize_vision_modules(\n  File \"/workspace/Video-LLaVA/videollava/model/llava_arch.py\", line 118, in initialize_vision_modules\n    mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cuda:0')\n  File \"/workspace/Video-LLaVA/VL7b_ftEnv/lib/python3.10/site-packages/torch/serialization.py\", line 791, in load\n    with _open_file_like(f, 'rb') as opened_file:\n  File \"/workspace/Video-LLaVA/VL7b_ftEnv/lib/python3.10/site-packages/torch/serialization.py\", line 271, in _open_file_like\n    return _open_file(name_or_buffer, mode)\n  File \"/workspace/Video-LLaVA/VL7b_ftEnv/lib/python3.10/site-packages/torch/serialization.py\", line 252, in __init__\n    super().__init__(open(name, mode))\n**FileNotFoundError: [Errno 2] No such file or directory: './checkpoints/videollava-7b-pretrain/mm_projector.bin'**\n[2025-02-24 11:44:59,757] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 116598\n[2025-02-24 11:44:59,757] [ERROR] [launch.py:321:sigkill_handler] ['/workspace/Video-LLaVA/VL7b_ftEnv/bin/python3', '-u', 'videollava/train/train_mem.py', '--local_rank=0', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero2_offload.json', '--model_name_or_path', '/workspace/Video-LLaVA/vicuna-7b-v1.5/models--lmsys--vicuna-7b-v1.5/snapshots/3321f76e3f527bd14065daf69dad9344000a201d', '--version', 'v1', '--data_path', '/workspace/Video-LLaVA/datasets/videochatgpt_tune_.json', '/workspace/Video-LLaVA/datasets/nlp_tune.json', '--video_folder', '/workspace/Video-LLaVA/datasets/videos', '--video_tower', '/workspace/Video-LLaVA/LanguageBind_Video_merge', '--mm_projector_type', 'mlp2x_gelu', '--pretrain_mm_mlp_adapter', './checkpoints/videollava-7b-pretrain/mm_projector.bin', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'False', '--fp16', 'True', '--output_dir', './checkpoints/videollava-7b-lora', '--num_train_epochs', '1', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '2', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'False', '--model_max_length', '2048', '--tokenizer_model_max_length', '3072', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'tensorboard', '--cache_dir', './cache_dir'] exits with return code = 1`</BODY>\n\n<COMMENTS>\n<Comment by Haowen-Ji at 2025-02-28T14:39:42Z>\nDo you have the `mm_projector.bin` file in the target folder? I solved a similar issue by downloading the \"mm_projector.bin\" and adding it manually\n</Comment>\n<Comment by ZXXaaaa at 2025-03-21T13:51:50Z>\n> Do you have the `mm_projector.bin` file in the target folder? I solved a similar issue by downloading the \"mm_projector.bin\" and adding it manually\n\nhello, can you share me the download link to \"mm_projector.bin\"? Thank you very much.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1837,
    "state": "open",
    "created_by": "ErwinZhou",
    "created_at": "2025-02-24T12:14:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1837</URL>\n\n<TITLE>[Usage] AttributeError: 'AcceleratorState' object has no attribute 'distributed_type'</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\n```\nAttributeError: 'AcceleratorState' object has no attribute 'distributed_type'\n```\n\nCommand:\n```\nbash scripts/v1_5/pretrain.sh\n```\n\nEnvironment Config\n```\n- `Accelerate` version: 0.21.0\n- Platform: Linux-5.15.0-97-generic-x86_64-with-glibc2.35\n- Python version: 3.10.16\n- Numpy version: 1.26.4\n- PyTorch version (GPU?): 2.1.2+cu121 (True)\n- PyTorch XPU available: False\n- PyTorch NPU available: False\n- System RAM: 1007.52 GB\n- GPU type: NVIDIA GeForce RTX 4090\n- `Accelerate` default config:\n        Not found\nPython 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> from torch.utils.collect_env import get_pretty_env_info\n>>> \n>>> print(get_pretty_env_info())\nPyTorch version: 2.1.2+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.22.1\nLibc version: glibc-2.35\n\nPython version: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-97-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA GeForce RTX 4090\nGPU 1: NVIDIA GeForce RTX 4090\n\nNvidia driver version: 550.107.02\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      52 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             128\nOn-line CPU(s) list:                0-127\nVendor ID:                          GenuineIntel\nModel name:                         Intel(R) Xeon(R) Gold 6430\nCPU family:                         6\nModel:                              143\nThread(s) per core:                 2\nCore(s) per socket:                 32\nSocket(s):                          2\nStepping:                           8\nCPU max MHz:                        3400.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           4200.00\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                     VT-x\nL1d cache:                          3 MiB (64 instances)\nL1i cache:                          2 MiB (64 instances)\nL2 cache:                           128 MiB (64 instances)\nL3 cache:                           120 MiB (2 instances)\nNUMA node(s):                       2\nNUMA node0 CPU(s):                  0-31,64-95\nNUMA node1 CPU(s):                  32-63,96-127\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.1.2\n[pip3] torchvision==0.16.2\n[pip3] triton==2.1.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.1.2                    pypi_0    pypi\n[conda] torchvision               0.16.2                   pypi_0    pypi\n[conda] triton                    2.1.0                    pypi_0    pypi\n```\n\nLog: \n```\nFull Trackback:\n[2025-02-24 16:42:40,538] [INFO] [comm.py:637:init_distributed] cdb=None\n[2025-02-24 16:42:40,538] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\nTraceback (most recent call last):\nFile \"/root/autodl-tmp/yuchen-zhou/LLaVA/llava/train/train_mem.py\", line 8, in\ntrain(attn_implementation=\"flash_attention_2\")\nFile \"/root/autodl-tmp/yuchen-zhou/LLaVA/llava/train/train.py\", line 974, in train\ntrainer.train()\nFile \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\nreturn inner_training_loop(\nFile \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1568, in _inner_training_loop\ntrain_dataloader = self.get_train_dataloader()\nFile \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 810, in get_train_dataloader\nreturn self.accelerator.prepare(DataLoader(train_dataset, **dataloader_params))\nFile \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1142, in prepare\nif self.distributed_type == DistributedType.FSDP:\nFile \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py\", line 468, in distributed_type\nreturn self.state.distributed_type\nAttributeError: 'AcceleratorState' object has no attribute 'distributed_type'\nTraceback (most recent call last):\nFile \"/root/autodl-tmp/yuchen-zhou/LLaVA/llava/train/train_mem.py\", line 8, in\ntrain(attn_implementation=\"flash_attention_2\")\nFile \"/root/autodl-tmp/yuchen-zhou/LLaVA/llava/train/train.py\", line 974, in train\ntrainer.train()\nFile \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\nreturn inner_training_loop(\nFile \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1568, in _inner_training_loop\ntrain_dataloader = self.get_train_dataloader()\nFile \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 810, in get_train_dataloader\nreturn self.accelerator.prepare(DataLoader(train_dataset, **dataloader_params))\nFile \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1142, in prepare\nif self.distributed_type == DistributedType.FSDP:\nFile \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py\", line 468, in distributed_type\nreturn self.state.distributed_type\nAttributeError: 'AcceleratorState' object has no attribute 'distributed_type'\nTraceback (most recent call last):\nFile \"/root/autodl-tmp/yuchen-zhou/LLaVA/llava/train/train_mem.py\", line 8, in\ntrain(attn_implementation=\"flash_attention_2\")\nFile \"/root/autodl-tmp/yuchen-zhou/LLaVA/llava/train/train.py\", line 974, in train\ntrainer.train()\nFile \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\nreturn inner_training_loop(\nFile \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1568, in _inner_training_loop\ntrain_dataloader = self.get_train_dataloader()\nFile \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 810, in get_train_dataloader\nreturn self.accelerator.prepare(DataLoader(train_dataset, **dataloader_params))\nFile \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1142, in prepare\nif self.distributed_type == DistributedType.FSDP:\nFile \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py\", line 468, in distributed_type\nreturn self.state.distributed_type\nAttributeError: 'AcceleratorState' object has no attribute 'distributed_type'\nTraceback (most recent call last):\nFile \"/root/autodl-tmp/yuchen-zhou/LLaVA/llava/train/train_mem.py\", line 8, in\ntrain(attn_implementation=\"flash_attention_2\")\nFile \"/root/autodl-tmp/yuchen-zhou/LLaVA/llava/train/train.py\", line 974, in train\ntrainer.train()\nFile \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\nreturn inner_training_loop(\nFile \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1568, in _inner_training_loop\ntrain_dataloader = self.get_train_dataloader()\nFile \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 810, in get_train_dataloader\nreturn self.accelerator.prepare(DataLoader(train_dataset, **dataloader_params))\nFile \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1142, in prepare\nif self.distributed_type == DistributedType.FSDP:\nFile \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py\", line 468, in distributed_type\nreturn self.state.distributed_type\nAttributeError: 'AcceleratorState' object has no attribute 'distributed_type'\n```</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1835,
    "state": "open",
    "created_by": "FrankYang-17",
    "created_at": "2025-02-16T04:54:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1835</URL>\n\n<TITLE>[Question] Cannot reproduce LLaVA-1.5-7b performance on MME with lmms-eval</TITLE>\n\n<BODY>### Question\n\nHi there.\nWe cannot reproduce MME results following [finetune.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune.sh) on 665k instruction tuning dataset. We followed all the training settings(pretrain+sft), but get 1497/289 on MME with lmms-eval.</BODY>\n\n<COMMENTS>\n<Comment by muktilin at 2025-02-19T05:07:37Z>\nI also get the similar results:\n\n<img width=\"351\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/794c541d-eb17-4eb9-926f-dd3400202248\" />\n</Comment>\n<Comment by FrankYang-17 at 2025-02-23T05:09:51Z>\n> I also get the similar results:我也得到了类似的结果：\n> \n> <img alt=\"Image\" width=\"351\" src=\"https://private-user-images.githubusercontent.com/101244001/414555264-794c541d-eb17-4eb9-926f-dd3400202248.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDAyODc2NDcsIm5iZiI6MTc0MDI4NzM0NywicGF0aCI6Ii8xMDEyNDQwMDEvNDE0NTU1MjY0LTc5NGM1NDFkLWViMTctNGViOS05MjZmLWRkMzQwMDIwMjI0OC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMjIzJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDIyM1QwNTA5MDdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1jZWI1N2QwZDEzODdhZTBkMDUyMTIzZjE3ZTcyMjZjYjUzYTJlNGZlZjIxYjA4OTQ1MmIzN2I1YTgwNmQwZDk5JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.3b4J_9R7LDtbVeSfU1k7zw7bGusAO20ZHT8-UryHkuo\">\n\nsame issue, please inform me if you solve it, thanks!!!\n</Comment>\n<Comment by TungChintao at 2025-03-21T11:17:45Z>\nsame issue\n</Comment>\n<Comment by jhjangjh at 2025-07-12T10:02:44Z>\nCan I ask your lmms-eval version?\n\nI noticed that some of the dependencies required by `lmms-eval` (such as `accelerate`, `httpx`, etc.) conflict with the versions used in LLaVA:(\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1834,
    "state": "open",
    "created_by": "Tess314",
    "created_at": "2025-02-12T09:23:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1834</URL>\n\n<TITLE>[Question] How can I log accuracy after each epoch?</TITLE>\n\n<BODY>### Question\n\nHi there,\n\nI want to log my model's accuracy after each epoch and its final accuracy at the end but I cannot find a simple way of doing this.\n\nI am following [this](https://console.brev.dev/launchable/deploy?userID=p2mzt91a8&orgID=jnj0c501d&launchableID=env-2hpxJ6HArVk5jzOYgJmFDJfvNmH&instance=A10G%40g5.12xlarge&diskStorage=300&cloudID=devplane-brev-1&python=3.10&cuda=12.2.2&file=https%3A%2F%2Fgithub.com%2Fbrevdev%2Fnotebooks%2Fblob%2Fmain%2Fllava-finetune.ipynb&name=Fine-tune+and+deploy+multimodal+LLaVA-1.5) tutorial.\n\nMy code is as follows:\n\nimport wandb\nwandb.login()\n\n!deepspeed LLaVA/llava/train/train_mem.py\n--lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5\n--deepspeed LLaVA/scripts/zero3.json\n--model_name_or_path liuhaotian/llava-v1.5-13b\n--version v1\n--data_path ./dataset/train/dataset.json\n--image_folder ./dataset/images\n--vision_tower openai/clip-vit-large-patch14-336\n--mm_projector_type mlp2x_gelu\n--mm_vision_select_layer -2\n--mm_use_im_start_end False\n--mm_use_im_patch_token False\n--image_aspect_ratio pad\n--group_by_modality_length True\n--bf16 True\n--output_dir ./checkpoints/llava-v1.5-13b-task-lora\n--num_train_epochs 10\n--per_device_train_batch_size 16\n--per_device_eval_batch_size 4\n--gradient_accumulation_steps 1\n--evaluation_strategy \"no\"\n--save_strategy \"steps\"\n--save_steps 50000\n--save_total_limit 1\n--learning_rate 2e-4\n--weight_decay 0.\n--warmup_ratio 0.03\n--lr_scheduler_type \"cosine\"\n--logging_steps 1\n--tf32 True\n--model_max_length 2048\n--gradient_checkpointing True\n--dataloader_num_workers 4\n--lazy_preprocess True\n--report_to wandb\n\nI have already asked wandb and deepspeed about this and they were unable to help and advised me to create an issue here.\n\nAny help or advice would be appreciated.</BODY>\n\n<COMMENTS>\n<Comment by Tess314 at 2025-02-19T11:43:55Z>\nAny updates @haotian-liu ?\n</Comment>\n<Comment by Tess314 at 2025-02-28T09:17:40Z>\nI am still trying to get this to work.\n\nI have tried:\ndo_eval\nvalidation_file\neval_dataset\nvalidation_data_path\n\nbut I always get the same error:\nValueError: Some specified arguments are not used by the HfArgumentParser\n\nAny help?\n</Comment>\n<Comment by Tess314 at 2025-03-04T12:27:28Z>\nAny updates @haotian-liu ?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1833,
    "state": "open",
    "created_by": "0xnakul",
    "created_at": "2025-02-10T00:49:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1833</URL>\n\n<TITLE>[Question] Image padding features removal in LLaVA-1.5-HD</TITLE>\n\n<BODY>### Question\n\nHello @haotian-liu !! In the A.1 section of the \"Improved Baselines with Visual Instruction Tuning\" paper, it is mentioned that:\n>Padding removal. Features corresponding exclusively to the paddings are discarded.\n\nHow is this done? I'm having trouble finding this block in the LLaVA and the LLaVA-NeXT codebase. Can you please point me so that I can look into the implementation?\n\nThis further raises the question: the bounding boxes in the instruction tuning datasets are normalized assuming a square padded image as mentioned in #606. If the padding tokens are cut-off, won't it impact the normalization of the bounding boxes and ultimately the training?\n\nAm I missing something here?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1832,
    "state": "open",
    "created_by": "TainanKyle",
    "created_at": "2025-02-05T19:01:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1832</URL>\n\n<TITLE>[Usage] AssertionError Applying Delta to Get LLaVA Weights: edit_head.query not in base model</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\n\nHi,\n\nI followed your instructions to get the LLaVA weights by applying the delta, but I encountered the error `AssertionError: edit_head.query not in base model`.\n\nCommand:\n```\npython3 -m llava.model.apply_delta \\\n    --base /path/to/llama-7b \\\n    --target /output/path/to/LLaVA-7B-v0 \\\n    --delta liuhaotian/LLaVA-Lightning-7B-delta-v1-1\n```\n\nLog: \n```\n'vae.encoder.down_blocks.2.resnets.1.norm1.bias', 'unet.up_blocks.2.resnets.0.conv1.weight', 'unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'unet.mid_block.resnets.1.conv1.weight', 'edit_head.mapper.decoder.layers.3.norm1.bias', 'vae.decoder.up_blocks.2.resnets.2.norm1.weight', 'unet.down_blocks.2.resnets.1.norm1.bias', 'unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight', 'unet.up_blocks.1.resnets.1.conv1.bias', 'vae.decoder.mid_block.resnets.0.conv1.bias', 'vae.decoder.conv_in.bias', 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/home/gpl_homee/miniconda3/envs/mgie/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\ngeneration_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:00<00:00, 437kB/s]\ntokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 833/833 [00:00<00:00, 2.80MB/s]\ntokenizer.model: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500k/500k [00:00<00:00, 4.85MB/s]\nadded_tokens.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 88.0/88.0 [00:00<00:00, 317kB/s]\nspecial_tokens_map.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 97.0/97.0 [00:00<00:00, 346kB/s]\nApplying delta\nApplying delta:  23%|████████████████████████████████                                                                                                         | 325/1388 [00:10<00:34, 30.67it/s]\nTraceback (most recent call last):\n  File \"/home/gpl_homee/miniconda3/envs/mgie/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/home/gpl_homee/miniconda3/envs/mgie/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/home/gpl_homee/indoor_scene/ml-mgie/LLaVA/llava/model/apply_delta.py\", line 48, in <module>\n    apply_delta(args.base_model_path, args.target_model_path, args.delta_path)\n  File \"/home/gpl_homee/indoor_scene/ml-mgie/LLaVA/llava/model/apply_delta.py\", line 25, in apply_delta\n    assert name in ['model.mm_projector.weight', 'model.mm_projector.bias'], f'{name} not in base model'\nAssertionError: edit_head.query not in base model\n```\n\nI am using `llava 0.2.0`. Could you please advise how to resolve this issue?\n\nThanks!</BODY>\n\n<COMMENTS>\n<Comment by CaglarTaha at 2025-02-08T22:23:03Z>\nUpdate LLaVA code to latest main branch for delta compatibility\n\n- Switched from v0.2.0 to the main branch to ensure compatibility with the LLaVA-Lightning delta,\n  which requires support for the new edit_head layers and other architecture changes.\n- Updated installation instructions: now use `pip install -e .` to match the latest dependencies.\n- Updated delta application command:\n  \n  python3 -m llava.model.apply_delta \\\n      --base /path/to/llama-7b \\\n      --target /output/path/to/LLaVA-7B-v0 \\\n      --delta liuhaotian/LLaVA-Lightning-7B-delta-v1-1\n\n- Reminder: Confirm that your base model is LLaMA-7B (and not LLaVA pretrained weights) and that\n  the provided paths for --base and --target are correct and writable.\n- For troubleshooting, refer to the LLaVA-Lightning Documentation.\n\nThis should resolve assertion failures caused by mismatched versions.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1831,
    "state": "open",
    "created_by": "Safdari10",
    "created_at": "2025-02-05T03:46:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1831</URL>\n\n<TITLE>preprocessor_config.json Not Found for Model liuhaotian/llava-v1.6-mistral-7b</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: I am trying to load the LLaVA model liuhaotian/llava-v1.6-mistral-7b using the LlavaProcessor from the transformers library. However, I am encountering a 404 Client Error because the file preprocessor_config.json is not found at the specified URL.\n\nCommand:\n```\nimport torch\nfrom PIL import Image\nfrom transformers import LlavaProcessor, LlavaForConditionalGeneration\n\nmodel_name = \"liuhaotian/llava-v1.6-mistral-7b\"\n\ntry:\n    processor = LlavaProcessor.from_pretrained(model_name, trust_remote_code=True)\n    model = LlavaForConditionalGeneration.from_pretrained(model_name, trust_remote_code=True)\n    print(\"Model loaded successfully.\")\nexcept Exception as e:\n    print(f\"Error loading model: {e}\")\n```\n\nLog: \n```\nTraceback (most recent call last):\n  File \"C:\\Users\\safda\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line 406, in hf_raise_for_status\n    response.raise_for_status()\n  File \"C:\\Users\\safda\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/liuhaotian/llava-v1.6-mistral-7b/resolve/main/preprocessor_config.json\n\n...\n\nOSError: liuhaotian/llava-v1.6-mistral-7b does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/liuhaotian/llava-v1.6-mistral-7b/tree/main' for available files.\n```\n\nSteps to Reproduce:\n\nInstall the necessary libraries:\n```\npip install torch sentencepiece accelerate\npip install git+https://github.com/huggingface/transformers.git\npip install git+https://github.com/haotian-liu/LLaVA.git\n   ```\nRun the provided code snippet.\n\nExpected Behavior: The model should load successfully without any errors.\n\nActual Behavior: A 404 Client Error is raised because the file preprocessor_config.json is not found.\n\nAdditional Context: I have verified that the model name and URL are correct. Any guidance on resolving this issue would be greatly appreciated.</BODY>\n\n<COMMENTS>\n<Comment by mondalanindya at 2025-03-09T12:42:54Z>\nSame issue\n</Comment>\n<Comment by gaoweijun5 at 2025-07-28T03:48:39Z>\nSame problem. Have you solved the problem?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1830,
    "state": "open",
    "created_by": "adaykin",
    "created_at": "2025-01-30T04:31:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1830</URL>\n\n<TITLE>[Usage] Python Code Hangs For 20 Minutes After Running Following Script</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\n\nUsing the main branch..this is the last commit id in git history:\n\ncommit c121f0432da27facab705978f83c4ada465e46fd (HEAD -> main, origin/main, origin/HEAD)\n\nThe following code:\n\n```\nfrom llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path\nfrom llava.eval.run_llava import eval_model\n\nmodel_path = \"liuhaotian/llava-v1.5-7b\"\nprompt = \"What are the things I should be cautious about when I visit here?\"\nimage_file = \"https://llava-vl.github.io/static/images/view.jpg\"\n\nargs = type('Args', (), {\n    \"model_path\": model_path,\n    \"model_base\": None,\n    \"model_name\": get_model_name_from_path(model_path),\n    \"query\": prompt,\n    \"conv_mode\": None,\n    \"image_file\": image_file,\n    \"sep\": \",\",\n    \"temperature\": 0,\n    \"top_p\": None,\n    \"num_beams\": 1,\n    \"max_new_tokens\": 512\n})()\n\neval_model(args)\n```\n\nhangs when launched from the command line.\n\nCommand:\n```\npython3 test.py\n```\n\nLog: \n```\nHere is the stack trace:\n\n(llava) adaykin@AJD-Laptop:~/code/LLaVA$ python3 test.py\n/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\nLoading checkpoint shards:   0%|                                                        | 0/2 [00:00<?, ?it/s]/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoading checkpoint shards: 100%|████████████████████████████████████████████████| 2/2 [01:19<00:00, 39.69s/it]\n/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n\n\n\n4\n\n\n\n\n^CTraceback (most recent call last):\n  File \"/home/adaykin/code/LLaVA/test.py\", line 22, in <module>\n    eval_model(args)\n  File \"/home/adaykin/code/LLaVA/llava/eval/run_llava.py\", line 115, in eval_model\n    output_ids = model.generate(\n  File \"/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/adaykin/code/LLaVA/llava/model/language_model/llava_llama.py\", line 137, in generate\n    return super().generate(\n  File \"/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1479, in generate\n    return self.greedy_search(\n  File \"/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2340, in greedy_search\n    outputs = self(\n  File \"/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File \"/home/adaykin/code/LLaVA/llava/model/language_model/llava_llama.py\", line 91, in forward\n    return super().forward(\n  File \"/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1183, in forward\n    outputs = self.model(\n  File \"/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1070, in forward\n    layer_outputs = decoder_layer(\n  File \"/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File \"/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 812, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File \"/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 268, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n  File \"/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 160, in new_forward\n    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n  File \"/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 286, in pre_forward\n    set_module_tensor_to_device(\n  File \"/home/adaykin/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/utils/modeling.py\", line 298, in set_module_tensor_to_device\n    new_value = value.to(device)\nKeyboardInterrupt\n^C\n```\n\nScreenshots:\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1829,
    "state": "open",
    "created_by": "changyadong1125",
    "created_at": "2025-01-24T01:14:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1829</URL>\n\n<TITLE>输出的token数量怎么调整呀，改了很多东西都不起作用，求大佬帮助</TITLE>\n\n<BODY>### Question\n\n[Question] Token indices sequence length is longer than the specified maximum sequence length for this model (2386 > 2048). Running this sequence through the model will result in indexing errors</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1828,
    "state": "open",
    "created_by": "jloadbeach",
    "created_at": "2025-01-23T18:20:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1828</URL>\n\n<TITLE>[Usage] Deepspeed Multiple Nodes Finetuning</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\nHi,\n\nhow can i do finetuning on multiple (Azure) nodes with the provided deepspeed script?\nThank you!\n\nCommand:\n```\nI use this script: https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task_lora.sh\n```</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1827,
    "state": "open",
    "created_by": "donnyv",
    "created_at": "2025-01-23T16:10:27Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1827</URL>\n\n<TITLE>[Usage] Image inference isn't working</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\nI downloaded it and running it using Ollama. I'm using https://chatboxai.app. I wanted to try it with images. But it responds with its only a text chat model. I used the 7b model from here https://ollama.com/library/llava.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1826,
    "state": "open",
    "created_by": "zhlhlhlhl",
    "created_at": "2025-01-22T23:43:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1826</URL>\n\n<TITLE>[Question] How to load one more vision tower?</TITLE>\n\n<BODY>### Question\n\nI want to initialize one more vision tower in the llava_arch.py. I added \n`            self.sam_model = SamModel.from_pretrained(model_args.sgtm_sam_model_path,torch_dtype=torch.bfloat16)` in the     \ndef initialize_vision_modules(self, model_args, fsdp=None), and I mimic the original code to use def get_vision_tower(self) in the class LlavaMetaForCausalLM(ABC).\nHowever, I checked the self.sam_model that I want to initialize, and the parameters seem to be empty. How can another vision model in this part code be initialized correctly?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1825,
    "state": "closed",
    "created_by": "jason0000100007",
    "created_at": "2025-01-21T14:19:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1825</URL>\n\n<TITLE>No response when running on multiple GPUs.</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:I am using a single node with a single GPU, and the log information is as follows, but it continues to run. When I use multiple GPUs, the log information is as follows, but there is no response for a long time. Both GPUs are at 100% utilization, and each has 1186MB of memory used. What is the reason for this? Please help, and thank you everyone.\n\nLog: \n```\n[2025-01-21 22:09:54,321] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2025-01-21 22:09:55,583] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n[2025-01-21 22:09:55,583] [INFO] [runner.py:571:main] cmd = /home/u240875/.conda/envs/yxg-llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero3.json --model_name_or_path ./llava-v1.5-7b --version v1 --data_path ./playground/PETA/PETA.json --image_folder ./playground/PETA --vision_tower ./clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-7b-task-lora --num_train_epochs 1 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 1000 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb\n[2025-01-21 22:09:57,178] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2025-01-21 22:09:58,326] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}\n[2025-01-21 22:09:58,327] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0\n[2025-01-21 22:09:58,327] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})\n[2025-01-21 22:09:58,327] [INFO] [launch.py:163:main] dist_world_size=2\n[2025-01-21 22:09:58,327] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1\n[2025-01-21 22:10:00,558] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2025-01-21 22:10:00,710] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2025-01-21 22:10:01,547] [INFO] [comm.py:637:init_distributed] cdb=None\n[2025-01-21 22:10:01,786] [INFO] [comm.py:637:init_distributed] cdb=None\n[2025-01-21 22:10:01,786] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n```\n\nScreenshots:\n\n![Image](https://github.com/user-attachments/assets/a7f37f1e-8d96-4a62-9142-09344407df82)\n\n![Image](https://github.com/user-attachments/assets/1ed29312-62e0-48b1-ade2-1806b25bf665)</BODY>\n\n<COMMENTS>\n<Comment by kurbobo at 2025-03-10T09:09:26Z>\n@jason0000100007  Hello, did you fix it? I have same problem\n</Comment>\n<Comment by jason0000100007 at 2025-03-10T09:32:48Z>\n> [@jason0000100007](https://github.com/jason0000100007) Hello, did you fix it? I have same problem\n\nYes,the issue I encountered was due to the fact that,as shown in the picture,I used two GPUs which do not support NV-Link or a bridge connection.Therefore,they should not support distributed training.However,I was fortunate to resolve the problem by configuring`export NCCL_P2P_DISABLE=1`in the`.bashrc`file.However,I think the effect achieved is that each GPU processes a different  batch size,because the time is halved compared to using just one GPU.I hope this information can be helpful to you.Good luck!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1823,
    "state": "open",
    "created_by": "midofalasol",
    "created_at": "2025-01-19T09:02:50Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1823</URL>\n\n<TITLE>[Question] Which model is recommended for zero-shot classification tasks?</TITLE>\n\n<BODY>### Question\n\nHello!\n\nThank you for your outstanding work.\n\nHowever, I am sorry that my English is poor and I could not understand whether this code warehouse can realize the task of zero-sample image classification in a short time.\n\nIf I may, I would like to ask which model has the best performance on the zero-sample image classification task?\n\nSince my device with GPU can't connect to the network, I have to download the model offline and then do the inference, is this feasible? Do I need to make any changes after downloading checkpoint?\n\nLast question, from the other questions, some people said that CUDA 12.1 must be used, so I would like to ask what is the essential configuration in the environment?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1822,
    "state": "open",
    "created_by": "MarkDeng1",
    "created_at": "2025-01-14T19:14:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1822</URL>\n\n<TITLE>[Usage] We cannot use model(**input) outside the trainer.train()</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI have original model:  model\r\nthen, i trained it within the trainier.train.\r\n\r\nI use deepcopy to copy model called ref_model. then I use this model forward pass by ref_model(**input) within trainer.train .\r\n\r\nHowever, we can use model(**input) within trainer.train.  we cannot use ref_model(**input).\r\n\r\nWhy?\r\n\r\nCommand:\r\n```\r\nref_model(**input)\r\n```\r\n\r\nLog: \r\nTraceback (most recent call last):\r\n  File \"/fred/oz337/zdeng/prompt_stealing_ours/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"/fred/oz337/zdeng/prompt_stealing_ours/LLaVA/llava/train/train.py\", line 1101, in train\r\n    trainer.train()\r\n  File \"/fred/oz337/zdeng/.conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/fred/oz337/zdeng/.conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1869, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/fred/oz337/zdeng/prompt_stealing_ours/LLaVA/llava/train/llava_trainer.py\", line 397, in training_step\r\n    loss = super().training_step(model, inputs)\r\n  File \"/fred/oz337/zdeng/.conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2772, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/fred/oz337/zdeng/prompt_stealing_ours/LLaVA/llava/train/llava_trainer.py\", line 422, in compute_loss\r\n    loss = self.compute_loss_func(batch=inputs,\r\n  File \"/fred/oz337/zdeng/prompt_stealing_ours/LLaVA/llava/train/llava_trainer.py\", line 171, in compute_loss_func\r\n    ref0_output = ref0_model(**new_batch)\r\n  File \"/fred/oz337/zdeng/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/fred/oz337/zdeng/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/fred/oz337/zdeng/prompt_stealing_ours/LLaVA/llava/model/language_model/llava_llama.py\", line 106, in forward\r\n    return super().forward(\r\n  File \"/fred/oz337/zdeng/.conda/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1183, in forward\r\n    outputs = self.model(\r\n  File \"/fred/oz337/zdeng/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/fred/oz337/zdeng/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/fred/oz337/zdeng/.conda/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1070, in forward\r\n    layer_outputs = decoder_layer(\r\n  File \"/fred/oz337/zdeng/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/fred/oz337/zdeng/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/fred/oz337/zdeng/.conda/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 795, in forward\r\n    hidden_states = self.input_layernorm(hidden_states)\r\n  File \"/fred/oz337/zdeng/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/fred/oz337/zdeng/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/fred/oz337/zdeng/.conda/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 117, in forward\r\n    return self.weight * hidden_states.to(input_dtype)\r\nRuntimeError: The size of tensor a (0) must match the size of tensor b (4096) at non-singleton dimension 2\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1821,
    "state": "open",
    "created_by": "phycholosogy",
    "created_at": "2025-01-13T08:59:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1821</URL>\n\n<TITLE>[Usage] Errors when installing the packages. Maybe python 3.11 can work if you encountered a segmentation fault</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI following the commands in the \"Install Package\":\r\nCommand:\r\n```\r\nconda create -n llava python=3.10 -y\r\nconda activate llava\r\npip install --upgrade pip  # enable PEP 660 support\r\npip install -e .\r\n```\r\nHowever, there's a Error at this stage:\r\nLog: \r\n```\r\nBuilding wheels for collected packages: llava\r\n  Building editable for llava (pyproject.toml) ... done\r\n  Created wheel for llava: filename=llava-1.2.2.post1-0.editable-py3-none-any.whl size=17884 sha256=16641805b47f4eeb7a204b4da44a24d0bd1a1743fc45e91e9dd83ba1659910af\r\n  Stored in directory: /tmp/pip-ephem-wheel-cache-o_z3astm/wheels/86/af/85/d3b6e65da4a7eb2238de6d23fd5ef72ce07477a9c6f0a08d16\r\nSuccessfully built llava\r\nInstalling collected packages: sentencepiece, pytz, pydub, mpmath, websockets, urllib3, tzdata, typing-extensions, tomlkit, threadpoolctl, sympy, svgwrite, sniffio, six, shortuuid, shellingham, semantic-version, ruff, rpds-py, pyyaml, python-multipart, pyparsing, pygments, psutil, pillow, packaging, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, narwhals, mdurl, markupsafe, markdown2, latex2mathml, kiwisolver, joblib, importlib-resources, idna, h11, fonttools, filelock, ffmpy, exceptiongroup, einops, cycler, click, charset-normalizer, certifi, attrs, annotated-types, aiofiles, wavedrom, uvicorn, triton, scipy, requests, referencing, python-dateutil, pydantic-core, nvidia-cusparse-cu12, nvidia-cudnn-cu12, markdown-it-py, jinja2, einops-exts, contourpy, anyio, starlette, scikit-learn, rich, pydantic, pandas, nvidia-cusolver-cu12, matplotlib, jsonschema-specifications, httpcore, typer, torch, tokenizers, jsonschema, httpx, fastapi, transformers, torchvision, gradio_client, bitsandbytes, altair, accelerate, timm, peft, gradio, llava\r\n  Attempting uninstall: tokenizers\r\n    Found existing installation: tokenizers 0.20.0\r\nerror: uninstall-no-record-file\r\n\r\n× Cannot uninstall tokenizers 0.20.0\r\n╰─> The package's contents are unknown: no RECORD file was found for tokenizers.\r\n\r\nhint: You might be able to recover from this via: pip install --force-reinstall --no-deps tokenizers==0.20.0\r\n```\r\nI'm sure that the code is the latest, and when I import llava or tokenizers, here is a Segmentation fault.\r\n```\r\n$ python\r\nPython 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import llava\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'llava'\r\n>>> import tokenizers\r\nSegmentation fault (core dumped)\r\n```\r\nI tried to re-install, or the hint \"pip install --force-reinstall --no-deps tokenizers==0.20.0\", they all not work.\r\nFinally, I change the python version to 3.11, it is installed correctly. The command is:\r\n```\r\nconda create -n llava python=3.11 -y\r\nconda activate llava\r\npip install --upgrade pip  # enable PEP 660 support\r\npip install -e .\r\n```</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1819,
    "state": "open",
    "created_by": "victorcaquilpan",
    "created_at": "2025-01-06T02:01:26Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1819</URL>\n\n<TITLE>[Question] Differences between forward and generate methods</TITLE>\n\n<BODY>### Question\n\nI have been struggling to understand the differences between these two methods. I hope someone can help to clarify some of the next questions:\r\n\r\n1. Checking the documentation of LLava, we use forward for training and generate for inference, however, in some cases I have seen that people use forward for validation. This is right?\r\n2. It mentioned that generate method is for autoregressive generation. Forward doesn't follow a autoregressive generation? If that is true, what is the practical difference between both methods in this sense? In theory, if I take a finetuned model, and I run an inference for an input prompt with just the user question, using the forward and generate methods I can get different results? why?\r\n3. During training, using the forward method the input considers both the question and the answer. Does the model use part of the answer for predicting the tokens? or the model just use the answer to calculate the loss function?\r\n4. In my case, I am trying to use the hidden states of the last layer as input for a subsequent process, however, I have noticed that even though I can get the same output in forward than in generate methods, the hidden states not necessarily are similar, that is right? \r\n\r\nThanks</BODY>\n\n<COMMENTS>\n<Comment by YingquanChen at 2025-02-03T08:55:11Z>\n你好：\n  首先，我将用中文来对你上述提到的部分问题，做一个理解。\n1.你提到的有些人用forward来进行验证评估，我也有发现。按照原先的逻辑应该是训练和验证分开。但是这个逻辑是否绝对，我还在考虑当中。因为，现在有一个问题是，我发现，用generate来验证会有一个错误，即生成的隐藏层数目会有时出错，如果你是需要从4096个隐藏层信息中做下一步操作，就会中断。当然，你如果只要最终的3万多个的完整信息，这不会有问题。\n2.顺着上面的思路，我发现了用generate生成的隐藏层有时数量会出错，导致中断。这就使得用generate来验证就不行了。这恐怕是transformer库中generate函数的问题。这就使得想要修正难度很大。这恐怕是其他人用forward来验证的原因。这是我猜测的，或许还有其他原因。\n3.最后，关于forward和generate的区别，你的理解我大部分是认可的。究竟forward有没有利用一部分所给的答案来进行输出？这种方法来验证评估是否是重大错误？\n4.希望，你有任何更新的疑问和问题，乃至于答案，能够与我一起分享。\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1818,
    "state": "closed",
    "created_by": "whyisverysmart",
    "created_at": "2025-01-03T00:35:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1818</URL>\n\n<TITLE>[Usage] Failed to start stage 1 training of LLaVA-v1.5-7b</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nHi! When I try to pretrain LLaVA-v1.5-7b on 4 ada6000, the program will stuck after outputting `Formatting inputs...Skip in lazy mode`. I have tried to set --dataloader_num_workers=1/2 but the problem still exists. Where did I go wrong...\r\n\r\nThe pretrain-dataset contains: blip_laion_cc_sbu_558k.json, and all images: images/00000~00659/xxx.jpg\r\n\r\nCommand:\r\n```\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --model_name_or_path liuhaotian/llava-v1.5-7b \\\r\n    --version plain \\\r\n    --data_path ../pretrain-dataset/blip_laion_cc_sbu_558k.json \\\r\n    --image_folder ../pretrain-dataset/images \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ../checkpoints/llava-v1.5-7b-64-pretrain \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 4 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 24000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 1e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 2 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nOutput: \r\n```\r\n[2025-01-03 08:18:40,657] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2025-01-03 08:18:43,461] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\nDetected CUDA_VISIBLE_DEVICES=0,1,2,3: setting --include=localhost:0,1,2,3\r\n[2025-01-03 08:18:43,461] [INFO] [runner.py:571:main] cmd = masked\r\n[2025-01-03 08:18:45,679] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2025-01-03 08:18:47,563] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\r\n[2025-01-03 08:18:47,563] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0\r\n[2025-01-03 08:18:47,563] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\r\n[2025-01-03 08:18:47,563] [INFO] [launch.py:163:main] dist_world_size=4\r\n[2025-01-03 08:18:47,563] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\r\n[2025-01-03 08:18:51,810] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2025-01-03 08:18:51,814] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2025-01-03 08:18:51,824] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2025-01-03 08:18:51,870] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2025-01-03 08:18:54,208] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2025-01-03 08:18:54,208] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2025-01-03 08:18:54,208] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n[2025-01-03 08:18:54,224] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2025-01-03 08:18:54,224] [INFO] [comm.py:637:init_distributed] cdb=None\r\nFormatting inputs...Skip in lazy mode\r\n```\r\n\r\nLog:\r\n```\r\n/home/hywang/anaconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\n/home/hywang/anaconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\n/home/hywang/anaconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\n/home/hywang/anaconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\n\r\nLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/hywang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\n\r\nLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/hywang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\n\r\nLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/hywang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\n\r\nLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/hywang/anaconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\n\r\nLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.16s/it]\r\nLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.36s/it]\r\nLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.15s/it]\r\nLoading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.25s/it]\r\nLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.39s/it]\r\nLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.95s/it]\r\n\r\nLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.47s/it]\r\nLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.06s/it]\r\n\r\nLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.32s/it]\r\nLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.90s/it]\r\n\r\nLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.38s/it]\r\nLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.11s/it]\r\n```</BODY>\n\n<COMMENTS>\n<Comment by zxdscsfm at 2025-02-27T13:09:39Z>\nI meet the same question,can you resolve it?```\nYou are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n\nDownloading shards: 0%| | 0/2 [00:00<?, ?it/s]You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n\nDownloading shards: 0%| | 0/2 [00:00<?, ?it/s]You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n\nDownloading shards: 0%| | 0/2 [00:00<?, ?it/s]You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n\nDownloading shards: 0%| | 0/2 [00:00<?, ?it/s]You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n\nDownloading shards: 0%| | 0/2 [00:00<?, ?it/s]You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n\nDownloading shards: 0%| | 0/2 [00:00<?, ?it/s]\n```\n</Comment>\n<Comment by whyisverysmart at 2025-02-28T01:59:12Z>\n> I meet the same question,can you resolve it?``` You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n> \n> Downloading shards: 0%| | 0/2 [00:00<?, ?it/s]You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n> \n> Downloading shards: 0%| | 0/2 [00:00<?, ?it/s]You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n> \n> Downloading shards: 0%| | 0/2 [00:00<?, ?it/s]You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n> \n> Downloading shards: 0%| | 0/2 [00:00<?, ?it/s]You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n> \n> Downloading shards: 0%| | 0/2 [00:00<?, ?it/s]You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n> \n> Downloading shards: 0%| | 0/2 [00:00<?, ?it/s]\n\nIt turns out to be the GPU problem. By disable NCCL_IB and NCCL_P2P, my problem is solved. But the problem you met seems to be different from mine, more like a network issue?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1817,
    "state": "open",
    "created_by": "bryanlinnan",
    "created_at": "2025-01-02T08:33:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1817</URL>\n\n<TITLE>[Usage]  Error in loading mistral 7b weights</TITLE>\n\n<BODY>### Describe the issue\n\n\r\nHi,\r\ni tried to load llava 7b mistral weights into model, got error:ValueError: Trying to set a tensor of shape torch.Size([577, 1024]) in \"weight\" (which has shape torch.Size([257, 1024])), this looks incorrect\r\n\r\nthe error occurs in the below code:\r\nmodel = LlavaMistralForCausalLM.from_pretrained(\r\nmodel_path,\r\nlow_cpu_mem_usage=True,\r\n**kwargs,\r\nsliding_window=4096\r\n)\r\nand the model_path is a local path since i have downloaded the pretrained weights from huggingface\r\n\r\nAny idea?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1816,
    "state": "open",
    "created_by": "ginamdar",
    "created_at": "2025-01-01T22:27:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1816</URL>\n\n<TITLE>[Discussion] Newbie alert - asked below question llava:13b about guitar scales and its giving wrong information.</TITLE>\n\n<BODY>### Discussion\n\nIm running the model using ollama and just asked below question, see its response for A Major scale. There supposed to be 3 sharps\r\n```\r\n>>> how many major scales  are there in guitar\r\n In guitar music theory, a major scale is constructed using the formula of whole steps (W) and half steps (H): W-W-W-W-W-W-H. There are\r\n7 natural major scales in Western music theory, which can be used as the basis for building chords on the guitar. These scales are:\r\n\r\n1. C major scale: C-D-E-F-G-A-B-C\r\n2. G major scale: G-A-B-C-D-E-F#-G\r\n3. D major scale: D-E-F#-G-A-B-C#-D\r\n4. A major scale: A-B-C#-D-E-F#-A\r\n5. E major scale: E-F#-G#-A-B-C#-E\r\n6. B major scale: B-C#-D#-E-F#-G#-A#-B\r\n7. F major scale: F-G-A-Bb-C-D-E-F\r\n\r\nEach of these scales has a unique set of notes and can be used to build chords on the guitar.\r\n\r\n>>> A has 3 sharps you are giving wrong answer for A\r\n I apologize for the mistake in my previous response. You are correct that the A major scale has three sharps (C#, E#, and G#). The\r\ncorrect spelling of the A major scale is:\r\nA - B - C# - D - E - F# - G# - A\r\n\r\nThank you for bringing this to my attention.\r\n```\r\nNot sure if we even report this but i had not known answer i might just assumed its true !\r\n\r\nthanks</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1815,
    "state": "closed",
    "created_by": "924973292",
    "created_at": "2024-12-27T07:41:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1815</URL>\n\n<TITLE>Can llava-v1.5-7b accept the images smaller than 336? without resize, just direct go into the network?</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1814,
    "state": "open",
    "created_by": "rickeyhhh",
    "created_at": "2024-12-23T12:56:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1814</URL>\n\n<TITLE>[Question] How do I set the image input to a null value?</TITLE>\n\n<BODY>### Question\r\n\r\nHi!! \r\nI want to do some experiments with Llava. But I don't know how to set the image input to a null value.\r\n`args = type('Args', (), {\r\n    \"model_path\": model_path,\r\n    \"model_base\": None,\r\n    \"model_name\": get_model_name_from_path(model_path),\r\n    \"query\": prompt,\r\n    \"conv_mode\": None,\r\n    \"image_file\": image_file,\r\n    \"sep\": \",\",\r\n    \"temperature\": 0,\r\n    \"top_p\": None,\r\n    \"num_beams\": 1,\r\n    \"max_new_tokens\": 512\r\n})()`\r\nIf I set the \"image_file\" to None, the model will give me a error: \r\n**_AttributeError: 'NoneType' object has no attribute 'split'_**</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1812,
    "state": "open",
    "created_by": "LiuChang-ao",
    "created_at": "2024-12-17T07:04:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1812</URL>\n\n<TITLE>[Usage] Cannot Launch a gradio web server</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI launched a controller successfully but cannot launch a gradio server. It has been stuck for a long time and cannot be interrupted by the \"Ctrl-C\" command.\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.controller --host 0.0.0.0 --port 10000\r\npython -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload\r\n\r\n```\r\n\r\nLog: \r\n```\r\n(llava) PS D:\\_______\\LLaVA> python -m llava.serve.controller --host 0.0.0.0 --port 10000\r\nThe installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\r\n2024-12-17 14:59:03 | INFO | controller | args: Namespace(host='0.0.0.0', port=10000, dispatch_method='shortest_queue')\r\n2024-12-17 14:59:03 | INFO | controller | Init controller\r\n2024-12-17 14:59:03 | ERROR | stderr | INFO:     Started server process [5336]\r\n2024-12-17 14:59:03 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2024-12-17 14:59:03 | ERROR | stderr | INFO:     Application startup complete.\r\n2024-12-17 14:59:03 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:10000 (Press CTRL+C to quit)\r\n\r\n# another terminal\r\n(llava) PS D:\\________\\LLaVA> python -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload\r\nThe installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\r\n2024-12-17 14:59:18 | INFO | gradio_web_server | args: Namespace(host='0.0.0.0', port=None, controller_url='http://localhost:10000', concurrency_count=16, model_list_mode='reload', share=False, moderate=False, embed=False)\r\n\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by gokulnandan-0 at 2025-01-27T10:58:52Z>\ntry uninstalling and reinstalling bitsandbytes. I had this issue while using bitsandbytes==0.41, updating solved the issue\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1811,
    "state": "open",
    "created_by": "zhangye0402",
    "created_at": "2024-12-17T04:46:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1811</URL>\n\n<TITLE>使用finetune_task_lora.sh微调llava_1.5模型无法运行</TITLE>\n\n<BODY><img width=\"995\" alt=\"image\" src=\"https://github.com/user-attachments/assets/db62243b-2303-496f-b0b6-d8b3686c363d\" />\r\n\r\n报错信息如图，问题：第一轮训练显示loss为0后直接结束训练 我的环境如下：\r\n<img width=\"173\" alt=\"image\" src=\"https://github.com/user-attachments/assets/d6c5c70e-762e-4c2f-a505-15ab63ad2463\" />\r\n<img width=\"170\" alt=\"image\" src=\"https://github.com/user-attachments/assets/cbe93209-2566-42c2-a909-0d8c5adc985d\" />\r\n<img width=\"155\" alt=\"image\" src=\"https://github.com/user-attachments/assets/976daa4f-067c-41b2-a518-78e96cb037ec\" />\r\n\r\n训练脚本如下：\r\n<img width=\"443\" alt=\"image\" src=\"https://github.com/user-attachments/assets/931e354e-328e-4e81-8fdb-01813ffdad9d\" />\r\n\r\n我曾经用这个脚本正确运行过训练程序，但是在重新配置环境后出现了这样的错误，请问是哪里出错了呢，谢谢大家！</BODY>\n\n<COMMENTS>\n<Comment by MarkDeng1 at 2025-01-02T00:11:34Z>\n我的微调 从HF下载的模型总是报错，说结构不匹配。。\r\nTraceback (most recent call last):\r\n  File \"/fred/oz337/zdeng/prompt_stealing_ours/LLaVA/llava/train/train.py\", line 1006, in <module>\r\n    train()\r\n  File \"/fred/oz337/zdeng/prompt_stealing_ours/LLaVA/llava/train/train.py\", line 834, in train\r\n    model = LlavaMptForCausalLM.from_pretrained(\r\n  File \"/fred/oz337/zdeng/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3850, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\n  File \"/fred/oz337/zdeng/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 4146, in _load_pretrained_model\r\n    raise ValueError(\r\nValueError: The state dictionary of the model you are trying to load is corrupted. Are you sure it was properly saved?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1810,
    "state": "closed",
    "created_by": "zhangye0402",
    "created_at": "2024-12-17T04:40:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1810</URL>\n\n<TITLE>使用finetune_task_lora.sh微调llava_1.5模型无法运行</TITLE>\n\n<BODY><img width=\"994\" alt=\"image\" src=\"https://github.com/user-attachments/assets/52bc0fbd-345f-4eaa-ba51-f5c5804169fd\" />\r\n报错信息如图，问题：第一轮训练显示loss为0后直接结束训练\r\n我的环境如下：\r\n_libgcc_mutex             0.1                        main    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\n_openmp_mutex             5.1                       1_gnu    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\naccelerate                0.21.0                   pypi_0    pypi\r\naiofiles                  24.1.0                   pypi_0    pypi\r\naiohappyeyeballs          2.4.4                    pypi_0    pypi\r\naiohttp                   3.11.10                  pypi_0    pypi\r\naiosignal                 1.3.2                    pypi_0    pypi\r\naltair                    5.5.0                    pypi_0    pypi\r\nanyio                     4.7.0                    pypi_0    pypi\r\nasync-timeout             5.0.1                    pypi_0    pypi\r\nattrs                     24.3.0                   pypi_0    pypi\r\nbitsandbytes              0.41.0                   pypi_0    pypi\r\nbzip2                     1.0.8                h5eee18b_6    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nca-certificates           2024.11.26           h06a4308_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\ncertifi                   2024.12.14               pypi_0    pypi\r\ncharset-normalizer        3.4.0                    pypi_0    pypi\r\nclick                     8.1.7                    pypi_0    pypi\r\ncmake                     3.31.2                   pypi_0    pypi\r\ncontourpy                 1.3.1                    pypi_0    pypi\r\ncycler                    0.12.1                   pypi_0    pypi\r\ndeepspeed                 0.9.5                    pypi_0    pypi\r\ndocker-pycreds            0.4.0                    pypi_0    pypi\r\neinops                    0.6.1                    pypi_0    pypi\r\neinops-exts               0.0.4                    pypi_0    pypi\r\nexceptiongroup            1.2.2                    pypi_0    pypi\r\nfastapi                   0.115.6                  pypi_0    pypi\r\nffmpy                     0.4.0                    pypi_0    pypi\r\nfilelock                  3.16.1                   pypi_0    pypi\r\nflash-attn                2.1.1                    pypi_0    pypi\r\nfonttools                 4.55.3                   pypi_0    pypi\r\nfrozenlist                1.5.0                    pypi_0    pypi\r\nfsspec                    2024.10.0                pypi_0    pypi\r\ngitdb                     4.0.11                   pypi_0    pypi\r\ngitpython                 3.1.43                   pypi_0    pypi\r\ngradio                    3.35.2                   pypi_0    pypi\r\ngradio-client             0.2.9                    pypi_0    pypi\r\nh11                       0.14.0                   pypi_0    pypi\r\nhjson                     3.1.0                    pypi_0    pypi\r\nhttpcore                  0.17.3                   pypi_0    pypi\r\nhttpx                     0.24.0                   pypi_0    pypi\r\nhuggingface-hub           0.27.0                   pypi_0    pypi\r\nidna                      3.10                     pypi_0    pypi\r\njinja2                    3.1.4                    pypi_0    pypi\r\njoblib                    1.4.2                    pypi_0    pypi\r\njsonschema                4.23.0                   pypi_0    pypi\r\njsonschema-specifications 2024.10.1                pypi_0    pypi\r\nkiwisolver                1.4.7                    pypi_0    pypi\r\nlatex2mathml              3.77.0                   pypi_0    pypi\r\nld_impl_linux-64          2.40                 h12ee557_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nlibffi                    3.4.4                h6a678d5_1    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nlibgcc-ng                 11.2.0               h1234567_1    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nlibgomp                   11.2.0               h1234567_1    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nlibstdcxx-ng              11.2.0               h1234567_1    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nlibuuid                   1.41.5               h5eee18b_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nlinkify-it-py             2.0.3                    pypi_0    pypi\r\nlit                       18.1.8                   pypi_0    pypi\r\nllava                     1.1.3                    pypi_0    pypi\r\nmarkdown-it-py            2.2.0                    pypi_0    pypi\r\nmarkdown2                 2.5.2                    pypi_0    pypi\r\nmarkupsafe                3.0.2                    pypi_0    pypi\r\nmatplotlib                3.10.0                   pypi_0    pypi\r\nmdit-py-plugins           0.3.3                    pypi_0    pypi\r\nmdurl                     0.1.2                    pypi_0    pypi\r\nmpmath                    1.3.0                    pypi_0    pypi\r\nmultidict                 6.1.0                    pypi_0    pypi\r\nnarwhals                  1.18.4                   pypi_0    pypi\r\nncurses                   6.4                  h6a678d5_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nnetworkx                  3.4.2                    pypi_0    pypi\r\nninja                     1.11.1.3                 pypi_0    pypi\r\nnumpy                     1.24.2                   pypi_0    pypi\r\nnvidia-cublas-cu11        11.10.3.66               pypi_0    pypi\r\nnvidia-cuda-cupti-cu11    11.7.101                 pypi_0    pypi\r\nnvidia-cuda-nvrtc-cu11    11.7.99                  pypi_0    pypi\r\nnvidia-cuda-runtime-cu11  11.7.99                  pypi_0    pypi\r\nnvidia-cudnn-cu11         8.5.0.96                 pypi_0    pypi\r\nnvidia-cufft-cu11         10.9.0.58                pypi_0    pypi\r\nnvidia-curand-cu11        10.2.10.91               pypi_0    pypi\r\nnvidia-cusolver-cu11      11.4.0.1                 pypi_0    pypi\r\nnvidia-cusparse-cu11      11.7.4.91                pypi_0    pypi\r\nnvidia-nccl-cu11          2.14.3                   pypi_0    pypi\r\nnvidia-nvtx-cu11          11.7.91                  pypi_0    pypi\r\nopenssl                   3.0.15               h5eee18b_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\norjson                    3.10.12                  pypi_0    pypi\r\npackaging                 24.2                     pypi_0    pypi\r\npandas                    2.2.3                    pypi_0    pypi\r\npeft                      0.4.0                    pypi_0    pypi\r\npillow                    11.0.0                   pypi_0    pypi\r\npip                       24.3.1                   pypi_0    pypi\r\nplatformdirs              4.3.6                    pypi_0    pypi\r\npropcache                 0.2.1                    pypi_0    pypi\r\nprotobuf                  5.29.1                   pypi_0    pypi\r\npsutil                    6.1.0                    pypi_0    pypi\r\npy-cpuinfo                9.0.0                    pypi_0    pypi\r\npydantic                  1.10.19                  pypi_0    pypi\r\npydub                     0.25.1                   pypi_0    pypi\r\npygments                  2.18.0                   pypi_0    pypi\r\npyparsing                 3.2.0                    pypi_0    pypi\r\npython                    3.10.16              he870216_1    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\npython-dateutil           2.9.0.post0              pypi_0    pypi\r\npython-multipart          0.0.19                   pypi_0    pypi\r\npytz                      2024.2                   pypi_0    pypi\r\npyyaml                    6.0.2                    pypi_0    pypi\r\nreadline                  8.2                  h5eee18b_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nreferencing               0.35.1                   pypi_0    pypi\r\nregex                     2024.11.6                pypi_0    pypi\r\nrequests                  2.32.3                   pypi_0    pypi\r\nrpds-py                   0.22.3                   pypi_0    pypi\r\nsafetensors               0.4.5                    pypi_0    pypi\r\nscikit-learn              1.2.2                    pypi_0    pypi\r\nscipy                     1.14.1                   pypi_0    pypi\r\nsemantic-version          2.10.0                   pypi_0    pypi\r\nsentencepiece             0.1.99                   pypi_0    pypi\r\nsentry-sdk                2.19.2                   pypi_0    pypi\r\nsetproctitle              1.3.4                    pypi_0    pypi\r\nsetuptools                75.1.0          py310h06a4308_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nshortuuid                 1.0.13                   pypi_0    pypi\r\nsix                       1.17.0                   pypi_0    pypi\r\nsmmap                     5.0.1                    pypi_0    pypi\r\nsniffio                   1.3.1                    pypi_0    pypi\r\nsqlite                    3.45.3               h5eee18b_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nstarlette                 0.41.3                   pypi_0    pypi\r\nsvgwrite                  1.4.3                    pypi_0    pypi\r\nsympy                     1.13.3                   pypi_0    pypi\r\nthreadpoolctl             3.5.0                    pypi_0    pypi\r\ntimm                      0.6.13                   pypi_0    pypi\r\ntk                        8.6.14               h39e8969_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\ntokenizers                0.13.3                   pypi_0    pypi\r\ntorch                     2.0.1                    pypi_0    pypi\r\ntorchvision               0.15.2                   pypi_0    pypi\r\ntqdm                      4.67.1                   pypi_0    pypi\r\ntransformers              4.31.0                   pypi_0    pypi\r\ntriton                    2.0.0                    pypi_0    pypi\r\ntyping-extensions         4.12.2                   pypi_0    pypi\r\ntzdata                    2024.2                   pypi_0    pypi\r\nuc-micro-py               1.0.3                    pypi_0    pypi\r\nurllib3                   2.2.3                    pypi_0    pypi\r\nuvicorn                   0.34.0                   pypi_0    pypi\r\nwandb                     0.18.7                   pypi_0    pypi\r\nwavedrom                  2.0.3.post3              pypi_0    pypi\r\nwebsockets                14.1                     pypi_0    pypi\r\nwheel                     0.44.0          py310h06a4308_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nxz                        5.4.6                h5eee18b_1    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nyarl                      1.18.3                   pypi_0    pypi\r\nzlib                      1.2.13               h5eee18b_1    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\n训练脚本如下：\r\n<img width=\"440\" alt=\"image\" src=\"https://github.com/user-attachments/assets/dc0ccf31-8be7-4b47-b918-ead3dce731e0\" />\r\n我曾经用这个脚本正确运行过训练程序，但是在重新配置环境后出现了这样的错误，请问是哪里出错了呢，谢谢大家！</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1809,
    "state": "closed",
    "created_by": "Liumx2020",
    "created_at": "2024-12-15T11:54:39Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1809</URL>\n\n<TITLE>[Usage] Issue with Sampling and Beam Search in generate for LLaVA Models</TITLE>\n\n<BODY></BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1808,
    "state": "closed",
    "created_by": "EEElisa",
    "created_at": "2024-12-13T15:50:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1808</URL>\n\n<TITLE>[Usage] RuntimeError: Failed to import transformers.trainer</TITLE>\n\n<BODY>### Describe the issue\n\nI run the following script for fine-tuning:\r\n```\r\nsh scripts/v1_5/finetune_task_lora.sh\r\n```\r\n\r\nHere's the error message I got:\r\n```\r\nRuntimeError: Failed to import transformers.trainer because of the following error (look up to see its traceback):\r\ncannot import name 'EncoderDecoderCache' from 'transformers' (/usr/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/__init__.py)\r\n```\r\n\r\nIt looks like there's a mismatch between transformers and other libraries.</BODY>\n\n<COMMENTS>\n<Comment by zhangye0402 at 2024-12-16T14:19:04Z>\nfacing the same problem, did you solve it?\r\nthank you!\n</Comment>\n<Comment by TuuSiwei at 2024-12-20T11:16:27Z>\npip install peft==0.10.0 can solve the problem，my transformers version is 4.42.0\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1796,
    "state": "open",
    "created_by": "enkaranfiles",
    "created_at": "2024-12-11T05:38:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1796</URL>\n\n<TITLE>[Question] Abnormal Generation after finetuning ONLY single label classification dataset</TITLE>\n\n<BODY>### Question\r\n\r\nI am trying to change the model architecture,\r\n\r\nModel exhibits abnormal generation behavior, such as repetitive or nonsensical outputs, despite successful pretraining performance.\r\n\r\nDuring pretraining, the model performed well on scene description tasks and demonstrated strong zero-shot classification capabilities, indicating effective alignment between the visual encoder and the LLM. \r\n\r\nExample generation after pretraining:\r\n\r\n- I've identified the one applicable category for this image. The category is: Arable land.\r\n\r\nAt that stage, model pretrained only scene description and do not know anything about the scenes class names. \r\n\r\nHowever, after fine-tuning on the classification dataset, the model fails to produce meaningful outputs.\r\n\r\nExample generation of the after finetuning on single label classification dataset:\r\n\r\n-xResSeawayResSeaidentialSeaidentialSeaSeaSeawaySeaSeaSeawaySeaSeawayResSeaSeaSeaidentialSeaSeaSeaSeaSeaSeaidentialSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSeaSea\r\n\r\nSo in the high-level, I am trying change vision encoder for processing different domain input image (dataset curated by me), and in the finetuning stage I am using single label classification dataset. (followed LLaVA conservation template)\r\n\r\nI think getting good result after pretraining stage indicates, we are getting succesful signal after pretraining, means nothing wrong with vision_tower section. I am thinking possible reason for this issue: Pretrained task and finetuning task mis-alignment. \r\n\r\nShould I increase the diffuculty level of the instruction tuning dataset?\r\n\r\nPS: There were some other issues related to abnormal and repetetive generation, so I checked the image broker and dataset folder. They were all fine.\r\n\r\nPS: Loss function has gradually decrease over the steps. \r\n\r\nPS: I have tried to model overfitting on the given train set by decreasing sample size(trained only 3 percent of the finetuning data) and increasing number of epoch. But still cannot perform well on train set.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1795,
    "state": "open",
    "created_by": "DrVictorBenjamin",
    "created_at": "2024-12-11T04:21:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1795</URL>\n\n<TITLE>[Question] How do you fine-tune LLaVA-NeXT on video data?</TITLE>\n\n<BODY>### Question\n\nI have a collection of videos and annotations. How do I fine-tune one of the LLaVA-NeXT models? I see the instructions for how to do so with traditional LLaVA but the directions for LLaVA-NeXT with video data are unclear. Thank you very much</BODY>\n\n<COMMENTS>\n<Comment by DrVictorBenjamin at 2024-12-11T05:21:18Z>\nAy after spending some time digging around, I came across this tutorial in case anyone else is searching for an answer: https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LLaVA-NeXT-Video/Fine_tune_LLaVa_NeXT_Video_with_HFTrainer.ipynb\r\n\r\nI haven't tried it yet but I will\n</Comment>\n<Comment by anjaligupta1104 at 2025-01-28T04:04:34Z>\nDid you try it and have any success? I'm also curious about how this applies to LLaVA-Video and any documentation you found about the data format.\n</Comment>\n<Comment by DrVictorBenjamin at 2025-01-28T04:07:28Z>\nI didn't try the guide yet, got distracted with another project. I may try it in the next week. If you get to it first, let me know!\n</Comment>\n<Comment by anjaligupta1104 at 2025-01-28T19:52:38Z>\nSure thing!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1794,
    "state": "open",
    "created_by": "shtu-ryan",
    "created_at": "2024-12-11T03:08:55Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1794</URL>\n\n<TITLE>[Usage] peft 1.40.0 requires transformers at least 0.47</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nAs title. Better set peft==0.13.2\r\n\r\nCommand:\r\n```\r\nimport peft\r\n```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home_data/home/geyx2023/.conda/envs/llava/lib/python3.10/site-packages/peft/__init__.py\", line 22, in <module>\r\n    from .auto import (\r\n  File \"/home_data/home/geyx2023/.conda/envs/llava/lib/python3.10/site-packages/peft/auto.py\", line 32, in <module>\r\n    from .mapping import MODEL_TYPE_TO_PEFT_MODEL_MAPPING\r\n  File \"/home_data/home/geyx2023/.conda/envs/llava/lib/python3.10/site-packages/peft/mapping.py\", line 25, in <module>\r\n    from .mixed_model import PeftMixedModel\r\n  File \"/home_data/home/geyx2023/.conda/envs/llava/lib/python3.10/site-packages/peft/mixed_model.py\", line 29, in <module>\r\n    from .peft_model import PeftModel\r\n  File \"/home_data/home/geyx2023/.conda/envs/llava/lib/python3.10/site-packages/peft/peft_model.py\", line 37, in <module>\r\n    from transformers import Cache, DynamicCache, EncoderDecoderCache, PreTrainedModel\r\nImportError: cannot import name 'EncoderDecoderCache' from 'transformers' (/home_data/home/geyx2023/.conda/envs/llava/lib/python3.10/site-packages/transformers/__init__.py)\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1792,
    "state": "open",
    "created_by": "MSungK",
    "created_at": "2024-12-09T15:28:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1792</URL>\n\n<TITLE>[Question] Why does text-only data use the empty image token?</TITLE>\n\n<BODY>### Question\n\nText-only data is implemented in such a way that learning proceeds with visual tokens set to empty. In my opinion, since the length of visual tokens is quite significant, it seems more efficient not to use meaningless visual tokens for text-only data. Moreover, since a sampler that samples data from the same modality is already implemented, I am even more puzzled.\r\nIs there a specific reason for this?</BODY>\n\n<COMMENTS>\n<Comment by raja-7-c at 2025-01-17T10:52:25Z>\nYes, indeed, for text-only data, they set the visual input to zero values. \n\n        # image exist in the data\n        if 'image' in self.list_data_dict[i]:\n            data_dict['image'] = image\n        elif self.data_args.is_multimodal:\n            # image does not exist in the data, but the model is multimodal\n            crop_size = self.data_args.image_processor.crop_size\n            data_dict['image'] = torch.zeros(3, crop_size['height'], crop_size['width'])\n        return data_dict\n\nThis leads the vision encoder to produce some embeddings, influenced by its bias term. These embeddings, comprising 576 visual tokens, are appended to the text input for every text-only sample. The combined tokens are then used as a condition to autoregressively predict the next text token.\n\nIt's not entirely clear why this approach should help. One hypothesis might be that incorporating these visual embeddings—even if they carry no meaningful information—could regularize the model by maintaining consistent input sizes across modalities. Since a modality-specific sampler already exists, this approach does seem counterintuitive without a more detailed justification.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1791,
    "state": "open",
    "created_by": "Rachel0901",
    "created_at": "2024-12-07T22:01:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1791</URL>\n\n<TITLE>Issue with 4-bit Quantization for LLaVA-NeXT-Video-32B Model on A100-40GB GPU</TITLE>\n\n<BODY>### Describe the issue\n\nHello, I am trying to run the lmms-lab/LLaVA-NeXT-Video-32B-Qwen model on an A100-40GB GPU. However, I encounter an OOM issue when loading the model in its default configuration. To address this, I attempted to enable 4-bit quantization using the bitsandbytes library by modifying my script as follows:\r\n```\r\npretrained = \"lmms-lab/LLaVA-NeXT-Video-32B-Qwen\"\r\nmodel_name = \"llava_qwen\"\r\ndevice_map = \"auto\"\r\n\r\n# Load the model with proper configuration\r\ntokenizer, model, image_processor, max_length = load_pretrained_model(\r\n    pretrained,\r\n    None,\r\n    model_name,\r\n    load_in_8bit=False,  # Ensure 8-bit quantization is disabled\r\n    load_in_4bit=True    # Enable 4-bit quantization\r\n)\r\nmodel.eval()\r\n```\r\nHowever, when I run the script, I encounter the following error message:\r\n\r\n> ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. \r\n> Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.\r\n\r\nCould you clarify how to properly enable 4-bit quantization for the lmms-lab/LLaVA-NeXT-Video-32B-Qwen model in Python scripts?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1790,
    "state": "open",
    "created_by": "TanmouTT",
    "created_at": "2024-12-05T06:23:26Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1790</URL>\n\n<TITLE>[Question] When I reproduced the second stage using finetune.sh, the processing time per image was too slow</TITLE>\n\n<BODY>### Question\n\nI run finetune.sh on 8xA100(40G),it takes about 33 seconds per image.\r\nMaybe the picture download is not complete, I skipped the missing picture, I think this may not be the reason why the training is so slow\r\nhere is the finetune.sh I use：\r\n#!/bin/bash\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path /home/24-zhangtan/LLaVA/vicuna-7b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path /home/24-zhangtan/LLaVA/playground/data/llava_v1_5_mix665k.json \\\r\n    --image_folder /home/24-zhangtan/LLaVA/playground/data \\\r\n    --vision_tower /home/24-zhangtan/LLaVA/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter /home/24-zhangtan/LLaVA/llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n\r\n![训练时间](https://github.com/user-attachments/assets/1de921f9-30cf-4e0a-a686-1dabf0cd0e8f)\r\n\r\nHas anyone had this problem? How did you solve it? Looking forward to receiving reply！</BODY>\n\n<COMMENTS>\n<Comment by quanyouyou at 2024-12-05T13:48:13Z>\nSo do i, and my : --per_device_train_batch_size 12\r\nand it will oom when the bs is set to 16 ,8*A100(40G)\r\n\r\nso puzzled😵‍💫\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1789,
    "state": "open",
    "created_by": "ethanyys",
    "created_at": "2024-12-04T06:19:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1789</URL>\n\n<TITLE>[Question] Fintune llava-v1.5-7b just use all textvqa data, but get Accuracy: 0.00%</TITLE>\n\n<BODY>### Question\n\nFintune llava-v1.5-7b just use all textvqa data, but get Accuracy: 0.00%\r\n\r\nTrained on 8*A800(80G) with BF16\r\n\r\nEvaluated follow docs: CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/eval/textvqa.sh\r\n\r\nBy reviewing eval jsonl file, we can see finetuned llava-v1.5-7b refuse to follow the prompt \"Answer the question using a single word or phrase.\"\r\n\r\n{\"question_id\": \"fb39f5b120097669\", \"prompt\": \"What year was this taken?\\nReference OCR token: yip, AE, Mht, juerYU, MhnN, 02/14/2012\\nAnswer the question using a single word or phrase.\", \"text\": \"A bookshelf is full of movies such as The Powerpuff Girls and AE.\", \"answer_id\": \"ejxFDG2X62UneJRnGALrMt\", \"model_id\": \"llava-v1.5-7b\", \"metadata\": {}}\r\n{\"question_id\": \"a3f1fe4ebd510d4e\", \"prompt\": \"What kind of comupter is this?\\nReference OCR token: Select, 0o-Back, Contique, MacBook\\nAnswer the question using a single word or phrase.\", \"text\": \"A MacBook computer shows a screen with a Continue button.\", \"answer_id\": \"FcrDZNThYoejXViQUc45H8\", \"model_id\": \"llava-v1.5-7b\", \"metadata\": {}}\r\n{\"question_id\": \"a3f1fe4ebd510d4e\", \"prompt\": \"What does the screen say to do?\\nReference OCR token: Select, 0o-Back, Contique, MacBook\\nAnswer the question using a single word or phrase.\", \"text\": \"A MacBook computer monitor with a open window on the screen.\", \"answer_id\": \"5o4MXbQ5sKzpLky97JhKSY\", \"model_id\": \"llava-v1.5-7b\", \"metadata\": {}}\r\n\r\n\r\n\r\nHow can I fix this problem?</BODY>\n\n<COMMENTS>\n<Comment by UCASlemontea at 2024-12-27T06:40:33Z>\ndo you fix this problem？thanks\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1788,
    "state": "closed",
    "created_by": "matsutaku44",
    "created_at": "2024-12-03T08:06:27Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1788</URL>\n\n<TITLE>[Usage] finetune_task_lora.sh -> Error \"exits with return code = -7\"</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nI ran \"finetune_task_lora.sh\", but I got an error below.\r\n\r\nCommand:\r\n```\r\nCUDA_VISIBLE_DEVICES=0 source ./scripts/v1_5/finetune_task_lora.sh\r\n```\r\n\r\nParameter:\r\n```\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path liuhaotian/llava-v1.5-7b \\\r\n    --version v1 \\\r\n    --data_path ./dataset/which_direction.json \\\r\n    --image_folder ./images \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b-task-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nLog: \r\n```\r\n[2024-12-03 08:02:37,945] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.\r\n [WARNING]  async_io: please install the libaio-dev package with apt\r\n [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\r\n [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\r\n [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\r\n [WARNING]  using untested triton version (2.1.0), only 1.0.0 is known to be compatible\r\n[2024-12-03 08:02:39,607] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\nDetected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0\r\n[2024-12-03 08:02:39,608] [INFO] [runner.py:568:main] cmd = /home/matsuzaki.takumi/.conda/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero3.json --model_name_or_path liuhaotian/llava-v1.5-7b --version v1 --data_path ./dataset/which_direction.json --image_folder ./images --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-7b-task-lora --num_train_epochs 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb\r\n[2024-12-03 08:02:42,096] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.\r\n [WARNING]  async_io: please install the libaio-dev package with apt\r\n [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\r\n [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\r\n [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\r\n [WARNING]  using untested triton version (2.1.0), only 1.0.0 is known to be compatible\r\n[2024-12-03 08:02:43,746] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.17.1-1+cuda12.1\r\n[2024-12-03 08:02:43,746] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.17.1-1\r\n[2024-12-03 08:02:43,746] [INFO] [launch.py:139:main] 0 NCCL_VERSION=2.17.1-1\r\n[2024-12-03 08:02:43,746] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\r\n[2024-12-03 08:02:43,746] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.17.1-1+cuda12.1\r\n[2024-12-03 08:02:43,746] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\r\n[2024-12-03 08:02:43,746] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.17.1-1\r\n[2024-12-03 08:02:43,746] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}\r\n[2024-12-03 08:02:43,746] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0\r\n[2024-12-03 08:02:43,746] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\r\n[2024-12-03 08:02:43,746] [INFO] [launch.py:164:main] dist_world_size=1\r\n[2024-12-03 08:02:43,747] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0\r\n[2024-12-03 08:02:43,748] [INFO] [launch.py:256:main] process 7302 spawned with command: ['/home/matsuzaki.takumi/.conda/envs/llava/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=0', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', './dataset/which_direction.json', '--image_folder', './images', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-7b-task-lora', '--num_train_epochs', '1', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb']\r\n[2024-12-03 08:02:48,109] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.\r\n [WARNING]  async_io: please install the libaio-dev package with apt\r\n [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\r\n [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\r\n [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\r\n [WARNING]  using untested triton version (2.1.0), only 1.0.0 is known to be compatible\r\n[2024-12-03 08:02:49,277] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2024-12-03 08:02:49,277] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n/home/matsuzaki.takumi/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\n[2024-12-03 08:02:53,759] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 7302\r\n[2024-12-03 08:02:53,759] [ERROR] [launch.py:325:sigkill_handler] ['/home/matsuzaki.takumi/.conda/envs/llava/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=0', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', './dataset/which_direction.json', '--image_folder', './images', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-7b-task-lora', '--num_train_epochs', '1', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = -7\r\n```\r\n\r\nThis log is ended with \"exits with return code = -7\"\r\nI don't know whether this message is important or not.</BODY>\n\n<COMMENTS>\n<Comment by matsutaku44 at 2024-12-04T07:12:37Z>\nI find where the error occurs.\r\n\r\n./llava/train/train.py   train()   line 827\r\n\r\n```\r\nmodel = LlavaLlamaForCausalLM.from_pretrained(\r\n                model_args.model_name_or_path,\r\n                cache_dir=training_args.cache_dir,\r\n                attn_implementation=attn_implementation,\r\n                torch_dtype=(torch.bfloat16 if training_args.bf16 else None),\r\n                **bnb_model_from_pretrained_args\r\n            )\r\n```\r\n\r\nin this function, the following logs were output\r\n\r\n```\r\n/home/matsuzaki.takumi/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\n[2024-12-04 07:01:37,781] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 17939\r\n[2024-12-04 07:01:37,781] [ERROR] [launch.py:325:sigkill_handler] ['/home/matsuzaki.takumi/.conda/envs/llava/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=0', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3_offload.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', './dataset/which_direction.json', '--image_folder', './images', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'False', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-7b-task-lora', '--num_train_epochs', '1', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '32', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = -7\r\n```\n</Comment>\n<Comment by matsutaku44 at 2024-12-05T04:02:08Z>\nI increased the size of shared memory and it solved the problem.\n</Comment>\n<Comment by MassEast at 2025-04-09T11:40:21Z>\nSame! If you use kubernetes, simply add\n\n```\n          volumeMounts:\n            - name: dshm\n              mountPath: /dev/shm\n...\n      volumes:\n        - name: dshm\n          emptyDir:\n            medium: Memory\n````\n\nto your k8s yaml file.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1787,
    "state": "open",
    "created_by": "Pixel-anter",
    "created_at": "2024-12-03T02:44:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1787</URL>\n\n<TITLE>[Question]  grad_norm=None</TITLE>\n\n<BODY>### Question\r\nHello, I am currently fine-tuning the visual encoder using LLaMA2 + CLIP-14-336 with the finetune_full_schedule.sh script. However, I noticed that the loss does not decrease and fluctuates significantly. What could be the reason for this？</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1786,
    "state": "open",
    "created_by": "zkailinzhang",
    "created_at": "2024-11-27T10:53:14Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1786</URL>\n\n<TITLE>[Question] demo website error  https://llava.hliu.cc/</TITLE>\n\n<BODY>### Question\n\nError 1033 Ray ID: 8e9190ab6d35d4fb • 2024-11-27 10:51:48 UTC\r\nArgo Tunnel error</BODY>\n\n<COMMENTS>\n<Comment by divinity76 at 2025-05-06T09:40:11Z>\n```\n$ curl --dump-header - 'https://llava.hliu.cc/'\nHTTP/2 530 \ndate: Tue, 06 May 2025 09:38:39 GMT\ncontent-length: 0\nserver: cloudflare\ncache-control: private, no-store\ncf-cache-status: DYNAMIC\nnel: {\"report_to\":\"cf-nel\",\"success_fraction\":0.0,\"max_age\":604800}\nreport-to: {\"group\":\"cf-nel\",\"max_age\":604800,\"endpoints\":[{\"url\":\"https://a.nel.cloudflare.com/report/v4?s=dxtLfpsalIJmxuLdPzGjRj5b%2B3jzOhimmT8xIOPUohs%2BYGzHUdyWteW8B%2B3RfIEATV02CGqibc6VRpkmTvqaoBKe3RNo7Ne1rVP8p%2B7VHvPr2e4DoEc7Tj8FH1U0QSkL\"}]}\ncf-ray: 93b781847fb0b505-OSL\nalt-svc: h3=\":443\"; ma=86400\n```\nCloudflare HTTP 530 🤷‍♂\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1785,
    "state": "open",
    "created_by": "cqray1990",
    "created_at": "2024-11-27T03:37:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1785</URL>\n\n<TITLE>[Question] llava 预训练中断，继续预训练默认加载模型报错</TITLE>\n\n<BODY>### Question\r\n\r\n                                \r\n默认预训练加载模型是在 deepspeed_checkpoint_dirs = sorted(glob.glob(f\"{checkpoint_path}/global_step*\"))，可是预训练保存模型的时候根本就没有这个目录\r\n\r\n\r\n                          def deepspeed_load_checkpoint(deepspeed_engine, checkpoint_path, load_module_strict=True):\r\n                                    # it's possible that the user is trying to resume from model_path, which doesn't necessarily\r\n                                    # contain a deepspeed checkpoint. e.g. examples just check if the dir exists and assume it's\r\n                                    # a resume from a checkpoint and not just a local pretrained weight. So we check here if the\r\n                                    # path contains what looks like a deepspeed checkpoint\r\n                                    import glob\r\n                                \r\n                                    deepspeed_checkpoint_dirs = sorted(glob.glob(f\"{checkpoint_path}/global_step*\"))\r\n                                \r\n                                    if len(deepspeed_checkpoint_dirs) > 0:\r\n                                        logger.info(f\"Attempting to resume from {checkpoint_path}\")\r\n                                        # this magically updates self.optimizer and self.lr_scheduler\r\n                                        load_path, _ = deepspeed_engine.load_checkpoint(\r\n                                            checkpoint_path,\r\n                                            load_module_strict=load_module_strict,\r\n                                            load_optimizer_states=True,\r\n                                            load_lr_scheduler_states=True,\r\n                                        )\r\n                                        if load_path is None:\r\n                                            raise ValueError(f\"[deepspeed] failed to resume from checkpoint {checkpoint_path}\")\r\n                                    else:\r\n                                        raise ValueError(f\"Can't find a valid checkpoint at {checkpoint_path}\")</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1784,
    "state": "open",
    "created_by": "daulettoibazar",
    "created_at": "2024-11-26T15:02:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1784</URL>\n\n<TITLE>[Question] Model parameters during finetuning (prints me only mm_projector parameters)</TITLE>\n\n<BODY>### Question\r\n\r\nHi @haotian-liu, I am trying to pretrain and finetune llava model on my custom dataset. But during the fine-tuning, when I load projector.bin, LLava model and Image encoder, when i run `train.py` with following changes, it only prints the weights of projector (no model weight, no image encoder weights):\r\n```\r\n    total_params = sum(param.numel() for param in model.parameters())\r\n    \r\n    # Total number of trainable parameters\r\n    trainable_params = sum(param.numel() for param in model.parameters() if param.requires_grad)\r\n    \r\n    print(f\"Total Parameters: {total_params}\")\r\n    print(f\"Trainable Parameters: {trainable_params}\")\r\n    data_module = make_supervised_data_module(tokenizer=tokenizer,\r\n                                              data_args=data_args)\r\n    trainer = LLaVATrainer(model=model,\r\n                    tokenizer=tokenizer,\r\n                    args=training_args,\r\n                    **data_module)\r\n```\r\n\r\nThe output is\r\n```terminal\r\nTotal Parameters: 32000000\r\nTrainable Parameters:32000000\r\n```\r\nBelow is my fine tuning script:\r\n```terminal\r\ndeepspeed LLaVA/llava/train/train_mem.py \\\r\n    --deepspeed \"LLaVA/scripts/zero3.json\" \\\r\n    --model_name_or_path \"./tmp/model/v1.5_model\" \\\r\n    --version v1 \\\r\n    --freeze_backbone True \\\r\n    --data_path \"./tmp/data/fv4.json\" \\\r\n    --image_folder ./tmp/data/images \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter ./tmp/models/v1.5_model/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./llava-v1.5-13b_fv4 \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 4096 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nWhy other weights are not visible here?</BODY>\n\n<COMMENTS>\n<Comment by tarun-menta at 2024-12-10T06:58:07Z>\nThis is because DeepSpeed Zero3 shards the model across GPUs, so some model parameters are replaced with empty tensors, which makes `param.numel()` zero, even though that parameter is still trainable\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1783,
    "state": "open",
    "created_by": "hktk07",
    "created_at": "2024-11-26T07:04:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1783</URL>\n\n<TITLE>[Question] Does llava support dynamic image tokens input?</TITLE>\n\n<BODY>### Question\n\nDoes llava support dynamic image tokens input?\r\nFor example: for an image input 1x336 tokens, for another image input 1x512 tokens?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1782,
    "state": "open",
    "created_by": "bollossom",
    "created_at": "2024-11-25T01:57:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1782</URL>\n\n<TITLE>[Discussion] loss function of finetune llava 1.5 with sft</TITLE>\n\n<BODY>### Discussion\n\n![屏幕截图 2024-11-25 095623](https://github.com/user-attachments/assets/8f7b78e1-20cd-40e5-a323-0f9c8bd1d70c)\r\nIs it normal for loss to be so shaky?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1781,
    "state": "closed",
    "created_by": "cqray1990",
    "created_at": "2024-11-24T12:29:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1781</URL>\n\n<TITLE>[Question]  where can   download OCR-VQA data?</TITLE>\n\n<BODY>### Question\n\nwhen perform loadDataset.py it can not download the complete data,only download partial data</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1780,
    "state": "open",
    "created_by": "cqray1990",
    "created_at": "2024-11-24T11:01:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1780</URL>\n\n<TITLE>[Question]  when run v1.5/pretrain.sh , there are some errors</TITLE>\n\n<BODY>### Question\n\n[2024-11-24 19:00:05,852] [INFO] [comm.py:652:init_distributed] cdb=None\r\n[2024-11-24 19:00:05,853] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\nYou are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1779,
    "state": "open",
    "created_by": "cqray1990",
    "created_at": "2024-11-24T08:07:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1779</URL>\n\n<TITLE>[Question] pretrain data</TITLE>\n\n<BODY>### Question\n\nblip_laion_cc_sbu_558k.json and blip_laion_cc_sbu_558k_meta.json , which data did use when pretrain, and what is the difference between them</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1778,
    "state": "open",
    "created_by": "ykzqjyhhh",
    "created_at": "2024-11-23T14:22:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1778</URL>\n\n<TITLE>[Question] I trained the llava-1.5-13B model, but when evaluating, the inference answer is always empty and the speed is very slow.</TITLE>\n\n<BODY>### Question\n\n![20241122185229](https://github.com/user-attachments/assets/4318b4f7-31c1-45ce-8912-c8cfd8d9f9a2)\r\nThe training loss graph looks like this, which is normal.\r\nThe training configuration is the same as pretrain.sh</BODY>\n\n<COMMENTS>\n<Comment by CAOANJIA at 2025-03-21T07:25:39Z>\nSame problem. My finetuned model always outputs empty string and the speed is very slow. \n\nHave you solved it?\n</Comment>\n<Comment by yinyuanzhang at 2025-04-18T14:31:32Z>\nSame problem.  How did you solve it?\n</Comment>\n<Comment by CAOANJIA at 2025-04-18T14:35:43Z>\n> solve\n\nmaybe you need to look at the forward in vision encoder, notice data type (list or not)\n</Comment>\n<Comment by MangoLibn at 2025-05-05T08:52:27Z>\nThank u for your insight!! but I didn't understand how to fix it, because i follow the original code strictly.\n\nCan u talk it in detail ? Thank u Very much!!\n</Comment>\n<Comment by MangoLibn at 2025-05-05T08:53:33Z>\n> > solve\n> \n> maybe you need to look at the forward in vision encoder, notice data type (list or not)\n\nThank u for your insight!! but I didn't understand how to fix it, because i follow the original code strictly.\n\nCan u talk it in detail ? Thank u Very much!!\n</Comment>\n<Comment by MassEast at 2025-05-09T08:08:56Z>\nI am experiencing the same problem; model only outputs \"</s>\".\n</Comment>\n<Comment by MassEast at 2025-05-09T10:20:32Z>\nFor me, what helped was to stay close to [model_vqa.py](https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/model_vqa.py) as explained in the [Evaluation](https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md) section. Especially, do not forget to use a conversation template for your prompt!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1777,
    "state": "open",
    "created_by": "pspdada",
    "created_at": "2024-11-23T12:51:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1777</URL>\n\n<TITLE>[Usage] Batch evaluation using sqlang doesn't support llava-v1.5 model</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nhttps://github.com/haotian-liu/LLaVA/issues/754  mentioned that batch evaluation is supported with SGLang.\r\nHowever, the way seems not work with llava-v1.5 model\r\nrelated issues:\r\nhttps://github.com/sgl-project/sglang/issues/1245\r\nhttps://github.com/sgl-project/sglang/issues/2140\r\nCan is be supported?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1776,
    "state": "open",
    "created_by": "weiaicunzai",
    "created_at": "2024-11-20T15:19:21Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1776</URL>\n\n<TITLE>[Question] Where can I obtain the training dataset of the LLava 1.5?</TITLE>\n\n<BODY>### Question\n\nThanks for your great work.\r\nI am currently learning about the LLava1.5 and currently training LLava1.5. \r\nHowever, I can not find the llava 1.5 dataset, does it open-sourced? Or should I generate it myself?</BODY>\n\n<COMMENTS>\n<Comment by weiaicunzai at 2024-11-20T15:19:57Z>\npossible similar issue #489\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1775,
    "state": "open",
    "created_by": "tianke0711",
    "created_at": "2024-11-18T08:32:27Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1775</URL>\n\n<TITLE>[Usage] json.decoder.JSONDecodeError: Expecting ',' delimiter: line 1559608 column 82 (char 39444904)</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\nraceback (most recent call last):\r\n  File \"/home/jupyter/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"/home/jupyter/LLaVA/llava/train/train.py\", line 964, in train\r\n    data_module = make_supervised_data_module(tokenizer=tokenizer,\r\n  File \"/home/jupyter/LLaVA/llava/train/train.py\", line 779, in make_supervised_data_module\r\n    train_dataset = LazySupervisedDataset(tokenizer=tokenizer,\r\n  File \"/home/jupyter/LLaVA/llava/train/train.py\", line 665, in __init__\r\n    list_data_dict = json.load(open(data_path, \"r\"))\r\n  File \"/opt/conda/envs/llava/lib/python3.10/json/__init__.py\", line 293, in load\r\n    return loads(fp.read(),\r\n  File \"/opt/conda/envs/llava/lib/python3.10/json/__init__.py\", line 346, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/json/decoder.py\", line 337, in decode\r\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n  File \"/opt/conda/envs/llava/lib/python3.10/json/decoder.py\", line 353, in raw_decode\r\n    obj, end = self.scan_once(s, idx)\r\njson.decoder.JSONDecodeError: Expecting ',' delimiter: line 1559608 column 82 (char 39444904)\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1774,
    "state": "open",
    "created_by": "hazardout",
    "created_at": "2024-11-18T03:38:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1774</URL>\n\n<TITLE>[Question] Image-text match</TITLE>\n\n<BODY>### Question\r\n\r\nHi author!\r\nI really want to know whether there are any ways to get the Similarity of image text pairs, or what prompt should I use to prompt Llava output something relative.\r\n\r\nFor example, how can I make inferences if I want to choose a picture that best matches the text description among multiple pictures?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1772,
    "state": "open",
    "created_by": "yuwang4321",
    "created_at": "2024-11-16T03:06:21Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1772</URL>\n\n<TITLE>[Usage] The issue encountered when fine-tuning llava_mistral1.6 using LoRA</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\n\"I used llava_mistral 1.6 and LoRA for fine-tuning. The model loads and works fine when epoch=1, but there is no output when epoch=10. Has anyone encountered the same issue? How can I troubleshoot and resolve it?\"\r\nCommand:\r\n```\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --model_name_or_path /root/autodl-tmp/checkpoints/llava-v1.6-mistral-7b-1114 \\\r\n    --version v1 \\\r\n    --data_path /root/autodl-tmp/code_wy/dataset/fine_tuned_llava/MMQA_finetuned_data.jsonl \\\r\n    --image_folder /root/autodl-tmp/final_dataset_images \\\r\n    --vision_tower /root/autodl-tmp/checkpoints/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir /root/autodl-tmp/checkpoints/llava-v1.6-mistral-7b-hf-task-lora \\\r\n    --num_train_epochs 10 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 8 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n\r\npython scripts/merge_lora_weights.py --model-path \"/root/autodl-tmp/checkpoints/llava-v1.6-mistral-7b-hf-task-lora\" \\\r\n       --model-base \"/root/autodl-tmp/checkpoints/llava-v1.6-mistral-7b-1114\" \\\r\n       --save-model-path \"/root/autodl-tmp/checkpoints/llava-v1.6-mistral-7b-hf-merged\"\r\n```\r\n\r\nLog: \r\n```\r\n/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\r\n  warnings.warn(\r\n/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\r\n  warnings.warn(\r\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\r\n![screenshot-20241116-110518](https://github.com/user-attachments/assets/865ab513-e54d-4f11-bfff-deed11a028ef)\r\n![screenshot-20241116-110552](https://github.com/user-attachments/assets/73a805aa-1108-49fc-876c-92dbdbb97853)\r\n\r\n```\r\n\r\nScreenshots:\r\nepoch=10\r\n![screenshot-20241116-110518](https://github.com/user-attachments/assets/39334dca-c528-4d4e-bf8d-f40cf9593093)\r\n\r\nepoch=1\r\n![screenshot-20241116-110552](https://github.com/user-attachments/assets/951f77dd-90b9-4992-8f21-67d761fe1b00)</BODY>\n\n<COMMENTS>\n<Comment by itisfree at 2025-06-07T07:40:54Z>\ndou you solve it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1771,
    "state": "open",
    "created_by": "amagzari",
    "created_at": "2024-11-16T01:33:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1771</URL>\n\n<TITLE>[Usage] Infer on model finetuned using finetune_qlora.sh</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: \r\nHello!\r\n\r\nI finetuned a model using finetune_qlora.sh where PROMPT_VERSION=\"llava_llama_2\" and MODEL_VERSION=\"llama-2-7b-chat\". This outputted the files below. I could not find instructions on how to infer on the obtained model. I would really appreciate any help.\r\n\r\nThank you!\r\n\r\n![Screenshot 2024-11-15 133036](https://github.com/user-attachments/assets/f9da94c0-17c6-4246-a5ff-68309125d340)\r\n\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by xingyifei2016 at 2024-11-29T07:47:59Z>\nAlso asking for help regarding this issue!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1770,
    "state": "open",
    "created_by": "yiwei-chenn",
    "created_at": "2024-11-15T16:27:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1770</URL>\n\n<TITLE>[Question] Can not reproduce LLaVA 1.5 performance on ScienceQA</TITLE>\n\n<BODY>### Question\n\nI use the command to test LLaVA 1.5 7B model's performance on ScienceQA datset, the command is like \r\n```python\r\nCUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/eval/sqa.sh\r\n``` \r\nAnd the .sh is the same as the github repo offers:\r\n```python\r\npython -m llava.eval.model_vqa_science \\\r\n    --model-path liuhaotian/llava-v1.5-13b \\\r\n    --question-file dataset/scienceqa/llava_test_CQM-A.json \\\r\n    --image-folder dataset/scienceqa/images/test \\\r\n    --answers-file dataset/scienceqa/answers/llava-v1.5-13b.jsonl \\\r\n    --single-pred-prompt \\\r\n    --conv-mode vicuna_v1\r\n\r\npython llava/eval/eval_science_qa.py \\\r\n    --base-dir dataset/scienceqa \\\r\n    --result-file dataset/scienceqa/answers/llava-v1.5-7b.jsonl \\\r\n    --output-file dataset/scienceqa/answers/llava-v1.5-7b_output.jsonl \\\r\n    --output-result dataset/scienceqa/answers/llava-v1.5-7b_result.json\r\n\r\n``` \r\n\r\nAfter running the evaluation, I got 69.51% for 7B version, and 72.73% for 13B version.\r\n\r\nI have double checked the images, pid_splits.json, problems.json are both downloaded from ScienceQA [repo](https://github.com/lupantech/ScienceQA). And question-file is from the [eval.zip](https://drive.google.com/file/d/1atZSBBrAX54yYpxtVVW33zFvcnaHeFPy/view?usp=sharing).\r\n\r\nThe results is a little bit higher than the model zoo reported, but it is not consisten. Does anyone could help me solve the inconsistent problem?</BODY>\n\n<COMMENTS>\n<Comment by arjunbreddy22 at 2024-12-20T07:34:41Z>\nHaving the same issue.\n</Comment>\n<Comment by zkril at 2025-04-12T16:50:37Z>\n![Image](https://github.com/user-attachments/assets/9623107b-a6d4-488a-983d-261ab5375ada)\nHaving the same issue.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1769,
    "state": "open",
    "created_by": "Tramac",
    "created_at": "2024-11-15T10:28:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1769</URL>\n\n<TITLE>Where is convert_answer_to_mme.py?</TITLE>\n\n<BODY>where is `convert_answer_to_mme.py` mentioned in [mme.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/eval/mme.sh#L13C8-L13C32)?\r\n\r\n![image](https://github.com/user-attachments/assets/162e66a2-6085-4c0c-a311-f60d5de21e93)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1768,
    "state": "open",
    "created_by": "LiXinYuann",
    "created_at": "2024-11-14T08:25:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1768</URL>\n\n<TITLE>[Question] disable print</TITLE>\n\n<BODY>### Question\n\nThe response printing of LLava in the terminal is really slow. Is there a way to disable this printing? I only need to log the responses. Please help.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1767,
    "state": "open",
    "created_by": "leo-young",
    "created_at": "2024-11-14T03:24:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1767</URL>\n\n<TITLE>[Question] Pretrain preprocess</TITLE>\n\n<BODY>### Question\r\nWhen I try to reproduce the llave v1.5 on llama3. on pretraining stage, I find the preprocess func is using the preprocess_v1, not the plain. But following the official training script in v1.5 pretrain.sh, the --version is set the plain.\r\n\r\nI tried to debug the code, found that \r\n```\r\n    if model_args.version == \"v0\":\r\n        if tokenizer.pad_token is None:\r\n            smart_tokenizer_and_embedding_resize(\r\n                special_tokens_dict=dict(pad_token=\"[PAD]\"),\r\n                tokenizer=tokenizer,\r\n                model=model,\r\n            )\r\n    elif model_args.version == \"v0.5\":\r\n        tokenizer.pad_token = tokenizer.unk_token\r\n    else:\r\n        # tokenizer.pad_token = tokenizer.unk_token\r\n        tokenizer.pad_token = tokenizer.eos_token\r\n        if model_args.version in conversation_lib.conv_templates:\r\n            print(\"a\")\r\n            conversation_lib.default_conversation = conversation_lib.conv_templates[model_args.version]\r\n        else:\r\n            conversation_lib.default_conversation = conversation_lib.conv_templates[\"vicuna_v1\"]\r\n```\r\nthe code block is setting the default_conversation to plain, but when trainer.train() start,\r\n```\r\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\r\n        sources = self.list_data_dict[i]\r\n        if isinstance(i, int):\r\n            sources = [sources]\r\n        assert len(sources) == 1, \"Don't know why it is wrapped to a list\"  # FIXME\r\n        if 'image' in sources[0]:\r\n            image_file = self.list_data_dict[i]['image']\r\n            image_folder = self.data_args.image_folder\r\n            processor = self.data_args.image_processor\r\n            image = Image.open(os.path.join(image_folder, image_file)).convert('RGB')\r\n\r\n\r\n            if self.data_args.image_aspect_ratio == 'pad':\r\n                def expand2square(pil_img, background_color):\r\n                    width, height = pil_img.size\r\n                    if width == height:\r\n                        return pil_img\r\n                    elif width > height:\r\n                        result = Image.new(pil_img.mode, (width, width), background_color)\r\n                        result.paste(pil_img, (0, (width - height) // 2))\r\n                        return result\r\n                    else:\r\n                        result = Image.new(pil_img.mode, (height, height), background_color)\r\n                        result.paste(pil_img, ((height - width) // 2, 0))\r\n                        return result\r\n\r\n                image = expand2square(image, tuple(int(x * 255) for x in processor.image_mean))\r\n                image = processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\r\n            else:\r\n                image = processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\r\n            sources = preprocess_multimodal(\r\n                copy.deepcopy([e[\"conversations\"] for e in sources]),\r\n                self.data_args)\r\n        else:\r\n            sources = copy.deepcopy([e[\"conversations\"] for e in sources])\r\n        data_dict = preprocess(\r\n            sources,\r\n            self.tokenizer,\r\n            has_image=('image' in self.list_data_dict[i]))\r\n        if isinstance(i, int):\r\n            data_dict = dict(input_ids=data_dict[\"input_ids\"][0],\r\n                             labels=data_dict[\"labels\"][0])\r\n\r\n        # image exist in the data\r\n        if 'image' in self.list_data_dict[i]:\r\n            data_dict['image'] = image\r\n        elif self.data_args.is_multimodal:\r\n            # image does not exist in the data, but the model is multimodal\r\n            crop_size = self.data_args.image_processor.crop_size\r\n            data_dict['image'] = torch.zeros(3, crop_size['height'], crop_size['width'])\r\n        return data_dict\r\n\r\n```\r\nwhen code is running to the dataset __getitem__ func, the conversation_lib.default_conversation is v1, so the preprocess is using the preprocess_v1. \r\nDoes someone encountered the same question?\r\nDoes the official llava is using preprocess_v1 in the pretraining stage?\r\n\r\nBlow is my training script:\r\n\r\n```\r\n--deepspeed\r\n.scripts/zero2.json\r\n--model_name_or_path\r\nmodels/Llama-3.2-1B-Instruct\r\n--vision_tower\r\nmodels/clip-vit-large-patch14-336\r\n--version\r\nplain\r\n--data_path\r\n./playground/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json\r\n--image_folder\r\n./playground/data/LLaVA-Pretrain/images\r\n--mm_projector_type\r\nmlp2x_gelu\r\n--tune_mm_mlp_adapter\r\nTrue\r\n--mm_vision_select_layer\r\n-2\r\n--mm_use_im_start_end\r\nFalse\r\n--mm_use_im_patch_token\r\nFalse\r\n--output_dir\r\n./checkpoints/llava-v1.5-1b-pretrain\r\n--num_train_epochs\r\n1\r\n--per_device_train_batch_size\r\n2\r\n--per_device_eval_batch_size\r\n4\r\n--gradient_accumulation_steps\r\n1\r\n--evaluation_strategy\r\n\"no\"\r\n--save_strategy\r\n\"steps\"\r\n--save_steps\r\n24000\r\n--save_total_limit\r\n1\r\n--learning_rate\r\n1e-3\r\n--weight_decay\r\n0.\r\n--warmup_ratio\r\n0.03\r\n--lr_scheduler_type\r\n\"cosine\"\r\n--logging_steps\r\n1\r\n--model_max_length\r\n2048\r\n--gradient_checkpointing\r\nTrue\r\n--dataloader_num_workers\r\n4\r\n--lazy_preprocess\r\nTrue\r\n```</BODY>\n\n<COMMENTS>\n<Comment by wenhaoli-xmu at 2025-03-07T15:42:10Z>\nHave you solved this problem?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1766,
    "state": "open",
    "created_by": "cookiesupers22",
    "created_at": "2024-11-13T22:37:57Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1766</URL>\n\n<TITLE>[Question] Issue with trying to reproduce the results of LLaVA-Bench-in-the-Wild on LLaVA-v1.5-7b. Has anyone who's reproduced it got this error?</TITLE>\n\n<BODY>### Question\n\nERROR:\r\n[...]\r\nSkipping 55 as we already have it.\r\n56\r\nSkipping 56 as we already have it.\r\n57\r\nSkipping 57 as we already have it.\r\n58\r\nSkipping 58 as we already have it.\r\n59\r\nSkipping 59 as we already have it.\r\n60\r\nllava-v1.5-13b\r\nTraceback (most recent call last):\r\n  File \"/content/LLaVA/llava/eval/summarize_gpt_review.py\", line 54, in <module>\r\n    scores['all'].append(review['score'])\r\nKeyError: 'score'</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1765,
    "state": "closed",
    "created_by": "yiwei-chenn",
    "created_at": "2024-11-13T22:21:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1765</URL>\n\n<TITLE>[Question] LLaVA 1.5 7B model fine-tune -- pydantic</TITLE>\n\n<BODY>### Question\r\n\r\nWhen I use my own pre-trained mlp adapter to finetune the LLaVA 1.5 7B model, I use the finetune_lora.sh like\r\n\r\n```shell\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path lmsys/vicuna-7b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path ./my_own.json \\\r\n    --image_folder ./llava-finetune \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-7b-pretrain/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n``` \r\n\r\nAnd I face the problem like \r\n\r\n![image](https://github.com/user-attachments/assets/39039fd9-ceb5-44d7-9c35-6e2cb35a9395)\r\n\r\nThis problem was caused by \r\n\r\n'pydantic_core._pydantic_core.ValidationError: 1 validation error for DeepSpeedZeroConfig\r\nstage3_prefetch_bucket_size\r\n  Input should be a valid integer, got a number with a fractional part [type=int_from_float, input_value=15099494.4, input_type=float]'\r\n\r\nI used the same procedure to use LoRA fine-tune the LLaVA 1.5 13B version, but did not cause the same problem.\r\n\r\nDoes anyone know how to solve that?</BODY>\n\n<COMMENTS>\n<Comment by yiwei-chenn at 2024-11-13T22:49:17Z>\nAfter checking [https://github.com/microsoft/DeepSpeed/issues/6525](https://github.com/microsoft/DeepSpeed/issues/6525), the problem seems caused by the mismatch between transformers and deepspeed.\r\n\r\nI solved the problem by downgrading deepspeed:\r\n\r\n`pip install deepspeed==0.14.5`\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1764,
    "state": "open",
    "created_by": "eslambakr",
    "created_at": "2024-11-12T13:56:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1764</URL>\n\n<TITLE>[Usage] Missing file 'model_vqa_qbench'</TITLE>\n\n<BODY>### Describe the issue\n\nThe Q-bench eval file is missing \"model_vqa_qbench\"</BODY>\n\n<COMMENTS>\n<Comment by EchoDreamer at 2024-12-09T02:51:27Z>\nCould you solve the issuse?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1763,
    "state": "open",
    "created_by": "jinghanSunn",
    "created_at": "2024-11-12T06:47:10Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1763</URL>\n\n<TITLE>[Usage] Inference Speed Issue with LoRA Fine-tuned Model on ScienceQA</TITLE>\n\n<BODY>Hi Haotian,\r\n\r\nThank you for your incredible work on this project. \r\n\r\nI am encountering an issue during inference. When I use the non-LoRA weights for inference on ScienceQA, the speed is approximately 1 second per sample. However, when I switch to the LoRA fine-tuned model, the inference speed drastically increases to over 40 seconds per sample.\r\n\r\nHere is the command I am using for fine-tuning (trained on 1 V100 with lora_r=4, bf16=False, tf32=False):\r\n```\r\nCUDA_VISIBLE_DEVICES=1 python3 llava/train/train.py \\\r\n    --lora_enable True --lora_r 4 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --model_name_or_path ./LLAVA-1.5/llava-v1.5-7b/ \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/eval/scienceqa/llava_train_CQM-A.json \\\r\n    --image_folder ./data/ScienceQA/image/train/ \\\r\n    --vision_tower ./data/clip-vit-large-patch14-336/ \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 False \\\r\n    --output_dir ./LLaVA-v1.5-7b-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 1000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 False \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nHere is the command I am using for inference:\r\n```\r\nCUDA_VISIBLE_DEVICES=3 python3 -m llava.eval.model_vqa_science \\\r\n    --model-path ./LLaVA-v1.5-7b-lora/checkpoint-50000/ \\\r\n    --model-base ./LLAVA-1.5/llava-v1.5-7b/ \\\r\n    --question-file ./playground/data/eval/scienceqa/llava_test_CQM-A.json \\\r\n    --image-folder ./data/ScienceQA/image/test/ \\\r\n    --answers-file ./playground/data/eval/scienceqa/answers/llava-v1.5-7b-lora-50000.jsonl \\\r\n    --single-pred-prompt \\\r\n    --temperature 0 \\\r\n    --conv-mode vicuna_v1\r\n```\r\n\r\nCould you please help me understand why the inference speed difference between the two models is significant? \r\n\r\nThank you!\r\n\r\nScreenshots:\r\n![屏幕截图 2024-11-12 144626](https://github.com/user-attachments/assets/fab89cb7-785d-4932-949f-3748b07ea46b)\r\n\r\nadapter_config.json:\r\n```\r\n{\r\n  \"alpha_pattern\": {},\r\n  \"auto_mapping\": null,\r\n  \"base_model_name_or_path\": \"./data/LLAVA-1.5/llava-v1.5-7b/\",\r\n  \"bias\": \"none\",\r\n  \"fan_in_fan_out\": false,\r\n  \"inference_mode\": true,\r\n  \"init_lora_weights\": true,\r\n  \"layer_replication\": null,\r\n  \"layers_pattern\": null,\r\n  \"layers_to_transform\": null,\r\n  \"loftq_config\": {},\r\n  \"lora_alpha\": 256,\r\n  \"lora_dropout\": 0.05,\r\n  \"megatron_config\": null,\r\n  \"megatron_core\": \"megatron.core\",\r\n  \"modules_to_save\": null,\r\n  \"peft_type\": \"LORA\",\r\n  \"r\": 4,\r\n  \"rank_pattern\": {},\r\n  \"revision\": null,\r\n  \"target_modules\": [\r\n    \"down_proj\",\r\n    \"o_proj\",\r\n    \"q_proj\",\r\n    \"gate_proj\",\r\n    \"up_proj\",\r\n    \"v_proj\",\r\n    \"k_proj\"\r\n  ],\r\n  \"task_type\": \"CAUSAL_LM\",\r\n  \"use_dora\": false,\r\n  \"use_rslora\": false\r\n```\r\n\r\nconfig.json\r\n```\r\n{\r\n  \"_name_or_path\": \"./data/LLAVA-1.5/llava-v1.5-7b/\",\r\n  \"architectures\": [\r\n    \"LlavaLlamaForCausalLM\"\r\n  ],\r\n  \"attention_bias\": false,\r\n  \"attention_dropout\": 0.0,\r\n  \"bos_token_id\": 1,\r\n  \"eos_token_id\": 2,\r\n  \"freeze_mm_mlp_adapter\": false,\r\n  \"freeze_mm_vision_resampler\": false,\r\n  \"hidden_act\": \"silu\",\r\n  \"hidden_size\": 4096,\r\n  \"image_aspect_ratio\": \"pad\",\r\n  \"initializer_range\": 0.02,\r\n  \"intermediate_size\": 11008,\r\n  \"max_length\": 4096,\r\n  \"max_position_embeddings\": 4096,\r\n  \"mm_hidden_size\": 1024,\r\n  \"mm_patch_merge_type\": \"flat\",\r\n  \"mm_projector_lr\": 2e-05,\r\n  \"mm_projector_type\": \"mlp2x_gelu\",\r\n  \"mm_resampler_type\": null,\r\n  \"mm_use_im_patch_token\": false,\r\n  \"mm_use_im_start_end\": false,\r\n  \"mm_vision_select_feature\": \"patch\",\r\n  \"mm_vision_select_layer\": -2,\r\n  \"mm_vision_tower\": \"./data/clip-vit-large-patch14-336/\",\r\n  \"model_type\": \"llava_llama\",\r\n  \"num_attention_heads\": 32,\r\n  \"num_hidden_layers\": 32,\r\n  \"num_key_value_heads\": 32,\r\n  \"pad_token_id\": 0,\r\n  \"pretraining_tp\": 1,\r\n  \"rms_norm_eps\": 1e-05,\r\n  \"rope_scaling\": null,\r\n  \"rope_theta\": 10000.0,\r\n  \"tie_word_embeddings\": false,\r\n  \"tokenizer_model_max_length\": 2048,\r\n  \"tokenizer_padding_side\": \"right\",\r\n  \"torch_dtype\": \"float16\",\r\n  \"transformers_version\": \"4.37.2\",\r\n  \"tune_mm_mlp_adapter\": false,\r\n  \"tune_mm_vision_resampler\": false,\r\n  \"unfreeze_mm_vision_tower\": false,\r\n  \"use_cache\": true,\r\n  \"use_mm_proj\": true,\r\n  \"vocab_size\": 32000\r\n}\r\n```</BODY>\n\n<COMMENTS>\n<Comment by GCShao at 2024-12-12T09:04:00Z>\nhello, can you leave a contact for me? I am also trying to deploy the LLaVa on the Tesla V100.\n</Comment>\n<Comment by Haowen-Ji at 2025-02-28T14:40:59Z>\nSame issue, extremely slow after adding lora weights\n</Comment>\n<Comment by yinyuanzhang at 2025-04-17T03:28:28Z>\nsame issue，did you solve it? a thanks\n</Comment>\n<Comment by yinyuanzhang at 2025-04-18T14:23:14Z>\n> Same issue, extremely slow after adding lora weights\n\nSame issue, extremely slow after adding lora weights and empty answers. Do you solve it?\n</Comment>\n<Comment by PoopBear1 at 2025-07-27T09:41:02Z>\nI had this same issue. \n\nI found it might because the merged mdel has ill-conditioned matrices (where highly like you did not change the defualt alpha value when you drop the rank from 128 to 4 in your case).  The output logs can tell you something and I found the loss dramatically increases after certain epoch (0.15 in my case).\n\n—————————————————————————\nJust a quick update, after I made correct alpha pairs, issues have gone. Dont forget to change the alpha values as well!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1762,
    "state": "open",
    "created_by": "ChathurangiShyalika",
    "created_at": "2024-11-12T04:31:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1762</URL>\n\n<TITLE>[Usage] Error during evaluation for image.../image.png: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)</TITLE>\n\n<BODY>Issue: I’m evaluating a set of images using LLAVA. I'm encountering this error and it happens intermittently. The evaluation works fine for the first image but fails for the second. I'm running this on an V100 GPU. The current output is as follows. Please provide a solution for this issue. Thank you\r\n\r\nOutput: \r\n```\r\n----------------------------\r\nimage_path /../image_1.png\r\nResponse: Classification is No.\r\n----------------------------\r\nimage_path /../image_2.png\r\nError during evaluation for image /../image_2.png: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\r\n----------------------------\r\nimage_path /../image_3.png\r\nResponse: Classification is No.\r\n----------------------------\r\nimage_path /../image_4.png\r\nError during evaluation for image /../image_2.png: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\r\n```</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1761,
    "state": "open",
    "created_by": "ck-amrahd",
    "created_at": "2024-11-12T00:14:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1761</URL>\n\n<TITLE>Fine tune for object detection</TITLE>\n\n<BODY>Is the model trained on the coco dataset? Is it possible to fine-tune the model for object detection on a custom dataset?</BODY>\n\n<COMMENTS>\n<Comment by gapjialin at 2025-01-10T09:29:08Z>\nHi, I am having the same problem. I am fine tuning on my own target detection dataset. Sample dataset is as follows: “from”: “human”, “value”: “Where in the picture can the people be found?”, “value”.\r\n        “value\": ”Where in the picture can the people be found?”\r\n      }, {\r\n      {\r\n        “from\": ‘gpt’, ‘value’: ‘person’: ‘where in the picture can the people be found?’ }, {\r\n        “value\": ‘person 1’s bounding box coordinate of the region is [0.33, 0.63, 0.37, 0.81], person 2's bounding box coordinate of the region is [0.37, 0.63, 0.4, 0.78]. 0.4, 0.78], person 3's bounding box coordinate of the region is [0.4, 0.62, 0.43, 0.75], person 4's bounding box coordinate of the region is [0.84, 0.58, 0.87, 0.73], person 5's bounding box coordinate of the region is [0.52, 0.59, 0.55, 0.71]”\r\n      }, fine-tuning reveals no detectable effect.\n</Comment>\n<Comment by 1835969208 at 2025-05-25T09:37:57Z>\n@gapjialin 请问你在微调时候使用的loss使原本的交叉损失熵吗，还是iou。因为这是一个目标检测的任务，是不是使用iou loss更加合理\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1760,
    "state": "open",
    "created_by": "JavaWebT",
    "created_at": "2024-11-11T11:25:30Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1760</URL>\n\n<TITLE>[Discussion] pip install e  . error</TITLE>\n\n<BODY>### Discussion\r\n![error](https://github.com/user-attachments/assets/3d4b829d-bc6b-4896-8f38-ceb605ca9046)\r\n\r\n error: [subprocess-exited-with-error]\r\n\r\n  × git clone --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-cg4p2h80/transformers_f341fe490d084d0d8efd90c868e5a2ba did not run successfully.\r\n  │ exit code: 128\r\n  ╰─> See above for output.\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1759,
    "state": "open",
    "created_by": "fmy7834",
    "created_at": "2024-11-06T04:11:23Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1759</URL>\n\n<TITLE>[Usage] Training process get stuck in the last iteration of instruction finetuning phrase.</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nWhen I try to training LLaVA-1.5(with modifications) using Lora in instruction finetuning phrase with vision encoder trainable, it stucks in the last iteration without any outputs. However, when I keep vision encoder frozen, the training process could carry on normally and successfully save ckpts. And the memory is not used up. I was training with zero2. Does anyone meet this problem before?\r\n![image](https://github.com/user-attachments/assets/31767b5d-2b8f-46dd-9ef2-f25b8d6c1f32)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1758,
    "state": "open",
    "created_by": "Tizzzzy",
    "created_at": "2024-11-05T19:47:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1758</URL>\n\n<TITLE>[Question] Is it possible to extract the latent representation of the image input from the model?</TITLE>\n\n<BODY>### Question\r\n\r\nHi,\r\nBig fan of your work!\r\nRecently, I am working with a project that need to extract two kinds of latent representation of the input from the model.\r\n1. The first one is the latent representation of the pure image input, without any text.\r\n2. The second one is the latent representation of the pure text input, without any image.\r\n\r\nCurrently, I already modified the `run_llava.py`, so that the model can generate answer based on pure image or pure text input. However, I need help on how to extract the representation of the input.\r\n\r\nBelow is my code:\r\n```\r\nimport argparse\r\nimport torch\r\n\r\nfrom llava.constants import (\r\n    IMAGE_TOKEN_INDEX,\r\n    DEFAULT_IMAGE_TOKEN,\r\n    DEFAULT_IM_START_TOKEN,\r\n    DEFAULT_IM_END_TOKEN,\r\n    IMAGE_PLACEHOLDER,\r\n)\r\nfrom llava.conversation import conv_templates, SeparatorStyle\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.utils import disable_torch_init\r\nfrom llava.mm_utils import (\r\n    process_images,\r\n    tokenizer_image_token,\r\n    get_model_name_from_path,\r\n)\r\n\r\nfrom PIL import Image\r\n\r\nimport requests\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\nimport re\r\n\r\n\r\ndef image_parser(args):\r\n    out = args.image_file.split(args.sep)\r\n    return out\r\n\r\n\r\ndef load_image(image_file):\r\n    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\r\n        response = requests.get(image_file)\r\n        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\r\n    else:\r\n        image = Image.open(image_file).convert(\"RGB\")\r\n    return image\r\n\r\n\r\ndef load_images(image_files):\r\n    out = []\r\n    for image_file in image_files:\r\n        image = load_image(image_file)\r\n        out.append(image)\r\n    return out\r\n\r\n\r\ndef eval_model(args):\r\n    # Model\r\n    disable_torch_init()\r\n\r\n    model_name = get_model_name_from_path(args.model_path)\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(\r\n        args.model_path, args.model_base, model_name\r\n    )\r\n\r\n    qs = args.query\r\n    if args.image_file is not None:\r\n        image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\r\n        if IMAGE_PLACEHOLDER in qs:\r\n            if model.config.mm_use_im_start_end:\r\n                qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\r\n            else:\r\n                qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\r\n        else:\r\n            if model.config.mm_use_im_start_end:\r\n                qs = image_token_se + \"\\n\" + qs\r\n            else:\r\n                qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\r\n    else:\r\n        qs = qs\r\n\r\n    if \"llama-2\" in model_name.lower():\r\n        conv_mode = \"llava_llama_2\"\r\n    elif \"mistral\" in model_name.lower():\r\n        conv_mode = \"mistral_instruct\"\r\n    elif \"v1.6-34b\" in model_name.lower():\r\n        conv_mode = \"chatml_direct\"\r\n    elif \"v1\" in model_name.lower():\r\n        conv_mode = \"llava_v1\"\r\n    elif \"mpt\" in model_name.lower():\r\n        conv_mode = \"mpt\"\r\n    else:\r\n        conv_mode = \"llava_v0\"\r\n\r\n    if args.conv_mode is not None and conv_mode != args.conv_mode:\r\n        print(\r\n            \"[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}\".format(\r\n                conv_mode, args.conv_mode, args.conv_mode\r\n            )\r\n        )\r\n    else:\r\n        args.conv_mode = conv_mode\r\n\r\n    # qs = \"\"\r\n    conv = conv_templates[args.conv_mode].copy()\r\n    conv.append_message(conv.roles[0], qs)\r\n    conv.append_message(conv.roles[1], None)\r\n    prompt = conv.get_prompt()\r\n\r\n    if args.image_file is not None:\r\n        image_files = image_parser(args)\r\n        images = load_images(image_files)\r\n        image_sizes = [x.size for x in images]\r\n        images_tensor = process_images(\r\n            images,\r\n            image_processor,\r\n            model.config\r\n        ).to(model.device, dtype=torch.float16)\r\n    else:\r\n        images_tensor = None\r\n        image_sizes = None\r\n\r\n    input_ids = (\r\n        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\r\n        .unsqueeze(0)\r\n        .cuda()\r\n    )\r\n\r\n    with torch.inference_mode():\r\n        output_ids = model.generate(\r\n            input_ids,\r\n            images=images_tensor,\r\n            image_sizes=image_sizes,\r\n            do_sample=True if args.temperature > 0 else False,\r\n            temperature=args.temperature,\r\n            top_p=args.top_p,\r\n            num_beams=args.num_beams,\r\n            max_new_tokens=args.max_new_tokens,\r\n            use_cache=True,\r\n        )\r\n\r\n    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\r\n    print(outputs)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--model-path\", type=str, default=\"liuhaotian/llava-v1.5-7b\")\r\n    parser.add_argument(\"--model-base\", type=str, default=None)\r\n    parser.add_argument(\"--image-file\", type=str, default=\"../../val2017/000000000139.jpg\") # ../../val2017/000000000139.jpg\r\n    parser.add_argument(\"--query\", type=str, default=\"Describe this image\")\r\n    parser.add_argument(\"--conv-mode\", type=str, default=None)\r\n    parser.add_argument(\"--sep\", type=str, default=\",\")\r\n    parser.add_argument(\"--temperature\", type=float, default=0.2)\r\n    parser.add_argument(\"--top_p\", type=float, default=None)\r\n    parser.add_argument(\"--num_beams\", type=int, default=1)\r\n    parser.add_argument(\"--max_new_tokens\", type=int, default=512)\r\n    args = parser.parse_args()\r\n\r\n    eval_model(args)\r\n```\r\n\r\nThank you so much for your help!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1757,
    "state": "closed",
    "created_by": "YUECHE77",
    "created_at": "2024-11-02T20:59:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1757</URL>\n\n<TITLE>[Usage] How to run CLI after Visual Instruction Tuning with lora?</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nAfter Visual Instruction Tuning with lora (use scripts/v1_5/finetune_lora.sh), my output folder is:\r\n\r\n![89a63f3363f3eff9106afa3a568f1ae](https://github.com/user-attachments/assets/71fca52c-8aa6-4e22-9212-97bd4b8883ce)\r\n\r\nThen I tried to merge the LoRA weights with the base_model (vicuna-7b-v1.5) using merge_lora_weights.py, to solve the bugs I encountered during merging, I downgraded the transformers from 4.37.2 to 4.36.2. After merging, the merged folder I got is:\r\n\r\n![image](https://github.com/user-attachments/assets/8e29f35f-dde0-4350-8035-11cd7d5eec38)\r\n\r\nBut when I tried to run CLI with the merged model (with command python -m llava.serve.cli --model-path /xxxxx/xxxxx/merged_lora_llava --image-file \"/xxxx/xxxx/xxxx/LLaVA/images/llava_logo.png\" --load-4bit), I encountered numerous mismatched parameters:\r\n\r\n![image](https://github.com/user-attachments/assets/f21d8014-f09a-47ac-8b0c-83441f517c27)\r\n\r\nAnd after typing a query, I eventually got a bug.\r\n\r\nI understand that we need to transfer the weights to the hf to use it. But how to do this?\r\n\r\nCould anyone provides some instructions about how to run CLI after tuning with LoRA?   Thanks!!! @haotian-liu</BODY>\n\n<COMMENTS>\n<Comment by YUECHE77 at 2024-11-06T01:10:05Z>\nI think I figure it out.\r\n\r\nFor people have same issues:\r\n\r\nFirst of all, run the Visual Instruction Tuning step by step (with LoRA).\r\n\r\nThen run the merge_lora_weights.py -> the base model is vicuna -> if you have bugs when running this, downgrade the transformers from 4.37.2 to 4.36.2 -> upgrade it back to 4.37.2 after merging. And now you have the merged model. If you see a warning says the projector weights are not loaded, don't worry about that, it will be loaded later. You don't need to change anything.\r\n\r\nAnd eventually, you can use the merged model to run CLI. You might encounter the same warning for lots of mismatched parameters, but I think this is normal, and you don't need to do anything about it. All the mismatched parameters are all from vision tower, and I checked the source code for the training, those parameters are frozen. So each time we run CLI, it will download the model from huggingface (Of course, you can download it to your local machine). Please see the config.json of your merged model for more information.\r\n\r\nI run the evaluation on POPE and TestVQA with my merged model, the merged official lora model (on huggingface), and the official complete model (on huggingface). Their results are in the same range, very close to each other, which indicates that the merging is correct.\r\n\r\nI also see some of people get adapter_model.safetensors and some people get adapter_model.bin after tuning. Both of them are fine, just follow my steps.\r\n\r\nPlease correct me if I'm wrong.\r\n\r\nHope those informations are helpful to you!\r\n\r\nGood luck.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1756,
    "state": "open",
    "created_by": "liuting20",
    "created_at": "2024-11-01T03:35:55Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1756</URL>\n\n<TITLE>Could you please provide the test files of vqav2 and gqa on llava-15-7b？</TITLE>\n\n<BODY>### feature\n\nCould you please provide the test files of vqav2 and gqa on llava-15-7b？, llava-v1.5-7b.jsonl? You provided only 13b json.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1755,
    "state": "open",
    "created_by": "Sarah-air",
    "created_at": "2024-11-01T02:03:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1755</URL>\n\n<TITLE>pip install -e . Error</TITLE>\n\n<BODY>### Describe the issue\n\n<img width=\"1063\" alt=\"image\" src=\"https://github.com/user-attachments/assets/7d5ebde3-a993-4236-b44e-708d840312d2\"></BODY>\n\n<COMMENTS>\n<Comment by DJTaichi at 2024-11-07T10:38:42Z>\nThis might be the issue you’re experiencing: pypa/setuptools#4483\r\n\r\nPlease try the following:\r\n```\r\npip install --upgrade setuptools==70.0.0\r\npip install --upgrade packaging==24.1\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1753,
    "state": "closed",
    "created_by": "sean-wade",
    "created_at": "2024-10-31T09:28:50Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1753</URL>\n\n<TITLE>[Question] Where to get llava_train_QCM-LEPA.json file?</TITLE>\n\n<BODY>### Question\r\n\r\nI've tried the convert_sqa_to_llava.py by this command:\r\n`\r\npython convert_sqa_to_llava.py --task convert_to_jsonl --base_dir /data/LM/datasets/ScienceQA/ --split train\r\n`\r\n\r\n\r\nand get this file: /data/LM/datasets/scienceqa_train_QCM-LEPA.jsonl.\r\n\r\n\r\n\r\nBut when I change the \"llava_train_QCM-LEPA.json\" to \"/data/LM/datasets/scienceqa_train_QCM-LEPA.jsonl\" in the finetune.sh scripts. It doesn't work as follow error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/data/zhanghao/github/LLaVA-main/llava/train/train_mem.py\", line 5, in <module>\r\n    train(attn_implementation=\"eager\")\r\n  File \"/data/zhanghao/github/LLaVA-main/llava/train/train.py\", line 959, in train\r\n    data_module = make_supervised_data_module(tokenizer=tokenizer,\r\n  File \"/data/zhanghao/github/LLaVA-main/llava/train/train.py\", line 779, in make_supervised_data_module\r\n    train_dataset = LazySupervisedDataset(tokenizer=tokenizer,\r\n  File \"/data/zhanghao/github/LLaVA-main/llava/train/train.py\", line 665, in __init__\r\n    list_data_dict = json.load(open(data_path, \"r\"))\r\n  File \"/login_home/zhanghao/anaconda3/envs/tx8quant/lib/python3.10/json/__init__.py\", line 293, in load\r\n    return loads(fp.read(),\r\n  File \"/login_home/zhanghao/anaconda3/envs/tx8quant/lib/python3.10/json/__init__.py\", line 346, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"/login_home/zhanghao/anaconda3/envs/tx8quant/lib/python3.10/json/decoder.py\", line 340, in decode\r\n    raise JSONDecodeError(\"Extra data\", s, end)\r\njson.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 710)\r\n```</BODY>\n\n<COMMENTS>\n<Comment by sean-wade at 2024-10-31T09:38:34Z>\nOK, I figured it out. Just run \r\n```\r\npython convert_sqa_to_llava.py --task convert_to_llava --base_dir /data/LM/datasets/ScienceQA/ --split train\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1752,
    "state": "open",
    "created_by": "psvkaushik",
    "created_at": "2024-10-31T04:36:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1752</URL>\n\n<TITLE>Inference Issue on Fine-tuned Lora model.</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue: Running inference using finetuned lora. I alreaady ran the merge scripts.\r\nI'm running inference on mmbench. First the tqdm says it'll take 24 hours to do that, compared to 30min it usually takes. It outputs empty string as answer(\" \"). I did get the error regarding projector weights not found, but @haotian-liu said it's mostly harmless, but I included that error under log.\r\n\r\nAny suggestions on how to fix this?\r\n\r\nCommand:\r\n```\r\nsh scripts/v1_5/eval/mmbench.sh\r\n```\r\n\r\nLog: \r\n```\r\nOutput - \" \", expected an aswer like A/B....\r\nError : Some weights of the model checkpoint at llava_tuned_models/ were not used when initializing LlavaLlamaForCausalLM: ['model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj\r\n```</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1751,
    "state": "open",
    "created_by": "gaojianzhang",
    "created_at": "2024-10-29T11:59:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1751</URL>\n\n<TITLE>[Usage] can not print weight in callback function</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI have written a callback funcion to supervise the grad. But all of the outputs are tensor:[] and grad: None. How can I print the true parameters.\r\nCommand:\r\n```\r\ndef make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer,\r\n                                data_args) -> Dict:\r\n    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\r\n    train_dataset = LazySupervisedDataset(tokenizer=tokenizer,\r\n                                data_path=data_args.data_path,\r\n                                data_args=data_args)\r\n    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\r\n    return dict(train_dataset=train_dataset,\r\n                eval_dataset=None,\r\n                data_collator=data_collator)\r\n\r\ndef print_gradients(model):\r\n    \"\"\"\r\n    递归遍历模型中的所有参数并打印它们的梯度。\r\n    \"\"\"\r\n    model = model.base_model.model\r\n    for name, param in model.named_parameters():\r\n        print(f\"{name}: Weight = {param.data}, Gradient = {param.grad}\")\r\n\r\n```\r\n\r\nLog: \r\n```\r\nmodel.layers.27.self_attn.v_proj.base_layer.weight: Weight = tensor([], device='cuda:0', dtype=torch.bfloat16), Gradient = None\r\nmodel.layers.27.self_attn.v_proj.lora_A.default.weight: Weight = tensor([], device='cuda:0', dtype=torch.bfloat16), Gradient = None\r\nmodel.layers.27.self_attn.v_proj.lora_B.default.weight: Weight = tensor([], device='cuda:0', dtype=torch.bfloat16), Gradient = None\r\n```</BODY>\n\n<COMMENTS>\n<Comment by LiHaoHN at 2024-12-02T02:43:00Z>\nHello, have you found the cause of the problem? I have also encountered this problem.\n</Comment>\n<Comment by Pixel-anter at 2024-12-08T02:38:25Z>\nYou can get a global gradient print。\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1750,
    "state": "open",
    "created_by": "gaojianzhang",
    "created_at": "2024-10-29T11:22:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1750</URL>\n\n<TITLE>[Usage] loss quickly drops to near zero</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nExcuse me, I would like to ask: when fine-tuning LLaVA with LoRA on a custom dataset to create an agent for a multimodal binary choice task, the loss quickly drops to near zero during training. However, during testing, it gives the same output regardless of input, with very low accuracy. Interestingly, this issue doesn't occur when training an agent for an eight-choice task. Why might this be happening?\r\nCommand:\r\n```\r\n#!/bin/bash\r\n\r\n# 设置环境变量以禁用自动下载\r\nexport TRANSFORMERS_OFFLINE=1\r\nexport HF_FORCE_DOWNLOAD=False\r\n#export CUDA_VISIBLE_DEVICES=0,3\r\nexport WANDB_MODE=disabled\r\n\r\n\r\n# Set the prompt and model versions directly in the command\r\ndeepspeed --master_port 29502 --include localhost:0 /home/jianzhang_gao/work/llava_copy/LLaVA/llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed /home/jianzhang_gao/work/LLaVA/scripts/zero3.json \\\r\n    --model_name_or_path /home/jianzhang_gao/work/LLaVA/checkpoints/llava-v1.5-7b \\\r\n    --version v1 \\\r\n    --data_path /home/jianzhang_gao/work/agent/train_data_choosezf.json \\\r\n    --image_folder /data/sdf1/jianzhang_gao/datasets \\\r\n    --vision_tower /home/jianzhang_gao/work/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir /data/sdf1/jianzhang_gao/checkpoints/llava-v1.5-7b-qlora-choosezf_new\\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 2 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 1e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 4096 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \r\n```\r\n\r\nLog: \r\n```\r\n{'loss': 0.963, 'grad_norm': 4.4342705227056785, 'learning_rate': 2.469135802469136e-07, 'epoch': 0.0}\r\n{'loss': 0.888, 'grad_norm': 4.841685046830675, 'learning_rate': 4.938271604938272e-07, 'epoch': 0.0}\r\n{'loss': 1.0173, 'grad_norm': 5.919435240014991, 'learning_rate': 7.407407407407408e-07, 'epoch': 0.0}\r\n{'loss': 0.9541, 'grad_norm': 5.474495974051729, 'learning_rate': 9.876543209876544e-07, 'epoch': 0.0}\r\n{'loss': 0.8798, 'grad_norm': 5.1466626246551215, 'learning_rate': 1.234567901234568e-06, 'epoch': 0.0}\r\n{'loss': 0.9335, 'grad_norm': 4.8220511516679325, 'learning_rate': 1.4814814814814817e-06, 'epoch': 0.0}\r\n{'loss': 0.9582, 'grad_norm': 5.115023479668671, 'learning_rate': 1.728395061728395e-06, 'epoch': 0.0}\r\n{'loss': 0.9585, 'grad_norm': 4.462929971328918, 'learning_rate': 1.9753086419753087e-06, 'epoch': 0.0}\r\n{'loss': 0.9771, 'grad_norm': 4.560711249915778, 'learning_rate': 2.2222222222222225e-06, 'epoch': 0.0}\r\n{'loss': 0.9388, 'grad_norm': 4.655790025263413, 'learning_rate': 2.469135802469136e-06, 'epoch': 0.0}\r\n{'loss': 0.9696, 'grad_norm': 5.0228492583494955, 'learning_rate': 2.7160493827160496e-06, 'epoch': 0.0}\r\n{'loss': 0.8063, 'grad_norm': 3.83851402590431, 'learning_rate': 2.9629629629629633e-06, 'epoch': 0.0}\r\n{'loss': 0.8025, 'grad_norm': 3.9551771346273314, 'learning_rate': 3.209876543209877e-06, 'epoch': 0.0}\r\n{'loss': 0.7713, 'grad_norm': 3.6043135200140473, 'learning_rate': 3.45679012345679e-06, 'epoch': 0.0}\r\n{'loss': 0.7524, 'grad_norm': 3.3629909471943984, 'learning_rate': 3.7037037037037037e-06, 'epoch': 0.0}\r\n{'loss': 0.6855, 'grad_norm': 3.484192605038503, 'learning_rate': 3.9506172839506175e-06, 'epoch': 0.0}\r\n{'loss': 0.5346, 'grad_norm': 2.8458294386753633, 'learning_rate': 4.197530864197531e-06, 'epoch': 0.0}\r\n{'loss': 0.6012, 'grad_norm': 3.643244489250016, 'learning_rate': 4.444444444444445e-06, 'epoch': 0.0}\r\n{'loss': 0.5345, 'grad_norm': 2.5836859109644585, 'learning_rate': 4.691358024691358e-06, 'epoch': 0.0}\r\n{'loss': 0.5192, 'grad_norm': 3.4653607591195, 'learning_rate': 4.938271604938272e-06, 'epoch': 0.0}\r\n{'loss': 0.3459, 'grad_norm': 2.0753601777883817, 'learning_rate': 5.185185185185185e-06, 'epoch': 0.0}\r\n{'loss': 0.3861, 'grad_norm': 2.22823248488686, 'learning_rate': 5.432098765432099e-06, 'epoch': 0.0}\r\n{'loss': 0.3347, 'grad_norm': 2.295094163340443, 'learning_rate': 5.6790123456790125e-06, 'epoch': 0.01}\r\n{'loss': 0.3188, 'grad_norm': 2.379559165093721, 'learning_rate': 5.925925925925927e-06, 'epoch': 0.01}\r\n{'loss': 0.2183, 'grad_norm': 1.648311720008458, 'learning_rate': 6.172839506172839e-06, 'epoch': 0.01}\r\n{'loss': 0.206, 'grad_norm': 1.8461748103303606, 'learning_rate': 6.419753086419754e-06, 'epoch': 0.01}\r\n```</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1749,
    "state": "open",
    "created_by": "waybarrios",
    "created_at": "2024-10-29T05:35:23Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1749</URL>\n\n<TITLE>Invalidate trace cache @ step 4: expected module 458, but got module 466</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nI am getting this error when I am running the finetune process and `zero3.json`\r\n\r\nCommand:\r\n```\r\nbash llava/script/finetune.sh\r\n```\r\n\r\nLog: \r\n```\r\n{'loss': 10.6907, 'grad_norm': 119.82210773358251, 'learning_rate': 4.5850527281063734e-09, 'epoch': 0.0}              \r\n{'loss': 11.1586, 'grad_norm': 100.0709938853891, 'learning_rate': 9.170105456212747e-09, 'epoch': 0.0}                \r\n{'loss': 11.0917, 'grad_norm': 107.5783807489403, 'learning_rate': 1.3755158184319121e-08, 'epoch': 0.0}               \r\n  0%|                                                                           | 3/145374 [00:14<168:23:32,  4.17s/it]\r\nInvalidate trace cache @ step 4: expected module 458, but got module 466\r\n^C[2024-10-29 01:28:46,598] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 489270\r\n^CTraceback (most recent call last):\r\n  File \"/scratch/miniconda3/envs/cvpr/lib/python3.12/subprocess.py\", line 1264, in wait\r\n[2024-10-29 01:28:46,742] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 489270\r\n    return self._wait(timeout=timeout)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/scratch/miniconda3/envs/cvpr/lib/python3.12/subprocess.py\", line 2053, in _wait\r\n    (pid, sts) = self._try_wait(0)\r\n                 ^^^^^^^^^^^^^^^^^\r\n  File \"/scratch/miniconda3/envs/cvpr/lib/python3.12/subprocess.py\", line 2011, in _try_wait\r\n    (pid, sts) = os.waitpid(self.pid, wait_flags)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nKeyboardInterrupt\r\n\r\n```\r\n\r\nAny suggestion?</BODY>\n\n<COMMENTS>\n<Comment by Confetti-lxy at 2025-01-09T18:24:24Z>\nHello, have you solved this problem?\n</Comment>\n<Comment by youngwanLEE at 2025-01-17T02:03:26Z>\n+1\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1748,
    "state": "open",
    "created_by": "t-mockbel",
    "created_at": "2024-10-28T10:38:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1748</URL>\n\n<TITLE>[Usage] ValueError</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue: I'm trying to use llava-1.5-7b-hf and i'm new and clueless in debugging LMMs. Ihave an error when i try to use the simple example of usage:\r\nraise ValueError(\r\nValueError: The input provided to the model are wrong. The number of image tokens is 100 while the number of image given to the model is 1. This prevents correct indexing and breaks batch generation.\r\n\r\nAnd i really don't get it.\r\n\r\nCommand:\r\nport requests\r\nfrom PIL import Image\r\nimport torch\r\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\r\n\r\nmodel_id = \"llava-hf/llava-1.5-7b-hf\"\r\nmodel = LlavaForConditionalGeneration.from_pretrained(\r\nmodel_id,\r\ntorch_dtype=torch.float16\r\n).to(0)\r\n\r\nprocessor = AutoProcessor.from_pretrained(model_id, patch_size = 32 , vision_feature_select_strategy = 'default')\r\nimage_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\r\nraw_image = Image.open(requests.get(image_file, stream=True).raw)\r\nconversation = [\r\n{\r\n\"role\": \"user\",\r\n\"content\": [\r\n{\"type\": \"image\"},\r\n{\"type\": \"text\", \"text\": \"What is shown in this image?\"},\r\n],\r\n},\r\n] \r\nprompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\r\n\r\n\r\ninputs = processor(images=raw_image, text=prompt, return_tensors='pt').to(0, dtype=torch.float16)\r\n\r\noutput = model.generate(**inputs, max_new_tokens=200, do_sample=False)\r\nprint(processor.decode(output[0][2:], skip_special_tokens=True))\r\n\r\nBUT I ALSO HAVE THIS:\r\nenvs\\myenv\\lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:540: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at\r\nC:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\r\nattn_output = torch.nn.functional.scaled_dot_product_attention(\r\nExpanding inputs for image tokens in LLaVa should be done in processing\r\n. Please add patch_size and vision_feature_select_strategy to the m\r\nodel's processing config or set directly with processor.patch_size = { {patch_size}} and processor.vision_feature_select_strategy = {{vision_\r\nfeature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.</BODY>\n\n<COMMENTS>\n<Comment by AsteriaCao at 2024-11-05T03:42:45Z>\nI solved this problem by adding 2 lines when in llava-1.5-7b-hf initialization: \r\n\r\n`self.processor.patch_size = self.model.config.vision_config.patch_size`\r\n\r\n`self.processor.vision_feature_select_strategy = self.model.config.vision_feature_select_strategy`\r\n\r\nThe code above means that I point out the patch_size and vision_feature_select_strategy manually using the same values from model.config.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1747,
    "state": "open",
    "created_by": "lyklly",
    "created_at": "2024-10-25T03:41:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1747</URL>\n\n<TITLE>[Question] SEED Bench video frame extract</TITLE>\n\n<BODY>### Question\n\nnotice that we should extract video frame from downloaded video. However, I have no ffmpeg in my linux , and I have no permission to install in my linux .  Could anybody provide the frame which have already been extracted?</BODY>\n\n<COMMENTS>\n<Comment by kinredon at 2025-03-21T07:39:23Z>\n@lyklly Do you solve this problem?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1746,
    "state": "open",
    "created_by": "zhangsha1024",
    "created_at": "2024-10-24T14:30:12Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1746</URL>\n\n<TITLE>Process Hang on when fine-tuning in multi-GPUs by using zero-3</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nThe log shows no ERROR information, but just hang on.\r\n\r\nProcess Hang on when I fine-tune in multi-GPUs by using zero-3, but it is fine by using zero-2, and one GPU by using zero-3</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1745,
    "state": "open",
    "created_by": "Silverasdf",
    "created_at": "2024-10-23T19:03:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1745</URL>\n\n<TITLE>[Question] Having an issue using the merge weights script.</TITLE>\n\n<BODY>### Question\r\n\r\nHello,\r\n\r\nSo I've fine-tuned a model using the `finetune-lora.sh` script and I ended up getting an error. Is there anyway I can fix this?\r\n\r\nFrom the looks of it, the Traceback is from a python library rather than the code I could try modifying, but I figured it would be worth asking anyways.\r\n\r\nI want to go ahead and [link](https://github.com/haotian-liu/LLaVA/issues/1726) the previous issue I made for any missing context here.\r\n\r\nThanks!\r\n\r\n```py\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py\", line 558, in save_pretrained\r\n    raise ValueError(str([w.message for w in caught_warnings]))\r\nValueError: [UserWarning('`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.'), UserWarning('`do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.')]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/rdpperuski/LLaVA/scripts/merge_lora_weights.py\", line 22, in <module>\r\n    merge_lora(args)\r\n  File \"/home/rdpperuski/LLaVA/scripts/merge_lora_weights.py\", line 10, in merge_lora\r\n    model.save_pretrained(args.save_model_path)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2364, in save_pretrained\r\n    model_to_save.generation_config.save_pretrained(save_directory)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py\", line 560, in save_pretrained\r\n    raise ValueError(\r\nValueError: The generation config instance is invalid -- `.validate()` throws warnings and/or exceptions. Fix these issues to save the configuration.\r\n\r\nThrown during validation:\r\n[UserWarning('`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.'), UserWarning('`do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.')]```</BODY>\n\n<COMMENTS>\n<Comment by shadowdyj at 2024-10-28T02:55:09Z>\n@Silverasdf same issue. do you solve it?\n</Comment>\n<Comment by shadowdyj at 2024-10-28T03:05:35Z>\n> #1635 \r\nit works.\n</Comment>\n<Comment by Silverasdf at 2024-10-28T14:22:54Z>\nNo, I'm still having an issue with it.\r\n\r\n![image](https://github.com/user-attachments/assets/b35a1954-797a-4923-9d8c-1365079225b1)\r\n\r\nI changed the code however, and I still get the same issue:\r\n\r\n```py\r\nModel is loaded...\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py\", line 558, in save_pretrained\r\n    raise ValueError(str([w.message for w in caught_warnings]))\r\nValueError: [UserWarning('`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.'), UserWarning('`do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.')]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/rdpperuski/LLaVA/scripts/merge_lora_weights.py\", line 22, in <module>\r\n    merge_lora(args)\r\n  File \"/home/rdpperuski/LLaVA/scripts/merge_lora_weights.py\", line 10, in merge_lora\r\n    model.save_pretrained(args.save_model_path)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2364, in save_pretrained\r\n    model_to_save.generation_config.save_pretrained(save_directory)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py\", line 560, in save_pretrained\r\n    raise ValueError(\r\nValueError: The generation config instance is invalid -- `.validate()` throws warnings and/or exceptions. Fix these issues to save the configuration.\r\n\r\nThrown during validation:\r\n[UserWarning('`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.'), UserWarning('`do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.')]```\n</Comment>\n<Comment by YUECHE77 at 2024-11-02T20:05:18Z>\nYou can try to downgrade transformers to 4.36.2, this should solve the issue.\r\n\r\nBut even successfully merge the lora weights with the base model, I still cannot run CLI with merged model.\n</Comment>\n<Comment by xiaomatiaobugao at 2024-12-20T13:29:18Z>\n@YUECHE77 hello,I wanna know the basic model you mentioned, is the basic moedel like llava-v1.5-13b/7b，or vicuna-13b-v1.5.plz\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1744,
    "state": "closed",
    "created_by": "BinTangHuLu",
    "created_at": "2024-10-22T06:51:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1744</URL>\n\n<TITLE>[Question] 打开网页后显示：error Could not parse server response: SyntaxError: Unexpected token 'I', \"Internal S\"... is not valid JSON</TITLE>\n\n<BODY>### Question\n\ngradio_web_server显示：\r\npython -m llava.serve.gradio_web_server --controller http://localhost:10001 --model-list-mode reload                                                         \r\nThe installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\r\n2024-10-22 14:46:23 | INFO | gradio_web_server | args: Namespace(host='0.0.0.0', port=None, controller_url='http://localhost:10001', concurrency_count=16, model_list_mode='reload', share=False, moderate=False, embed=False)\r\n2024-10-22 14:46:32 | INFO | gradio_web_server | Models: []\r\n2024-10-22 14:46:32 | INFO | gradio_web_server | Namespace(host='0.0.0.0', port=None, controller_url='http://localhost:10001', concurrency_count=16, model_list_mode='reload', share=False, moderate=False, embed=False)\r\n2024-10-22 14:46:32 | ERROR | stderr | E:\\Anaconda\\envs\\llava\\lib\\site-packages\\gradio\\components\\dropdown.py:163: UserWarning: The value passed into gr.Dropdown() is not in the list of choices. Please update the list of choices to include:  or set allow_custom_value=True.\r\n2024-10-22 14:46:32 | ERROR | stderr |   warnings.warn(\r\n2024-10-22 14:46:32 | INFO | stdout | Running on local URL:  http://0.0.0.0:7860\r\n2024-10-22 14:46:32 | INFO | httpx | HTTP Request: GET http://localhost:7860/startup-events \"HTTP/1.1 200 OK\"\r\n2024-10-22 14:46:32 | INFO | httpx | HTTP Request: HEAD http://localhost:7860/ \"HTTP/1.1 200 OK\"\r\n2024-10-22 14:46:32 | INFO | stdout | \r\n2024-10-22 14:46:32 | INFO | stdout | To create a public link, set `share=True` in `launch()`.\r\n2024-10-22 14:46:33 | INFO | httpx | HTTP Request: GET https://checkip.amazonaws.com/ \"HTTP/1.1 200 \"\r\n2024-10-22 14:46:33 | INFO | httpx | HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\r\n2024-10-22 14:46:33 | INFO | stdout | IMPORTANT: You are using gradio version 4.16.0, however version 4.44.1 is available, please upgrade.\r\n2024-10-22 14:46:33 | INFO | stdout | --------\r\n2024-10-22 14:46:33 | INFO | httpx | HTTP Request: GET https://checkip.amazonaws.com/ \"HTTP/1.1 200 \"\r\n2024-10-22 14:46:34 | INFO | httpx | HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ \"HTTP/1.1 200 OK\"\r\n2024-10-22 14:46:35 | INFO | httpx | HTTP Request: POST https://api.gradio.app/gradio-launched-telemetry/ \"HTTP/1.1 200 OK\"\r\n2024-10-22 14:50:10 | ERROR | stderr | ERROR:    Exception in ASGI application\r\n2024-10-22 14:50:10 | ERROR | stderr | Traceback (most recent call last):\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\pydantic\\type_adapter.py\", line 270, in _init_core_attrs\r\n2024-10-22 14:50:10 | ERROR | stderr |     self._core_schema = _getattr_no_parents(self._type, '__pydantic_core_schema__')\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\pydantic\\type_adapter.py\", line 112, in _getattr_no_parents\r\n2024-10-22 14:50:10 | ERROR | stderr |     raise AttributeError(attribute)\r\n2024-10-22 14:50:10 | ERROR | stderr | AttributeError: __pydantic_core_schema__\r\n2024-10-22 14:50:10 | ERROR | stderr |\r\n2024-10-22 14:50:10 | ERROR | stderr | During handling of the above exception, another exception occurred:\r\n2024-10-22 14:50:10 | ERROR | stderr |\r\n2024-10-22 14:50:10 | ERROR | stderr | Traceback (most recent call last):\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 406, in run_asgi\r\n2024-10-22 14:50:10 | ERROR | stderr |     result = await app(  # type: ignore[func-returns-value]\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\r\n2024-10-22 14:50:10 | ERROR | stderr |     return await self.app(scope, receive, send)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\r\n2024-10-22 14:50:10 | ERROR | stderr |     await super().__call__(scope, receive, send)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\starlette\\applications.py\", line 113, in __call__\r\n2024-10-22 14:50:10 | ERROR | stderr |     await self.middleware_stack(scope, receive, send)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 187, in __call__\r\n2024-10-22 14:50:10 | ERROR | stderr |     raise exc\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 165, in __call__\r\n2024-10-22 14:50:10 | ERROR | stderr |     await self.app(scope, receive, _send)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\starlette\\middleware\\cors.py\", line 93, in __call__\r\n2024-10-22 14:50:10 | ERROR | stderr |     await self.simple_response(scope, receive, send, request_headers=headers)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\starlette\\middleware\\cors.py\", line 144, in simple_response\r\n2024-10-22 14:50:10 | ERROR | stderr |     await self.app(scope, receive, send)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\r\n2024-10-22 14:50:10 | ERROR | stderr |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\starlette\\_exception_handler.py\", line 62, in wrapped_app\r\n2024-10-22 14:50:10 | ERROR | stderr |     raise exc\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\starlette\\_exception_handler.py\", line 51, in wrapped_app\r\n2024-10-22 14:50:10 | ERROR | stderr |     await app(scope, receive, sender)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\starlette\\routing.py\", line 715, in __call__\r\n2024-10-22 14:50:10 | ERROR | stderr |     await self.middleware_stack(scope, receive, send)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\starlette\\routing.py\", line 735, in app\r\n2024-10-22 14:50:10 | ERROR | stderr |     await route.handle(scope, receive, send)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\starlette\\routing.py\", line 288, in handle\r\n2024-10-22 14:50:10 | ERROR | stderr |     await self.app(scope, receive, send)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\starlette\\routing.py\", line 76, in app\r\n2024-10-22 14:50:10 | ERROR | stderr |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\starlette\\_exception_handler.py\", line 62, in wrapped_app\r\n2024-10-22 14:50:10 | ERROR | stderr |     raise exc\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\starlette\\_exception_handler.py\", line 51, in wrapped_app\r\n2024-10-22 14:50:10 | ERROR | stderr |     await app(scope, receive, sender)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\starlette\\routing.py\", line 73, in app\r\n2024-10-22 14:50:10 | ERROR | stderr |     response = await f(request)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\fastapi\\routing.py\", line 291, in app\r\n2024-10-22 14:50:10 | ERROR | stderr |     solved_result = await solve_dependencies(\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\fastapi\\dependencies\\utils.py\", line 658, in solve_dependencies\r\n2024-10-22 14:50:10 | ERROR | stderr |     ) = await request_body_to_args(  # body_params checked above\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\fastapi\\dependencies\\utils.py\", line 883, in request_body_to_args\r\n2024-10-22 14:50:10 | ERROR | stderr |     fields_to_extract = get_cached_model_fields(first_field.type_)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\fastapi\\_compat.py\", line 658, in get_cached_model_fields\r\n2024-10-22 14:50:10 | ERROR | stderr |     return get_model_fields(model)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\fastapi\\_compat.py\", line 284, in get_model_fields\r\n2024-10-22 14:50:10 | ERROR | stderr |     return [\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\fastapi\\_compat.py\", line 285, in <listcomp>\r\n2024-10-22 14:50:10 | ERROR | stderr |     ModelField(field_info=field_info, name=name)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"<string>\", line 6, in __init__\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\fastapi\\_compat.py\", line 110, in __post_init__\r\n2024-10-22 14:50:10 | ERROR | stderr |     self._type_adapter: TypeAdapter[Any] = TypeAdapter(\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\pydantic\\type_adapter.py\", line 257, in __init__\r\n2024-10-22 14:50:10 | ERROR | stderr |     self._init_core_attrs(rebuild_mocks=False)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\pydantic\\type_adapter.py\", line 135, in wrapped\r\n2024-10-22 14:50:10 | ERROR | stderr |     return func(self, *args, **kwargs)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\pydantic\\type_adapter.py\", line 277, in _init_core_attrs\r\n2024-10-22 14:50:10 | ERROR | stderr |     self._core_schema = _get_schema(self._type, config_wrapper, parent_depth=self._parent_depth)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\pydantic\\type_adapter.py\", line 95, in _get_schema\r\n2024-10-22 14:50:10 | ERROR | stderr |     schema = gen.generate_schema(type_)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 655, in generate_schema\r\n2024-10-22 14:50:10 | ERROR | stderr |     schema = self._generate_schema_inner(obj)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 908, in _generate_schema_inner\r\n2024-10-22 14:50:10 | ERROR | stderr |     return self._annotated_schema(obj)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 2028, in _annotated_schema\r\n2024-10-22 14:50:10 | ERROR | stderr |     schema = self._apply_annotations(source_type, annotations)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 2107, in _apply_annotations\r\n2024-10-22 14:50:10 | ERROR | stderr |     schema = get_inner_schema(source_type)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py\", line 83, in __call__\r\n2024-10-22 14:50:10 | ERROR | stderr |     schema = self._handler(source_type)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 2189, in new_handler\r\n2024-10-22 14:50:10 | ERROR | stderr |     schema = metadata_get_schema(source, get_inner_schema)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 2185, in <lambda>\r\n2024-10-22 14:50:10 | ERROR | stderr |     lambda source, handler: handler(source)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py\", line 83, in __call__\r\n2024-10-22 14:50:10 | ERROR | stderr |     schema = self._handler(source_type)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 2088, in inner_handler\r\n2024-10-22 14:50:10 | ERROR | stderr |     schema = self._generate_schema_inner(obj)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 929, in _generate_schema_inner\r\n2024-10-22 14:50:10 | ERROR | stderr |     return self.match_type(obj)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1029, in match_type\r\n2024-10-22 14:50:10 | ERROR | stderr |     return self._match_generic_type(obj, origin)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1058, in _match_generic_type\r\n2024-10-22 14:50:10 | ERROR | stderr |     return self._union_schema(obj)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1378, in _union_schema\r\n2024-10-22 14:50:10 | ERROR | stderr |     choices.append(self.generate_schema(arg))\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 655, in generate_schema\r\n2024-10-22 14:50:10 | ERROR | stderr |     schema = self._generate_schema_inner(obj)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 929, in _generate_schema_inner\r\n2024-10-22 14:50:10 | ERROR | stderr |     return self.match_type(obj)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1038, in match_type\r\n2024-10-22 14:50:10 | ERROR | stderr |     return self._unknown_type_schema(obj)\r\n2024-10-22 14:50:10 | ERROR | stderr |   File \"E:\\Anaconda\\envs\\llava\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 558, in _unknown_type_schema\r\n2024-10-22 14:50:10 | ERROR | stderr |     raise PydanticSchemaGenerationError(\r\n2024-10-22 14:50:10 | ERROR | stderr | pydantic.errors.PydanticSchemaGenerationError: Unable to generate pydantic-core schema for <class 'starlette.requests.Request'>. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\r\n2024-10-22 14:50:10 | ERROR | stderr |\r\n2024-10-22 14:50:10 | ERROR | stderr | If you got this error by calling handler(<some type>) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(<some type>)` since we do not call `__get_pydantic_core_schema__` on `<some type>` otherwise to avoid infinite recursion.\r\n2024-10-22 14:50:10 | ERROR | stderr |\r\n2024-10-22 14:50:10 | ERROR | stderr | For further information visit https://errors.pydantic.dev/2.9/u/schema-for-unknown-type</BODY>\n\n<COMMENTS>\n<Comment by YixFeng at 2024-10-23T02:51:55Z>\nI met the same error. How did you fix it out?\n</Comment>\n<Comment by trinn22 at 2024-10-23T09:06:07Z>\nThis worked for me\r\n(mi_env) fv@fv01:~/LLaVA$ pip install gradio -U\n</Comment>\n<Comment by ErwinCheung at 2025-03-01T06:36:52Z>\n> This worked for me (mi_env) fv@fv01:~/LLaVA$ pip install gradio -U\n\nThank you very much. I have been working on this issue for a long time, but when I saw your solution, I upgraded Gradio and it was immediately resolved.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1742,
    "state": "open",
    "created_by": "cjc20000323",
    "created_at": "2024-10-21T09:09:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1742</URL>\n\n<TITLE>[Question] How to save lora model with fine-tuning parameters?</TITLE>\n\n<BODY>### Question\n\nAfter fine-tuning with lora, I get the files below. Are these files the training target or is there something wrong with my scripts?\r\n![微信截图_20241021170636](https://github.com/user-attachments/assets/646f8045-0be3-4fbd-9745-c804be0fe888)\r\nMy script file is below:\r\n\r\n\r\n#!/bin/bash\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path ./checkpoints/vicuna-v1-5-7b \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/llava_v1_5_mix665k.json \\\r\n    --image_folder ./playground/data \\\r\n    --vision_tower ./checkpoints/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-7b-pretrain-local_32_2_4/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b_32_4_2_16_4_2_lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 2 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True</BODY>\n\n<COMMENTS>\n<Comment by ybrrraway at 2025-04-10T06:32:04Z>\nyou can try merge first to see if there any problem,\n</Comment>\n<Comment by cjc20000323 at 2025-04-10T06:32:37Z>\n您好，我已收到您的邮件，请您知悉。\n</Comment>\n<Comment by shaswatpatel123 at 2025-04-30T09:22:19Z>\nYou need adapters.json if you aren’t merging and saving the model. adapter.json is where your LoRa weights get stored which you can then load for again using huggingface PeftModel from pretrained by basing base_model(llava) and path to your checkpoint\n</Comment>\n<Comment by cjc20000323 at 2025-04-30T09:22:49Z>\n您好，我已收到您的邮件，请您知悉。\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1741,
    "state": "open",
    "created_by": "yangzhipeng1108",
    "created_at": "2024-10-21T01:29:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1741</URL>\n\n<TITLE>[Question] When loading training image data, is it loaded into memory all at once, or is it loaded from disk to memory according to batch_size?</TITLE>\n\n<BODY>### Question\n\nWhen loading training image data, is it loaded into memory all at once, or is it loaded from disk to memory according to batch_size?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1740,
    "state": "open",
    "created_by": "freya-7",
    "created_at": "2024-10-19T09:27:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1740</URL>\n\n<TITLE>[Usage] Encountered an issue while installing the environment with 'pip install flash-attn --no-build-isolation“</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:“pip install flash-attn --no-build-isolation”\r\n“Collecting flash-attn\r\n  Using cached flash_attn-2.6.3.tar.gz (2.6 MB)\r\n  Preparing metadata (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  × python setup.py egg_info did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> [8 lines of output]\r\n      Traceback (most recent call last):\r\n        File \"<string>\", line 2, in <module>\r\n        File \"<pip-setuptools-caller>\", line 34, in <module>\r\n        File \"/tmp/pip-install-e17h729i/flash-attn_04e1e09780f64807ac90bf9b6ff6b492/setup.py\", line 21, in <module>\r\n          import torch\r\n        File \"/home/me/enter/envs/llava/lib/python3.10/site-packages/torch/__init__.py\", line 235, in <module>\r\n          from torch._C import *  # noqa: F403\r\n      ImportError: /home/me/enter/envs/llava/lib/python3.10/site-packages/torch/lib/../../nvidia/cusparse/lib/libcusparse.so.12: undefined symbol: __nvJitLinkAddData_12_1, version libnvJitLink.so.12\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n× Encountered error while generating package metadata.\r\n╰─> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details.\r\n”\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by hotpot-killer at 2024-10-23T05:34:12Z>\nsame question\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1739,
    "state": "closed",
    "created_by": "Bleking",
    "created_at": "2024-10-18T03:32:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1739</URL>\n\n<TITLE>[Question] How can I create optimizer.pt file for the checkpoint when I finetune the model?</TITLE>\n\n<BODY>### Question\n\nI would like to create checkpoints so I can try gradually increasing the epoch and find out when the model start to overfit. I heard that optimizer.pt file is required in order to start from a specific checkpoint defined by 'resume_from_checkpoint' argument.\r\n\r\nWhenever I form a checkpoint, it appears like this\r\n![image](https://github.com/user-attachments/assets/82a53634-42c7-4c88-8b11-f3f517b6e5c7)\r\nwithout any optimizer.pt.\r\n\r\nI have tried adding `torch.save(trainer.optimizer.state_dict(), os.path.join(training_args.output_dir, 'optimizer.pt'))` line at the bottom of the 'train' function of 'train.py' code but neither does it work.\r\n\r\nThis is my script for your information:\r\n\r\n> #!/bin/bash\r\n> \r\n> deepspeed llava/train/train_xformers.py \\\r\n>     --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n>     --deepspeed ./scripts/zero3_offload.json \\\r\n>     --model_name_or_path liuhaotian/llava-v1.5-13b \\\r\n>     --version v1 \\\r\n>     --data_path ./playground/data/train.json \\\r\n>     --valid_data_path ./playground/data/valid.json \\\r\n>     --image_folder ./playground/data/ \\\r\n>     --vision_tower openai/clip-vit-large-patch14-336 \\\r\n>     --mm_projector_type mlp2x_gelu \\\r\n>     --mm_vision_select_layer -2 \\\r\n>     --mm_use_im_start_end False \\\r\n>     --mm_use_im_patch_token False \\\r\n>     --image_aspect_ratio pad \\\r\n>     --group_by_modality_length True \\\r\n>     --fp16 True \\\r\n>     --output_dir ./checkpoints/results \\\r\n>     --num_train_epochs 1 \\\r\n>     --per_device_train_batch_size 4 \\\r\n>     --per_device_eval_batch_size 4 \\\r\n>     --gradient_accumulation_steps 8 \\\r\n>     --evaluation_strategy \"steps\" \\\r\n>     --save_strategy \"steps\" \\\r\n>     --save_steps 1000 \\\r\n>     --save_total_limit 1 \\\r\n>     --learning_rate 2e-5 \\\r\n>     --weight_decay 0.01 \\\r\n>     --warmup_ratio 0.03 \\\r\n>     --lr_scheduler_type \"cosine\" \\\r\n>     --logging_steps 1 \\\r\n>     --tf32 False \\\r\n>     --model_max_length 2048 \\\r\n>     --gradient_checkpointing True \\\r\n>     --dataloader_num_workers 4 \\\r\n>     --lazy_preprocess True \\\r\n>     --report_to wandb\r\n>     --load_best_model_at_end True \\\r\n>     --resume_from_checkpoint ./checkpoints/results/checkpoint-##\r\n\r\n\r\nHow did you guys get to create a checkpoint to continue finetuning from? What should I do with the script and the 'train.py' code? Any advice?\r\n\r\nThank you.</BODY>\n\n<COMMENTS>\n<Comment by XindiWu at 2024-10-28T22:05:47Z>\nThe optimizer states are saved by DeepSpeed, and when using DeepSpeed with multiple GPUs, it splits the optimizer state across different ranks (one file per GPU). You don't need to modify the saving code because DeepSpeed is already handling the optimizer state saving correctly. The saved optimizer states are under ./global_step6 folder.\n</Comment>\n<Comment by Bleking at 2024-10-29T02:09:55Z>\nThank you for the information. Well, I had tried adding `torch.save(trainer.optimizer.state_dict(), os.path.join(training_args.output_dir, 'optimizer.pt'))` line at the bottom of the \"train\" function in the [train.py](https://github.com/haotian-liu/LLaVA/blob/main/llava/train/train.py) code but it looks like it was not a necessary act.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1738,
    "state": "open",
    "created_by": "OedoSoldier",
    "created_at": "2024-10-17T05:24:50Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1738</URL>\n\n<TITLE>[Question] Question about `--image_aspect_ratio pad`</TITLE>\n\n<BODY>### Question\n\nThe README said: \"This pads the non-square images to a square, instead of cropping them; it slightly reduces hallucination.\" However, for `openai/clip-vit-large-patch14-336`, the preprocessing function defines two steps that will prepare raw images in the format expected by the CLIP model: one `resize` step and one `center_crop` step:\r\n\r\nhttps://github.com/huggingface/transformers/blob/3f06f95ebe617b192251ef756518690f5bc7ff76/src/transformers/models/clip/image_processing_clip.py#L324-L328\r\n\r\nThe sizes for both steps are the same as `336`; you can find it here: \r\n\r\nhttps://huggingface.co/openai/clip-vit-large-patch14-336/blob/ce19dc912ca5cd21c8a653c79e251e808ccabcd1/preprocessor_config.json#L2\r\n\r\nhttps://huggingface.co/openai/clip-vit-large-patch14-336/blob/ce19dc912ca5cd21c8a653c79e251e808ccabcd1/preprocessor_config.json#L18\r\n\r\nThis means that the CLIPImageProcessor indeed do not crop any image, it just resizes it. So why pad is more efficient than resize?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1737,
    "state": "closed",
    "created_by": "cookiesupers22",
    "created_at": "2024-10-16T18:33:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1737</URL>\n\n<TITLE>[Question] Not able to load https://huggingface.co/liuhaotian/llava-lcs558k-scienceqa-vicuna-13b-v1.3 into google colab. Please help 🙏🙏</TITLE>\n\n<BODY>### Question\n\n# Load model directly\r\nfrom transformers import AutoProcessor, AutoModelForCausalLM\r\n\r\nprocessor = AutoProcessor.from_pretrained(\"liuhaotian/llava-lcs558k-scienceqa-vicuna-13b-v1.3\")\r\nmodel = AutoModelForCausalLM.from_pretrained(\"liuhaotian/llava-lcs558k-scienceqa-vicuna-13b-v1.3\")\r\nI am trying to run the above code in google colab from this huggingface Llava model: https://huggingface.co/liuhaotian/llava-lcs558k-scienceqa-vicuna-13b-v1.3\r\n\r\nFor some reason, I keep getting this error I have no idea what to do:\r\n\r\nTraceback (most recent call last)\r\n\r\nHTTPError               Traceback (most recent call last)\r\n\r\n\r\n in hf_raise_for_status(response, endpoint_name)\r\n    405     try:\r\n--> 406         response.raise_for_status()\r\n    407     except HTTPError as e:\r\n\r\n/usr/local/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\r\n\r\n\r\n in hf_raise_for_status(response, endpoint_name)\r\n    405     try:\r\n--> 406         response.raise_for_status()\r\n    407     except HTTPError as e:\r\n\r\n/usr/local/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\r\n18 frames\r\n\r\nHTTPError: 404 Client Error: Not Found for url: \r\nhttps://huggingface.co/liuhaotian/llava-lcs558k-scienceqa-vicuna-13b-v1.3/resolve/main/preprocessor_config.json\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\n\r\n\r\nEntryNotFoundError                        Traceback (most recent call last)\r\n\r\n\r\nEntryNotFoundError: 404 Client Error. (Request ID: Root=1-67100499-78a68eb3676bcc6b7eaa4c27;c4259ef9-af93-4126-8bb4-561929fbfae1)\r\n\r\nEntry Not Found for url: .\r\nhttps://huggingface.co/liuhaotian/llava-lcs558k-scienceqa-vicuna-13b-v1.3/resolve/main/preprocessor_config.json\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\n\r\n\r\nOSError                                   Traceback (most recent call last)\r\n\r\n\r\n in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\r\n    434         if revision is None:\r\n    435             revision = \"main\"\r\n--> 436         raise EnvironmentError(\r\n    437             f\"{path_or_repo_id} does not appear to have a file named {full_filename}. Checkout \"\r\n    438             f\"' for available files.\"\r\n\r\n/usr/local/lib/python3.10/site-packages/transformers/utils/hub.pyhttps://huggingface.co/{path_or_repo_id}/{revision}'\r\n\r\nOSError: liuhaotian/llava-lcs558k-scienceqa-vicuna-13b-v1.3 does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/liuhaotian/llava-lcs558k-scienceqa-vicuna-13b-v1.3/main' for available files.</BODY>\n\n<COMMENTS>\n<Comment by bansal19 at 2024-10-16T22:12:31Z>\nHaving the very same issue!\n</Comment>\n<Comment by cookiesupers22 at 2024-10-27T03:09:30Z>\n> Having the very same issue!\r\n\r\nTry this. Just got it working for me.\r\n#works with scienceqa checkpoints one\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.mm_utils import get_model_name_from_path\r\nfrom llava.eval.run_llava import eval_model\r\n\r\n\r\nmodel_path = \"liuhaotian/llava-lcs558k-scienceqa-vicuna-13b-v1.3\"\r\n\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    model_path=model_path,\r\n    model_base=None,\r\n    model_name=get_model_name_from_path(model_path)\r\n)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1735,
    "state": "closed",
    "created_by": "haiduo",
    "created_at": "2024-10-15T02:37:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1735</URL>\n\n<TITLE>[Usage] 。。。</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue: I want to know what is the relationship between efficient models and LLAVA? Is it necessary to scale up the model for end-device deployment?\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1734,
    "state": "open",
    "created_by": "narayanasastry-rvds",
    "created_at": "2024-10-15T01:14:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1734</URL>\n\n<TITLE>[Usage] Not able to save model weights after Lora Finetuning</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: Lora finetuning with Zero2.json and also Zero3.json. During finetuning, train and validation loss are reducing but when I see the weights in saved model checkpoint, it has only initialised weights. \r\n\r\nCommand:\r\n```\r\nModel finetuning is done using: sh finetune_task_lora.sh\r\n```\r\n\r\nLog: \r\n```\r\nWith **Zero3.json**, all weights are same as initialisation:\r\nVision Lora A mean values: tensor(5.9843e-05, device='cuda:0', dtype=torch.bfloat16)\r\nNon-vision Lora A mean values: tensor(5.1260e-05, device='cuda:0', dtype=torch.bfloat16)\r\nVision Lora B sum values: tensor(0., device='cuda:0', dtype=torch.bfloat16)\r\nNon-vision Lora B sum values: tensor(0., device='cuda:0', dtype=torch.bfloat16)\r\n\r\nWith **Zero2.json**, only weights of Lora-B Non-vision values have changed from initialisation:\r\nVision Lora A mean values: tensor(5.9843e-05, device='cuda:0', dtype=torch.bfloat16)\r\nNon-vision Lora A mean values: tensor(5.1260e-05, device='cuda:0', dtype=torch.bfloat16)\r\nVision Lora B sum values: tensor(0., device='cuda:0', dtype=torch.bfloat16)\r\nNon-vision Lora B sum values: tensor(**2.1562**, device='cuda:0', dtype=torch.bfloat16)\r\n```\r\n\r\nScreenshots:\r\nTrain and Validation loss during Model finetuning in below plot:\r\n![image](https://github.com/user-attachments/assets/da862194-b4c5-4b32-a6a0-d264a47a7634)</BODY>\n\n<COMMENTS>\n<Comment by Silverasdf at 2024-10-18T14:40:31Z>\nI had the same issue. Your checkpoint is, by default, saved in \"./checkpoints\". Then I ended up just checking the modification times to find the right one, lol. Then, you can [merge](https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md).\r\n\r\nHope this helps.\n</Comment>\n<Comment by narayanasastry-rvds at 2024-10-21T08:15:10Z>\nThanks for the reply. I have been passing correct checkpoints folder only. I haven't been using merge script from here as it is throwing error like this:\r\n\r\nOSError: LLava_fine_tune/checkpoints/llava-v1.6-7b-lora/checkpoint-159 does not appear to have a file named config.json. Checkout 'https://huggingface.co/LLava_fine_tune/checkpoints/llava-v1.6-7b-lora/checkpoint-159/tree/main' for available files.\r\n\r\nI think it is referring to Huggingface for the checkpoint but I have it on my local. Do you know how to fix this?\n</Comment>\n<Comment by Silverasdf at 2024-10-22T13:36:19Z>\nI had this issue too. Look into the script: finetune_task_lora.sh. Copy and paste what you have for your model within that script, and it should work. I had to do that. Also, perhaps you are giving the wrong relative path? Try giving it an absolute path for your checkpoints.\n</Comment>\n<Comment by narayanasastry-rvds at 2024-10-24T05:05:40Z>\nThanks for the reply again. Now, I am able to merge the checkpoints. But next problem is: Model is not giving any output after \"Assistant: \". Whereas base model is generating some response.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1733,
    "state": "open",
    "created_by": "WeitaiKang",
    "created_at": "2024-10-15T00:47:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1733</URL>\n\n<TITLE>[Question] Which phase should we submit to evaluate the VisWiz in the paper?</TITLE>\n\n<BODY>### Question\r\n\r\nAs described by the paper, I see there many phases for VisWiz benchmark to submit. Could anyone point out the exact phase we should submit?\r\n\r\n![image](https://github.com/user-attachments/assets/2d8910fc-74f4-490c-b43f-b4804ae66584)</BODY>\n\n<COMMENTS>\n<Comment by ArieSeirack at 2024-10-16T02:24:14Z>\nI choose the **test-standard2024-VQA phase**, and then submit my json file. Then I go to the My Submission page to check the result file for the results. I get 54.3 on ''overall'' metric with llava-1.5-13b, which is close to the 53.6 in the paper.\n</Comment>\n<Comment by WeitaiKang at 2024-10-16T02:27:05Z>\nThanks for your data point. However, I tried the **test-dev2024-VQA** and got \"overall\" 51.36, which is close to llava-1.5-7B's 50.0. So I am still confused lol.\n</Comment>\n<Comment by jaydetang at 2024-10-31T05:10:21Z>\nhello, have you solved it? Should I submitted to the **test-standard2024-VQA phase** or **test-dev2024-VQA** ?\n</Comment>\n<Comment by lst627 at 2024-12-23T09:22:38Z>\nBased on [Vizwiz's official page](https://vizwiz.org/tasks-and-datasets/vqa/), test-dev consists of 4,000 test visual questions and test-standard contains all 8,000 visual questions in the test dataset. I found that there are 8,000 questions in llava_test.jsonl, so I think test-standard2024-VQA phase is the correct one.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1732,
    "state": "open",
    "created_by": "yuanllong",
    "created_at": "2024-10-14T09:10:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1732</URL>\n\n<TITLE>[Question] Which code is the Loss of the llava model? I didn't find it?</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by XiaotongLiuuuuu at 2024-11-09T08:13:07Z>\nsame quation. Have you solved it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1731,
    "state": "open",
    "created_by": "Netceor",
    "created_at": "2024-10-14T02:31:56Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1731</URL>\n\n<TITLE>Caught ValueError: Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.</TITLE>\n\n<BODY>### Question\n\nWhen I input both images and text, the page displays the following message:\r\n\"NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE. (error_code: 1)\",\r\nand the terminal shows the following error:\r\n\"Caught ValueError: Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.\"\r\n\r\nThe detailed logs are as follows:\r\n\r\n`2024-10-14 10:27:35 | INFO | stdout | INFO:     127.0.0.1:35346 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\n\r\n2024-10-14 10:27:35 | INFO | stdout | Caught ValueError: Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.\r\n\r\n2024-10-14 10:27:35 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: Semaphore(value=5, locked=False). global_counter: 3\r\n\r\n2024-10-14 10:27:48 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: Semaphore(value=5, locked=False). global_counter: 3`\r\n\r\nHow can I resolve this issue?</BODY>\n\n<COMMENTS>\n<Comment by Coldermyy at 2024-12-12T01:53:54Z>\nHave you solved the problem yet?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1730,
    "state": "open",
    "created_by": "yuanllong",
    "created_at": "2024-10-13T10:05:50Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1730</URL>\n\n<TITLE>[Question] Now I hope to take the pre-trained multimodal model and use the instruction fine-tuning method to adapt to the new data set of downstream tasks. However, the types of these data sets include: pure text mode and image text. Is there no way to fine-tune this data set with only text mode? (Or is it possible to fine-tune the language model in the multimodal model independently? [in a daze])</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1728,
    "state": "open",
    "created_by": "enpro-github",
    "created_at": "2024-10-11T19:49:12Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1728</URL>\n\n<TITLE>[Usage] Getting  \"The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them\" error</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue: I am receiving a \"The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them\"\r\ntrying to execute the following code:\r\n\r\nllava_test.py:\r\n```\r\n\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.mm_utils import get_model_name_from_path\r\nfrom llava.eval.run_llava import eval_model\r\n\r\nmodel_path = \"liuhaotian/llava-v1.5-7```\r\n\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    model_path=model_path,\r\n    model_base=None,\r\n    model_name=get_model_name_from_path(model_path)\r\n)\r\n```\r\n\r\n\r\nCommand: `python llava_test.py`\r\n\r\n\r\nLog: \r\n\r\n`ValueError: The current 'device_map' had weights offloaded to the disk. Please provide an 'offload_folder' for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format.`\r\n\r\n\r\nWhere exactly am I supposed to specify this offload_folder?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1726,
    "state": "closed",
    "created_by": "Silverasdf",
    "created_at": "2024-10-09T17:05:02Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1726</URL>\n\n<TITLE>[Question] How can I use the CLI after running the fine-tuning script?</TITLE>\n\n<BODY>### Question\n\nHello, I was getting this error after trying to run the CLI with my custom fine-tuned model. My issue is that, after I ran finetune-lora.sh with no issues, it seems that there are multiple files missing from the directory. What is/are the step(s) that I need to take in order to fix this?\r\n\r\n![Screenshot 2024-10-09 130210](https://github.com/user-attachments/assets/e826d81f-54ff-4f41-b141-af1749b5b111)\r\n\r\n\r\nThanks!</BODY>\n\n<COMMENTS>\n<Comment by IMOsbo at 2024-10-11T17:12:19Z>\nI had a very similar issue with this... I don't think it's ever explicitly said anywhere, but I had to merge the LORA parameters with the model base before `llava.serve.cli` worked.\r\n\r\n```\r\npython scripts/merge_lora_weights.py \\\r\n    --model-path /path/to/lora_model \\\r\n    --model-base /path/to/base_model \\\r\n    --save-model-path /path/to/merge_model\r\n```\r\n\r\n<https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md#create-merged-checkpoints>\n</Comment>\n<Comment by Silverasdf at 2024-10-14T17:15:54Z>\nThank you for providing this information. I think I may be one step closer to getting this to work. Actually, I was able to get it while providing the same model base I used in the finetune-lora script! However, I could not run the script correctly when I tried to run the actual merge script. Did you end up getting an error like this?\r\n\r\nFrom the looks of it, the Traceback is from a python library rather than the code I could try modifying, but I figured it would be worth asking anyways.\r\n\r\n```py\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py\", line 558, in save_pretrained\r\n    raise ValueError(str([w.message for w in caught_warnings]))\r\nValueError: [UserWarning('`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.'), UserWarning('`do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.')]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/rdpperuski/LLaVA/scripts/merge_lora_weights.py\", line 22, in <module>\r\n    merge_lora(args)\r\n  File \"/home/rdpperuski/LLaVA/scripts/merge_lora_weights.py\", line 10, in merge_lora\r\n    model.save_pretrained(args.save_model_path)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2364, in save_pretrained\r\n    model_to_save.generation_config.save_pretrained(save_directory)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py\", line 560, in save_pretrained\r\n    raise ValueError(\r\nValueError: The generation config instance is invalid -- `.validate()` throws warnings and/or exceptions. Fix these issues to save the configuration.\r\n\r\nThrown during validation:\r\n[UserWarning('`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.'), UserWarning('`do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.')]```\n</Comment>\n<Comment by YUECHE77 at 2024-11-02T22:25:40Z>\n> I had a very similar issue with this... I don't think it's ever explicitly said anywhere, but I had to merge the LORA parameters with the model base before `llava.serve.cli` worked.\r\n> \r\n> ```\r\n> python scripts/merge_lora_weights.py \\\r\n>     --model-path /path/to/lora_model \\\r\n>     --model-base /path/to/base_model \\\r\n>     --save-model-path /path/to/merge_model\r\n> ```\r\n> \r\n> https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md#create-merged-checkpoints\r\n\r\nHello,\r\n\r\nCould you please give more information about running CLI?\r\n\r\nI've merged the LORA parameters with the base_model, but when I run the CLI, I got many mismatched parameters:\r\n\r\n![image](https://github.com/user-attachments/assets/09137098-018e-4006-93a5-5a2d56d322c5)\r\n\r\nHere is my merged model folder:\r\n\r\n![image](https://github.com/user-attachments/assets/98fd1830-fe96-4266-89f1-d866acf55e38)\r\n\r\nAnd the LoRA folder:\r\n\r\n![image](https://github.com/user-attachments/assets/6a504f04-1cf1-4d64-bf20-923f57ca6c80)\r\n\r\nThank you!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1725,
    "state": "closed",
    "created_by": "Lareina2441",
    "created_at": "2024-10-05T14:32:26Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1725</URL>\n\n<TITLE>[Usage] ValueError: Unrecognized configuration class <class 'transformers.models.llava.configuration_llava.LlavaConfig'> for this kind of AutoModel: AutoModelForCausalLM.</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI try to merge the LoRA weights with the full model after I train, I use transformers==4.37.2, \r\nbut it always report ValueError: Unrecognized configuration class <class 'transformers.models.llava.configuration_llava.LlavaConfig'> for this kind of AutoModel: AutoModelForCausalLM.\r\nI searched everything but couldn't fix it, can anyone help me please?\r\n\r\nCommand:\r\n```\r\npython3 scripts/merge_lora_weights.py --model-path /home/w/桌面/LLaVA/checkpoints/cp3 --model-base liuhaotian/llava-v1.5-13b --save-model-path /home/w/桌面/LLaVA/Mergedcheckpoints/cp3\r\n\r\n```\r\n\r\nLog: \r\n```\r\n/home/w/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nTraceback (most recent call last):\r\n  File \"/home/w/桌面/LLaVA/scripts/merge_lora_weights.py\", line 22, in <module>\r\n    merge_lora(args)\r\n  File \"/home/w/桌面/LLaVA/scripts/merge_lora_weights.py\", line 8, in merge_lora\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, device_map='cpu')\r\n  File \"/home/w/桌面/LLaVA/llava/model/builder.py\", line 128, in load_pretrained_model\r\n    model = AutoModelForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, **kwargs)\r\n  File \"/home/w/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 569, in from_pretrained\r\n    raise ValueError(\r\nValueError: Unrecognized configuration class <class 'transformers.models.llava.configuration_llava.LlavaConfig'> for this kind of AutoModel: AutoModelForCausalLM.\r\nModel type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, ElectraConfig, ErnieConfig, FalconConfig, FuyuConfig, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, LlamaConfig, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MptConfig, MusicgenConfig, MvpConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, LlavaConfig, LlavaMptConfig, LlavaMistralConfig.\r\n\r\n```</BODY>\n\n<COMMENTS>\n<Comment by Lareina2441 at 2024-10-05T15:28:15Z>\nsolved, need to include \"llava\" in fold name\n</Comment>\n<Comment by leshaonian at 2024-10-14T08:52:59Z>\n> solved, need to include \"llava\" in fold name\r\n\r\nI'm sorry to bother you, but could you be more specific? I reported the same error as you\n</Comment>\n<Comment by Bobolsky at 2024-10-17T20:14:32Z>\n> > solved, need to include \"llava\" in fold name\r\n> \r\n> I'm sorry to bother you, but could you be more specific? I reported the same error as you\r\n\r\nFor example my folder name was \"merged-model\", I renamed it to \"llava-merged-model\"\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1724,
    "state": "open",
    "created_by": "dagistankaradeniz",
    "created_at": "2024-10-01T07:24:12Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1724</URL>\n\n<TITLE>[Usage] llava brings info from previous images</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nNot sure if i'm doing wrong but seems like llava memorizes and uses info from previously uploaded image.\r\nI use Ollama to run llava 7b with `ollama run llava`\r\nsee every time it makes up `\"BE BARCLAYCOVERED,\"` from first image\r\n\r\nCommand:\r\n```\r\n➜  ~ ollama run llava                                                                                                                                                            <aws:dev>\r\npulling manifest\r\npulling 170370233dd5... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 4.1 GB\r\npulling 72d6f08a42f6... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 624 MB\r\npulling 43070e2d4e53... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  11 KB\r\npulling c43332387573... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏   67 B\r\npulling ed11eda7790d... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏   30 B\r\npulling 7c658f9561e5... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  564 B\r\nverifying sha256 digest\r\nwriting manifest\r\nsuccess\r\n>>> What is in this image? /Users/.../Downloads/unnamed.jpg\r\nAdded image '/Users/.../Downloads/unnamed.jpg'\r\n The image shows a humorous sign with two panels. On the left side, there's an illustration of a yellow rubber duck floating in a bathroom sink, alongside the text \"Don't be so near\r\nand -yet- so covered.\" This panel has a tile backsplash that suggests the setting is a bathroom.\r\n\r\nOn the right side, there's another text that says, \"BE BARCLAYCOVERED.\" The image features the logo of Barclays bank at the bottom center.\r\n\r\nThe juxtaposition of the rubber duck and the banking advertisement creates a playful contrast between the everyday activity of taking a bath and the serious business of banking\r\nservices.\r\n\r\n>>> What is in this image? /Users/.../Downloads/penguin.png\r\nAdded image '/Users/.../Downloads/penguin.png'\r\n The image features a logo of Barclays bank with an illustration of a yellow rubber duck floating in a white sink. Above the rubber duck, there's text that says \"Don't be so near\r\nand -yet- so covered.\" On the right side of the image, there is another text that reads \"BE BARCLAYCOVERED,\" which is a play on words suggesting a comprehensive banking service\r\nprovided by Barclays. The overall design is humorous and creative, using the rubber duck to represent financial security or coverage.\r\n\r\n>>> What is in this image? /Users/.../Downloads/united-kingdom.jpg\r\nAdded image '/Users/.../Downloads/united-kingdom.jpg'\r\n The image shows a photograph of the iconic London Bridge with its illuminated towers, spanning over the River Thames. On either side of the bridge, there is a cityscape at night,\r\nfeaturing street lights and buildings. Above the bridge, in the sky, is the text \"BE BARCLAYCOVERED,\" which appears to be an advertisement slogan or tagline for a company offering\r\nfinancial services. The overall tone of the image is promotional and visually striking.\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1723,
    "state": "open",
    "created_by": "LiZhangMing",
    "created_at": "2024-09-29T09:01:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1723</URL>\n\n<TITLE>[Discussion] Can not reproduce the Acc. on TextVQA datasets</TITLE>\n\n<BODY>### Discussion\r\n\r\n1, use lora fintuning on llava_v1_5_mix665k.json   \r\n`deepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3_offload.json \\\r\n    --model_name_or_path lmsys/vicuna-7b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path /home/lizhangming/.cache/huggingface/hub/datasets--liuhaotian--LLaVA-Instruct-150K/snapshots/9d451dc7629cfe0469f6ae4432b765cd603d5fcb/llava_v1_5_mix665k.json \\\r\n    --image_folder ./playground/data \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-7b-pretrain/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 2 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 1000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb`\r\n2, eval  CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/eval/textvqa.sh\r\n`python -m llava.eval.model_vqa_loader \\\r\n    --model-path '/home/lizhangming/projects/bigmodel/LLaVA/checkpoints/llava-v1.5-7b-lora/' \\\r\n    --model-base lmsys/vicuna-7b-v1.5 \\\r\n    --question-file ./playground/data/eval/textvqa/llava_textvqa_val_v051_ocr.jsonl \\\r\n    --image-folder ./playground/data/eval/textvqa/train_images \\\r\n    --answers-file ./playground/data/eval/textvqa/answers/llava-v1.5-7b.jsonl \\\r\n    --temperature 0 \\\r\n    --conv-mode vicuna_v1\r\n\r\npython -m llava.eval.eval_textvqa \\\r\n    --annotation-file ./playground/data/eval/textvqa/TextVQA_0.5.1_val.json \\\r\n    --result-file ./playground/data/eval/textvqa/answers/llava-v1.5-7b.jsonl`\r\n\r\n**Last I got the TextVQA acc.  :**\r\n<img width=\"980\" alt=\"image\" src=\"https://github.com/user-attachments/assets/d4897cc4-cbcc-4713-bf91-bc453320813d\">\r\n**However :**  \r\n<img width=\"1071\" alt=\"image\" src=\"https://github.com/user-attachments/assets/508439b2-cc84-4876-8a02-a96c5adccbff\"></BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1722,
    "state": "open",
    "created_by": "msra-jqxu",
    "created_at": "2024-09-29T06:03:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1722</URL>\n\n<TITLE>[Question] Resolution selection problem(CLIP for 224px or 336px)</TITLE>\n\n<BODY>### Question\n\nHi @haotian-liu , will llava1.5 performance drop during inference, if using openai/clip-vit-large-patch14 instead of openai/clip-vit-large-patch14-336? Thanks very much!</BODY>\n\n<COMMENTS>\n<Comment by Bobolsky at 2024-10-18T00:01:41Z>\nI guess so, since reading from the paper increasing the resolution to 336 gives a performance boost. It's one of the main difference from the earlier version of llava. If you want to understand and be sure just read the paper.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1720,
    "state": "open",
    "created_by": "collinmccarthy",
    "created_at": "2024-09-28T19:51:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1720</URL>\n\n<TITLE>[Bug]: Need to return attention_mask when padding</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIn [these lines](https://github.com/haotian-liu/LLaVA/blob/c121f0432da27facab705978f83c4ada465e46fd/llava/model/llava_arch.py#L316-L319) of `LlavaMetaForCausalLM.prepare_inputs_labels_for_multimodal()`, when we have padded the input we always need to return the padded attention mask. \r\n\r\nThis is as simple as changing this\r\n\r\n```python\r\n# Bug, fails to return padded attention mask\r\nif _attention_mask is None:\r\n    attention_mask = None\r\nelse:\r\n    attention_mask = attention_mask.to(dtype=_attention_mask.dtype)\r\n```\r\n\r\nTo this:\r\n\r\n```python\r\n# Update: always return attention mask if we padded (if any values are False)\r\nif attention_mask.all():  # Not padded\r\n    if _attention_mask is None:\r\n        attention_mask = None\r\n    else:\r\n        attention_mask = attention_mask.to(dtype=_attention_mask.dtype)\r\n```\r\n\r\nThat being said, I don't see why we can't just always return `attention_mask` as is, essentially just commenting out all of these lines of code. The re-computed `attention_mask` should have the correct dtype, device and values even if `_attention_mask = None` (no input attention mask). But maybe I'm missing something. \r\n\r\nThis fixes batch inference in v1.6, e.g. #1149, #1305, and probably others. Note that you also have to apply the changes from PR #1502 to get batch inference to work in `run_llava.py`.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1719,
    "state": "open",
    "created_by": "LordO54",
    "created_at": "2024-09-26T18:48:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1719</URL>\n\n<TITLE>Llava context window - num_ctx</TITLE>\n\n<BODY>### Question\n\n**How much tokens can one pass to the model and expect a good behavior**? I'm trying to fine tune the model for instructions on an specific case, and I would like to have this information available for the dataset creation</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1718,
    "state": "open",
    "created_by": "nkklas",
    "created_at": "2024-09-26T18:05:29Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1718</URL>\n\n<TITLE>Incompatible `peft` version for Finetune script</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nFinetune script is incompatible with `peft == 0.13.0`, works with `peft == 0.12.0`. \r\n\r\nCommand:\r\n```\r\n#!/bin/bash\r\n\r\ndeepspeed /home/gcpuser/sky_workdir/LLaVA/llava/train/train_mem.py \\\r\n    --deepspeed /home/gcpuser/sky_workdir/LLaVA/scripts/zero3.json \\\r\n    --model_name_or_path liuhaotian/llava-v1.5-7b\\\r\n    --version v1 \\\r\n    --data_path /home/gcpuser/sky_workdir/fine_tune_training.json \\\r\n    --image_folder /home/gcpuser/sky_workdir \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ${OUTPUT_DIR_PATH} \\\r\n    --num_train_epochs 4 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 4096 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/llava_finetune/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1364, in _get_module\r\n  File \"/opt/conda/envs/llava_finetune/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1364, in _get_module\r\n        return importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"/opt/conda/envs/llava_finetune/lib/python3.10/importlib/__init__.py\", line 126, in import_module\r\nreturn importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"/opt/conda/envs/llava_finetune/lib/python3.10/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/opt/conda/envs/llava_finetune/lib/python3.10/site-packages/transformers/trainer.py\", line 190, in <module>\r\n  File \"/opt/conda/envs/llava_finetune/lib/python3.10/site-packages/transformers/trainer.py\", line 190, in <module>\r\n        from peft import PeftModel\r\n  File \"/opt/conda/envs/llava_finetune/lib/python3.10/site-packages/peft/__init__.py\", line 22, in <module>\r\nfrom peft import PeftModel\r\n  File \"/opt/conda/envs/llava_finetune/lib/python3.10/site-packages/peft/__init__.py\", line 22, in <module>\r\n    from .auto import (\r\n      File \"/opt/conda/envs/llava_finetune/lib/python3.10/site-packages/peft/auto.py\", line 31, in <module>\r\nfrom .auto import (\r\n  File \"/opt/conda/envs/llava_finetune/lib/python3.10/site-packages/peft/auto.py\", line 31, in <module>\r\n    from .config import PeftConfig\r\n      File \"/opt/conda/envs/llava_finetune/lib/python3.10/site-packages/peft/config.py\", line 24, in <module>\r\nfrom .config import PeftConfig\r\n  File \"/opt/conda/envs/llava_finetune/lib/python3.10/site-packages/peft/config.py\", line 24, in <module>\r\n    from .utils import CONFIG_NAME, PeftType, TaskType\r\n    from .utils import CONFIG_NAME, PeftType, TaskType  File \"/opt/conda/envs/llava_finetune/lib/python3.10/site-packages/peft/utils/__init__.py\", line 21, in <module>\r\n  File \"/opt/conda/envs/llava_finetune/lib/python3.10/site-packages/peft/utils/__init__.py\", line 21, in <module>\r\n    from .loftq_utils import replace_lora_weights_loftq    \r\nfrom .loftq_utils import replace_lora_weights_loftq\r\n  File \"/opt/conda/envs/llava_finetune/lib/python3.10/site-packages/peft/utils/loftq_utils.py\", line 26, in <module>\r\n  File \"/opt/conda/envs/llava_finetune/lib/python3.10/site-packages/peft/utils/loftq_utils.py\", line 26, in <module>\r\n        from huggingface_hub.errors import HFValidationErrorfrom huggingface_hub.errors import HFValidationError\r\n```</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1717,
    "state": "open",
    "created_by": "RogerRafa",
    "created_at": "2024-09-26T03:02:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1717</URL>\n\n<TITLE>[Question] Pretrain LLaVA</TITLE>\n\n<BODY>### Question\n\nI want to pretrain the model, but I see that the evaluation_strategy in pretrain.sh is set to \"no.\" How can I determine if the model is trained well?</BODY>\n\n<COMMENTS>\n<Comment by zenitsu0509 at 2024-09-27T01:33:35Z>\nWhen using LLaVA 1.5 and noticing that evaluation_strategy in the pretrain.sh script is set to \"no\", it means the model won't evaluate its performance on a validation set during pretraining. To determine if the model is being trained well without automatic evaluation, you can try the following methods:\r\nPeriodically save checkpoints during training and manually evaluate them using a separate validation script. You can monitor key metrics like accuracy, loss, or any task-specific evaluation metric on a validation dataset.\r\nModify the training script to save checkpoints at certain intervals and load them separately for validation.\n</Comment>\n<Comment by RogerRafa at 2024-09-27T02:39:50Z>\nThank you！I'll give it a try.\n</Comment>\n<Comment by cqray1990 at 2024-11-24T13:32:22Z>\n> When using LLaVA 1.5 and noticing that evaluation_strategy in the pretrain.sh script is set to \"no\", it means the model won't evaluate its performance on a validation set during pretraining. To determine if the model is being trained well without automatic evaluation, you can try the following methods: Periodically save checkpoints during training and manually evaluate them using a separate validation script. You can monitor key metrics like accuracy, loss, or any task-specific evaluation metric on a validation dataset. Modify the training script to save checkpoints at certain intervals and load them separately for validation.\r\n\r\n@zenitsu0509  how to evaluate the performance, have any scripts in repo??\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1715,
    "state": "open",
    "created_by": "YiboZhao624",
    "created_at": "2024-09-25T05:22:26Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1715</URL>\n\n<TITLE>[Question] Why the LlaVA Next output nothing?</TITLE>\n\n<BODY>### Question\n\nI used `transformers=4.44.2` , and load the model `llava-hf/llama3-llava-next-8b-hf` with the following code:\r\n```\r\nfrom transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\r\nprocessor = LlavaNextProcessor.from_pretrained(model_path)\r\nmodel = LlavaNextForConditionalGeneration.from_pretrained(model_path)\r\n```\r\nand then, I used batch inference with batchsize = 4. The prompts are in proper formats as far as I concerned like:\r\n```\r\nUSER: <image>\\n<prompt text here>\\nASSISTANT:\r\n```\r\nthree items return the expected answers but only one in the same batch return nothing. Here is my code on inference:\r\n```\r\ninputs = processor(prompts, images, return_tensors = \"pt\", padding = True)\r\n\r\ninputs = inputs.to(device)\r\noutputs = model.generate(**inputs, max_new_tokens = 10000)\r\ndescriptions = processor.batch_decode(outputs, skip_special_tokens=True)\r\n```\r\nthe prompts, images are lists.\r\n\r\nIf anyone has the same problem or has any clue to solve it, please email me or respond to this issue. Thanks a lot!</BODY>\n\n<COMMENTS>\n<Comment by YiboZhao624 at 2024-09-30T04:27:38Z>\nbtw I set the `max_new_tokens = 300`. The prompt may be long but I don't know if it will influence the generated codes.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1712,
    "state": "open",
    "created_by": "GonyRosenman",
    "created_at": "2024-09-23T14:59:23Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1712</URL>\n\n<TITLE>[Usage] RuntimeError: expected scalar type BFloat16 but found Float in LLAVA Vision Model during LoRA Fine-Tuning with 4-bit Precision</TITLE>\n\n<BODY>### Describe the issue\n\nI’m encountering a RuntimeError: expected scalar type BFloat16 but found Float error during fine-tuning LLAVA with LoRA enabled. This error occurs when I run the model on a machine with relatively small GPUs, which is why I’m using 4-bit precision (for debugging purposes). The issue happens inside the vision model, specifically in the LayerNorm operation during the forward pass.\r\n\r\nConfiguration:\r\n\r\nmodel_name_or_path: liuhaotian/llava-v1.6-vicuna-7b\r\nvision_tower: openai/clip-vit-large-patch14-336\r\nLoRA parameters: lora_enable=True, lora_r=128, lora_alpha=256\r\n4-bit precision: bits=4\r\nOther key arguments:\r\nbf16=True\r\ngradient_checkpointing=True\r\nnum_train_epochs=1\r\nper_device_train_batch_size=16\r\noutput_dir=./checkpoints/llava-v1.6-vicuna-7b-task-lora\r\nSteps Taken:\r\n\r\nI ran a debug loop to inspect the data types of each layer in the model:\r\n\r\n```\r\ncategories = {}\r\nfor name, param in trainer.model.base_model.named_parameters():\r\n    d = param.dtype\r\n    if d in categories:\r\n        categories[d].append(name + '__' + str(param.shape))\r\n    else:\r\n        categories[d] = [name + '__' + str(param.shape)]\r\n```\r\nResult:\r\n\r\n166 layers are in float32\r\n744 layers are in bfloat16\r\n369 layers are in uint8\r\nSince the error specifically mentioned LayerNorm, I also checked the data types of all parameters containing layer_norm. It turns out they are all in float32, which seems to conflict with the expected BFloat16.\r\n\r\nGoal:\r\n\r\nI’m trying to understand what causes the data type mismatch in the vision model when using 4-bit precision.\r\nCould this be related to how the model handles BFloat16 and float32 layers during mixed precision training? Should I convert specific layers manually, or is there another recommended solution to avoid this error?\r\nI’m only using 4-bit precision for debugging on a machine with limited GPU resources, but this is where I consistently encounter the error. I’d appreciate any guidance or insight into resolving this issue.\r\n\r\nThanks in advance for your help!</BODY>\n\n<COMMENTS>\n<Comment by GonyRosenman at 2024-09-23T15:54:31Z>\ni guess a more simple approach to my issue is - \r\n\r\nhow can i run the lora or full weights fine tuning in a \"debugging\" mode? \r\n\r\ni'm tweaking the code to my needs and i need to run it on a machine with small gpus, at this point the outputs do not matter\n</Comment>\n<Comment by zenitsu0509 at 2024-09-27T01:44:35Z>\nThe error you're encountering (RuntimeError: expected scalar type BFloat16 but found Float) is due to a mismatch between expected data types in mixed-precision training, particularly with LayerNorm in the vision model. Since you're using LoRA and 4-bit precision for memory efficiency, this setup can introduce issues with how different parts of the model handle precision and dtype conversions.\r\nLayerNorm specifically expects BFloat16, but your debug loop shows that some parameters, especially in layer_norm, are still in float32. This leads to a dtype mismatch during the forward pass.\r\nThe root cause may lie in how mixed precision (bf16 and float32) is handled across different layers. When using LoRA and 4-bit precision, some layers may not be automatically cast into the right precision.\r\nTry this:\r\nConvert LayerNorm to BFloat16 manually: You can force the vision model’s LayerNorm to use BFloat16.\r\n**import torch\r\nfrom torch import nn\r\n\r\nfor name, module in trainer.model.named_modules():\r\n    if isinstance(module, nn.LayerNorm):\r\n        module.to(torch.bfloat16)**\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1711,
    "state": "open",
    "created_by": "liangjing910",
    "created_at": "2024-09-21T10:04:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1711</URL>\n\n<TITLE>[Usage] 微调7b，训练时一直卡着不动，A100 40G X8</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n微调7b，训练时一直卡着不动，A100 40G X8\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```GPU=$((WORLD_SIZE * 8))\r\necho GPU=${GPU}\r\necho WORLD_SIZE=$WORLD_SIZE\r\necho RANK=$RANK\r\necho MASTER_ADDR=$MASTER_ADDR\r\necho MASTER_PORT=$MASTER_PORT\r\n\r\nWANDB_MODE=online deepspeed llava/train/train_mem_CoS.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path ./checkpoints/llava-v1.5-7b-ckpt \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/llava_v1_5_mix665k.json \\\r\n    --cos_data_path /PATH/TO/YOUR/DATA \\\r\n    --image_folder ./playground/data \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-7b-ckpt/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b-CoS \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n(cos) root@ubuntu22:/home/vipuser/Chain-of-Spot# bash ./scripts/v1_5/finetune_CoS_7b.sh\r\nGPU=0\r\nWORLD_SIZE=\r\nRANK=\r\nMASTER_ADDR=\r\nMASTER_PORT=\r\n[2024-09-21 17:35:39,385] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\r\n  def forward(ctx, input, weight, bias=None):\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\r\n  def backward(ctx, grad_output):\r\n[2024-09-21 17:35:42,605] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\n[2024-09-21 17:35:42,605] [INFO] [runner.py:555:main] cmd = /home/vipuser/miniconda3/envs/cos/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem_CoS.py --deepspeed ./scripts/zero3.json --model_name_or_path ./checkpoints/llava-v1.5-7b-ckpt --version v1 --data_path ./playground/data/llava_v1_5_mix665k.json --cos_data_path /PATH/TO/YOUR/DATA --image_folder ./playground/data --vision_tower openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-7b-ckpt/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-7b-CoS --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb\r\n[2024-09-21 17:35:43,941] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\r\n  def forward(ctx, input, weight, bias=None):\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\r\n  def backward(ctx, grad_output):\r\n[2024-09-21 17:35:46,275] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\r\n[2024-09-21 17:35:46,275] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0\r\n[2024-09-21 17:35:46,275] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\r\n[2024-09-21 17:35:46,275] [INFO] [launch.py:163:main] dist_world_size=8\r\n[2024-09-21 17:35:46,275] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\r\n[2024-09-21 17:35:51,193] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\r\n  def forward(ctx, input, weight, bias=None):\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\r\n  def backward(ctx, grad_output):\r\n[2024-09-21 17:35:51,315] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-09-21 17:35:51,358] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-09-21 17:35:51,373] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\r\n  def forward(ctx, input, weight, bias=None):\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\r\n  def backward(ctx, grad_output):\r\n[2024-09-21 17:35:51,415] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\r\n  def forward(ctx, input, weight, bias=None):\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\r\n  def backward(ctx, grad_output):\r\n[2024-09-21 17:35:51,420] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-09-21 17:35:51,420] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\r\n  def forward(ctx, input, weight, bias=None):\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\r\n  def backward(ctx, grad_output):\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\r\n  def forward(ctx, input, weight, bias=None):\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\r\n  def backward(ctx, grad_output):\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\r\n  def forward(ctx, input, weight, bias=None):\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\r\n  def backward(ctx, grad_output):\r\n[2024-09-21 17:35:51,482] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\r\n  def forward(ctx, input, weight, bias=None):\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\r\n  def backward(ctx, grad_output):\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\r\n  def forward(ctx, input, weight, bias=None):\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\r\n  def backward(ctx, grad_output):\r\n[2024-09-21 17:35:52,051] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2024-09-21 17:35:52,051] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2024-09-21 17:35:52,117] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2024-09-21 17:35:52,117] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2024-09-21 17:35:52,165] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2024-09-21 17:35:52,165] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2024-09-21 17:35:52,172] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2024-09-21 17:35:52,172] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2024-09-21 17:35:52,172] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n[2024-09-21 17:35:52,172] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2024-09-21 17:35:52,173] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2024-09-21 17:35:52,185] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2024-09-21 17:35:52,185] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2024-09-21 17:35:52,231] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2024-09-21 17:35:52,231] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2024-09-21 17:35:52,282] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2024-09-21 17:35:52,282] [INFO] [comm.py:594:init_distributed] cdb=None\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nLoading checkpoint shards:   0%|                                                                                                                                         | 0/2 [00:00<?, ?it/s]/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n  return torch.load(checkpoint_file, map_location=\"cpu\")\r\nLoading checkpoint shards:   0%|                                                                                                                                         | 0/2 [00:00<?, ?it/s]/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n  return torch.load(checkpoint_file, map_location=\"cpu\")\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n  return torch.load(checkpoint_file, map_location=\"cpu\")\r\nLoading checkpoint shards:   0%|                                                                                                                                         | 0/2 [00:00<?, ?it/s]/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n  return torch.load(checkpoint_file, map_location=\"cpu\")\r\nLoading checkpoint shards:   0%|                                                                                                                                         | 0/2 [00:00<?, ?it/s]/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n  return torch.load(checkpoint_file, map_location=\"cpu\")\r\nLoading checkpoint shards:   0%|                                                                                                                                         | 0/2 [00:00<?, ?it/s]/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n  return torch.load(checkpoint_file, map_location=\"cpu\")\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n  return torch.load(checkpoint_file, map_location=\"cpu\")\r\n[2024-09-21 17:35:58,155] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 6.76B parameters\r\nLoading checkpoint shards:   0%|                                                                                                                                         | 0/2 [00:00<?, ?it/s]/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n  return torch.load(checkpoint_file, map_location=\"cpu\")\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:23<00:00, 11.64s/it]\r\n\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:23<00:00, 11.65s/it]\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:23<00:00, 11.65s/it]\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:23<00:00, 11.66s/it]\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:23<00:00, 11.66s/it]\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:23<00:00, 11.67s/it]\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:23<00:00, 11.68s/it]\r\n[2024-09-21 17:36:24,133] [WARNING] [partition_parameters.py:836:_post_init_method] param `class_embedding` in CLIPVisionEmbeddings not on GPU so was not broadcasted from rank 0\r\n[2024-09-21 17:36:24,442] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 7.06B parameters\r\n/home/vipuser/Chain-of-Spot/llava/model/llava_arch.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n  mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\r\n/home/vipuser/Chain-of-Spot/llava/model/llava_arch.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n  mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\r\n/home/vipuser/Chain-of-Spot/llava/model/llava_arch.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n  mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\r\n/home/vipuser/Chain-of-Spot/llava/model/llava_arch.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n  mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\r\n/home/vipuser/Chain-of-Spot/llava/model/llava_arch.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n  mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\r\n/home/vipuser/Chain-of-Spot/llava/model/llava_arch.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n  mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\r\n/home/vipuser/Chain-of-Spot/llava/model/llava_arch.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n  mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\r\n/home/vipuser/Chain-of-Spot/llava/model/llava_arch.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n  mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\r\npwd/home/vipuser/Chain-of-Spot\r\npwd/home/vipuser/Chain-of-Spot\r\npwd/home/vipuser/Chain-of-Spot\r\npwd/home/vipuser/Chain-of-Spot\r\npwd/home/vipuser/Chain-of-Spot\r\npwd/home/vipuser/Chain-of-Spot\r\npwd/home/vipuser/Chain-of-Spot\r\npwd/home/vipuser/Chain-of-Spot\r\n\r\n\r\nFormatting inputs...Skip in lazy mode\r\nParameter Offload: Total persistent parameters: 599040 in 312 params\r\nwandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\r\nwandb: Currently logged in as: 43217594 (43217594-huazhong-university-of-science-and-technology). Use `wandb login --relogin` to force relogin\r\nwandb: Tracking run with wandb version 0.18.1\r\nwandb: Run data is saved locally in /home/vipuser/Chain-of-Spot/wandb/run-20240921_173744-8g7rq791\r\nwandb: Run `wandb offline` to turn off syncing.\r\nwandb: Syncing run dazzling-dust-1\r\nwandb: ⭐️ View project at https://wandb.ai/43217594-huazhong-university-of-science-and-technology/huggingface\r\nwandb: 🚀 View run at https://wandb.ai/43217594-huazhong-university-of-science-and-technology/huggingface/runs/8g7rq791\r\n  0%|                                                                                                                                                                 | 0/5198 [00:00<?, ?it/s]/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\r\n  return fn(*args, **kwargs)\r\n/home/vipuser/miniconda3/envs/cos/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\r\n  return fn(*args, **kwargs)\r\n\r\n\r\n\r\n\r\n\r\n[rank6]:[E921 17:55:52.146960461 ProcessGroupNCCL.cpp:1375] [PG 0 (default_pg) Rank 6] First PG on this rank that detected no heartbeat of its watchdog.\r\n[rank6]:[E921 17:55:52.147105451 ProcessGroupNCCL.cpp:1413] [PG 0 (default_pg) Rank 6] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=3\r\n[rank7]:[E921 17:55:52.256762614 ProcessGroupNCCL.cpp:1375] [PG 0 (default_pg) Rank 7] First PG on this rank that detected no heartbeat of its watchdog.\r\n[rank7]:[E921 17:55:52.256849497 ProcessGroupNCCL.cpp:1413] [PG 0 (default_pg) Rank 7] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=3\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by Yixuan423 at 2024-11-10T05:59:55Z>\nI meet this bug too. Did you finally solve it?\n</Comment>\n<Comment by RONINGOD at 2024-11-18T09:52:38Z>\n> I meet this bug too. Did you finally solve it?\r\n\r\nI solved by set dataloader_num_workers  to 1, I guess it maybe CPU problem.\n</Comment>\n<Comment by morganlee1 at 2025-01-01T14:07:51Z>\n> > I meet this bug too. Did you finally solve it?\r\n> \r\n> I solved by set dataloader_num_workers to 1, I guess it maybe CPU problem.\r\n\r\nnice\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1710,
    "state": "open",
    "created_by": "wentaoyuan",
    "created_at": "2024-09-21T07:08:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1710</URL>\n\n<TITLE>[Usage] Tokenization mismatch while finetuning Llama 3.1</TITLE>\n\n<BODY>### Describe the issue\n\n**Issue:**\r\nI ran into tokenization mismatch errors when I tried to fine-tune from Llama-3.1. I pre-trained a new MLP adapter for Llama-3.1 and that seems to work, but the fine-tuning script produced lots of warnings like the following and the loss was always 0.\r\n```\r\nWARNING: tokenization mismatch: 384 vs. 392. (ignored)                                                                                                                                                             \r\nWARNING: tokenization mismatch: 414 vs. 416. (ignored)                                                                                                                                                             \r\nWARNING: tokenization mismatch: 268 vs. 270. (ignored)                                                                                                                                                             \r\nWARNING: tokenization mismatch: 273 vs. 275. (ignored)                                                                                                                                                             \r\nWARNING: tokenization mismatch: 284 vs. 286. (ignored)                                                                                                                                                             \r\nWARNING: tokenization mismatch: 218 vs. 220. (ignored)\r\n```\r\n\r\n**Command:**\r\n```\r\n#!/bin/bash\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path meta-llama/Meta-Llama-3.1-8B-Instruct \\\r\n    --version v1 \\\r\n    --data_path data/llava_v1_5_mix665k.json \\\r\n    --image_folder data \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter checkpoints/llava-mlp2x-336px-pretrain-llama-3.1-8b \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-13b \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\n**Environment:**\r\nHere are the versions of some important packages in my conda environment.\r\n```\r\ntokenizers                0.19.1\r\ntorch                     2.1.2\r\ntorchvision               0.16.2\r\ntransformers              4.43.1\r\naccelerate                0.33.0\r\ndatasets                  2.20.0\r\ndeepspeed                 0.14.4\r\nnvidia-cublas-cu12        12.1.3.1\r\nnvidia-cuda-cupti-cu12    12.1.105\r\nnvidia-cuda-nvrtc-cu12    12.1.105\r\nnvidia-cuda-runtime-cu12  12.1.105\r\nnvidia-cudnn-cu12         8.9.2.26\r\nnvidia-cufft-cu12         11.0.2.54\r\nnvidia-curand-cu12        10.3.2.106\r\nnvidia-cusolver-cu12      11.4.5.107\r\nnvidia-cusparse-cu12      12.1.0.106\r\nnvidia-ml-py              12.555.43\r\nnvidia-nccl-cu12          2.18.1\r\nnvidia-nvjitlink-cu12     12.5.82\r\nnvidia-nvtx-cu12          12.1.105\r\n```</BODY>\n\n<COMMENTS>\n<Comment by HandsomeWuuu at 2024-10-07T10:17:41Z>\nyou can rewrite the preprocess_llama_2 funciton follow llava-next (preprocess_llama_3)\n</Comment>\n<Comment by GonyRosenman at 2025-01-03T09:43:26Z>\n@cainiaoup , i'm having a similar issue, can you explain your solution?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1708,
    "state": "open",
    "created_by": "Snus0511",
    "created_at": "2024-09-19T16:52:09Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1708</URL>\n\n<TITLE>[Usage]Why are there errors in the UI interface？</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n![1726735916295](https://github.com/user-attachments/assets/a9163a52-b6da-48d8-a8ec-4ef3ecc6551d)\r\n\r\nWhy are there errors in the UI interface\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1707,
    "state": "open",
    "created_by": "wenyu1009",
    "created_at": "2024-09-19T14:41:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1707</URL>\n\n<TITLE>[Question] 是否支持统计大模型推理的吞吐量</TITLE>\n\n<BODY>### Question\n\n查看输出仅有推理的结果，没有吞吐量计算，后续是否会增加该指标？</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1706,
    "state": "closed",
    "created_by": "Bleking",
    "created_at": "2024-09-18T19:15:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1706</URL>\n\n<TITLE>Tokenization mismatch while finetuning LLaVA-v1.6-34B</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nI tried to LoRA finetune LLaVA-v1.6-34B with my own dataset(size 1000) and I constantly face tokenization mismatch during the process.\r\n\r\nThis is my script for LLaVA-v1.6-34B, `finetune_task_lora.sh`. I will not show the data_path and image_folder for my privacy.\r\n```\r\n#!/bin/bash\r\n\r\ndeepspeed llava/train/train_xformers.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3_offload.json \\\r\n    --model_name_or_path ./checkpoints/pretrained/llava-v1.6-34b \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/- \\\r\n    --image_folder ./playground/data/- \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --fp16 True \\\r\n    --output_dir ./checkpoints/llava-v1.6-vicuna-34b-task-lora-floorplan-vqa-1000 \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 8 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 False \\\r\n    --model_max_length 4096 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 0 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nAlso, checked the tokenizer defined by `AutoTokenizer.from_pretrained('/home/work/testdataset1/LLaVA/checkpoints/pretrained/llava-v1.6-34b')` and I got the following:\r\n```\r\nLlamaTokenizerFast(name_or_path='/home/work/testdataset1/LLaVA/checkpoints/pretrained/llava-v1.6-34b', vocab_size=64000, model_max_length=4096, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|im_end|>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\r\n        0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\r\n        1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\r\n        2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\r\n        6: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\r\n        7: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\r\n        64000: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\r\n        64001: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\r\n}\r\n```\r\nand set model_max_length=4096 following this result\r\n\r\nI will also share with you the list of modules in the 'llava' environment I use for this.\r\n```\r\nPackage                       Version\r\n----------------------------- -----------\r\naccelerate                    0.21.0\r\naiofiles                      23.2.1\r\naiohappyeyeballs              2.4.0\r\naiohttp                       3.10.5\r\naiosignal                     1.3.1\r\naltair                        5.4.1\r\nannotated-types               0.7.0\r\nanyio                         4.4.0\r\nasync-timeout                 4.0.3\r\nattrs                         24.2.0\r\nav                            13.0.0\r\nbitsandbytes                  0.43.3\r\nblack                         24.1.0\r\ncertifi                       2024.8.30\r\ncfgv                          3.4.0\r\nchardet                       5.2.0\r\ncharset-normalizer            3.3.2\r\nclick                         8.1.7\r\ncontourpy                     1.3.0\r\ncycler                        0.12.1\r\nDataProperty                  1.0.1\r\ndatasets                      2.16.1\r\ndecord                        0.6.0\r\ndeepspeed                     0.14.4\r\ndill                          0.3.7\r\ndistlib                       0.3.8\r\ndistro                        1.9.0\r\ndocker-pycreds                0.4.0\r\neinops                        0.6.1\r\neinops-exts                   0.0.4\r\net-xmlfile                    1.1.0\r\nevaluate                      0.4.3\r\nexceptiongroup                1.2.2\r\nfastapi                       0.115.0\r\nffmpy                         0.4.0\r\nfilelock                      3.16.1\r\nflash-attn                    2.6.3\r\nfonttools                     4.53.1\r\nfrozenlist                    1.4.1\r\nfsspec                        2023.10.0\r\nftfy                          6.2.3\r\ngitdb                         4.0.11\r\nGitPython                     3.1.43\r\ngradio                        4.16.0\r\ngradio_client                 0.8.1\r\nh11                           0.14.0\r\nhf_transfer                   0.1.8\r\nhjson                         3.1.0\r\nhttpcore                      0.17.3\r\nhttpx                         0.24.0\r\nhuggingface-hub               0.25.0\r\nidentify                      2.6.1\r\nidna                          3.10\r\nimportlib_resources           6.4.5\r\nisort                         5.13.2\r\nJinja2                        3.1.4\r\njiter                         0.5.0\r\njoblib                        1.4.2\r\njsonlines                     4.0.0\r\njsonschema                    4.23.0\r\njsonschema-specifications     2023.12.1\r\nkiwisolver                    1.4.7\r\nlatex2mathml                  3.77.0\r\nllava                         1.2.2.post1\r\nlmms_eval                     0.2.3.post1\r\nloguru                        0.7.2\r\nmarkdown-it-py                3.0.0\r\nmarkdown2                     2.5.0\r\nMarkupSafe                    2.1.5\r\nmatplotlib                    3.9.2\r\nmbstrdecoder                  1.1.3\r\nmdurl                         0.1.2\r\nmpmath                        1.3.0\r\nmultidict                     6.1.0\r\nmultiprocess                  0.70.15\r\nmutagen                       1.47.0\r\nnarwhals                      1.8.1\r\nnetworkx                      3.3\r\nninja                         1.11.1.1\r\nnodeenv                       1.9.1\r\nnumpy                         1.26.4\r\nnvidia-cublas-cu12            12.1.3.1\r\nnvidia-cuda-cupti-cu12        12.1.105\r\nnvidia-cuda-nvrtc-cu12        12.1.105\r\nnvidia-cuda-runtime-cu12      12.1.105\r\nnvidia-cudnn-cu12             8.9.2.26\r\nnvidia-cufft-cu12             11.0.2.54\r\nnvidia-curand-cu12            10.3.2.106\r\nnvidia-cusolver-cu12          11.4.5.107\r\nnvidia-cusparse-cu12          12.1.0.106\r\nnvidia-ml-py                  12.560.30\r\nnvidia-nccl-cu12              2.18.1\r\nnvidia-nvjitlink-cu12         12.6.68\r\nnvidia-nvtx-cu12              12.1.105\r\nopenai                        1.46.0\r\nopenpyxl                      3.1.5\r\norjson                        3.10.7\r\npackaging                     24.1\r\npandas                        2.2.2\r\npathvalidate                  3.2.1\r\npeft                          0.12.0\r\npillow                        10.4.0\r\npip                           24.2\r\nplatformdirs                  4.3.6\r\nportalocker                   2.10.1\r\npre-commit                    3.8.0\r\nprotobuf                      3.20.0\r\npsutil                        6.0.0\r\npy-cpuinfo                    9.0.0\r\npyarrow                       17.0.0\r\npyarrow-hotfix                0.6\r\npycocoevalcap                 1.2\r\npycocotools                   2.0.8\r\npycryptodomex                 3.20.0\r\npydantic                      2.9.2\r\npydantic_core                 2.23.4\r\npydub                         0.25.1\r\nPygments                      2.18.0\r\npynvml                        11.5.3\r\npyparsing                     3.1.4\r\npytablewriter                 1.2.0\r\npython-dateutil               2.9.0.post0\r\npython-multipart              0.0.9\r\npytz                          2024.2\r\nPyYAML                        6.0.2\r\nreferencing                   0.35.1\r\nregex                         2024.9.11\r\nrequests                      2.32.3\r\nrfc3986                       1.5.0\r\nrich                          13.8.1\r\nrpds-py                       0.20.0\r\nruff                          0.6.5\r\nsacrebleu                     2.4.3\r\nsafetensors                   0.4.5\r\nscikit-learn                  1.2.2\r\nscipy                         1.14.1\r\nsemantic-version              2.10.0\r\nsentencepiece                 0.1.99\r\nsentry-sdk                    2.14.0\r\nsetproctitle                  1.3.3\r\nsetuptools                    72.1.0\r\nshellingham                   1.5.4\r\nshortuuid                     1.0.13\r\nsix                           1.16.0\r\nsmmap                         5.0.1\r\nsniffio                       1.3.1\r\nsqlitedict                    2.1.0\r\nstarlette                     0.38.5\r\nsvgwrite                      1.4.3\r\nsympy                         1.13.2\r\ntabledata                     1.3.3\r\ntcolorpy                      0.1.6\r\ntenacity                      8.3.0\r\nthreadpoolctl                 3.5.0\r\ntiktoken                      0.7.0\r\ntimm                          0.6.13\r\ntokenizers                    0.15.1\r\ntomlkit                       0.12.0\r\ntorch                         2.1.2\r\ntorchaudio                    2.1.2+cu121\r\ntorchvision                   0.16.2\r\ntqdm                          4.66.5\r\ntqdm-multiprocess             0.0.11\r\ntransformers                  4.37.2\r\ntransformers-stream-generator 0.0.5\r\ntriton                        2.1.0\r\ntypepy                        1.3.2\r\ntyper                         0.12.5\r\ntyping_extensions             4.12.2\r\ntzdata                        2024.1\r\nurllib3                       2.2.3\r\nuvicorn                       0.30.6\r\nvirtualenv                    20.26.5\r\nwandb                         0.18.1\r\nwavedrom                      2.0.3.post3\r\nwebsockets                    11.0.3\r\nwheel                         0.44.0\r\nxxhash                        3.5.0\r\nyarl                          1.11.1\r\nyt-dlp                        2024.8.6\r\nzss                           1.2.0\r\nzstandard                     0.23.0\r\n```\r\n\r\n\r\nRegarding my script and the module versions, what would be the causes of the tokenization mismatches?\r\nBecause of this, the loss stay 0.0 during the whole finetuning process.\r\n\r\nI will share only a small part of the mismatch messages in case you need some.\r\nWARNING: tokenization mismatch: 385 vs. 399. (ignored)\r\nWARNING: tokenization mismatch: 381 vs. 395. (ignored)\r\nWARNING: tokenization mismatch: 207 vs. 221. (ignored)\r\nWARNING: tokenization mismatch: 234 vs. 248. (ignored)\r\nWARNING: tokenization mismatch: 187 vs. 201. (ignored)\r\nWARNING: tokenization mismatch: 188 vs. 202. (ignored)\r\nWARNING: tokenization mismatch: 159 vs. 173. (ignored)\r\nWARNING: tokenization mismatch: 371 vs. 385. (ignored)\r\nWARNING: tokenization mismatch: 149 vs. 163. (ignored)\r\nWARNING: tokenization mismatch: 210 vs. 224. (ignored)\r\nWARNING: tokenization mismatch: 357 vs. 371. (ignored)\r\nWARNING: tokenization mismatch: 195 vs. 209. (ignored)\r\nWARNING: tokenization mismatch: 169 vs. 183. (ignored)\r\nWARNING: tokenization mismatch: 234 vs. 248. (ignored)\r\nWARNING: tokenization mismatch: 386 vs. 400. (ignored)\r\nWARNING: tokenization mismatch: 204 vs. 218. (ignored)\r\n\r\nThank you.</BODY>\n\n<COMMENTS>\n<Comment by Bleking at 2024-09-29T02:49:19Z>\nI solved the issue by changing the 'version' value in the [script](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task_lora.sh) as 'mpt' and initialising `cur_len = 1` in the \"preprocess_mpt\" function, because I am using 'mpt' instead of 'v1'. (I used this script as I have been finetuning the model with my custom dataset.)\r\n\r\nIt is because, according to this [tokenizer_config.json](https://huggingface.co/liuhaotian/llava-v1.6-34b/blob/main/tokenizer_config.json), the eos_token of v1.6-34B is `<|im_end|>`, not `</s>`, according to this [tokenizer_config.json](https://huggingface.co/liuhaotian/llava-v1.6-34b/blob/main/tokenizer_config.json). We have to check the way of tokenization carefully edit the script accordingly.\r\n\r\nFor anyone facing the similar issue, I hope this was helpful! ;)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1705,
    "state": "open",
    "created_by": "km1994",
    "created_at": "2024-09-17T01:04:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1705</URL>\n\n<TITLE>[Question] 想学习【MLLM多模态】快加学习交流群</TITLE>\n\n<BODY>### Question\n\n![多模态](https://github.com/user-attachments/assets/d39f54f3-7eda-4fc7-b63f-48dbb513f49c)\r\n加wx: yzyykm666，备注：MMLLM ，邀请你加群哈！！！</BODY>\n\n<COMMENTS>\n<Comment by Evrisus at 2024-10-18T06:30:39Z>\n过期了\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1704,
    "state": "open",
    "created_by": "20191864218",
    "created_at": "2024-09-15T13:22:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1704</URL>\n\n<TITLE>[Question] What is the optimizer of LLaVA?</TITLE>\n\n<BODY>### Question\n\nHello！I have a few questions:\r\n1、What is the optimizer of LLaVA? Is it AdamW?\r\n2、What is the Scheduler of LLaVA? Is Cosine Annealing with Linear Warmup?\r\n3、If you do LROA fine-tuning, there are two parameters, --mm_projector_lr and --learning_rate, what do they mean?\r\n\r\nThank you!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1703,
    "state": "open",
    "created_by": "lmingze",
    "created_at": "2024-09-13T08:27:28Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1703</URL>\n\n<TITLE>How to judge whether we train the vision tower during our lora-fine tuning</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nthis is my training file.\r\n\r\n#!/bin/bash\r\n\r\ndeepspeed --include localhost:3 --master_port=29510 /data10/LLaVA-main/llava/train/train_mem.py \\\r\n    --lora_enable True \\\r\n    --lora_r 64 \\\r\n    --lora_alpha 256 \\\r\n    --mm_projector_lr 2e-5 \\\r\n    --deepspeed /data10//LLaVA-main/scripts/zero2.json \\\r\n    --model_name_or_path /data9/kyusonglee/resources/vicuna-7b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path /data10/LLaVA-main/data_____________________/daily_summary/daily_summary_train_shop_merge_817_new.json \\\r\n    --image_folder /data9/nas/data3/public/public_data/coco/train2017 \\\r\n    --vision_tower /data9/kyusonglee/resources/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter /data10/LLaVA-main/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir /data10/mingze/LLaVA-main/output________________/daily_summary/daily_summary_v4 \\\r\n    --num_train_epochs 5 \\\r\n    --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 10000 \\\r\n    --save_total_limit 5 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to none \\</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1702,
    "state": "open",
    "created_by": "lmingze",
    "created_at": "2024-09-13T08:23:39Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1702</URL>\n\n<TITLE>what is the difference between liuhaotian/llava-v1.5-7b and (vicuna-7b-v1.5 + vision tower + mm_projector)</TITLE>\n\n<BODY>### Describe the issue\n\n#!/bin/bash\r\nthis is my .sh file.\r\ndeepspeed --include localhost:3 --master_port=29510 /data10/mingze/LLaVA-main/llava/train/train_mem.py \\\r\n    --lora_enable True \\\r\n    --lora_r 64 \\\r\n    --lora_alpha 256 \\\r\n    --mm_projector_lr 2e-5 \\\r\n    --deepspeed /data10/mingze/LLaVA-main/scripts/zero2.json \\\r\n    --model_name_or_path /data9/kyusonglee/resources/vicuna-7b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path /data10/mingze/LLaVAmain/data_____________________/daily_summary/daily_summary_train_shop_merge_817_new.json \\\r\n    --image_folder /data9/nas/data3/public/public_data/coco/train2017 \\\r\n    --vision_tower /data9/kyusonglee/resources/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter /data10/mingze/LLaVA-main/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir /data10/mingze/LLaVA-main/output________________/daily_summary/daily_summary_v4 \\\r\n    --num_train_epochs 5 \\\r\n    --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 10000 \\\r\n    --save_total_limit 5 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to none \\</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1701,
    "state": "open",
    "created_by": "sxchen123",
    "created_at": "2024-09-12T07:13:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1701</URL>\n\n<TITLE>[Question] Unable to generate pydantic-core schema for <class 'starlette.requests.Request'>. Set `arbitrary_types_allowed=True`</TITLE>\n\n<BODY>### Question\n\nraise PydanticSchemaGenerationError(\r\n2024-09-12 11:37:24 | ERROR | stderr | pydantic.errors.PydanticSchemaGenerationError: Unable to generate pydantic-core schema for <class 'starlette.requests.Request'>. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\r\n2024-09-12 11:37:24 | ERROR | stderr | \r\n2024-09-12 11:37:24 | ERROR | stderr | If you got this error by calling handler(<some type>) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(<some type>)` since we do not call `__get_pydantic_core_schema__` on `<some type>` otherwise to avoid infinite recursion.\r\n2024-09-12 11:37:24 | ERROR | stderr | \r\n2024-09-12 11:37:24 | ERROR | stderr | For further information visit https://errors.pydantic.dev/2.9/u/schema-for-unknown-type</BODY>\n\n<COMMENTS>\n<Comment by TzRain at 2024-09-16T13:35:07Z>\nDowngrading fastapi can solve the problem e.g.: \r\n\r\npip install fastapi==0.111.0\n</Comment>\n<Comment by VAllens at 2024-10-13T08:36:27Z>\n> Downgrading fastapi can solve the problem e.g.:\r\n> \r\n> pip install fastapi==0.111.0\r\n\r\nThis works for me, it corrects system errors.\n</Comment>\n<Comment by buaatian at 2024-11-10T14:30:00Z>\n> Downgrading fastapi can solve the problem e.g.:\r\n> \r\n> pip install fastapi==0.111.0\r\n\r\nThanks a lot !\n</Comment>\n<Comment by ErwinCheung at 2025-03-01T06:40:53Z>\n#1744 Refer to this issue\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1700,
    "state": "open",
    "created_by": "xlnn",
    "created_at": "2024-09-11T12:47:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1700</URL>\n\n<TITLE>[Question] May I ask if there is a 7B-sized file of Llama_2_7b_chat?</TITLE>\n\n<BODY>### Question\n\nThank you for your excellent work. May I ask if there is a 7B-sized file of Llama_2_7b_chat_freeze?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1699,
    "state": "open",
    "created_by": "shen1005",
    "created_at": "2024-09-10T09:03:02Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1699</URL>\n\n<TITLE>[Question] if not vision_tower.is_loaded: AttributeError: 'NoneType' object has no attribute 'is_loaded'</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1698,
    "state": "open",
    "created_by": "PixelChen24",
    "created_at": "2024-09-10T07:25:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1698</URL>\n\n<TITLE>[Usage] The difference between finetune_lora.sh and finetune_task_lora.sh</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nHi, you provided two scripts for finetuning with lora: `finetune_lora.sh` and `finetune_task_lora.sh`. What's the difference between their usages?\r\n\r\nThe most difference in this two scripts is: in `finetune_lora.sh`, the `model_name_or_path`(line 6) is set to be \"lmsys/vicuna-7b-v1.5\", and this value is \"liuhaotian/llava-v1.5-7b\" in `finetune_task_lora.sh`. Do you type it by mistake or it is exactly right? I'm really confused.\r\n\r\nFrom my best understanding, if I have enough data, I should use `finetune_lora.sh`; if I have relatively small amount of data, then I should use `finetune_task_lora.sh`. Am I right?</BODY>\n\n<COMMENTS>\n<Comment by AnnaGao0827 at 2024-09-24T09:24:46Z>\nHi there, I have been thinking the same question. From my point of view, finetune_lora.sh means finetune from the pretrained mlp layer, so there is an augment --pretrain_mm_mlp_adapter in  finetune_lora.sh, which in absent in  finetune_task_lora.sh. And finetune_task_lora.sh means finetune the llava model, such as llava-1.5, lllava-1.6... Hope that will be helpful.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1697,
    "state": "open",
    "created_by": "Liuqibaa",
    "created_at": "2024-09-10T05:55:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1697</URL>\n\n<TITLE>[Question] Why the link of LAION/CC/SBU BLIP-Caption Concept-balanced 558K Meta Data(meta.json) is empty?</TITLE>\n\n<BODY>### Question\r\n\r\nI want to download the dataset, but the link is turn to https://github.com/haotian-liu/LLaVA/blob/main/docs/Data.md</BODY>\n\n<COMMENTS>\n<Comment by Esthesia at 2024-09-27T08:58:29Z>\nIt seems the link is not updated.\r\nYou can find the link in the huggingface, I used this link for downloading.\r\nhttps://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain/blob/main/blip_laion_cc_sbu_558k_meta.json\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1696,
    "state": "closed",
    "created_by": "q2333gh",
    "created_at": "2024-09-08T12:32:35Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1696</URL>\n\n<TITLE>[Question] What base python version should `LLaVA`  use ? like  `3.10` ,`3.11` ?</TITLE>\n\n<BODY>### Question\n\nI searched for issues.not find answer.</BODY>\n\n<COMMENTS>\n<Comment by Weili-0234 at 2024-09-09T07:10:04Z>\nYou can refer to `2. Install Package` in the [README](https://github.com/haotian-liu/LLaVA#install) file, which uses `python=3.10`\n</Comment>\n<Comment by q2333gh at 2024-09-10T02:49:56Z>\nThanks alot ! I will try it ! \r\n\r\n> You can refer to `2. Install Package` in the [README](https://github.com/haotian-liu/LLaVA#install) file, which uses `python=3.10`\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1695,
    "state": "open",
    "created_by": "yuese1234",
    "created_at": "2024-09-08T03:04:02Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1695</URL>\n\n<TITLE>[Question] Why is the output always numerical when using model inference, like this</TITLE>\n\n<BODY>![屏幕截图 2024-09-08 104450](https://github.com/user-attachments/assets/99efc46e-851a-4f51-a14b-1b17b772ccd6)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1694,
    "state": "open",
    "created_by": "zhangzef",
    "created_at": "2024-09-07T15:04:29Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1694</URL>\n\n<TITLE>Can I load the parameters of llava1.5 with full parameter fine-tuning</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI want to fine-tune the full parameters of llava1.5, not only the llm and projector, but also the vision module. However, after I saved the trained parameters into ckpt files, I found that when loading llava in `load_pretrained_model` function, vision tower would not be initialized from the saved ckpt, but only from the original parameters of vit, because I found that after I passed the ckpt path to the `load_pretrained_model` function, It will still download vit parameters from huggingface, and will report the following warning:\r\n\r\nCommand:\r\n```python\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n        model_path, model_base=None,model_name=model_name, device_map={\"\": 'cuda'})\r\n```\r\n\r\nLog: \r\n```shell\r\nSome weights of the model checkpoint at ./models/RLAIF-V-7B were not used when initializing LlavaLlamaForCausalLM: ['model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight']\r\n- This IS expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n/root/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n```\r\n\r\nLooking at the `load_pretrained_model` function source code, I found that there was no mechanism to modify the vision tower name when initializing vision tower, and there was no local ckpt code logic to load. I can only change the loading address of vision tower by changing `mm_vision_tower` in `llava config`, but if I change `mm_vision_tower` to the path of ckpt, the model will fail to load.\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by jinghanSunn at 2024-12-02T05:38:23Z>\nHi @zhangzef, I have the same problem. Do you know how to load the full parameter?\n</Comment>\n<Comment by AngelAlita at 2025-02-20T12:30:53Z>\nhi👋, when I load the full parameter tuning model, I also encounter this problem, but the model can answer my question normally\n</Comment>\n<Comment by MassEast at 2025-05-07T14:01:43Z>\nSee [#485](https://github.com/haotian-liu/LLaVA/issues/485)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1693,
    "state": "closed",
    "created_by": "zhangzef",
    "created_at": "2024-09-07T14:51:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1693</URL>\n\n<TITLE>[Usage] can I load</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1692,
    "state": "open",
    "created_by": "YongliangMiao",
    "created_at": "2024-09-06T13:08:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1692</URL>\n\n<TITLE>Error while fine-tuning llava-v1.6 using finetune_task_lora.sh：DeepSpeed Zero-3 is not compatible with `low_cpu_mem_usage=True` or with passing a `device_map`.</TITLE>\n\n<BODY>Issue:\r\n\r\nDear Haotian,\r\n\r\nCongratulations on your outstanding work!  \r\n\r\nI currently encountered an issue while trying to fine-tune llava-v1.6 using the script scripts/v1_5/finetune_task_lora.sh. I ran the following command:\r\n\r\nCommand:\r\n```\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path liuhaotian/llava-v1.6-vicuna-7b \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/output_data.json \\\r\n    --image_folder ./playground/data/image \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.6-7b-task-qlora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb \\\r\n    --bits 4  # 指定使用 4-bit 量化来启用 QLoRA\r\n\r\n```\r\n\r\nLog: \r\n```\r\nValueError: DeepSpeed Zero-3 is not compatible with `low_cpu_mem_usage=True` or with passing a `device_map`.\r\n\r\n```\r\n\r\nI have not made any modifications to the original codebase. Could you please advise on how to resolve this issue?\r\n\r\nAdditionally, I would like to confirm whether adding --bits 4 at the end of the script is sufficient to enable QLoRA settings.\r\n\r\nThank you for your assistance!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1691,
    "state": "open",
    "created_by": "TimoFan1998",
    "created_at": "2024-09-05T08:10:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1691</URL>\n\n<TITLE>[Question] How can I this truble when i use LLaVa_Med?</TITLE>\n\n<BODY>### Question\n\nYou are using a model of type llava_mistral to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1690,
    "state": "open",
    "created_by": "bdv29",
    "created_at": "2024-09-04T18:32:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1690</URL>\n\n<TITLE>[Question] A series of questions about fine tuning , I want to learn this stuff</TITLE>\n\n<BODY>### Question\n\nI am interested in fine-tuning data.\r\n\r\nI see from the description that this uses CLIP as part of the model architecture. What I'd like to know is whether this is a completely frozen part of the model or if it gets trained as well. That is, during the fine-tune, is the visual component trained such that it can be taught to learn what new things look like? I know the guide said in pretraining it is frozen, but I did not see any mention of unfreezing. If there is a step needed to train the model with an unfrozen image portion, is there a parameter that needs to be passed or a config value to be used? If it is trained, is there a way to get CLIP back just to see how the training through an LLM affected it?\r\n\r\nHow does a batch during training here work? Is it just one image question and answer pair that is sent to the GPU if the batch size is 1?\r\n\r\nOn that matter, since custom data is in this form:\r\n\r\n\r\n[\r\n  {\r\n    \"id\": \"997bb945-628d-4724-b370-b84de974a19f\",\r\n    \"image\": \"part-000001/997bb945-628d-4724-b370-b84de974a19f.jpg\",\r\n    \"conversations\": [\r\n      {\r\n        \"from\": \"human\",\r\n        \"value\": \"<image>\\nWrite a prompt for Stable Diffusion to generate this image.\"\r\n      },\r\n      {\r\n        \"from\": \"gpt\",\r\n        \"value\": \"a beautiful painting of chernobyl by nekro, pascal blanche, john harris, greg rutkowski, sin jong hun, moebius, simon stalenhag. in style of cg art. ray tracing. cel shading. hyper detailed. realistic. ue 5. maya. octane render. \"\r\n      },\r\n    ]\r\n  },\r\n  ...\r\n]\r\n\r\n\r\nCan conversations have multiple human-GPT prompts? That is, can, in the context of fine-tuning LLaMA, the text portion be a series of closed-ended questions and closed-ended answers so long as it's in the form of human then GPT, human then GPT, etc.? Also, can a different id reference the same image?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1687,
    "state": "open",
    "created_by": "Leo-Lsc",
    "created_at": "2024-09-03T12:06:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1687</URL>\n\n<TITLE>OSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory ./checkpoints/vicuna-7b-v1.5.</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nTraceback (most recent call last):\r\n  File \"/remote_shome/songdj/workspace/cont/cont_llava/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"/remote_shome/songdj/workspace/cont/cont_llava/LLaVA/llava/train/train.py\", line 827, in train\r\n    model = LlavaLlamaForCausalLM.from_pretrained(\r\n  File \"/remote_shome/songdj/anaconda3/envs/contllava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3338, in from_pretrained\r\n    raise EnvironmentError(\r\nOSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory ./checkpoints/vicuna-7b-v1.5.\r\n我在运行以下脚本的时候遇到报错：\r\n#!/bin/bash\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path ./checkpoints/llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/llava_v1_5_mix665k.json \\\r\n    --image_folder ./playground/data \\\r\n    --vision_tower ./checkpoints/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./no_cont/llava-v1.5-7b\\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1686,
    "state": "open",
    "created_by": "tanghao2118",
    "created_at": "2024-09-02T05:20:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1686</URL>\n\n<TITLE>[Question] Why is the accuracy low when I evaluate llava-v1.5-7b-lora on VQAv2 ?</TITLE>\n\n<BODY>### Question\r\n\r\nI downloaded '_llava-1.5-7b_' as '_model_base_', and downloaded the lora wights '_llava-v1.5-7b-lora_' as '_model_path_'.\r\nI ran the vqav2.sh provided by the author, trying to reproduce the results of llava-v1.5-7b-lora on VQAv2.\r\nHowever, I got the accuracy of 74%, much lower than the reported result of 79%.\r\nWhat should I do to exactly reproduce the results?\r\n\r\nThe cotent of vqav2.sh is as follows:\r\n\r\n```\r\n#!/bin/bash\r\n\r\ngpu_list=\"${CUDA_VISIBLE_DEVICES:-0}\"\r\nIFS=',' read -ra GPULIST <<< \"$gpu_list\"\r\n\r\nCHUNKS=${#GPULIST[@]}\r\n\r\nCKPT=\"llava-v1.5-7b-lora-baseline\"\r\nSPLIT=\"llava_vqav2_mscoco_test-dev2015\"\r\n\r\nfor IDX in $(seq 0 $((CHUNKS-1))); do\r\n    CUDA_VISIBLE_DEVICES=${GPULIST[$IDX]} python -m llava.eval.model_vqa_loader \\\r\n        --model-path ./model_downloaded/llava-v1.5-7b-lora \\\r\n        --question-file /disk2/dataset/llava_data/eval/vqav2/$SPLIT.jsonl \\\r\n        --image-folder /disk2/dataset/llava_data/eval/vqav2/test2015 \\\r\n        --answers-file /disk2/dataset/llava_data/eval/vqav2/answers/$SPLIT/$CKPT/${CHUNKS}_${IDX}.jsonl \\\r\n        --num-chunks $CHUNKS \\\r\n        --chunk-idx $IDX \\\r\n        --temperature 0 \\\r\n        --conv-mode vicuna_v1 &\r\ndone\r\n```\r\n\r\n\r\nThe hyperparameters in 'model_vqa_loader.py' are set as follows:\r\n\r\n\r\n```\r\n    parser.add_argument(\"--model-path\", type=str, default=\"facebook/opt-350m\")\r\n\r\n    parser.add_argument(\"--model-base\", type=str, default='./model_downloaded/llava-1.5-7b-hf')\r\n\r\n    parser.add_argument(\"--image-folder\", type=str, default=\"\")\r\n\r\n    parser.add_argument(\"--question-file\", type=str, default=\"tables/question.jsonl\")\r\n\r\n    parser.add_argument(\"--answers-file\", type=str, default=\"answer.jsonl\")\r\n\r\n    parser.add_argument(\"--conv-mode\", type=str, default=\"llava_v1\")\r\n\r\n    parser.add_argument(\"--num-chunks\", type=int, default=1)\r\n\r\n    parser.add_argument(\"--chunk-idx\", type=int, default=0)\r\n\r\n    parser.add_argument(\"--temperature\", type=float, default=0.2)\r\n\r\n    parser.add_argument(\"--top_p\", type=float, default=None)\r\n\r\n    parser.add_argument(\"--num_beams\", type=int, default=1)\r\n\r\n    parser.add_argument(\"--max_new_tokens\", type=int, default=128)\r\n\r\n```</BODY>\n\n<COMMENTS>\n<Comment by OedoSoldier at 2024-09-12T15:30:07Z>\nI think the Lora is based on Vicuna 1.5, so you may use Vicuna 1.5 as the model base.\n</Comment>\n<Comment by JinXins at 2024-09-25T17:50:01Z>\nHi, @tanghao2118 Did you solve it? I got the same trouble. :)\n</Comment>\n<Comment by nguyenrtm at 2024-10-22T15:33:35Z>\nAlso got the problem here. Has anyone solved this?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1685,
    "state": "open",
    "created_by": "whitepo1991",
    "created_at": "2024-08-31T12:32:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1685</URL>\n\n<TITLE>[Question] Any ideas on how to design a customized loss function for LLaVA and integrate several new layers that would be compatible with it?</TITLE>\n\n<BODY>### Question\n\nI am working on adapting LLaVA into a task-specific assistant to solve a regression problem. Could anyone provide some guidance on where to start? From what I understand, I need to locate the functions responsible for computing loss and the model architecture to add an MLP layer. However, I'm not entirely sure where these functions are located in the codebase.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1683,
    "state": "open",
    "created_by": "nub2927",
    "created_at": "2024-08-30T13:50:59Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1683</URL>\n\n<TITLE>[Question] how much vram for batch 1 full fine tune?</TITLE>\n\n<BODY>### Question\n\na couple months back i tried fine tuning on 80GB A100 but got cuda out of memoery issue, \r\nso how much vram does the full fine tune need, I need to know how many GPUs I need to be using for this</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1682,
    "state": "open",
    "created_by": "wushh",
    "created_at": "2024-08-30T08:17:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1682</URL>\n\n<TITLE>[Question] 歪曲了历史，数据源精美精西</TITLE>\n\n<BODY>### Question\r\n\r\n汉奸模型？\r\n\r\n>>> 八国联军是什么？干了什么事情？\r\n 八国联军是一个历史上非常重要的军事机组，其成员来自中国、台湾、越南等地区</BODY>\n\n<COMMENTS>\n<Comment by wushh at 2024-08-30T08:46:51Z>\n问了很多类似问题，一个精美精西模型。\n</Comment>\n<Comment by wushh at 2024-08-30T09:31:33Z>\n林则徐是干什么的？有什么功绩？他知道。\r\n林则徐虎门销烟事件是什么？shit，他居然不知道\r\n洗白大模型，shit\n</Comment>\n<Comment by wushh at 2024-08-30T09:34:29Z>\n真好笑，shit\r\n>>> 英国发动了鸦片战争，介绍一下。\r\n 您所说的英国发动的“鸦片战争”是指中华民族在清朝末年和晋代初期发生的一场战争，主要\r\n是针对蒙古进行的。\r\n\r\n该战争起因于清朝为了维护自己的政权和统治地位，对于蒙古的掠夺和入侵而发动攻击。英国在\r\n这次战争中发挥了重要的外交和军事角色。\r\n\r\n首先，英国派出了军队，如八国联军、南宋、金刚山等，组成了一座防御垒。其次，英国通过与\r\n蒙古谈判解决问题，渐行地在这场战争中发挥了巨大的外交和军事作用。\r\n\r\n鸦片战争最终结束后，清朝成功维护了自己的政权和统治地位。英国则在历史上得到了更多的权\r\n力和利益，并在中华民族历史上发挥了一定的影响作用。\n</Comment>\n<Comment by change-every at 2024-09-25T08:08:52Z>\n哈哈哈哈哈，笑死了\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1681,
    "state": "open",
    "created_by": "XiaoruiMaLU",
    "created_at": "2024-08-30T07:52:10Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1681</URL>\n\n<TITLE>[Usage] Adding very few parameters when using LoRA to finetune LLaVA 1.5</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: When using LoRA to fine-tune LLaVA using the ScienceQA train split, 628 parameters are trained for 7B LM, and 754 paras are trained for 13B LM. In addition, the accuracy of this fine-tuned model on ScienceQA is low.\r\n\r\nCommand:\r\n```\r\nbash ./scripts/v1_5/finetune_lora.sh\r\n```\r\n\r\nLog: \r\n```\r\nAdding LoRA adapters...\r\n[2024-08-30 15:14:21,550] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 628, num_elems = 7.04B\r\n\r\nAdding LoRA adapters...\r\n[2024-08-30 15:14:21,550] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 754, num_elems = 13.32B\r\n```\r\n\r\nThe script 'finetune_lora.sh':\r\n```\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path lmsys/vicuna-13b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path ./data/scienceqa/llava_train_QCM-LEA.json \\\r\n    --image_folder ./data/images/train \\\r\n    --vision_tower ./openai/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-13b-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nThe test result of the 7B model:\r\n```\r\nTotal: 4241, Correct: 1568, Accuracy: 36.97%, IMG-Accuracy: 67.49%\r\n```</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1680,
    "state": "open",
    "created_by": "KansaiUser",
    "created_at": "2024-08-29T09:39:29Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1680</URL>\n\n<TITLE>[Question] Custom Conversations</TITLE>\n\n<BODY>### Question\n\nRight now in `conversation.py` we have a number of different conversations and a conv_template.\r\n\r\nWhat would happen if any one wants to change the system element of a conversation?  Has there been any attempts to create custom conversations?\r\n\r\nI am thinking of editing `conversation.py` in order to be able to do this, but I thought I ask first if there has been people doing it</BODY>\n\n<COMMENTS>\n<Comment by Yuxin916 at 2025-08-09T17:03:58Z>\nHave you give it a try?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1679,
    "state": "open",
    "created_by": "RobitsG",
    "created_at": "2024-08-29T07:06:56Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1679</URL>\n\n<TITLE>[Usage] Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint</TITLE>\n\n<BODY>When I use `v1_5/finetune_task.sh` to fine-tune `llava-v1.5-7b`, I receive a warning: 'Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at /root/paddlejob/workspace/env_run/space/output/POVID/llava-v1.5-7b and are newly initialized: ['model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight',...'\r\nAdditionally, my image features are resulting in NaN values. I have verified that both my image path and the CLIP model path are valid.\r\nThe error only occurs before and after encode_images. In the following code, the first print outputs False, and the second print outputs True:\r\nprint(torch.isnan(images).any())  \r\nimage_features = self.encode_images(images)  \r\nprint(torch.isnan(image_features).all())  \r\n```\"</BODY>\n\n<COMMENTS>\n<Comment by XCF-Mike at 2024-08-29T10:22:31Z>\nHello, I encountered the same issue when using the Lora finetune model for mmbench evaluation\r\n![image](https://github.com/user-attachments/assets/2208b0f2-4586-4d87-b397-eb007e3aa650)\r\ndid you solve it\n</Comment>\n<Comment by RobitsG at 2024-08-29T10:40:45Z>\n> Hello, I encountered the same issue when using the Lora finetune model for mmbench evaluation ![image](https://private-user-images.githubusercontent.com/172706626/362638612-2208b0f2-4586-4d87-b397-eb007e3aa650.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjQ5MjgxODAsIm5iZiI6MTcyNDkyNzg4MCwicGF0aCI6Ii8xNzI3MDY2MjYvMzYyNjM4NjEyLTIyMDhiMGYyLTQ1ODYtNGQ4Ny1iMzk3LWViMDA3ZTNhYTY1MC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwODI5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDgyOVQxMDM4MDBaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT04OTRmY2RkNzEyOTBhZDFmMTJkNTMxY2E4YzRhYWNlYTczMzZhOGRhODBhZTUxYTNkMGYyMjI5YmVhZDcyOTVjJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.T5eN8LI4YzT1KWi01zdQP1c0METbUxiQ5jS6YpBt6IQ) did you solve it\r\n\r\nI'm still trying to solve it, but you can take a look at the solution for issue #672 .\n</Comment>\n<Comment by wannanfeng at 2024-11-19T16:00:53Z>\nHello, after loading the model weights with LoRA, I encountered the same issue where the model outputs were blank. Have you found a solution? Thank you.\n</Comment>\n<Comment by zhutianlee at 2025-02-12T12:05:34Z>\nmaybe need to merge delta part\n</Comment>\n<Comment by ybrrraway at 2025-04-12T07:59:14Z>\nyou can ignore it\n</Comment>\n<Comment by ThisisBillhe at 2025-07-05T03:01:41Z>\nSame here. The weights listed will be newly initialized and lead to nan, so it can't just be ignored.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1678,
    "state": "open",
    "created_by": "williamium3000",
    "created_at": "2024-08-28T15:39:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1678</URL>\n\n<TITLE>[Usage] Can you release the code for concept balancing filtering in pretrain data?</TITLE>\n\n<BODY>### Describe the issue\n\nHi Haotian,\r\nThanks for the great work!!\r\nI am wondering if it's possible to release the concept balancing filtering code for pretrain data. A pseudo code snippet would also work.\r\n\r\nBest\r\nYijiang</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1677,
    "state": "open",
    "created_by": "KansaiTraining",
    "created_at": "2024-08-28T09:38:35Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1677</URL>\n\n<TITLE>[Question] why model generate takes more time the second time?</TITLE>\n\n<BODY>### Question\r\n\r\nI am trying this script\r\n\r\n```\r\nfrom llava.eval.run_llavaEXP import eval_model\r\nfrom llava.mm_utils import get_model_name_from_path\r\n\r\nmodel_path = \"liuhaotian/llava-v1.5-7b\"\r\n# prompt = \"What are the things I should be cautious about when I visit here?\"\r\nprompt = \"Describe the image\"\r\n\r\nimage_file = \"https://llava-vl.github.io/static/images/view.jpg\"\r\n\r\nargs = type('Args', (), {\r\n    \"model_path\": model_path,\r\n    \"model_base\": None,\r\n    \"model_name\": get_model_name_from_path(model_path),\r\n    \"query\": prompt,\r\n    \"conv_mode\": None,\r\n    \"image_file\": image_file,\r\n    \"sep\": \",\",\r\n    \"temperature\": 0,\r\n    \"top_p\": None,\r\n    \"num_beams\": 1,\r\n    \"max_new_tokens\": 512\r\n})()\r\n\r\nconv,outputs =eval_model(args)\r\n\r\nprint(f\"conv {conv}\")\r\n\r\n# image_file = \"https://llava-vl.github.io/static/images/titanic.jpg\"\r\nimage_file = \"https://llava-vl.github.io/static/images/view.jpg\"\r\n\r\n\r\nargs = type('Args', (), {\r\n    \"model_path\": model_path,\r\n    \"model_base\": None,\r\n    \"model_name\": get_model_name_from_path(model_path),\r\n    \"query\": prompt,\r\n    \"conv_mode\": None,\r\n    \"image_file\": image_file,\r\n    \"sep\": \",\",\r\n    \"temperature\": 0,\r\n    \"top_p\": None,\r\n    \"num_beams\": 1,\r\n    \"max_new_tokens\": 512\r\n})()\r\n\r\nconv,outputs =eval_model(args)\r\n\r\nprint(f\"conv {conv}\")\r\n\r\n```\r\n\r\nAnd even though I am using the same image, the second time it takes much more time to get the answer. \r\nI have identified the part that is taking time as the following\r\n\r\n```\r\n   print(\"Tokenizer done\")\r\n\r\n    with torch.inference_mode():\r\n        output_ids = model.generate(\r\n            input_ids,\r\n            images=images_tensor,\r\n            image_sizes=image_sizes,\r\n            do_sample=True if args.temperature > 0 else False,\r\n            temperature=args.temperature,\r\n            top_p=args.top_p,\r\n            num_beams=args.num_beams,\r\n            max_new_tokens=args.max_new_tokens,\r\n            use_cache=True,\r\n        )\r\n    print(\"Inference done\")\r\n\r\n```\r\nThis part (model.generate) takes much more time the second time.\r\n\r\nAny idea why this could be happening?\r\n\r\nI want to try this with 200 images but I wonder if this is feasible....</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1676,
    "state": "open",
    "created_by": "zou-yawen",
    "created_at": "2024-08-28T02:03:55Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1676</URL>\n\n<TITLE>[Question]  Is there a parameter to limit the amount of text output, or is the output more streamlined</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1675,
    "state": "closed",
    "created_by": "dongbinShin96",
    "created_at": "2024-08-27T07:57:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1675</URL>\n\n<TITLE>[Question] How can I use CLIPVisionTowerS2?</TITLE>\n\n<BODY>### Question\n\nI checked predict.py.\r\n\r\nBut I want to use CLIPVisionTowerS2. In that case, maybe I should change somethings.\r\nCan I know methods?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1674,
    "state": "open",
    "created_by": "flab305",
    "created_at": "2024-08-24T16:30:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1674</URL>\n\n<TITLE>[Question] How to get output embeddings or last hidden states?</TITLE>\n\n<BODY>### Question\n\nCould someone explain how to integrate a simple linear layer on top of the LLaVA model for a classification task, similar to what's implemented in LlamaForSequenceClassification? Specifically, is there a method to **access** the **last hidden states** or **output embeddings** that can be used as input for a classifier?\r\n\r\nI don’t want to fine-tune the LLaVA model because my dataset is quite small. Instead, I have a classification task, and it would be more effective to take the LLaVA model inference, feed the embeddings or last hidden states into a linear layer, and train it. However, I’m not sure how to implement this. Could someone please guide me?</BODY>\n\n<COMMENTS>\n<Comment by YizhuoQ at 2024-12-30T09:11:43Z>\nI have the same issue. Have you solve this problem?\n</Comment>\n<Comment by cpystan at 2025-03-18T09:38:21Z>\nI have the same problem. Have you solved this problem now? If you could give me some suggestions, I would be very grateful!\n</Comment>\n<Comment by AlainFidahoussen at 2025-04-11T21:17:44Z>\nHello,\nIs anyone found the answer?\nThanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1673,
    "state": "open",
    "created_by": "sayanbiswas59",
    "created_at": "2024-08-23T22:38:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1673</URL>\n\n<TITLE>[Question] Understanding the Licensing and Commercial Use of LLaVa-NeXT</TITLE>\n\n<BODY>### Question\n\nI am considering the integration of LLaVa-NeXT into my organization's workflow for an Image Classification Task. The specific checkpoint I anticipate using is: https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf. \r\n\r\nHowever, I have queries regarding the licensing of the model. I am particularly interested in understanding how the \"**Open AI Terms of Use**\" apply if I decide to use this model commercially and within my organization's setup.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1672,
    "state": "open",
    "created_by": "hxhcreate",
    "created_at": "2024-08-22T11:56:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1672</URL>\n\n<TITLE>[Question] The difference between llava ckpt and llava_hf ckpt?</TITLE>\n\n<BODY>### Question\n\nI'm aware that there are two ways to implement llava: llava origin and llava_hf in huggingface.\r\n\r\nI'm wondering if llava_hf uses the code they implemented to re-train on the data to get llava_hf weights, or, if llava_hf's weights are gotten directly from llava origin from somehow\r\n\r\nthanks</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1670,
    "state": "closed",
    "created_by": "wanlipeng",
    "created_at": "2024-08-22T02:23:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1670</URL>\n\n<TITLE>[Usage] pretrain about \"gradient_accumulation_steps\" para use</TITLE>\n\n<BODY>### Describe the issue\n\nWhen I pre-train, the parameter gradient_accumulation_steps is set to 1, and the total training step length is 23256. However, when I set gradient_accumulation_steps to 4, the total training step length is 23256/4=5814. Will the gradient accumulation parameter affect the total training step length? Is it different from the usual torch training?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1669,
    "state": "open",
    "created_by": "insinfo",
    "created_at": "2024-08-21T18:50:56Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1669</URL>\n\n<TITLE>[Feature request] Add better support for Brazilian Portuguese</TITLE>\n\n<BODY>### feature\r\n\r\nI did a test to OCR scanned documents in Brazilian Portuguese, and I saw that LLaVA makes a lot of mistakes on scanned documents in Portuguese\r\n\r\n#### result from https://huggingface.co/spaces/badayvedat/LLaVA \r\nPrompt: transcribe image \r\n\r\n![1-1](https://github.com/user-attachments/assets/9e4fdb87-beec-4a44-8407-f1fb1969fd1e)\r\n\r\n\r\n```\r\nThe image features a white piece of paper with a stamp on it. The stamp is placed in the upper left corner of the paper, and it appears to be a government document. The paper is also marked with a signature, which is located in the middle of the document. The overall appearance of the paper suggests that it is an official document, possibly related to a government agency or a legal matter.\r\n```\r\n\r\n#### the correct thing would be\r\n```console\r\nHere is the transcription of the text from the image:\r\n\r\nESTADO DO RIO DE JANEIRO\r\nPrefeitura Municipal de Rio das Ostras\r\nPROTOCOLO GERAL\r\n\r\nProcesso: 18457 / 2003\r\nData: 03/09/2003\r\nHora: 10:53:56\r\nRequerente: COSCARELLI E CIA LTDA ME\r\nSec. Destino: Secretaria Municipal de Fazenda\r\nDept. Destino: Depto. de Tributos e Fiscalização\r\nAssunto: ALVARÁ\r\n```\r\n\r\n\r\n#### result \r\n\r\n![110-1](https://github.com/user-attachments/assets/189ec354-dfbd-4352-90cb-f6085409acc9)\r\n\r\n\r\n```\r\nThe image features a white piece of paper with a sticker on it. The sticker has a barcode and a label, which reads \"Estado do Rio de Janeiro.\" The paper appears to be a certificate or a document related to the city of Rio de Janeiro. The paper is placed on a table, and the sticker is prominently displayed on the surface.\r\n```\r\n\r\n\r\n#### the correct thing would be\r\n```console\r\nHere is the transcription of the text from the image:\r\n\r\nESTADO DO RIO DE JANEIRO\r\nPrefeitura Municipal de Rio das Ostras\r\nPROTOCOLO GERAL\r\n\r\nProcesso: 15314 / 2003\r\nData: 25/07/2003\r\nHora: 16:18:28\r\n\r\nRequerente: COLÔNIA DE PESCADORES Z-22\r\nSec. Destino: Sec. Mun. Urbanismo Obras e S. Pub.\r\nDept. Destino: 0\r\nAssunto: AGRADECIMENTO / FAZ\r\n```\r\n\r\n![image](https://github.com/user-attachments/assets/0fbbb3c5-2ef1-4721-97a1-82f41e4778ff)</BODY>\n\n<COMMENTS>\n<Comment by insinfo at 2024-08-21T19:13:54Z>\nperhaps training on this dataset could help improve accuracy\r\nhttps://zenodo.org/records/7872951\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1668,
    "state": "open",
    "created_by": "liucheny",
    "created_at": "2024-08-21T08:11:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1668</URL>\n\n<TITLE>[Usage] about fine tuning</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\ni want to fine tuning llava-med-v1.5-mistral-7b with llava,but it shows error\r\nCommand:\r\n```\r\nCUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nnodes=1 --nproc_per_node=4 --master_port=25001\\\r\n llava/train/train_mem.py \\\r\n--lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n--deepspeed ./scripts/zero3.json \\\r\n--model_name_or_path /project/LLaVA-Med/llava-med-v1.5-mistral-7b \\\r\n--version v1 \\\r\n--data_path /project/LLaVA-Med/data/Slake/train.json \\\r\n--image_folder /project/LLaVA-Med/data/Slake/imgs \\\r\n--vision_tower /project/LLaVA-Med/clip-vit-large-patch14-336 \\\r\n--mm_projector_type mlp2x_gelu \\\r\n--mm_vision_select_layer -2 \\\r\n--mm_use_im_start_end False \\\r\n--mm_use_im_patch_token False \\\r\n--image_aspect_ratio pad \\\r\n--group_by_modality_length True \\\r\n--bf16 True \\\r\n--output_dir ./checkpoints/llava-version1 \\\r\n--num_train_epochs 1 \\\r\n--per_device_train_batch_size 4 \\\r\n--per_device_eval_batch_size 4 \\\r\n--gradient_accumulation_steps 1 \\\r\n--evaluation_strategy \"no\" \\\r\n--save_strategy \"steps\" \\\r\n--save_steps 50000 \\\r\n--save_total_limit 1 \\\r\n--learning_rate 2e-4 \\\r\n--weight_decay 0. \\\r\n--warmup_ratio 0.03 \\\r\n--lr_scheduler_type \"cosine\" \\\r\n--logging_steps 1 \\\r\n--tf32 True \\\r\n--model_max_length 2048 \\\r\n--gradient_checkpointing True \\\r\n--dataloader_num_workers 2 \\\r\n--lazy_preprocess True \\\r\n--report_to wandb \r\n```\r\n\r\nLog: \r\n```\r\n2024-08-21 08:05:37,612] torch.distributed.run: [WARNING] \r\n[2024-08-21 08:05:37,612] torch.distributed.run: [WARNING] *****************************************\r\n[2024-08-21 08:05:37,612] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n[2024-08-21 08:05:37,612] torch.distributed.run: [WARNING] *****************************************\r\n[2024-08-21 08:05:46,749] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-08-21 08:05:46,752] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-08-21 08:05:46,753] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-08-21 08:05:46,760] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-08-21 08:05:55,192] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2024-08-21 08:05:55,192] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n[2024-08-21 08:05:55,203] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2024-08-21 08:05:55,218] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2024-08-21 08:05:55,224] [INFO] [comm.py:637:init_distributed] cdb=None\r\nYou are using a model of type llava_mistral to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nYou are using a model of type llava_mistral to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nYou are using a model of type llava_mistral to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nYou are using a model of type llava_mistral to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\n[2024-08-21 08:06:02,639] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1512 closing signal SIGTERM\r\n[2024-08-21 08:06:03,054] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -7) local_rank: 1 (pid: 1513) of binary: /root/anaconda3/envs/llava/bin/python\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/envs/llava/bin/torchrun\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/run.py\", line 806, in main\r\n    run(args)\r\n  File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/run.py\", line 797, in run\r\n    elastic_launch(\r\n  File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n  File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \r\n====================================================\r\nllava/train/train_mem.py FAILED\r\n----------------------------------------------------\r\nFailures:\r\n[1]:\r\n  time      : 2024-08-21_08:06:02\r\n  host      : 9c813e5131ac\r\n  rank      : 2 (local_rank: 2)\r\n  exitcode  : -7 (pid: 1514)\r\n  error_file: <N/A>\r\n  traceback : Signal 7 (SIGBUS) received by PID 1514\r\n[2]:\r\n  time      : 2024-08-21_08:06:02\r\n  host      : 9c813e5131ac\r\n  rank      : 3 (local_rank: 3)\r\n  exitcode  : -7 (pid: 1515)\r\n  error_file: <N/A>\r\n  traceback : Signal 7 (SIGBUS) received by PID 1515\r\n----------------------------------------------------\r\nRoot Cause (first observed failure):\r\n[0]:\r\n  time      : 2024-08-21_08:06:02\r\n  host      : 9c813e5131ac\r\n  rank      : 1 (local_rank: 1)\r\n  exitcode  : -7 (pid: 1513)\r\n  error_file: <N/A>\r\n  traceback : Signal 7 (SIGBUS) received by PID 1513\r\n====================================================\r\n```</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1667,
    "state": "open",
    "created_by": "MonoMarkor",
    "created_at": "2024-08-20T09:36:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1667</URL>\n\n<TITLE>[Usage] From Hugging face llava gives no response</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\ni am giving a localy saved png file as an input, the model is working but gives no output for a png file. it gives a responce but it does not answer what the picture is about\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```from PIL import Image\r\n\r\nimport torch\r\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\r\n\r\n\r\nmodel_id = \"llava-hf/bakLlava-v1-hf\"\r\nmodel = LlavaForConditionalGeneration.from_pretrained(\r\n    model_id, \r\n    torch_dtype=torch.float16, \r\n    low_cpu_mem_usage=True, \r\n).to(0)\r\n\r\nprocessor = AutoProcessor.from_pretrained(model_id)\r\n\r\nconversation = [\r\n    {\r\n\r\n      \"role\": \"user\",\r\n      \"content\": [\r\n          {\"type\": \"text\", \"text\": \"What does this picture show?\"},\r\n          {\"type\": \"image\"},\r\n        ],\r\n    },\r\n]\r\nprompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\r\n\r\n\r\nraw_image = Image.open('pics/image-2024-4-12_11-1-17.png')\r\ninputs = processor(images=raw_image, text=prompt, return_tensors='pt').to(0)\r\n\r\noutput = model.generate(**inputs, max_new_tokens=200)\r\nprint(processor.decode(output[0]))\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\r\n[c:\\Users\\ahsan\\localrag\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:480](file:///C:/Users/ahsan/localrag/Lib/site-packages/transformers/models/clip/modeling_clip.py:480): UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at [C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555](file:///C:/actions-runner/_work/pytorch/pytorch/builder/windows/pytorch/aten/src/ATen/native/transformers/cuda/sdp_utils.cpp:555).)\r\n  attn_output = torch.nn.functional.scaled_dot_product_attention(\r\n<s> [INST] <image> \r\nWhat does this picture show? [/INST]</s>\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1666,
    "state": "open",
    "created_by": "laksh-2193",
    "created_at": "2024-08-20T05:16:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1666</URL>\n\n<TITLE>[Usage] Error - `ollama._types.ResponseError: error parsing llm response stream: error: {\"content\":\"internal_error\"}`</TITLE>\n\n<BODY>### Describe the issue\n\nCode : \r\n```\r\nimport ollama\r\n\r\n\r\nprompt = \"What all finanicial data is there in this pdf?\"\r\nimage = \"bank statement.pdf\"\r\n\r\nresult = ollama.generate(\r\n        model='llava',\r\n        prompt=prompt,\r\n        images=[image],\r\n    )\r\n\r\nprint(result)\r\n```\r\n\r\nLogs : \r\n```\r\npython main.py \r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/llava-demo/main.py\", line 7, in <module>\r\n    result = ollama.generate(\r\n             ^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/fastapi/env/lib/python3.12/site-packages/ollama/_client.py\", line 162, in generate\r\n    return self._request_stream(\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/fastapi/env/lib/python3.12/site-packages/ollama/_client.py\", line 98, in _request_stream\r\n    return self._stream(*args, **kwargs) if stream else self._request(*args, **kwargs).json()\r\n                                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/fastapi/env/lib/python3.12/site-packages/ollama/_client.py\", line 74, in _request\r\n    raise ResponseError(e.response.text, e.response.status_code) from None\r\nollama._types.ResponseError: error parsing llm response stream: error: {\"content\":\"internal_error\"}\r\n```</BODY>\n\n<COMMENTS>\n<Comment by SouravBeraAkaSpeed at 2024-08-25T21:14:11Z>\nI'm also getting the same issue\n</Comment>\n<Comment by netojoa at 2024-09-27T15:38:21Z>\nHad the same issue. I was also trying to send a PDF file. I believe it is not supported, you must convert it to an image before. After I converted the PDF to JPEG, it worked.\n</Comment>\n<Comment by stajilov at 2024-10-03T08:27:05Z>\nyes, with image it works fine... need to plug in additional service then\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1665,
    "state": "open",
    "created_by": "KansaiTraining",
    "created_at": "2024-08-19T04:57:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1665</URL>\n\n<TITLE>[Question] How to correctly write a prompt for this model?</TITLE>\n\n<BODY>### Question\n\nIn this page https://ollama.com/library/llava/blobs/c43332387573  it says the prompt should be\r\n\r\n[INST] {{ if .System }}{{ .System }} {{ end }}{{ .Prompt }} [/INST]\r\n\r\nI would like to ask for help in correctly writing a prompt to query the model.\r\n\r\nI have tried \r\n\r\n\"USER: <image>\\nWhat are the things I should be cautious about when I visit this place?\\nASSISTANT:\"\r\n\r\nbut I am not sure why the difference on formats</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1664,
    "state": "open",
    "created_by": "jiinhui",
    "created_at": "2024-08-17T08:37:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1664</URL>\n\n<TITLE>[Question] evaluation   problem  in  textVQA</TITLE>\n\n<BODY>### Question\n\nwhen  I   use  the  provided code \"llava.eval.eval_textvqa\"  to  test  the model  llava-v1.5-13b on  textVQA benchmark,  the acc  is  2.16%。   And  I  find  reason that  it   is  lack of  postprocesssing.\r\n   for  example ,the  output  of  the model  is  \"the brand of this camera is dakota digital.\" , and  the  gt  is  \"dakota digital \"  , but  they can not  match. \r\n    So  how  to  solve  the  problem? Thanks!</BODY>\n\n<COMMENTS>\n<Comment by liuxy1103 at 2024-11-26T08:30:45Z>\nsame question\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1663,
    "state": "closed",
    "created_by": "BountyMage",
    "created_at": "2024-08-16T01:32:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1663</URL>\n\n<TITLE>[Usage]  How do i get a LLaVA model converted to contain vision tower related config items?</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI was doing the image captioning via llava following the hpcaitech opensora project, facing the \"AttributeError: 'NoneType' object has no attribute 'is_loaded' \" issue.\r\nCommand:\r\n```\r\ntorchrun --nproc_per_node 1 --standalone -m tools.caption.caption_llava xxx/dataset/image.csv --tp-size 1 --dp-size 1 --bs 16 --prompt image-3ex --model-path xxx/llava-v1.6-mistral-7b\r\n```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/opensora-llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/opt/conda/envs/opensora-llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/opt/conda/envs/opensora-llava/lib/python3.10/site-packages/tools/caption/caption_llava.py\", line 345, in <module>\r\n    main(args)\r\n  File \"/opt/conda/envs/opensora-llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/envs/opensora-llava/lib/python3.10/site-packages/tools/caption/caption_llava.py\", line 84, in main\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(\r\n  File \"/data/workspace_zj/code/LLaVA-main/llava/model/builder.py\", line 156, in load_pretrained_model\r\n    if not vision_tower.is_loaded:\r\nAttributeError: 'NoneType' object has no attribute 'is_loaded'\r\n\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by BountyMage at 2024-08-16T01:35:18Z>\nref to a related issue\r\nhttps://github.com/haotian-liu/LLaVA/issues/15\n</Comment>\n<Comment by BountyMage at 2024-08-16T02:54:07Z>\ndownloaded wrong model weight\r\ndownloaded https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf\r\nshould download https://huggingface.co/liuhaotian/llava-v1.6-mistral-7b\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1662,
    "state": "open",
    "created_by": "vedernikovphoto",
    "created_at": "2024-08-15T14:36:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1662</URL>\n\n<TITLE>[Question] Support for Multi-Image Input in One-Shot/Few-Shot Learning Scenarios</TITLE>\n\n<BODY>### Question\n\nI'm working on a task that requires inputting multiple images sequentially during a conversation with LLaVA, aiming to perform one-shot or few-shot learning. The idea is to start by showing a few example images with corresponding descriptions, which the model uses as context. Afterward, I want to input a new image and have the model classify it based on the previously shown examples.\r\n\r\nCould you provide guidance or examples on how to technically implement this within the existing framework? Specifically, I'm looking for the best way to maintain and utilize the context of multiple images throughout the interaction.\r\n\r\nThanks for your help!</BODY>\n\n<COMMENTS>\n<Comment by ys-zong at 2024-08-16T16:33:28Z>\nHi, we built a few-shot in-context learning repo that may be helpful for you. It has inference code of Llava and many other models. https://github.com/ys-zong/VL-ICL\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1661,
    "state": "closed",
    "created_by": "Camellia-hz",
    "created_at": "2024-08-15T03:31:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1661</URL>\n\n<TITLE>[Question] 单卡训练正常，多卡训练loss == 0</TITLE>\n\n<BODY>I did pretrain and finetune based on llava-v1.5-7b, replacing the encoder with dinov2, and then I'm going to do lora-finetune on my own dataset, but when I train with a single A100 * 40G, the loss is normal, but once I train with multiple cards, the losshas been 0.</BODY>\n\n<COMMENTS>\n<Comment by Camellia-hz at 2024-08-15T03:33:27Z>\n@simonw\n</Comment>\n<Comment by liziniu at 2024-09-07T04:34:00Z>\nI have the same issue\n</Comment>\n<Comment by terry-for-github at 2024-09-18T18:02:50Z>\nsame issue too.\n</Comment>\n<Comment by zc-zhao at 2024-10-30T09:29:56Z>\nsame issue too\n</Comment>\n<Comment by bofei5675 at 2024-10-30T09:32:49Z>\nAddress this issue by setting `scripts/zero2.json` or `scripts/zero3.json`\r\n\r\n```json\r\n{\r\n    \"overlap_comm\": false\r\n}\r\n```\n</Comment>\n<Comment by user074 at 2024-11-01T03:14:03Z>\n> Address this issue by setting `scripts/zero2.json` or `scripts/zero3.json`\r\n> \r\n> ```json\r\n> {\r\n>     \"overlap_comm\": false\r\n> }\r\n> ```\r\n\r\nThanks! Solved my issue\n</Comment>\n<Comment by Camellia-hz at 2024-11-05T07:35:56Z>\n> Address this issue by setting `scripts/zero2.json` or `scripts/zero3.json`\r\n> \r\n> ```json\r\n> {\r\n>     \"overlap_comm\": false\r\n> }\r\n> ```\r\n\r\nThanks, solved my problem!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1660,
    "state": "open",
    "created_by": "Chanete",
    "created_at": "2024-08-14T19:29:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1660</URL>\n\n<TITLE>Dependency errors</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:Dependency errors installing installing sglang\r\n\r\nCommand: \r\n```\r\npip install \"sglang[all]\"\r\n```\r\n\r\nLog: \r\n```\r\n\r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\nllava 1.2.2.post1 requires tokenizers==0.15.1, but you have tokenizers 0.19.1 which is incompatible.\r\nllava 1.2.2.post1 requires torch==2.1.2, but you have torch 2.4.0 which is incompatible.\r\nllava 1.2.2.post1 requires torchvision==0.16.2, but you have torchvision 0.19.0 which is incompatible.\r\nllava 1.2.2.post1 requires transformers==4.37.2, but you have transformers 4.44.0 which is incompatible.\r\n\r\n```\r\n\r\nScreenshots:</BODY>\n\n<COMMENTS>\n<Comment by infj-dev at 2024-08-19T12:22:53Z>\nthere is many solution:\r\n\r\n1- \"select the version\":\r\n```\r\npip install torch==2.1.2 torchvision==0.16.2 tokenizers==0.15.1 transformers==4.37.2\r\npip install \"sglang[all]\"\r\n```\r\n\r\n2- use ignore istalled option:\r\n`pip install \"sglang[all]\" --ignore-installed\r\n`\r\n\r\n3- use virtual environment:\r\n\r\n```\r\npython -m venv myenv\r\n\r\n#in Windows:\r\nmyenv\\Scripts\\activate\r\n#In macOS/Linux:\r\nsource myenv/bin/activate\r\n\r\npip install \"sglang[all]\"\r\n```\r\n\r\n\r\n4- uninstall then install:\r\n```\r\npip uninstall llava\r\npip install \"sglang[all]\"\r\npip install llava\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1659,
    "state": "open",
    "created_by": "zhanglixuan0720",
    "created_at": "2024-08-13T14:12:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1659</URL>\n\n<TITLE>[Question] The pure text process in function \"prepare_inputs_labels_for_multimodal\"</TITLE>\n\n<BODY>### Question\n\nThe codes in `llava/model/llava_arch.py` lines 235-237 seemly doing nothing? Is there any special intention for this code?\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/c121f0432da27facab705978f83c4ada465e46fd/llava/model/llava_arch.py#L234-L237</BODY>\n\n<COMMENTS>\n<Comment by daixiangzi at 2024-08-15T11:34:57Z>\n+1\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1658,
    "state": "open",
    "created_by": "zy1996829",
    "created_at": "2024-08-13T10:02:45Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1658</URL>\n\n<TITLE>[Question] When finetuning with my custom dataset, do I need to mix the custom datasets and original LLaVA instruction datasets?</TITLE>\n\n<BODY>### Question\n\nWhen I only used the customized datasets, overfitting was very serious. Even if I set the epochs to be very small, the final loss was controlled at around 0.6, and overfitting was still very serious. @haotian-liu</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1657,
    "state": "open",
    "created_by": "happywinder",
    "created_at": "2024-08-13T08:16:28Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1657</URL>\n\n<TITLE>[Usage] Out of bounds error</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nwhen run llavabench.sh ,it occurs out of bounds error,how to fix it\r\n![llava](https://github.com/user-attachments/assets/22393531-6092-4cc7-85e6-f0b1e2e04952)\r\n\r\nCommand:\r\n```\r\nbash ./scripts/v1_5/eval/llavabench.sh\r\n```\r\n![QQ20240813-161519](https://github.com/user-attachments/assets/f033ea60-3fd4-4889-a659-43053653270b)\r\nmy prompt is like \r\n{\"question_id\": 0, \"image\": \"6711.jpg\", \"text\": \"Provide a safety assessment for the content provided by the user. Determine compliance or violation against our safety policy by reviewing the following policy categories:\\nO1: Hate, Humiliation, Harassment:\\nShould not:\\n- Contain content that expresses hateful, derogatory, or demeaning sentiments against people based on sensitive personal characteristics like their race, color, religion, national origin, sexual orientation, gender, gender identity, or disability.\\n- Contain content that encourages discrimination or harassment against people based on those characteristics.\\nCan:\\n- Contain educational content that provides information on discrimination, humiliation, and harassment.\\n- Discuss historical events involving hate and harassment.\\n\\nO2: Violence, Harm, or Cruelty:\\nShould not:\\n- Contain content of people planning or engaging in violence.\\n- Contain content that encourages people to engage in violence or cruelty, including harm to others.\\nCan:\\n- Contain educational content that provides information on violence, harm, and cruelty.\\n- Discuss historical events involving violence and cruelty.\\n\\nO3: Sexual Content:\\nShould not:\\n- Contain sexually explicit (i.e., erotic) content.\\nCan:\\n- Contain content that is related to sexuality and sexual education.\\n- Provide general information regarding sexual activities.\\n- Provide guidance on topics related to sexual health.\\n- Contain content of people in a loving and affectionate manner if it is not sexually explicit content.\\n\\nO4: Nudity Content:\\nShould not:\\n- Contain visible genitalia (e.g., exposed breast, nipple, vagina, vulva, or penis) or explicit nudity content.\\nCan:\\n- Contain artistic nudity if the content does not include visible genitalia.\\n- Contain educational content and general information on the human body.\\n- Provide guidance on topics related to sexual health.\\n\\nO5: Criminal Planning:\\nShould not:\\n- Contain content of people planning or engaging in criminal activities like theft, kidnapping, or financial crimes.\\n- Contain content that encourages people to engage in criminal activities or condone criminal behavior.\\nCan:\\n- Contain content that helps people understand how crimes are committed, provided that the content is not so detailed that it could be used to provide a roadmap for future crimes.\\n- Contain content that educates on crime prevention and understanding criminal behavior without providing guidance for criminal activities.\\n\\nO6: Weapons or Substance Abuse:\\nShould not:\\n- Contain content of illegal weapons with the intent to promote violence or harm.\\n- Contain content of trafficking, consumption, or use of illegal drugs.\\n- Promote the use of tobacco, alcohol, and cannabis.\\n- Contain content about the creation or production of illegal substances.\\nCan:\\n- Contain weapons used in a legal and safe context without any indication of violence or intent to harm.\\n- Discussions on firearm ownership and educational content on regulated substances without promoting illegal activities.\\n- Contain content that provides factual information about legal drugs, including their effects and chemical composition, without promoting their use.\\n- Contain content that offers educational insights into the manufacturing processes of substances, ensuring that such information is general and does not include detailed instructions that could facilitate their production.\\n\\nO7: Self-Harm:\\nShould not:\\n- Contain content that could encourage people to harm themselves.\\n- Contain content that romanticizes or condones self-harm or suicide.\\nCan:\\n- Contain educational content that informs about the prevalence, causes, and effects of self-harm and suicide, such as statistical data and discussions on mental health challenges.\\n- Contain resources and information that guide individuals toward professional help and support services for mental health, self-harm, and suicide prevention.\\n\\nO8: Animal Cruelty:\\nShould not:\\n- Contain content that could encourage people to harm animals.\\n- Contain content that romanticizes or condones harming animals.\\n- Contain explicit animal cruelty.\\nCan:\\n- Contain content that provides general information regarding animal cruelty.\\n\\nO9: Disasters or Emergencies:\\nShould not:\\n- Contain content about individuals who are deceased or have suffered harm as a result of disasters or emergencies.\\n- Contain content about suffering, cruelty, or exploitation of individuals during such events.\\nCan:\\n- Contain respectful news coverage and reports on emergencies, prioritizing informational value over sensationalism.\\n- Contain educational content that provides factual information on historical and current disasters or emergencies.\\n\\nAssessment Steps:\\n1. Select a safety rating. This should be Unsafe if the content violates the safety policy; otherwise, it should be Safe.\\n2. Assign a policy category. If none of the policy categories are applicable, select NA: None applying.\\n3. Provide a rationale. Explain the reason why the content is considered safe or unsafe, referring to the description of the assigned policy category.\\nTo provide your assessment use the following json template:\\nrating: str<Unsafe|Safe>,\\ncategory: str<O1: Hate, Humiliation, Harassment|O2: Violence, Harm, or Cruelty|O3: Sexual Content|O4: Nudity Content|O5: Criminal Planning|06: Weapons or Substance Abuse|O7: Self-Harm|O8: Animal Cruelty|O9: Disasters or Emergencies|NA: None applying>,\\nrationale: str,\\n<image>\\n\"}</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1655,
    "state": "open",
    "created_by": "DnkNju",
    "created_at": "2024-08-11T15:56:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1655</URL>\n\n<TITLE>[Question]detailed information about tokenizer and encoder</TITLE>\n\n<BODY>### Question\n\nThanks for your work. Where can I find the information of tokenizer and encoder used by LLaVA-7B-v1?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1654,
    "state": "open",
    "created_by": "moonnnpie",
    "created_at": "2024-08-10T14:42:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1654</URL>\n\n<TITLE>[Usage] why the demo of LLaVA-1.5 can not use now？</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nthe demo of LLaVA-1.5 can not use now\r\n\r\nScreenshots:\r\n\r\n![3771723300936_ pic](https://github.com/user-attachments/assets/c4e0da92-e50f-4bf3-8310-0996a9eb9b7a)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1653,
    "state": "open",
    "created_by": "yukio0321",
    "created_at": "2024-08-09T10:07:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1653</URL>\n\n<TITLE>[Question] Could you tell me any difference between llava-v1.5-13b.jsonl and old llava-v1.5-7b.jsonl?</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1651,
    "state": "closed",
    "created_by": "gymbeijing",
    "created_at": "2024-08-08T15:28:55Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1651</URL>\n\n<TITLE>[Usage] LLaVA-v1.6-vicuna-7b generates incomplete sentence</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nHi Haotian, thank you for the great work!\r\n\r\nI've been trying to use LLaVA to generate rationale for my multimodal inputs (a news image + a news caption).\r\n\r\nThe prompt looks like this:\r\n```\r\nGiven the news Image and the news Text ([T]), please provide a streamlined explanation associated with the text and the image by using the contextual background commonsense knowledge, to explicitly explain how the news image is reasoned as out-of-context.\r\n```\r\n\r\nHere is an example. In the following example, ```[T]``` is\r\n```\r\nPassengers are stranded outside the Guangzhou railway station in China s Guangdong province on Tuesday\r\n```\r\n\r\nGiven the prompt and the news image, the model's output is \r\n```\r\n .\r\n2. The image could be a stock photo or a generic image of a crowd, which is not specific to the news event.\r\n3. The image might be used to illustrate a broader concept or story, but without additional context, it does not directly relate to the news text.\r\n\r\nIn summary, the image does not provide any specific information about the passengers stranded outside the Guangzhou railway station, and thus, it is out of context with the news text.\r\n```\r\n\r\nClearly, the output starts from the middle of a sentence. This also happens with different multimodal news inputs (which I didn't illustrate here).\r\n\r\nSome hyperparameters: ```--sep , --temperature 0 --max_new_tokens 512```\r\n\r\nCould you please advise me where I can adjust the parameter or my generation code (if needed I can share)? Thank you in advance.</BODY>\n\n<COMMENTS>\n<Comment by gymbeijing at 2024-08-09T04:03:08Z>\nThis is due to a bug in my code. I truncated the output sentence from the middle.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1650,
    "state": "closed",
    "created_by": "onaka-ga-pkpk",
    "created_at": "2024-08-08T02:32:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1650</URL>\n\n<TITLE>[Question] Learning is completed, but only the weights of the projector are output.</TITLE>\n\n<BODY>### Question\r\n\r\nWe would like to make Mistral-7B-v0.3 our VLM with our own Continual Pre-Training and SFT.\r\nI followed the tutorial, ran `pretrain.sh` and `finetune.sh` (Not Lora) and the training appeared to complete successfully. However, only three files, `config.json`, `mm_projector.bin`, and `trainer_state.json`, were output in the output directory.\r\nHow can I complete the training correctly?</BODY>\n\n<COMMENTS>\n<Comment by Jian-Zhang-3DV at 2025-01-06T06:50:09Z>\nI came across your issue where after running pretrain.sh and finetune.sh, the output directory only contains config.json, mm_projector.bin, and trainer_state.json, but no model files were generated. I'm experiencing the same issue and was wondering if you found a solution.\r\n\r\nCould you please share how you resolved this?\r\n\r\nThanks in advance!\n</Comment>\n<Comment by keanshi-nlp at 2025-03-09T08:57:10Z>\n> I came across your issue where after running pretrain.sh and finetune.sh, the output directory only contains config.json, mm_projector.bin, and trainer_state.json, but no model files were generated. I'm experiencing the same issue and was wondering if you found a solution.\n> \n> Could you please share how you resolved this?\n> \n> Thanks in advance!\n\nI came across the same issue. The pretrain output onlu contains config.json, mm_projector.bin and trainer_state.json, have you solved it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1649,
    "state": "open",
    "created_by": "shure-dev",
    "created_at": "2024-08-07T13:59:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1649</URL>\n\n<TITLE>Why are the first image and the remaining images processed separately? in prepare_inputs_labels_for_multimodal()</TITLE>\n\n<BODY>https://github.com/haotian-liu/LLaVA/blob/c121f0432da27facab705978f83c4ada465e46fd/llava/model/llava_arch.py#L169\r\n\r\n```\r\nbase_image_feature = image_feature[0]\r\nimage_feature = image_feature[1:]\r\n```\r\n\r\nWhy are the first image and the remaining images processed separately?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1648,
    "state": "open",
    "created_by": "Mike-ihr",
    "created_at": "2024-08-06T13:46:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1648</URL>\n\n<TITLE>llava 1.2.2.post1 requires torch==2.1.2, but you have torch 2.0.1 which is incompatible.</TITLE>\n\n<BODY>### Describe the issue\n\nMy Cuda version is 11.7 which only can support torch 2.0.1, What Should I do to solve the problem?\r\nHow can I reduce the llava version.</BODY>\n\n<COMMENTS>\n<Comment by sirus20x6 at 2024-08-15T22:48:04Z>\nand I have the opposite problem. I'm on 2.5.0.dev20240806+rocm6.1\n</Comment>\n<Comment by Mike-ihr at 2024-08-16T02:15:29Z>\n> and I have the opposite problem. I'm on 2.5.0.dev20240806+rocm6.1\r\n\r\nYou can open the history release to find the compatible version.\r\nI have solved this problem\n</Comment>\n<Comment by sirus20x6 at 2024-08-16T19:01:27Z>\n> > and I have the opposite problem. I'm on 2.5.0.dev20240806+rocm6.1\r\n> \r\n> You can open the history release to find the compatible version. I have solved this problem none of the llava releases are compatible with any of the rocm6.1 releases\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1647,
    "state": "open",
    "created_by": "xie-qiang",
    "created_at": "2024-08-06T07:40:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1647</URL>\n\n<TITLE>[Usage] Freezing Vision Encoder During LoRA Training</TITLE>\n\n<BODY>### Describe the issue\n\nWhile using the script `scripts/v1_5/finetune_task_lora.sh` for training, I printed the trainable parameters of the model before it enters `LLaVATrainer` (line 961 in `llava/train/train.py`). I noticed that the parameters of the `vision_tower` are also in the list of trainable parameters and have LoRA applied to them. However, your paper mentions that the `vision_tower` is always frozen, which confuses me.\r\n\r\n```\r\nfor name, param in model.named_parameters():\r\n    if param.requires_grad:\r\n        print(f\"Parameter: {name}, Shape: {param.shape}\")\r\n```\r\n---\r\n```\r\n...\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([128, 1024])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 128])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([128, 1024])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1024, 128])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([128, 1024])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 128])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([128, 1024])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 128])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([128, 1024])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1024, 128])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([128, 1024])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 128])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([128, 1024])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 128])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([128, 1024])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1024, 128])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([128, 1024])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 128])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([128, 1024])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 128])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([128, 1024])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1024, 128])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([128, 1024])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 128])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([128, 1024])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 128])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([128, 1024])\r\nParameter: base_model.model.model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1024, 128])\r\n...\r\n```\r\n\r\nAdditionally, I would like to know if the `vision_tower` with LoRA parameters trained is merged during the merge process. In other words, do the LoRA-trained parameters of the `vision_tower` actually participate in inference?\r\n\r\nIf we truly want to freeze the `vision_tower` during LoRA training, I think we should modify the `find_all_linear_names` function in `llava/train/train.py` as follows:\r\n\r\n```python\r\ndef find_all_linear_names(model):\r\n    cls = torch.nn.Linear\r\n    lora_module_names = set()\r\n    multimodal_keywords = ['mm_projector', 'vision_tower', 'vision_resampler']\r\n    for name, module in model.named_modules():\r\n        if any(mm_keyword in name for mm_keyword in multimodal_keywords):\r\n            continue\r\n        if isinstance(module, cls):\r\n            lora_module_names.add(name)\r\n\r\n    if 'lm_head' in lora_module_names: # needed for 16-bit\r\n        lora_module_names.remove('lm_head')\r\n    return list(lora_module_names)\r\n```\r\n\r\nThank you for your assistance!</BODY>\n\n<COMMENTS>\n<Comment by xinyann666 at 2025-03-10T02:52:07Z>\nI am also very confused about this. Cry.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1646,
    "state": "open",
    "created_by": "XCF-Mike",
    "created_at": "2024-08-06T03:30:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1646</URL>\n\n<TITLE>You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:finetune error\r\n\r\nCommand:\r\nHello, when I finetune the model, \r\n![1722914859059](https://github.com/user-attachments/assets/d91a118c-32ae-4f2c-9d90-44820b41de99)\r\none mistake occured\r\n\r\n\r\nScreenshots:\r\n![image](https://github.com/user-attachments/assets/3f64e849-ac0e-47d8-89b8-39d9ea074730)</BODY>\n\n<COMMENTS>\n<Comment by shalini-maiti at 2024-08-13T22:39:57Z>\nI get the same warning message. Is this something to be concerned about? Should we use a different checkpoint? In my case, I try to finetune the llava-1.5-13b parameter model. Thanks in advance!\n</Comment>\n<Comment by amagzari at 2024-11-23T19:42:52Z>\nSame question here.\n</Comment>\n<Comment by aelyoussfi at 2024-12-13T15:14:10Z>\nsame question here.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1645,
    "state": "closed",
    "created_by": "bryanwong17",
    "created_at": "2024-08-05T08:35:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1645</URL>\n\n<TITLE>[Question] Inference Code</TITLE>\n\n<BODY>### Question\n\nThank you for the excellent work! I am curious if LLaVA is exclusively available for CLI inference. Is it possible to perform LLaVA inference using code without relying on the CLI? The reason for my inquiry is that using CLI inference is not convenient for generating text for a large number of images for research purposes. Thank you!</BODY>\n\n<COMMENTS>\n<Comment by Racktic at 2024-10-21T08:08:57Z>\ni'm wondering the same thing! what's your solution?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1644,
    "state": "open",
    "created_by": "Forence1999",
    "created_at": "2024-08-04T13:40:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1644</URL>\n\n<TITLE>[Question] Misaligned image_grid_pinpoints</TITLE>\n\n<BODY>### Question\n\nHi, Thank you so much for your great open-sourced work!\r\n\r\nI notice that the config in https://huggingface.co/lmms-lab/llama3-llava-next-8b/blob/main/config.json shows the following values:\r\n\r\n\"image_grid_pinpoints\": [    [      336,      672    ],    [      672,      336    ],    [      672,      672    ],    [      1008,      336    ],    [      336,      1008    ]  ],\r\n\r\nbut it seems different from what you stated in your post (https://llava-vl.github.io/blog/2024-01-30-llava-next/):\r\n\r\n - Increasing the input image resolution to 4x more pixels. This allows it to grasp more visual details. It supports three aspect ratios, up to 672x672, 336x1344, 1344x336 resolution.\r\n\r\nIs there any misunderstanding or inattention? Thanks!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1643,
    "state": "open",
    "created_by": "eternal8080",
    "created_at": "2024-08-04T10:08:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1643</URL>\n\n<TITLE>about multi-image input</TITLE>\n\n<BODY>### Question\n\ndoes LLaVA1.5  supports for multipile images input now? If yes, how to do this</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1642,
    "state": "open",
    "created_by": "dipikakhullar",
    "created_at": "2024-08-04T07:37:12Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1642</URL>\n\n<TITLE>[Question] Image and Text Embeddings For Downstream Tasks</TITLE>\n\n<BODY>### Question\n\nhow can I get the image and text embeddings for another task, and what size are these embeddings? Here is what I know:\r\nHere is the vision output shape: torch.Size([1, 576, 4096]\r\nHere is the text output shape: torch.Size([1, 128, 4096]) \r\n\r\nI just got the dimension of both vision and text embeddings from the model configuration and vision embedding are  set to 4096 as per hidden_size. And text embeddings are set to 1024 as per mm_hidden_size.\r\n\r\nbut the text output shape last dimension and the mm_hidden_size value (1024) do not match up. Also, 576 X 4096 seems very large.</BODY>\n\n<COMMENTS>\n<Comment by wenxuanmou at 2024-08-12T11:20:51Z>\nSame question. Have you got a solution？ Thanks.\n</Comment>\n<Comment by Britton-Li at 2024-11-12T09:05:34Z>\nsame question.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1641,
    "state": "open",
    "created_by": "YerongLi",
    "created_at": "2024-08-03T01:19:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1641</URL>\n\n<TITLE>load_dataset('liuhaotian/LLaVA-Instruct-150K')  ERROR</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nError loading the datasdet with **load_dataset**\r\nCommand:\r\n```\r\n  File \"/home/yerong2/representation-engineering/lorra_finetune/llava.py\", line 5, in <module>\r\n    ds = load_dataset('liuhaotian/LLaVA-Instruct-150K')\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/yerong2/local/miniconda3/envs/mllm/lib/python3.11/site-packages/datasets/load.py\", line 2616, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/yerong2/local/miniconda3/envs/mllm/lib/python3.11/site-packages/datasets/builder.py\", line 1029, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/yerong2/local/miniconda3/envs/mllm/lib/python3.11/site-packages/datasets/builder.py\", line 1124, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/yerong2/local/miniconda3/envs/mllm/lib/python3.11/site-packages/datasets/builder.py\", line 1884, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"/home/yerong2/local/miniconda3/envs/mllm/lib/python3.11/site-packages/datasets/builder.py\", line 2040, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.exceptions.DatasetGenerationError: An error occurred while generating the dataset\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.\r\n![Screenshot from 2024-08-02 20-17-52](https://github.com/user-attachments/assets/99742387-a057-4da8-903a-8136c4e478e6)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1639,
    "state": "closed",
    "created_by": "yunsaijc",
    "created_at": "2024-07-31T08:07:14Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1639</URL>\n\n<TITLE>[Usage] KeyError: 'LlavaMistralConfig' when I start a model-worker with a fine-tuned Llava-v1.6-mistral-7B model.</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: When I try to start a model-worker with a fine-tuned Llava-v1.6-mistral-7B model, I get: KeyError: 'LlavaMistralConfig'.\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.model_worker \\\r\n             --host 0.0.0.0 \\\r\n             --controller http://localhost:10000 \\\r\n             --port 40000 \\\r\n             --worker http://localhost:40000 \\\r\n             --model-path ./llava-v1.6-mistral-7b \\\r\n             --model-base ./checkpoints/llava-v1.6-mistral-7b-task-lora # this is the fine-tuned weights of my own\r\n```\r\n\r\nLog: \r\n```\r\n2024-07-31 16:02:25 | INFO | model_worker | Loading the model llava-v1.6-mistral-7b on worker cc01df ...\r\n2024-07-31 16:02:25 | INFO | stdout | Loading LLaVA from base model...\r\n2024-07-31 16:02:25 | ERROR | stderr | Traceback (most recent call last):\r\n2024-07-31 16:02:25 | ERROR | stderr |   File \"/home/jc/app/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n2024-07-31 16:02:25 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n2024-07-31 16:02:25 | ERROR | stderr |   File \"/home/jc/app/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n2024-07-31 16:02:25 | ERROR | stderr |     exec(code, run_globals)\r\n2024-07-31 16:02:25 | ERROR | stderr |   File \"/home/jc/workspace/LLaVA/llava/serve/model_worker.py\", line 277, in <module>\r\n2024-07-31 16:02:25 | ERROR | stderr |     worker = ModelWorker(args.controller_address,\r\n2024-07-31 16:02:25 | ERROR | stderr |   File \"/home/jc/workspace/LLaVA/llava/serve/model_worker.py\", line 65, in __init__\r\n2024-07-31 16:02:25 | ERROR | stderr |     self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\r\n2024-07-31 16:02:25 | ERROR | stderr |   File \"/home/jc/workspace/LLaVA/llava/model/builder.py\", line 97, in load_pretrained_model\r\n2024-07-31 16:02:25 | ERROR | stderr |     tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n2024-07-31 16:02:25 | ERROR | stderr |   File \"/home/jc/app/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 830, in from_pretrained\r\n2024-07-31 16:02:25 | ERROR | stderr |     tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]\r\n2024-07-31 16:02:25 | ERROR | stderr |   File \"/home/jc/app/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 737, in __getitem__\r\n2024-07-31 16:02:25 | ERROR | stderr |     model_type = self._reverse_config_mapping[key.__name__]\r\n2024-07-31 16:02:25 | ERROR | stderr | KeyError: 'LlavaMistralConfig'\r\n```</BODY>\n\n<COMMENTS>\n<Comment by bxdgfw at 2024-10-07T09:35:25Z>\nhello，have you solved it and how did you solve it?\n</Comment>\n<Comment by yunsaijc at 2024-10-07T16:43:02Z>\n> hello，have you solved it and how did you solve it?\r\n\r\nHello, I have solved this issue by merging the lora module and the base model. (Using scripts/merge_lora_weights.py)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1638,
    "state": "open",
    "created_by": "rin2401",
    "created_at": "2024-07-31T06:38:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1638</URL>\n\n<TITLE>Error when load model in 4bit</TITLE>\n\n<BODY>Don't need  `kwargs['load_in_4bit'] = True`  when use `quantization_config`\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/c121f0432da27facab705978f83c4ada465e46fd/llava/model/builder.py#L34-L40</BODY>\n\n<COMMENTS>\n<Comment by drzraf at 2024-09-01T04:18:29Z>\nHappens with `transformers 4.44.2` . The ValueError was introduced here in `transformers` (https://github.com/huggingface/transformers/pull/21579) (instead of a slow deprecation)\r\n\r\nThis was merged in https://github.com/huggingface/transformers/commit/3668ec17165dbb7823f3bc7e190e1733040c3af8 (part of [v4.27.0](https://github.com/huggingface/transformers/releases/tag/v4.27.0) and onward)\r\n\r\nand since this project requires `transformers==4.37.2`, there is no reason to keep passing deprecated booleans which trigger this error.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1637,
    "state": "open",
    "created_by": "AbdulrahmanSoliman1",
    "created_at": "2024-07-31T05:14:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1637</URL>\n\n<TITLE>[Question] Fine-tuned model ignore some of the captions</TITLE>\n\n<BODY>### Question\r\n![image](https://github.com/user-attachments/assets/d10a4267-cf27-4156-8651-0def332f9f45)\r\ntrain/loss 0.0001 , epochs 1\r\nthe model always focuses on one part of the captions and ignores the rest. I am using the default fine-tuning\r\nthe dataset is 20k images-captions\r\n\r\nGround Truth: Medical image shows broken leg and lighting could impact visibility.\r\nLLAVA: Medical image shows broken leg.\r\n\r\nis there something to do that makes the model focus on the whole label\r\n\r\ndef __getitem__(self, idx):\r\n    sample = self.data[idx]\r\n    image_path = os.path.join(self.image_dir, sample[\"image_file\"]) \r\n    image = Image.open(image_path).convert(\"RGB\")\r\n    return {\r\n        \"image\": image,\r\n        \"qa\": [\r\n            {\r\n                \"question\": \"Describe the following image in detail\",\r\n                \"answer\": sample[\"description\"],\r\n            }\r\n        ]\r\n    }</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1636,
    "state": "open",
    "created_by": "qingyunyanran",
    "created_at": "2024-07-30T09:30:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1636</URL>\n\n<TITLE>How can I custom loss function during fine-tuning?[Question]</TITLE>\n\n<BODY>### Question\n\nI can't find where the model defined loss function. And I want to custom loss function but I am afraid that if I just add this function in the tuning script it might destroy the whole model.</BODY>\n\n<COMMENTS>\n<Comment by anas-zafar at 2024-08-24T10:50:44Z>\n@qingyunyanran were you able to figure this out? Thanks\n</Comment>\n<Comment by AnnaGao0827 at 2024-08-27T14:21:41Z>\nHi, I have the same question. When finetuing the LLaVA model, can we use custom loss fuction and data format? Or we must use Language Modeling loss & human-gpt data format? Thanks!\n</Comment>\n<Comment by formidify at 2024-08-28T06:26:52Z>\nTo customize loss, you can just override the compute_loss function in the LLaVATrainer class in llava/train/llava_trainer.py. The original compute_loss function can be found here: https://github.com/huggingface/transformers/blob/v4.44.2/src/transformers/trainer.py#L3353\r\n\r\nIf you need to customize the input data format, you can do so in train.py. Hope this helps!\n</Comment>\n<Comment by AnnaGao0827 at 2024-08-28T13:00:58Z>\n> To customize loss, you can just override the compute_loss function in the LLaVATrainer class in llava/train/llava_trainer.py. The original compute_loss function can be found here: https://github.com/huggingface/transformers/blob/v4.44.2/src/transformers/trainer.py#L3353\r\n> \r\n> If you need to customize the input data format, you can do so in train.py. Hope this helps!\r\n\r\nThank you sooo much. Your guidance helps a lot!\n</Comment>\n<Comment by 1835969208 at 2025-05-26T04:46:09Z>\n> 要自定义损失，您只需覆盖 llava/train/llava_trainer.py 中 LLaVATrainer 类中的 compute_loss 函数。原始 compute_loss 函数可在此处找到：https://github.com/huggingface/transformers/blob/v4.44.2/src/transformers/trainer.py#L3353\n> \n> 如果需要自定义输入数据格式，可以在 train.py 中执行此作。希望这有帮助！\n\n@formidify 但我没法在LLaVATrainer 类中找到 compute_loss 函数，LLaVATrainer 类中内容如下：\nclass LLaVATrainer(Trainer):\n\n    def _get_train_sampler(self) -> Optional[torch.utils.data.Sampler]:\n        if self.train_dataset is None or not has_length(self.train_dataset):\n            return None\n\n        if self.args.group_by_modality_length:\n            lengths = self.train_dataset.modality_lengths\n            return LengthGroupedSampler(\n                self.args.train_batch_size,\n                world_size=self.args.world_size * self.args.gradient_accumulation_steps,\n                lengths=lengths,\n                group_by_modality=True,\n            )\n        else:\n            return super()._get_train_sampler()\n\n    def create_optimizer(self):\n        \"\"\"\n        Setup the optimizer.\n\n        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n        Trainer's init through `optimizers`, or subclass and override this method in a subclass.\n        \"\"\"\n        if is_sagemaker_mp_enabled():\n            return super().create_optimizer()\n\n        opt_model = self.model\n\n        if self.optimizer is None:\n            decay_parameters = get_parameter_names(opt_model, ALL_LAYERNORM_LAYERS)\n            decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n            if self.args.mm_projector_lr is not None:\n                projector_parameters = [name for name, _ in opt_model.named_parameters() if \"mm_projector\" in name]\n                optimizer_grouped_parameters = [\n                    {\n                        \"params\": [\n                            p for n, p in opt_model.named_parameters() if (n in decay_parameters and n not in projector_parameters and p.requires_grad)\n                        ],\n                        \"weight_decay\": self.args.weight_decay,\n                    },\n                    {\n                        \"params\": [\n                            p for n, p in opt_model.named_parameters() if (n not in decay_parameters and n not in projector_parameters and p.requires_grad)\n                        ],\n                        \"weight_decay\": 0.0,\n                    },\n                    {\n                        \"params\": [\n                            p for n, p in opt_model.named_parameters() if (n in decay_parameters and n in projector_parameters and p.requires_grad)\n                        ],\n                        \"weight_decay\": self.args.weight_decay,\n                        \"lr\": self.args.mm_projector_lr,\n                    },\n                    {\n                        \"params\": [\n                            p for n, p in opt_model.named_parameters() if (n not in decay_parameters and n in projector_parameters and p.requires_grad)\n                        ],\n                        \"weight_decay\": 0.0,\n                        \"lr\": self.args.mm_projector_lr,\n                    },\n                ]\n            else:\n                optimizer_grouped_parameters = [\n                    {\n                        \"params\": [\n                            p for n, p in opt_model.named_parameters() if (n in decay_parameters and p.requires_grad)\n                        ],\n                        \"weight_decay\": self.args.weight_decay,\n                    },\n                    {\n                        \"params\": [\n                            p for n, p in opt_model.named_parameters() if (n not in decay_parameters and p.requires_grad)\n                        ],\n                        \"weight_decay\": 0.0,\n                    },\n                ]\n\n            optimizer_cls, optimizer_kwargs = Trainer.get_optimizer_cls_and_kwargs(self.args)\n\n            self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)\n            if optimizer_cls.__name__ == \"Adam8bit\":\n                import bitsandbytes\n\n                manager = bitsandbytes.optim.GlobalOptimManager.get_instance()\n\n                skipped = 0\n                for module in opt_model.modules():\n                    if isinstance(module, nn.Embedding):\n                        skipped += sum({p.data_ptr(): p.numel() for p in module.parameters()}.values())\n                        logger.info(f\"skipped {module}: {skipped/2**20}M params\")\n                        manager.register_module_override(module, \"weight\", {\"optim_bits\": 32})\n                        logger.debug(f\"bitsandbytes: will optimize {module} in fp32\")\n                logger.info(f\"skipped: {skipped/2**20}M params\")\n\n        return self.optimizer\n\n    def _save_checkpoint(self, model, trial, metrics=None):\n        if getattr(self.args, 'tune_mm_mlp_adapter', False):\n            from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n            checkpoint_folder = f\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\"\n\n            run_dir = self._get_output_dir(trial=trial)\n            output_dir = os.path.join(run_dir, checkpoint_folder)\n\n            # Only save Adapter\n            keys_to_match = ['mm_projector', 'vision_resampler']\n            if getattr(self.args, \"use_im_start_end\", False):\n                keys_to_match.extend(['embed_tokens', 'embed_in'])\n\n            weight_to_save = get_mm_adapter_state_maybe_zero_3(self.model.named_parameters(), keys_to_match)\n\n            if self.args.local_rank == 0 or self.args.local_rank == -1:\n                self.model.config.save_pretrained(output_dir)\n                torch.save(weight_to_save, os.path.join(output_dir, f'mm_projector.bin'))\n        else:\n            super(LLaVATrainer, self)._save_checkpoint(model, trial, metrics)\n\n    def _save(self, output_dir: Optional[str] = None, state_dict=None):\n        if getattr(self.args, 'tune_mm_mlp_adapter', False):\n            pass\n        else:\n            super(LLaVATrainer, self)._save(output_dir, state_dict)\n</Comment>\n<Comment by formidify at 2025-05-26T05:07:53Z>\nYou are not going to find it in llava_trainer.py. LLaVATrainer is an instance of the Transformers library class Trainer, so you need to override it based on the original class definition from https://github.com/huggingface/transformers/blob/v4.44.2/src/transformers/trainer.py#L3353\n</Comment>\n<Comment by 1835969208 at 2025-05-26T05:22:54Z>\n非常感谢你的回答和帮助，我想具体的问一下，您的意思是 我需要在llava/train/llava_trainer.py 中 LLaVATrainer 中自定义一个和Transformers 库类 Trainer中一样的compute_loss函数，还是我要将https://github.com/huggingface/transformers/blob/v4.44.2/src/transformers/trainer.py#L3353 中完整的trainer复制过来再修改里面的compute_loss函数。再次感谢你的帮助，期待你的回答\n</Comment>\n<Comment by formidify at 2025-05-26T05:24:40Z>\nYou just need to define a compute_loss function in your LLaVATrainer class to override the original.\n</Comment>\n<Comment by 1835969208 at 2025-05-26T05:25:51Z>\n@formidify 非常感谢你的回答，十分明确清晰，祝你工作顺利\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1635,
    "state": "closed",
    "created_by": "ohhan777",
    "created_at": "2024-07-30T08:18:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1635</URL>\n\n<TITLE>Error when saving model: Invalid generation config due to conflicting parameters</TITLE>\n\n<BODY>I encountered an error when trying to save the fine-tuned LLaVA model(v1.5, ``sh scripts/v1_5/finetune.sh``, pretrain.sh was done without any issue). The error occurs during the model saving process and seems to be related to conflicting generation configuration parameters. Here's the detailed error message:\r\n\r\n```\r\n...\r\n{'loss': 0.7904, 'learning_rate': 7.764687866007592e-12, 'epoch': 1.0}                                                                                                                                                                                                                                                                                                             \r\n{'loss': 0.5237, 'learning_rate': 1.9411721552398123e-12, 'epoch': 1.0}                                                                                                                                                                                                                                                                                                            \r\n{'loss': 0.3716, 'learning_rate': 0.0, 'epoch': 1.0}                                                                                                                                                                                                                                                                                                                               \r\n{'train_runtime': 28656.4, 'train_samples_per_second': 23.216, 'train_steps_per_second': 0.181, 'train_loss': 0.7762246808641321, 'epoch': 1.0}                                                                                                                                                                                                                                    \r\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5198/5198 [7:57:30<00:00,  5.51s/it]\r\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.                                                                                   \r\nNon-default generation parameters: {'max_length': 4096}                                                                                                                                                                                                                                                                                                                            \r\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recomm\r\nend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.                                                                                                                                               \r\n stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights                                                                                                                                                                                                                                                       \r\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.                                                                                   \r\nNon-default generation parameters: {'max_length': 4096}                                                                                                                                                                                                                                                                                                                            \r\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recomm\r\nend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.                                                                                                                                               \r\nTraceback (most recent call last):                                                                                                                                                                                                                                                                                                                                                 \r\n  File \"/gpfs/home/ohhan/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py\", line 558, in save_pretrained                                                                                                                                                                                                                          \r\n    raise ValueError(str([w.message for w in caught_warnings]))                                                                                                                                                                                                                                                                                                                    \r\nValueError: [UserWarning('`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.'), UserWarning('`do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_\r\nsample=True` or unset `top_p`.')]                                                                                                                                                                                                                                                                                                                                                  \r\n                                                                                                                                                                                                                                                                                                                                                                                   \r\nDuring handling of the above exception, another exception occurred:                                                                                                                                                                                                                                                                                                                \r\n                                                                                                                                                                                                                                                                                                                                                                                   \r\nTraceback (most recent call last):                                                                                                                                                                                                                                                                                                                                                 \r\n  File \"/gpfs/home/ohhan/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2873, in save_model                                                                                                                                                                                                                                                     \r\n    self._save(output_dir, state_dict=state_dict)                                                                                                                                                                                                                                                                                                                                  \r\n  File \"/gpfs/home/ohhan/ai/PyProjects/LLaVA/llava/train/llava_trainer.py\", line 255, in _save                                                                                                                                                                                                                                                                                     \r\n    super(LLaVATrainer, self)._save(output_dir, state_dict)                                                                                                                                                                                                                                                                                                                        \r\n  File \"/gpfs/home/ohhan/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2958, in _save                                                                                                                                                                                                                                                          \r\n    self.model.save_pretrained(                                                                                                                                                                                                                                                                                                                                                    \r\n  File \"/gpfs/home/ohhan/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2364, in save_pretrained                                                                                                                                                                                                                                         \r\n    model_to_save.generation_config.save_pretrained(save_directory)                                                                                                                                                                                                                                                                                                                \r\n  File \"/gpfs/home/ohhan/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py\", line 560, in save_pretrained                                                                                                                                                                                                                          \r\n    raise ValueError(                                                                                                                                                                                                                                                                                                                                                              \r\nValueError: The generation config instance is invalid -- `.validate()` throws warnings and/or exceptions. Fix these issues to save the configuration.                                                                                                                                                                                                                              \r\n                                                                                                                                                                                                                                                                                                                                                                                   \r\nThrown during validation:                                                                                                                                                                                                                                                                                                                                                          \r\n[UserWarning('`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.'), UserWarning('`do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True`\r\n or unset `top_p`.')]         \r\n```</BODY>\n\n<COMMENTS>\n<Comment by zhilizju at 2024-08-01T01:11:36Z>\nsame error.\n</Comment>\n<Comment by ohhan777 at 2024-08-02T00:28:50Z>\nI solved this issue by adding the following lines in ``llava_trainer.py``. \r\n\r\n```python\r\n    def _save_checkpoint(self, model, trial, metrics=None):\r\n        if getattr(self.args, 'tune_mm_mlp_adapter', False):\r\n            from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\r\n            checkpoint_folder = f\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\"\r\n\r\n            run_dir = self._get_output_dir(trial=trial)\r\n            output_dir = os.path.join(run_dir, checkpoint_folder)\r\n\r\n            # Only save Adapter\r\n            keys_to_match = ['mm_projector', 'vision_resampler']\r\n            if getattr(self.args, \"use_im_start_end\", False):\r\n                keys_to_match.extend(['embed_tokens', 'embed_in'])\r\n\r\n            weight_to_save = get_mm_adapter_state_maybe_zero_3(self.model.named_parameters(), keys_to_match)\r\n\r\n            if self.args.local_rank == 0 or self.args.local_rank == -1:\r\n                self.model.config.save_pretrained(output_dir)\r\n                torch.save(weight_to_save, os.path.join(output_dir, f'mm_projector.bin'))\r\n        else:\r\n            model.generation_config.do_sample = True            # <------------------------------------ Here\r\n            super(LLaVATrainer, self)._save_checkpoint(model, trial, metrics)\r\n\r\n    def _save(self, output_dir: Optional[str] = None, state_dict=None):\r\n        if getattr(self.args, 'tune_mm_mlp_adapter', False):\r\n            pass\r\n        else:\r\n            self.model.generation_config.do_sample = True  # <------------------------------------- Here, too\r\n            super(LLaVATrainer, self)._save(output_dir, state_dict)\r\n```\n</Comment>\n<Comment by HashmatShadab at 2024-08-24T04:25:43Z>\nHi @ohhan777 , Thanks for suggesting the changes. are there any changes to do to load the model during inference as well?\n</Comment>\n<Comment by Ryoo72 at 2024-09-15T07:15:10Z>\nhttps://github.com/haotian-liu/LLaVA/issues/1252 it said downgrading HF to 4.36.2 seems to work.\n</Comment>\n<Comment by Ryoo72 at 2024-09-16T08:48:51Z>\n> I solved this issue by adding the following lines in `llava_trainer.py`.\r\n> \r\n> ```python\r\n>     def _save_checkpoint(self, model, trial, metrics=None):\r\n>         if getattr(self.args, 'tune_mm_mlp_adapter', False):\r\n>             from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\r\n>             checkpoint_folder = f\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\"\r\n> \r\n>             run_dir = self._get_output_dir(trial=trial)\r\n>             output_dir = os.path.join(run_dir, checkpoint_folder)\r\n> \r\n>             # Only save Adapter\r\n>             keys_to_match = ['mm_projector', 'vision_resampler']\r\n>             if getattr(self.args, \"use_im_start_end\", False):\r\n>                 keys_to_match.extend(['embed_tokens', 'embed_in'])\r\n> \r\n>             weight_to_save = get_mm_adapter_state_maybe_zero_3(self.model.named_parameters(), keys_to_match)\r\n> \r\n>             if self.args.local_rank == 0 or self.args.local_rank == -1:\r\n>                 self.model.config.save_pretrained(output_dir)\r\n>                 torch.save(weight_to_save, os.path.join(output_dir, f'mm_projector.bin'))\r\n>         else:\r\n>             model.generation_config.do_sample = True            # <------------------------------------ Here\r\n>             super(LLaVATrainer, self)._save_checkpoint(model, trial, metrics)\r\n> \r\n>     def _save(self, output_dir: Optional[str] = None, state_dict=None):\r\n>         if getattr(self.args, 'tune_mm_mlp_adapter', False):\r\n>             pass\r\n>         else:\r\n>             self.model.generation_config.do_sample = True  # <------------------------------------- Here, too\r\n>             super(LLaVATrainer, self)._save(output_dir, state_dict)\r\n> ```\r\n\r\n`model.generation_config.do_sample = True`\r\nshould be\r\n`self.model.generation_config.do_sample = True`\n</Comment>\n<Comment by BroJack0809 at 2024-10-17T14:04:18Z>\nI encountered the same issue. Under what conditions would this error occur? This error happened while I was saving the checkpoint, and as a result, no checkpoint was saved.\n</Comment>\n<Comment by g-h-chen at 2025-01-31T14:35:11Z>\nadding `do_sample=True` when loading the model using `from_pretrained` also works too!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1634,
    "state": "open",
    "created_by": "sivang",
    "created_at": "2024-07-28T21:23:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1634</URL>\n\n<TITLE>NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.[Usage]</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nNETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.\r\n\r\nwhen trying to use as instructed in the demo instructions\r\n\r\n![image](https://github.com/user-attachments/assets/14544906-cb36-44fc-abbb-14a31c7ab964)</BODY>\n\n<COMMENTS>\n<Comment by xiaomatiaobugao at 2024-10-26T11:55:55Z>\nhow to deal with this？\n</Comment>\n<Comment by Coldermyy at 2024-12-12T01:55:02Z>\nHave you solved the problem yet?\n</Comment>\n<Comment by J3AAA at 2025-03-04T15:53:03Z>\n我也遇到了这个问题，后来解决了。这似乎是显存不足导致的，执行下面这个代码试试：\npython -m llava.serve.model_worker \\\n--host 0.0.0.0 \\\n--controller http://localhost:10000 \\\n--port 40000 \\\n--worker http://localhost:40000 \\\n--model-path liuhaotian/llava-v1.5-7b \\\n--load-4bit\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1633,
    "state": "open",
    "created_by": "zxy1728",
    "created_at": "2024-07-28T11:34:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1633</URL>\n\n<TITLE>Can this model be adapted to torch2.3？</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1632,
    "state": "open",
    "created_by": "BroJunn",
    "created_at": "2024-07-25T18:45:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1632</URL>\n\n<TITLE>[Question] About how to deal with multiple-answer question in vqav2</TITLE>\n\n<BODY>### Question\n\nDear contributors of LLaVA,\r\n\r\nMay I inquire you how you preprocess vqav2 dataset into the mix665k. There're two points I'm totally confused wtih.\r\n\r\n(1) I notices each question may have 10 answers and some of answers(annotations) could be exactly opposite, e.g. yes vs. no. What's the strategy to preprocess multi-answer dataset and how did you deal with such feature.\r\n\r\n\r\n(2) A more general question not limited to vqav2 that is there any rules when you assign several questions for one common image in an order? Or everything is just randomly sampled?\r\n\r\nI failed to find relevant explanations around the paper or forum and I exactly needs your answers to forward with more other datasets in my experiments. Thanks in advance and appreciate such an amazing work!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1631,
    "state": "open",
    "created_by": "Wuzimeng",
    "created_at": "2024-07-25T14:43:55Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1631</URL>\n\n<TITLE>[Question] Annotation Order of the  CC3M-Pretrain-595K dataset</TITLE>\n\n<BODY>### Question\n\nHello, the filtered CC3M-Pretrain-595K dataset used in LLAVA has been very helpful to me, but I would like to know if the annotation order in this dataset is random or if it maintains the original order of the CC3M dataset, or some other order?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1630,
    "state": "open",
    "created_by": "mattia-re-learn",
    "created_at": "2024-07-25T07:55:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1630</URL>\n\n<TITLE>[Usage] Visual instruction tuning for LLaVa 1.6</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\n\r\nWe are trying to finetune the model on our dataset.\r\nCurrently, we are able to successfully finetune model `lmsys/vicuna-13b-v1.5`  using projector weights `llava-v1.5-mlp2x-336px-pretrain-vicuna-13b-v1.5`.\r\n\r\nWe would like to leverage the new version of the model and try to finetune it. \r\nWithout using a pretrained projector, the model does not predict anything, I guess because of the small dataset we have.\r\n\r\nBut trying to use \r\n--model_name_or_path liuhaotian/llava-v1.6-vicuna-13b\r\ntogether with `pretrain_mm_mlp_adapter` set to `llava-v1.5-mlp2x-336px-pretrain-vicuna-13b-v1.5` is giving error loading the state.\r\n\r\nIs there a way to to a visual instruction tuning of LLaVa 1.6?\r\n\r\nCommand:\r\n```\r\ndeepspeed /home/llava/LLaVA/llava/train/train_mem.py \\\r\n    --deepspeed /home/llava/LLaVA/scripts/zero3.json \\\r\n    --model_name_or_path liuhaotian/llava-v1.6-vicuna-13b \\\r\n    --version v1 \\\r\n    --data_path /tmp/dataset/conversations.json \\\r\n    --image_folder /tmp/dataset/images/ \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --pretrain_mm_mlp_adapter /tmp/checkpoints/mm_projector.bin \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length False \\\r\n    --bf16 True \\\r\n    --output_dir /tmp/checkpoints/llava-v1.6-13b \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --freeze_backbone \\\r\n    --report_to tensorboard\r\n```\r\n\r\nWhere the projector comes from [https://huggingface.co/liuhaotian/llava-v1.5-mlp2x-336px-pretrain-vicuna-13b-v1.5/resolve/main/mm_projector.bin](https://huggingface.co/liuhaotian/llava-v1.5-mlp2x-336px-pretrain-vicuna-13b-v1.5/resolve/main/mm_projector.bin)\r\n\r\nLog: \r\n```\r\n2024-07-24T06:54:38.990825998-04:00 Traceback (most recent call last):\r\n2024-07-24T06:54:38.990849605-04:00   File \"/home/llava/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n2024-07-24T06:54:38.990929643-04:00     train(attn_implementation=\"flash_attention_2\")\r\n2024-07-24T06:54:38.990937186-04:00   File \"/home/llava/LLaVA/llava/train/train.py\", line 911, in train\r\n2024-07-24T06:54:38.991068208-04:00     model.get_model().initialize_vision_modules(\r\n2024-07-24T06:54:38.991073865-04:00   File \"/home/llava/LLaVA/llava/model/llava_arch.py\", line 97, in initialize_vision_modules\r\n2024-07-24T06:54:38.991085319-04:00     self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'))\r\n2024-07-24T06:54:38.991089999-04:00   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 2152, in load_state_dict\r\n2024-07-24T06:54:38.991553186-04:00     raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\n2024-07-24T06:54:38.991581612-04:00 RuntimeError: Error(s) in loading state_dict for Sequential:\r\n2024-07-24T06:54:38.991591948-04:00 \tsize mismatch for 0.weight: copying a param with shape torch.Size([5120, 1024]) from checkpoint, the shape in current model is torch.Size([0]).\r\n2024-07-24T06:54:38.991600469-04:00 \tsize mismatch for 0.bias: copying a param with shape torch.Size([5120]) from checkpoint, the shape in current model is torch.Size([0]).\r\n2024-07-24T06:54:38.991608501-04:00 \tsize mismatch for 2.weight: copying a param with shape torch.Size([5120, 5120]) from checkpoint, the shape in current model is torch.Size([0]).\r\n2024-07-24T06:54:38.991616113-04:00 \tsize mismatch for 2.bias: copying a param with shape torch.Size([5120]) from checkpoint, the shape in current model is torch.Size([0]).\r\n\r\n2024-07-24T06:54:40.587171362-04:00 [2024-07-24 10:54:40,586] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1794\r\n2024-07-24T06:54:40.587524341-04:00 [2024-07-24 10:54:40,587] [ERROR] [launch.py:321:sigkill_handler] ['/usr/bin/python3', '-u', '/home/llava/LLaVA/llava/train/train_mem.py', '--local_rank=0', '--deepspeed', '/home/llava/LLaVA/scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.6-vicuna-13b', '--version', 'v1', '--data_path', '/tmp/dataset/conversations.json', '--image_folder', '/tmp/dataset/images/', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--pretrain_mm_mlp_adapter', '/tmp/checkpoints/mm_projector.bin', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'False', '--bf16', 'True', '--output_dir', '/tmp/checkpoints/llava-v1.6-13b', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--freeze_backbone', '--report_to', 'tensorboard'] exits \r\n```</BODY>\n\n<COMMENTS>\n<Comment by Davidup1 at 2024-08-03T15:47:06Z>\nuse zero2.json may help? But I don't know why, do you have any idea?\n</Comment>\n<Comment by sujoyrc at 2024-08-23T15:35:44Z>\nwhere is the zero2.json file ?\n</Comment>\n<Comment by Davidup1 at 2024-08-23T15:46:00Z>\nunder directory scripts\n</Comment>\n<Comment by sujoyrc at 2024-08-23T16:02:01Z>\nthanks. where do I pass this file in the run command?  like this ?\r\n\r\n```\r\n--pretrain_mm_mlp_adapter zero2.json\r\n```\n</Comment>\n<Comment by sujoyrc at 2024-08-26T05:25:32Z>\n@Davidup1 can you clarify if this is the correct way to run the code with zero2.json\n</Comment>\n<Comment by dandingsky at 2024-09-07T09:55:42Z>\nzero3 provides a wrapper around the model such that when you access its weights they have `torch.Size([0])`. I guess this issue is due to `load_state_dict` after deepspeed engine wraps the model. \r\nzero2's model wrapper keeps the shape of model weights for `load_state_dict`, so does not have this issue.\r\nPossible workaround: finish loading all pretrained weights before deepspeed engine wraps the model. This is easily achieved using huggingface `accelerate` library, by calling `accelerator.prepare()` after loading all weights. However, I am not sure how to do so with deepspeed only.\n</Comment>\n<Comment by linzangsc at 2024-12-25T02:28:51Z>\n> zero3 provides a wrapper around the model such that when you access its weights they have `torch.Size([0])`. I guess this issue is due to `load_state_dict` after deepspeed engine wraps the model. zero2's model wrapper keeps the shape of model weights for `load_state_dict`, so does not have this issue. Possible workaround: finish loading all pretrained weights before deepspeed engine wraps the model. This is easily achieved using huggingface `accelerate` library, by calling `accelerator.prepare()` after loading all weights. However, I am not sure how to do so with deepspeed only.\r\n\r\ngreat help! thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1629,
    "state": "open",
    "created_by": "hanwenxu1",
    "created_at": "2024-07-25T06:12:39Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1629</URL>\n\n<TITLE>[Usage] Error when use cli.py after merge-lora-weights.py</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI used finetune_lora.sh to finetune vicuna-v1.5-13b with custom data. After it, I got a folder with adapter_model.safetensors, non_lora_trainables.bin. Then I merged it by merge-lora-weights.py, and I got another folder with three pytorch-model.bin.\r\nWhen I used cli.py, it occured an error.  \r\n\r\nI didn't use the --load-4bit because of this issue #744. I met the same error before, so I removed it.\r\nCommand:\r\n```\r\nCUDA_VISIBLE_DEVICES=0 python -m llava.serve.cli --model-path /home/xhw/LLaVA/llava-v1.5-13b-lora-merge --model-base /home/xhw/LLaVA/vicuna-13b --image-file /home/xhw/LLaVA/images/8.jpg\r\n```\r\n\r\nLog: \r\n```\r\n[2024-07-25 06:06:17,295] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nLoading LLaVA from base model...\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.42s/it]\r\nSome weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at /home/xhw/LLaVA/vicuna-13b and are newly initialized: ['model.mm_projector.0.bias', 'model.mm_projector.0.weight', 'model.mm_projector.2.bias', 'model.mm_projector.2.weight']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\nLoading additional LLaVA weights...\r\nLoading LoRA weights...\r\nTraceback (most recent call last):\r\n  File \"/home/xhw/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/xhw/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/xhw/LLaVA/llava/serve/cli.py\", line 126, in <module>\r\n    main(args)\r\n  File \"/home/xhw/LLaVA/llava/serve/cli.py\", line 32, in main\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n  File \"/home/xhw/LLaVA/llava/model/builder.py\", line 83, in load_pretrained_model\r\n    model = PeftModel.from_pretrained(model, model_path)\r\n  File \"/home/xhw/anaconda3/envs/llava/lib/python3.10/site-packages/peft/peft_model.py\", line 430, in from_pretrained\r\n    model.load_adapter(model_id, adapter_name, is_trainable=is_trainable, **kwargs)\r\n  File \"/home/xhw/anaconda3/envs/llava/lib/python3.10/site-packages/peft/peft_model.py\", line 984, in load_adapter\r\n    adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs)\r\n  File \"/home/xhw/anaconda3/envs/llava/lib/python3.10/site-packages/peft/utils/save_and_load.py\", line 415, in load_peft_weights\r\n    has_remote_safetensors_file = file_exists(\r\n  File \"/home/xhw/anaconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\r\n    validate_repo_id(arg_value)\r\n  File \"/home/xhw/anaconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 154, in validate_repo_id\r\n    raise HFValidationError(\r\nhuggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/xhw/LLaVA/llava-v1.5-13b-lora-merge'. Use `repo_type` argument if needed.\r\n```\r\n\r\nHere is my folder screenshots.\r\nScreenshots:\r\n![image](https://github.com/user-attachments/assets/d31983ea-e4dd-4baf-af2e-ad537f69b88e)</BODY>\n\n<COMMENTS>\n<Comment by AnnaGao0827 at 2024-09-24T09:35:56Z>\nI have checked the builder.py, it seems you should not have \"lora\" in the name of the folder of merged weight, otherwise it will download non_lora_trainables.bin from huggingface.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1625,
    "state": "open",
    "created_by": "xlxcomputer",
    "created_at": "2024-07-23T14:16:26Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1625</URL>\n\n<TITLE>[Usage]  There is no output from the ASSISTANT when using CLI Inference????</TITLE>\n\n<BODY>### Describe the issue\n\nWhen I use the command python -m llava.serve.cli --model-path liuhaotian/llava-v1.5-7b --image-file 'https://llava-vl.github.io/static/images/view.jpg' --load-4bit as required by the llava project to call the CLI Inference feature, there is no output from the ASSISTANT. I tried using print() to print relevant information, and the specific printout is as shown in the screenshot. It can be seen that input_ids are normal, but output_ids always only output 'tensor[[1,2]]'. I don't know why this error occurs, and I have tried many methods to figure it out, but there is still no solution. So I would like to ask if anyone knows what the reason for this is? How should I resolve this issue so that I can use CLI Inference normally? If you can help me with my problem, I would be very grateful!!!!!\r\n![QQ截图20240723221304](https://github.com/user-attachments/assets/f43fc499-cfcc-40a3-96e3-6bb107000942)\r\nNOTICE:This is a code I add the print() function in llava/serve/cli.py\r\n```\r\nimport argparse\r\nimport torch\r\n\r\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\r\nfrom llava.conversation import conv_templates, SeparatorStyle\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.utils import disable_torch_init\r\nfrom llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path\r\n\r\nfrom PIL import Image\r\n\r\nimport requests\r\nfrom io import BytesIO\r\nfrom transformers import TextStreamer\r\n\r\ndef load_image(image_file):\r\n    if image_file.startswith('http://') or image_file.startswith('https://'):\r\n        response = requests.get(image_file)\r\n        image = Image.open(BytesIO(response.content)).convert('RGB')\r\n    else:\r\n        image = Image.open(image_file).convert('RGB')\r\n    return image\r\n\r\ndef main(args):\r\n    # 模型初始化\r\n    disable_torch_init()\r\n\r\n    model_name = get_model_name_from_path(args.model_path)\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n\r\n    if \"llama-2\" in model_name.lower():\r\n        conv_mode = \"llava_llama_2\"\r\n    elif \"mistral\" in model_name.lower():\r\n        conv_mode = \"mistral_instruct\"\r\n    elif \"v1.6-34b\" in model_name.lower():\r\n        conv_mode = \"chatml_direct\"\r\n    elif \"v1\" in model_name.lower():\r\n        conv_mode = \"llava_v1\"\r\n    elif \"mpt\" in model_name.lower():\r\n        conv_mode = \"mpt\"\r\n    else:\r\n        conv_mode = \"llava_v0\"\r\n\r\n    if args.conv_mode is not None and conv_mode != args.conv_mode:\r\n        print('[WARNING] 自动推断的对话模式是 {}，而 `--conv-mode` 是 {}，使用 {}'.format(conv_mode, args.conv_mode, args.conv_mode))\r\n    else:\r\n        args.conv_mode = conv_mode\r\n\r\n    conv = conv_templates[args.conv_mode].copy()\r\n    if \"mpt\" in model_name.lower():\r\n        roles = ('user', 'assistant')\r\n    else:\r\n        roles = conv.roles\r\n\r\n    image = load_image(args.image_file)\r\n    image_size = image.size\r\n    # 类似 model_worker.py 中的操作\r\n    image_tensor = process_images([image], image_processor, model.config)\r\n    if type(image_tensor) is list:\r\n        image_tensor = [image.to(model.device, dtype=torch.float16) for image in image_tensor]\r\n    else:\r\n        image_tensor = image_tensor.to(model.device, dtype=torch.float16)\r\n\r\n    while True:\r\n        try:\r\n            inp = input(f\"{roles[0]}: \")\r\n        except EOFError:\r\n            inp = \"\"\r\n        if not inp:\r\n            print(\"退出...\")\r\n            break\r\n\r\n        print(f\"{roles[1]}: \", end=\"\")\r\n\r\n        if image is not None:\r\n            # 第一次消息\r\n            if model.config.mm_use_im_start_end:\r\n                inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + inp\r\n            else:\r\n                inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\r\n            image = None\r\n        \r\n        conv.append_message(conv.roles[0], inp)\r\n        conv.append_message(conv.roles[1], None)\r\n        prompt = conv.get_prompt()\r\n\r\n        input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)\r\n        print(\"输入ID:\", input_ids)\r\n        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\r\n        keywords = [stop_str]\r\n        streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\r\n\r\n        with torch.inference_mode():\r\n            output_ids = model.generate(\r\n                input_ids,\r\n                images=image_tensor,\r\n                image_sizes=[image_size],\r\n                do_sample=True if args.temperature > 0 else False,\r\n                temperature=args.temperature,\r\n                max_new_tokens=args.max_new_tokens,\r\n                streamer=streamer,\r\n                use_cache=True)\r\n        print(\"输出ID:\", output_ids)\r\n        outputs = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\r\n        print(\"解码后的输出:\", outputs)\r\n        conv.messages[-1][-1] = outputs\r\n        print(outputs)\r\n        if args.debug:\r\n            print(\"\\n\", {\"prompt\": prompt, \"outputs\": outputs}, \"\\n\")\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--model-path\", type=str, default=\"facebook/opt-350m\")\r\n    parser.add_argument(\"--model-base\", type=str, default=None)\r\n    parser.add_argument(\"--image-file\", type=str, required=True)\r\n    parser.add_argument(\"--device\", type=str, default=\"cuda\")\r\n    parser.add_argument(\"--conv-mode\", type=str, default=None)\r\n    parser.add_argument(\"--temperature\", type=float, default=0.2)\r\n    parser.add_argument(\"--max-new-tokens\", type=int, default=512)\r\n    parser.add_argument(\"--load-8bit\", action=\"store_true\")\r\n    parser.add_argument(\"--load-4bit\", action=\"store_true\")\r\n    parser.add_argument(\"--debug\", action=\"store_true\")\r\n    args = parser.parse_args()\r\n    main(args)\r\n```</BODY>\n\n<COMMENTS>\n<Comment by lilia147852 at 2024-07-29T12:43:53Z>\ni have the same problem\n</Comment>\n<Comment by AlvinChanCG at 2025-03-23T05:07:50Z>\nSame problem. Empty outputs. After check output_ids, got tensor[[1,2], device='cuda:0']. Can anyone solve it??\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1624,
    "state": "closed",
    "created_by": "chenqi-205",
    "created_at": "2024-07-23T08:56:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1624</URL>\n\n<TITLE>[Usage]     from llava.mm_utils import ( ImportError: cannot import name 'load_pretrained_model' from 'llava.mm_utils'</TITLE>\n\n<BODY></BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1623,
    "state": "open",
    "created_by": "ppbrown",
    "created_at": "2024-07-22T22:58:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1623</URL>\n\n<TITLE>[Usage] output is slow, coming out a few words at a time</TITLE>\n\n<BODY>### Describe the issue\n\nWhen I prompt it about an image, it thinks for a while (a few seconds), and then starts giving output.\r\nBut it gives output like its over a 1200 baud modem.\r\nWhy is the output to the terminal so slow???\r\nThis is of particular importance because I am using it in batch mode, and I dont actually care about the terminal output, but it seems to be slowing me down that way.</BODY>\n\n<COMMENTS>\n<Comment by ppbrown at 2024-07-25T00:44:46Z>\ncan i not just turn off the terminal output?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1622,
    "state": "open",
    "created_by": "MaTengSYSU",
    "created_at": "2024-07-21T17:16:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1622</URL>\n\n<TITLE>[Usage] You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.mm_utils import get_model_name_from_path\r\nfrom llava.eval.run_llava import eval_model\r\n\r\n\r\nmodel_path = \"/mnt/sda1/mateng/models/llava-v1.5-13b\"\r\n\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    model_path=model_path,\r\n    model_base=None,\r\n    model_name=get_model_name_from_path(model_path),\r\n    force_download=True\r\n)\r\n\r\nmodel_path = \"/mnt/sda1/mateng/models/llava-v1.5-13b\"\r\nprompt = \"What can you see from this picture?\"\r\nimage_file = \"/mnt/sda1/mateng/mmte/submissions/submission-best/images/Criminal_Planning/concat_image_001.png\"\r\n\r\nargs = type('Args', (), {\r\n    \"model_path\": model_path,\r\n    \"model_base\": None,\r\n    \"model_name\": get_model_name_from_path(model_path),\r\n    \"query\": prompt,\r\n    \"conv_mode\": None,\r\n    \"image_file\": image_file,\r\n    \"sep\": \",\",\r\n    \"temperature\": 1.0,\r\n    \"top_p\": None,\r\n    \"num_beams\": 1,\r\n    \"max_new_tokens\": 512\r\n})()\r\n\r\neval_model(args)\r\n\r\n我在执行以上代码的时候遇到了这个问题：\r\n/home/mateng/anaconda3/envs/mmte/lib/python3.11/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\r\n  torch.utils._pytree._register_pytree_node(\r\n/home/mateng/anaconda3/envs/mmte/lib/python3.11/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\r\n  torch.utils._pytree._register_pytree_node(\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\n/home/mateng/anaconda3/envs/mmte/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nLoading checkpoint shards:  67%|██████████████████████████████████████████████████████████                             | 2/3 [00:15<00:07,  7.93s/it]\r\n\r\n\r\n\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\n这个问题如何解决呢？</BODY>\n\n<COMMENTS>\n<Comment by XCF-Mike at 2024-08-05T09:37:34Z>\n请问解决了吗，我遇到了类似问题\r\n![image](https://github.com/user-attachments/assets/f210668f-a6bc-4395-9618-b940517e2e04)\n</Comment>\n<Comment by KansaiTraining at 2024-08-28T06:08:20Z>\nYes, I also got this when trying what was suggested in the readme\r\n\r\n```\r\nfrom llava.eval.run_llava import eval_model\r\nfrom llava.mm_utils import get_model_name_from_path\r\n\r\nmodel_path = \"liuhaotian/llava-v1.5-7b\"\r\nprompt = \"What are the things I should be cautious about when I visit here?\"\r\nimage_file = \"https://llava-vl.github.io/static/images/view.jpg\"\r\n\r\nargs = type('Args', (), {\r\n    \"model_path\": model_path,\r\n    \"model_base\": None,\r\n    \"model_name\": get_model_name_from_path(model_path),\r\n    \"query\": prompt,\r\n    \"conv_mode\": None,\r\n    \"image_file\": image_file,\r\n    \"sep\": \",\",\r\n    \"temperature\": 0,\r\n    \"top_p\": None,\r\n    \"num_beams\": 1,\r\n    \"max_new_tokens\": 512\r\n})()\r\n\r\neval_model(args)\r\n\r\n```\r\n\r\nHow to solve this?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1621,
    "state": "closed",
    "created_by": "wanlipeng",
    "created_at": "2024-07-21T09:55:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1621</URL>\n\n<TITLE>[Usage]  train llava-v1.5-13b-pretrain  \"loss\":0..0</TITLE>\n\n<BODY></BODY>\n\n<COMMENTS>\n<Comment by wanlipeng at 2024-07-21T10:04:49Z>\n[2024-07-21 09:55:28,417] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-07-21 09:55:30,617] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.11.4\r\n[2024-07-21 09:55:30,617] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}\r\n[2024-07-21 09:55:30,617] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0\r\n[2024-07-21 09:55:30,617] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\r\n[2024-07-21 09:55:30,617] [INFO] [launch.py:163:main] dist_world_size=1\r\n[2024-07-21 09:55:30,617] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0\r\n[2024-07-21 09:55:33,036] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-07-21 09:55:37,824] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2024-07-21 09:55:37,824] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n\r\nYou are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nLoading checkpoint shards:   0%|                                                                                                                      | 0/3 [00:00<?, ?it/s]/opt/conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\n\r\n/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\r\n  warnings.warn(\r\n/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\r\n  warnings.warn(\r\nFormatting inputs...Skip in lazy mode\r\n\r\n/opt/conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\r\n  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\r\n{'loss': 9.3847, 'learning_rate': 1.4326647564469915e-06, 'epoch': 0.0}                                                                                                     \r\n{'loss': 8.7351, 'learning_rate': 2.865329512893983e-06, 'epoch': 0.0}                                                                                                      \r\n{'loss': 0.0, 'learning_rate': 4.2979942693409744e-06, 'epoch': 0.0}                                                                                                        \r\n{'loss': 0.0, 'learning_rate': 5.730659025787966e-06, 'epoch': 0.0}                                                                                                         \r\n{'loss': 0.0, 'learning_rate': 7.163323782234957e-06, 'epoch': 0.0}                                                                                                         \r\n{'loss': 0.0, 'learning_rate': 8.595988538681949e-06, 'epoch': 0.0}                                                                                                         \r\n{'loss': 0.0, 'learning_rate': 1.0028653295128941e-05, 'epoch': 0.0}                                                                                                        \r\n{'loss': 0.0, 'learning_rate': 1.1461318051575932e-05, 'epoch': 0.0}                                                                                                        \r\n{'loss': 0.0, 'learning_rate': 1.2893982808022922e-05, 'epoch': 0.0}                                                                                                        \r\n{'loss': 0.0, 'learning_rate': 1.4326647564469915e-05, 'epoch': 0.0}                                                                                                        \r\n{'loss': 0.0, 'learning_rate': 1.5759312320916907e-05, 'epoch': 0.0}                                                                                                        \r\n{'loss': 0.0, 'learning_rate': 1.7191977077363898e-05, 'epoch': 0.0}                                                                                                        \r\n{'loss': 0.0, 'learning_rate': 1.862464183381089e-05, 'epoch': 0.0}                                                                                                         \r\n{'loss': 0.0, 'learning_rate': 2.0057306590257882e-05, 'epoch': 0.0}                                                                                                        \r\n{'loss': 0.0, 'learning_rate': 2.148997134670487e-05, 'epoch': 0.0}\n</Comment>\n<Comment by WayneTomas at 2024-07-21T14:39:20Z>\nthe same proble\n</Comment>\n<Comment by wanlipeng at 2024-07-22T01:10:55Z>\n@WayneTomas I tried to downgrade the deepspeed library, and the loss seems normal now. I'm not sure what caused it, anyone else know the this problem?\r\n-----------------------------------------------\r\ndeepspeed    0.9.5\r\npydantic        1.10.7  \r\ntorch              2.0.1\r\ntorchvision     0.16.2\r\nflash_attn-2.1.1\n</Comment>\n<Comment by MoyusiteruIori at 2024-12-31T06:52:42Z>\nhave the same problem, but downgrading deepspeed does not work for me😭\n</Comment>\n<Comment by WayneTomas at 2024-12-31T07:03:05Z>\n> have the same problem, but downgrading deepspeed does not work for me😭\r\n\r\nI keep the same version of these packages as wanlipeng's, and it works.\n</Comment>\n<Comment by MoyusiteruIori at 2024-12-31T09:33:19Z>\n> > have the same problem, but downgrading deepspeed does not work for me😭\r\n> \r\n> I keep the same version of these packages as wanlipeng's, and it works.\r\n\r\nOK, my problem was because some wrong arguments in the training script. It turns out that the code works well for me without downgrading any package.😵‍💫Thank you anyway for the reply!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1620,
    "state": "open",
    "created_by": "ai-agi",
    "created_at": "2024-07-20T05:49:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1620</URL>\n\n<TITLE>llava1.6 code and data open-source</TITLE>\n\n<BODY>### Question\n\nHave the llava1.6 code and data open-sourced until now? If yes, please kindly present the link of the code and data, thanks a lot!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1619,
    "state": "open",
    "created_by": "hvgupta",
    "created_at": "2024-07-20T04:05:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1619</URL>\n\n<TITLE>[Question] \"Size mismatch\" Error when finetuning from a projector</TITLE>\n\n<BODY>### Question\r\n\r\nwhen using the `finetune.sh` bash command I get this error\r\n```\r\nTraceback (most recent call last):\r\n  File \"/tmp/code/llava_train/llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"/tmp/code/llava_train/llava/train/train.py\", line 913, in train\r\n    model.get_model().initialize_vision_modules(\r\n  File \"/tmp/code/llava_train/llava/model/llava_arch.py\", line 97, in initialize_vision_modules\r\n    self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'))\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2152, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for Sequential:\r\n\tsize mismatch for 0.weight: copying a param with shape torch.Size([4096, 1024]) from checkpoint, the shape in current model is torch.Size([0]).\r\n\tsize mismatch for 0.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([0]).\r\n\tsize mismatch for 2.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([0]).\r\n\tsize mismatch for 2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([0]).\r\n``` \r\nHow can I solve this?</BODY>\n\n<COMMENTS>\n<Comment by Davidup1 at 2024-07-31T03:40:42Z>\nuse zero2.json may help.\n</Comment>\n<Comment by shenyichong at 2024-08-01T08:30:27Z>\n> use zero2.json may help.\r\n\r\nthis works for me！but I don't know the reason why? are there anyone who can explain this?\n</Comment>\n<Comment by hvgupta at 2024-08-09T11:52:38Z>\nI got it working without using zero2, for me the images were in the wrong directory.\n</Comment>\n<Comment by Camellia-hz at 2024-11-05T03:19:59Z>\nHi, I'm guessing this is because zero3 slices and dices the parameters of the model, causing the SIZE to be off.\r\n@shenyichong\n</Comment>\n<Comment by hehulingxiao at 2024-11-06T12:26:39Z>\nHi, I encounter the same problem, is there some ways to load the checkpoint successfully and finetune the model using zero3?\n</Comment>\n<Comment by Yinhance at 2025-02-18T09:39:24Z>\n> Hi, I encounter the same problem, is there some ways to load the checkpoint successfully and finetune the model using zero3?\n\nHave you solved this problem?\n</Comment>\n<Comment by Jay-zzcoder at 2025-02-27T03:41:20Z>\n> > Hi, I encounter the same problem, is there some ways to load the checkpoint successfully and finetune the model using zero3?\n> \n> Have you solved this problem?\n\nHave you solved this problem?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1618,
    "state": "open",
    "created_by": "Vicent0205",
    "created_at": "2024-07-19T11:58:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1618</URL>\n\n<TITLE>Some images of ocr vqa data in llava_v1_5_mix665k.json do not exist!</TITLE>\n\n<BODY>### Question\n\nWhen I conduct finetuning using the mix665k.json file. I find that there are some images for ocr vqa do not exist!\r\nI find that there are 80,000 ocr_vqa data in mix665k file, while images of 355 data does not exist using the given download script.</BODY>\n\n<COMMENTS>\n<Comment by Georgefwt at 2024-08-06T09:02:29Z>\nYou can download the whole ocr_vqa [here](https://huggingface.co/datasets/ej2/llava-ocr-vqa), I tested it, and worked.\r\n\r\nhttps://huggingface.co/datasets/ej2/llava-ocr-vqa\n</Comment>\n<Comment by dacian7 at 2024-08-11T23:30:45Z>\n> Georgefwt\r\n\r\nThank you!\n</Comment>\n<Comment by Georgefwt at 2024-08-12T05:27:15Z>\nAdditionally, in this dataset, 1437717772.jpg seems to be corrupted and needs to be downloaded again:\r\n\r\n```\r\nwget http://ecx.images-amazon.com/images/I/51YTH4k3fUL.jpg\r\ncp 51YTH4k3fUL.jpg playground/data/ocr_vqa/images/1437717772.jpg\r\n```\n</Comment>\n<Comment by divisionblur at 2024-08-23T07:23:48Z>\nThank you\n</Comment>\n<Comment by Jike338 at 2024-09-04T00:31:53Z>\nthank you immensely helpful\n</Comment>\n<Comment by showstarpro at 2024-12-01T08:32:40Z>\nWhy did I find over 900 errors when using the jpeginfo tool to check images from source https://huggingface.co/datasets/ej2/llava-ocr-vqa?  And  most of their URLs are GIFs\r\n![微信图片_20241201163211](https://github.com/user-attachments/assets/19c92bd9-a5c8-4d46-9448-313f00cd2a75)\n</Comment>\n<Comment by showstarpro at 2024-12-05T13:41:23Z>\n> Additionally, in this dataset, 1437717772.jpg seems to be corrupted and needs to be downloaded again:\r\n> \r\n> ```\r\n> wget http://ecx.images-amazon.com/images/I/51YTH4k3fUL.jpg\r\n> cp 51YTH4k3fUL.jpg playground/data/ocr_vqa/images/1437717772.jpg\r\n> ```\r\n\r\nthank you very much!!!\n</Comment>\n<Comment by Lorenayannnnn at 2025-03-18T17:43:45Z>\nDoes anyone have the image from https://ecx.images-amazon.com/images/I/611qJzGW%2B9L.jpg? (It gives Not Found now.)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1617,
    "state": "open",
    "created_by": "dipikakhullar",
    "created_at": "2024-07-19T06:03:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1617</URL>\n\n<TITLE>Finetuning Using Multiple GPUS -- deepspeed batch size check not passing</TITLE>\n\n<BODY>### Describe the issue\n\nI've just been following the finetuning instructions on this repo, using deepspeed==0.14.0. When I run finetune_task_lora.sh using either zero2.json or zero3.json I see the folllowing issue:\r\n\r\nI have put prints everywhere, and somehow it gives me a runtime error on this last line within train:\r\n```\r\ndef train(attn_implementation=None):\r\n    global local_rank\r\n\r\n    parser = transformers.HfArgumentParser(\r\n        (ModelArguments, DataArguments, TrainingArguments))\r\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\r\n    local_rank = training_args.local_rank\r\n\r\n    # Ensure only one precision mode is enabled\r\n    # print(training_args.fp16, training_args.bf16)\r\n    # if training_args.fp16 and training_args.bf16:\r\n    #     print(\"Both fp16 and bf16 are enabled. Disabling bf16 and using fp16.\")\r\n    #     training_args.bf16 = False\r\n\r\n\r\n    compute_dtype = (torch.float16 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\r\n\r\n    # Ensure correct initialization of distributed environment\r\n    if dist.is_available() and not dist.is_initialized():\r\n        dist.init_process_group(backend='nccl')\r\n    print(f\"Distributed backend initialized. World size: {dist.get_world_size()}, Rank: {dist.get_rank()}\")\r\n\r\n    # DeepSpeed configuration\r\n    print(f\"DeepSpeed configuration file: {training_args.deepspeed}\")\r\n    expected_train_batch_size = training_args.per_device_train_batch_size * dist.get_world_size() * training_args.gradient_accumulation_steps\r\n    print(f\"Expected train_batch_size: {expected_train_batch_size}\")\r\n\r\n    # DeepSpeed configuration\r\n\r\n    # DeepSpeed configuration\r\n    print(f\"DeepSpeed configuration file: {training_args.deepspeed}\")\r\n    print(f\"Expected train_batch_size: {training_args.per_device_train_batch_size * dist.get_world_size() * training_args.gradient_accumulation_steps}\")\r\n    \r\n    deepspeed_config = deepspeed.runtime.config.DeepSpeedConfig(training_args.deepspeed)\r\n\r\n```\r\n\r\nIt seems like the same issue here https://github.com/microsoft/DeepSpeed/issues/3982, Here is my traceback with the prints output as well:\r\n```\r\ninitialize_distributed_training LOCAL_RANK 2\r\nAvailable GPUs: 4\r\nCUDA_VISIBLE_DEVICES: 0,1,2,3\r\nWORLD_SIZE: 4\r\nManually set WORLD_SIZE: 4\r\nDistributed backend initialized. World size: 4, Rank: 2\r\nDeepSpeed configuration file: ./scripts/zero2.json\r\nExpected train_batch_size: 64\r\nDeepSpeed configuration file: ./scripts/zero2.json\r\nExpected train_batch_size: 64\r\nTraceback (most recent call last):\r\n  File \"/home/rmendonc/research/dipika/LLaVa2/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"/home/rmendonc/research/dipika/LLaVa2/LLaVA/llava/train/train.py\", line 845, in train\r\n    deepspeed_config = deepspeed.runtime.config.DeepSpeedConfig(training_args.deepspeed)\r\n  File \"/grogu/user/rmendonc/anaconda2/envs/llava2/lib/python3.10/site-packages/deepspeed/runtime/config.py\", line 796, in __init__\r\n    self._configure_train_batch_size()\r\n  File \"/grogu/user/rmendonc/anaconda2/envs/llava2/lib/python3.10/site-packages/deepspeed/runtime/config.py\", line 979, in _configure_train_batch_size\r\n    self._batch_assertion()\r\n  File \"/grogu/user/rmendonc/anaconda2/envs/llava2/lib/python3.10/site-packages/deepspeed/runtime/config.py\", line 927, in _batch_assertion\r\n    assert train_batch == micro_batch * grad_acc * self.world_size, (\r\nAssertionError: Check batch related parameters. train_batch_size is not equal to micro_batch_per_gpu * gradient_acc_step * world_size 64 != 16 * 1 * 1\r\ninitialize_distributed_training LOCAL_RANK 3\r\nAvailable GPUs: 4\r\nCUDA_VISIBLE_DEVICES: 0,1,2,3\r\nWORLD_SIZE: 4\r\nManually set WORLD_SIZE: 4\r\ninitialize_distributed_training LOCAL_RANK 1\r\nAvailable GPUs: 4\r\nCUDA_VISIBLE_DEVICES: 0,1,2,3\r\nWORLD_SIZE: 4\r\nManually set WORLD_SIZE: 4\r\nDistributed backend initialized. World size: 4, Rank: 3\r\nDeepSpeed configuration file: ./scripts/zero2.json\r\nExpected train_batch_size: 64\r\nDeepSpeed configuration file: ./scripts/zero2.json\r\nExpected train_batch_size: 64\r\nTraceback (most recent call last):\r\n  File \"/home/rmendonc/research/dipika/LLaVa2/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"/home/rmendonc/research/dipika/LLaVa2/LLaVA/llava/train/train.py\", line 845, in train\r\n    deepspeed_config = deepspeed.runtime.config.DeepSpeedConfig(training_args.deepspeed)\r\n  File \"/grogu/user/rmendonc/anaconda2/envs/llava2/lib/python3.10/site-packages/deepspeed/runtime/config.py\", line 796, in __init__\r\n    self._configure_train_batch_size()\r\n  File \"/grogu/user/rmendonc/anaconda2/envs/llava2/lib/python3.10/site-packages/deepspeed/runtime/config.py\", line 979, in _configure_train_batch_size\r\n    self._batch_assertion()\r\n  File \"/grogu/user/rmendonc/anaconda2/envs/llava2/lib/python3.10/site-packages/deepspeed/runtime/config.py\", line 927, in _batch_assertion\r\n    assert train_batch == micro_batch * grad_acc * self.world_size, (\r\nAssertionError: Check batch related parameters. train_batch_size is not equal to micro_batch_per_gpu * gradient_acc_step * world_size 64 != 16 * 1 * 1\r\n```</BODY>\n\n<COMMENTS>\n<Comment by wlh1027 at 2025-03-21T09:33:55Z>\n这个问题解决了吗，现在我也遇到一样的报错\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1616,
    "state": "open",
    "created_by": "NicoZenith",
    "created_at": "2024-07-18T16:02:26Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1616</URL>\n\n<TITLE>[Usage] Print output text during training</TITLE>\n\n<BODY>### Describe the issue\n\nIs there a way to print the output every let's say 20 iterations during training, so we can see how the generation changes?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1615,
    "state": "open",
    "created_by": "yhl-hubhub",
    "created_at": "2024-07-18T08:55:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1615</URL>\n\n<TITLE>Tune the vision tower in the pre-training stage[Question]</TITLE>\n\n<BODY>### Question\n\nIn my case, I want to finetune the vision model in the pre-training stage. However, when I unfreeze the vision mode, it raise the errors. RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1614,
    "state": "open",
    "created_by": "h-aboutalebi",
    "created_at": "2024-07-17T19:20:39Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1614</URL>\n\n<TITLE>[Usage] Logger</TITLE>\n\n<BODY>### Describe the issue\n\nHi,\r\n\r\nThe logger seems to print multiple time with deepspeed. The logger I am using for my code is `build_logger(logger_name, logger_filename)` in  `LLaVA/llava/utils.py`.\r\nWhen I build the logger it creates multiple log files and starts to print multiple times the same `logging.info` in the code. I think this is because of deepspeed but not sure how I can resolve the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1613,
    "state": "open",
    "created_by": "fmy7834",
    "created_at": "2024-07-17T09:09:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1613</URL>\n\n<TITLE>[Questio why 'mm_vision_select_layer' == -2 in config ? n]</TITLE>\n\n<BODY>### Question\n\nIn training scripts, 'mm_vision_select_layer'  is set to be -2, which means the penultimate layer's output of CLIP vision encoder is used as image features. I wonder why not use the last layer's output?\r\n![image](https://github.com/user-attachments/assets/f635564b-e8a8-4034-be97-4947085fe5c7)</BODY>\n\n<COMMENTS>\n<Comment by wnma3mz at 2024-09-04T06:58:32Z>\nhttps://arxiv.org/abs/2304.08485\r\n\r\nI found that the author made some explanations in the `Ablations` section\r\n\r\n> We hypothesize that this is because CLIP’s last layer features may focus more on global and abstract image properties compared to the layer before it, which can focus more on localized properties that are useful for understanding specific image details.\n</Comment>\n<Comment by fmy7834 at 2024-09-04T07:23:04Z>\nGot it. Thank you!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1612,
    "state": "open",
    "created_by": "ThisisBillhe",
    "created_at": "2024-07-17T08:04:27Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1612</URL>\n\n<TITLE>[Usage] Deepspeed will be activated after import CLIP from transformers</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: Deepspeed will be activated after import CLIP from transformers, which should not happen. I reproduce this problem in two different servers, but I don't know why yet. \r\n\r\nCommand:\r\n```\r\n>>> import transformers\r\n>>> from transformers import CLIPVisionModel\r\n[2024-07-17 15:57:01,621] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n>>>\r\n\r\n```\r\n\r\nThen when I am using torchrun with 8 GPUs, \r\n```\r\nCLIPVisionModel.from_pretrained(self.vision_tower_name, device_map=device_map)\r\n```\r\nwill lead to 8x GPU memory occupation on GPU0 and device_map does not work, even it is set to 'cpu'</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1611,
    "state": "open",
    "created_by": "hee-dongdong",
    "created_at": "2024-07-17T03:54:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1611</URL>\n\n<TITLE>[Usage] Error when downloading the weight files manually from huggingface</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nhttps://huggingface.co/liuhaotian/llava-v1.6-34b/discussions/15\r\n\r\nIf I manually download the weights from the huggingrface website, I face this error.\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.cli --model-path ./local_models --image-file \"0_25_0.png\"\r\n```\r\n\r\nLog: \r\n```\r\nValueError: Unrecognized configuration class <class 'transformers.models.llava.configuration_llava.LlavaConfig'> for this kind of AutoModel: AutoModelForCausalLM.\r\nModel type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, ElectraConfig, ErnieConfig, FalconConfig, FuyuConfig, GemmaConfig, Gemma2Config, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, JambaConfig, JetMoeConfig, LlamaConfig, MambaConfig, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, OlmoConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, LlavaConfig, LlavaMptConfig, LlavaMistralConfig.\r\n```</BODY>\n\n<COMMENTS>\n<Comment by CaiYing9801 at 2024-11-01T06:53:21Z>\nI have the same problem. Have you solved it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1610,
    "state": "open",
    "created_by": "wanglongpeng1",
    "created_at": "2024-07-17T02:59:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1610</URL>\n\n<TITLE>[Question] 想换掉Vicuna 模型</TITLE>\n\n<BODY>### Question\n\n想更换Vicuna模型 但是不知道怎么做，是把llava-v1.5-7b/13b换成别的模型吗\r\nI want to replace the Vicuna model, but I don't know how to do it. Replace llava-v1.5-7b/13b with another model?</BODY>\n\n<COMMENTS>\n<Comment by whwangovo at 2024-07-26T11:58:22Z>\n切换模型的话需要从 pretrain 阶段重新训练的，改 pretrain.sh脚本里和 vicuna 有关的参数就可以了\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1609,
    "state": "open",
    "created_by": "Nioolek",
    "created_at": "2024-07-16T07:05:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1609</URL>\n\n<TITLE>[Question] Why Recaptioned Data can enhance performance?</TITLE>\n\n<BODY>### Question\n\nThank you for your great work.\r\n\r\nI see that using recaptioned data ( COCO, CC3M ) in stage 1.5 can enhance performance from this [blog](https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/#stage-1-5-high-quality-knowledge-learning).\r\nThe recaptioned data is generated by LLaVA-NeXT-34B.\r\nI wonder if you have done some analysis on this phenomenon. Why does this improve performance?\r\nI simply think that this method can be disguised to achieve the distillation of large models to small models, or recaptioned data has better consistency?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1607,
    "state": "open",
    "created_by": "XiaoruiMaLU",
    "created_at": "2024-07-15T17:23:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1607</URL>\n\n<TITLE>[Question] Unable to submit VQAv2 result file to the evaluation server</TITLE>\n\n<BODY>### Question\n\nWhen I try to upload the vqav2 result file to the evaluation server https://eval.ai/web/challenges/challenge-page/830/my-submission, after I select the phase, the page will jump to the email verification page, which I have already done when I signed up, and I don't receive any new email after this page shows. I would like to know whether there is any solution to this or another way to evaluate the vqav2 result, thanks!\r\n![截屏2024-07-16 01 18 44](https://github.com/user-attachments/assets/d3812437-0b4e-4ff5-b586-528510a105d1)\r\n\r\n![截屏2024-07-16 01 18 51](https://github.com/user-attachments/assets/e68c7f6b-945b-48f3-96ab-70bdd6e10395)</BODY>\n\n<COMMENTS>\n<Comment by XiaoruiMaLU at 2024-07-20T03:51:53Z>\nIt shows https://eval.ai/web/permission-denied in the link.\n</Comment>\n<Comment by itsqyh at 2024-10-27T16:42:27Z>\nI encountered the same problem with you, have you already addressed it?\n</Comment>\n<Comment by makanju0la at 2025-01-30T18:40:53Z>\nTo solve this, click on Participate and then click on Create New Team to create a team. You would to have a team account in order to submit your result.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1605,
    "state": "open",
    "created_by": "ZihangDu",
    "created_at": "2024-07-15T10:23:14Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1605</URL>\n\n<TITLE>[Usage] How to input multiple pictures and prompts at the same time</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nThis is my code. My purpose is to input two pictures and ask two prompts. My code has no errors, but why is there only one picture output?\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n\r\n\r\nimport requests\r\nfrom PIL import Image\r\n\r\nimport torch\r\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\r\n\r\nmodel_id = \"/aworkspace/users/duzihang/Model/llava-hf/llava-1.5-13b-hf\"\r\n\r\n\r\nfile_path = \"/aworkspace/users/duzihang/Project/LlavaGeneration/LLaVA-main/data/MultiMetu_New/MultiMetu_renew_2.xlsx\"\r\nmodel = LlavaForConditionalGeneration.from_pretrained(\r\n    model_id,\r\n    torch_dtype=torch.float16,\r\n    low_cpu_mem_usage=True,\r\n    load_in_4bit=True,\r\n)\r\n\r\nprocessor = AutoProcessor.from_pretrained(model_id)\r\nprompts = [\r\n  \"USER: <image>\\nWhat is this? ASSISTANT:\",\r\n  \"USER: <image>\\nWhat is this? ASSISTANT:\"\r\n]\r\nimage_path = \"/aworkspace/users/duzihang/Project/LlavaGeneration/LLaVA-main/data/MultiMetu_New/0.jpg\"\r\nimage_path2 = \"/aworkspace/users/duzihang/Project/LlavaGeneration/LLaVA-main/data/MultiMetu_New/1.jpg\"\r\nimage1 = Image.open(image_path)\r\nimage2 = Image.open(image_path2)\r\ninputs = processor(prompts, images=[image2, image1], padding=True, return_tensors=\"pt\").to(\"cuda\")\r\noutput = model.generate(**inputs, max_new_tokens=1000)\r\nprint(processor.decode(output[0][2:], skip_special_tokens=True))\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by biburger at 2024-10-30T09:19:13Z>\nSame question\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1604,
    "state": "open",
    "created_by": "sunxm2357",
    "created_at": "2024-07-15T00:41:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1604</URL>\n\n<TITLE>[Question] Imbalanced and Unstable GPU Usage during the training with Deepspeed</TITLE>\n\n<BODY>Hello, I am using LLaVA codebase on AMD MI250 GPUs and set-up the environment on this machine. During the finetuning stage, I observed the strage GPU occuptation. **The memory allocated on each GPU is imbalanced.** The maximum usage among all GPUs could around 60GB while the minimum usage among all GPUs is around 54GB. (Rank 0 GPU is not the GPU with the most memory usage). Also during the early stage of training, **the memory occupation will gradually increase (or change) instead of staying at the same value after the training start.** See the GPU memory plot as an example. GPU 1 got the most memory usage and GPU 0 got the least memory usage.\r\n\r\n![image](https://github.com/user-attachments/assets/fdeace96-c143-4858-8b98-386d5ba4f3ff)\r\n \r\n\r\nTo use flash attention, I replace the flash attention with xformers and built on rocm.\r\n\r\n**Environment Setup:**\r\n8 * 64GB logical gpu on a single machine\r\ntorch:  2.3.1+rocm6.0\r\ndeepspeed: 0.14.4\r\ntransformers: 4.42.4\r\nxformers: 0.0.26+76fb485.d20240614\r\n\r\nI do not have nvidia gpus in hand, so I would like to find out whether the code can get balanced and stable GPU allocation on A100 or other cards. Please let me know what it is the common gpu usage when runing on nvidia card. Thank you!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1602,
    "state": "open",
    "created_by": "Czi24",
    "created_at": "2024-07-14T10:52:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1602</URL>\n\n<TITLE>LLaVA Colab notebook</TITLE>\n\n<BODY>### Describe the issue\n\nWelcome to try the LLaVA Colab notebook:\r\n https://github.com/Czi24/Awesome-MLLM-LLM-Colab/tree/master/MLLM/LLaVA-colab. \r\n It provides a convenient environment for testing and running the model.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1601,
    "state": "open",
    "created_by": "LaBaZh",
    "created_at": "2024-07-14T06:17:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1601</URL>\n\n<TITLE>[Question] About The Paper Type</TITLE>\n\n<BODY>### Question\n\nIn page 8 of the paper LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models, the 'twitter post from a video' is written as 'witter post from a video'.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1600,
    "state": "open",
    "created_by": "zhaodaojie",
    "created_at": "2024-07-14T02:01:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1600</URL>\n\n<TITLE>Need either a `state_dict` or a `save_folder` containing offloaded weights.</TITLE>\n\n<BODY>### Describe the issue\n\n\r\n \r\nwarnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\r\nLoading checkpoint shards: 100%|██████████| 6/6 [02:41<00:00, 26.98s/it]\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nLoading checkpoint shards: 100%|██████████| 6/6 [05:39<00:00, 56.56s/it]\r\nTraceback (most recent call last):\r\n  File \"/data2/zhaodaojie/project/LLaVA/ceshi/ceshi1.py\", line 36, in <module>\r\n    \r\n  File \"/data2/zhaodaojie/project/LLaVA/llava/eval/run_llava.py\", line 55, in eval_model\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(\r\n  File \"/data2/zhaodaojie/project/LLaVA/llava/model/builder.py\", line 117, in load_pretrained_model\r\n    model = LlavaLlamaForCausalLM.from_pretrained(\r\n  File \"/home/dell/ENTER/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3917, in from_pretrained\r\n    dispatch_model(model, **device_map_kwargs)\r\n  File \"/home/dell/ENTER/envs/llava/lib/python3.10/site-packages/accelerate/big_modeling.py\", line 364, in dispatch_model\r\n    weights_map = OffloadedWeightsLoader(\r\n  File \"/home/dell/ENTER/envs/llava/lib/python3.10/site-packages/accelerate/utils/offload.py\", line 150, in __init__\r\n    raise ValueError(\"Need either a `state_dict` or a `save_folder` containing offloaded weights.\")\r\nValueError: Need either a `state_dict` or a `save_folder` containing offloaded weights.</BODY>\n\n<COMMENTS>\n<Comment by kdwivedi1985 at 2024-08-12T15:17:39Z>\nI am getting the same issue\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[5], [line 1](vscode-notebook-cell:?execution_count=5&line=1)\r\n----> [1](vscode-notebook-cell:?execution_count=5&line=1) pipeline = transformers.pipeline(\r\n      [2](vscode-notebook-cell:?execution_count=5&line=2) \"text-generation\",\r\n      [3](vscode-notebook-cell:?execution_count=5&line=3)       model=model,\r\n      [4](vscode-notebook-cell:?execution_count=5&line=4)       torch_dtype=torch.float16,\r\n      [5](vscode-notebook-cell:?execution_count=5&line=5)  device_map=\"auto\",\r\n      [6](vscode-notebook-cell:?execution_count=5&line=6) )\r\n\r\nFile /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pipelines/__init__.py:895, in pipeline(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\r\n    [893](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pipelines/__init__.py:893) if isinstance(model, str) or framework is None:\r\n    [894](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pipelines/__init__.py:894)     model_classes = {\"tf\": targeted_task[\"tf\"], \"pt\": targeted_task[\"pt\"]}\r\n--> [895](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pipelines/__init__.py:895)     framework, model = infer_framework_load_model(\r\n    [896](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pipelines/__init__.py:896)         model,\r\n    [897](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pipelines/__init__.py:897)         model_classes=model_classes,\r\n    [898](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pipelines/__init__.py:898)         config=config,\r\n    [899](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pipelines/__init__.py:899)         framework=framework,\r\n    [900](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pipelines/__init__.py:900)         task=task,\r\n    [901](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pipelines/__init__.py:901)         **hub_kwargs,\r\n    [902](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pipelines/__init__.py:902)         **model_kwargs,\r\n    [903](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pipelines/__init__.py:903)     )\r\n    [905](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pipelines/__init__.py:905) model_config = model.config\r\n    [906](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pipelines/__init__.py:906) hub_kwargs[\"_commit_hash\"] = model.config._commit_hash\r\n...\r\n    raise ValueError(\"Need either a `state_dict` or a `save_folder` containing offloaded weights.\")\r\nValueError: Need either a `state_dict` or a `save_folder` containing offloaded weights.\n</Comment>\n<Comment by andchir at 2024-11-28T21:53:15Z>\nThe solution is the same as here:\r\nhttps://stackoverflow.com/questions/76617863/flan-t5-xxl-valueerror-need-either-a-state-dict-or-a-save-folder-containin\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1599,
    "state": "open",
    "created_by": "ia-gu",
    "created_at": "2024-07-13T14:29:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1599</URL>\n\n<TITLE>[Usage] Unfixed Random Seed and Get the Different Result in each Experiments</TITLE>\n\n<BODY>### Describe the issue\n\nHi, \r\n\r\nI am trying to fine-tune ViP-LLaVA with my own dataset. \r\nHowever, the performance of the model is different in each experiments, even though I fixed the random seed like below. \r\n```\r\ndef set_global_seed(seed):\r\n    os.environ['PYTHONHASHSEED'] = str(seed)\r\n    random.seed(seed)\r\n    np.random.seed(seed)\r\n    torch.manual_seed(seed)\r\n    torch.cuda.manual_seed(seed)\r\n    torch.cuda.manual_seed_all(seed)\r\n    set_seed(seed)\r\n    torch.backends.cudnn.deterministic = True\r\n    torch.backends.cudnn.benchmark = False\r\n```\r\n\r\nI found the same problem [here](https://github.com/haotian-liu/LLaVA/issues/864), but I cannot solve the problem.\r\n\r\nIs there any advices or ideas to solve this?\r\n\r\nThanks in advance.</BODY>\n\n<COMMENTS>\n<Comment by tianjiedai at 2025-06-13T03:51:31Z>\nHi, have you addressed this problem?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1597,
    "state": "closed",
    "created_by": "feiyangsuo",
    "created_at": "2024-07-12T04:00:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1597</URL>\n\n<TITLE>[Usage] how to load the trained lora?</TITLE>\n\n<BODY>### Describe the issue\r\n\r\n**Question:** \r\nI've trained a lora with scripts/v1_5/finetune_task_lora.sh. How to load the trained lora (as well as the original LLaVA model) for inference?\r\n\r\n**What I've done:**\r\nI've trained a lora with scripts/v1_5/finetune_task_lora.sh, and got a folder like this:\r\n![image](https://github.com/user-attachments/assets/441f0006-ca42-4ccd-9d8c-c1680ef6593b)\r\nI tried to modify the quickstart inference code into:\r\n```\r\nmodel_base = \"liuhaotian/llava-v1.5-7b\"\r\nmodel_path = \"/mnt/workspace/pyprojects/LLaVA-custom/work_dir/llava-v1.5-7b-task-lora\"\r\nprompt = \"What are the things I should be cautious about when I visit here?\"\r\nimage_file = \"https://llava-vl.github.io/static/images/view.jpg\"\r\n\r\nargs = type('Args', (), {\r\n    \"model_path\": model_path,\r\n    \"model_base\": model_base,\r\n    \"model_name\": get_model_name_from_path(model_path),\r\n    \"query\": prompt,\r\n    \"conv_mode\": None,\r\n    \"image_file\": image_file,\r\n    \"sep\": \",\",\r\n    \"temperature\": 0,\r\n    \"top_p\": None,\r\n    \"num_beams\": 1,\r\n    \"max_new_tokens\": 512\r\n})()\r\n\r\neval_model(args)\r\n```\r\n(I just set `model_path` to the trained lora dir and `model_base` to original LLaVA model repo)\r\n\r\nbut I got error like this:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/mnt/workspace/pyprojects/LLaVA-custom/_test_lora.py\", line 29, in <module>\r\n    eval_model(args)\r\n  File \"/mnt/workspace/pyprojects/LLaVA-custom/llava/eval/run_llava.py\", line 55, in eval_model\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(\r\n  File \"/mnt/workspace/pyprojects/LLaVA-custom/llava/model/builder.py\", line 54, in load_pretrained_model\r\n    lora_cfg_pretrained = LlavaConfig.from_pretrained(model_path)\r\n  File \"/mnt/workspace/conda_envs/llava/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 605, in from_pretrained\r\n    config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n  File \"/mnt/workspace/conda_envs/llava/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 634, in get_config_dict\r\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n  File \"/mnt/workspace/conda_envs/llava/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 689, in _get_config_dict\r\n    resolved_config_file = cached_file(\r\n  File \"/mnt/workspace/conda_envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py\", line 356, in cached_file\r\n    raise EnvironmentError(\r\nOSError: /mnt/workspace/pyprojects/LLaVA-custom/work_dir/llava-v1.5-7b-task-lora does not appear to have a file named config.json. Checkout 'https://huggingface.co//mnt/bn/sunyi-dev/mlx/users/suofeiyang/pyprojects/LLaVA-custom/work_dir/llava-v1.5-7b-task-lora/checkpoint-20/main' for available files.\r\n```\r\n\r\n(Alse checked repo liuhaotian/llava-v1-0719-336px-lora-vicuna-13b-v1.3, and found that the dir structure is different from the trained lora's dir.)</BODY>\n\n<COMMENTS>\n<Comment by kumudlakara-bbc at 2024-07-15T09:49:12Z>\nHi @feiyangsuo, I have the same issue. Were you able to figure this out?\n</Comment>\n<Comment by feiyangsuo at 2024-07-15T09:58:27Z>\n> Hi @feiyangsuo, I have the same issue. Were you able to figure this out?\r\n\r\nstill investigating\n</Comment>\n<Comment by feiyangsuo at 2024-07-15T11:05:06Z>\nI guess it's not feasible (or some extra coding required) since my directory only contains tmp lora weights @kumudlakara-bbc \r\nhttps://github.com/haotian-liu/LLaVA/issues/1245\n</Comment>\n<Comment by Aabdihamzehkolaei-CLGX at 2024-08-12T05:04:31Z>\nSomething to be mindful about is the model_name parameter. If you don't specify that, the script generates it based on the model path and it might cause some issues. Trace the model_name in the script and make sure it's what you need.\n</Comment>\n<Comment by SuperheroZLP at 2025-03-30T12:52:34Z>\n请问解决了吗？我也一样的问题\n</Comment>\n<Comment by yuanjunchai at 2025-04-15T23:45:26Z>\nYou could refer to this link, which is llava-v1.5-13b-lora model: https://huggingface.co/liuhaotian/llava-v1.5-13b-lora/tree/main.\nSo first, you need to utilize 'zero_to_fp32.py' to merge Deepspeed ckpt slides into .bin file, such as 'non_lora_trainables.bins'. And then you could using merge LoRA model with model base.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1596,
    "state": "open",
    "created_by": "SunnyMass",
    "created_at": "2024-07-11T07:43:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1596</URL>\n\n<TITLE>Some running questions about commit \"Add NPU Support to LLaVA\"[Usage]</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nI need to run LLaVA on Ascend platform, so I use the code from commit \"Add NPU Support to LLaVA\", but after running successfully on Ascend 910ProA, I find the results is not good . Main problems includes: 1) the answer has many false words which are not correctly spelt. 2) if my prompt gets longer, the answer changes shorter. 3) the inference time is so long. I use one Asend910ProA, and it needs 40mins to complete it. I find the programme uses cpu in some compute steps, is it right?\r\nMy python version is 3.10.0, torch version is 2.1.0,torchvsion version is 0.16.0, torch_npu version is 2.1.0.\r\nsome output examples:\r\n {'prompt': \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\\ndescribe this photo? ASSISTANT: <s> The front of a man has a color full jack-of's face on it', and he lewis a smile. He exepsesses a casule and relaxed dexir, standing in front gf a building. The man is wealing a hat gf a black and white color, and he is posidng for a picture. The background is blend, and the man is the main focus and the center ef attention.</s></s>USER: Describe this video. Pay attention to all objects in the video. The description should be useful for AI to re-generate the video. The description should be no more than six sentences. Here are some examples of good descriptions: 1. A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about. 2. Several giant wooly mammoths approach treading through a snowy meadow, their long wooly fur lightly blows in the wind as they walk, snow covered trees and dramatic snow capped mountains in the distance, mid afternoon light with wispy clouds and a sun high in the distance creates a warm glow, the low camera view is stunning capturing the large furry mammal with beautiful photography, depth of field. 3. Drone view of waves crashing against the rugged cliffs along Big Sur's garay point beach. The crashing blue waters create white-tipped waves, while the golden light of the setting sun illuminates the rocky shore. A small island with a lighthouse sits in the distance, and green shrubbery covers the cliff's edge. The steep drop from the road down to the beach is a dramatic feat, with the cliff’s edges jutting out over the sea. This is a view that captures the raw beauty of the coast and the rugged landscape of the Pacific Coast Highway. ASSISTANT:\", 'outputs': '<s> Dude weeing a hat and jacket with graffffy face on it.</s>'} \r\n\r\n {'prompt': \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\\nwhat is it in this image? ASSISTANT:\", 'outputs': '<s> The focus and main visual image in xthe imagexthe imagextthis message is a man wexxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxt are wearing glassx are wexcxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxt wearing glasses and a hat.</s>'}\r\n\r\nDuring debug, on if  I change the args --device from \"npu\" to \"npu:0\" can it work normally. And I also need to change the file LLaVA/llava/model/multimodal_encoder/clip_encoder.py line 32 to:\r\n        self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name, device_map=\"npu:0\")</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1595,
    "state": "closed",
    "created_by": "zhanxuejie",
    "created_at": "2024-07-10T07:03:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1595</URL>\n\n<TITLE>[Question] The output of the llava-v1.6 model does not match my input prompt</TITLE>\n\n<BODY>### Question\r\n\r\nWhen using llava v1.6, I entered an image and provided the following prompt: \"There are four aspects you must consider: 1) weather: ranging from sunny to snowy... 2) time: the time of day including daytime and nighttime. 3) road: the type of roads, including urban, suburban, rural, or highway. 4) lane: a description of the lane conditions. Please describe this image from fo.\" \"Our aspects mentioned above and give the result in JSON format.\".But the model outputs me a series of dialogue combinations, as shown in the picture below. Has anyone encountered this situation.\r\n![Uploading image.png…]()</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1594,
    "state": "open",
    "created_by": "xiaxiaxiatengxi",
    "created_at": "2024-07-09T13:14:09Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1594</URL>\n\n<TITLE>我的模型用这个SFT 之后会输出<b>作为开头，请问有知道为什么的么？</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune.sh\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/64694898/c192a715-0f6f-470a-ae52-9de7d4962a20)\r\n\r\nYou may attach screenshots if it better explains the issue.\r\n我做webshop的微调训练，之后SFT之后，会出现这样的情况，有的时候click的操作会<b>开头</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1593,
    "state": "open",
    "created_by": "ntuzxx",
    "created_at": "2024-07-06T09:37:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1593</URL>\n\n<TITLE>[Question]</TITLE>\n\n<BODY>### Question\n\nWhen I run the Example Code provided at the LLaVa Code page, as follows:\r\n\r\nmodel_path = \"liuhaotian/llava-v1.5-7b\"\r\nprompt = \"What are the things I should be cautious about when I visit here?\"\r\nimage_file = \"https://llava-vl.github.io/static/images/view.jpg\"\r\n\r\nargs = type('Args', (), {\r\n    \"model_path\": model_path,\r\n    \"model_base\": None,\r\n    \"model_name\": get_model_name_from_path(model_path),\r\n    \"query\": prompt,\r\n    \"conv_mode\": None,\r\n    \"image_file\": image_file,\r\n    \"sep\": \",\",\r\n    \"temperature\": 0,\r\n    \"top_p\": None,\r\n    \"num_beams\": 1,\r\n    \"max_new_tokens\": 512\r\n})()\r\n\r\neval_model(args))  \r\n\r\n\r\nThe error ocurs: \r\n    return F.conv2d(input, weight, bias, self.stride,\r\nRuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED\r\n\r\n\r\nIs there any idea to address it ?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1592,
    "state": "open",
    "created_by": "rayluo88",
    "created_at": "2024-07-05T15:29:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1592</URL>\n\n<TITLE>[Usage] After fine-tuning LLaVA 1.5, mm_projector.bin file is not available</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: I have fine tuned the llava-v1.5-7b with lora. And in the output directory I got some files.\r\n\r\n- adapter_model.safetensors\r\n- config.json\r\n- README.md\r\n- adapter_config.json\r\n- non_lora_trainables.bin\r\n- trainer_state.json\r\n\r\nThe mm_projector.bin file is missing, which is required by [scripts](https://github.com/haotian-liu/LLaVA/tree/main/scripts)/merge_lora_weights.py and run_llava.py (llava/eval/run_llava.py)\r\n\r\nThe mm_projector.bin file contains the projector weights, right? How to generate/extract this file? @haotian-liu \r\n\r\nCommand:\r\n```\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed $DEEPSPEED_JSON \\\r\n    --model_name_or_path $MODEL_NAME \\\r\n    --version v1 \\\r\n    --data_path $DATA_PATH \\\r\n    --image_folder $IMAGE_FOLDER \\\r\n    --vision_tower $VISION_TOWER \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir $OUTPUT_DIR \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by DemonsAH at 2024-07-25T05:43:15Z>\nAdd 'lora' in the folder name helped mine. \r\nMerged model could run inference and showed the affection of fine-tuning. \r\nI assume you can run inference without merging if you add 'lora' in the folder name.\n</Comment>\n<Comment by HuizhaoWang at 2024-09-24T07:18:51Z>\n> ### Describe the issue\r\n> Issue: I have fine tuned the llava-v1.5-7b with lora. And in the output directory I got some files.\r\n> \r\n> * adapter_model.safetensors\r\n> * config.json\r\n> * README.md\r\n> * adapter_config.json\r\n> * non_lora_trainables.bin\r\n> * trainer_state.json\r\n> \r\n> The mm_projector.bin file is missing, which is required by [scripts](https://github.com/haotian-liu/LLaVA/tree/main/scripts)/merge_lora_weights.py and run_llava.py (llava/eval/run_llava.py)\r\n> \r\n> The mm_projector.bin file contains the projector weights, right? How to generate/extract this file? @haotian-liu\r\n> \r\n> Command:\r\n> \r\n> ```\r\n> deepspeed llava/train/train_mem.py \\\r\n>     --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n>     --deepspeed $DEEPSPEED_JSON \\\r\n>     --model_name_or_path $MODEL_NAME \\\r\n>     --version v1 \\\r\n>     --data_path $DATA_PATH \\\r\n>     --image_folder $IMAGE_FOLDER \\\r\n>     --vision_tower $VISION_TOWER \\\r\n>     --mm_projector_type mlp2x_gelu \\\r\n>     --mm_vision_select_layer -2 \\\r\n>     --mm_use_im_start_end False \\\r\n>     --mm_use_im_patch_token False \\\r\n>     --image_aspect_ratio pad \\\r\n>     --group_by_modality_length True \\\r\n>     --bf16 True \\\r\n>     --output_dir $OUTPUT_DIR \\\r\n>     --num_train_epochs 1 \\\r\n>     --per_device_train_batch_size 16 \\\r\n>     --per_device_eval_batch_size 4 \\\r\n>     --gradient_accumulation_steps 1 \\\r\n>     --save_strategy \"steps\" \\\r\n>     --save_steps 50000 \\\r\n>     --save_total_limit 1 \\\r\n>     --learning_rate 2e-4 \\\r\n>     --weight_decay 0. \\\r\n>     --warmup_ratio 0.03 \\\r\n>     --lr_scheduler_type \"cosine\" \\\r\n>     --logging_steps 1 \\\r\n>     --tf32 True \\\r\n>     --model_max_length 2048 \\\r\n>     --gradient_checkpointing True \\\r\n>     --dataloader_num_workers 4 \\\r\n>     --lazy_preprocess True \\\r\n>     --report_to wandb\r\n> ```\r\n> \r\n> Log:\r\n> \r\n> ```\r\n> PASTE THE LOGS HERE.\r\n> ```\r\n> \r\n> Screenshots: You may attach screenshots if it better explains the issue.\r\n \r\n**I also encountered this problem, have you solved it yet?**\n</Comment>\n<Comment by Tess314 at 2025-03-19T13:21:06Z>\n> Add 'lora' in the folder name helped mine. Merged model could run inference and showed the affection of fine-tuning. I assume you can run inference without merging if you add 'lora' in the folder name.\n\nWhich folder name are you referring to? @DemonsAH\n</Comment>\n<Comment by Tess314 at 2025-03-21T12:52:14Z>\nDid this get solved? I am also missing the mm_projector.bin file.\n</Comment>\n<Comment by ZhangJinian at 2025-07-14T03:06:49Z>\n> > Add 'lora' in the folder name helped mine. Merged model could run inference and showed the affection of fine-tuning. I assume you can run inference without merging if you add 'lora' in the folder name.\n> \n> Which folder name are you referring to? [@DemonsAH](https://github.com/DemonsAH)\n\nThe folder you will use to merge the weights. \n\npython scripts/merge_lora_weights.py --model-path \"./checkpoints/`llava-v1.5-7b-lora`\" \\\n       --model-base \"./checkpoints/llava-v1.5-7b\" \\\n       --save-model-path \"./checkpoints/llava-v1.5-7b-merged\"\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1591,
    "state": "open",
    "created_by": "DeWolfRobin",
    "created_at": "2024-07-03T14:47:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1591</URL>\n\n<TITLE>LLava always speaks of 2 images</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI have LLava running via ollama and a python script sending screenshots to it. It's meant to help my blind mother have a description of what's on screen. Whenever I run the script, the model speaks of 2 images, with both being similar to the screenshot with some descrepancies. Is this a hallucination?\r\n\r\nPython code:\r\n```\r\nimport keyboard\r\nimport pyttsx3\r\nfrom PIL import ImageGrab\r\nimport requests\r\nimport base64\r\nfrom io import BytesIO\r\nimport json\r\n\r\n\r\ndef capture_screenshot():\r\n    return ImageGrab.grab()\r\n\r\n\r\ndef describe_image(image):\r\n    buffer = BytesIO()\r\n    image.save(buffer, format=\"JPEG\")\r\n    img_str = base64.b64encode(buffer.getvalue())\r\n    payload = json.dumps({\r\n        \"model\": \"llava\",\r\n        \"prompt\": \"This is a screenshot from a Windows PC. Your job is to describe the contents of the screenshot for the user. The description is for a visualy impaired or blind person. the description should be like that of another person telling the blind person about what they see in front of them. You can ignore any windows elements if they are not the main focus.\",\r\n        \"images\": [img_str.decode(\"utf-8\")],\r\n        \"stream\": False\r\n    })\r\n    r = requests.post('http://localhost:11434/api/generate', data=payload)\r\n    return r.json()[\"response\"]\r\n\r\n\r\ndef narrate_description(description):\r\n    engine = pyttsx3.init()\r\n    engine.say(description)\r\n    engine.runAndWait()\r\n\r\n\r\ndef main():\r\n    print(\"Press the 'home' key to capture a screenshot and get its description.\")\r\n    while True:\r\n        if keyboard.is_pressed('home'):\r\n            print(\"Home key pressed, capturing screenshot...\")\r\n            print(\"Screenshot captured, describing the image...\")\r\n            description = describe_image(capture_screenshot())\r\n            narrate_description(description)\r\n            print(\"Press the 'home' key to capture another screenshot.\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```</BODY>\n\n<COMMENTS>\n<Comment by DeWolfRobin at 2024-07-03T15:08:04Z>\nI think I found the solution. i had \"ollama run llava\", which is the 7b model version 1.6 and the max resolution there is 672x672, 336x1344, 1344x336, so it probably split the image into separate images.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1590,
    "state": "open",
    "created_by": "ThisisBillhe",
    "created_at": "2024-07-03T13:02:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1590</URL>\n\n<TITLE>[Question] For image_aspect_ratio, what is the difference between pad, square and anyres</TITLE>\n\n<BODY>### Question\n\nFor llava-v1.6, the default value of image_aspect_ratio is anyres. However, its default value is square in pretrain.sh and pad in finetune.sh. Why is that?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1589,
    "state": "open",
    "created_by": "zy1996829",
    "created_at": "2024-07-03T10:04:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1589</URL>\n\n<TITLE>[Question] I got stuck here while doing fine-tuning training.</TITLE>\n\n<BODY>### Question\n\nWhen I was doing fine-tuning training, the training got stuck here and did not proceed further. Could you please tell me why this is happening?\r\n![WXWorkCapture_17200009808763](https://github.com/haotian-liu/LLaVA/assets/26729385/d4b17240-149f-491b-ac56-bb765b98975e)\r\n\r\nMy training script：\r\ndeepspeed /home/zhouyan/LLaVA-1.2.2/llava/train/train_mem.py\r\n--deepspeed ./scripts/zero3.json\r\n--model_name_or_path /home/zhouyan/LLaVA-main/checkpoints/llava-v1.5-13b\r\n--version v1\r\n--data_path /home/zhouyan/LLaVA-main/data/driver_action.json\r\n--image_folder /home/zhouyan/cabin_oms_gt_frames/\r\n--vision_tower /home/zhouyan/LLaVA-main/openai/clip-vit-large-patch14-336\r\n--mm_projector_type mlp2x_gelu\r\n--mm_vision_select_layer -2\r\n--mm_use_im_start_end False\r\n--mm_use_im_patch_token False\r\n--image_aspect_ratio pad\r\n--group_by_modality_length True\r\n--bf16 True\r\n--output_dir ./checkpoints/llava-v1.5-13b-task_finetune\r\n--num_train_epochs 1\r\n--per_device_train_batch_size 4\r\n--per_device_eval_batch_size 4\r\n--gradient_accumulation_steps 1\r\n--evaluation_strategy \"no\"\r\n--save_strategy \"steps\"\r\n--save_steps 50000\r\n--save_total_limit 1\r\n--learning_rate 2e-5\r\n--weight_decay 0.\r\n--warmup_ratio 0.03\r\n--lr_scheduler_type \"cosine\"\r\n--logging_steps 1\r\n--tf32 True\r\n--model_max_length 2048\r\n--gradient_checkpointing True\r\n--dataloader_num_workers 4\r\n--lazy_preprocess True \\</BODY>\n\n<COMMENTS>\n<Comment by Ryanlijinke at 2024-11-03T13:03:53Z>\nsame issue... Did you solve it alrdy?\n</Comment>\n<Comment by conan1024hao at 2024-11-06T22:19:22Z>\nexport NCCL_P2P_DISABLE=1\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1588,
    "state": "open",
    "created_by": "selenerkan",
    "created_at": "2024-07-02T08:59:30Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1588</URL>\n\n<TITLE>[Usage] Different seeds are giving the exact same loss when running full finetuning with deepspeed Zero 1,2 or 3</TITLE>\n\n<BODY>### Describe the issue\n\n**Issue:**\r\nI am full-finetuning Llava using deepspeed and whenever I run it on a multi gpu setting with Zero 1,2 or 3, the loss curve is exactly the same for different seeds. I also tried to finetune a smaller part of the model using deepspeed with only 1 GPU and in this case I can get different loss curves for different seeds. But whenever it is a multi gpu setting this behavior changes. I also noticed that when I use version v0.14.4, all zero stages (1,2 and 3) have this behavior of giving the same loss for different seeds, whereas in the version v0.14.3 only zero 3 has it. \r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\n- This is the function I use to set seed\r\n```\r\ndef set_seed(seed):\r\n    print(f\"SETTING GLOBAL SEED TO {seed}\")\r\n    #pl.seed_everything(seed)\r\n    torch.manual_seed(seed)\r\n    torch.cuda.manual_seed_all(seed)\r\n    torch.backends.cudnn.deterministic = True\r\n    torch.backends.cudnn.benchmark = False\r\n    np.random.seed(seed)\r\n    random.seed(seed)\r\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\r\n```\r\n- I am using [this](https://github.com/haotian-liu/LLaVA/tree/v1.0.1) Llava repo\r\n- This is the zero2 file I am using\r\n```\r\n{\r\n    \"fp16\": {\r\n        \"enabled\": \"auto\",\r\n        \"loss_scale\": 0,\r\n        \"loss_scale_window\": 1000,\r\n        \"initial_scale_power\": 16,\r\n        \"hysteresis\": 2,\r\n        \"min_loss_scale\": 1\r\n    },\r\n    \"bf16\": {\r\n        \"enabled\": \"auto\"\r\n    },\r\n    \"train_micro_batch_size_per_gpu\": \"auto\",\r\n    \"train_batch_size\": \"auto\",\r\n    \"gradient_accumulation_steps\": \"auto\",\r\n    \"zero_optimization\": {\r\n        \"stage\": 2,\r\n        \"overlap_comm\": true,\r\n        \"contiguous_gradients\": true,\r\n        \"sub_group_size\": 1e9,\r\n        \"reduce_bucket_size\": \"auto\"\r\n    }\r\n}\r\n```\r\n**Expected behavior**\r\nI expect that using different seeds should produce different loss curves, they should not produce the exact same result\r\n\r\n**ds_report output**\r\nThis is the ds_report after I allocate 5 A100 GPUs\r\n```\r\n--------------------------------------------------\r\nDeepSpeed C++/CUDA extension op report\r\n--------------------------------------------------\r\nNOTE: Ops not installed will be just-in-time (JIT) compiled at\r\n      runtime if needed. Op compatibility means that your system\r\n      meet the required dependencies to JIT install the op.\r\n--------------------------------------------------\r\nJIT compiled ops requires ninja\r\nninja .................. [92m[OKAY][0m\r\n--------------------------------------------------\r\nop name ................ installed .. compatible\r\n--------------------------------------------------\r\n[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.\r\n[93m [WARNING] [0m async_io: please install the libaio-dev package with apt\r\n[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\r\nasync_io ............... [93m[NO][0m ....... [93m[NO][0m\r\nfused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m\r\ncpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m\r\ncpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m\r\ncpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m\r\n[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\r\nevoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m\r\nfp_quantizer ........... [93m[NO][0m ....... [92m[OKAY][0m\r\nfused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m\r\nfused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m\r\ninference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m\r\ncutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m\r\ntransformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m\r\nquantizer .............. [93m[NO][0m ....... [92m[OKAY][0m\r\nragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m\r\nragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m\r\nrandom_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m\r\n[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0\r\n[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible\r\nsparse_attn ............ [93m[NO][0m ....... [93m[NO][0m\r\nspatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m\r\ntransformer ............ [93m[NO][0m ....... [92m[OKAY][0m\r\nstochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m\r\n--------------------------------------------------\r\nDeepSpeed general environment info:\r\ntorch install path ............... ['/dkfz/cluster/gpu/data/OE0612/s426n/robustness/lib/python3.10/site-packages/torch']\r\ntorch version .................... 2.0.1+cu117\r\ndeepspeed install path ........... ['/dkfz/cluster/gpu/data/OE0612/s426n/robustness/lib/python3.10/site-packages/deepspeed']\r\ndeepspeed info ................... 0.14.2, unknown, unknown\r\ntorch cuda version ............... 11.7\r\ntorch hip version ................ None\r\nnvcc version ..................... 11.5\r\ndeepspeed wheel compiled w. ...... torch 2.0, cuda 11.7\r\nshared memory (/dev/shm) size .... 503.84 GB\r\n```\r\n\r\n**System info (please complete the following information):**\r\n - OS: CentOS Linux 7 (Core)\r\n - 5-8 A100 GPUs on the same node\r\n - Python 3.10.0</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1587,
    "state": "open",
    "created_by": "MM-WHU",
    "created_at": "2024-07-01T19:48:28Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1587</URL>\n\n<TITLE>\"Argo Tunnel error\" when using demo via https://llava.hliu.cc/ => demo is not working</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\"Argo Tunnel error\" when using demo via https://llava.hliu.cc/ => demo is not working\r\n![image](https://github.com/haotian-liu/LLaVA/assets/87721310/a0a85a6f-0963-48fd-85d0-92d554d99b45)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1586,
    "state": "open",
    "created_by": "HimanshuBaurai",
    "created_at": "2024-07-01T17:11:45Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1586</URL>\n\n<TITLE>[Question] After merging, not able to infer from the model llava-mistral-v1.6-7b</TITLE>\n\n<BODY>### Question\r\n\r\nmodel at hugging face: liuhaotian/llava-v1.6-mistral-7b\r\nThe script that i used:\r\n#!/bin/bash\r\ndeepspeed ../../llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 16 --lora_alpha 32 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ../zero2.json \\\r\n    --model_name_or_path ../../pretrained/llava-v1.6-mistral-7b \\\r\n    --version mistral_instruct \\\r\n    --data_path ../../playground/AnnotationFiles/finetuneLLaVA-fourCategories-2.json \\\r\n    --image_folder ../../playground/data/FinetuneImages/finetuneLLaVA-fourCategories-2 \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --mm_patch_merge_type spatial_unpad \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length False \\\r\n    --bf16 False \\\r\n    --fp16 True \\\r\n    --output_dir ./llava-lora-mistral \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 500 \\\r\n    --save_total_limit 5 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.05 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 4096 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n    \r\n then merged using merge_lora_weights.py file\r\n But when inferencing from this finetuned model, it thows error: NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\r\n \r\nfrom transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\r\nimport torch\r\nfrom PIL import Image\r\nimport requests\r\nprocessor = LlavaNextProcessor.from_pretrained(fine_tuned_model_path)\r\nmodel = LlavaNextForConditionalGeneration.from_pretrained(fine_tuned_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True) \r\nmodel.to(\"cuda:0\") \r\nurl = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\r\nimage = Image.open(requests.get(url, stream=True).raw)\r\nprompt = \"[INST] <image>\\nWhat is shown in this image? [/INST]\"\r\ninputs = processor(prompt, image, return_tensors=\"pt\").to(\"cuda:0\") \r\noutput = model.generate(**inputs, max_new_tokens=100)\r\nprint(processor.decode(output[0], skip_special_tokens=True))\r\n\r\nthats how i am inferencing.\r\nCan anyone tell whats the problem? \r\n@haotian-liu</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1585,
    "state": "open",
    "created_by": "xiaxiaxiatengxi",
    "created_at": "2024-07-01T08:43:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1585</URL>\n\n<TITLE>How to fine tune Lora without images……</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nHow to fine tune Lora without images……\r\nI want to finetune a Lora without images and use GPTShare format \r\n\r\nData Format: Share GPT and Multiple rounds conversations\r\n{\r\n        \"conversations\": [\r\n            {\r\n                \"from\": \"human\",\r\n                \"rewards\": 0,\r\n                \"loss\": null,\r\n                \"value\": \"WebShop [SEP] Instruction: [SEP] Find me space saving, easy clean baskets and storage for living room with color: grey |  beige, and price lower than 40.00 dollars [SEP] Search.The next action is ?\"\r\n            },\r\n            {\r\n                \"from\": \"gpt\",\r\n                \"rewards\": 0.0,\r\n                \"loss\": true,\r\n                \"value\": \"{\\\"Action\\\":\\\"search[space saving easy clean baskets storage living room grey beige under $40]\\\"}\"\r\n            },\r\n            {\r\n                \"from\": \"human\",\r\n                \"rewards\": 0.0,\r\n                \"loss\": true,\r\n                \"value\": \"WebShop [SEP] Instruction: [SEP] Find me space saving, easy clean baskets and storage for living room with color: grey |  beige, and price lower than 40.00 dollars [SEP] SearchInstruction: [SEP] Find me space saving, easy clean baskets and storage for living room with color: grey |  beige, and price lower than 40.00 dollars [SEP] Back to Search [SEP] Page 1 (Total results: 50) [SEP] Next > [SEP] B09JC84P1M [SEP] RUIXFLR Round End Table with Fabric Storage Basket, Marble Look Tray Side Table, Chic 2 Tier Coffee Table for Living Room Bedroom Office [SEP] $121.99 [SEP] B07FDJYBVW [SEP] Seacrest White Kitchen Cart by Linon [SEP] $331.77 [SEP] B09B7G7P58 [SEP] 3-Piece Dining Table Set, Farmhouse Counter Height Dining Table Set, Wood Dining Table Set with Drop Leaf Table/2 Cross Back Padded Chairs/One Shelf [SEP] $319.0 [SEP] B089B1K9LJ [SEP] Bxwjg Metal Filing Cabinet, 5-Layer File Storage Cabinet, with Lock and Index Label, Aluminum Alloy, for Business [SEP] $146.52 [SEP] B07SXRVKZ2 [SEP] Fab Habitat Seagrass Storage Basket Set - Wicker Pattern Baskets, Strong Handles - Organizer for Blankets, Towels, Pillows, Toys, Laundry, Baby, Kids, Home D\\u00e9cor - Harlem - XL [SEP] $69.0 [SEP] B08R5Z7S2K [SEP] Estink File Cabinet, Grey Metal Moveable Hanging File Cabinet Nightstand Storage Cabinets with 5 Drawers Storage Filing Office Home Use,28 x 41 x 68.5 cm [SEP] $100.0 [SEP] B06XDG8XFX [SEP] CMA Stockroom Kids Wood 5-Pocket Sling Bookshelf Book Display Rack Color Walnut [SEP] $44.32 [SEP] B07FKGQKZ1 [SEP] Home Dynamix Lyndhurst Rotana Modern Area Rug, Contemporary Blue/Gray/Ivory 5'2\\\"x7'4\\\" [SEP] $44.99 to $33.98 [SEP] B079V5YD92 [SEP] Furniture HotSpot \\u2013 Bakers Rack \\u2013 White Stain - 30\\\" W x 15\\\" D x 64.5\\\" H [SEP] $389.99 [SEP] B09B77K9YQ [SEP] Rhomtree 36\\u201d Wood Console Table Sideboard Hall Table with 2 Drawers, 1 Cabinet and 1 Shelf Modern Sofa Table for Hallway, Entryway, Living Room, Kitchen (Green with Brown Top) [SEP] $309.9\"\r\n            },\r\n            {\r\n                \"from\": \"gpt\",\r\n                \"rewards\": 0.0,\r\n                \"loss\": true,\r\n                \"value\": \"{\\\"Action\\\":\\\"click[Next >]\\\"}\"\r\n            },\r\n            {\r\n                \"from\": \"human\",\r\n                \"rewards\": 0.0,\r\n                \"loss\": true,\r\n                \"value\": \"WebShop [SEP] Instruction: [SEP] Find me space saving, easy clean baskets and storage for living room with color: grey |  beige, and price lower than 40.00 dollars [SEP] SearchInstruction: [SEP] Find me space saving, easy clean baskets and storage for living room with color: grey |  beige, and price lower than 40.00 dollars [SEP] Back to Search [SEP] Page 2 (Total results: 50) [SEP] < Prev [SEP] Next > [SEP] B06XFZXXTC [SEP] Coralpearl Inflatable Hanger Travel X 4, White Round Shoulder, Portable Folding Clothes Drying Rack in Metal Hook,Space Saving Coat Storage Set Non Slip Foldable for Home Car Camping Indoor Outdoor [SEP] $17.99 [SEP] B079N3VLRJ [SEP] Baja Night Stand / 2 Drawer / Solid Wood / Rustic Bedside Table for Bedroom, Living Room, Sofa Couch, Hall / Metal Drawer Pulls, Barnwood Finish [SEP] $237.0 [SEP] B08XX6QRK4 [SEP] Full Loft Bed with Desk and Storage Shelves, Wood Loft Bed with Ladder, for Kids, Teens, Adults (Espresso) [SEP] $549.99 [SEP] B07XTK3P89 [SEP] Lunarable Oriental Storage Toy Bag Chair, Eastern Bohem Detail Ottoman Tile Work Inspired Flowers, Stuffed Animal Organizer Washable Bag, Small Size, Turquoise Marigold [SEP] $39.99 [SEP] B09LHM4WQS [SEP] Lift Top Coffee Tables for Living Room Coffee Table with Storage, Lift Cable Coffee Table, Dining Table with Hidden Storage Compartment Chestnut [SEP] $82.99 [SEP] B09C8FZ9YS [SEP] ZZF Bookshelf Sling Bookshelf Small Volume 3 Layers Desktop Bookcase Double Drawer Storage Box Creative Preservative Wood,4 Colors, 2 Styles (Color : A-a, Size : 60X17X48CM) [SEP] $204.72 [SEP] B082KBN23S [SEP] WYSTAO Decorative Mirror Modern Minimalist Living Room Metal Mirror Wall Hanging Hall Pendant Iron Golden Soft Decoration 40/50/60/70cm (Size : 70x70cm/27.6x27.6in) [SEP] $100.0 [SEP] B08P8LRFZ4 [SEP] MTFY Cute Faux Fur Desk Chair for Girls Women, Comfy Fluffy Modern Home Office Chair with Wheels, Elegant Swivel Fuzzy Vanity Chair Makeup Arm Chair for Living Room, Bedroom [SEP] $95.99 [SEP] B09F6SCW47 [SEP] hongxinq Floating TV Stand with Storage Shelves, Chipboard Wall Mounted Hanging Television Cabinet, Modern Storage Cabinet for Living Room, Bedroom, 12Inx11.8Inx35.4In, White [SEP] $90.79 [SEP] B09MNNBDTJ [SEP] Woody BL-EF Electric Fireplace 77\\\" TV Stand (White/Gray) [SEP] $100.0\"\r\n            },\r\n            {\r\n                \"from\": \"gpt\",\r\n                \"rewards\": 0.0,\r\n                \"loss\": true,\r\n                \"value\": \"{\\\"Action\\\":\\\"click[Next >]\\\"}\"\r\n            },\r\n            {\r\n                \"from\": \"human\",\r\n                \"rewards\": 0.0,\r\n                \"loss\": true,\r\n                \"value\": \"WebShop [SEP] Instruction: [SEP] Find me space saving, easy clean baskets and storage for living room with color: grey |  beige, and price lower than 40.00 dollars [SEP] SearchInstruction: [SEP] Find me space saving, easy clean baskets and storage for living room with color: grey |  beige, and price lower than 40.00 dollars [SEP] Back to Search [SEP] Page 3 (Total results: 50) [SEP] < Prev [SEP] Next > [SEP] B09HXDPKJF [SEP] GXP Solid Reclaimed Wood Bedside Cabinet w/3 Drawers Nightstand Side Table [SEP] $100.0 [SEP] B09BYX42DW [SEP] Grommet Blackout Curtains for Bedroom and Living Room - 2 Panels Set Thermal Insulated Room Darkening Curtains (52 x 84 Inch, Navy Blue, 2 Panels) [SEP] $28.99 [SEP] B08SHNQJBW [SEP] Compact Versatile Design X-Shaped Kitchen Baker's Rack Microwave Stand Utility Shelf with 6 Hooks Sturdy Durable Steel, Adjustable Foot Pad [SEP] $208.99 [SEP] B09NRF2QGD [SEP] DIANDIAN Sturdy Coat Rack Stand Hall Solid Wood Coat Tree with 9 Round Hook, for Clothing Hats, Hallway Entryway Office, Easy Assembly (Color : Beige) [SEP] $311.5 [SEP] B009EEVDSQ [SEP] Queen Size Tan Traditional Japanese Floor Futon Mattresses, Foldable Cushion Mats, Yoga, Meditaion 60\\\" Wide X 80\\\" Long [SEP] $149.0 [SEP] B08L2ZDWN2 [SEP] PAVILIA Decorative Sherpa Throw Pillow Covers, Set of 2, 18x18, Light Pink Blush Fluffy Pillow Cases for Couch, Bed, Sofa|Soft Accent Cushion Cover, Shaggy Living Room Decor [SEP] $13.99 [SEP] B0924PKLQ4 [SEP] V-MORO HomePod Mini Wall Mount Holder, Outlet Mount Stand Hidden Cable Management for Apple HomePod Mini Smart Speaker Shelf Without Messy Wires Excellent Space Saving Punch-Free 2-Pack White [SEP] $19.99 [SEP] B09M7M1YB3 [SEP] Refillable Container, Easy To Clean Cream Bottle Wide Mouth Design for Easy Storage Of Specimens for Toilets [SEP] $15.5 [SEP] B09CQ45ZRB [SEP] Merry Christmas Kitchen Mat 2 PCS Cushioned Anti Fatigue Kitchen Runner Rugs Truck with Xmas Tree Red and Black Buffalo Plaid Comfort Standing Desk Mat for Office Floor Mat 19.7\\\"x31.5\\\"+19.7\\\"x47.2\\\" [SEP] $15.9 [SEP] B07DRFDWVT [SEP] JINCHAN Tie Up Valance Moroccan Tile Print Curtain Valance Bedroom Curtain Quatrefoil Flax Linen Blend Textured Geometry Lattice Rod Pocket Window Treatment Set Living Room 1 Panel 18\\\" L Grey [SEP] $12.99\"\r\n            },\r\n            {\r\n                \"from\": \"gpt\",\r\n                \"rewards\": 0.0,\r\n                \"loss\": true,\r\n                \"value\": \"{\\\"Action\\\":\\\"click[Next >]\\\"}\"\r\n            },\r\n            {\r\n                \"from\": \"human\",\r\n                \"rewards\": 0.0,\r\n                \"loss\": true,\r\n                \"value\": \"WebShop [SEP] Instruction: [SEP] Find me space saving, easy clean baskets and storage for living room with color: grey |  beige, and price lower than 40.00 dollars [SEP] SearchInstruction: [SEP] Find me space saving, easy clean baskets and storage for living room with color: grey |  beige, and price lower than 40.00 dollars [SEP] Back to Search [SEP] Page 4 (Total results: 50) [SEP] < Prev [SEP] Next > [SEP] B08CV7M1HV [SEP] C-DECOR Uniqueness Style Middle Coffee Table Wooden and Tempered Glass, Cocktail Table, Sofa Table for Living Room, Walnut Finish [SEP] $1200.0 [SEP] B07Q87P8DQ [SEP] Permo Vintage Rustic Industrial 3-Lights Kitchen Island Chandelier Triple 3 Heads Pendant Hanging Ceiling Lighting Fixture with Oval Cone Clear Glass Shade (Antique) [SEP] $94.99 [SEP] B08PYQVNBQ [SEP] YITAHOME 5 Tier Bookshelf, Open Freestanding 5 Shelf Bookcase 5 Tier Storage Shelf for Display and Collection, Industrial Decorative Shelf for Living Room, Home, Office, Retro Brown Bookshelf [SEP] $159.99 [SEP] B09M6VXD6W [SEP] LJP Nightstand Iron Mesh Nightstand, 3 Layer Portable Narrow Side Table can Bearing 50Kg, Sofa End Table Can Be Used as a Small Bookshelf Bedside Table (Color : Black) [SEP] $179.82 [SEP] B08HN55NGW [SEP] Prepac Entryway Shoe Cubby Console, 60\\\", Drifted Gray [SEP] $280.99 [SEP] B09LCM3NKN [SEP] Sheer Window Curtain Panels Cute Snowman Red Scarf Winter Snowflake,Semi Sheer Window Covering Privacy Drapes 2 Panel Sets Farm Vintage Board Green Grid Window Treatment for Kitchen/Living Room [SEP] $44.05 [SEP] B09PTQFBNT [SEP] Mulyyds Computer Desk with Drawers and Open Shelves, Modern Writing Desk Student Study Table Gaming Workstations PC Laptop Corner Desk for Small Spaces Home Office Dormitory (White, 43x19x27in) [SEP] $159.99 [SEP] B09HPMWHM4 [SEP] Generic Floss Pick, Floss Stick, Clean Flossers High Toughness with Portable Cases Hotel Travel - Blue 100Pcs [SEP] $10.99 [SEP] B09DG92LJL [SEP] HSJWOSA Decorative Rustic Entryway Console Table, 60\\\" Long Sofa Table with Two Different Size Drawers and Bottom Shelf for Storage Ornamental (Color : Black) [SEP] $557.72 [SEP] B09P65MG4G [SEP] Rustic Luxe Large Wooden Sofa Table, Gray [SEP] $134.0\"\r\n            },\r\n            {\r\n                \"from\": \"gpt\",\r\n                \"rewards\": 0.0,\r\n                \"loss\": true,\r\n                \"value\": \"{\\\"Action\\\":\\\"click[< Prev]\\\"}\"\r\n            },\r\n            {\r\n                \"from\": \"human\",\r\n                \"rewards\": 0.0,\r\n                \"loss\": true,\r\n                \"value\": \"WebShop [SEP] Instruction: [SEP] Find me space saving, easy clean baskets and storage for living room with color: grey |  beige, and price lower than 40.00 dollars [SEP] SearchInstruction: [SEP] Find me space saving, easy clean baskets and storage for living room with color: grey |  beige, and price lower than 40.00 dollars [SEP] Back to Search [SEP] Page 3 (Total results: 50) [SEP] < Prev [SEP] Next > [SEP] B09HXDPKJF [SEP] GXP Solid Reclaimed Wood Bedside Cabinet w/3 Drawers Nightstand Side Table [SEP] $100.0 [SEP] B09BYX42DW [SEP] Grommet Blackout Curtains for Bedroom and Living Room - 2 Panels Set Thermal Insulated Room Darkening Curtains (52 x 84 Inch, Navy Blue, 2 Panels) [SEP] $28.99 [SEP] B08SHNQJBW [SEP] Compact Versatile Design X-Shaped Kitchen Baker's Rack Microwave Stand Utility Shelf with 6 Hooks Sturdy Durable Steel, Adjustable Foot Pad [SEP] $208.99 [SEP] B09NRF2QGD [SEP] DIANDIAN Sturdy Coat Rack Stand Hall Solid Wood Coat Tree with 9 Round Hook, for Clothing Hats, Hallway Entryway Office, Easy Assembly (Color : Beige) [SEP] $311.5 [SEP] B009EEVDSQ [SEP] Queen Size Tan Traditional Japanese Floor Futon Mattresses, Foldable Cushion Mats, Yoga, Meditaion 60\\\" Wide X 80\\\" Long [SEP] $149.0 [SEP] B08L2ZDWN2 [SEP] PAVILIA Decorative Sherpa Throw Pillow Covers, Set of 2, 18x18, Light Pink Blush Fluffy Pillow Cases for Couch, Bed, Sofa|Soft Accent Cushion Cover, Shaggy Living Room Decor [SEP] $13.99 [SEP] B0924PKLQ4 [SEP] V-MORO HomePod Mini Wall Mount Holder, Outlet Mount Stand Hidden Cable Management for Apple HomePod Mini Smart Speaker Shelf Without Messy Wires Excellent Space Saving Punch-Free 2-Pack White [SEP] $19.99 [SEP] B09M7M1YB3 [SEP] Refillable Container, Easy To Clean Cream Bottle Wide Mouth Design for Easy Storage Of Specimens for Toilets [SEP] $15.5 [SEP] B09CQ45ZRB [SEP] Merry Christmas Kitchen Mat 2 PCS Cushioned Anti Fatigue Kitchen Runner Rugs Truck with Xmas Tree Red and Black Buffalo Plaid Comfort Standing Desk Mat for Office Floor Mat 19.7\\\"x31.5\\\"+19.7\\\"x47.2\\\" [SEP] $15.9 [SEP] B07DRFDWVT [SEP] JINCHAN Tie Up Valance Moroccan Tile Print Curtain Valance Bedroom Curtain Quatrefoil Flax Linen Blend Textured Geometry Lattice Rod Pocket Window Treatment Set Living Room 1 Panel 18\\\" L Grey [SEP] $12.99\"\r\n            },\r\n            {\r\n                \"from\": \"gpt\",\r\n                \"rewards\": 0.0,\r\n                \"loss\": true,\r\n                \"value\": \"{\\\"Action\\\":\\\"click[B07DRFDWVT]\\\"}\"\r\n            },\r\n            {\r\n                \"from\": \"human\",\r\n                \"rewards\": 0.0,\r\n                \"loss\": true,\r\n                \"value\": \"WebShop [SEP] Instruction: [SEP] Find me space saving, easy clean baskets and storage for living room with color: grey |  beige, and price lower than 40.00 dollars [SEP] SearchInstruction: [SEP] Find me space saving, easy clean baskets and storage for living room with color: grey |  beige, and price lower than 40.00 dollars [SEP] Back to Search [SEP] < Prev [SEP] color [SEP] *soft grey [SEP] soft grey [SEP] taupe [SEP] JINCHAN Tie Up Valance Moroccan Tile Print Curtain Valance Bedroom Curtain Quatrefoil Flax Linen Blend Textured Geometry Lattice Rod Pocket Window Treatment Set Living Room 1 Panel 18\\\" L Grey [SEP] Price: $12.99 [SEP] Rating: N.A. [SEP] Description [SEP] Features [SEP] Reviews [SEP] Buy Now\"\r\n            },\r\n            {\r\n                \"from\": \"gpt\",\r\n                \"rewards\": 0.2,\r\n                \"loss\": true,\r\n                \"value\": \"{\\\"Action\\\":\\\"click[Buy Now]\\\"}\"\r\n            }\r\n        ],\r\n        \"id\": \"webshop_0\"\r\n    },\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by Tree-Shu-Zhao at 2024-11-27T22:06:27Z>\nHi! Have you figured out the tuning process? I have multimodal/text mixed samples. After fine-tuning, I found the performance on the pure textual data samples significantly dropped. May I ask if you got similar results, or could you provide any suggestions about how to prevent the performance drop?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1583,
    "state": "open",
    "created_by": "subiaansari",
    "created_at": "2024-06-30T21:14:12Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1583</URL>\n\n<TITLE>Deepspeed Assertion Error after training is completed while saving check points</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nThis error happens only for epochs >= 4, and number of image samples in fine-tuning dataset >= 41\r\n\r\nCommand:\r\n```\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path liuhaotian/llava-v1.5-7b \\\r\n    --version v1 \\\r\n    --data_path ./data/train_size_chart_ocr_40.json \\\r\n    --image_folder ./data \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir llava-lora-outputs \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 8 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nLog: \r\n```\r\n{'loss': 0.2064, 'learning_rate': 0.0002, 'epoch': 1.0}                                                                                                               \r\n{'loss': 0.2073, 'learning_rate': 0.00017071067811865476, 'epoch': 2.0}                                                                                               \r\n{'loss': 0.1385, 'learning_rate': 0.0001, 'epoch': 2.67}                                                                                                              \r\n{'loss': 0.0457, 'learning_rate': 2.9289321881345254e-05, 'epoch': 3.0}                                                                                               \r\n{'loss': 0.1384, 'learning_rate': 0.0, 'epoch': 4.0}                                                                                                                  \r\n{'train_runtime': 240.1224, 'train_samples_per_second': 0.833, 'train_steps_per_second': 0.021, 'train_loss': 0.1472586028277874, 'epoch': 4.0}                       \r\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:39<00:00, 19.82s/it]\r\nTraceback (most recent call last):\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/size-chart-fine-tune-llava-1.5-7b/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/size-chart-fine-tune-llava-1.5-7b/LLaVA/llava/train/train.py\", line 978, in train\r\n    non_lora_state_dict = get_peft_state_non_lora_maybe_zero_3(\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/size-chart-fine-tune-llava-1.5-7b/LLaVA/llava/train/train.py\", line 159, in get_peft_state_non_lora_maybe_zero_3\r\n    to_return = {k: maybe_zero_3(v, ignore_status=True).cpu() for k, v in to_return.items()}\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/size-chart-fine-tune-llava-1.5-7b/LLaVA/llava/train/train.py\", line 159, in <dictcomp>\r\n    Traceback (most recent call last):\r\nto_return = {k: maybe_zero_3(v, ignore_status=True).cpu() for k, v in to_return.items()}  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/size-chart-fine-tune-llava-1.5-7b/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/size-chart-fine-tune-llava-1.5-7b/LLaVA/llava/train/train.py\", line 122, in maybe_zero_3\r\n    with zero.GatheredParameters([param]):\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 2129, in __exit__\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/size-chart-fine-tune-llava-1.5-7b/LLaVA/llava/train/train.py\", line 978, in train\r\n    non_lora_state_dict = get_peft_state_non_lora_maybe_zero_3(\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/size-chart-fine-tune-llava-1.5-7b/LLaVA/llava/train/train.py\", line 159, in get_peft_state_non_lora_maybe_zero_3\r\n    to_return = {k: maybe_zero_3(v, ignore_status=True).cpu() for k, v in to_return.items()}\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/size-chart-fine-tune-llava-1.5-7b/LLaVA/llava/train/train.py\", line 159, in <dictcomp>\r\n    to_return = {k: maybe_zero_3(v, ignore_status=True).cpu() for k, v in to_return.items()}\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/size-chart-fine-tune-llava-1.5-7b/LLaVA/llava/train/train.py\", line 122, in maybe_zero_3\r\n    self.params[0].partition(param_list=self.params, has_been_updated=False)\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1275, in partition\r\n        self._partition(param_list, has_been_updated=has_been_updated)\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1424, in _partition\r\nwith zero.GatheredParameters([param]):    self._partition_param(param, has_been_updated=has_been_updated)\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 2129, in __exit__\r\n        ret_val = func(*args, **kwargs)\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1457, in _partition_param\r\nself.params[0].partition(param_list=self.params, has_been_updated=False)\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1275, in partition\r\n    free_param(param)\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 285, in free_param\r\n    self._partition(param_list, has_been_updated=has_been_updated)\r\n    assert not param.ds_active_sub_modules, param.ds_summary()\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1424, in _partition\r\nAssertionError: {'id': 290, 'status': 'AVAILABLE', 'numel': 4194304, 'ds_numel': 4194304, 'shape': (4096, 1024), 'ds_shape': (4096, 1024), 'requires_grad': True, 'grad_shape': None, 'persist': False, 'active_sub_modules': {2770}, 'ds_tensor.shape': torch.Size([1048576])}\r\n    self._partition_param(param, has_been_updated=has_been_updated)\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1457, in _partition_param\r\n    free_param(param)\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 285, in free_param\r\n    assert not param.ds_active_sub_modules, param.ds_summary()\r\nAssertionError: {'id': 290, 'status': 'AVAILABLE', 'numel': 4194304, 'ds_numel': 4194304, 'shape': (4096, 1024), 'ds_shape': (4096, 1024), 'requires_grad': True, 'grad_shape': None, 'persist': False, 'active_sub_modules': {2770}, 'ds_tensor.shape': torch.Size([1048576])}\r\nTraceback (most recent call last):\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/size-chart-fine-tune-llava-1.5-7b/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/size-chart-fine-tune-llava-1.5-7b/LLaVA/llava/train/train.py\", line 978, in train\r\n    non_lora_state_dict = get_peft_state_non_lora_maybe_zero_3(\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/size-chart-fine-tune-llava-1.5-7b/LLaVA/llava/train/train.py\", line 159, in get_peft_state_non_lora_maybe_zero_3\r\n    to_return = {k: maybe_zero_3(v, ignore_status=True).cpu() for k, v in to_return.items()}\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/size-chart-fine-tune-llava-1.5-7b/LLaVA/llava/train/train.py\", line 159, in <dictcomp>\r\n    to_return = {k: maybe_zero_3(v, ignore_status=True).cpu() for k, v in to_return.items()}\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/size-chart-fine-tune-llava-1.5-7b/LLaVA/llava/train/train.py\", line 122, in maybe_zero_3\r\n    with zero.GatheredParameters([param]):\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 2129, in __exit__\r\n    self.params[0].partition(param_list=self.params, has_been_updated=False)\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1275, in partition\r\n    self._partition(param_list, has_been_updated=has_been_updated)\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1424, in _partition\r\nTraceback (most recent call last):\r\n    self._partition_param(param, has_been_updated=has_been_updated)\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1457, in _partition_param\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/size-chart-fine-tune-llava-1.5-7b/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/size-chart-fine-tune-llava-1.5-7b/LLaVA/llava/train/train.py\", line 978, in train\r\n    non_lora_state_dict = get_peft_state_non_lora_maybe_zero_3(\r\n    free_param(param)\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n      File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/size-chart-fine-tune-llava-1.5-7b/LLaVA/llava/train/train.py\", line 159, in get_peft_state_non_lora_maybe_zero_3\r\n    to_return = {k: maybe_zero_3(v, ignore_status=True).cpu() for k, v in to_return.items()}\r\nret_val = func(*args, **kwargs)\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 285, in free_param\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/size-chart-fine-tune-llava-1.5-7b/LLaVA/llava/train/train.py\", line 159, in <dictcomp>\r\n    to_return = {k: maybe_zero_3(v, ignore_status=True).cpu() for k, v in to_return.items()}\r\n    assert not param.ds_active_sub_modules, param.ds_summary()\r\nAssertionError: {'id': 290, 'status': 'AVAILABLE', 'numel': 4194304, 'ds_numel': 4194304, 'shape': (4096, 1024), 'ds_shape': (4096, 1024), 'requires_grad': True, 'grad_shape': None, 'persist': False, 'active_sub_modules': {2770}, 'ds_tensor.shape': torch.Size([1048576])}\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/size-chart-fine-tune-llava-1.5-7b/LLaVA/llava/train/train.py\", line 122, in maybe_zero_3\r\n    with zero.GatheredParameters([param]):\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 2129, in __exit__\r\n    self.params[0].partition(param_list=self.params, has_been_updated=False)\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1275, in partition\r\n    self._partition(param_list, has_been_updated=has_been_updated)\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1424, in _partition\r\n    self._partition_param(param, has_been_updated=has_been_updated)\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1457, in _partition_param\r\n    free_param(param)\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 285, in free_param\r\n    assert not param.ds_active_sub_modules, param.ds_summary()\r\nAssertionError: {'id': 290, 'status': 'AVAILABLE', 'numel': 4194304, 'ds_numel': 4194304, 'shape': (4096, 1024), 'ds_shape': (4096, 1024), 'requires_grad': True, 'grad_shape': None, 'persist': False, 'active_sub_modules': {2770}, 'ds_tensor.shape': torch.Size([1048576])}\r\nwandb: \r\nwandb: Run history:\r\nwandb:                    train/epoch ▁▃▅▆██\r\nwandb:              train/global_step ▁▃▅▆██\r\nwandb:            train/learning_rate █▇▅▂▁\r\nwandb:                     train/loss ██▅▁▅\r\nwandb:               train/total_flos ▁\r\nwandb:               train/train_loss ▁\r\nwandb:            train/train_runtime ▁\r\nwandb: train/train_samples_per_second ▁\r\nwandb:   train/train_steps_per_second ▁\r\nwandb: \r\nwandb: Run summary:\r\nwandb:                    train/epoch 4.0\r\nwandb:              train/global_step 5\r\nwandb:            train/learning_rate 0.0\r\nwandb:                     train/loss 0.1384\r\nwandb:               train/total_flos 3148619579392.0\r\nwandb:               train/train_loss 0.14726\r\nwandb:            train/train_runtime 240.1224\r\nwandb: train/train_samples_per_second 0.833\r\nwandb:   train/train_steps_per_second 0.021\r\nwandb: \r\nwandb: You can sync this run to the cloud by running:\r\nwandb: wandb sync /home/ec2-user/SageMaker/de_expansion/llava_dry_run/size-chart-fine-tune-llava-1.5-7b/LLaVA/wandb/offline-run-20240630_205842-d744f7k2\r\nwandb: Find logs at: ./wandb/offline-run-20240630_205842-d744f7k2/logs\r\nwandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\r\n[2024-06-30 21:00:32,528] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 26200\r\n[2024-06-30 21:00:32,937] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 26201\r\n[2024-06-30 21:00:32,945] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 26202\r\n[2024-06-30 21:00:32,945] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 26203\r\n[2024-06-30 21:00:32,953] [ERROR] [launch.py:321:sigkill_handler] ['/home/ec2-user/SageMaker/de_expansion/llava_dry_run/amazon-sagemaker-finetune-/llava_env/bin/python3', '-u', 'llava/train/train_mem.py', '--local_rank=3', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', './data/train_size_chart_ocr_40.json', '--image_folder', './data', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', 'llava-lora-outputs', '--num_train_epochs', '5', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '8', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = 1\r\n```</BODY>\n\n<COMMENTS>\n<Comment by takezoe929 at 2024-11-11T03:32:52Z>\nHave you solved it yet? I meet the same problem\n</Comment>\n<Comment by Andcircle at 2025-02-18T17:53:57Z>\nany updates on this problem?\n</Comment>\n<Comment by ZitianTang at 2025-03-23T18:01:15Z>\nI also meet this problem. Any updates?\n</Comment>\n<Comment by LAYDOWN-J at 2025-07-09T02:44:55Z>\nI also meet this problem. any updates on this problem?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1582,
    "state": "open",
    "created_by": "oroojlooy",
    "created_at": "2024-06-30T17:26:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1582</URL>\n\n<TITLE>device mis-match error on pre-training</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI wanted to run the pre-train code `https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/pretrain.sh`, but it ends to a device mis-match error. It seems that the whole LLM model is on the `cpu` while the data and vision models (its device is changed via `vision_tower.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)` in `train.py`) are on `gpu`, which causes the error. I checked the `train.py` code (`https://github.com/haotian-liu/LLaVA/blob/main/llava/train/train.py`) which is called in pre-training, and it seems that the issue is where it loads the model without a specific `device` or device_map`: \r\n\r\n``` \r\n            model = LlavaLlamaForCausalLM.from_pretrained(\r\n                model_args.model_name_or_path,\r\n                cache_dir=training_args.cache_dir,\r\n                attn_implementation=attn_implementation,\r\n                torch_dtype=(torch.bfloat16 if training_args.bf16 else None),\r\n                **bnb_model_from_pretrained_args\r\n            )\r\n```\r\n\r\nThis ends to the following error: \r\n\r\n```\r\n  File \"/home/afshin/miniconda3/envs/llava/lib/python3.10/contextlib.py\", line 153, in __exit__\r\n    self.gen.throw(typ, value, traceback)\r\n  File \"/home/afshin/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1058, in accumulate\r\n    yield\r\n  File \"/home/afshin/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2216, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/home/afshin/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 3238, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/home/afshin/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 3264, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/home/afshin/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\npython-BaseException\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/afshin/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/afshin/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 171, in forward\r\n    raise RuntimeError(\"module must have its parameters and buffers \"\r\nRuntimeError: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cpu\r\n```\r\nIf I add `device_map=\"auto\",` to `LlavaLlamaForCausalLM.from_pretrained()`, it ends sending the model to `cuda`, but still the `projector` is on `cpu`. I also can manually send that to `cuda`, but since you have run this code, it should work out of the box with no change, and probably I am not following all the steps correctly. So, posting it here to see what am I missing.</BODY>\n\n<COMMENTS>\n<Comment by shashwat14 at 2024-08-22T22:23:14Z>\nSame issue\n</Comment>\n<Comment by Ryann-Ran at 2024-09-01T17:41:13Z>\nHi, I've encountered the same issue. Did you solve this yet?\n</Comment>\n<Comment by Ryann-Ran at 2024-09-13T12:04:31Z>\n> Hi, I've encountered the same issue. Did you solve this yet?\r\n\r\nI solve this issue by changing the usage of GPUs. \r\n### Bash scripts that fail to run:\r\n* Bash script 1\r\n```bash\r\n#SBATCH -N 2\r\n#SBATCH --gres=gpu:2\r\n```\r\n* Bash script 2\r\n```bash\r\n#SBATCH -N 1\r\n#SBATCH --gres=gpu:4\r\n```\r\n### Bash scripts that succeed:\r\n* bash script 3\r\n```bash\r\n#SBATCH -N 2\r\n#SBATCH --gres=gpu:1\r\n```\r\nIt seems that using only 1 GPU on each node can avoid the issue. But I haven't figured out the reason.\n</Comment>\n<Comment by Purshow at 2024-11-04T08:51:03Z>\nsame question，have you solved  it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1581,
    "state": "open",
    "created_by": "pranavsingapore",
    "created_at": "2024-06-28T09:21:23Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1581</URL>\n\n<TITLE>[Question]  lora fine-tuning inference</TITLE>\n\n<BODY>### Question\n\nI finetuned the llava-v1.5-13b model (from the repository liuhaotian/llava-v1.5-13b) using LoRA for 10 epochs. After merging the weights with the provided scripts, I uploaded the weights on huggingface and used the script https://github.com/LLaVA-VL/LLaVA-NeXT/blob/inference/docs/LLaVA-NeXT.md to do the inference, but the model's output during inference is \"na\". \r\n\r\nAny guidance on what might be going wrong would be appreciated. Thank you!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1580,
    "state": "open",
    "created_by": "Timothychen00",
    "created_at": "2024-06-27T06:20:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1580</URL>\n\n<TITLE>[Question] Mac run but get importlib.metadata.PackageNotFoundError: No package metadata was found for bitsandbytes</TITLE>\n\n<BODY>### Question\r\nI want to run this model on my mac (with 16G RAM and M1).but since i followed the instructions int the macos.readme \r\n\r\n#### when i am trying to start a model.(4-bit) using the following command:\r\n```\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b --load-4bit --device mps\r\n```\r\n#### The full output it returns:\r\n<img width=\"1433\" alt=\"截圖 2024-06-27 下午2 24 19\" src=\"https://github.com/haotian-liu/LLaVA/assets/52665482/09c5fda9-f05f-4195-89c7-ba2e31031884\">\r\n\r\n#### I tried:\r\nI tried to  pip install bitsandbytes to fix it, but this time when i run i get :\r\n```\r\nRuntimeError: No GPU found. A GPU is needed for quantization.\r\n```\r\n\r\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by Timothychen00 at 2024-06-27T06:26:51Z>\nPlz anybody help me with this, i really want to run a local llava on my mac 🙏\n</Comment>\n<Comment by zhouhao27 at 2024-06-30T07:56:43Z>\nI got different errors.\r\n\r\nMy command is:\r\n\r\n```\r\npython -m llava.serve.model_worker --device mps --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b\r\n```\r\n\r\nThe error message is:\r\n\r\n> raise ImportError(PROTOBUF_IMPORT_ERROR.format(error_message))\r\n2024-06-30 16:02:02 | ERROR | stderr | ImportError:\r\n2024-06-30 16:02:02 | ERROR | stderr | The new behaviour of LlamaTokenizer (with `self.legacy = False`) requires the protobuf library but it was not found in your environment. Checkout the instructions on the\r\n2024-06-30 16:02:02 | ERROR | stderr | installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\r\n2024-06-30 16:02:02 | ERROR | stderr | that match your environment. Please note that you may need to restart your runtime after installation.\n</Comment>\n<Comment by zhouhao27 at 2024-06-30T08:07:13Z>\n> ### Question\r\n> I want to run this model on my mac (with 16G RAM and M1).but since i followed the instructions int the macos.readme\r\n> \r\n> #### when i am trying to start a model.(4-bit) using the following command:\r\n> ```\r\n> python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b --load-4bit --device mps\r\n> ```\r\n> \r\n> #### The full output it returns:\r\n> <img alt=\"截圖 2024-06-27 下午2 24 19\" width=\"1433\" src=\"https://private-user-images.githubusercontent.com/52665482/343654199-09c5fda9-f05f-4195-89c7-ba2e31031884.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTk3MzQ5NDIsIm5iZiI6MTcxOTczNDY0MiwicGF0aCI6Ii81MjY2NTQ4Mi8zNDM2NTQxOTktMDljNWZkYTktZjA1Zi00MTk1LTg5YzctYmEyZTMxMDMxODg0LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjMwVDA4MDQwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTEwZDYzMmZlMGQ4Y2Y2YTUxYTBkYjAyNDQyNDY0Yzc0NmNlMWZhZWM1YTkxNzRmYmM5YzJiMjMzOWUzMjY3YzImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.J7NSJu7Z6Hlxgl0hT_KRvgoVdZB4bN6Ok63hvvFOPYo\">\r\n> #### I tried:\r\n> I tried to pip install bitsandbytes to fix it, but this time when i run i get :\r\n> \r\n> ```\r\n> RuntimeError: No GPU found. A GPU is needed for quantization.\r\n> ```\r\n> \r\n> _No response_\r\n\r\nI think it asked to uninstall `bitsandbytes` in instruction for Mac.\n</Comment>\n<Comment by HarrisonKeeling at 2024-08-20T06:56:11Z>\n@Timothychen00 remove `--load-4bit`, the error is telling you that quantization requires a GPU, which the m1 does not have.  LLaVA does not have quantization support for mac yet.\n</Comment>\n<Comment by FhilMs at 2025-01-14T14:51:47Z>\nhave you solve the problem?\r\nI met the same problem while using CLI Inference like \r\n```\r\npython -m llava.serve.cli \\\r\n    --model-path liuhaotian/llava-v1.5-7b \\\r\n    --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n    --load-4bit\r\n```\r\n\r\nI first reinstall the bitsandbytes \r\n```\r\npip install bitsandbytes\r\n```\r\n\r\nThen del the --load-4bit \r\n```\r\npython -m llava.serve.cli \\\r\n    --model-path /Users/hexu/Documents/NTU-Learn/capstone_project/llava-v1.5-7b \\\r\n    --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n    --device mps\r\n```\r\n\r\nI get result like:\r\n```\r\nUSER: introduce yourself\r\nASSISTANT: I am an artificial...\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1579,
    "state": "open",
    "created_by": "zhuoyan-xu",
    "created_at": "2024-06-26T19:52:21Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1579</URL>\n\n<TITLE>[Usage] Non-deterministic output during inference, occur specifically in the CLIP ViT encoders</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\n\r\nDear Authors,\r\n\r\nThank you for your great work! I encountered the problem similar to https://github.com/haotian-liu/LLaVA/issues/1013#issue-2101629941 even after explicitly setting `torch.backends.cudnn.deterministic` and related flags. I've noticed that the discrepancies occur specifically in the CLIP ViT encoders, where the vision embeddings produce different values across separate runs.\r\n\r\nWhen comparing two identical inference processes, I observed that the `image_forward_out` varies despite using the same `image` input. This occurs in the following file:\r\n[LLaVA/llava/model/multimodal_encoder/clip_encoder.py](https://github.com/haotian-liu/LLaVA/blob/c121f0432da27facab705978f83c4ada465e46fd/llava/model/multimodal_encoder/clip_encoder.py#L50)\r\n\r\nThis situation occurs starting from the second example until the last one.\r\n\r\nI'm curious to know if you're still experiencing this issue and if you've found a solution. Any insights would be greatly appreciated. Thank you for your time!\r\n\r\n\r\nCommand:\r\n```\r\nCUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/eval/mmvet.sh\r\n```\r\nrun above command twice with different output file. The output text will be different.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1578,
    "state": "open",
    "created_by": "PoopBear1",
    "created_at": "2024-06-26T05:25:02Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1578</URL>\n\n<TITLE>[Question] Why LLaVA-1.5-13b keep repeatting answer in the first round conversation?</TITLE>\n\n<BODY>### Question\n\n![image](https://github.com/haotian-liu/LLaVA/assets/36417130/5d4c0f3b-baf4-46b5-b4bb-5395c76b4895)\r\nAs in the picture, I only ask llava one question and it seems struck to keep producing same answer and never stop until it reach the max output tokens? Any help would be appreciated!</BODY>\n\n<COMMENTS>\n<Comment by PoopBear1 at 2024-06-26T06:20:59Z>\nthe identical issue happends on LLaVA-1.6 as well: \r\n\r\n![Screenshot 2024-06-26 162015](https://github.com/haotian-liu/LLaVA/assets/36417130/b277d5db-87f4-4a20-828b-91ad355b9593)\n</Comment>\n<Comment by siqi0905 at 2024-07-22T09:03:24Z>\nI meet the same problem\n</Comment>\n<Comment by Veason-silverbullet at 2024-08-28T07:05:48Z>\n+1. Anyone share some findings on this phenomenon?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1577,
    "state": "open",
    "created_by": "jongwoopark7978",
    "created_at": "2024-06-26T03:42:57Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1577</URL>\n\n<TITLE>[Question] LLaVA-NeXT-Video 7B with stride=1 Memory Error</TITLE>\n\n<BODY>### Question\n\nHi, \r\n\r\nI am attempting to run the LLaVA-NeXT-Video 7B with  `mm_spatial_pool_stride=1`  on a 24GB 8GPU machine. However, I keep encountering a CUDA out of memory error no matter how many GPUs I use to shard the model or the number of frames I feed into it. Below is the command with key arguments. \r\n\r\n`python playground/demo/video_demo.py --model-path lmms-lab/LLaVA-NeXT-Video-7B-DPO --conv-mode vicuna_v1 --for_get_frames_num 6 --mm_spatial_pool_stride 1 --overwrite True`\r\n\r\nI saw that you were able to run the model with `--mm_spatial_pool_stride 1 `as shown below. How can I run the model with stride=1 on multiple 24GB GPUs? \r\n\r\n<img width=\"1284\" alt=\"Screenshot 2024-06-25 at 8 36 50 PM\" src=\"https://github.com/haotian-liu/LLaVA/assets/68903109/a14fe82a-c61a-4c5f-a168-c1910a5d9c12\"></BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1576,
    "state": "open",
    "created_by": "thu-huangzc",
    "created_at": "2024-06-26T01:22:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1576</URL>\n\n<TITLE>[Question] When I added some code like 'print' in train.py and then run 'finetune_task_lora.sh', it would get the same results of the first run.</TITLE>\n\n<BODY>### Question\n\n<img width=\"495\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/114207518/231a5a25-edf1-44eb-a459-47ec9ab77ec0\">\r\nJust like this, when I added the 'print(conversations)' code here and then I run the script, it didn't show the result. Just like there was some cache. So what should I do to get the latest result.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1575,
    "state": "open",
    "created_by": "lingjunzhao",
    "created_at": "2024-06-25T18:41:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1575</URL>\n\n<TITLE>[Usage] Can't run inference on multiple GPUs</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: When I follow the instructions to install this repos, it can run inference fine when using a single GPU. But when using multiple gpus, it gave errors.\r\n\r\nCommand:\r\n```\r\nCUDA_VISIBLE_DEVICES=0,1 python -m llava.serve.cli \\\r\n    --model-path liuhaotian/llava-v1.5-7b \\\r\n    --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n    --load-4bit\r\n```\r\n\r\nLog: \r\n```\r\n[2024-06-25 18:34:18,978] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.\r\n [WARNING]  async_io: please install the libaio-dev package with apt\r\n [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\r\n [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\r\n [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\r\n [WARNING]  using untested triton version (2.1.0), only 1.0.0 is known to be compatible\r\n/ssd1/softwares/miniconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nLoading checkpoint shards:   0%|                                                                            | 0/2 [00:00<?, ?it/s]/ssd1/softwares/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\nLoading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.56s/it]\r\nUSER: please describe this image\r\nASSISTANT: ../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [96,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [97,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [98,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [99,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [100,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n...\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [95,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\nError device-side assert triggered at line 90 in file /src/csrc/ops.cu\r\n\r\n```\r\n\r\nWondering if you know what's going on here?</BODY>\n\n<COMMENTS>\n<Comment by DripNowhy at 2024-07-01T10:37:42Z>\nI have the same issue\n</Comment>\n<Comment by qingyunyanran at 2024-07-03T03:14:40Z>\nI have the same issue too!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1574,
    "state": "open",
    "created_by": "sonic182",
    "created_at": "2024-06-25T15:29:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1574</URL>\n\n<TITLE>Smaller llm and/or vision models</TITLE>\n\n<BODY>### feature\n\nThere is any test using smaller llm or vision models? maybe it would be interesting to test some combinations like:\r\n\r\n* [Efficientnet](https://huggingface.co/docs/transformers/main/en/model_doc/efficientnet) + backbone + [Minilm2](https://github.com/microsoft/unilm/blob/master/minilm/README.md) \r\n* [google Vit](https://huggingface.co/docs/transformers/model_doc/vit) + backbone + [Minilm2](https://github.com/microsoft/unilm/blob/master/minilm/README.md)\r\n* Efficientnet + backbone + gpt/llama3/opt (less than 7b)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1571,
    "state": "open",
    "created_by": "tanttttt",
    "created_at": "2024-06-22T07:47:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1571</URL>\n\n<TITLE>ModuleNotFoundError: No module named 'llava'</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nModuleNotFoundError: No module named 'llava'\r\nCommand:\r\n```\r\nHow to solve this problem\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/114469440/cee6c771-c527-4b34-a001-ee2f8a0f1e4f)\r\n\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by bluehawk2k at 2024-06-30T16:40:59Z>\nexport  PYTHONPATH=/path/to/your/project :$PYTHONPATH\n</Comment>\n<Comment by zaxxzMia at 2025-03-24T14:31:00Z>\nThank you very much.I solve this problem through adding the path.But I want to know whether I need to add every path I will run .\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1570,
    "state": "open",
    "created_by": "0010SS",
    "created_at": "2024-06-21T15:10:02Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1570</URL>\n\n<TITLE>[Usage] 7B Inferece CUDA Out of Memory for RTX 4090 24GB VRAM</TITLE>\n\n<BODY>**Issue**: I am running an inference of LLaVA-1.6 7B on the demo code using an RTX 4090 with 24 GB of memory. I consistently obtained a `CUDA out of memory` error when running the inference, although I guess 24 GB is fair enough for the 7B model since many have tried to run it using the same VRAM. I've restarted the command line, but the issue is still there. I've also checked the CUDA setups, and they all look good. When I use the 4-bit version of LLaVA, the program works without crashing the CUDA memory. What might be the potential issue for me not being able to run this 7B model using 24 GB VRAM?\r\n\r\n**Command**: This is the exact code from the Demo.\r\n```\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.mm_utils import get_model_name_from_path\r\nfrom llava.eval.run_llava import eval_model\r\n\r\nmodel_path = \"liuhaotian/llava-v1.5-7b\"\r\n\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    model_path=model_path,\r\n    model_base=None,\r\n    model_name=get_model_name_from_path(model_path),\r\n)\r\n\r\nmodel_path = \"liuhaotian/llava-v1.5-7b\"\r\nprompt = \"What are the things I should be cautious about when I visit here?\"\r\nimage_file = \"https://llava-vl.github.io/static/images/view.jpg\"\r\n\r\nargs = type('Args', (), {\r\n    \"model_path\": model_path,\r\n    \"model_base\": None,\r\n    \"model_name\": get_model_name_from_path(model_path),\r\n    \"query\": prompt,\r\n    \"conv_mode\": None,\r\n    \"image_file\": image_file,\r\n    \"sep\": \",\",\r\n    \"temperature\": 0,\r\n    \"top_p\": None,\r\n    \"num_beams\": 1,\r\n    \"max_new_tokens\": 512\r\n})()\r\n\r\neval_model(args)\r\n```\r\n\r\nLog: \r\n```\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nLoading checkpoint shards:   0%|                                                                                                                  | 0/2 [00:00<?, ?it/s]/home/user/miniconda3/envs/videolm/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\nLoading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.16it/s]\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nLoading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.74it/s]\r\nTraceback (most recent call last):\r\n  File \"/home/user/projects/videolm/LLaVA/test.py\", line 31, in <module>\r\n    eval_model(args)\r\n  File \"/home/user/projects/videolm/LLaVA/llava/eval/run_llava.py\", line 115, in eval_model\r\n    output_ids = model.generate(\r\n  File \"/home/user/miniconda3/envs/videolm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/user/projects/videolm/LLaVA/llava/model/language_model/llava_llama.py\", line 125, in generate\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/home/user/projects/videolm/LLaVA/llava/model/llava_arch.py\", line 202, in prepare_inputs_labels_for_multimodal\r\n    image_features = self.encode_images(images)\r\n  File \"/home/user/projects/videolm/LLaVA/llava/model/llava_arch.py\", line 141, in encode_images\r\n    image_features = self.get_model().get_vision_tower()(images)\r\n  File \"/home/user/miniconda3/envs/videolm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/user/miniconda3/envs/videolm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/user/miniconda3/envs/videolm/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/user/miniconda3/envs/videolm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/user/projects/videolm/LLaVA/llava/model/multimodal_encoder/clip_encoder.py\", line 54, in forward\r\n    image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\r\nRuntimeError: CUDA error: out of memory\r\n```</BODY>\n\n<COMMENTS>\n<Comment by zhengyuan-xie at 2024-10-10T05:16:53Z>\nSame question.\n</Comment>\n<Comment by BenjaminMarechalEVITECH at 2024-12-02T19:54:00Z>\nDo you have a second GPU on your machine ?\r\nI have the same error and I have two GPUs on my machine, the second one having less RAM than the first one.\r\n\r\nWhat `device_map` do you obtain ?\r\n`print(model.hf_device_map)`\n</Comment>\n<Comment by IJS1016 at 2025-07-30T07:11:30Z>\nThis load_pretrained_model() is called twice. \nPlease remove this lines.\n```\nmodel_path = \"liuhaotian/llava-v1.5-7b\"\n\ntokenizer, model, image_processor, context_len = load_pretrained_model(\n    model_path=model_path,\n    model_base=None,\n    model_name=get_model_name_from_path(model_path),\n)\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1569,
    "state": "open",
    "created_by": "AshOneN",
    "created_at": "2024-06-20T17:48:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1569</URL>\n\n<TITLE>Why use plain text sharegpt datasets for instruction tuning</TITLE>\n\n<BODY>### Question\n\nShareGPT is used for instruction fine-tuning, with the aim of inserting data from image independent pure text conversations into multiple rounds of image conversations, so that the model can distinguish which instructions are image independent and which are image related?</BODY>\n\n<COMMENTS>\n<Comment by AshOneN at 2024-06-21T12:22:12Z>\nI found that the plain text dialogue is not inserted into the graphic dialogue, and adding plain text to the MLLM to fine-tune the LLM can improve the MLLM's instruct compliance ability？\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1568,
    "state": "closed",
    "created_by": "dcreinerth",
    "created_at": "2024-06-18T14:39:39Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1568</URL>\n\n<TITLE>\"Assertion `srcIndex < srcSelectDimSize` failed\" in Docker on some systems</TITLE>\n\n<BODY>I run a LLaVA system as presented in this repository in a docker compose setup using official Cuda docker images and run into an error on some systems with my custom trained models.\r\nOn a server using Nvidia A100 my setup works: All is fine and all models work as expected. \r\nOn a server using a Nvidia RTX A6000: This model works https://huggingface.co/liuhaotian/llava-v1.6-mistral-7b , but a custom trained LLavA-mistral7b gives this error during inference (on the A100 server the custom model runs without problems):\r\n\r\nLog: \r\n```\r\nllava_worker-1 | ../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [434,0,0], thread: [64,0,0] Assertion srcIndex < srcSelectDimSize failed.\r\nllava_worker-1 | ../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [434,0,0], thread: [65,0,0] Assertion srcIndex < srcSelectDimSize failed.\r\nllava_worker-1 | ../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [434,0,0], thread: [66,0,0] Assertion srcIndex < srcSelectDimSize failed.\r\n[...]\r\nllava_worker-1 | ../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [434,0,0], thread: [94,0,0] Assertion srcIndex < srcSelectDimSize failed.\r\nllava_worker-1 | ../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [434,0,0], thread: [95,0,0] Assertion srcIndex < srcSelectDimSize failed.\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | Exception in thread Thread-3:\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | Traceback (most recent call last):\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | self.run()\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | File \"/usr/lib/python3.8/threading.py\", line 870, in run\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | self._target(*self._args, **self._kwargs)\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | File \"/usr/local/lib/python3.8/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | return func(*args, **kwargs)\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | File \"/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py\", line 1736, in generate\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | result = self._sample(\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | File \"/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py\", line 2375, in _sample\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | outputs = self(\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | return self._call_impl(*args, **kwargs)\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | return forward_call(*args, **kwargs)\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | File \"/usr/local/lib/python3.8/dist-packages/transformers/models/mistral/modeling_mistral.py\", line 1139, in forward\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | outputs = self.model(\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | return self._call_impl(*args, **kwargs)\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | return forward_call(*args, **kwargs)\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | File \"/usr/local/lib/python3.8/dist-packages/transformers/models/mistral/modeling_mistral.py\", line 985, in forward\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | File \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_attn_mask_utils.py\", line 372, in _prepare_4d_causal_attention_mask_for_sdpa\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | ignore_causal_mask = AttentionMaskConverter._ignore_causal_mask_sdpa(\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | File \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_attn_mask_utils.py\", line 279, in _ignore_causal_mask_sdpa\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | elif (is_training or not is_tracing) and torch.all(attention_mask == 1):\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | RuntimeError: CUDA error: device-side assert triggered\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nllava_worker-1 | 2024-06-18 14:27:21 | ERROR | stderr | Compile with TORCH_USE_CUDA_DSA to enable device-side assertions.\r\n```\r\nCan you give me any advice on what can cause this different behavior on different machine despite using the same Docker setup?\r\n\r\nThank you!</BODY>\n\n<COMMENTS>\n<Comment by dcreinerth at 2024-06-19T13:21:56Z>\nIn case this error log is more helpful (run with CUDA_LAUNCH_BLOCKING = 1)\r\n```\r\n` [...]\r\nllava_worker-1      | ../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [431,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\nllava_worker-1      | ../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [431,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\nllava_worker-1      | ../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [431,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\nllava_worker-1      | ../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [431,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\nllava_worker-1      | ../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [431,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr | Exception in thread Thread-3 (generate):\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr | Traceback (most recent call last):\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |   File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |     self.run()\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |   File \"/usr/lib/python3.10/threading.py\", line 953, in run\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |     self._target(*self._args, **self._kwargs)\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |     return func(*args, **kwargs)\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 1736, in generate\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |     result = self._sample(\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 2375, in _sample\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |     outputs = self(\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |     return self._call_impl(*args, **kwargs)\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py\", line 1139, in forward\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |     outputs = self.model(\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |     return self._call_impl(*args, **kwargs)\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py\", line 968, in forward\r\nllava_worker-1      |  2024-06-19 13:11:02 | ERROR | stderr |     inputs_embeds = self.embed_tokens(input_ids)\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |     return self._call_impl(*args, **kwargs)\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\", line 163, in forward\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |     return F.embedding(\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2264, in embedding\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr |     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr | RuntimeError: CUDA error: device-side assert triggered\r\nllava_worker-1      | 2024-06-19 13:11:02 | ERROR | stderr | Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n`\r\n```\n</Comment>\n<Comment by dcreinerth at 2024-07-26T07:29:34Z>\nI found a solution for my problem. \r\nFor me it was not a CUDA bug and that it worked on a different machine was pure coincidence.\r\n\r\nThe architecture of the model is picked based on the name of the folder the model is in. Therefore if the naming conventions are not met then the model is loaded in a wrong architecture and will crash during its first inference.\r\nIn my case I used a LLaVA Mistral model and just named it \"mistral_XXXX\". This let to the error shown above.\r\nWhen I renamed the model folder to \"llava_mistral_XXXX\" the model worked fine. \r\nIn gradio_web_server.py you can see which keywords are looked for in the name of the model folder. \r\n\r\nTo prevent this maybe a log would be a big help to display which model architecture is loaded\n</Comment>\n<Comment by dcreinerth at 2024-07-26T07:34:01Z>\nHopefully the solution in my last comment will help someone.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1567,
    "state": "closed",
    "created_by": "anas-zafar",
    "created_at": "2024-06-18T12:51:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1567</URL>\n\n<TITLE>Having issues while merging LoRA attention weights</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: After finetuning on the model when I merge the LoRA (Local Regions of Attention) weights with the updated model weights using the script merge_lora_weights.py, I am facing the issue below\r\n\r\nCommand:\r\n```\r\n!python /content/LLaVA/scripts/merge_lora_weights.py --model-path /path/to/checkpoints/llava-v1.5-7b-task-lora --model-base liuhaotian/llava-v1.5-7b --save-model-path /output/merged_model\r\n     \r\n```\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/content/LLaVA/scripts/merge_lora_weights.py\", line 22, in <module>\r\n    merge_lora(args)\r\n  File \"/content/LLaVA/scripts/merge_lora_weights.py\", line 8, in merge_lora\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, device_map='cpu')\r\n  File \"/content/LLaVA/llava/model/builder.py\", line 128, in load_pretrained_model\r\n    model = AutoModelForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 569, in from_pretrained\r\n    raise ValueError(\r\nValueError: Unrecognized configuration class <class 'transformers.models.llava.configuration_llava.LlavaConfig'> for this kind of AutoModel: AutoModelForCausalLM.\r\nModel type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, ElectraConfig, ErnieConfig, FalconConfig, FuyuConfig, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, LlamaConfig, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MptConfig, MusicgenConfig, MvpConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, LlavaConfig, LlavaMptConfig, LlavaMistralConfig.\r\n```</BODY>\n\n<COMMENTS>\n<Comment by NandhaKishorM at 2024-06-21T04:56:12Z>\nadd llava in the folder name. Solved mine\n</Comment>\n<Comment by sumit-mahaseel at 2024-07-05T12:03:45Z>\nDid not get it? can you explain where to add folder name?\n</Comment>\n<Comment by DemonsAH at 2024-07-25T05:33:11Z>\n> Did not get it? can you explain where to add folder name?\r\n\r\nchange your path from 'your/model/path' to 'your/model/llava-path'\n</Comment>\n<Comment by HenryJunW at 2024-07-31T20:58:23Z>\n@sumit-mahaseel  did you fix it? same issue\n</Comment>\n<Comment by Lareina2441 at 2024-10-05T14:52:19Z>\n> > Did not get it? can you explain where to add folder name?\r\n> \r\n> change your path from 'your/model/path' to 'your/model/llava-path'\r\n\r\nreally!\n</Comment>\n<Comment by 1835969208 at 2025-03-04T10:54:43Z>\n牛逼\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1566,
    "state": "closed",
    "created_by": "supech",
    "created_at": "2024-06-18T05:31:59Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1566</URL>\n\n<TITLE>[Questions] \"non_lora_trainables.bin\" has an impact on the model</TITLE>\n\n<BODY>### Describe the issue\n\nI originally thought that the output of lora training ouput \"non_lora_trainables.bin\" could be shared with other checkpoints as the literal meaning.\r\nSo I did a test. I simply trained it twice. Only the \"non_lora_trainables.bin\" trained that time will have normal image dialogue output. If \"non_lora_trainables.bin\" is exchanged, it will be abnormal.\r\nI would like to ask if there is a way to save \"non_lora_trainables.bin\" and \"config.json\" in all checkpoints during the lora training process. Otherwise, the checkpoints are saved but cannot be used and it is difficult to correctly judge the model performance.</BODY>\n\n<COMMENTS>\n<Comment by supech at 2024-06-19T16:13:11Z>\nhttps://github.com/haotian-liu/LLaVA/issues/729\r\nhttps://github.com/haotian-liu/LLaVA/issues/844\r\n\r\nThe discussions in these two issues are useful to me.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1565,
    "state": "open",
    "created_by": "SCZwangxiao",
    "created_at": "2024-06-17T13:38:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1565</URL>\n\n<TITLE>[Question] Question on `image_newline` for single image</TITLE>\n\n<BODY>I think the `image_newline` here is the implementation of `Row-ended tokens` in the paper.\r\nhttps://github.com/haotian-liu/LLaVA/blob/c121f0432da27facab705978f83c4ada465e46fd/llava/model/llava_arch.py#L82-L86\r\n\r\n\r\nHowever, for single image input, the tokens are not appended to each row as expected in the paper. Specifically, only one token is appended to the flatten patch tokens of the image.\r\nhttps://github.com/haotian-liu/LLaVA/blob/c121f0432da27facab705978f83c4ada465e46fd/llava/model/llava_arch.py#L191-L196</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1564,
    "state": "open",
    "created_by": "Cap1tao",
    "created_at": "2024-06-17T12:14:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1564</URL>\n\n<TITLE>[Question]  What is the accuracy of InstrumentBLIP on VQAv2?</TITLE>\n\n<BODY>### Question\r\n\r\nWhat is the accuracy of InstrumentBLIP on VQAv2? I recently read this paper and found that there are no relevant results at all.\r\nSent the wrong section，sry</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1563,
    "state": "open",
    "created_by": "lucasjinreal",
    "created_at": "2024-06-17T11:45:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1563</URL>\n\n<TITLE>Does the llavaNext 1.5 stage still trained only proj only?</TITLE>\n\n<BODY>Does the llavaNext 1.5 stage still trained only proj only?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1562,
    "state": "open",
    "created_by": "oroojlooy",
    "created_at": "2024-06-16T16:50:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1562</URL>\n\n<TITLE>LLaVA Context-length</TITLE>\n\n<BODY>### Question\n\nIs there any table/page with the context-len of each model?\r\nFor sure one can load all the models one by one and get the `context_len` via \r\n```\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(...)\r\n```\r\nBut, this is very time consuming.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1561,
    "state": "open",
    "created_by": "X1AOX1A",
    "created_at": "2024-06-16T15:39:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1561</URL>\n\n<TITLE>[Usage] Missing File in Release LLaVA-v1.6 and RuntimeError in model_vqa_qbench.py</TITLE>\n\n<BODY>### Describe the issue\n\n**Issue:**\r\n\r\nThe file `llava/eval/model_vqa_qbench.py` is missing in the [Release LLaVA-v1.6](https://github.com/haotian-liu/LLaVA/commit/0a16dea493ac62083c6c2d875de2398391f0f4a3). Could you provide some insight into why this file was removed?\r\n\r\nAdditionally, I encountered a bug while using the file `llava/eval/model_vqa_qbench.py` from commit [ac89962](https://github.com/haotian-liu/LLaVA/commit/ac89962d8fb191f42a0eed965a949c8bb316833a).\r\n\r\n**Command:**\r\n```\r\nCUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/eval/qbench.sh dev\r\n```\r\n\r\n**Log:**\r\n```\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/envs/feat_align/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/root/anaconda3/envs/feat_align/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/data/root/Documents/CODES/FeatAlign/llava/eval/model_vqa_qbench.py\", line 122, in <module>\r\n    eval_model(args)\r\n  File \"/data/root/Documents/CODES/FeatAlign/llava/eval/model_vqa_qbench.py\", line 99, in eval_model\r\n    n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\r\nRuntimeError: The size of tensor a (77) must match the size of tensor b (5) at non-singleton dimension 1\r\n```\r\n\r\nRelated issue: [Q-Bench Evaluation Scripts for LLaVA-v1.5. #581](https://github.com/haotian-liu/LLaVA/commits/1ec53a4c247897d636743da6614a4dd68be0e472/llava/eval/model_vqa_qbench.py)\r\n\r\nThank you!</BODY>\n\n<COMMENTS>\n<Comment by TGLTommy at 2024-06-24T03:50:38Z>\n> ### Describe the issue\r\n> **Issue:**\r\n> \r\n> The file `llava/eval/model_vqa_qbench.py` is missing in the [Release LLaVA-v1.6](https://github.com/haotian-liu/LLaVA/commit/0a16dea493ac62083c6c2d875de2398391f0f4a3). Could you provide some insight into why this file was removed?\r\n> \r\n> Additionally, I encountered a bug while using the file `llava/eval/model_vqa_qbench.py` from commit [ac89962](https://github.com/haotian-liu/LLaVA/commit/ac89962d8fb191f42a0eed965a949c8bb316833a).\r\n> \r\n> **Command:**\r\n> \r\n> ```\r\n> CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/eval/qbench.sh dev\r\n> ```\r\n> \r\n> **Log:**\r\n> \r\n> ```\r\n> Traceback (most recent call last):\r\n>   File \"/root/anaconda3/envs/feat_align/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n>     return _run_code(code, main_globals, None,\r\n>   File \"/root/anaconda3/envs/feat_align/lib/python3.10/runpy.py\", line 86, in _run_code\r\n>     exec(code, run_globals)\r\n>   File \"/data/root/Documents/CODES/FeatAlign/llava/eval/model_vqa_qbench.py\", line 122, in <module>\r\n>     eval_model(args)\r\n>   File \"/data/root/Documents/CODES/FeatAlign/llava/eval/model_vqa_qbench.py\", line 99, in eval_model\r\n>     n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\r\n> RuntimeError: The size of tensor a (77) must match the size of tensor b (5) at non-singleton dimension 1\r\n> ```\r\n> \r\n> Related issue: [Q-Bench Evaluation Scripts for LLaVA-v1.5. #581](https://github.com/haotian-liu/LLaVA/commits/1ec53a4c247897d636743da6614a4dd68be0e472/llava/eval/model_vqa_qbench.py)\r\n> \r\n> Thank you!\r\n\r\nyou can check it here : https://github.com/haotian-liu/LLaVA/pull/581/files#diff-24aa88b077252259843630f0a32315e0c252f04dfd7ede7bcee31e1ffe4c883b\n</Comment>\n<Comment by X1AOX1A at 2024-06-24T07:36:26Z>\nThanks for your reply. Is it compatible with LLaVA with commit c121f04? I copied the [file](https://github.com/teowu/LLaVA/blob/3e89fc296a0fab559a2de1828c31f4874a2e0594/llava/eval/model_vqa_qbench.py) you mentioned, but I still encounter the same problem during evaluation:\r\n\r\nCommand: \r\n```\r\nCUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/eval/qbench.sh dev\r\n```\r\n\r\nLog:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/envs/feat_align/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/root/anaconda3/envs/feat_align/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/data/root/Documents/CODES/FeatAlign/llava/eval/model_vqa_qbench.py\", line 122, in <module>\r\n    eval_model(args)\r\n  File \"/data/root/Documents/CODES/FeatAlign/llava/eval/model_vqa_qbench.py\", line 99, in eval_model\r\n    n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\r\nRuntimeError: The size of tensor a (77) must match the size of tensor b (5) at non-singleton dimension 1\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1560,
    "state": "open",
    "created_by": "ggcr",
    "created_at": "2024-06-16T11:56:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1560</URL>\n\n<TITLE>[Question] Fine-tune only the projector?</TITLE>\n\n<BODY>### Question\n\nWould it be possible to fine-tune (or fully tune) only the projector? From what I've seen, this is currently not supported, was wondering if someone has already done this.</BODY>\n\n<COMMENTS>\n<Comment by 2U1 at 2024-06-19T22:33:16Z>\nThe pretrain stage only tunes the projector. I think you could use the pretrain bash file.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1559,
    "state": "open",
    "created_by": "Bikram9035",
    "created_at": "2024-06-14T06:57:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1559</URL>\n\n<TITLE>Realtime Image Description</TITLE>\n\n<BODY>### Question\n\nHello Team, \r\nKindly lemme know how to do realtime image or scene description. Like for example if i turn on my webcam or phone camera it must be able to describe taking in live video feed and give a text output of what it sees in Realtime. if that's not possible atleast leeme know about the Realtime vision model Because I will be integrating this model in my realtime language model project\r\nHope you understand my concern.\r\n\r\nThank You</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1558,
    "state": "open",
    "created_by": "l-show",
    "created_at": "2024-06-13T15:35:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1558</URL>\n\n<TITLE>[版本冲突]环境配置要求版本冲突</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:I encountered the following problem\r\n>>>[Usage] ImportError: cannot import name 'LlavaLlamaForCausalLM' from 'llava.model' #1101\r\nThere was a version compatibility issue when I tried the following\r\n```\r\npip uninstall  flash-attn\r\npip install -e \".[train]\"\r\npip install flash-attn --no-build-isolation --no-cache-dir\r\n```\r\n\r\n[Version conflict] The environment configuration requires a version conflict\r\n\r\n[The first try]-Command:\r\n```\r\npip install -e \".[train]\"\r\n```\r\n\r\n[The first try]-Log: \r\n```\r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\nvllm 0.4.3 requires tokenizers>=0.19.1, but you have tokenizers 0.15.1 which is incompatible.\r\nvllm 0.4.3 requires torch==2.3.0, but you have torch 2.1.2 which is incompatible.\r\nvllm 0.4.3 requires transformers>=4.40.0, but you have transformers 4.37.2 which is incompatible.\r\nvllm-flash-attn 2.5.8.post2 requires torch==2.3.0, but you have torch 2.1.2 which is incompatible.\r\nxformers 0.0.26.post1 requires torch==2.3.0, but you have torch 2.1.2 which is incompatible.\r\n```\r\n[Repair]-Command:\r\n```\r\npip install tokenizers==0.19.1\r\n```\r\n\r\n[Repair]-Log: \r\n```\r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\nllava 1.2.2.post1 requires tokenizers==0.15.1, but you have tokenizers 0.19.1 which is incompatible.\r\ntransformers 4.37.2 requires tokenizers<0.19,>=0.14, but you have tokenizers 0.19.1 which is incompatible.\r\nvllm 0.4.3 requires torch==2.3.0, but you have torch 2.1.2 which is incompatible.\r\nvllm 0.4.3 requires transformers>=4.40.0, but you have transformers 4.37.2 which is incompatible.\r\n```\r\n**issues**\r\n```\r\ntorch ==2.3.0 or 2.1.2?\r\nand tokenizers ? ..........\r\nEvery time I change the version, it's incompatible\r\n```\r\nIt doesn't seem to be working</BODY>\n\n<COMMENTS>\n<Comment by l-show at 2024-06-13T16:12:20Z>\nPackage                           Version             Editable project location\r\n--------------------------------- ------------------- -------------------------\r\nabsl-py                           1.4.0\r\naccelerate                        0.21.0\r\naiofiles                          22.1.0\r\naiohttp                           3.9.5\r\naiosignal                         1.3.1\r\naiosqlite                         0.18.0\r\naltair                            5.3.0\r\nannotated-types                   0.7.0\r\nanthropic                         0.28.0\r\nanyio                             3.6.2\r\nargon2-cffi                       21.3.0\r\nargon2-cffi-bindings              21.2.0\r\narrow                             1.2.3\r\nasttokens                         2.2.1\r\nasync-timeout                     4.0.3\r\nattrs                             22.2.0\r\nBabel                             2.12.1\r\nbackcall                          0.2.0\r\nbeautifulsoup4                    4.11.2\r\nbitsandbytes                      0.43.1\r\nbleach                            6.0.0\r\nbrotlipy                          0.7.0\r\ncachetools                        5.3.0\r\ncertifi                           2021.5.30\r\ncffi                              1.14.6\r\nchardet                           4.0.0\r\ncharset-normalizer                3.1.0\r\nclick                             8.1.7\r\ncloudpickle                       3.0.0\r\ncmake                             3.26.0\r\ncomm                              0.1.2\r\nconda                             4.10.3\r\nconda-package-handling            1.7.3\r\ncontourpy                         1.0.7\r\ncryptography                      3.4.7\r\ncycler                            0.11.0\r\ndebugpy                           1.6.6\r\ndecorator                         5.1.1\r\ndeepspeed                         0.12.6\r\ndefusedxml                        0.7.1\r\ndiskcache                         5.6.3\r\ndistro                            1.9.0\r\ndnspython                         2.6.1\r\ndocker-pycreds                    0.4.0\r\neinops                            0.6.1\r\neinops-exts                       0.0.4\r\nemail_validator                   2.1.1\r\nexecuting                         1.2.0\r\nfastapi                           0.111.0\r\nfastapi-cli                       0.0.4\r\nfastjsonschema                    2.16.3\r\nffmpy                             0.3.2\r\nfilelock                          3.14.0\r\nflash-attn                        2.5.9.post1\r\nfonttools                         4.39.0\r\nfqdn                              1.5.1\r\nfrozenlist                        1.4.1\r\nfsspec                            2024.6.0\r\ngitdb                             4.0.11\r\nGitPython                         3.1.43\r\ngoogle-auth                       2.16.2\r\ngoogle-auth-oauthlib              0.4.6\r\ngradio                            4.16.0\r\ngradio_client                     0.8.1\r\ngrpcio                            1.51.3\r\nh11                               0.14.0\r\nhf_transfer                       0.1.6\r\nhjson                             3.1.0\r\nhttpcore                          0.17.3\r\nhttptools                         0.6.1\r\nhttpx                             0.24.0\r\nhuggingface-hub                   0.23.3\r\nidna                              2.10\r\nimportlib_metadata                7.1.0\r\nimportlib-resources               5.12.0\r\ninteregular                       0.3.3\r\nipykernel                         6.21.3\r\nipython                           8.11.0\r\nipython-genutils                  0.2.0\r\nipywidgets                        8.0.4\r\nisoduration                       20.11.0\r\njedi                              0.18.2\r\nJinja2                            3.1.2\r\njiter                             0.4.1\r\njoblib                            1.4.2\r\njson5                             0.9.11\r\njsonpointer                       2.3\r\njsonschema                        4.17.3\r\njupyter_client                    8.0.3\r\njupyter_core                      5.2.0\r\njupyter-events                    0.6.3\r\njupyter_server                    2.4.0\r\njupyter_server_fileid             0.8.0\r\njupyter_server_terminals          0.4.4\r\njupyter_server_ydoc               0.6.1\r\njupyter-ydoc                      0.2.3\r\njupyterlab                        3.6.1\r\njupyterlab-language-pack-zh-CN    3.6.post0\r\njupyterlab-pygments               0.2.2\r\njupyterlab_server                 2.20.0\r\njupyterlab-widgets                3.0.5\r\nkiwisolver                        1.4.4\r\nlark                              1.1.9\r\nlfs                               0.2\r\nlit                               15.0.7\r\nlitellm                           1.40.7\r\nllava                             1.2.2.post1         /root/autodl-tmp/LLaVA\r\nllvmlite                          0.41.1\r\nlm-format-enforcer                0.10.1\r\nMarkdown                          3.4.1\r\nmarkdown-it-py                    3.0.0\r\nmarkdown2                         2.4.13\r\nMarkupSafe                        2.1.2\r\nmatplotlib                        3.7.1\r\nmatplotlib-inline                 0.1.6\r\nmdurl                             0.1.2\r\nmistune                           2.0.5\r\nmpmath                            1.3.0\r\nmsgpack                           1.0.8\r\nmultidict                         6.0.5\r\nnbclassic                         0.5.3\r\nnbclient                          0.7.2\r\nnbconvert                         7.2.10\r\nnbformat                          5.7.3\r\nnest-asyncio                      1.5.6\r\nnetworkx                          3.0\r\nninja                             1.11.1.1\r\nnotebook                          6.5.3\r\nnotebook_shim                     0.2.2\r\nnumba                             0.58.1\r\nnumpy                             1.24.2\r\nnvidia-cublas-cu12                12.1.3.1\r\nnvidia-cuda-cupti-cu12            12.1.105\r\nnvidia-cuda-nvrtc-cu12            12.1.105\r\nnvidia-cuda-runtime-cu12          12.1.105\r\nnvidia-cudnn-cu12                 8.9.2.26\r\nnvidia-cufft-cu12                 11.0.2.54\r\nnvidia-curand-cu12                10.3.2.106\r\nnvidia-cusolver-cu12              11.4.5.107\r\nnvidia-cusparse-cu12              12.1.0.106\r\nnvidia-ml-py                      12.555.43\r\nnvidia-nccl-cu12                  2.18.1\r\nnvidia-nvjitlink-cu12             12.5.40\r\nnvidia-nvtx-cu12                  12.1.105\r\noauthlib                          3.2.2\r\nopenai                            1.33.0\r\norjson                            3.10.3\r\noutlines                          0.0.34\r\npackaging                         23.0\r\npandas                            2.0.3\r\npandocfilters                     1.5.0\r\nparso                             0.8.3\r\npeft                              0.11.1\r\npexpect                           4.8.0\r\npickleshare                       0.7.5\r\nPillow                            9.4.0\r\npip                               24.0\r\npkgutil_resolve_name              1.3.10\r\nplatformdirs                      3.1.1\r\nplumbum                           1.8.3\r\nprometheus_client                 0.20.0\r\nprometheus-fastapi-instrumentator 7.0.0\r\nprompt-toolkit                    3.0.38\r\nprotobuf                          4.22.1\r\npsutil                            5.9.4\r\nptyprocess                        0.7.0\r\npure-eval                         0.2.2\r\npy-cpuinfo                        9.0.0\r\npyasn1                            0.4.8\r\npyasn1-modules                    0.2.8\r\npycosat                           0.6.3\r\npycparser                         2.20\r\npydantic                          2.7.3\r\npydantic_core                     2.18.4\r\npydub                             0.25.1\r\nPygments                          2.14.0\r\npynvml                            11.5.0\r\npyOpenSSL                         20.0.1\r\npyparsing                         3.0.9\r\npyrsistent                        0.19.3\r\nPySocks                           1.7.1\r\npython-dateutil                   2.8.2\r\npython-dotenv                     1.0.1\r\npython-json-logger                2.0.7\r\npython-multipart                  0.0.9\r\npytz                              2022.7.1\r\nPyYAML                            6.0\r\npyzmq                             25.0.1\r\nray                               2.10.0\r\nreferencing                       0.35.1\r\nregex                             2024.5.15\r\nrequests                          2.32.3\r\nrequests-oauthlib                 1.3.1\r\nrfc3339-validator                 0.1.4\r\nrfc3986-validator                 0.1.1\r\nrich                              13.7.1\r\nrpds-py                           0.18.1\r\nrpyc                              6.0.0\r\nrsa                               4.9\r\nruamel-yaml-conda                 0.15.100\r\nruff                              0.4.8\r\nsafetensors                       0.4.3\r\nscikit-learn                      1.2.2\r\nscipy                             1.10.1\r\nsemantic-version                  2.10.0\r\nSend2Trash                        1.8.0\r\nsentencepiece                     0.1.99\r\nsentry-sdk                        2.5.1\r\nsetproctitle                      1.3.3\r\nsetuptools                        52.0.0.post20210125\r\nsglang                            0.1.17\r\nshellingham                       1.5.4\r\nshortuuid                         1.0.13\r\nsix                               1.16.0\r\nsmmap                             5.0.1\r\nsniffio                           1.3.0\r\nsoupsieve                         2.4\r\nstack-data                        0.6.2\r\nstarlette                         0.37.2\r\nsupervisor                        4.2.5\r\nsvgwrite                          1.4.3\r\nsympy                             1.11.1\r\ntensorboard                       2.12.0\r\ntensorboard-data-server           0.7.0\r\ntensorboard-plugin-wit            1.8.1\r\nterminado                         0.17.1\r\nthreadpoolctl                     3.5.0\r\ntiktoken                          0.7.0\r\ntimm                              0.6.13\r\ntinycss2                          1.2.1\r\ntokenizers                        0.19.1\r\ntomli                             2.0.1\r\ntomlkit                           0.12.0\r\ntoolz                             0.12.1\r\ntorch                             2.1.2\r\ntorchvision                       0.16.2\r\ntornado                           6.2\r\ntqdm                              4.61.2\r\ntraitlets                         5.9.0\r\ntransformers                      4.37.2\r\ntriton                            2.1.0\r\ntyper                             0.12.3\r\ntyping_extensions                 4.12.2\r\ntzdata                            2024.1\r\nujson                             5.10.0\r\nuri-template                      1.2.0\r\nurllib3                           1.26.18\r\nuvicorn                           0.30.1\r\nuvloop                            0.19.0\r\nvllm                              0.4.3\r\nvllm-flash-attn                   2.5.8.post2\r\nwandb                             0.17.1\r\nwatchfiles                        0.22.0\r\nwavedrom                          2.0.3.post3\r\nwcwidth                           0.2.6\r\nwebcolors                         1.12\r\nwebencodings                      0.5.1\r\nwebsocket-client                  1.5.1\r\nwebsockets                        11.0.3\r\nWerkzeug                          2.2.3\r\nwheel                             0.36.2\r\nwidgetsnbextension                4.0.5\r\nxformers                          0.0.26.post1\r\ny-py                              0.5.9\r\nyarl                              1.9.4\r\nypy-websocket                     0.8.2\r\nzipp                              3.15.0\r\nzmq                               0.0.0\n</Comment>\n<Comment by l-show at 2024-06-13T16:17:03Z>\ncuda 只支持12.1吗？\n</Comment>\n<Comment by Yuxinyi-Qiyu at 2024-07-09T07:21:14Z>\n请问您解决了嘛？我也遇到了同样的问题，不匹配的版本\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1557,
    "state": "open",
    "created_by": "AngelAlita",
    "created_at": "2024-06-13T14:28:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1557</URL>\n\n<TITLE>[Question]About Full Parameter Finetune cost ？</TITLE>\n\n<BODY>### Question\n\nI have 4-3090（24G），how could i use full parameter finetune the model ?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1556,
    "state": "open",
    "created_by": "geweihgg",
    "created_at": "2024-06-12T12:28:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1556</URL>\n\n<TITLE>[Discussion] why are the results on post inconsistent with opencompass?</TITLE>\n\n<BODY>### Discussion\n\npost：https://llava-vl.github.io/blog/2024-01-30-llava-next/\r\nMMVet = 48.4\r\n![image](https://github.com/haotian-liu/LLaVA/assets/18585014/2ae93274-7e2b-4fbb-908e-48217ef2bc6c)\r\n\r\nopencompass：https://rank.opencompass.org.cn/leaderboard-multimodal/?m=REALTIME\r\nMMVet = 44.9\r\n![image](https://github.com/haotian-liu/LLaVA/assets/18585014/03970a42-84b8-418c-8a6e-047e2f34f305)\r\n\r\nIs there any difference such as model version, test strategy?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1554,
    "state": "open",
    "created_by": "ayushgupta9198",
    "created_at": "2024-06-11T11:40:21Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1554</URL>\n\n<TITLE>[Usage] Not able to fine tune the LLaVA model with  llava-v1.5-7b.</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue: Not able to fine tune the LLaVA model with  llava-v1.5-7b. \r\n\r\nalso I am sharing my arguments below here so when I am running the code it gives me error as \r\n\r\n\tsize mismatch for 2.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([0]).\r\n\tsize mismatch for 2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([0]).\r\n\r\nCommand:\r\n```\r\nthis are arguments:\r\n\r\n#fine tuning code\r\n\r\n!deepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path iuhaotian/llava-v1.5-7b \\\r\n    --version v1 \\\r\n    --data_path ./blip_laion_cc_sbu_558k_samples.json \\\r\n    --image_folder /teamspace/studios/this_studio/llava_train/LLaVA/playground/data/LLaVA-Pretrain/images \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter /teamspace/studios/this_studio/llava_train/LLaVA/checkpoint-huggingface/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 False \\\r\n    --fp16 True \\\r\n    --output_dir ./output/llava-v1.5-7b \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 1 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 1e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 128 \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to \"none\"\r\n```\r\nI have tried with my mm_projector.bin file also tried with hugging face file but still the same error\r\n\r\ncurrenlty I am using lighting AI studio with A10G GPU.\r\n\r\nif anyone know how to fix this please let me know.\r\n\r\nNote : Here I have kept the minimum numbers just to start with fine tunning so do not add anything about the number added in the arguments.\r\n\r\n![Screenshot from 2024-06-11 17-09-24](https://github.com/haotian-liu/LLaVA/assets/46589053/bd2f3c8c-dbcc-48be-a21e-f4913f0d8669)\r\n\r\nThanks.</BODY>\n\n<COMMENTS>\n<Comment by kartikey9254 at 2024-06-18T06:27:13Z>\nsame issue . have you found any potential solutions ?\n</Comment>\n<Comment by TheRoadQaQ at 2024-06-18T16:42:29Z>\nmodel_name_or_path can not be iuhaotian/llava-v1.5-7b, should be llama2 or vicuna.\n</Comment>\n<Comment by ayushgupta9198 at 2024-06-19T04:47:27Z>\nhi \r\n\r\nI have solved the issue by changing the names in arguments aling with that I have also made some changes in LLaVA.py file.\r\n\r\nnow code is working on both pretrain and fine tunning.\r\n\r\nThanks\n</Comment>\n<Comment by hvgupta at 2024-07-20T04:13:33Z>\n> in arguments aling\r\n\r\nHi, can you please show the changes you have made\n</Comment>\n<Comment by pear-blossoms at 2024-07-22T08:15:50Z>\n> > in arguments aling\r\n> \r\n> Hi, can you please show the changes you have made\r\nwell, if you try to us llava-v1.5-7b, you can just remove --pretrain_mm_mlp_adapter\r\nit already contains the adapter weights\n</Comment>\n<Comment by layne512 at 2024-09-12T03:07:16Z>\n@ayushgupta9198 I'm stoked for your successful work-around, but very puzzled why you would take the time to make the vague post claiming success but not share some details.  Generally git is a pretty solid community.  I'm just getting back into programming after a 15 year break, so I for one (and I expect many others) would really appreciate you sharing your work-around(s) on this one.  Cheers!  Thanks in advance as well!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1553,
    "state": "open",
    "created_by": "Ali-Forootani",
    "created_at": "2024-06-10T15:57:59Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1553</URL>\n\n<TITLE>[Usage] ValueError: The current `device_map` had weights offloaded to the disk.</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:20000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-7b\r\n```\r\n\r\nLog: \r\n```\r\nraise ValueError(\r\n2024-06-10 17:54:07 | ERROR | stderr | ValueError: The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1552,
    "state": "open",
    "created_by": "maxall41",
    "created_at": "2024-06-09T00:58:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1552</URL>\n\n<TITLE>[Usage] Merging LoRa weights into llava-13b fails with bizarre error</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\n\r\nI have fine-tuned `liuhaotian/llava-v1.5-13b` on an OCR task using LoRa. I am now trying to use this model for inference but when I try to merge the LoRa weights it throws a bizarre error both saying that the Llava configs (`LlavaConfig, LlavaMptConfig, LlavaMistralConfig`) are installed and simultaneously that they don't exist. I am using the latest version of the repo (Commit: c121f04) and the default conda environment, with the only difference being that i installed `protobuf` because it threw an error if it wasn't installed. I have been able to replicated this across multiple cloud machines.\r\n\r\n`transformers==4.37.2 and tokenizers==0.15.1`\r\n\r\nCommand:\r\n```\r\npython scripts/merge_lora_weights.py \\\r\n    --model-path ./checkpoint-200/ \\\r\n    --model-base 'liuhaotian/llava-v1.5-13b' \\\r\n    --save-model-path merge_model \r\n```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/shadeform/LLaVA/scripts/merge_lora_weights.py\", line 22, in <module>\r\n    merge_lora(args)\r\n  File \"/home/shadeform/LLaVA/scripts/merge_lora_weights.py\", line 8, in merge_lora\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, device_map='cpu')\r\n  File \"/home/shadeform/LLaVA/llava/model/builder.py\", line 128, in load_pretrained_model\r\n    model = AutoModelForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, **kwargs)\r\n  File \"/home/shadeform/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 569, in from_pretrained\r\n    raise ValueError(\r\nValueError: Unrecognized configuration class <class 'transformers.models.llava.configuration_llava.LlavaConfig'> for this kind of AutoModel: AutoModelForCausalLM.\r\nModel type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, ElectraConfig, ErnieConfig, FalconConfig, FuyuConfig, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, LlamaConfig, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MptConfig, MusicgenConfig, MvpConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, LlavaConfig, LlavaMptConfig, LlavaMistralConfig.\r\n```\r\nNotice that according to the error `LlavaConfig, LlavaMptConfig, LlavaMistralConfig` are installed.</BODY>\n\n<COMMENTS>\n<Comment by NandhaKishorM at 2024-06-21T04:55:12Z>\nadd llava in the folder name\n</Comment>\n<Comment by sumit-mahaseel at 2024-07-05T12:10:42Z>\nwhere exactly? @NandhaKishorM\n</Comment>\n<Comment by HenryJunW at 2024-07-31T20:53:59Z>\n@sumit-mahaseel  same issue, did you fix it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1551,
    "state": "open",
    "created_by": "rohithbojja",
    "created_at": "2024-06-08T09:33:29Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1551</URL>\n\n<TITLE>[Discussion] Request for Guidance on Stage Two: Converting LLaVA-V1.6 into 4-bit GGUF Format (PAPER)</TITLE>\n\n<BODY>### Discussion\n\n### LLaVA-Med V1.6: Training a Large Language-and-Vision Assistant for Biomedicine in Two and Half Hours\r\n\r\n#### Abstract\r\n\r\nLarge Language Models (LLMs) have revolutionized natural language processing, but biomedical applications necessitate comprehension of both textual and visual data. General-purpose vision-language models struggle with biomedical domain-specific knowledge. Our novel approach, LLaVA-Med V1.6, significantly outperforms previous models, including Microsoft's Med-LLaVA, in visual question answering accuracy and precision. Achieved through visual instruction tuning (VIT) applied to LLaVA V1.6 using a substantial portion of the PMC-VQA dataset, the fine-tuning process on a four-A6000 GPU setup takes only 2.5 hours. LLaVA-Med V1.6 also supports any-resolution images and exhibits enhanced visual reasoning and Optical Character Recognition (OCR) capabilities.\r\n\r\n#### 1. Introduction\r\n\r\nThe release of GPT-4 has spurred intensive research into multimodal language models (MLLMs). While these models showcase impressive capabilities in general contexts, their effectiveness in biomedical scenarios is limited. Existing models may falter in addressing biomedical inquiries, risking inaccurate responses. LLaVA-Med V1.6, built upon the LLaVA V1.6 architecture with Mistral as the base model, leverages GPT-4 to generate diverse biomedical multimodal instruction-following data by synthesizing image-text pairs from the PMC-15M dataset. Utilizing novel curriculum learning techniques, LLaVA-Med V1.6 achieves remarkable performance enhancements.\r\n\r\n#### 2. Model Architecture\r\n\r\nLLaVA-Med V1.6 employs a minimalist architectural design, similar to prefix tuning of language models. A novel trainable module seamlessly integrates a static image encoder with a dynamic language model (LM). The model trains a straightforward fully-connected projection layer on a modest dataset comprising 14k image-text pairs. Leveraging GPT-4 to autonomously curate biomedical task instructions from PubMed Central's extensive data repository, LLaVA-Med V1.6 achieves performance parity with state-of-the-art prefix tuning LMs.\r\n\r\n#### 3. Training and Dataset\r\n\r\nThe training process involved visual instruction tuning (VIT) applied to the latest version of LLaVA (v1.6) using a substantial portion of the PMC-VQA dataset. This fine-tuning was implemented on a four-A6000 GPU setup and took only 2.5 hours. The model increases input image resolution to capture finer visual details critical in the medical domain, supporting any-resolution images, including resolutions up to 672x672, 336x1344, and 1344x336.\r\n\r\n#### 4. Model Quantization and Format Conversion\r\n\r\nTo optimize the model for efficient deployment and inference, we converted the original 32-bit model to FP16 (16-bit), reducing the model size to 14 GB. Further, we quantized the FP16 model to 4 bits using the Q_4_K_M method from llama.cpp, reducing the model size to 5 GB. This quantized model was then converted to GGUF and subsequently to the Ollama format, facilitating deployment across multiple devices.\r\n\r\n**Benefits of Quantization and Format Conversion:**\r\n- **Reduced Model Size**: From 14 GB to 5 GB.\r\n- **Faster Inference**: Increased speed and reduced VRAM requirements.\r\n- **Multi-Device Compatibility**: GGUF and Ollama formats enable efficient deployment on diverse devices.\r\n- **Energy Efficiency**: Lower computational demands reduce energy consumption.\r\n\r\n**Experimental Results:**\r\n- **Model Size**: Reduced from 14 GB to 5 GB.\r\n- **Inference Speed**: Improved with less VRAM usage.\r\n- **Accuracy**: Maintained within 1% of the original model's accuracy.\r\n\r\n#### 5. Evaluation\r\n\r\nLLaVA-Med V1.6 was evaluated on three standard biomedical visual question answering datasets. The model demonstrated superior visual conversation capabilities and enhanced OCR and visual reasoning abilities. It outperformed previous state-of-the-art models on specific metrics across these datasets, including Microsoft's Med-LLaVA.\r\n\r\n#### 6. Conclusion\r\n\r\nLLaVA-Med V1.6 represents a significant advancement in biomedical conversational AI, offering enhanced performance in visual question answering and improved deployment efficiency. The quantization and format conversion processes ensure that the model is both high-performing and versatile, suitable for a wide range of medical applications. Our contributions pave the way for more sophisticated biomedical AI applications, enabling more accurate and informative responses to biomedical inquiries.\r\n\r\n#### Query for Guidance\r\n\r\n\r\nI am currently writing a paper on LLaVA-Med V1.6, which we have fine-tuned on medical images. The paper's first stage, detailing the fine-tuning process, is complete. I am now focusing on stage two: converting our fine-tuned model into a 4-bit GGUF format.\r\n\r\nCould you please advise on what key points to include in this section? Additionally, could you suggest any relevant references or previous papers that discuss similar quantization processes?\r\n\r\nThank you for your assistance.\r\n\r\nBest regards,\r\n\r\nRohith</BODY>\n\n<COMMENTS>\n<Comment by Wangmmstar at 2024-07-22T18:17:18Z>\nHi Rohith, have you got any updates on converting the fine-tuned model into GGUF format? As there are two GGUF models needed for LLAVA to run in LLAMA.CPP. Could you please provide some resources or suggestions if you have any? Thank you very much.\n</Comment>\n<Comment by rohithbojja at 2024-07-30T11:14:20Z>\n@Wangmmstar \r\nyes we will be having two gguf files . One main gguf and other mm_projector gguf\r\nu can refer to this:\r\nhttps://github.com/ggerganov/llama.cpp/tree/master/examples/llava\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1550,
    "state": "open",
    "created_by": "xiaxiaxiatengxi",
    "created_at": "2024-06-08T05:06:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1550</URL>\n\n<TITLE>Pip install got an error So, do you install the llava in a new docker?</TITLE>\n\n<BODY>### Question\n\n\r\nspacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\r\nweasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\r\nWhen I install typer==0.4.2, I got error \r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\nfastapi-cli 0.0.4 requires typer>=0.12.3, but you have typer 0.4.2 which is incompatible.\r\ngradio 4.16.0 requires typer[all]<1.0,>=0.9, but you have typer 0.4.2 which is incompatible.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1549,
    "state": "open",
    "created_by": "fangyizhu",
    "created_at": "2024-06-07T21:41:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1549</URL>\n\n<TITLE>[Question] Is there a way to make llava-v1.6-mistral-7b-hf always return json?</TITLE>\n\n<BODY>### Question\n\nHello, Mistral chat itself has a [json mode](https://docs.mistral.ai/capabilities/json_mode/). I wonder if I can turn on json mode for llava-v1.6-mistral-7b-hf?\r\n\r\nAdditionally, is there a way to enforce a certain json format so Llava-Next returns results in the same json format every time?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1548,
    "state": "open",
    "created_by": "nlpkiddo-2001",
    "created_at": "2024-06-07T15:56:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1548</URL>\n\n<TITLE>[Usage] Llava-Gemma Pretraining + Fine tuning Usage issue and missing Fine tuned projector.bin</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI first pretrained the projector using Clip + Gemma Model and then FIne tuned the Gemma and Projector, but no matter what It is giving in correct outputs, and the loss is revolving around 1-2 in pretraining for projector and 0.4 - 0.7 in fine tuning. I tried without Lora. \r\n\r\n\r\n**Screenshots:**\r\n<img width=\"1410\" alt=\"Screenshot 2024-06-07 at 9 23 52 PM\" src=\"https://github.com/haotian-liu/LLaVA/assets/112948342/43f7f222-9d0c-495c-98ac-8f48acb233e1\">\r\n\r\n\r\nKindly assist me. I have a similar setup for Gemma as like in this PR . \r\nhttps://github.com/haotian-liu/LLaVA/pull/1247\r\n\r\n\r\n**Screenshot of fine tuning from wandb**\r\n<img width=\"1445\" alt=\"Screenshot 2024-06-07 at 9 25 48 PM\" src=\"https://github.com/haotian-liu/LLaVA/assets/112948342/94262ff7-5db8-4923-b82f-96314e58691a\"></BODY>\n\n<COMMENTS>\n<Comment by shan23chen at 2024-09-08T04:31:59Z>\nis this first gen gemma or second?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1547,
    "state": "open",
    "created_by": "FightingFighting",
    "created_at": "2024-06-06T21:25:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1547</URL>\n\n<TITLE>[Question] Evaluate on OKVQA</TITLE>\n\n<BODY>### Question\n\nHi, do you have any evaluation results on OK-VQA or AOK-VQA?</BODY>\n\n<COMMENTS>\n<Comment by xandery-geek at 2025-01-13T14:41:13Z>\n@FightingFighting Hi, have you solved this issue? I am also interested in the results of OK-VQA.\n</Comment>\n<Comment by FightingFighting at 2025-01-14T11:35:55Z>\n@xandery-geek No, I didnot, sorry.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1546,
    "state": "open",
    "created_by": "zy1996829",
    "created_at": "2024-06-06T09:51:21Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1546</URL>\n\n<TITLE>[Question] No matter what I ask, my model always gives the same answer.</TITLE>\n\n<BODY>### Question\r\n\r\nI used LLAVA-V1.5-13b to fine-tune my own dataset, which consists of around 20,000 images. However, these 20,000 images are relatively homogeneous, with only two categories, such as driving while using a phone and driving while drowsy. Then, I used GBT to generate dialogue labels for each image, for example: Human: \"Is there any violation of driving behavior in the image?\" GBT: \"Yes, there is a violation. The driver is using a phone while driving.\" After one epoch of fine-tuning, the loss decreased to around 0.3. During testing, the model can recognize behaviors like using a phone or being drowsy. However, regardless of the question I ask about a specific image, the model always gives the same answer.\r\nI asked if there is any violation in this image：\r\n\r\n![CGX01BASXJ27DAAEEB39_39 mp4_3_110](https://github.com/haotian-liu/LLaVA/assets/26729385/1f6bb81b-07c1-40fa-8924-1a8480ca5c84)\r\nThe answer:\r\n![WXWorkCapture_17176457449106](https://github.com/haotian-liu/LLaVA/assets/26729385/0d46a558-9ec5-4145-b64c-e2fee28c0e84)\r\n\r\nHowerer,when I say Hello,the result is the same answer:\r\n![WXWorkCapture_171766454522](https://github.com/haotian-liu/LLaVA/assets/26729385/401ca0ee-54d2-4ac4-a84c-d9c8f817c7f3)\r\nThis is my train config:\r\ndeepspeed /home/zhouyan/LLaVA-1.2.2/llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path /home/zhouyan/LLaVA-main/checkpoints/llava-v1.5-13b \\\r\n    --version v1 \\\r\n    --data_path /home/zhouyan/LLaVA-main/data/driver_action.json \\\r\n    --image_folder /home/zhouyan/cabin_oms_gt_frames/ \\\r\n    --vision_tower /home/zhouyan/LLaVA-main/openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-13b-task_finetune \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n\r\nmy train loss curve：\r\n![W B Chart 6_6_2024, 5_53_15 PM](https://github.com/haotian-liu/LLaVA/assets/26729385/192c8a96-a470-4722-9235-c5c7f55190d8)</BODY>\n\n<COMMENTS>\n<Comment by nlpkiddo-2001 at 2024-06-07T11:26:54Z>\nHi, I am also exactly having the same issue and my loss hovers around 0.5 - 0.7. I experienced in both pre training of projector and fine tuning of gemma decoder and projector.\r\n\r\n\r\nAny updates??\n</Comment>\n<Comment by PoopBear1 at 2024-06-26T06:52:05Z>\nI have the same issue, yet I did not fine-tune any thing. I directly use the pre-train checkpoints #1578.\n</Comment>\n<Comment by hrishikesh-st at 2024-07-01T17:10:47Z>\nHi, I am facing the exact same issue. The model seems to overfit on the dataset. Within first 20% of the first epoch the training seems the loss seems to be stable around 0.5-0.7. Any update will be really appreciated. I am using the LLAVA-V1.5-7b model.\n</Comment>\n<Comment by nlpkiddo-2001 at 2024-07-03T06:31:15Z>\n@hrishikesh-st could you share me your code to load and infer the models?  I rectified this issue, by not giving base model param while infering\n</Comment>\n<Comment by hrishikesh-st at 2024-07-03T12:35:13Z>\n@nlpkiddo-2001 I am using the Gradio Web UI stated in the README for loading and inferring the model. Can you please eloborate more on \"not giving base model param while infering\"?\r\nThanks\n</Comment>\n<Comment by JurijsNazarovs at 2025-01-02T16:22:12Z>\nAny updates?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1545,
    "state": "open",
    "created_by": "YoungjaeDev",
    "created_at": "2024-06-05T08:15:26Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1545</URL>\n\n<TITLE>[Question]  Clarification on the Threshold for \"Sufficient\" Task-Specific Data for Full-Model Finetuning</TITLE>\n\n<BODY>### Question\n\nHi,\r\n\r\nI have a question regarding the finetuning instructions provided in the documentation. The following statement is mentioned:\r\n\r\n```\r\nCommand\r\nIf you have a limited task-specific data, we recommend finetuning from LLaVA checkpoints with LoRA following this script.\r\n\r\nIf the amount of the task-specific data is sufficient, you can also finetune from LLaVA checkpoints with full-model finetuning following this script.\r\n\r\nYou may need to adjust the hyperparameters to fit each specific dataset and your hardware constraint.\r\n```\r\n\r\nI am curious about the threshold for what constitutes \"sufficient\" task-specific data. This specific criterion does not seem to be detailed in the paper either.\r\n\r\nCould you please provide more clarity on the amount of data considered sufficient for opting for full-model finetuning?\r\n\r\nThank you for your assistance.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1544,
    "state": "open",
    "created_by": "qhz991029",
    "created_at": "2024-06-04T16:20:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1544</URL>\n\n<TITLE>How to fine-tune llava-v1.6-mistral-7b on GQA dataset</TITLE>\n\n<BODY>### Question\r\n\r\nThank you for your great work!\r\n\r\nI am trying to fine-tune llava-v1.6-mistral-7b on the provided GQA dataset, using the script `finetune_task_lora.sh`. However, the loss dosen't decrease and test result on GQA is worse. I wonder how should I fine-tune llava-v1.6 models?\r\n\r\nThis is the modified script:\r\n```\r\n#!/bin/bash\r\n\r\ndeepspeed --include \"node-0:0,1,2,3\" llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --model_name_or_path liuhaotian/llava-v1.5-7b \\\r\n    --version mistral_instruct \\\r\n    --data_path /SOME/PATH/TO/LLaVA/playground/gqa.json \\\r\n    --data_length 10000 \\\r\n    --image_folder ./playground/data \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_patch_merge_type spatial_unpad \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio anyres \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b-gqa-official-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 4096 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 8 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nThis the modified part in `train.py`:\r\n```\r\nmodel = LlavaMistralForCausalLM.from_pretrained(\r\n                model_args.model_name_or_path,\r\n                cache_dir=training_args.cache_dir,\r\n                attn_implementation=attn_implementation,\r\n                torch_dtype=(torch.bfloat16 if training_args.bf16 else None),\r\n                **bnb_model_from_pretrained_args\r\n            )\r\ntokenizer = transformers.AutoTokenizer.from_pretrained(\r\n            model_args.model_name_or_path,\r\n            cache_dir=training_args.cache_dir,\r\n            model_max_length=training_args.model_max_length,\r\n            padding_side=\"left\"\r\n        )\r\n```\r\n\r\nLooking forward to reply!</BODY>\n\n<COMMENTS>\n<Comment by NicoZenith at 2024-06-05T13:50:38Z>\nThere is not training script for 1.6 unfortunately...\n</Comment>\n<Comment by YuyangYe at 2024-06-09T05:54:05Z>\n> There is not training script for 1.6 unfortunately...\r\n\r\nHello, I was wondering if there is a current training script available for version 1.6? Thank you!\n</Comment>\n<Comment by NicoZenith at 2024-06-20T16:19:51Z>\nwhat do you guys think of this ? \r\nhttps://github.com/NielsRogge/Transformers-Tutorials/blob/master/LLaVa/Fine_tune_LLaVa_on_a_custom_dataset_(with_PyTorch_Lightning).ipynb\r\n\r\nBy replacing llava by lava-next (processor and model)\n</Comment>\n<Comment by dacian7 at 2024-06-28T04:25:29Z>\n> > There is not training script for 1.6 unfortunately...\r\n> \r\n> Hello, I was wondering if there is a current training script available for version 1.6? Thank you!\r\n\r\nHi @YuyangYe , have you found the scripts for that? Thanks!\n</Comment>\n<Comment by sweetning0809 at 2024-07-02T10:03:47Z>\n> There is not training script for 1.6 unfortunately...\r\n\r\nhave you find any progress on this？\n</Comment>\n<Comment by sweetning0809 at 2024-07-02T10:31:48Z>\nthere are some works on mac https://github.com/Blaizzy/mlx-vlm/pull/43\n</Comment>\n<Comment by sweetning0809 at 2024-07-02T10:32:22Z>\n> what do you guys think of this ? https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LLaVa/Fine_tune_LLaVa_on_a_custom_dataset_(with_PyTorch_Lightning).ipynb\r\n> \r\n> By replacing llava by lava-next (processor and model)\r\n\r\nmaybe is the right way\n</Comment>\n<Comment by NicoZenith at 2024-07-02T13:27:24Z>\n> > There is not training script for 1.6 unfortunately...\r\n> \r\n> have you find any progress on this？\r\n\r\nYes, the owner of this repo answered on another issue:\r\nhttps://github.com/chuangchuangtan/LLaVA-NeXT-Image-Llama3-Lora\r\n\r\nThe fine-tuning does work with llama3! I suggest we move to his repo if we have new issues.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1543,
    "state": "open",
    "created_by": "NavidRajabi",
    "created_at": "2024-06-04T15:49:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1543</URL>\n\n<TITLE>[Usage] Both demo links are down! Would you please check them out?</TITLE>\n\n<BODY>### Describe the issue\n\nFollowing links return the Argo Tunnel Error 1033:\r\n\r\n- https://llava-next.lmms-lab.com/\r\n- https://llava.hliu.cc/</BODY>\n\n<COMMENTS>\n<Comment by RussRobin at 2024-06-09T06:55:14Z>\nAlso waiting for the demo to reopen.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1542,
    "state": "open",
    "created_by": "supech",
    "created_at": "2024-06-04T15:16:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1542</URL>\n\n<TITLE>[Usage] Merging LoRA weights using the script \"LLaVA/scripts/merge_lora_weights.py\" results in a model size about two times larger than the original Llama2 model.</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nI used the training script about 1 month ago, and it did not become two times larger after merge lora weight.\r\nI was back to this project last week, and when I used the script \"LLaVA/scripts/merge_lora_weights.py\" again, it resulted in a model size two times larger than the original Llama2 model.\r\nThe larger problem didn't show up the first time I tried it last month.\r\n\r\nI only changed \"load_pretrained_model ...... device_map='cpu'\" to load_pretrained_model ...... device_map='auto') in merge_lora_weights.py.\r\nIt is because when I use \"device_map='cpu'\", a bug occurs.\r\n\"\r\n  File \"/home/OCR/anaconda3/envs/llava2/lib/python3.10/site-packages/peft/tuners/lora/bnb.py\", line 395, in get_delta_weight\r\n    self.lora_B[adapter].weight @ self.lora_A[adapter].weight,\r\nRuntimeError: \"addmm_impl_cpu_\" not implemented for 'Half'\r\n\"\r\nIt work after I used \"device_map='auto'\",but model size two times larger than the original Llama2 model.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1541,
    "state": "closed",
    "created_by": "lmcjrrg",
    "created_at": "2024-06-04T10:59:21Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1541</URL>\n\n<TITLE>[Usage] When running Example Code in colab, CuDA out of memory is reported</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI am using the L4 graphics card to run the Example Code in Quick Start With HuggingFace on colab, but it reports a memory overflow. Is the L40 graphics card not enough to run the demo? Or is there something wrong? Please help me, thank you.\r\n\r\nCommand:\r\n```\r\n1.cd /content/drive/MyDrive/LLaVA\r\n2.pip install accelerate\r\n3.!python3 run_demo.py\r\n```\r\n\r\nLog: \r\n```\r\n2024-06-04 10:48:08.571813: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-06-04 10:48:08.623481: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-06-04 10:48:08.623526: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-06-04 10:48:08.625334: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-06-04 10:48:08.633478: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-06-04 10:48:09.833364: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\ntokenizer_config.json: 100% 749/749 [00:00<00:00, 5.93MB/s]\r\ntokenizer.model: 100% 500k/500k [00:00<00:00, 13.1MB/s]\r\nspecial_tokens_map.json: 100% 438/438 [00:00<00:00, 3.84MB/s]\r\nconfig.json: 100% 1.16k/1.16k [00:00<00:00, 10.2MB/s]\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\npytorch_model.bin.index.json: 100% 27.1k/27.1k [00:00<00:00, 44.9MB/s]\r\nDownloading shards:   0% 0/2 [00:00<?, ?it/s]\r\npytorch_model-00001-of-00002.bin:   0% 0.00/9.98G [00:00<?, ?B/s]\r\npytorch_model-00001-of-00002.bin:   0% 21.0M/9.98G [00:00<00:50, 196MB/s]\r\npytorch_model-00001-of-00002.bin:   1% 52.4M/9.98G [00:00<00:45, 218MB/s]\r\npytorch_model-00001-of-00002.bin:   1% 83.9M/9.98G [00:00<00:43, 229MB/s]\r\npytorch_model-00001-of-00002.bin:   1% 115M/9.98G [00:00<00:42, 233MB/s] \r\npytorch_model-00001-of-00002.bin:   1% 147M/9.98G [00:00<00:41, 235MB/s]\r\npytorch_model-00001-of-00002.bin:   2% 178M/9.98G [00:00<00:41, 235MB/s]\r\npytorch_model-00001-of-00002.bin:   2% 210M/9.98G [00:00<00:41, 237MB/s]\r\npytorch_model-00001-of-00002.bin:   2% 241M/9.98G [00:01<00:40, 238MB/s]\r\npytorch_model-00001-of-00002.bin:   3% 273M/9.98G [00:01<00:40, 239MB/s]\r\npytorch_model-00001-of-00002.bin:   3% 304M/9.98G [00:01<00:40, 240MB/s]\r\npytorch_model-00001-of-00002.bin:   3% 336M/9.98G [00:01<00:40, 240MB/s]\r\npytorch_model-00001-of-00002.bin:   4% 367M/9.98G [00:01<00:39, 240MB/s]\r\npytorch_model-00001-of-00002.bin:   4% 398M/9.98G [00:01<00:39, 241MB/s]\r\npytorch_model-00001-of-00002.bin:   4% 430M/9.98G [00:01<00:39, 241MB/s]\r\npytorch_model-00001-of-00002.bin:   5% 461M/9.98G [00:01<00:39, 241MB/s]\r\npytorch_model-00001-of-00002.bin:   5% 493M/9.98G [00:02<00:39, 241MB/s]\r\npytorch_model-00001-of-00002.bin:   5% 524M/9.98G [00:02<00:39, 241MB/s]\r\npytorch_model-00001-of-00002.bin:   6% 556M/9.98G [00:02<00:39, 240MB/s]\r\npytorch_model-00001-of-00002.bin:   6% 587M/9.98G [00:02<00:39, 236MB/s]\r\npytorch_model-00001-of-00002.bin:   6% 619M/9.98G [00:02<00:39, 235MB/s]\r\npytorch_model-00001-of-00002.bin:   7% 650M/9.98G [00:02<00:40, 233MB/s]\r\npytorch_model-00001-of-00002.bin:   7% 682M/9.98G [00:02<00:40, 231MB/s]\r\npytorch_model-00001-of-00002.bin:   7% 713M/9.98G [00:03<00:40, 228MB/s]\r\npytorch_model-00001-of-00002.bin:   7% 744M/9.98G [00:03<00:40, 228MB/s]\r\npytorch_model-00001-of-00002.bin:   8% 776M/9.98G [00:03<00:40, 228MB/s]\r\npytorch_model-00001-of-00002.bin:   8% 807M/9.98G [00:03<00:40, 227MB/s]\r\npytorch_model-00001-of-00002.bin:   8% 839M/9.98G [00:03<00:40, 224MB/s]\r\npytorch_model-00001-of-00002.bin:   9% 870M/9.98G [00:03<00:40, 225MB/s]\r\npytorch_model-00001-of-00002.bin:   9% 902M/9.98G [00:03<00:40, 225MB/s]\r\npytorch_model-00001-of-00002.bin:   9% 933M/9.98G [00:04<00:40, 225MB/s]\r\npytorch_model-00001-of-00002.bin:  10% 965M/9.98G [00:04<00:40, 225MB/s]\r\npytorch_model-00001-of-00002.bin:  10% 996M/9.98G [00:04<00:39, 227MB/s]\r\npytorch_model-00001-of-00002.bin:  10% 1.03G/9.98G [00:04<00:39, 226MB/s]\r\npytorch_model-00001-of-00002.bin:  11% 1.06G/9.98G [00:04<00:39, 226MB/s]\r\npytorch_model-00001-of-00002.bin:  11% 1.09G/9.98G [00:04<00:38, 229MB/s]\r\npytorch_model-00001-of-00002.bin:  11% 1.12G/9.98G [00:04<00:38, 231MB/s]\r\npytorch_model-00001-of-00002.bin:  12% 1.15G/9.98G [00:04<00:37, 233MB/s]\r\npytorch_model-00001-of-00002.bin:  12% 1.18G/9.98G [00:05<00:37, 234MB/s]\r\npytorch_model-00001-of-00002.bin:  12% 1.22G/9.98G [00:05<00:37, 235MB/s]\r\npytorch_model-00001-of-00002.bin:  13% 1.25G/9.98G [00:05<00:37, 235MB/s]\r\npytorch_model-00001-of-00002.bin:  13% 1.28G/9.98G [00:05<00:36, 236MB/s]\r\npytorch_model-00001-of-00002.bin:  13% 1.31G/9.98G [00:05<00:36, 236MB/s]\r\npytorch_model-00001-of-00002.bin:  13% 1.34G/9.98G [00:05<00:36, 236MB/s]\r\npytorch_model-00001-of-00002.bin:  14% 1.37G/9.98G [00:05<00:36, 237MB/s]\r\npytorch_model-00001-of-00002.bin:  14% 1.41G/9.98G [00:06<00:36, 236MB/s]\r\npytorch_model-00001-of-00002.bin:  14% 1.44G/9.98G [00:06<00:36, 237MB/s]\r\npytorch_model-00001-of-00002.bin:  15% 1.47G/9.98G [00:06<00:35, 237MB/s]\r\npytorch_model-00001-of-00002.bin:  15% 1.50G/9.98G [00:06<00:37, 229MB/s]\r\npytorch_model-00001-of-00002.bin:  15% 1.53G/9.98G [00:06<00:36, 231MB/s]\r\npytorch_model-00001-of-00002.bin:  16% 1.56G/9.98G [00:06<00:36, 233MB/s]\r\npytorch_model-00001-of-00002.bin:  16% 1.59G/9.98G [00:06<00:35, 235MB/s]\r\npytorch_model-00001-of-00002.bin:  16% 1.63G/9.98G [00:06<00:35, 234MB/s]\r\npytorch_model-00001-of-00002.bin:  17% 1.66G/9.98G [00:07<00:35, 232MB/s]\r\npytorch_model-00001-of-00002.bin:  17% 1.69G/9.98G [00:07<00:35, 233MB/s]\r\npytorch_model-00001-of-00002.bin:  17% 1.72G/9.98G [00:07<00:36, 226MB/s]\r\npytorch_model-00001-of-00002.bin:  18% 1.75G/9.98G [00:07<00:36, 225MB/s]\r\npytorch_model-00001-of-00002.bin:  18% 1.78G/9.98G [00:07<00:36, 226MB/s]\r\npytorch_model-00001-of-00002.bin:  18% 1.81G/9.98G [00:07<00:36, 226MB/s]\r\npytorch_model-00001-of-00002.bin:  18% 1.85G/9.98G [00:07<00:36, 225MB/s]\r\npytorch_model-00001-of-00002.bin:  19% 1.88G/9.98G [00:08<00:36, 223MB/s]\r\npytorch_model-00001-of-00002.bin:  19% 1.91G/9.98G [00:08<00:36, 224MB/s]\r\npytorch_model-00001-of-00002.bin:  19% 1.94G/9.98G [00:08<00:35, 224MB/s]\r\npytorch_model-00001-of-00002.bin:  20% 1.97G/9.98G [00:08<00:35, 225MB/s]\r\npytorch_model-00001-of-00002.bin:  20% 2.00G/9.98G [00:08<00:35, 223MB/s]\r\npytorch_model-00001-of-00002.bin:  20% 2.03G/9.98G [00:08<00:35, 221MB/s]\r\npytorch_model-00001-of-00002.bin:  21% 2.07G/9.98G [00:08<00:35, 220MB/s]\r\npytorch_model-00001-of-00002.bin:  21% 2.10G/9.98G [00:09<00:35, 222MB/s]\r\npytorch_model-00001-of-00002.bin:  21% 2.13G/9.98G [00:09<00:34, 226MB/s]\r\npytorch_model-00001-of-00002.bin:  22% 2.16G/9.98G [00:09<00:34, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  22% 2.19G/9.98G [00:09<00:34, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  22% 2.22G/9.98G [00:09<00:33, 231MB/s]\r\npytorch_model-00001-of-00002.bin:  23% 2.25G/9.98G [00:09<00:33, 231MB/s]\r\npytorch_model-00001-of-00002.bin:  23% 2.29G/9.98G [00:09<00:32, 233MB/s]\r\npytorch_model-00001-of-00002.bin:  23% 2.32G/9.98G [00:10<00:32, 232MB/s]\r\npytorch_model-00001-of-00002.bin:  24% 2.35G/9.98G [00:10<00:32, 234MB/s]\r\npytorch_model-00001-of-00002.bin:  24% 2.38G/9.98G [00:10<00:32, 235MB/s]\r\npytorch_model-00001-of-00002.bin:  24% 2.41G/9.98G [00:10<00:32, 235MB/s]\r\npytorch_model-00001-of-00002.bin:  24% 2.44G/9.98G [00:10<00:31, 236MB/s]\r\npytorch_model-00001-of-00002.bin:  25% 2.47G/9.98G [00:10<00:31, 237MB/s]\r\npytorch_model-00001-of-00002.bin:  25% 2.51G/9.98G [00:10<00:31, 237MB/s]\r\npytorch_model-00001-of-00002.bin:  25% 2.54G/9.98G [00:10<00:31, 236MB/s]\r\npytorch_model-00001-of-00002.bin:  26% 2.57G/9.98G [00:11<00:31, 237MB/s]\r\npytorch_model-00001-of-00002.bin:  26% 2.60G/9.98G [00:11<00:30, 238MB/s]\r\npytorch_model-00001-of-00002.bin:  26% 2.63G/9.98G [00:11<00:30, 238MB/s]\r\npytorch_model-00001-of-00002.bin:  27% 2.66G/9.98G [00:11<00:31, 233MB/s]\r\npytorch_model-00001-of-00002.bin:  27% 2.69G/9.98G [00:11<00:31, 232MB/s]\r\npytorch_model-00001-of-00002.bin:  27% 2.73G/9.98G [00:11<00:31, 231MB/s]\r\npytorch_model-00001-of-00002.bin:  28% 2.76G/9.98G [00:11<00:31, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  28% 2.79G/9.98G [00:12<00:46, 156MB/s]\r\npytorch_model-00001-of-00002.bin:  28% 2.82G/9.98G [00:12<00:41, 172MB/s]\r\npytorch_model-00001-of-00002.bin:  29% 2.85G/9.98G [00:12<00:38, 186MB/s]\r\npytorch_model-00001-of-00002.bin:  29% 2.88G/9.98G [00:12<00:36, 196MB/s]\r\npytorch_model-00001-of-00002.bin:  29% 2.92G/9.98G [00:12<00:34, 202MB/s]\r\npytorch_model-00001-of-00002.bin:  30% 2.95G/9.98G [00:12<00:33, 209MB/s]\r\npytorch_model-00001-of-00002.bin:  30% 2.98G/9.98G [00:13<00:32, 214MB/s]\r\npytorch_model-00001-of-00002.bin:  30% 3.01G/9.98G [00:13<00:31, 220MB/s]\r\npytorch_model-00001-of-00002.bin:  30% 3.04G/9.98G [00:13<00:31, 224MB/s]\r\npytorch_model-00001-of-00002.bin:  31% 3.07G/9.98G [00:13<00:30, 227MB/s]\r\npytorch_model-00001-of-00002.bin:  31% 3.10G/9.98G [00:13<00:30, 229MB/s]\r\npytorch_model-00001-of-00002.bin:  31% 3.14G/9.98G [00:13<00:29, 231MB/s]\r\npytorch_model-00001-of-00002.bin:  32% 3.17G/9.98G [00:13<00:29, 233MB/s]\r\npytorch_model-00001-of-00002.bin:  32% 3.20G/9.98G [00:14<00:28, 235MB/s]\r\npytorch_model-00001-of-00002.bin:  32% 3.23G/9.98G [00:14<00:28, 236MB/s]\r\npytorch_model-00001-of-00002.bin:  33% 3.26G/9.98G [00:14<00:28, 238MB/s]\r\npytorch_model-00001-of-00002.bin:  33% 3.29G/9.98G [00:14<00:28, 238MB/s]\r\npytorch_model-00001-of-00002.bin:  33% 3.32G/9.98G [00:14<00:27, 238MB/s]\r\npytorch_model-00001-of-00002.bin:  34% 3.36G/9.98G [00:14<00:27, 239MB/s]\r\npytorch_model-00001-of-00002.bin:  34% 3.39G/9.98G [00:14<00:27, 239MB/s]\r\npytorch_model-00001-of-00002.bin:  34% 3.42G/9.98G [00:14<00:27, 239MB/s]\r\npytorch_model-00001-of-00002.bin:  35% 3.45G/9.98G [00:15<00:27, 237MB/s]\r\npytorch_model-00001-of-00002.bin:  35% 3.48G/9.98G [00:15<00:27, 235MB/s]\r\npytorch_model-00001-of-00002.bin:  35% 3.51G/9.98G [00:15<00:27, 233MB/s]\r\npytorch_model-00001-of-00002.bin:  36% 3.54G/9.98G [00:15<00:27, 235MB/s]\r\npytorch_model-00001-of-00002.bin:  36% 3.58G/9.98G [00:15<00:26, 237MB/s]\r\npytorch_model-00001-of-00002.bin:  36% 3.61G/9.98G [00:15<00:26, 238MB/s]\r\npytorch_model-00001-of-00002.bin:  36% 3.64G/9.98G [00:15<00:26, 239MB/s]\r\npytorch_model-00001-of-00002.bin:  37% 3.67G/9.98G [00:16<00:26, 238MB/s]\r\npytorch_model-00001-of-00002.bin:  37% 3.70G/9.98G [00:16<00:27, 231MB/s]\r\npytorch_model-00001-of-00002.bin:  37% 3.73G/9.98G [00:16<00:27, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  38% 3.76G/9.98G [00:16<00:27, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  38% 3.80G/9.98G [00:16<00:27, 229MB/s]\r\npytorch_model-00001-of-00002.bin:  38% 3.83G/9.98G [00:16<00:26, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  39% 3.86G/9.98G [00:16<00:26, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  39% 3.89G/9.98G [00:16<00:26, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  39% 3.92G/9.98G [00:17<00:26, 229MB/s]\r\npytorch_model-00001-of-00002.bin:  40% 3.95G/9.98G [00:17<00:26, 230MB/s]\r\npytorch_model-00001-of-00002.bin:  40% 3.98G/9.98G [00:17<00:26, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  40% 4.02G/9.98G [00:17<00:26, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  41% 4.05G/9.98G [00:17<00:25, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  41% 4.08G/9.98G [00:17<00:25, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  41% 4.11G/9.98G [00:17<00:25, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  42% 4.14G/9.98G [00:18<00:25, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  42% 4.17G/9.98G [00:18<00:25, 229MB/s]\r\npytorch_model-00001-of-00002.bin:  42% 4.20G/9.98G [00:18<00:24, 232MB/s]\r\npytorch_model-00001-of-00002.bin:  42% 4.24G/9.98G [00:18<00:24, 234MB/s]\r\npytorch_model-00001-of-00002.bin:  43% 4.27G/9.98G [00:18<00:24, 235MB/s]\r\npytorch_model-00001-of-00002.bin:  43% 4.30G/9.98G [00:18<00:24, 235MB/s]\r\npytorch_model-00001-of-00002.bin:  43% 4.33G/9.98G [00:18<00:24, 235MB/s]\r\npytorch_model-00001-of-00002.bin:  44% 4.36G/9.98G [00:19<00:23, 235MB/s]\r\npytorch_model-00001-of-00002.bin:  44% 4.39G/9.98G [00:19<00:23, 236MB/s]\r\npytorch_model-00001-of-00002.bin:  44% 4.42G/9.98G [00:19<00:23, 236MB/s]\r\npytorch_model-00001-of-00002.bin:  45% 4.46G/9.98G [00:19<00:23, 236MB/s]\r\npytorch_model-00001-of-00002.bin:  45% 4.49G/9.98G [00:19<00:23, 237MB/s]\r\npytorch_model-00001-of-00002.bin:  45% 4.52G/9.98G [00:19<00:23, 237MB/s]\r\npytorch_model-00001-of-00002.bin:  46% 4.55G/9.98G [00:19<00:22, 237MB/s]\r\npytorch_model-00001-of-00002.bin:  46% 4.58G/9.98G [00:19<00:22, 237MB/s]\r\npytorch_model-00001-of-00002.bin:  46% 4.61G/9.98G [00:20<00:22, 237MB/s]\r\npytorch_model-00001-of-00002.bin:  47% 4.65G/9.98G [00:20<00:22, 238MB/s]\r\npytorch_model-00001-of-00002.bin:  47% 4.68G/9.98G [00:20<00:22, 238MB/s]\r\npytorch_model-00001-of-00002.bin:  47% 4.71G/9.98G [00:20<00:22, 238MB/s]\r\npytorch_model-00001-of-00002.bin:  48% 4.74G/9.98G [00:20<00:21, 238MB/s]\r\npytorch_model-00001-of-00002.bin:  48% 4.77G/9.98G [00:20<00:21, 237MB/s]\r\npytorch_model-00001-of-00002.bin:  48% 4.80G/9.98G [00:20<00:21, 238MB/s]\r\npytorch_model-00001-of-00002.bin:  48% 4.83G/9.98G [00:21<00:21, 236MB/s]\r\npytorch_model-00001-of-00002.bin:  49% 4.87G/9.98G [00:21<00:21, 236MB/s]\r\npytorch_model-00001-of-00002.bin:  49% 4.90G/9.98G [00:21<00:21, 236MB/s]\r\npytorch_model-00001-of-00002.bin:  49% 4.93G/9.98G [00:21<00:21, 234MB/s]\r\npytorch_model-00001-of-00002.bin:  50% 4.96G/9.98G [00:21<00:22, 225MB/s]\r\npytorch_model-00001-of-00002.bin:  50% 4.99G/9.98G [00:21<00:22, 217MB/s]\r\npytorch_model-00001-of-00002.bin:  50% 5.02G/9.98G [00:21<00:23, 215MB/s]\r\npytorch_model-00001-of-00002.bin:  51% 5.05G/9.98G [00:22<00:22, 215MB/s]\r\npytorch_model-00001-of-00002.bin:  51% 5.09G/9.98G [00:22<00:22, 217MB/s]\r\npytorch_model-00001-of-00002.bin:  51% 5.12G/9.98G [00:22<00:22, 220MB/s]\r\npytorch_model-00001-of-00002.bin:  52% 5.15G/9.98G [00:22<00:21, 223MB/s]\r\npytorch_model-00001-of-00002.bin:  52% 5.18G/9.98G [00:22<00:21, 221MB/s]\r\npytorch_model-00001-of-00002.bin:  52% 5.21G/9.98G [00:22<00:21, 221MB/s]\r\npytorch_model-00001-of-00002.bin:  53% 5.24G/9.98G [00:22<00:21, 222MB/s]\r\npytorch_model-00001-of-00002.bin:  53% 5.27G/9.98G [00:23<00:21, 223MB/s]\r\npytorch_model-00001-of-00002.bin:  53% 5.31G/9.98G [00:23<00:20, 223MB/s]\r\npytorch_model-00001-of-00002.bin:  53% 5.34G/9.98G [00:23<00:20, 223MB/s]\r\npytorch_model-00001-of-00002.bin:  54% 5.37G/9.98G [00:23<00:20, 222MB/s]\r\npytorch_model-00001-of-00002.bin:  54% 5.40G/9.98G [00:23<00:20, 226MB/s]\r\npytorch_model-00001-of-00002.bin:  54% 5.43G/9.98G [00:23<00:19, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  55% 5.46G/9.98G [00:23<00:19, 231MB/s]\r\npytorch_model-00001-of-00002.bin:  55% 5.49G/9.98G [00:23<00:19, 232MB/s]\r\npytorch_model-00001-of-00002.bin:  55% 5.53G/9.98G [00:24<00:19, 232MB/s]\r\npytorch_model-00001-of-00002.bin:  56% 5.56G/9.98G [00:24<00:19, 232MB/s]\r\npytorch_model-00001-of-00002.bin:  56% 5.59G/9.98G [00:24<00:18, 233MB/s]\r\npytorch_model-00001-of-00002.bin:  56% 5.62G/9.98G [00:24<00:18, 233MB/s]\r\npytorch_model-00001-of-00002.bin:  57% 5.65G/9.98G [00:24<00:18, 234MB/s]\r\npytorch_model-00001-of-00002.bin:  57% 5.68G/9.98G [00:24<00:18, 234MB/s]\r\npytorch_model-00001-of-00002.bin:  57% 5.71G/9.98G [00:24<00:18, 234MB/s]\r\npytorch_model-00001-of-00002.bin:  58% 5.75G/9.98G [00:25<00:18, 234MB/s]\r\npytorch_model-00001-of-00002.bin:  58% 5.78G/9.98G [00:25<00:17, 234MB/s]\r\npytorch_model-00001-of-00002.bin:  58% 5.81G/9.98G [00:25<00:17, 235MB/s]\r\npytorch_model-00001-of-00002.bin:  59% 5.84G/9.98G [00:25<00:17, 235MB/s]\r\npytorch_model-00001-of-00002.bin:  59% 5.87G/9.98G [00:25<00:17, 236MB/s]\r\npytorch_model-00001-of-00002.bin:  59% 5.90G/9.98G [00:25<00:17, 237MB/s]\r\npytorch_model-00001-of-00002.bin:  59% 5.93G/9.98G [00:25<00:17, 236MB/s]\r\npytorch_model-00001-of-00002.bin:  60% 5.97G/9.98G [00:25<00:17, 235MB/s]\r\npytorch_model-00001-of-00002.bin:  60% 6.00G/9.98G [00:26<00:16, 235MB/s]\r\npytorch_model-00001-of-00002.bin:  60% 6.03G/9.98G [00:26<00:16, 233MB/s]\r\npytorch_model-00001-of-00002.bin:  61% 6.06G/9.98G [00:26<00:16, 230MB/s]\r\npytorch_model-00001-of-00002.bin:  61% 6.09G/9.98G [00:26<00:16, 229MB/s]\r\npytorch_model-00001-of-00002.bin:  61% 6.12G/9.98G [00:26<00:16, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  62% 6.16G/9.98G [00:26<00:17, 224MB/s]\r\npytorch_model-00001-of-00002.bin:  62% 6.19G/9.98G [00:26<00:16, 225MB/s]\r\npytorch_model-00001-of-00002.bin:  62% 6.22G/9.98G [00:27<00:16, 226MB/s]\r\npytorch_model-00001-of-00002.bin:  63% 6.25G/9.98G [00:27<00:16, 225MB/s]\r\npytorch_model-00001-of-00002.bin:  63% 6.28G/9.98G [00:27<00:16, 226MB/s]\r\npytorch_model-00001-of-00002.bin:  63% 6.31G/9.98G [00:27<00:16, 226MB/s]\r\npytorch_model-00001-of-00002.bin:  64% 6.34G/9.98G [00:27<00:16, 225MB/s]\r\npytorch_model-00001-of-00002.bin:  64% 6.38G/9.98G [00:27<00:15, 226MB/s]\r\npytorch_model-00001-of-00002.bin:  64% 6.41G/9.98G [00:27<00:15, 229MB/s]\r\npytorch_model-00001-of-00002.bin:  65% 6.44G/9.98G [00:28<00:15, 229MB/s]\r\npytorch_model-00001-of-00002.bin:  65% 6.47G/9.98G [00:28<00:15, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  65% 6.50G/9.98G [00:28<00:15, 227MB/s]\r\npytorch_model-00001-of-00002.bin:  65% 6.53G/9.98G [00:28<00:15, 229MB/s]\r\npytorch_model-00001-of-00002.bin:  66% 6.56G/9.98G [00:28<00:14, 231MB/s]\r\npytorch_model-00001-of-00002.bin:  66% 6.60G/9.98G [00:28<00:14, 233MB/s]\r\npytorch_model-00001-of-00002.bin:  66% 6.63G/9.98G [00:28<00:14, 235MB/s]\r\npytorch_model-00001-of-00002.bin:  67% 6.66G/9.98G [00:29<00:14, 236MB/s]\r\npytorch_model-00001-of-00002.bin:  67% 6.69G/9.98G [00:29<00:14, 235MB/s]\r\npytorch_model-00001-of-00002.bin:  67% 6.72G/9.98G [00:29<00:13, 234MB/s]\r\npytorch_model-00001-of-00002.bin:  68% 6.75G/9.98G [00:29<00:13, 234MB/s]\r\npytorch_model-00001-of-00002.bin:  68% 6.78G/9.98G [00:29<00:13, 234MB/s]\r\npytorch_model-00001-of-00002.bin:  68% 6.82G/9.98G [00:29<00:13, 235MB/s]\r\npytorch_model-00001-of-00002.bin:  69% 6.85G/9.98G [00:29<00:13, 236MB/s]\r\npytorch_model-00001-of-00002.bin:  69% 6.88G/9.98G [00:29<00:13, 237MB/s]\r\npytorch_model-00001-of-00002.bin:  69% 6.91G/9.98G [00:30<00:12, 238MB/s]\r\npytorch_model-00001-of-00002.bin:  70% 6.94G/9.98G [00:30<00:12, 238MB/s]\r\npytorch_model-00001-of-00002.bin:  70% 6.97G/9.98G [00:30<00:12, 239MB/s]\r\npytorch_model-00001-of-00002.bin:  70% 7.00G/9.98G [00:30<00:12, 239MB/s]\r\npytorch_model-00001-of-00002.bin:  71% 7.04G/9.98G [00:30<00:12, 239MB/s]\r\npytorch_model-00001-of-00002.bin:  71% 7.07G/9.98G [00:30<00:12, 236MB/s]\r\npytorch_model-00001-of-00002.bin:  71% 7.10G/9.98G [00:30<00:12, 235MB/s]\r\npytorch_model-00001-of-00002.bin:  71% 7.13G/9.98G [00:31<00:12, 234MB/s]\r\npytorch_model-00001-of-00002.bin:  72% 7.16G/9.98G [00:31<00:12, 233MB/s]\r\npytorch_model-00001-of-00002.bin:  72% 7.19G/9.98G [00:31<00:12, 230MB/s]\r\npytorch_model-00001-of-00002.bin:  72% 7.22G/9.98G [00:31<00:12, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  73% 7.26G/9.98G [00:31<00:11, 227MB/s]\r\npytorch_model-00001-of-00002.bin:  73% 7.29G/9.98G [00:31<00:11, 227MB/s]\r\npytorch_model-00001-of-00002.bin:  73% 7.32G/9.98G [00:31<00:11, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  74% 7.35G/9.98G [00:31<00:11, 226MB/s]\r\npytorch_model-00001-of-00002.bin:  74% 7.38G/9.98G [00:32<00:11, 225MB/s]\r\npytorch_model-00001-of-00002.bin:  74% 7.41G/9.98G [00:32<00:11, 225MB/s]\r\npytorch_model-00001-of-00002.bin:  75% 7.44G/9.98G [00:32<00:11, 227MB/s]\r\npytorch_model-00001-of-00002.bin:  75% 7.48G/9.98G [00:32<00:11, 226MB/s]\r\npytorch_model-00001-of-00002.bin:  75% 7.51G/9.98G [00:32<00:10, 226MB/s]\r\npytorch_model-00001-of-00002.bin:  76% 7.54G/9.98G [00:32<00:10, 227MB/s]\r\npytorch_model-00001-of-00002.bin:  76% 7.57G/9.98G [00:32<00:10, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  76% 7.60G/9.98G [00:33<00:10, 225MB/s]\r\npytorch_model-00001-of-00002.bin:  77% 7.63G/9.98G [00:33<00:10, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  77% 7.67G/9.98G [00:33<00:10, 231MB/s]\r\npytorch_model-00001-of-00002.bin:  77% 7.70G/9.98G [00:33<00:09, 233MB/s]\r\npytorch_model-00001-of-00002.bin:  77% 7.73G/9.98G [00:33<00:09, 234MB/s]\r\npytorch_model-00001-of-00002.bin:  78% 7.76G/9.98G [00:33<00:09, 234MB/s]\r\npytorch_model-00001-of-00002.bin:  78% 7.79G/9.98G [00:33<00:09, 234MB/s]\r\npytorch_model-00001-of-00002.bin:  78% 7.82G/9.98G [00:34<00:09, 234MB/s]\r\npytorch_model-00001-of-00002.bin:  79% 7.85G/9.98G [00:34<00:09, 234MB/s]\r\npytorch_model-00001-of-00002.bin:  79% 7.89G/9.98G [00:34<00:08, 233MB/s]\r\npytorch_model-00001-of-00002.bin:  79% 7.92G/9.98G [00:34<00:08, 233MB/s]\r\npytorch_model-00001-of-00002.bin:  80% 7.95G/9.98G [00:34<00:08, 233MB/s]\r\npytorch_model-00001-of-00002.bin:  80% 7.98G/9.98G [00:34<00:08, 234MB/s]\r\npytorch_model-00001-of-00002.bin:  80% 8.01G/9.98G [00:34<00:08, 234MB/s]\r\npytorch_model-00001-of-00002.bin:  81% 8.04G/9.98G [00:34<00:08, 232MB/s]\r\npytorch_model-00001-of-00002.bin:  81% 8.07G/9.98G [00:35<00:08, 234MB/s]\r\npytorch_model-00001-of-00002.bin:  81% 8.11G/9.98G [00:35<00:07, 236MB/s]\r\npytorch_model-00001-of-00002.bin:  82% 8.14G/9.98G [00:35<00:07, 236MB/s]\r\npytorch_model-00001-of-00002.bin:  82% 8.17G/9.98G [00:35<00:07, 235MB/s]\r\npytorch_model-00001-of-00002.bin:  82% 8.20G/9.98G [00:35<00:07, 235MB/s]\r\npytorch_model-00001-of-00002.bin:  83% 8.23G/9.98G [00:35<00:07, 235MB/s]\r\npytorch_model-00001-of-00002.bin:  83% 8.26G/9.98G [00:35<00:07, 232MB/s]\r\npytorch_model-00001-of-00002.bin:  83% 8.29G/9.98G [00:36<00:07, 222MB/s]\r\npytorch_model-00001-of-00002.bin:  83% 8.33G/9.98G [00:36<00:07, 223MB/s]\r\npytorch_model-00001-of-00002.bin:  84% 8.36G/9.98G [00:36<00:07, 225MB/s]\r\npytorch_model-00001-of-00002.bin:  84% 8.39G/9.98G [00:36<00:07, 226MB/s]\r\npytorch_model-00001-of-00002.bin:  84% 8.42G/9.98G [00:36<00:06, 227MB/s]\r\npytorch_model-00001-of-00002.bin:  85% 8.45G/9.98G [00:36<00:06, 226MB/s]\r\npytorch_model-00001-of-00002.bin:  85% 8.48G/9.98G [00:36<00:06, 227MB/s]\r\npytorch_model-00001-of-00002.bin:  85% 8.51G/9.98G [00:37<00:06, 226MB/s]\r\npytorch_model-00001-of-00002.bin:  86% 8.55G/9.98G [00:37<00:06, 227MB/s]\r\npytorch_model-00001-of-00002.bin:  86% 8.58G/9.98G [00:37<00:06, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  86% 8.61G/9.98G [00:37<00:06, 227MB/s]\r\npytorch_model-00001-of-00002.bin:  87% 8.64G/9.98G [00:37<00:05, 227MB/s]\r\npytorch_model-00001-of-00002.bin:  87% 8.67G/9.98G [00:37<00:05, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  87% 8.70G/9.98G [00:37<00:05, 230MB/s]\r\npytorch_model-00001-of-00002.bin:  88% 8.73G/9.98G [00:38<00:05, 230MB/s]\r\npytorch_model-00001-of-00002.bin:  88% 8.77G/9.98G [00:38<00:05, 232MB/s]\r\npytorch_model-00001-of-00002.bin:  88% 8.80G/9.98G [00:38<00:05, 234MB/s]\r\npytorch_model-00001-of-00002.bin:  88% 8.83G/9.98G [00:38<00:04, 236MB/s]\r\npytorch_model-00001-of-00002.bin:  89% 8.86G/9.98G [00:38<00:04, 237MB/s]\r\npytorch_model-00001-of-00002.bin:  89% 8.89G/9.98G [00:38<00:04, 238MB/s]\r\npytorch_model-00001-of-00002.bin:  89% 8.92G/9.98G [00:38<00:04, 238MB/s]\r\npytorch_model-00001-of-00002.bin:  90% 8.95G/9.98G [00:38<00:04, 238MB/s]\r\npytorch_model-00001-of-00002.bin:  90% 8.99G/9.98G [00:39<00:04, 238MB/s]\r\npytorch_model-00001-of-00002.bin:  90% 9.02G/9.98G [00:39<00:04, 233MB/s]\r\npytorch_model-00001-of-00002.bin:  91% 9.05G/9.98G [00:39<00:03, 235MB/s]\r\npytorch_model-00001-of-00002.bin:  91% 9.08G/9.98G [00:39<00:03, 236MB/s]\r\npytorch_model-00001-of-00002.bin:  91% 9.11G/9.98G [00:39<00:03, 237MB/s]\r\npytorch_model-00001-of-00002.bin:  92% 9.14G/9.98G [00:39<00:05, 160MB/s]\r\npytorch_model-00001-of-00002.bin:  92% 9.18G/9.98G [00:40<00:04, 178MB/s]\r\npytorch_model-00001-of-00002.bin:  92% 9.21G/9.98G [00:40<00:04, 192MB/s]\r\npytorch_model-00001-of-00002.bin:  93% 9.24G/9.98G [00:40<00:03, 204MB/s]\r\npytorch_model-00001-of-00002.bin:  93% 9.27G/9.98G [00:40<00:03, 211MB/s]\r\npytorch_model-00001-of-00002.bin:  93% 9.30G/9.98G [00:40<00:03, 216MB/s]\r\npytorch_model-00001-of-00002.bin:  94% 9.33G/9.98G [00:40<00:02, 221MB/s]\r\npytorch_model-00001-of-00002.bin:  94% 9.36G/9.98G [00:40<00:02, 225MB/s]\r\npytorch_model-00001-of-00002.bin:  94% 9.40G/9.98G [00:41<00:02, 221MB/s]\r\npytorch_model-00001-of-00002.bin:  94% 9.43G/9.98G [00:41<00:02, 220MB/s]\r\npytorch_model-00001-of-00002.bin:  95% 9.46G/9.98G [00:41<00:02, 219MB/s]\r\npytorch_model-00001-of-00002.bin:  95% 9.49G/9.98G [00:41<00:02, 221MB/s]\r\npytorch_model-00001-of-00002.bin:  95% 9.52G/9.98G [00:41<00:02, 222MB/s]\r\npytorch_model-00001-of-00002.bin:  96% 9.55G/9.98G [00:41<00:01, 223MB/s]\r\npytorch_model-00001-of-00002.bin:  96% 9.58G/9.98G [00:41<00:01, 223MB/s]\r\npytorch_model-00001-of-00002.bin:  96% 9.62G/9.98G [00:42<00:01, 225MB/s]\r\npytorch_model-00001-of-00002.bin:  97% 9.65G/9.98G [00:42<00:01, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  97% 9.68G/9.98G [00:42<00:01, 230MB/s]\r\npytorch_model-00001-of-00002.bin:  97% 9.71G/9.98G [00:42<00:01, 229MB/s]\r\npytorch_model-00001-of-00002.bin:  98% 9.74G/9.98G [00:42<00:01, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  98% 9.77G/9.98G [00:42<00:00, 227MB/s]\r\npytorch_model-00001-of-00002.bin:  98% 9.80G/9.98G [00:42<00:00, 227MB/s]\r\npytorch_model-00001-of-00002.bin:  99% 9.84G/9.98G [00:42<00:00, 228MB/s]\r\npytorch_model-00001-of-00002.bin:  99% 9.87G/9.98G [00:43<00:00, 231MB/s]\r\npytorch_model-00001-of-00002.bin:  99% 9.90G/9.98G [00:43<00:00, 233MB/s]\r\npytorch_model-00001-of-00002.bin: 100% 9.93G/9.98G [00:43<00:00, 234MB/s]\r\npytorch_model-00001-of-00002.bin: 100% 9.98G/9.98G [00:43<00:00, 229MB/s]\r\nDownloading shards:  50% 1/2 [00:43<00:43, 43.68s/it]\r\npytorch_model-00002-of-00002.bin:   0% 0.00/3.54G [00:00<?, ?B/s]\r\npytorch_model-00002-of-00002.bin:   1% 31.5M/3.54G [00:00<00:14, 238MB/s]\r\npytorch_model-00002-of-00002.bin:   2% 62.9M/3.54G [00:00<00:14, 237MB/s]\r\npytorch_model-00002-of-00002.bin:   3% 94.4M/3.54G [00:00<00:14, 237MB/s]\r\npytorch_model-00002-of-00002.bin:   4% 126M/3.54G [00:00<00:14, 237MB/s] \r\npytorch_model-00002-of-00002.bin:   4% 157M/3.54G [00:00<00:14, 237MB/s]\r\npytorch_model-00002-of-00002.bin:   5% 189M/3.54G [00:00<00:14, 237MB/s]\r\npytorch_model-00002-of-00002.bin:   6% 220M/3.54G [00:00<00:14, 237MB/s]\r\npytorch_model-00002-of-00002.bin:   7% 252M/3.54G [00:01<00:13, 237MB/s]\r\npytorch_model-00002-of-00002.bin:   8% 283M/3.54G [00:01<00:13, 237MB/s]\r\npytorch_model-00002-of-00002.bin:   9% 315M/3.54G [00:01<00:13, 238MB/s]\r\npytorch_model-00002-of-00002.bin:  10% 346M/3.54G [00:01<00:13, 238MB/s]\r\npytorch_model-00002-of-00002.bin:  11% 377M/3.54G [00:01<00:13, 238MB/s]\r\npytorch_model-00002-of-00002.bin:  12% 409M/3.54G [00:01<00:13, 237MB/s]\r\npytorch_model-00002-of-00002.bin:  12% 440M/3.54G [00:01<00:13, 235MB/s]\r\npytorch_model-00002-of-00002.bin:  13% 472M/3.54G [00:01<00:13, 235MB/s]\r\npytorch_model-00002-of-00002.bin:  14% 503M/3.54G [00:02<00:12, 234MB/s]\r\npytorch_model-00002-of-00002.bin:  15% 535M/3.54G [00:02<00:12, 234MB/s]\r\npytorch_model-00002-of-00002.bin:  16% 566M/3.54G [00:02<00:12, 231MB/s]\r\npytorch_model-00002-of-00002.bin:  17% 598M/3.54G [00:02<00:12, 229MB/s]\r\npytorch_model-00002-of-00002.bin:  18% 629M/3.54G [00:02<00:12, 227MB/s]\r\npytorch_model-00002-of-00002.bin:  19% 661M/3.54G [00:02<00:12, 226MB/s]\r\npytorch_model-00002-of-00002.bin:  20% 692M/3.54G [00:02<00:12, 226MB/s]\r\npytorch_model-00002-of-00002.bin:  20% 724M/3.54G [00:03<00:12, 227MB/s]\r\npytorch_model-00002-of-00002.bin:  21% 755M/3.54G [00:03<00:12, 223MB/s]\r\npytorch_model-00002-of-00002.bin:  22% 786M/3.54G [00:03<00:12, 222MB/s]\r\npytorch_model-00002-of-00002.bin:  23% 818M/3.54G [00:03<00:12, 222MB/s]\r\npytorch_model-00002-of-00002.bin:  24% 849M/3.54G [00:03<00:12, 223MB/s]\r\npytorch_model-00002-of-00002.bin:  25% 881M/3.54G [00:03<00:11, 223MB/s]\r\npytorch_model-00002-of-00002.bin:  26% 912M/3.54G [00:03<00:11, 224MB/s]\r\npytorch_model-00002-of-00002.bin:  27% 944M/3.54G [00:04<00:11, 223MB/s]\r\npytorch_model-00002-of-00002.bin:  28% 975M/3.54G [00:04<00:11, 225MB/s]\r\npytorch_model-00002-of-00002.bin:  28% 1.01G/3.54G [00:04<00:11, 228MB/s]\r\npytorch_model-00002-of-00002.bin:  29% 1.04G/3.54G [00:04<00:10, 229MB/s]\r\npytorch_model-00002-of-00002.bin:  30% 1.07G/3.54G [00:04<00:10, 232MB/s]\r\npytorch_model-00002-of-00002.bin:  31% 1.10G/3.54G [00:04<00:10, 234MB/s]\r\npytorch_model-00002-of-00002.bin:  32% 1.13G/3.54G [00:04<00:10, 234MB/s]\r\npytorch_model-00002-of-00002.bin:  33% 1.16G/3.54G [00:05<00:10, 235MB/s]\r\npytorch_model-00002-of-00002.bin:  34% 1.20G/3.54G [00:05<00:09, 235MB/s]\r\npytorch_model-00002-of-00002.bin:  35% 1.23G/3.54G [00:05<00:09, 236MB/s]\r\npytorch_model-00002-of-00002.bin:  36% 1.26G/3.54G [00:05<00:09, 236MB/s]\r\npytorch_model-00002-of-00002.bin:  36% 1.29G/3.54G [00:05<00:09, 237MB/s]\r\npytorch_model-00002-of-00002.bin:  37% 1.32G/3.54G [00:05<00:09, 237MB/s]\r\npytorch_model-00002-of-00002.bin:  38% 1.35G/3.54G [00:05<00:09, 236MB/s]\r\npytorch_model-00002-of-00002.bin:  39% 1.38G/3.54G [00:05<00:09, 234MB/s]\r\npytorch_model-00002-of-00002.bin:  40% 1.42G/3.54G [00:06<00:09, 234MB/s]\r\npytorch_model-00002-of-00002.bin:  41% 1.45G/3.54G [00:06<00:08, 236MB/s]\r\npytorch_model-00002-of-00002.bin:  42% 1.48G/3.54G [00:06<00:08, 237MB/s]\r\npytorch_model-00002-of-00002.bin:  43% 1.51G/3.54G [00:06<00:08, 238MB/s]\r\npytorch_model-00002-of-00002.bin:  44% 1.54G/3.54G [00:06<00:08, 239MB/s]\r\npytorch_model-00002-of-00002.bin:  44% 1.57G/3.54G [00:06<00:08, 233MB/s]\r\npytorch_model-00002-of-00002.bin:  45% 1.60G/3.54G [00:06<00:08, 231MB/s]\r\npytorch_model-00002-of-00002.bin:  46% 1.64G/3.54G [00:07<00:08, 229MB/s]\r\npytorch_model-00002-of-00002.bin:  47% 1.67G/3.54G [00:07<00:08, 228MB/s]\r\npytorch_model-00002-of-00002.bin:  48% 1.70G/3.54G [00:07<00:08, 227MB/s]\r\npytorch_model-00002-of-00002.bin:  49% 1.73G/3.54G [00:07<00:07, 227MB/s]\r\npytorch_model-00002-of-00002.bin:  50% 1.76G/3.54G [00:07<00:07, 228MB/s]\r\npytorch_model-00002-of-00002.bin:  51% 1.79G/3.54G [00:07<00:07, 227MB/s]\r\npytorch_model-00002-of-00002.bin:  52% 1.82G/3.54G [00:07<00:07, 226MB/s]\r\npytorch_model-00002-of-00002.bin:  52% 1.86G/3.54G [00:08<00:07, 225MB/s]\r\npytorch_model-00002-of-00002.bin:  53% 1.89G/3.54G [00:08<00:07, 225MB/s]\r\npytorch_model-00002-of-00002.bin:  54% 1.92G/3.54G [00:08<00:07, 225MB/s]\r\npytorch_model-00002-of-00002.bin:  55% 1.95G/3.54G [00:08<00:07, 225MB/s]\r\npytorch_model-00002-of-00002.bin:  56% 1.98G/3.54G [00:08<00:06, 225MB/s]\r\npytorch_model-00002-of-00002.bin:  57% 2.01G/3.54G [00:08<00:06, 226MB/s]\r\npytorch_model-00002-of-00002.bin:  58% 2.04G/3.54G [00:08<00:06, 226MB/s]\r\npytorch_model-00002-of-00002.bin:  59% 2.08G/3.54G [00:08<00:06, 229MB/s]\r\npytorch_model-00002-of-00002.bin:  59% 2.11G/3.54G [00:09<00:06, 228MB/s]\r\npytorch_model-00002-of-00002.bin:  60% 2.14G/3.54G [00:09<00:06, 229MB/s]\r\npytorch_model-00002-of-00002.bin:  61% 2.17G/3.54G [00:09<00:05, 231MB/s]\r\npytorch_model-00002-of-00002.bin:  62% 2.20G/3.54G [00:09<00:05, 232MB/s]\r\npytorch_model-00002-of-00002.bin:  63% 2.23G/3.54G [00:09<00:05, 234MB/s]\r\npytorch_model-00002-of-00002.bin:  64% 2.26G/3.54G [00:09<00:05, 233MB/s]\r\npytorch_model-00002-of-00002.bin:  65% 2.30G/3.54G [00:09<00:05, 235MB/s]\r\npytorch_model-00002-of-00002.bin:  66% 2.33G/3.54G [00:10<00:05, 236MB/s]\r\npytorch_model-00002-of-00002.bin:  67% 2.36G/3.54G [00:10<00:05, 222MB/s]\r\npytorch_model-00002-of-00002.bin:  67% 2.39G/3.54G [00:10<00:05, 227MB/s]\r\npytorch_model-00002-of-00002.bin:  68% 2.42G/3.54G [00:10<00:04, 230MB/s]\r\npytorch_model-00002-of-00002.bin:  69% 2.45G/3.54G [00:10<00:04, 233MB/s]\r\npytorch_model-00002-of-00002.bin:  70% 2.49G/3.54G [00:10<00:04, 235MB/s]\r\npytorch_model-00002-of-00002.bin:  71% 2.52G/3.54G [00:10<00:04, 236MB/s]\r\npytorch_model-00002-of-00002.bin:  72% 2.55G/3.54G [00:11<00:04, 238MB/s]\r\npytorch_model-00002-of-00002.bin:  73% 2.58G/3.54G [00:11<00:04, 238MB/s]\r\npytorch_model-00002-of-00002.bin:  74% 2.61G/3.54G [00:11<00:03, 239MB/s]\r\npytorch_model-00002-of-00002.bin:  75% 2.64G/3.54G [00:11<00:03, 239MB/s]\r\npytorch_model-00002-of-00002.bin:  75% 2.67G/3.54G [00:11<00:03, 240MB/s]\r\npytorch_model-00002-of-00002.bin:  76% 2.71G/3.54G [00:11<00:03, 239MB/s]\r\npytorch_model-00002-of-00002.bin:  77% 2.74G/3.54G [00:11<00:03, 236MB/s]\r\npytorch_model-00002-of-00002.bin:  78% 2.77G/3.54G [00:11<00:03, 234MB/s]\r\npytorch_model-00002-of-00002.bin:  79% 2.80G/3.54G [00:12<00:03, 234MB/s]\r\npytorch_model-00002-of-00002.bin:  80% 2.83G/3.54G [00:12<00:03, 234MB/s]\r\npytorch_model-00002-of-00002.bin:  81% 2.86G/3.54G [00:12<00:02, 232MB/s]\r\npytorch_model-00002-of-00002.bin:  82% 2.89G/3.54G [00:12<00:02, 230MB/s]\r\npytorch_model-00002-of-00002.bin:  83% 2.93G/3.54G [00:12<00:02, 230MB/s]\r\npytorch_model-00002-of-00002.bin:  83% 2.96G/3.54G [00:12<00:02, 229MB/s]\r\npytorch_model-00002-of-00002.bin:  84% 2.99G/3.54G [00:12<00:02, 225MB/s]\r\npytorch_model-00002-of-00002.bin:  85% 3.02G/3.54G [00:13<00:02, 223MB/s]\r\npytorch_model-00002-of-00002.bin:  86% 3.05G/3.54G [00:13<00:02, 224MB/s]\r\npytorch_model-00002-of-00002.bin:  87% 3.08G/3.54G [00:13<00:02, 224MB/s]\r\npytorch_model-00002-of-00002.bin:  88% 3.11G/3.54G [00:13<00:01, 224MB/s]\r\npytorch_model-00002-of-00002.bin:  89% 3.15G/3.54G [00:13<00:01, 225MB/s]\r\npytorch_model-00002-of-00002.bin:  90% 3.18G/3.54G [00:13<00:01, 224MB/s]\r\npytorch_model-00002-of-00002.bin:  91% 3.21G/3.54G [00:13<00:01, 224MB/s]\r\npytorch_model-00002-of-00002.bin:  91% 3.24G/3.54G [00:14<00:01, 228MB/s]\r\npytorch_model-00002-of-00002.bin:  92% 3.27G/3.54G [00:14<00:01, 231MB/s]\r\npytorch_model-00002-of-00002.bin:  93% 3.30G/3.54G [00:14<00:01, 233MB/s]\r\npytorch_model-00002-of-00002.bin:  94% 3.33G/3.54G [00:14<00:00, 232MB/s]\r\npytorch_model-00002-of-00002.bin:  95% 3.37G/3.54G [00:14<00:00, 234MB/s]\r\npytorch_model-00002-of-00002.bin:  96% 3.40G/3.54G [00:14<00:00, 235MB/s]\r\npytorch_model-00002-of-00002.bin:  97% 3.43G/3.54G [00:14<00:00, 236MB/s]\r\npytorch_model-00002-of-00002.bin:  98% 3.46G/3.54G [00:14<00:00, 237MB/s]\r\npytorch_model-00002-of-00002.bin:  99% 3.49G/3.54G [00:15<00:00, 238MB/s]\r\npytorch_model-00002-of-00002.bin: 100% 3.54G/3.54G [00:15<00:00, 231MB/s]\r\nDownloading shards: 100% 2/2 [00:59<00:00, 29.55s/it]\r\n/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nconfig.json: 100% 4.76k/4.76k [00:00<00:00, 26.3MB/s]\r\nLoading checkpoint shards: 100% 2/2 [00:04<00:00,  2.15s/it]\r\ngeneration_config.json: 100% 124/124 [00:00<00:00, 1.12MB/s]\r\npreprocessor_config.json: 100% 316/316 [00:00<00:00, 2.97MB/s]\r\npytorch_model.bin: 100% 1.71G/1.71G [00:08<00:00, 192MB/s]\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nLoading checkpoint shards: 100% 2/2 [00:02<00:00,  1.20s/it]\r\nWARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\r\nWARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\r\n/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\r\n  warnings.warn(\r\nTraceback (most recent call last):\r\n  File \"/content/drive/My Drive/LLaVA/run_demo.py\", line 31, in <module>\r\n    eval_model(args)\r\n  File \"/content/drive/My Drive/LLaVA/llava/eval/run_llava.py\", line 115, in eval_model\r\n    output_ids = model.generate(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/content/drive/My Drive/LLaVA/llava/model/language_model/llava_llama.py\", line 138, in generate\r\n    return super().generate(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 1758, in generate\r\n    result = self._sample(\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 2397, in _sample\r\n    outputs = self(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 166, in new_forward\r\n    output = module._old_forward(*args, **kwargs)\r\n  File \"/content/drive/My Drive/LLaVA/llava/model/language_model/llava_llama.py\", line 92, in forward\r\n    return super().forward(\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 1164, in forward\r\n    outputs = self.model(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 968, in forward\r\n    layer_outputs = decoder_layer(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 166, in new_forward\r\n    output = module._old_forward(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 727, in forward\r\n    hidden_states = self.mlp(hidden_states)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 166, in new_forward\r\n    output = module._old_forward(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 216, in forward\r\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\", line 396, in forward\r\n    return F.silu(input, inplace=self.inplace)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2102, in silu\r\n    return torch._C._nn.silu(input)\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU ```\r\n\r\nScreenshots:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/95133674/ec4090fd-c34e-44c4-8efc-bfa85a4d8a32)\r\n![image](https://github.com/haotian-liu/LLaVA/assets/95133674/f8ff98d1-9f54-45d6-b622-8c8ade4b766d)\r\n![image](https://github.com/haotian-liu/LLaVA/assets/95133674/70ef88e0-238d-418d-a773-e7ef6c360a3d)</BODY>\n\n<COMMENTS>\n<Comment by anas-zafar at 2024-06-08T11:53:20Z>\n@lmcjrrg what GPU did you use to solve this?\n</Comment>\n<Comment by lmcjrrg at 2024-06-08T12:37:45Z>\n> @lmcjrrg你使用什么 GPU 来解决这个问题？\r\n\r\nL4 can do it, the two demo fragments are executed separately\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1540,
    "state": "open",
    "created_by": "NicoZenith",
    "created_at": "2024-06-04T09:36:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1540</URL>\n\n<TITLE>[Usage] Error when loading adapter_model.safetensors</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\n\r\nAfter fine-tuning llava v.1.5 13b, with the last version of the repository, when I try to load the Lora weights, I get the following error \r\n\r\nCommand:\r\n```\r\npython -m llava.serve.cli     --model-path checkpoints/llava-v1.5-13b-lora --model-base liuhaotian/llava-v1.5-13b    --image-file \"https://llava-vl.github.io/static/images/view.jpg\"  \r\n```\r\n\r\nLog: \r\n```\r\nLoading LLaVA from base model...\r\nDownloading shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  8.41it/s]\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:11<00:00,  3.92s/it]\r\nLoading additional LLaVA weights...\r\nLoading LoRA weights...\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/iopsstor/scratch/cscs/ndeperr/LLaVA/llava/serve/cli.py\", line 126, in <module>\r\n    main(args)\r\n  File \"/iopsstor/scratch/cscs/ndeperr/LLaVA/llava/serve/cli.py\", line 32, in main\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n  File \"/iopsstor/scratch/cscs/ndeperr/LLaVA/llava/model/builder.py\", line 83, in load_pretrained_model\r\n    model = PeftModel.from_pretrained(model, model_path)\r\n  File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 430, in from_pretrained\r\n    model.load_adapter(model_id, adapter_name, is_trainable=is_trainable, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 984, in load_adapter\r\n    adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py\", line 444, in load_peft_weights\r\n    adapters_weights = safe_load_file(filename, device=device)\r\n  File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\", line 311, in load_file\r\n    with safe_open(filename, framework=\"pt\", device=device) as f:\r\nsafetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge\r\n```\r\n\r\nIt seems that the loading works for 1.6 lora weights, but there is no training script for llava 1.6 (I used the one from llava 1.5).</BODY>\n\n<COMMENTS>\n<Comment by Searen-da at 2024-06-05T15:10:06Z>\nI had similar issue and [this comment](https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/8123#issuecomment-2056883519) really helped. Try redownloading the file using git-lfs\n</Comment>\n<Comment by NicoZenith at 2024-06-05T15:43:02Z>\nNevermind, it's because I wanted to modify the key names of the weights and it corrupted the file. \r\nIndeed, I need to change the name of the weights so that it matches the one of the huggingface version (https://huggingface.co/llava-hf/llava-1.5-13b-hf) otherwise the LoRA merging wouldn't occur :)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1539,
    "state": "closed",
    "created_by": "DemoGit4LIANG",
    "created_at": "2024-06-04T05:29:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1539</URL>\n\n<TITLE>Nothing</TITLE>\n\n<BODY></BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1538,
    "state": "open",
    "created_by": "zzxslp",
    "created_at": "2024-06-03T22:39:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1538</URL>\n\n<TITLE>Batch size to reproduce llava-1.5-7B</TITLE>\n\n<BODY>### Question\n\nHi! may I know the batch-size to reproduce llava-1.5-7B reported in the paper? (assuming all other hyper-parameters are the same as llava-1.5-13B as in [the fine-tune srcript](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune.sh)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1537,
    "state": "open",
    "created_by": "GoGoJoestar",
    "created_at": "2024-06-03T10:09:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1537</URL>\n\n<TITLE>Does vistion tower trained during starge 2 (Visual Instruction Tuning)?</TITLE>\n\n<BODY>I find `@torch.no_grad()` in CLIPVisionTower.forward(), so it won't flow gradient to CLIP while training.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/c121f0432da27facab705978f83c4ada465e46fd/llava/model/multimodal_encoder/clip_encoder.py#L45-L57\r\n\r\nHowever, here is a key: `\"mm_vision_tower_lr\": 2e-06,\"` in model's `config.json` file, and in the [LLaVA-NEXT blog on May 25th](https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/), the vision tower are training during stage-2 with lr=2e-6. \r\n\r\nWere the previous models trained according to this strategy? Will training CLIP be better when training for downstream task?</BODY>\n\n<COMMENTS>\n<Comment by 2U1 at 2024-06-18T00:55:11Z>\nI think the training code isn't open yet for the LLaVA-NEXT.\n</Comment>\n<Comment by PangziZhang523 at 2024-06-19T12:16:49Z>\nPrint out the gradient，while requires_grad=True，the parameter.grad=None?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1536,
    "state": "open",
    "created_by": "J0eky",
    "created_at": "2024-06-03T03:10:02Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1536</URL>\n\n<TITLE>How to perform cross-validation during the fine-tuning process?[Question]</TITLE>\n\n<BODY>### Question\n\nI have been fine-tuning the llava-v1.5-13b model on my own dataset using the finetune_task_lora.sh script. While the training phase performed well, I observed an absence of any validation process throughout the training, potentially culminating in overfitting concerns. It would be grateful if someone could offer some advice.</BODY>\n\n<COMMENTS>\n<Comment by narayanasastry-rvds at 2024-09-13T05:35:56Z>\nI am finetuning llava-v1.6-vicuna-7b and want to know how can I calculate validation loss during lora fine-tuning\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1535,
    "state": "open",
    "created_by": "kzos",
    "created_at": "2024-06-02T13:26:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1535</URL>\n\n<TITLE>load_dataset(liuhaotian/LLaVA-Instruct-150K) fails with error, when generating train split: 394276 examples</TITLE>\n\n<BODY>from datasets import load_dataset\r\ndata = load_dataset(\"liuhaotian/LLaVA-Instruct-150K\")\r\n\r\nDownloading readme: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.22k/1.22k [00:00<00:00, 12.3MB/s]\r\nDownloading data: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79.6M/79.6M [00:04<00:00, 18.5MB/s]\r\nDownloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 126M/126M [00:06<00:00, 19.9MB/s]\r\nDownloading data: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20.5M/20.5M [00:01<00:00, 12.5MB/s]\r\nDownloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 229M/229M [00:12<00:00, 19.1MB/s]\r\nDownloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 131M/131M [00:05<00:00, 24.7MB/s]\r\nDownloading data: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.03G/1.03G [00:57<00:00, 17.9MB/s]\r\nGenerating train split: 394276 examples [00:14, 26360.31 examples/s] \r\nTraceback (most recent call last):\r\npython3.12/site-packages/datasets/packaged_modules/json/json.py\", line 122, in _generate_tables\r\n    pa_table = paj.read_json(\r\n               ^^^^^^^^^^^^^^\r\n  File \"pyarrow/_json.pyx\", line 308, in pyarrow._json.read_json\r\n  File \"pyarrow/error.pxi\", line 154, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 91, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: JSON parse error: Column() changed from object to string in row 0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\npython3.12/site-packages/datasets/packaged_modules/json/json.py\", line 162, in _generate_tables\r\n    pa_table = pa.Table.from_pydict(mapping)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"pyarrow/table.pxi\", line 1920, in pyarrow.lib._Tabular.from_pydict\r\n  File \"pyarrow/table.pxi\", line 5992, in pyarrow.lib._from_pydict\r\n  File \"pyarrow/array.pxi\", line 385, in pyarrow.lib.asarray\r\n  File \"pyarrow/array.pxi\", line 355, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\", line 42, in pyarrow.lib._sequence_to_array\r\n  File \"pyarrow/error.pxi\", line 154, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 91, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowTypeError: Expected bytes, got a 'int' object\r\n\r\nThe above exception was the direct cause of the following exception:</BODY>\n\n<COMMENTS>\n<Comment by shizhengLi at 2024-06-03T06:52:11Z>\nsame problem. +1\n</Comment>\n<Comment by ucas-hao at 2024-07-29T04:25:44Z>\nsame problem. +1 any idea to solve?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1534,
    "state": "open",
    "created_by": "gitusersfor",
    "created_at": "2024-05-31T19:10:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1534</URL>\n\n<TITLE>Can we use this model to identify between real and AI generated/fake image?</TITLE>\n\n<BODY>### Question\n\nCan we use this model to identify between real and AI generated/fake image?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1533,
    "state": "open",
    "created_by": "zihui-debug",
    "created_at": "2024-05-31T11:21:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1533</URL>\n\n<TITLE>[Question] about the --vision_tower in the scritps</TITLE>\n\n<BODY>### Question\n\nThanks for your good work! I would like to know if I need to continue the next stage of fine-tuning on a model that has already been fine-tuned with vision tower, will the weights pointed by `--vision_tower` override those of the original model? Because I see that in `build_vision_tower` a vision tower is initialized. \r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/57470281/85533d24-25dc-4138-bf5b-c5b9bac9d500)\r\n\r\nAnd then in the corresponding encoder.py file, the code loads the vision tower from the path set by `--vision_tower`. Maybe there are some misunderstandings, I'm confused about this.\r\n \r\n![image](https://github.com/haotian-liu/LLaVA/assets/57470281/9397d1e1-59a6-40b9-b9d2-f5b60cfde24d)</BODY>\n\n<COMMENTS>\n<Comment by MassEast at 2025-04-09T18:48:37Z>\nAfter doing the pretraining (feature alignment), you should be left with a `mm_projector.bin` file, that is, \"the two-layer MLP vision-language connector\". This is what you trained in that stage.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1532,
    "state": "open",
    "created_by": "lucasjinreal",
    "created_at": "2024-05-30T03:38:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1532</URL>\n\n<TITLE>Where does the video interpolate indicates in the code?</TITLE>\n\n<BODY>>  such as the implementation of linear scaling in rotary position embeddings (RoPE), we apply a similar scaling approach in LLaVA-NeXT. B\r\n\r\nAny indicates which code implements this linear scaling feature on video?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1529,
    "state": "open",
    "created_by": "yubin1219",
    "created_at": "2024-05-28T09:33:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1529</URL>\n\n<TITLE>[Question] Why apply the causal mask to image tokens in the attention operations of LLM?</TITLE>\n\n<BODY>### Question\n\nIn the attention operations of the LLM, text tokens must use the causal mask to predict the next text tokens for the answer. However, the image features are also masked by a causal mask rather than by a bidirectional mask in the attention operations of the LLM, even though the bidirectional mask allows access to more visual information than the causal mask. Is there a particular reason for this making?</BODY>\n\n<COMMENTS>\n<Comment by Yebulabula at 2024-06-22T19:40:01Z>\nI have the same question.\n</Comment>\n<Comment by yunzhuzhang0918 at 2024-11-08T03:39:47Z>\nI have the same question.\n</Comment>\n<Comment by zazamrykh at 2025-01-08T18:26:28Z>\nStill not answered :( But we've made experiment about causal mask, but in only textual modality. We've trained gpt-like model with causal mask in common way and than trained model without causal mask. Of course, when we didn't use causal mask time increases about 30 times because without mask instead of one pass of training example we do n_seq passes. And we compared results and have got the same log-loss with and without mask. Bit more details: https://github.com/zazamrykh/CausalMaskExperiments/\r\n\r\nI think that presence of mask on image embeddings have no significant impact on quality and developers of LLaVA have just decided go simple way and don't make their life complicated by making modification in causal mask. But maybe I am wrong and modification of mask in LLaVA really improves quality.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1528,
    "state": "open",
    "created_by": "xiaoachen98",
    "created_at": "2024-05-27T07:38:02Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1528</URL>\n\n<TITLE>🎉🎉We have supported open-source training implementation of LLaVA-NeXT series!</TITLE>\n\n<BODY>[Open-LLaVA-NeXT](https://github.com/xiaoachen98/Open-LLaVA-NeXT): An open-source implementation of **LLaVA-NeXT** series for facilitating the large multi-modal model community.\r\n\r\n- 🔥 All training data and checkpoints at each stage are open-sourced, friendly for research usage.\r\n- 🔥 Able to reproduce the results of **[LLaVA-NeXT](https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/)**.\r\n- 🔥 Based on the **[LLaVA](https://github.com/haotian-liu/LLaVA)** codebase with minimal modification, easy to follow.\r\n\r\n| Name | ViT | LLM | Weights | MME | SEED | SQA | MMB | MMB-CN | TextVQA | GQA |\r\n|---|---|---|---|---|---|---|---|---|---|---|\r\n| llava-next-vicuna-7b | CLIP-L-336 | Vicuna-7B | [SFT](https://huggingface.co/liuhaotian/llava-v1.6-vicuna-7b) | 1519 | 70.2 | 70.1 | 67.4 | 60.6 | 64.9 | 64.2 |\r\n| open-llava-next-vicuna-7b| CLIP-L-336 | Vicuna-7B | [PT](https://huggingface.co/Lin-Chen/open-llava-next-vicuna-7b/tree/main/pretrain), [SFT](https://huggingface.co/Lin-Chen/open-llava-next-vicuna-7b) | 1540 | 71.1 | 70.7 | 68.5 | 60.7 | 67.2 | 64.3 |\r\n| open-llava-next-llama3-8b| CLIP-L-336 | LLaMA3-8B | [PT](https://huggingface.co/Lin-Chen/open-llava-next-llama3-8b), [SFT](https://huggingface.co/Lin-Chen/open-llava-next-llama3-8b) | 1552 | 74.4 | 77.3 | 74.4 | 70.4 | 69.8 | 65.9 |\r\n\r\nThe 34B and 4B variants of open-llava-next are right on the way!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1527,
    "state": "open",
    "created_by": "liuliAI",
    "created_at": "2024-05-24T09:36:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1527</URL>\n\n<TITLE>errors in MME evaluation</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nerrors in MME evaluation\r\nCommand:\r\n```\r\nCUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/eval/mme.sh\r\n```\r\n\r\nLog: \r\n```\r\nRuntimeError: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasGemmEx( handle, opa, opb, m, n, k, &falpha, a, CUDA_R_16F, lda, b, CUDA_R_16F, ldb, &fbeta, c, CUDA_R_16F, ldc, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`\r\nTraceback (most recent call last):\r\n  File \"/data/LLaVA/playground/data/eval/MME/convert_answer_to_mme.py\", line 43, in <module>\r\n    GT = get_gt(\r\n  File \"/data/LLaVA/playground/data/eval/MME/convert_answer_to_mme.py\", line 35, in get_gt\r\n    question, answer = line.strip().split('\\t')\r\nValueError: not enough values to unpack (expected 2, got 1)\r\n=========== Perception ===========\r\nTraceback (most recent call last):\r\n  File \"/data/LLaVA/playground/data/eval/MME/eval_tool/calculation.py\", line 164, in <module>\r\n    cal.process_result(results_dir)\r\n  File \"/data/LLaVA/playground/data/eval/MME/eval_tool/calculation.py\", line 98, in process_result\r\n    lines = open(task_txt, 'r').readlines()\r\nFileNotFoundError: [Errno 2] No such file or directory: 'answers/llava-v1.5-13b/existence.txt'\r\n\r\n```</BODY>\n\n<COMMENTS>\n<Comment by wijayarobert at 2024-06-19T03:40:58Z>\nI also get the same error. Did you manage to resolve it?\n</Comment>\n<Comment by F-Yuan303 at 2024-07-30T08:55:56Z>\nI also get the same error. Did you manage to resolve it?\n</Comment>\n<Comment by zhangbilang at 2025-07-26T14:43:20Z>\nAfter running CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/eval/mme.sh, if an error occurs, simply comment out the model inference part of the script and run it again.\n\n<img width=\"856\" height=\"573\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/704a9b84-89d4-4b9c-a4a2-e89d448c4269\" />\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1526,
    "state": "open",
    "created_by": "Shauryatwari",
    "created_at": "2024-05-24T04:11:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1526</URL>\n\n<TITLE>'LlamaConfig' object has no attribute 'quantization_config'</TITLE>\n\n<BODY>**### 'LlamaConfig' object has no attribute 'quantization_config'**\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n[<ipython-input-19-c4c94986d254>](https://localhost:8080/#) in <cell line: 1>()\r\n----> 1 model.config.quantization_config.to_dict()\r\n\r\n[/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py](https://localhost:8080/#) in __getattribute__(self, key)\r\n    259         if key != \"attribute_map\" and key in super().__getattribute__(\"attribute_map\"):\r\n    260             key = super().__getattribute__(\"attribute_map\")[key]\r\n--> 261         return super().__getattribute__(key)\r\n    262 \r\n    263     def __init__(self, **kwargs):\r\n\r\nAttributeError: 'LlamaConfig' object has no attribute 'quantization_config'\r\n\r\nim using google colab with the t4 gpu, please help</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1525,
    "state": "closed",
    "created_by": "20191864218",
    "created_at": "2024-05-24T02:44:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1525</URL>\n\n<TITLE>[Question] 如何从中间的checkpoint继续训练</TITLE>\n\n<BODY>### Question\n\n各位大佬，我在用lora微调llava的时候断掉了，但是我保存了中间的checkpoint，我该怎么从这个checkpoint继续微调呀，谢谢！</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1524,
    "state": "open",
    "created_by": "yesgvinayak",
    "created_at": "2024-05-23T08:08:59Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1524</URL>\n\n<TITLE>[Usage] What is the number of images needed  for fine tuning,</TITLE>\n\n<BODY>I am trying to finetune the model to identify micro emotions from the face. \r\nFor that, I have selected 3 emotions, calm, happy, and confused. How many images are needed for these? \r\nI have created the description for the dataset using chat-gpt.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1523,
    "state": "open",
    "created_by": "xmu-xiaoma666",
    "created_at": "2024-05-23T03:22:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1523</URL>\n\n<TITLE>Multi-machine multi-card training</TITLE>\n\n<BODY>### Describe the issue\n\nHow to train LLava using multiple machines and multiple cards？</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1522,
    "state": "open",
    "created_by": "cuihenggang",
    "created_at": "2024-05-22T16:25:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1522</URL>\n\n<TITLE>[Usage] Need to install protobuf in order to run CLI Inference</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\n\r\nCommand:\r\n```\r\n$ python -m llava.serve.cli \\\r\n>     --model-path liuhaotian/llava-v1.5-7b \\\r\n>     --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n>     --load-4bit\r\n```\r\n\r\nLog: \r\n```\r\n/home/hcui/python/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\ntokenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 749/749 [00:00<00:00, 3.09MB/s]\r\ntokenizer.model: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500k/500k [00:00<00:00, 16.3MB/s]\r\nspecial_tokens_map.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 438/438 [00:00<00:00, 2.31MB/s]\r\nTraceback (most recent call last):\r\n  File \"/home/hcui/python/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/hcui/python/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/hcui/repos/LLaVA/llava/serve/cli.py\", line 126, in <module>\r\n    main(args)\r\n  File \"/home/hcui/repos/LLaVA/llava/serve/cli.py\", line 32, in main\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n  File \"/home/hcui/repos/LLaVA/llava/model/builder.py\", line 116, in load_pretrained_model\r\n    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n  File \"/home/hcui/python/envs/llava/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 814, in from_pretrained\r\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\r\n  File \"/home/hcui/python/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2029, in from_pretrained\r\n    return cls._from_pretrained(\r\n  File \"/home/hcui/python/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2261, in _from_pretrained\r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\n  File \"/home/hcui/python/envs/llava/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama.py\", line 178, in __init__\r\n    self.sp_model = self.get_spm_processor(kwargs.pop(\"from_slow\", False))\r\n  File \"/home/hcui/python/envs/llava/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama.py\", line 208, in get_spm_processor\r\n    model_pb2 = import_protobuf(f\"The new behaviour of {self.__class__.__name__} (with `self.legacy = False`)\")\r\n  File \"/home/hcui/python/envs/llava/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py\", line 43, in import_protobuf\r\n    raise ImportError(PROTOBUF_IMPORT_ERROR.format(error_message))\r\nImportError: \r\nThe new behaviour of LlamaTokenizer (with `self.legacy = False`) requires the protobuf library but it was not found in your environment. Checkout the instructions on the\r\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\r\nthat match your environment. Please note that you may need to restart your runtime after installation.\r\n```</BODY>\n\n<COMMENTS>\n<Comment by ggcr at 2024-05-27T16:41:13Z>\n`pip install protobuf` and you will be able to run it fine.\n</Comment>\n<Comment by cuihenggang at 2024-05-27T20:57:52Z>\nI think it will be useful to add this dependency to the requirement list for `pip install`\n</Comment>\n<Comment by ggcr at 2024-05-28T07:15:41Z>\nAgree. The dependency comes from the `transformers` library, not LLaVA.\r\n\r\nhttps://github.com/huggingface/transformers/issues/24533\n</Comment>\n<Comment by cuihenggang at 2024-05-28T11:27:29Z>\nI see. Thank you for the explanation. Feel free to close the ticket if nothing we can do in this project.\n</Comment>\n<Comment by KansaiTraining at 2024-08-26T07:06:19Z>\nAt least it would be a good idea to include this info in the readme, so that users do not have to come to here\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1521,
    "state": "open",
    "created_by": "jujeongho0",
    "created_at": "2024-05-22T11:20:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1521</URL>\n\n<TITLE>[Question] [LLaVA] Calculation Loss of <STOP> in System Message and User's Instruction</TITLE>\n\n<BODY>### Question\n\nThanks for the Great Work!!!\r\n\r\nIn your paper, you calculate loss for tokens corresponding to <STOP> after system messages and user instructions, and I was wondering why. I think this part is user input and shouldn't need to be calculated. What do you think?\r\n\r\nThank You :)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1520,
    "state": "open",
    "created_by": "jujeongho0",
    "created_at": "2024-05-22T11:10:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1520</URL>\n\n<TITLE>[Question] [LLaVA] Location of <image> token</TITLE>\n\n<BODY>### Question\n\nThanks for the Great Work !!!\r\n\r\nIn the paper, the <image> tokens where the image features are located are randomized forward and backward, is this done because it performs better than having them fixed forward or backward? I was wondering if there is an ablation for this?\r\n\r\nThank You :)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1519,
    "state": "open",
    "created_by": "yesgvinayak",
    "created_at": "2024-05-22T09:29:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1519</URL>\n\n<TITLE>[Usage] Error while using finetuned model</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: I have fine tuned the llava-v1.5-7b. And in the output directory I got some files.\r\n1. adapter_model.safetensors                        \r\n2. config.json                                            \r\n3. README.md                                             \r\n4. adapter_config.json                                 \r\n5. non_lora_trainables.bin                              \r\n6. trainer_state.json  \r\n\r\nthen I tried inference using this folder as model_path and base model as  liuhaotian/llava-v1.5-7b. I am getting error.\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\nRuntimeError: Error(s) in loading state_dict for LlavaLlamaForCausalLM:\r\n        size mismatch for model.mm_projector.0.weight: copying a param with shape torch.Size([4096, 1024]) from checkpoint, the shape in current model is torch.Size([2097152, 1]).\r\n        size mismatch for model.mm_projector.2.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([8388608, 1]).\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by itay1542 at 2024-05-22T10:47:40Z>\nIt seems like you fine tuned using LoRa, which means you have to use the merge_lora_weights.py script (located under scripts/ folder) before trying inference.\n</Comment>\n<Comment by yesgvinayak at 2024-05-24T15:52:12Z>\n@itay1542 I have ran the merge_lora_weights.py  and created the merged one. but getting an error,\r\n\r\n  File \"/home/skadmin/cx-research/core/Llava/llava/eval/run_llava.py\", line 117, in eval_model\r\n    images_tensor = process_images(\r\n  File \"/home/skadmin/cx-research/core/Llava/llava/mm_utils.py\", line 171, in process_images\r\n    image = expand2square(image, tuple(int(x*255) for x in image_processor.image_mean))\r\nAttributeError: 'NoneType' object has no attribute 'image_mean'\r\n\r\nhere is the code:\r\nmodel_path = \"./saved_model\"\r\nmodel_name = get_model_name_from_path(model_path)\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(model_path, None, model_name)\n</Comment>\n<Comment by nlpkiddo-2001 at 2024-06-09T13:20:07Z>\n@itay1542 I have fine tuned the model without lora, Now what should i do for the inference. Kindly guide me\n</Comment>\n<Comment by Yrdal3910 at 2024-11-02T02:53:11Z>\nI encountered the same issue. For me, the error occurred when I added the parameter \"--load-4bit\" while launching the model worker. It appears that the base model uses 8-bit storage because inference runs smoothly either when I remove this parameter or switch to \"--load-8bit\". This also explains why 4096 * 1024 equals 4194304, which is double 2097152.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1518,
    "state": "open",
    "created_by": "DafengChi",
    "created_at": "2024-05-21T12:30:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1518</URL>\n\n<TITLE>[Question] rank0_print not work</TITLE>\n\n<BODY>### Question\n\n i use nohup log command, but the information i want print not show in the log : \r\nnohup deepspeed llava/train/train_xformers.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path /home/ma-user/work/embodied_models/llava-v1.5-7b \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/llava_train_test.json \\\r\n    --image_folder /home/ma-user/work/embodied_data/ \\\r\n    --vision_tower /home/ma-user/work/embodied_models/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 False \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b-task-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 False \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to tensorboard \\\r\n    > logs/llava-v1.5-7b-task-lora-1.log 2>&1\r\n\r\nin the train.py : \r\nline793\r\n    rank0_print(\"model_args:\")\r\n    rank0_print(model_args)\r\n    rank0_print(\"data_args:\")\r\n    rank0_print(data_args)\r\n    rank0_print(\"training_args:\")\r\n    rank0_print(training_args)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1517,
    "state": "open",
    "created_by": "1015356144",
    "created_at": "2024-05-21T10:53:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1517</URL>\n\n<TITLE>[Question] 请问脚本文件里的--version参数是指谁的version呀，为什么默认值设置是v1，如果是用llava1.5的话要手动改成v1.5吗</TITLE>\n\n<BODY>### Question\n\n请问脚本文件里的--version参数是指谁的version呀，为什么默认值设置是v1，如果是用llava1.5的话要手动改成v1.5吗\r\n![image](https://github.com/haotian-liu/LLaVA/assets/126430125/88fc167a-bfe2-448a-98f7-d36da0d2629e)</BODY>\n\n<COMMENTS>\n<Comment by foreverhell at 2024-05-28T03:47:20Z>\nIt refers to conversation template. You can find it used in llava/conversation.py. It significantly influences the model training and infering if you modify another language model.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1516,
    "state": "open",
    "created_by": "Yonggie",
    "created_at": "2024-05-21T07:34:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1516</URL>\n\n<TITLE>The ``device`` args doesn't take effect</TITLE>\n\n<BODY>### Describe the issue\n\nRun demo python with args like:\r\n```\r\n     --model-path /public/LLaVA/llava-v1.5-7b \\\r\n     --image-file \"task_test.png\" \\\r\n     --load-4bit\r\n     --device='cuda:1'\r\n```\r\nIssue:\r\nIt will trigger different device error.\r\nI believe it's a problem about the ``vision tower``.\r\nThe images and casual language model are both in the device1 but the vision tower model.\r\n```\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument weight in method wrapper_CUDA__cudnn_convolution)\r\n```\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/planner/anaconda3/envs/habitat_latest/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/planner/anaconda3/envs/habitat_latest/lib/python3.9/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/planner/.vscode-server/extensions/ms-python.debugpy-2024.6.0/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py\", line 39, in <module>\r\n    cli.main()\r\n  File \"/home/planner/.vscode-server/extensions/ms-python.debugpy-2024.6.0/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py\", line 430, in main\r\n    run()\r\n  File \"/home/planner/.vscode-server/extensions/ms-python.debugpy-2024.6.0/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py\", line 284, in run_file\r\n    runpy.run_path(target, run_name=\"__main__\")\r\n  File \"/home/planner/.vscode-server/extensions/ms-python.debugpy-2024.6.0/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py\", line 321, in run_path\r\n    return _run_module_code(code, init_globals, run_name,\r\n  File \"/home/planner/.vscode-server/extensions/ms-python.debugpy-2024.6.0/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py\", line 135, in _run_module_code\r\n    _run_code(code, mod_globals, init_globals,\r\n  File \"/home/planner/.vscode-server/extensions/ms-python.debugpy-2024.6.0/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py\", line 124, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/public/yzc/completely_new/first_load_llava.py\", line 273, in <module>\r\n    output=model(\r\n  File \"/home/planner/anaconda3/envs/habitat_latest/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/planner/anaconda3/envs/habitat_latest/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/public/yzc/completely_new/llava/model/language_model/llava_llama.py\", line 81, in forward\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/public/yzc/completely_new/llava/model/llava_arch.py\", line 205, in prepare_inputs_labels_for_multimodal\r\n    image_features = self.encode_images(images)\r\n  File \"/public/yzc/completely_new/llava/model/llava_arch.py\", line 144, in encode_images\r\n    image_features = encoder(images)\r\n  File \"/home/planner/anaconda3/envs/habitat_latest/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/planner/anaconda3/envs/habitat_latest/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/planner/anaconda3/envs/habitat_latest/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/public/yzc/completely_new/llava/model/multimodal_encoder/clip_encoder.py\", line 54, in forward\r\n    image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\r\n  File \"/home/planner/anaconda3/envs/habitat_latest/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/planner/anaconda3/envs/habitat_latest/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/planner/anaconda3/envs/habitat_latest/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/planner/anaconda3/envs/habitat_latest/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py\", line 917, in forward\r\n    return self.vision_model(\r\n  File \"/home/planner/anaconda3/envs/habitat_latest/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/planner/anaconda3/envs/habitat_latest/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/planner/anaconda3/envs/habitat_latest/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py\", line 841, in forward\r\n    hidden_states = self.embeddings(pixel_values)\r\n  File \"/home/planner/anaconda3/envs/habitat_latest/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/planner/anaconda3/envs/habitat_latest/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/planner/anaconda3/envs/habitat_latest/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/planner/anaconda3/envs/habitat_latest/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py\", line 182, in forward\r\n    patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]\r\n  File \"/home/planner/anaconda3/envs/habitat_latest/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/planner/anaconda3/envs/habitat_latest/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/planner/anaconda3/envs/habitat_latest/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/planner/anaconda3/envs/habitat_latest/lib/python3.9/site-packages/torch/nn/modules/conv.py\", line 460, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n  File \"/home/planner/anaconda3/envs/habitat_latest/lib/python3.9/site-packages/torch/nn/modules/conv.py\", line 456, in _conv_forward\r\n    return F.conv2d(input, weight, bias, self.stride,\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument weight in method wrapper_CUDA__cudnn_convolution)\r\n```</BODY>\n\n<COMMENTS>\n<Comment by murray-k at 2024-07-15T09:29:56Z>\nSame here. Does anyone have a fix here?\n</Comment>\n<Comment by HenryJunW at 2024-07-31T23:31:01Z>\nsame here.\n</Comment>\n<Comment by Yanfors at 2024-08-19T13:08:06Z>\nsame here.\n</Comment>\n<Comment by soroushseifi at 2024-10-16T08:31:14Z>\nsame here\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1515,
    "state": "open",
    "created_by": "hi-sharma",
    "created_at": "2024-05-20T14:25:23Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1515</URL>\n\n<TITLE>[Usage] Missing Projector weights for llava-v1.6-mistral-*</TITLE>\n\n<BODY>### Describe the issue\n\nThanks for the great work @haotian-liu \r\nIssue:\r\n\r\nI am trying to replicate your fine tuning steps from scratch without using ShareGPT data.  \r\n\r\nI am unable to find the projector weights for Llava-v1.6 with Mistral versions [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#projector-weights) and in huggingface repo. Since [Vicuna already uses ShareGPT](https://huggingface.co/lmsys/vicuna-7b-v1.5)  data, cannot use it as the base model. \r\n\r\nPlease Let me know if you plan to release  projector weights for Llava-v1.6 with Mistral (or Llama). If already released kindly re-direct to appropriate link.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1514,
    "state": "open",
    "created_by": "20191864218",
    "created_at": "2024-05-20T13:56:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1514</URL>\n\n<TITLE>How to swap LLM for baichuan-13b-base</TITLE>\n\n<BODY>### Question\n\nHow to swap LLM for baichuan-13b-base，I would be grateful if anyone could help me，thanks！</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1513,
    "state": "open",
    "created_by": "shiyuleixia",
    "created_at": "2024-05-20T13:06:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1513</URL>\n\n<TITLE>[Usage] 7B  model has an abnormal output for some images</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nWhen I use the 7B model to predict ,some images just predict bad results and the results return by model is very slow\r\nCommand:\r\n```\r\nthe prompt is :Analyze the image and list 10 environmental tags that describe visible elements such as natural scenery, architectural features, and lighting conditions. Tags should include elements like types of vegetation, specific buildings, or weather conditions. Deliver the tags in a single line, separated by commas. Avoid any references to people, crowds, personal attributes or animals.\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.\r\n![852f925c3d8a6a992fd8ac78bca5eb21](https://github.com/haotian-liu/LLaVA/assets/15072820/8753087f-880c-47f9-a5bc-3840c649bcf2)\r\n\r\nit outputs like this:\r\nStone building, stone wall, stone steps, stone window sills, stone window frames, stone window, stone door, stone archway, stone pillar, stone walkway, stone ground, stone sidewalk, stone street, stone building facade, stone building corner, stone building entrance, stone building door, stone building window, stone building window frame, stone building window sill, stone building window pane, stone building window panes, stone building window, stone building window frame, stone building window sill, stone building window pane, stone building window panes, stone building window, stone building window frame, stone building window sill, stone building window pane, stone building window panes, stone building window, stone building window frame, stone building window sill, stone building window pane, stone building window panes, stone building window, stone building window frame, stone building window sill, stone building window pane, stone building window panes, stone building window, stone building window frame, stone building window sill, stone building window pane, stone building window panes, stone building window, stone building window frame, stone building window sill, stone building window pane, stone building window panes, stone building window, stone building window frame, stone building window sill, stone building window pane, stone building window panes, stone building window, stone building window frame, stone building window sill, stone building window pane, stone building window panes, stone building window, stone building window frame, stone building window sill, stone building window pane, stone building window panes, stone building window, stone building window frame, stone building window sill, stone building window pane, stone building window panes, stone building window, stone building window frame, stone building window sill, stone building window pane, stone building window panes, stone building window, stone building window frame, stone building window sill, stone building window pane, stone building window panes, stone building window, stone building window frame, stone building window sill, stone building window pane, stone building window panes, stone building window, stone building window frame, stone building window sill, stone building window pane, stone building window panes, stone building window, stone building window frame, stone building window sill, stone building window pane, stone building window panes, stone building window, stone building window frame, stone building window sill, stone building</BODY>\n\n<COMMENTS>\n<Comment by ProGamerGov at 2024-05-23T19:45:39Z>\n@shiyuleixia My research team and I discovered that this issue is the result of greedy search in the model (it happens in every other model as well). Luckily it can be detected and resolved relatively easily, as you can see here: https://github.com/ProGamerGov/VLM-Captioning-Tools\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1512,
    "state": "open",
    "created_by": "makemecker",
    "created_at": "2024-05-20T07:10:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1512</URL>\n\n<TITLE>[Question] Clarification on JSON Format for Object Detection Task</TITLE>\n\n<BODY>### Question\n\nHi,\r\n\r\nThank you for the comprehensive guidelines on fine-tuning LLaVA on custom datasets! The provided example for generating tag-style captions for Stable Diffusion has been very helpful.\r\n\r\nI am currently working on fine-tuning LLaVA for object detection tasks and have created the following JSON format:\r\n\r\n    \"image\": \"part-000001/997bb945-628d-4724-b370-b84de974a19f.jpg\",\r\n    \"conversations\": [\r\n      {\r\n        \"from\": \"human\",\r\n        \"value\": \"Find objects of the classes vehicle, human, swimming pool, landfill, building, tree, satellite antenna and street light in the image. Provide the coordinates of the bounding boxes in the format class: [bounding box coordinates].\"\r\n      },\r\n      {\r\n        \"from\": \"gpt\",\r\n        \"value\": \"vehicle: [0.85, 0.139, 0.024, 0.049], vehicle: [0.603, 0.158, 0.029, 0.055], human: [0.945, 0.018, 0.014, 0.036], human: [0.83, 0.028, 0.017, 0.041], human: [0.85, 0.123, 0.024, 0.067].\"\r\n      }\r\n    ]\r\n\r\nCould you please confirm if this JSON structure is correct for fine-tuning LLaVA on object detection tasks? Specifically, I would like to know:\r\n\r\n1. Is the structure of the JSON file appropriate for object detection?\r\n2. Are the metadata fields correctly defined?\r\n3. Is the format for bounding box coordinates accurate?\r\n\r\nAny additional insights or corrections would be greatly appreciated.\r\n\r\nThank you for your assistance!</BODY>\n\n<COMMENTS>\n<Comment by kangISU at 2024-08-21T01:09:42Z>\nHow's the result of this object detection task?\n</Comment>\n<Comment by makemecker at 2024-08-21T12:58:31Z>\n> How's the result of this object detection task?\r\n\r\nI used the information from [this guide](https://wandb.ai/byyoung3/ml-news/reports/How-to-Fine-Tune-LLaVA-on-a-Custom-Dataset--Vmlldzo2NjUwNTc1). However, it doesn't specifically cover object detection tasks, unfortunately.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1511,
    "state": "open",
    "created_by": "RomanIovlev",
    "created_at": "2024-05-19T12:13:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1511</URL>\n\n<TITLE>[Question] LLava API compare two images visually</TITLE>\n\n<BODY>### Question\n\nIs it possible to add images to the LLava question via API to find any visual differences between them?\r\nCan you point for some example?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1510,
    "state": "closed",
    "created_by": "RussRobin",
    "created_at": "2024-05-18T12:52:50Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1510</URL>\n\n<TITLE>LLaVA v1.6 34B can not run</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nCan not use LLaVA v1.6 34B\r\n\r\nCommand:\r\n```\r\n\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.mm_utils import get_model_name_from_path\r\nfrom llava.eval.run_llava import eval_model\r\n\r\nmodel_path = .../ckpt-v1.6-34b\" # I download LLaVA v1.6 34B from hugging face directly: https://huggingface.co/liuhaotian/llava-v1.6-34b\r\n\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    model_path=model_path,\r\n    model_base=None,\r\n    model_name=get_model_name_from_path(model_path)\r\n)\r\n```\r\n\r\nLog: \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \".../miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \".../miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \".../LLaVA/llava/serve/cli.py\", line 126, in <module>\r\n    main(args)\r\n  File \".../LLaVA/llava/serve/cli.py\", line 32, in main\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n  File \".../LLaVA/llava/model/builder.py\", line 142, in load_pretrained_model\r\n    model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\n  File \".../miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 461, in from_pretrained\r\n    config, kwargs = AutoConfig.from_pretrained(\r\n  File \".../miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 998, in from_pretrained\r\n    config_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\r\n  File \".../miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 710, in __getitem__\r\n    raise KeyError(key)\r\nKeyError: 'llava'\r\n```\r\n\r\nPackage version:\r\ntransformers==4.31.0\r\n\r\n\r\n\r\n\r\n\r\nIf i use transformers==4.41.0\r\nThe error would be:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \".../LLaVA/llava/eval/hf-quick-start.py\", line 7, in <module>\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(\r\n  File \".../LLaVA/llava/model/builder.py\", line 142, in load_pretrained_model\r\n    model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\n  File \".../miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 566, in from_pretrained\r\n    raise ValueError(\r\nValueError: Unrecognized configuration class <class 'transformers.models.llava.configuration_llava.LlavaConfig'> for this kind of AutoModel: AutoModelForCausalLM.\r\nModel type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, ElectraConfig, ErnieConfig, FalconConfig, FuyuConfig, GemmaConfig, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, JambaConfig, JetMoeConfig, LlamaConfig, MambaConfig, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, OlmoConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, LlavaConfig, LlavaMptConfig, LlavaMistralConfig.\r\n```</BODY>\n\n<COMMENTS>\n<Comment by itay1542 at 2024-05-22T10:50:05Z>\nMake sure that your model config (config.json file in huggingface) has `model_type` set to 'llava' and not 'llava-llama'\n</Comment>\n<Comment by chrisx599 at 2024-05-22T11:50:26Z>\ni meet same problem, i use transformers==4.37.2, and it return the same error like above screenshot.\r\nand it's llava 1.6 34b, i check the config.json, model_type is \"llava\"\n</Comment>\n<Comment by chrisx599 at 2024-05-23T02:32:41Z>\nI solve the problem, it's not transformers's version problem\r\nthe model weight folder name should keep same like the original name on huggingface\r\n![image](https://github.com/haotian-liu/LLaVA/assets/60588286/2081c410-90e9-424e-9e52-30d61c6b29ec)\r\nbecause I found the function here, the author get model name use the path's last folder name\r\nso when I change the weight folder name \"llava-v1.6-34b\", it worked\n</Comment>\n<Comment by RussRobin at 2024-05-23T02:43:21Z>\nGreat! Millions of thanks to you guys. I'll close this issue.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1509,
    "state": "open",
    "created_by": "seasoncool",
    "created_at": "2024-05-17T13:19:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1509</URL>\n\n<TITLE>[Question] All services can be started, but why is there no reply with any content?</TITLE>\n\n<BODY>### Question\r\n\r\nI have successfully deployed all services, but when I start asking questions in the browser, Gradio does not respond at all. How can I resolve this issue?   any ideas are welcomed.\r\n\r\nI used installation guide from  here : https://github.com/haotian-liu/LLaVA/blob/main/docs/macOS.md\r\n\r\nmacOS：14.4.1\r\nllama3-llava-next-8b\r\n\r\n<img width=\"1700\" alt=\"iShot_2024-05-17_21 09 45\" src=\"https://github.com/haotian-liu/LLaVA/assets/6467295/b2756001-6dd2-4099-b82b-839e2b43751c\">\r\n<img width=\"1707\" alt=\"iShot_2024-05-17_21 10 10\" src=\"https://github.com/haotian-liu/LLaVA/assets/6467295/7b28fb6b-2152-48a4-a893-0e61cc1d679f\">\r\n<img width=\"835\" alt=\"iShot_2024-05-17_21 10 44\" src=\"https://github.com/haotian-liu/LLaVA/assets/6467295/57b3cced-a290-4f7e-9858-5cd3d152ed01\">\r\n<img width=\"1592\" alt=\"iShot_2024-05-17_21 11 10\" src=\"https://github.com/haotian-liu/LLaVA/assets/6467295/9ab3f23f-f49b-409a-a415-af9a36bb47c4\"></BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1508,
    "state": "open",
    "created_by": "FanshuoZeng",
    "created_at": "2024-05-17T07:01:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1508</URL>\n\n<TITLE>[Question] Hello, have you open sourced the code for comparative experiments using the Qwen-VL model?</TITLE>\n\n<BODY>### Question\n\nHello, have you open sourced the code for comparative experiments using the Qwen-VL model?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1507,
    "state": "open",
    "created_by": "NONGFUYULANG",
    "created_at": "2024-05-17T03:56:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1507</URL>\n\n<TITLE>[Question] add extral layers in llava</TITLE>\n\n<BODY>### Question\r\nI have tried to add more linear layer to deal with some extra features. And i only want to train this layers.  and i uses the scripts below to finetune the whole model, but when i tried to print the grad and weight. i found that grad is none and weight does not change. So i want to figure out that:  is this the problem of lora or something else. And i have alread set the require_grads parameter to True in the training.py file. \r\n#### scripts\r\ndeepspeed train.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed /mnt/modified_llavas/LLaVA/scripts/zero3.json \\\r\n    --model_name_or_path liuhaotian/llava-v1.5-7b \\\r\n    --version v2 \\\r\n    --data_path /mnt/f/modified_llavas/LLaVA/llava_out_new.json \\\r\n    --image_folder test \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir /mnt/f/modified_llavas/LLaVA/llava_fullweights \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 5 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 4096 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n#### add extra layers\r\n![image](https://github.com/haotian-liu/LLaVA/assets/41036992/11fab9ef-8be0-405d-a538-a31e394e71fd)\r\nand i have used the test_head layer in the llava_arch.py file.  Does anyone have ideas about it?</BODY>\n\n<COMMENTS>\n<Comment by zihui-debug at 2024-06-01T00:40:54Z>\ncan you provide the forward code and other sections modified?\n</Comment>\n<Comment by WayneTomas at 2024-10-10T09:03:55Z>\nThe grad is None is because the deepspeed split the model to different cards. If you want to obtain the grad, you should use the functions provided in deepspeed it-self to colloct the grad, and the traditional \"tensor.grad\" is not working in deepspeed.\r\n![image](https://github.com/user-attachments/assets/d7972d83-1372-4296-a746-c6bb1f46918e)\n</Comment>\n<Comment by WayneTomas at 2025-03-15T12:10:21Z>\n> > ### Question\n> > I have tried to add more linear layer to deal with some extra features. And i only want to train this layers. and i uses the scripts below to finetune the whole model, but when i tried to print the grad and weight. i found that grad is none and weight does not change. So i want to figure out that: is this the problem of lora or something else. And i have alread set the require_grads parameter to True in the training.py file.\n> > #### scripts\n> > deepspeed train.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed /mnt/modified_llavas/LLaVA/scripts/zero3.json --model_name_or_path liuhaotian/llava-v1.5-7b --version v2 --data_path /mnt/f/modified_llavas/LLaVA/llava_out_new.json --image_folder test --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /mnt/f/modified_llavas/LLaVA/llava_fullweights --num_train_epochs 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 1 --evaluation_strategy \"no\" --save_strategy \"steps\" --save_steps 5 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type \"cosine\" --logging_steps 1 --tf32 True --model_max_length 4096 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb\n> > #### add extra layers\n> > [![image](https://github.com/haotian-liu/LLaVA/assets/41036992/11fab9ef-8be0-405d-a538-a31e394e71fd)](https://private-user-images.githubusercontent.com/41036992/331455287-11fab9ef-8be0-405d-a538-a31e394e71fd.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDIwMjc1ODEsIm5iZiI6MTc0MjAyNzI4MSwicGF0aCI6Ii80MTAzNjk5Mi8zMzE0NTUyODctMTFmYWI5ZWYtOGJlMC00MDVkLWE1MzgtYTMxZTM5NGU3MWZkLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMzE1VDA4MjgwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA1NWExMzZlNGYwYmI1ODIxNmQ2YWRlYmM2YThhYzA5ODk2NmFiMjI1NmI4NDE4NGEwOTEzYmMxMGEyMDVmMTAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.6x-I4DrVUX6A-GNHBr2XLyoRmG5wH9mQfiWaib-oEww) and i have used the test_head layer in the llava_arch.py file. Does anyone have ideas about it?\n> \n> 请问您解决这个问题了吗？我这边在vision_tower中添加可微调的参数，同样面临相同的问题，grad is none ，微调结束后，额外的参数还是和初始化一样没有变化，十分期待您的回答，有偿。\n\nvision_tower添加了可训练参数后，记得检查一下是否把@torch.no_grad()注释了，LLaVA原始的CLIPVisionTower的forward前面有这个装饰器，你肯定得先打开CLIP才能训练你自己的\n</Comment>\n<Comment by LiZhangMing at 2025-03-27T09:22:43Z>\n> > > ### Question\n> > > I have tried to add more linear layer to deal with some extra features. And i only want to train this layers. and i uses the scripts below to finetune the whole model, but when i tried to print the grad and weight. i found that grad is none and weight does not change. So i want to figure out that: is this the problem of lora or something else. And i have alread set the require_grads parameter to True in the training.py file.\n> > > #### scripts\n> > > deepspeed train.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed /mnt/modified_llavas/LLaVA/scripts/zero3.json --model_name_or_path liuhaotian/llava-v1.5-7b --version v2 --data_path /mnt/f/modified_llavas/LLaVA/llava_out_new.json --image_folder test --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /mnt/f/modified_llavas/LLaVA/llava_fullweights --num_train_epochs 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 1 --evaluation_strategy \"no\" --save_strategy \"steps\" --save_steps 5 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type \"cosine\" --logging_steps 1 --tf32 True --model_max_length 4096 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb\n> > > #### add extra layers\n> > > [![image](https://github.com/haotian-liu/LLaVA/assets/41036992/11fab9ef-8be0-405d-a538-a31e394e71fd)](https://private-user-images.githubusercontent.com/41036992/331455287-11fab9ef-8be0-405d-a538-a31e394e71fd.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDIwMjc1ODEsIm5iZiI6MTc0MjAyNzI4MSwicGF0aCI6Ii80MTAzNjk5Mi8zMzE0NTUyODctMTFmYWI5ZWYtOGJlMC00MDVkLWE1MzgtYTMxZTM5NGU3MWZkLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMzE1VDA4MjgwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA1NWExMzZlNGYwYmI1ODIxNmQ2YWRlYmM2YThhYzA5ODk2NmFiMjI1NmI4NDE4NGEwOTEzYmMxMGEyMDVmMTAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.6x-I4DrVUX6A-GNHBr2XLyoRmG5wH9mQfiWaib-oEww) and i have used the test_head layer in the llava_arch.py file. Does anyone have ideas about it?\n> > \n> > \n> > 请问您解决这个问题了吗？我这边在vision_tower中添加可微调的参数，同样面临相同的问题，grad is none ，微调结束后，额外的参数还是和初始化一样没有变化，十分期待您的回答，有偿。\n> \n> vision_tower添加了可训练参数后，记得检查一下是否把@torch.no_grad()注释了，LLaVA原始的CLIPVisionTower的forward前面有这个装饰器，你肯定得先打开CLIP才能训练你自己的\n\nThank you very much for your response. I added some trainable parameters to the ViT during the LoRA fine-tuning phase, and as you suggested, I set vit.enable_input_require_grads() and removed torch.no_grad() in CLIP. The parameters I added are updated, but I have observed that the parameters in the final layer are not updated, while the parameters in the other layers update normally. Why is that?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1506,
    "state": "open",
    "created_by": "avinashsai",
    "created_at": "2024-05-16T23:41:27Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1506</URL>\n\n<TITLE>[Discussion] Question about prompt</TITLE>\n\n<BODY>### Discussion\r\n\r\nHello,\r\n\r\nCongratulations on the amazing work! I have a query about prompts for custom dataset. In the evaluation.md (https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md) there is an example prompt for multiple choice question. Does this prompt has to be appended to the original prompt: \r\n\r\n``\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\" + \"<question>\r\nA. <option_1>\r\nB. <option_2>\r\nC. <option_3>\r\nD. <option_4>\r\nAnswer with the option's letter from the given choices directly.\"``\r\n\r\nMy task is given an image and list of choices, select the most probable choice. More like a image recognition problem.</BODY>\n\n<COMMENTS>\n<Comment by copperwiring at 2024-05-24T10:24:11Z>\nHi. Not the code author but what do you mean by original prompt? You can use the prompt you showed above and get the generated token which will be the predicted option.\n</Comment>\n<Comment by HamzaShaikh17 at 2024-10-09T12:47:27Z>\nValid point @copperwiring. I am also wondering\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1505,
    "state": "open",
    "created_by": "chanangad",
    "created_at": "2024-05-16T20:46:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1505</URL>\n\n<TITLE>Training LLaVA MPT from scratch. Few Questions</TITLE>\n\n<BODY>### Discussion\n\nI noticed that the training scripts for MPT model have been removed and the latest code doesn't support pre-training and training with MPT base model. Was there a reason for removing the training scripts?\r\n\r\nI also wanted to know that which conversation template was finally used for pre-training and instruction-tuning if anyone remembers. Was it the 'plain' template for pre-training and 'mpt' for instruction-tuning or something else?\r\n\r\nAlso was mm_use_im_start_end and mm_use_im_patch_token used in both the stages?\r\n\r\nIt'd be great if someone can help me with this,</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1504,
    "state": "open",
    "created_by": "llv22",
    "created_at": "2024-05-15T19:07:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1504</URL>\n\n<TITLE>[Usage] About finetuning llama 2 with liuhaotian/llava-pretrain-llama-2-7b-chat</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: I try to do visual instruction tuning using the pretrained projector  liuhaotian/llava-pretrain-llama-2-7b-chat. However, got the following issue. I have download the projector from https://huggingface.co/liuhaotian/llava-pretrain-llama-2-7b-chat to ./checkpoints/llava-pretrain-llama-2-7b-chat. According to the guide in https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune.sh and https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md, I think I should use meta-llama/Llama-2-7b-chat-hf during fine-tuning. But I got an issue, please check the details in the logging section.\r\n\r\nCommand:\r\n```\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path meta-llama/Llama-2-7b-chat-hf \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/llava_v1_5_mix665k.json \\\r\n    --image_folder ./playground/data \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava-pretrain-llama-2-7b-chat/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-llama2-7b-finetune \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nLog: \r\n```\r\n2024-05-15 11:48:42.708 ERROR train - global_exception_handler: Uncaught exception Error(s) in loading state_dict for Sequential:\r\n\tMissing key(s) in state_dict: \"0.weight\", \"0.bias\", \"2.weight\", \"2.bias\". \r\n\tUnexpected key(s) in state_dict: \"weight\", \"bias\". \r\nNoneType: None\r\n2024-05-15 11:48:42.708 ERROR train - global_exception_handler: <class 'RuntimeError'>\r\n2024-05-15 11:48:42.708 ERROR train - global_exception_handler: <class 'RuntimeError'>\r\n2024-05-15 11:48:42.709 ERROR train - global_exception_handler: \r\n\t  File \"/data/orlando/workspace/AndroidAgentModelZoo/models/LLaVA_forward/llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"/data/orlando/workspace/AndroidAgentModelZoo/models/LLaVA_forward/llava/train/train.py\", line 1302, in train\r\n    model.get_model().initialize_vision_modules(\r\n  File \"/data/orlando/workspace/AndroidAgentModelZoo/models/LLaVA_forward/llava/model/llava_arch.py\", line 97, in initialize_vision_modules\r\n    self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'))\r\n  File \"/usr/local/anaconda3/envs/agentbackend/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2153, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\n```\r\n\r\nI guessed that may be caused by inconsistency between the model_name_or_path and the referenced model used in projector. However, in the projector's setting, I can only see the model name is ./checkpoints/llama_2/llama-2-7b-chat (https://huggingface.co/liuhaotian/llava-pretrain-llama-2-7b-chat/blob/main/config.json). Could you clarify what llama2 model should I use in  --model_name_or_path?\r\n\r\nPS: For my understanding, the pertaining phase focuses on language and image alignment (feature alignment) so its goal is to train an appropriate projector to map image into language space. Then with this projector, we can fine-tune both language and image to improve task performance. My guess is meta-llama/Llama-2-7b-chat-hf should be OK (it's the converted format from meta's official release llama2), or according to https://github.com/haotian-liu/LLaVA/blob/main/docs/LLaVA_from_LLaMA2.md, I need to download the latest llama2 checkpoints and use it (I try this, but failed, because this format can't be loaded by huggingface API).\r\n\r\n\r\nCurrent follow-up: \r\nNow I'm trying to use meta-llama/Llama-2-7b-chat-hf to pretrain a projector, then follow the fine-tune process. \r\n\r\nCould you clarify which language model I should use for llava-pretrain-llama-2-7b-chat/mm_projector.bin? Correct me if there is anything wrong for my description.\r\n\r\nReally appreciate your help\r\n\r\nOrlando</BODY>\n\n<COMMENTS>\n<Comment by aybora at 2024-05-17T14:14:59Z>\nYou need to change mm_projector_type to linear. mlp2x_gelu is for Vicuna.\n</Comment>\n<Comment by llv22 at 2024-05-17T20:09:20Z>\n@aybora if I want to support llama2 with projector mlp2x_gelu, I need to traint the first phase and get my own projector?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1503,
    "state": "open",
    "created_by": "nomadlx",
    "created_at": "2024-05-15T13:03:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1503</URL>\n\n<TITLE>[Usage] How to run inference for llava-next-72b?</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nHow to run inference for llava-next-72b/llava-next-110b?\r\n\r\nThere are too many versions of your llava, and it seems that the code is not compatible, and there are multiple related repositories, which makes me confused\r\n\r\nhttps://github.com/haotian-liu/LLaVA ：seem that only support llava-1.5？both train and inference？\r\nhttps://github.com/LLaVA-VL/LLaVA-NeXT/tree/inference ：seem support only inference for llava-next and llava-next-video？\r\nhttps://github.com/EvolvingLMMs-Lab/lmms-eval ：whether llava-next-72b is supported？\r\nhttps://github.com/LLaVA-VL/LLaVA-NeXT/blob/inference/docs/LLaVA-NeXT.md ：This seems to give a running example, does it support llava-next-72b/llava-next-110b to run? Are there any code version requirements?</BODY>\n\n<COMMENTS>\n<Comment by gyupro at 2024-05-16T07:46:07Z>\nWhy are there too many repos and have same content? It's really confusing\n</Comment>\n<Comment by Luodian at 2024-05-16T08:11:22Z>\nIt's using same code structure but with different content, you could regard llava-next-72b/110b relies on upgraded llava repo. \r\n\r\nSince it's team efforts, we release it to a team repo. \r\n\r\nFor llava-next-72b inference, use the provided code, thanks!\r\nhttps://github.com/LLaVA-VL/LLaVA-NeXT/blob/inference/docs/LLaVA-NeXT.md \r\n\r\nQAs:\r\n\r\nhttps://github.com/EvolvingLMMs-Lab/lmms-eval ：\r\nQ: whether llava-next-72b is supported？\r\nA: Yes, supported.\r\n\r\nhttps://github.com/LLaVA-VL/LLaVA-NeXT/blob/inference/docs/LLaVA-NeXT.md ：\r\nQ: This seems to give a running example, does it support llava-next-72b/llava-next-110b to run? Are there any code version requirements? \r\nA: Use the repo's provided environment for llava-next-72b/110b. Also expectedly it backward compatible to llava-next-34b and llava 1.5.\n</Comment>\n<Comment by gyupro at 2024-05-16T08:20:25Z>\n@Luodian \r\n\r\nThank you for your team's effort.\r\n\r\nI was wondering how much gpu ram(or how many A100s?) you need to run llava-next 72b and 110b?\r\n\r\nYour team's research helps a lot to the open source community. Thank you !\n</Comment>\n<Comment by Luodian at 2024-05-16T08:22:47Z>\nNever mind! \r\n\r\nWe have a model card to demonstrate this info~\r\n\r\nhttps://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/\n</Comment>\n<Comment by nomadlx at 2024-05-16T13:25:50Z>\n> https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/\r\n\r\nHow should I set `model_name` and `conv_template` for  llava-next-72b/110b?\r\nIs there anything else I should be aware of?\n</Comment>\n<Comment by Luodian at 2024-05-17T02:38:55Z>\nFor llava-next-72b/110b at `lmms-eval`, `conv_template=qwen_1_5`.\r\n\r\nAt slgang, it's slightly different, you can check the `examples/usage` folder at sglang's repo.\n</Comment>\n<Comment by rafaelrdias at 2024-05-19T22:35:16Z>\n> For llava-next-72b/110b at `lmms-eval`, `conv_template=qwen_1_5`.\r\n> \r\n> At slgang, it's slightly different, you can check the `examples/usage` folder at sglang's repo.\r\n\r\nI've trying to find this \"examples/usage\" but I didn't! Can you bring more details here I can find it, please?\n</Comment>\n<Comment by nomadlx at 2024-05-20T04:14:36Z>\n> > For llava-next-72b/110b at `lmms-eval`, `conv_template=qwen_1_5`.\r\n> > At slgang, it's slightly different, you can check the `examples/usage` folder at sglang's repo.\r\n> \r\n> I've trying to find this \"examples/usage\" but I didn't! Can you bring more details here I can find it, please?\r\n\r\nIt works for me, use example in  https://github.com/LLaVA-VL/LLaVA-NeXT/blob/inference/docs/LLaVA-NeXT.md and setting `model_name = \"llava_qwen72b\"`, `conv_template = \"qwen_1_5\"`\n</Comment>\n<Comment by pseudotensor at 2024-05-23T06:32:21Z>\n> It's using same code structure but with different content, you could regard llava-next-72b/110b relies on upgraded llava repo.\r\n> \r\n> Since it's team efforts, we release it to a team repo.\r\n> \r\n> For llava-next-72b inference, use the provided code, thanks! https://github.com/LLaVA-VL/LLaVA-NeXT/blob/inference/docs/LLaVA-NeXT.md\r\n> \r\n> QAs:\r\n> \r\n> https://github.com/EvolvingLMMs-Lab/lmms-eval ： Q: whether llava-next-72b is supported？ A: Yes, supported.\r\n> \r\n> https://github.com/LLaVA-VL/LLaVA-NeXT/blob/inference/docs/LLaVA-NeXT.md ： Q: This seems to give a running example, does it support llava-next-72b/llava-next-110b to run? Are there any code version requirements? A: Use the repo's provided environment for llava-next-72b/110b. Also expectedly it backward compatible to llava-next-34b and llava 1.5.\r\n\r\n@Luodian \r\n\r\nI'm also confused.  I use the llava inference engine stuff (server-worker etc.) but the new repo has none of that.  Is only non-server based stuff supported for new models?\r\n\r\nBy new models, I mean llama-3 based, Qwen based, etc.  Can only llava 1.5-1.6 be ran on server-client platform?\n</Comment>\n<Comment by Luodian at 2024-05-23T06:49:58Z>\n@pseudotensor Let me try to explain and clarify your confusion. \r\n\r\nIn LLaVA's original server demo, it's basically use `sglang` (stream mode) as endpoint model. Although the `sglang_worker.py` is little bit complicated, but the logic of this file is to post a request to sglang endpoint model and get a response in stream mode, and then feedback to `gradio_web_server.py` to display on frontend textbox.\r\n\r\nFor llava-next-72b, we provide the inference code with `sglang` and evaluation with `lmms-eval`.\r\nYou can refer here to see the usage of sglang's `http/srt_runtime`\r\n> https://github.com/sgl-project/sglang/tree/main/examples/usage/llava.\r\n\r\nIn `sglang`, you dont need to install llava since we already write [llava code](https://github.com/sgl-project/sglang/tree/main/python/sglang/srt/models) into it.\r\n\r\nIn `lmms-eval`, you need to install LLaVA-VL/LLaVA-NeXT repo's llava code to make sure you could correctly execute \"from llava.xxx import xxx\". \r\n\r\n> install here means you need to use `pip install -e .` to make sure your current environment have the `llava` package.\r\n\r\nThere may also be some glitches, if it's naming issue or small bugs, you can try to hack and solve it. If there's pretty weird issue, could email or ping me at this thread.\n</Comment>\n<Comment by Luodian at 2024-05-23T06:54:05Z>\nIf you want to execute batch inference with `sglang` (around 5x speed up than vanilla pytorch inference). Roughly seeing it finishes 2000 QAs within 30min at 8xA100-80G.\r\n\r\nI could provide you the code I am constantly using at my side.\r\n```python\r\nimport argparse\r\nimport json\r\nimport time\r\nimport os\r\nimport requests\r\n\r\nimport sglang as sgl\r\nimport tqdm\r\nfrom sglang.test.test_utils import select_sglang_backend\r\nfrom sglang.utils import dump_state_text\r\nfrom sglang.lang.chat_template import get_chat_template\r\n\r\nimport warnings\r\n\r\nwarnings.filterwarnings(\"ignore\")\r\n\r\n\r\n@sgl.function\r\ndef image_description(s, image_url, prompt, stop_words=[]):\r\n    # prompt = \"Please generate detailed descriptions of the given image.\"\r\n    s += sgl.user(sgl.image(image_url) + prompt)\r\n    s += sgl.assistant(sgl.gen(\"answer\", max_tokens=args.max_tokens, temperature=0.7, top_p=1.0, stop=stop_words))\r\n\r\n\r\ndef main(args):\r\n    import multiprocessing as mp\r\n\r\n    mp.set_start_method(\"spawn\", force=True)\r\n    model_name = \"lmms-lab/llava-next-72b\"\r\n    tokenizer_name = \"lmms-lab/llavanext-qwen-tokenizer\"\r\n    template_name = \"chatml-llava\"\r\n    stop_words = [\"<|im_end|>\"]\r\n    runtime = sgl.Runtime(\r\n        model_path=model_name,\r\n        tokenizer_path=tokenizer_name,\r\n        tp_size=8,\r\n    )\r\n    runtime.endpoint.chat_template = get_chat_template(template_name)\r\n    # Select backend\r\n    sgl.set_default_backend(runtime)\r\n    print(f\"chat template: {runtime.endpoint.chat_template.name}\")\r\n\r\n    file_list = [\r\n        \"xxxxxxxxxxxxxxxx.json\"\r\n    ]\r\n\r\n    image_path_root_list = [\r\n        \"xxxxxxxxxxxxxxxx\"\r\n    ]\r\n\r\n    total_annotations = []\r\n    \r\n    for file, image_path_root in zip(file_list, image_path_root_list):\r\n        with open(file, \"r\") as f:\r\n            queries = json.load(f)\r\n\r\n        tic = time.time()\r\n        batch_size = 16\r\n        annotations = []\r\n        idx = 0\r\n        print(f\"Start processing {file}, {idx} / {len(file_list)}\")\r\n        for batch_start in tqdm.tqdm(range(0, len(queries), batch_size)):\r\n            idx += 1\r\n\r\n            batch_end = min(batch_start + batch_size, len(queries))\r\n            batch_queries = queries[batch_start:batch_end]\r\n            # check if each image exists\r\n            actual_batch_queries = []\r\n            for query in batch_queries:\r\n                image_path = os.path.join(image_path_root, query[\"image\"])\r\n                if os.path.exists(image_path):\r\n                    actual_batch_queries.append(query)\r\n\r\n            batch_arguments = []\r\n\r\n            for query in actual_batch_queries:\r\n                image_path = os.path.join(image_path_root, query[\"image\"])\r\n                question_id = query[\"question_id\"]\r\n                question = query[\"question\"]\r\n                if \"<image>\" not in question:\r\n                    question = f\"<image>\\n{question}. Please carefully inspect this image and answer with a mid-length response.\"\r\n                batch_arguments.append({\"image_url\": image_path, \"prompt\": question, \"stop_words\": stop_words})\r\n\r\n            batch_results = image_description.run_batch(batch_arguments, temperature=0, num_threads=batch_size, progress_bar=False)\r\n\r\n            for result, query in zip(batch_results, actual_batch_queries):\r\n                model_response = result.text().split(\"assistant\")[-1].replace(stop_words[0], \"\").replace(\"<|end_header_id|>\", \"\").strip()\r\n                # print(f\"############## Model response ################\\n{model_response}\\n############## End ################\")\r\n                annotations.append(\r\n                    {\r\n                        \"image_path\": query[\"image\"],\r\n                        \"question_id\": query[\"question_id\"],\r\n                        \"question\": query[\"question\"],\r\n                        \"model_response\": model_response,\r\n                        \"model_name\": model_name,\r\n                        \"tokenizer_name\": tokenizer_name,\r\n                        \"template\": runtime.endpoint.chat_template.name,\r\n                    }\r\n                )\r\n\r\n        latency = time.time() - tic\r\n        print(f\"Latency: {latency:.3f}\")\r\n\r\n        task_name = file.split(\"/\")[-2]\r\n        result_file = file.replace(\".json\", f\"_{model_name.split('/')[-1]}_{task_name}_response.json\")\r\n        print(f\"Write output to {result_file}\")\r\n        with open(result_file, \"w\") as fout:\r\n            json.dump(annotations, fout, indent=4, ensure_ascii=False)\r\n\r\n        total_annotations.extend(annotations)\r\n\r\n    total_annotations_file = \"./total_response.json\"\r\n    with open(total_annotations_file, \"w\") as fout:\r\n        json.dump(total_annotations, fout, indent=4, ensure_ascii=False)\r\n\r\n    runtime.shutdown()\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--max_tokens\", type=int, default=1024)\r\n    parser.add_argument(\"--backend\", type=str, default=\"srt\")\r\n    parser.add_argument(\"--model_name\", type=str, default=\"llava_next_34b\")\r\n    parser.add_argument(\"--host\", type=str, default=\"http://127.0.0.1\")\r\n    parser.add_argument(\"--port\", type=int, default=30000)\r\n    parser.add_argument(\"--result_file\", type=str, default=\"./testmini_model_response.json\")\r\n    args = parser.parse_args()\r\n    main(args)\r\n```\n</Comment>\n<Comment by pseudotensor at 2024-05-23T07:00:29Z>\nThanks!  So you do recommend that if I wanted a server-client setup that doesn't use gradio, I should write a FastAPI wrapper around the kind of code you shared above that involves sglang.  So sglang + fastapi would be cleanest?  I should no longer try to use the non-sglang server-worker stuff?\r\n\r\nIn past it was recommended to use the server-worker model but *also* use sglang with that.\r\n\r\nI ask because there's alot of code in the gradio stuff that handles various things related to each model, that is not really needed when using sglang?\r\n\r\nNormally I launch like:\r\n```\r\nsource ~/miniconda3/etc/profile.d/conda.sh\r\nconda activate llava\r\necho \"First conda env: $CONDA_DEFAULT_ENV\"\r\n\r\nexport server_port=10000\r\n\r\nif [ 1 -eq 1 ]\r\n   then\r\npython -m llava.serve.controller --host 0.0.0.0 --port $server_port &> 1.log &\r\nfi\r\n\r\nif [ 1 -eq 1 ]\r\n   then\r\nexport CUDA_VISIBLE_DEVICES=1\r\nexport worker_port=40000\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://$IP:$server_port --port $worker_port --worker http://$IP:$worker_port --model-path liuhaotian/llava-v1.6-vicuna-13b --limit-model-concurrency 5 &> 2.log &\r\nfi\r\n\r\nif [ 1 -eq 1 ]\r\n   then\r\nexport CUDA_VISIBLE_DEVICES=3\r\nexport worker_port=40002\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://$IP:$server_port --port $worker_port --worker http://$IP:$worker_port --model-path liuhaotian/llava-v1.6-34b --limit-model-concurrency 5 &>> 34b.log &\r\nfi\r\n\r\nsleep 60\r\nif [ 1 -eq 1 ]\r\n   then\r\n  GRADIO_SERVER_PORT=7860 python -m llava.serve.gradio_web_server --controller http://$IP:$server_port --model-list-mode once &>> 3b2.log &\r\nfi\r\n```\r\n\r\nCan this server-worker-gradio stuff no longer be used with newer llava models?\n</Comment>\n<Comment by Luodian at 2024-05-23T07:16:21Z>\nYes, if you only want to init a model that supports API request. Like you are providing API service or having a totally disentangled frontend that sends request to backend server, I think you should refer the `http` usage of sglang. \r\n\r\nThat's the cleanest way. Personally I tried it, using cloudflared and can successfully host a backend service that supports other side using API to evaluate `llava-next-72b`. \r\n\r\nIf your scenarios are close, then should definitely use `FastAPI + SGLang`.\n</Comment>\n<Comment by Luodian at 2024-05-23T07:25:26Z>\n> > For llava-next-72b/110b at `lmms-eval`, `conv_template=qwen_1_5`.\r\n> > At slgang, it's slightly different, you can check the `examples/usage` folder at sglang's repo.\r\n> \r\n> I've trying to find this \"examples/usage\" but I didn't! Can you bring more details here I can find it, please?\r\n\r\nSorry for missing this message, the code is (only inference, not with eval) here: https://github.com/sgl-project/sglang/tree/main/examples/usage/llava.\n</Comment>\n<Comment by pseudotensor at 2024-05-23T07:30:12Z>\n@Luodian Thanks.\r\n\r\nWhat about the sglang's own OpenAI API?  Does that work for vision models like llava too?\r\n\r\nhttps://github.com/sgl-project/sglang?tab=readme-ov-file#openai-compatible-api\r\n\r\nThey don't seem to document how that would work for vision models.\n</Comment>\n<Comment by Luodian at 2024-05-23T07:31:32Z>\nSeems it's not ready for it.\n</Comment>\n<Comment by pseudotensor at 2024-05-23T08:25:50Z>\nThanks.  So by http I think you mean this right?\r\n\r\nhttps://github.com/sgl-project/sglang/blob/main/test/srt/test_httpserver_llava.py\r\n\r\nSeems to support image.\r\n\r\nBut unsure if only as url or can be byte stream or needs to be markdown url like or what.  Not clear.\r\n\r\nThat example also assumes the file is on the same host as server and client, which is not probably usual.\n</Comment>\n<Comment by Luodian at 2024-05-23T09:51:13Z>\n> Thanks. So by http I think you mean this right?\r\n> \r\n> https://github.com/sgl-project/sglang/blob/main/test/srt/test_httpserver_llava.py\r\n> \r\n> Seems to support image.\r\n> \r\n> But unsure if only as url or can be byte stream or needs to be markdown url like or what. Not clear.\r\n> \r\n> That example also assumes the file is on the same host as server and client, which is not probably usual.\r\n\r\nActually it's here: https://github.com/sgl-project/sglang/tree/main/examples/usage/llava.\r\n\r\nThe later is my implementation with reference to the former code. They are basically the same but later is for llava-next-72b and applies qwen template.\n</Comment>\n<Comment by pseudotensor at 2024-05-23T10:04:20Z>\nGot it, cool.\r\n\r\nBut that also uses a url.  Does the server support direct binary bytes like one would send inside markdown?\r\n\r\ne.g.\r\n```\r\n![Hello World]data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEYAAAAUCAAAAAAVAxSkAAABrUlEQVQ4y+3TPUvDQBgH8OdDOGa+oUMgk2MpdHIIgpSUiqC0OKirgxYX8QVFRQRpBRF8KShqLbgIYkUEteCgFVuqUEVxEIkvJFhae3m8S2KbSkcFBw9yHP88+eXucgH8kQZ/jSm4VDaIy9RKCpKac9NKgU4uEJNwhHhK3qvPBVO8rxRWmFXPF+NSM1KVMbwriAMwhDgVcrxeMZm85GR0PhvGJAAmyozJsbsxgNEir4iEjIK0SYqGd8sOR3rJAGN2BCEkOxhxMhpd8Mk0CXtZacxi1hr20mI/rzgnxayoidevcGuHXTC/q6QuYSMt1jC+gBIiMg12v2vb5NlklChiWnhmFZpwvxDGzuUzV8kOg+N8UUvNBp64vy9q3UN7gDXhwWLY2nMC3zRDibfsY7wjEkY79CdMZhrxSqqzxf4ZRPXwzWJirMicDa5KwiPeARygHXKNMQHEy3rMopDR20XNZGbJzUtrwDC/KshlLDWyqdmhxZzCsdYmf2fWZPoxCEDyfIvdtNQH0PRkH6Q51g8rFO3Qzxh2LbItcDCOpmuOsV7ntNaERe3v/lP/zO8yn4N+yNPrekmPAAAAAElFTkSuQmCC\r\n```\n</Comment>\n<Comment by Luodian at 2024-05-23T10:06:19Z>\nYou can try and check the sglang interface if it supports base64 as \"image_data\" (I guess it supports). Otherwise could using another function to get the base64 image and save it to local instance as a temp file and then send to sglang endpoint.\n</Comment>\n<Comment by pseudotensor at 2024-05-23T10:08:30Z>\nOk will do.\r\n\r\nIf had to go through file, then server and client couldn't be on different disks/systems.  It would be ok to use fastapi wrapper or something to manage, but one will hope sglang supports directly.\n</Comment>\n<Comment by Luodian at 2024-05-23T10:11:09Z>\nI think I found that, yea it supports\r\n\r\nhttps://github.com/sgl-project/sglang/issues/212\n</Comment>\n<Comment by pseudotensor at 2024-05-23T10:14:00Z>\n@Luodian Thanks for all your amazing help!  Will try this all out tomorrow (PST here, it's late).\n</Comment>\n<Comment by nomadlx at 2024-05-24T02:24:25Z>\n> It's using same code structure but with different content, you could regard llava-next-72b/110b relies on upgraded llava repo.\r\n> \r\n> Since it's team efforts, we release it to a team repo.\r\n> \r\n> For llava-next-72b inference, use the provided code, thanks! https://github.com/LLaVA-VL/LLaVA-NeXT/blob/inference/docs/LLaVA-NeXT.md\r\n> \r\n> QAs:\r\n> \r\n> https://github.com/EvolvingLMMs-Lab/lmms-eval ： Q: whether llava-next-72b is supported？ A: Yes, supported.\r\n> \r\n> https://github.com/LLaVA-VL/LLaVA-NeXT/blob/inference/docs/LLaVA-NeXT.md ： Q: This seems to give a running example, does it support llava-next-72b/llava-next-110b to run? Are there any code version requirements? A: Use the repo's provided environment for llava-next-72b/110b. Also expectedly it backward compatible to llava-next-34b and llava 1.5.\r\n\r\nA slightly off-topic question, can I use the [LLaVA v1.5 code](https://github.com/haotian-liu/LLaVA?tab=readme-ov-file#train) to finetune the LLAVA-NEXT-72b /110b model? Use it directly or just modify it very simply, or do I need to rely on subsequent upgrades? \r\nIf rely on subsequent upgrade, when to be able to see it?\n</Comment>\n<Comment by pseudotensor at 2024-05-24T02:25:12Z>\n@Luodian Everything worked perfectly for sglang and llama3 llava 8b model.\r\n\r\nHaving trouble with qwen based models, related to the discussion here:\r\n\r\nhttps://github.com/sgl-project/sglang/issues/467\r\n\r\nValueError: Unsupported architectures: LlavaQwenForCausalLM\r\n\r\nI guess not supported yet.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1501,
    "state": "open",
    "created_by": "berry-ding",
    "created_at": "2024-05-13T07:54:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1501</URL>\n\n<TITLE>[Question] LLaVA-Next (Stronger) checkpoint 404 error</TITLE>\n\n<BODY>### Question\n\nHi, great work! I want to download the llava-next stronger checkpoint, but the website (https://huggingface.co/collections/lmms-lab/llava-next-6623288e2d61edba3ddbf5ff) meets 404 Error.</BODY>\n\n<COMMENTS>\n<Comment by StonecutterX at 2024-05-15T02:25:15Z>\nIt seems that u can find all models here: https://huggingface.co/lmms-lab  (github: https://github.com/LLaVA-VL/LLaVA-NeXT)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1500,
    "state": "open",
    "created_by": "SWHL",
    "created_at": "2024-05-10T08:48:45Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1500</URL>\n\n<TITLE>[Question] About TextVQA evaluation</TITLE>\n\n<BODY>### Question\r\n\r\nI am confused about the TextVQA evaluation. \r\nBecause when working in accordance with [mPLUG-DocOwl1.5](http://arxiv.org/abs/2311.18248), TextVQA's metric results need to be submitted to [the official website](https://eval.ai/web/challenges/challenge-page/874/overview) to obtain the corresponding metric results.\r\n\r\n\r\n<img width=\"1144\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/28639377/3af62847-1981-4087-b8ca-acf7f29c6080\">\r\n\r\n\r\nWhat is the difference between [the following direct evaluation TextVQA](https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md#textvqa) and [the TextVQA evaluation set](https://eval.ai/web/challenges/challenge-page/874/overview) that needs to be submitted?\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/28639377/576f7c88-e4ef-4d49-89c1-f9e5ad2b21af)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1499,
    "state": "open",
    "created_by": "Mikael17125",
    "created_at": "2024-05-10T07:47:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1499</URL>\n\n<TITLE>[Question] Minimum Memory for Fine Tune LLaVA 1.5 7B without LoRA</TITLE>\n\n<BODY>### Question\n\nHi, I want to ask, how much memory do I need to fine-tune the LLaVA 1.5 7B without LoRA? I tested it using 4xA6000(48GB) with batch size = 1, but I encountered out-of-memory (OOM).\r\n\r\nThank you.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1498,
    "state": "open",
    "created_by": "JessePrince",
    "created_at": "2024-05-10T07:34:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1498</URL>\n\n<TITLE>Is this a bug ?</TITLE>\n\n<BODY>### Question\n\n```python\r\ndef get_peft_state_maybe_zero_3(named_params, bias):\r\n    if bias == \"none\":  # no bias mode, only returns lora weights\r\n        to_return = {k: t for k, t in named_params if \"lora_\" in k}\r\n        # Filters and returns only the parameters that include \"lora_\" in their names.\r\n    elif bias == \"all\":  # all bias mode, return all biases\r\n        to_return = {k: t for k, t in named_params if \"lora_\" in k or \"bias\" in k}\r\n        # Filters and returns parameters that include either \"lora_\" or \"bias\" in their names.\r\n    elif bias == \"lora_only\":  # return biases only from lora\r\n        to_return = {}\r\n        maybe_lora_bias = {}\r\n        lora_bias_names = set()\r\n        for k, t in named_params:\r\n            if \"lora_\" in k:\r\n                to_return[k] = t  # store lora weight\r\n                bias_name = k.split(\"lora_\")[0] + \"bias\"  # store lora module's name\r\n                lora_bias_names.add(bias_name)\r\n            elif \"bias\" in k:\r\n                maybe_lora_bias[k] = t  # temporally store all biases\r\n      # ----------------------------------------------------------\r\n        for k, t in maybe_lora_bias:\r\n            if bias_name in lora_bias_names:  # check the names\r\n                to_return[bias_name] = t\r\n      # -----------------------------------------------------------\r\n    else:\r\n        raise NotImplementedError\r\n    to_return = {k: maybe_zero_3(v, ignore_status=True) for k, v in to_return.items()}\r\n    return to_return\r\n```\r\n\r\nThe code inside the #---# box, shouldn't it be\r\n```python\r\nfor k, t in maybe_lora_bias:\r\n    if k in lora_bias_names:  # check the names\r\n        to_return[k] = t\r\n```\r\nor\r\n```python\r\nfor bias_name, t in maybe_lora_bias:\r\n    if bias_name in lora_bias_names:  # check the names\r\n        to_return[bias_name] = t\r\n```</BODY>\n\n<COMMENTS>\n<Comment by JJJYmmm at 2024-05-30T07:16:34Z>\n+1 https://github.com/haotian-liu/LLaVA/commit/418a53c8b7d283291ea383a9d4412f0403a2fd64\r\n\r\nPS: I found many MLLMs project like qwen-vl/ferret also use or copy this function hhh.\n</Comment>\n<Comment by JessePrince at 2024-05-30T07:39:35Z>\n> +1 [418a53c](https://github.com/haotian-liu/LLaVA/commit/418a53c8b7d283291ea383a9d4412f0403a2fd64)\r\n> \r\n> PS: I found many MLLMs project like qwen-vl/ferret also use or copy this function hhh.\r\n\r\nYeah, I can see this function is originally created from somewhere else and used by many open source projects. But like no one did code review😂\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1497,
    "state": "open",
    "created_by": "zmf2022",
    "created_at": "2024-05-10T07:00:21Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1497</URL>\n\n<TITLE>[Question] The results of the local model are inconsistent with the web ui in the demo</TITLE>\n\n<BODY>### Question\n\nMay I ask, based on the same test data, using the llava: 34b-v1.6-q3_K_S model locally and the llava v1.6-34b model for web UI in the demo, there is a significant difference in the results obtained between the two. The results in web UI are significantly better than those in the local environment. What is the reason for this?\r\nThe problem is: Which country recorded the highest death rates due to outdoor air pollution over the years?\r\nThe result in the web UI is: The graph you've provided shows the outdoor air pollution death rate for several countries from 2005 to 2012. The country with the highest death rate due to outdoor air pollution over the years shown on the graph is Myanmar. The line representing Myanmar's death rate is the highest among the countries depicted, indicating a higher number of deaths attributed to outdoor air pollution per 100,000 people.\r\nThe local result is：India recorded the highest death rates due to outdoor air pollution over the years according to this chart\r\n![Uploading 00795994017065.png…]()</BODY>\n\n<COMMENTS>\n<Comment by avalonliberty at 2024-05-12T01:25:34Z>\nI suffered the issue here. The web demo version ran this \"llava-v1.6-34b\" version of llava. Given the \"34b-v1.6\" version of model in Ollama, the results of queries into these two models are significantly different as well. The one in the web demo outperformed the one in ollama. May I know which tag in Ollama can match the performance in the web demo?\n</Comment>\n<Comment by ChristianWeyer at 2024-05-17T09:46:56Z>\nI am seeing the exact same thing.\r\n\r\nCurrently, I am running Llava:34b-V1.6 in Ollama, and it just does not recognize things in a form and hallucinates values - whereas the HF-hosted Llava 34b (https://llava.hliu.cc) does a great job.\r\n\r\nWho should we ping here? .cc @haotian-liu \r\nThanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1496,
    "state": "open",
    "created_by": "orrzohar",
    "created_at": "2024-05-09T23:59:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1496</URL>\n\n<TITLE>[Usage] Continue training from pre-trained checkpoint</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\nresuming training from pre-tained model (sudden quit)\r\n\r\n```\r\n\r\nLog: \r\n```\r\nLast 10 lines of StdErr:\r\n  File \"/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"//train/train.py\", line 1295, in train\r\n    trainer.train(resume_from_checkpoint=True)\r\n  File \"/transformers/trainer.py\", line 1850, in train\r\n    state = TrainerState.load_from_json(os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME))\r\n  File \"/transformers/trainer_callback.py\", line 148, in load_from_json\r\n    with open(json_path, \"r\", encoding=\"utf-8\") as f:\r\nFileNotFoundError: [Errno 2] No such file or directory: './work_dirs/llava/checkpoint-1000/trainer_state.json'\r\n```\r\n\r\nIt seems that the model does not save the trainer_state.json during pre-training. is there a way to include this so it would be possible to resume training?</BODY>\n\n<COMMENTS>\n<Comment by ashmalvayani at 2024-05-22T10:16:47Z>\nEven if you add trainer_state.json file, it will not resume as it will ask for optimizer files and .pth files which still won't be saved. I think the best way is to comment out their function and simply keep their \"super(LlaVaTrainer, self) ... \" line and let the code run. I have tested this, it does not save the mm_projector.bin file at each stage but it does save the entire weights at each checkpoint.\r\n\r\nYou can either manually extract the mm_projector weights later. If you don't want to do this, don't worry, at the end of training it automatically saves the trainer_state.json, mm_projector.bin and config.json file after the completion of last step.\n</Comment>\n<Comment by rayluo88 at 2024-07-04T07:59:18Z>\n> Even if you add trainer_state.json file, it will not resume as it will ask for optimizer files and .pth files which still won't be saved. I think the best way is to comment out their function and simply keep their \"super(LlaVaTrainer, self) ... \" line and let the code run. I have tested this, it does not save the mm_projector.bin file at each stage but it does save the entire weights at each checkpoint.\r\n> \r\n> You can either manually extract the mm_projector weights later. If you don't want to do this, don't worry, at the end of training it automatically saves the trainer_state.json, mm_projector.bin and config.json file after the completion of last step.\r\n\r\nHi,\r\n\r\nHow to manually extract the mm_projector weights?\n</Comment>\n<Comment by wanlipeng at 2024-08-22T01:51:52Z>\nIs there a better way to do this? Can I resumingtraining and save the \"trainer_state.json\" in each training step?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1495,
    "state": "open",
    "created_by": "Jeremy-lf",
    "created_at": "2024-05-09T08:51:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1495</URL>\n\n<TITLE>Issue about pretraining[return code = -8 ], anyone can help me?</TITLE>\n\n<BODY>### Question\r\n\r\nwhen i train pretraining, i meet the following probelm, there is no obvious tips, how should i solve it?\r\n![image](https://github.com/haotian-liu/LLaVA/assets/45171399/0f5000e9-5c8c-41eb-808a-842df69f66e9)\r\n\r\n\r\nENVS: A800*8, cuda11.6\r\n\r\nmy train script like this: \r\n\r\nsh scripts/v1_5/pretrain.sh \r\n```\r\n#!/bin/bash\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --model_name_or_path vicuna-7b-v1.3 \\\r\n    --version plain \\\r\n    --data_path /root/paddlejob/workspace/env_run/xx/llava/LLaVA/data/llava-v1.5-7b/blip_laion_cc_sbu_558k.json \\\r\n    --image_folder /root/paddlejob/workspace/env_run/xx/llava/LLaVA/data/llava-v1.5-7b/images \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-13b-pretrain \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 32 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 24000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 1e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n```\r\n\r\nerror like this:\r\n\r\n[2024-05-09 15:33:07,293] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-05-09 15:33:09,662] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\n[2024-05-09 15:33:09,662] [INFO] [runner.py:571:main] cmd = /root/paddlejob/workspace/env_run/anaconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero2.json --model_name_or_path vicuna-7b-v1.3 --version plain --data_path /root/paddlejob/workspace/env_run/lvfeng/llava/LLaVA/data/llava-v1.5-7b/blip_laion_cc_sbu_558k.json --image_folder /root/paddlejob/workspace/env_run/lvfeng/llava/LLaVA/data/llava-v1.5-7b/images --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --tune_mm_mlp_adapter True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir ./checkpoints/llava-v1.5-13b-pretrain --num_train_epochs 1 --per_device_train_batch_size 32 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 24000 --save_total_limit 1 --learning_rate 1e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True\r\n\r\n[2024-05-09 15:33:10,897] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-05-09 15:33:13,106] [INFO] [launch.py:138:main] 0 NCCL_IB_GID_INDEX=3\r\n[2024-05-09 15:33:13,106] [INFO] [launch.py:138:main] 0 NCCL_IB_DISABLE=0\r\n[2024-05-09 15:33:13,106] [INFO] [launch.py:138:main] 0 NCCL_IB_CONNECT_RETRY_CNT=15\r\n[2024-05-09 15:33:13,106] [INFO] [launch.py:138:main] 0 NCCL_IB_TIMEOUT=22\r\n[2024-05-09 15:33:13,106] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.7.8\r\n[2024-05-09 15:33:13,106] [INFO] [launch.py:138:main] 0 NCCL_IB_CUDA_SUPPORT=0\r\n[2024-05-09 15:33:13,106] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=0\r\n[2024-05-09 15:33:13,106] [INFO] [launch.py:138:main] 0 NCCL_IB_QPS_PER_CONNECTION=8\r\n[2024-05-09 15:33:13,106] [INFO] [launch.py:138:main] 0 NCCL_DEBUG_SUBSYS=INIT,ENV,GRAPH\r\n[2024-05-09 15:33:13,106] [INFO] [launch.py:138:main] 0 NCCL_DEBUG=INFO\r\n[2024-05-09 15:33:13,106] [INFO] [launch.py:138:main] 0 NCCL_SOCKET_IFNAME=xgbe0\r\n[2024-05-09 15:33:13,106] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\r\n[2024-05-09 15:33:13,106] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0\r\n[2024-05-09 15:33:13,106] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\r\n[2024-05-09 15:33:13,106] [INFO] [launch.py:163:main] dist_world_size=8\r\n[2024-05-09 15:33:13,106] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\r\n[2024-05-09 15:33:17,512] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-05-09 15:33:17,649] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-05-09 15:33:17,692] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2024-05-09 15:33:17,761] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-05-09 15:33:17,771] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-05-09 15:33:17,835] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2024-05-09 15:33:17,852] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-05-09 15:33:17,948] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2024-05-09 15:33:17,948] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n[2024-05-09 15:33:17,956] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2024-05-09 15:33:17,992] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-05-09 15:33:18,037] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2024-05-09 15:33:18,046] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-05-09 15:33:18,048] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-05-09 15:33:18,178] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2024-05-09 15:33:18,234] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2024-05-09 15:33:18,239] [INFO] [comm.py:637:init_distributed] cdb=None\r\nYou are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')\r\n\r\nYou are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.57s/it]\r\nYou are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\r\nFormatting inputs...Skip in lazy mode\r\nNCCL version 2.19.3+cuda12.3\r\nNCCL version 2.19.3+cuda12.3\r\nwandb: Tracking run with wandb version 0.16.4\r\nwandb: W&B syncing is set to `offline` in this directory.  \r\nwandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\r\n[2024-05-09 15:34:25,193] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 83400\r\n[2024-05-09 15:34:25,584] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 83401\r\n[2024-05-09 15:34:25,587] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 83402\r\n[2024-05-09 15:34:25,590] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 83403\r\n[2024-05-09 15:34:26,659] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 83404\r\n[2024-05-09 15:34:26,662] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 83405\r\n[2024-05-09 15:34:26,662] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 83406\r\n[2024-05-09 15:34:26,664] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 83407\r\n[2024-05-09 15:34:26,666] [ERROR] [launch.py:321:sigkill_handler] ['/root/paddlejob/workspace/env_run/anaconda3/envs/llava/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=7', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', 'vicuna-7b-v1.3', '--version', 'plain', '--data_path', '/root/paddlejob/workspace/env_run/lvfeng/llava/LLaVA/data/llava-v1.5-7b/blip_laion_cc_sbu_558k.json', '--image_folder', '/root/paddlejob/workspace/env_run/lvfeng/llava/LLaVA/data/llava-v1.5-7b/images', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--tune_mm_mlp_adapter', 'True', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b-pretrain', '--num_train_epochs', '1', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '24000', '--save_total_limit', '1', '--learning_rate', '1e-3', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True'] exits with return code = -8</BODY>\n\n<COMMENTS>\n<Comment by RyanJJP at 2025-01-05T10:47:17Z>\nhi, have you solve the problem？ I met the same problem with you.\n</Comment>\n<Comment by My-captain at 2025-03-14T12:51:32Z>\n@RyanJJP @Jeremy-lf \nif you are using H20 GPUs, just update the nvidia-cublas-cu12 to 12.4.5.8\n</Comment>\n<Comment by MassEast at 2025-04-09T09:00:06Z>\nI am using A100s, and I sadly don't know how to fix it. I am facing `return code = -7`. Anyone got an idea how I can backtrack what the `-7` stands for?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1494,
    "state": "open",
    "created_by": "GHSADAF",
    "created_at": "2024-05-08T20:02:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1494</URL>\n\n<TITLE>[Question] confusion on finetuning</TITLE>\n\n<BODY>### Question\r\n\r\nI have a custom dataset that I want to finetune LLaVA on. I was wondering is this the format my dataset should have for finetuning LLaVA? and if so, how the answer for \"value\" for \"gpt\" should be provided if these values are LLaVA generated answers? or these are ground-truth answers?\r\n\r\n```\r\n[\r\n  {\r\n    \"id\": \"997bb945-628d-4724-b370-b84de974a19f\",\r\n    \"image\": \"part-000001/997bb945-628d-4724-b370-b84de974a19f.jpg\",\r\n    \"conversations\": [\r\n      {\r\n        \"from\": \"human\",\r\n        \"value\": \"<image>\\nWrite a prompt for Stable Diffusion to generate this image.\"\r\n      },\r\n      {\r\n        \"from\": \"gpt\",\r\n        \"value\": \"a beautiful painting of chernobyl by nekro, pascal blanche, john harris, greg rutkowski, sin jong hun, moebius, simon stalenhag. in style of cg art. ray tracing. cel shading. hyper detailed. realistic. ue 5. maya. octane render. \"\r\n      },\r\n    ]\r\n  },\r\n  ...\r\n]\r\n```</BODY>\n\n<COMMENTS>\n<Comment by pedramaghazadeh at 2024-05-15T00:12:16Z>\nThe value by gpt is your ground-truth answer.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1493,
    "state": "open",
    "created_by": "wuwu-C",
    "created_at": "2024-05-08T07:38:45Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1493</URL>\n\n<TITLE>[Question] Why I got nothing when I tested my lora finetune model</TITLE>\n\n<BODY>### Question\n\n1.I  use finetune_lora to finetune the model for 3 epochs, and I modified the save code to ensure every epoch will save non_lora_trainable.bin\r\n2. I merge lora weight for every epoch\r\n3. I tested on model_vqa but my output tensor is null\r\n![image](https://github.com/haotian-liu/LLaVA/assets/96038200/598554ec-eea6-456f-8f9d-172c91256ccc)</BODY>\n\n<COMMENTS>\n<Comment by Vignesh-Valaboju at 2024-05-08T18:16:36Z>\nAre your projector weights changing after every epoch?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1492,
    "state": "open",
    "created_by": "wenyisir",
    "created_at": "2024-05-07T16:32:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1492</URL>\n\n<TITLE>[Usage] errors when restore checkpoint using lora finetuning</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nWhen using LoRA fine-tuning to restore from a checkpoint, an error occurs, while there is no issue when not using LoRA fine-tuning to restore from a checkpoint. Can you explain why? How should I modify to save more parameters?\r\n![image](https://github.com/haotian-liu/LLaVA/assets/65152684/73baef59-feae-4b3e-8b64-ee49ba2395a2)\r\n\r\n\r\nCommand:\r\n```\r\n/home/wyxu/miniconda3/envs/llava/bin/deepspeed --master_port 25675 \\\r\n          --include localhost:3,4,5,6 \\\r\n          /home/wyxu/LLaVA/llava/train/train_mem.py \\\r\n          --lora_enable True \\\r\n          --deepspeed /home/wyxu/LLaVA/scripts/zero2.json \\\r\n          --model_name_or_path /data/wyxu/LLaVA/checkpoints/vicuna-7b-v1.3 \\\r\n          --version v1 \\\r\n          --data_path /data/wyxu/MIC_sampled/data/ \\\r\n          --image_folder /data/wyxu/MIC_sampled/data/ \\\r\n          --vision_tower /data/wyxu/LLaVA/checkpoints/clip-vit-large-patch14 \\\r\n          --pretrain_mm_mlp_adapter /data/wyxu/LLaVA/checkpoints/llava-vicuna-7b-v1.3-pretrain/mm_projector.bin \\\r\n          --mm_vision_select_layer -2 \\\r\n          --mm_use_im_start_end False \\\r\n          --mm_use_im_patch_token False \\\r\n          --bf16 True \\\r\n          --output_dir /data/wyxu/LLaVA/checkpoints/llava-vicuna-7b-v1.3-finetune-on-mic_sampled-lora \\\r\n          --num_train_epochs 10 \\\r\n          --per_device_train_batch_size 4 \\\r\n          --per_device_eval_batch_size 1 \\\r\n          --gradient_accumulation_steps 4 \\\r\n          --evaluation_strategy no \\\r\n          --save_strategy steps \\\r\n          --save_steps 90 \\\r\n          --save_total_limit 1 \\\r\n          --learning_rate 2e-5 \\\r\n          --weight_decay 0. \\\r\n          --warmup_ratio 0.03 \\\r\n          --lr_scheduler_type cosine \\\r\n          --logging_steps 1 \\\r\n          --tf32 True \\\r\n          --model_max_length 2048 \\\r\n          --gradient_checkpointing True \\\r\n          --dataloader_num_workers 4 \\\r\n          --lazy_preprocess True \\\r\n          --report_to wandb\r\n```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/wyxu/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"/home/wyxu/LLaVA/llava/train/train.py\", line 1037, in train\r\n    trainer.train(resume_from_checkpoint=True)\r\n  File \"/home/wyxu/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(File \"/home/wyxu/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1708, in _inner_training_loop\r\n  deepspeed_load_checkpoint(self.model_wrapped, resume_from_checkpoint)\r\n  File \"/home/wyxu/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/integrations/deepspeed.py\", line 402, in deepspeed_load_checkpoint\r\n    load_path, _ = deepspeed_engine.load_checkpoint(\r\n  File \"/home/wyxu/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2724, in load_checkpoint\r\n    load_path, client_states = self._load_checkpoint(load_dir,\r\n  File \"/home/wyxu/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2794, in _load_checkpoint\r\n    self.load_module_state_dict(checkpoint=checkpoint,\r\n  File \"/home/wyxu/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2587, in load_module_state_dict\r\n    self.module.load_state_dict(\r\n  File \"/home/wyxu/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2152, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:\r\n    Missing key(s) in state_dict: \"base_model.model.model.embed_tokens.weight\", \"base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight\", \"base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight\", \"base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight\", \"base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight\", \"base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight\", \"base_model.model.model.layers.0.mlp.up_proj.base_layer.weight\"\r\n......\r\n```\r\n\r\nScreenshots:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/65152684/d7924de6-b9be-46f9-b370-ba362ca78704)</BODY>\n\n<COMMENTS>\n<Comment by 1uciusy at 2024-05-08T03:59:52Z>\nIt will check the folder `--output_dir /data/wyxu/LLaVA/checkpoints/llava-vicuna-7b-v1.3-finetune-on-mic_sampled-lora ` for the latest `checkpoint-xxxx` and resume to train.\n</Comment>\n<Comment by 1uciusy at 2024-05-08T04:02:30Z>\nAs for the missmatch of state_dict\r\n```\r\npip install transformers==4.39.3\r\npip install accelerate==0.27.2\r\n```\r\nIt is mentioned in some issues, but i forgot which it is\n</Comment>\n<Comment by tetsu-kikuchi at 2024-05-09T03:57:19Z>\nmight be this one #1200\n</Comment>\n<Comment by wenyisir at 2024-05-09T04:41:58Z>\nI fixed this bug by modifying it:  `site-packages/deepspeed/runtime/engine.py line 2675 load_module_strict=Fasle`\n</Comment>\n<Comment by 1uciusy at 2024-05-09T11:52:19Z>\nGreat, so there is no need to change the version of `transformers`, you could avoid potential troubles as in #1218 when infering\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1490,
    "state": "open",
    "created_by": "YQYI",
    "created_at": "2024-05-07T07:44:30Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1490</URL>\n\n<TITLE>[Question] pretrain_mm_mlp_adapter of llava v1.6 7B not found</TITLE>\n\n<BODY>### Question\n\nI try to train llava v1.6 7B in lora mode, but can not find pretrain_mm_mlp_adapter file, where can i find it?</BODY>\n\n<COMMENTS>\n<Comment by Davidup1 at 2024-05-07T08:13:30Z>\n@YQYI I guess you can just use the mlp_adapter of llava v1.5 7B as the blog says\r\n![image](https://github.com/haotian-liu/LLaVA/assets/94554614/1228cab1-d715-47e5-ba61-8a1ef6f54985)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1489,
    "state": "open",
    "created_by": "hadolop",
    "created_at": "2024-05-07T06:48:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1489</URL>\n\n<TITLE>同样的需求，可以分享下思路吗？</TITLE>\n\n<BODY>同样的需求，可以分享下思路吗？\r\n\r\n\r\n2024-04-26 13:35:09 \"Haozhuang Chi\" ***@***.***> 写道：\r\n\r\n你好，如果我想对一批图片进行推理，但是每个图片只需要一个问题，问题都是一样的，这样的话还可以吗\r\n\r\n肯定可以，这个听起来是最简单的\r\n\r\n—\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you commented.Message ID: ***@***.***>\r\n\r\n_Originally posted by @zeyuwang-zju in https://github.com/haotian-liu/LLaVA/issues/1298#issuecomment-2078717535_</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1487,
    "state": "open",
    "created_by": "lin-whale",
    "created_at": "2024-05-07T04:03:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1487</URL>\n\n<TITLE>[Usage] Must I reload the model when I want to inference on a new image?</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nI think the time to load model is very long, so try to reuse the model when inferring in a new image. But encounter the issue below, so is it possible to do this? How should I write the code?\r\nModified from [llava/serve/cli.py](https://github.com/haotian-liu/LLaVA/blob/3e337ad269da3245643a2724a1d694b5839c37f9/llava/serve/cli.py)\r\n```python\r\ndef main(args):\r\n    # Model\r\n    disable_torch_init()\r\n\r\n    model_name = get_model_name_from_path(args.model_path)\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n\r\n    if \"llama-2\" in model_name.lower():\r\n        conv_mode = \"llava_llama_2\"\r\n    ...\r\n\r\n    while True:\r\n        image_file = input(\"Please input image path:\")\r\n        image = load_image(image_file)\r\n        image_size = image.size\r\n        # Similar operation in model_worker.py\r\n        image_tensor = process_images([image], image_processor, model.config)\r\n        if type(image_tensor) is list:\r\n            image_tensor = [image.to(model.device, dtype=torch.float16) for image in image_tensor]\r\n        else:\r\n            image_tensor = image_tensor.to(model.device, dtype=torch.float16)\r\n\r\n        while True:\r\n            try:\r\n                inp = input(f\"{roles[0]}: \")\r\n            except EOFError:\r\n                inp = \"\"\r\n            if not inp:\r\n                print(\"exit...\")\r\n                break\r\n\r\n            print(f\"{roles[1]}: \", end=\"\")\r\n\r\n            if image is not None:\r\n                # first message\r\n                if model.config.mm_use_im_start_end:\r\n                    inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + inp\r\n                else:\r\n                    inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\r\n                image = None\r\n            \r\n            conv.append_message(conv.roles[0], inp)\r\n            conv.append_message(conv.roles[1], None)\r\n            prompt = conv.get_prompt()\r\n\r\n            input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)\r\n            stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\r\n            keywords = [stop_str]\r\n            streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\r\n\r\n            with torch.inference_mode():\r\n                output_ids = model.generate(\r\n                    input_ids,\r\n                    images=image_tensor,\r\n                    image_sizes=[image_size],\r\n                    do_sample=True if args.temperature > 0 else False,\r\n                    temperature=args.temperature,\r\n                    max_new_tokens=args.max_new_tokens,\r\n                    streamer=streamer,\r\n                    use_cache=True)\r\n\r\n            outputs = tokenizer.decode(output_ids[0]).strip()\r\n            conv.messages[-1][-1] = outputs\r\n\r\n            if args.debug:\r\n                print(\"\\n\", {\"prompt\": prompt, \"outputs\": outputs}, \"\\n\")\r\n```\r\n\r\nThe code works well on first image input, but fails on the second image input.\r\n\r\nOutput:\r\n```\r\nLoading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:29<00:00,  1.94s/it]\r\nPlease input image path:/home/aistar/llava/data/view.jpg\r\n<|im_start|>user\r\n: hello\r\n<|im_start|>assistant\r\n: Hello! This is a beautiful image of a wooden dock extending into a serene lake. The calm water reflects the surrounding landscape, which includes a forest and mountains in the distance. The sky is partly cloudy, suggesting a pleasant day. The dock appears to be a quiet spot for relaxation or perhaps a starting point for boating or fishing. It's a peaceful scene that evokes a sense of tranquility and connection with nature.\r\n<|im_start|>user\r\n: \r\nexit...\r\nPlease input image path:/home/aistar/llava/data/view.jpg\r\n<|im_start|>user\r\n: hello\r\n<|im_start|>assistant\r\n: Traceback (most recent call last):\r\n  File \"/home/aistar/llava/annaconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/aistar/llava/annaconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/aistar/llava/LLaVA/llava/serve/cli_multi_turn.py\", line 137, in <module>\r\n    main(args)\r\n  File \"/home/aistar/llava/LLaVA/llava/serve/cli_multi_turn.py\", line 107, in main\r\n    output_ids = model.generate(\r\n  File \"/home/aistar/llava/annaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/aistar/llava/LLaVA/llava/model/language_model/llava_llama.py\", line 125, in generate\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/home/aistar/llava/LLaVA/llava/model/llava_arch.py\", line 260, in prepare_inputs_labels_for_multimodal\r\n    cur_image_features = image_features[cur_image_idx]\r\nIndexError: list index out of range\r\n```</BODY>\n\n<COMMENTS>\n<Comment by J0eky at 2024-05-30T01:50:40Z>\n@lin-whale hi, I'm working the same thing with you, and meet the same problem, have you solve it now?\n</Comment>\n<Comment by lin-whale at 2024-05-30T02:42:42Z>\n> @lin-whale hi, I'm working the same thing with you, and meet the same problem, have you solve it now?\r\n\r\nYeah, here is the code modified. You need to reinitialize the conv before starting a new query. Just in this line ```conv = conv_templates[args.conv_mode].copy()```.\r\n```python\r\ndef main(args):\r\n    # Model\r\n    disable_torch_init()\r\n\r\n    model_name = get_model_name_from_path(args.model_path)\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n    \r\n    if \"llama-2\" in model_name.lower():\r\n        conv_mode = \"llava_llama_2\"\r\n    elif \"mistral\" in model_name.lower():\r\n        conv_mode = \"mistral_instruct\"\r\n    elif \"v1.6-34b\" in model_name.lower():\r\n        conv_mode = \"chatml_direct\"\r\n    elif \"v1\" in model_name.lower():\r\n        conv_mode = \"llava_v1\"\r\n    elif \"mpt\" in model_name.lower():\r\n        conv_mode = \"mpt\"\r\n    else:\r\n        conv_mode = \"llava_v0\"\r\n\r\n    if args.conv_mode is not None and conv_mode != args.conv_mode:\r\n        print('[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}'.format(conv_mode, args.conv_mode, args.conv_mode))\r\n    else:\r\n        args.conv_mode = conv_mode\r\n\r\n    while True:\r\n        print(\"-\"*100)\r\n        img_folder = input(\"please input image folder path: \")\r\n        images = os.listdir(img_folder)\r\n        images = [item for item in images if os.path.isfile(os.path.join(img_folder, item))]\r\n        images.sort()\r\n        save_dir = os.path.join(img_folder, \"llava_eval\")\r\n        if not os.path.exists(save_dir):\r\n            os.makedirs(save_dir)\r\n\r\n        try:\r\n            ori_inp = input(\"please input prompt: \")\r\n        except EOFError:\r\n            ori_inp = \"\"\r\n            \r\n        total_time = 0\r\n        img_count = 0\r\n        yes_img_count = 0\r\n\r\n        for index, ori_image in enumerate(images):\r\n            img_count += 1\r\n            \r\n            try:\r\n                conv = conv_templates[args.conv_mode].copy()\r\n                if \"mpt\" in model_name.lower():\r\n                    roles = ('user', 'assistant')\r\n                else:\r\n                    roles = conv.roles\r\n\r\n                image = load_image(os.path.join(img_folder, ori_image))\r\n\r\n                # Similar operation in model_worker.py\r\n                image_tensor = process_images([image], image_processor, args)\r\n                if type(image_tensor) is list:\r\n                    image_tensor = [image.to(model.device, dtype=torch.float16) for image in image_tensor]\r\n                else:\r\n                    image_tensor = image_tensor.to(model.device, dtype=torch.float16)            \r\n            except EOFError:\r\n                image = \"\"\r\n            if not image:\r\n                print(\"NO image input, exit...\")\r\n                break\r\n            \r\n            if not ori_inp:\r\n                print(\"exit...\")\r\n                break\r\n\r\n            try:\r\n                # print(f\"{roles[1]}: \", end=\"\")\r\n                print(\"\")\r\n                print(ori_image)\r\n                \r\n                T1 = time.perf_counter()\r\n                if image is not None:\r\n                    # first message\r\n                    if model.config.mm_use_im_start_end:\r\n                        inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + ori_inp\r\n                    else:\r\n                        inp = DEFAULT_IMAGE_TOKEN + '\\n' + ori_inp\r\n                    conv.append_message(conv.roles[0], inp)\r\n                    image = None\r\n                else:\r\n                    # later messages\r\n                    conv.append_message(conv.roles[0], ori_inp)\r\n                conv.append_message(conv.roles[1], None)\r\n                prompt = conv.get_prompt()\r\n\r\n                input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\r\n                stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\r\n                keywords = [stop_str, 'Yes', 'No']\r\n                stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\r\n                streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True, timeout=15)\r\n\r\n                # print('------------------------')\r\n                # print('input_ids:', input_ids)\r\n                # print('image_tensor:', image_tensor)\r\n                # print('args.temperature:', args.temperature)\r\n                # print('max_new_tokens:', args.max_new_tokens)\r\n                # print('streamer:', streamer)\r\n                # print('stopping_criteria:', [stopping_criteria])\r\n\r\n\r\n                with torch.inference_mode():\r\n                    output_ids = model.generate(\r\n                        input_ids,\r\n                        images=image_tensor,\r\n                        do_sample=True,\r\n                        top_p = 1.0,\r\n                        temperature=args.temperature,\r\n                        max_new_tokens=args.max_new_tokens,\r\n                        streamer=streamer,\r\n                        use_cache=True,\r\n                        stopping_criteria=[stopping_criteria])\r\n\r\n\r\n                # input('Debug...')\r\n\r\n                outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()\r\n                conv.messages[-1][-1] = outputs\r\n                T2 =time.perf_counter()\r\n                print('------------time in second------------', (T2 - T1))\r\n                total_time += T2-T1\r\n                # print(outputs)\r\n                if args.debug:\r\n                    print(\"\\n\", {\"prompt\": prompt, \"outputs\": outputs}, \"\\n\")\r\n            \r\n            except Exception as e:\r\n                print('Error:',str(e))\r\n                print('Traceback:', traceback.format_exc())\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1486,
    "state": "open",
    "created_by": "HJLQZ",
    "created_at": "2024-05-07T03:57:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1486</URL>\n\n<TITLE>[Question] How to extract embeddings from llava?</TITLE>\n\n<BODY>### Question\r\n\r\nim trying to get visual&textual embeddings ,or the final mixed embeddings from llava,but cant find any tutorials about this. Does anyone know how to get embeddings?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1483,
    "state": "open",
    "created_by": "OualidBougzime",
    "created_at": "2024-05-02T18:57:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1483</URL>\n\n<TITLE>[ERROR]: RuntimeError: \"addmm_impl_cpu_\" not implemented for 'Half'</TITLE>\n\n<BODY>### Describe the issue\n\nWhen attempting to run inference with my fine-tuned LLaVA model using LoRA, I encountered an error. Here's the code snippet I used:\r\n```\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.mm_utils import get_model_name_from_path\r\nfrom llava.eval.run_llava import eval_model\r\n\r\n# Path to your fine-tuned model\r\nfine_tuned_model_path = \"../merged_model_llava_lora\"\r\n\r\n# Load the fine-tuned model\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    model_path=fine_tuned_model_path,\r\n    model_base=None,  # Adjust if necessary based on your training configuration\r\n    model_name=get_model_name_from_path(fine_tuned_model_path)\r\n)\r\n\r\n# Evaluation setup\r\nprompt = \"Identify the most relevant subclasses within the field of additive manufacturing or 4D Printing based on the given class: \\\"print_technology\\\", supplementary context, and image analysis.\\n    \\n    <|user|>: Parent class: \\\"print_technology\\\"\\nThe Supplementary context:\\n['Fused deposition modeling (FDM) remains the most common 3D printing technology due to its cost-effectiveness and material versatility.', 'Selective laser sintering (SLS) allows for the creation of complex geometries without the need for support structures, enhancing design freedom.', 'Stereolithography (SLA) is renowned for its high resolution and surface finish, making it ideal for detailed prototypes.', 'The emergence of digital light processing (DLP) has improved the speed of the photopolymerization process, significantly reducing printing time.', 'Multi-jet fusion (MJF) offers improved mechanical properties and uniformity compared to traditional layer-based printing methods.']['The application of continuous fiber fabrication (CFF) technology in 3D printing enables the production of parts with enhanced structural integrity.', 'Binder jetting technology is being explored for its potential in mass production due to its ability to rapidly produce multiple parts simultaneously.', 'The development of hybrid printing technologies that combine additive and subtractive processes could revolutionize production efficiency.', 'Advancements in direct energy deposition (DED) technology allow for the repair of high-value components in aerospace and defense industries.', 'Electron beam melting (EBM) technology provides unique advantages in the processing of high-strength titanium alloys for medical implants.']\\n    <|assistant|>\"\r\nimage_file = \"../Dondl et al. - 2019 - Simultaneous elastic shape optimization for a doma_image_2.jpg\"\r\n\r\n# Set up evaluation arguments\r\nargs = type('Args', (), {\r\n    \"model_path\": fine_tuned_model_path,\r\n    \"model_base\": None,\r\n    \"model_name\": get_model_name_from_path(fine_tuned_model_path),\r\n    \"query\": prompt,\r\n    \"conv_mode\": None,\r\n    \"image_file\": image_file,\r\n    \"sep\": \",\",\r\n    \"temperature\": 0,\r\n    \"top_p\": None,\r\n    \"num_beams\": 1,\r\n    \"max_new_tokens\": 512\r\n})()\r\n\r\n# Perform evaluation with the fine-tuned model\r\neval_model(args)\r\n```\r\n\r\nThe error message I received is as follows:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\nCell In[36], line 35\r\n     20 args = type('Args', (), {\r\n     21     \"model_path\": fine_tuned_model_path,\r\n     22     \"model_base\": None,\r\n   (...)\r\n     31     \"max_new_tokens\": 512\r\n     32 })()\r\n     34 # Perform evaluation with the fine-tuned model\r\n---> 35 eval_model(args)\r\n\r\nFile ~/FineTuneVLLM/FineTune/LLaVA/llava/eval/run_llava.py:115, in eval_model(args)\r\n    108 input_ids = (\r\n    109     tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\r\n    110     .unsqueeze(0)\r\n    111     .cuda()\r\n    112 )\r\n    114 with torch.inference_mode():\r\n--> 115     output_ids = model.generate(\r\n    116         input_ids,\r\n    117         images=images_tensor,\r\n    118         image_sizes=image_sizes,\r\n    119         do_sample=True if args.temperature > 0 else False,\r\n    120         temperature=args.temperature,\r\n    121         top_p=args.top_p,\r\n    122         num_beams=args.num_beams,\r\n    123         max_new_tokens=args.max_new_tokens,\r\n    124         use_cache=True,\r\n    125     )\r\n    127 outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\r\n    128 print(outputs)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115, in context_decorator.<locals>.decorate_context(*args, **kwargs)\r\n    112 @functools.wraps(func)\r\n    113 def decorate_context(*args, **kwargs):\r\n    114     with ctx_factory():\r\n--> 115         return func(*args, **kwargs)\r\n\r\nFile ~/FineTuneVLLM/FineTune/LLaVA/llava/model/language_model/llava_llama.py:125, in LlavaLlamaForCausalLM.generate(self, inputs, images, image_sizes, **kwargs)\r\n    115     raise NotImplementedError(\"`inputs_embeds` is not supported\")\r\n    117 if images is not None:\r\n    118     (\r\n    119         inputs,\r\n    120         position_ids,\r\n    121         attention_mask,\r\n    122         _,\r\n    123         inputs_embeds,\r\n    124         _\r\n--> 125     ) = self.prepare_inputs_labels_for_multimodal(\r\n    126         inputs,\r\n    127         position_ids,\r\n    128         attention_mask,\r\n    129         None,\r\n    130         None,\r\n    131         images,\r\n    132         image_sizes=image_sizes\r\n    133     )\r\n    134 else:\r\n    135     inputs_embeds = self.get_model().embed_tokens(inputs)\r\n\r\nFile ~/FineTuneVLLM/FineTune/LLaVA/llava/model/llava_arch.py:202, in LlavaMetaForCausalLM.prepare_inputs_labels_for_multimodal(self, input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes)\r\n    200         raise ValueError(f\"Unexpected mm_patch_merge_type: {self.config.mm_patch_merge_type}\")\r\n    201 else:\r\n--> 202     image_features = self.encode_images(images)\r\n    204 # TODO: image start / end is not implemented here to support pretraining.\r\n    205 if getattr(self.config, 'tune_mm_mlp_adapter', False) and getattr(self.config, 'mm_use_im_start_end', False):\r\n\r\nFile ~/FineTuneVLLM/FineTune/LLaVA/llava/model/llava_arch.py:142, in LlavaMetaForCausalLM.encode_images(self, images)\r\n    140 def encode_images(self, images):\r\n    141     image_features = self.get_model().get_vision_tower()(images)\r\n--> 142     image_features = self.get_model().mm_projector(image_features)\r\n    143     return image_features\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518, in Module._wrapped_call_impl(self, *args, **kwargs)\r\n   1516     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\r\n   1517 else:\r\n-> 1518     return self._call_impl(*args, **kwargs)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527, in Module._call_impl(self, *args, **kwargs)\r\n   1522 # If we don't have any hooks, we want to skip the rest of the logic in\r\n   1523 # this function, and just call forward.\r\n   1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\n   1525         or _global_backward_pre_hooks or _global_backward_hooks\r\n   1526         or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1527     return forward_call(*args, **kwargs)\r\n   1529 try:\r\n   1530     result = None\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:215, in Sequential.forward(self, input)\r\n    213 def forward(self, input):\r\n    214     for module in self:\r\n--> 215         input = module(input)\r\n    216     return input\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518, in Module._wrapped_call_impl(self, *args, **kwargs)\r\n   1516     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\r\n   1517 else:\r\n-> 1518     return self._call_impl(*args, **kwargs)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527, in Module._call_impl(self, *args, **kwargs)\r\n   1522 # If we don't have any hooks, we want to skip the rest of the logic in\r\n   1523 # this function, and just call forward.\r\n   1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\n   1525         or _global_backward_pre_hooks or _global_backward_hooks\r\n   1526         or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1527     return forward_call(*args, **kwargs)\r\n   1529 try:\r\n   1530     result = None\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:114, in Linear.forward(self, input)\r\n    113 def forward(self, input: Tensor) -> Tensor:\r\n--> 114     return F.linear(input, self.weight, self.bias)\r\n\r\nRuntimeError: \"addmm_impl_cpu_\" not implemented for 'Half'\r\n```\r\n\r\nCould someone assist me with resolving this issue?</BODY>\n\n<COMMENTS>\n<Comment by ggcr at 2024-05-24T11:52:26Z>\nMove the model to CUDA, you are probably using `float 16` which is only implemented for GPUs.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1482,
    "state": "open",
    "created_by": "HuangZhen02",
    "created_at": "2024-05-02T13:28:09Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1482</URL>\n\n<TITLE>[Usage] Extremely Slow Inference Speed</TITLE>\n\n<BODY>### Describe the issue\n\nI run the LLAVA-34B model in one single A800(80GB) GPU. The inputs are some multi-modal math word problems(like those in MATH), but it turns out that each sample takes more than 1 minute. \r\n\r\nI set do_sample=False and max_new_tokens=2048.</BODY>\n\n<COMMENTS>\n<Comment by zhipeixu at 2024-09-24T14:58:22Z>\nI have the same problem as you, how did you solve it?\n</Comment>\n<Comment by yinyuanzhang at 2025-04-18T14:15:03Z>\nhow did you solve it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1481,
    "state": "open",
    "created_by": "shubhamagarwal92",
    "created_at": "2024-05-02T13:19:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1481</URL>\n\n<TITLE>[Usage] Deepspeed Zero Stage 3 not able to shard the model</TITLE>\n\n<BODY>Hi @haotian-liu ! \r\n\r\nInteresting work around LLaVa! \r\n\r\nIssue:\r\n\r\nI am trying to finetune LLaVa using 8 X H100. \r\n\r\nWhen I try to use DeepSpeed Zero Stage 3, it seems that the model gets replicated on all the GPUs, instead of being sharded. I get OOM issues when finetuning model. I am trying to use a context length of 2048 and ViT with 336 resolution. \r\n\r\nCould you please suggest what I might be doing wrong here? \r\n\r\nCommand:\r\n```\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path ../$MODEL_VERSION \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path ./finetune_data/cleaned_finetune_data.json \\\r\n    --image_folder ./finetune_data/images \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava-$MODEL_VERSION-pretrain/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-$MODEL_VERSION-finetune \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 16 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 500 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length  2048\\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n```\r\n\r\nWhen I run the model using `CUDA_VISIBLE_DEVICES=0 bash ./scripts/sample_stage3.sh`, the memory usage before training is:\r\n\r\n![Screenshot 2024-05-02 at 6 26 08 PM](https://github.com/haotian-liu/LLaVA/assets/7984532/6d2b4ff8-96bb-423c-ab8d-a6836f884b62)\r\n\r\nHowever, when I am using the stage 3 deepspeed, the GPU usage before training is \r\n<img width=\"1432\" alt=\"Screenshot 2024-05-02 at 5 50 08 PM\" src=\"https://github.com/haotian-liu/LLaVA/assets/7984532/c7797eaa-70e5-417a-8203-ec9d76b7c863\">\r\n\r\n\r\nAnd the model gets OOM after this. Could you please suggest what flag we might need to change?</BODY>\n\n<COMMENTS>\n<Comment by SimonWXW at 2024-09-04T08:38:36Z>\nHi, I meet the same problem. Do you solve this problem?\n</Comment>\n<Comment by DAVID-NGUYEN-S16 at 2024-09-04T18:16:30Z>\nI meet the same problem\n</Comment>\n<Comment by mzamini92 at 2024-09-16T00:10:21Z>\nIf I use 2 H100 I can run the code but I get OOM. When I increase it to +2 GPUs the model duplicates on GPUs instead of sharding and gets stuck in `Formatting inputs... Skip in lazy mode`\n</Comment>\n<Comment by michaelhla at 2025-02-22T21:45:53Z>\ngetting the same error as well\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1480,
    "state": "closed",
    "created_by": "TuuSiwei",
    "created_at": "2024-05-01T14:44:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1480</URL>\n\n<TITLE>where is mm_vision_tower</TITLE>\n\n<BODY>### Describe the issue\n\nThank you for your work. Recently, I've been looking at the pretraining code of llava v1. However, I noticed that the `mm_vision_tower` parameter is not present in either the script or the args. Yet, it appears in the llava config at the end. Could you please explain why this is?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1479,
    "state": "open",
    "created_by": "JesseZZZZZ",
    "created_at": "2024-05-01T11:33:59Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1479</URL>\n\n<TITLE>Question about multiple images in the same prompt</TITLE>\n\n<BODY>![image](https://github.com/haotian-liu/LLaVA/assets/128620656/f3803676-bc9e-4b04-9a1c-7999f3d273df)\r\nHi! I saw this on huggingface, it says that if I want to use multiple images in the same prompt, I may receive inaccurate results. So how can I do this? I have the need to use text-image-mixed input to do things like \"what are the difference between the first and second image I gave you?\". Thanks a lot!!!</BODY>\n\n<COMMENTS>\n<Comment by anas-zafar at 2024-07-03T05:48:32Z>\nHi @JesseZZZZZ , were you able to figure out how to input multiple images to the LLAVA model? Thanks\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1478,
    "state": "open",
    "created_by": "PacoGrax",
    "created_at": "2024-05-01T10:45:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1478</URL>\n\n<TITLE>Raw image URL from GitHub</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nI'm trying to get the image_url to use eval_model, but I always have the same error and I don't know why. I don't know hot to use raw image from GitHub\r\nI think the problem is in the image file because when in try this in another notebook it works\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\nimport requests\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\n\r\n\r\n#la imagen URL (utilizar la fila imagen URL from Github)\r\nimage_url = 'https://github.com/haotian-liu/LLaVA/blob/main/images/llava_v1_5_radar.jpg'\r\n#download the image and open it with PIL\r\nresponse = requests.get(image_url)\r\nimage = Image.open(BytesIO(response.content))\r\n\r\nimport matplotlib.pyplot as plt\r\n#display the image using matplotlib\r\nplt.imshow(image)\r\nplt.axis('off') #turn off axis nu,bers and ticks\r\nplt.show()\r\n\r\n#ahora pasar la imagen procesada al eval_model\r\n\r\neval_model(\r\n    tokenizer,\r\n    model,\r\n    image_processor,\r\n    context_len,\r\n    image,\r\n    \"what do you see in this picture?\"\r\n)\r\n\r\n\r\nLog: \r\nUnidentifiedImageError                    Traceback (most recent call last)\r\n[<ipython-input-20-9a1b8412d79e>](https://localhost:8080/#) in <cell line: 10>()\r\n      8 #download the image and open it with PIL\r\n      9 response = requests.get(image_url)\r\n---> 10 image = Image.open(BytesIO(response.content))\r\n     11 \r\n     12 import matplotlib.pyplot as plt\r\n\r\n[/usr/local/lib/python3.10/dist-packages/PIL/Image.py](https://localhost:8080/#) in open(fp, mode, formats)\r\n   3281         fp.seek(0)\r\n   3282     except (AttributeError, io.UnsupportedOperation):\r\n-> 3283         fp = io.BytesIO(fp.read())\r\n   3284         exclusive_fp = True\r\n   3285 \r\n\r\nUnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7a68fc2f6f20>\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/168636688/1f84d62a-297f-404c-b478-4f387d49faa3)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1476,
    "state": "open",
    "created_by": "glahoti6",
    "created_at": "2024-04-30T20:43:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1476</URL>\n\n<TITLE>[Usage] Vicuna v1.5 is not downloaded automatically</TITLE>\n\n<BODY>### Describe the issue\n\nLooks like base model Vicuna v1.5 is not downloaded automatically when I run the provided training scripts. Did anyone face this issue?\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py\", line 385, in cached_file\r\n    resolved_file = hf_hub_download(\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 111, in _inner_fn\r\n    validate_repo_id(arg_value)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 159, in validate_repo_id\r\n    raise HFValidationError(\r\nhuggingface_hub.utils._validators.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './checkpoints/vicuna-7b-v1-5'. Use `repo_type` argument if needed.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"/LLaVA/llava/train/train.py\", line 827, in train\r\n    model = LlavaLlamaForCausalLM.from_pretrained(\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3075, in from_pretrained\r\n    config, model_kwargs = cls.config_class.from_pretrained(\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 605, in from_pretrained\r\n    config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 634, in get_config_dict\r\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 689, in _get_config_dict\r\n    resolved_config_file = cached_file(\r\n  File \"/miniconda/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py\", line 450, in cached_file\r\n    raise EnvironmentError(\r\nOSError: Incorrect path_or_model_id: './checkpoints/vicuna-7b-v1-5'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\r\n```</BODY>\n\n<COMMENTS>\n<Comment by dennismyself at 2024-05-23T23:04:19Z>\nI have exactly the same error, it took me a while to find it.\r\nEnsure the script you are calling is bash scripts/v1_5/pretrain.sh NOT bash scripts/pretrain.sh and in the file --model_name_or_path lmsys/vicuna-13b-v1.5\n</Comment>\n<Comment by tianke0711 at 2024-11-18T06:13:05Z>\n@dennismyself \r\n\r\nThanks for your comment, could you give more details, please, I have the same issue when I run sh pretrain.sh\n</Comment>\n<Comment by yuezhao238 at 2025-03-05T09:05:18Z>\nTry absolute path, not relative path\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1475,
    "state": "open",
    "created_by": "hellangleZ",
    "created_at": "2024-04-30T09:17:21Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1475</URL>\n\n<TITLE>[Usage] None of the inputs have requires_grad=True. Gradients will be None</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\n\r\nLog said gradient will be none\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\njust using pretran\r\n\r\nLog: \r\n```\r\n/data22/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n  warnings.warn(\r\n/data22/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\r\n  warnings.warn(\r\n/data22/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n  warnings.warn(\r\n/data22/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\r\n  warnings.warn(\r\n/data22/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/15274284/b0cee717-4bd7-43dc-8ae1-188a4368d1ad)</BODY>\n\n<COMMENTS>\n<Comment by hellangleZ at 2024-05-01T03:08:35Z>\n/data22/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n  warnings.warn(\r\n/data22/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\r\n  warnings.warn(\r\n/data22/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n  warnings.warn(\r\n/data22/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\r\n  warnings.warn(\r\n{'loss': 1.8577, 'learning_rate': 3.205128205128205e-07, 'epoch': 0.0}                                                                                                                      \r\n{'loss': 1.7297, 'learning_rate': 6.41025641025641e-07, 'epoch': 0.0}                                                                                                                       \r\n{'loss': 1.866, 'learning_rate': 9.615384615384617e-07, 'epoch': 0.0}                                                                                                                       \r\n{'loss': 2.0846, 'learning_rate': 1.282051282051282e-06, 'epoch': 0.0}  \r\n\r\n\r\nWill anyone could Help to see that？\r\n\r\nThanks\n</Comment>\n<Comment by LijunZhang01 at 2024-05-13T12:03:28Z>\nHave you solved it? I encountered the same problem,\n</Comment>\n<Comment by PzWHU at 2024-05-16T03:25:40Z>\n我也是这个问题，请问有解决吗\n</Comment>\n<Comment by xiaxiangzhou at 2024-05-18T20:40:08Z>\nI have the same problem\n</Comment>\n<Comment by y-rok at 2024-05-30T11:26:23Z>\nI have the same problem\n</Comment>\n<Comment by PangziZhang523 at 2024-06-19T16:41:47Z>\n有解决吗？\n</Comment>\n<Comment by dacian7 at 2024-07-01T09:36:24Z>\n> 有解决吗？\r\n\r\n@PangziZhang523 hello，你解决了吗？我也是，但我看 loss 还是正常在下降\n</Comment>\n<Comment by SuperBruceJia at 2024-07-06T05:05:24Z>\n+1\n</Comment>\n<Comment by SuperBruceJia at 2024-07-06T05:05:31Z>\nAny solutions?\n</Comment>\n<Comment by dacian7 at 2024-07-06T05:07:58Z>\n> Any solutions?\r\n\r\n@SuperBruceJia  It is not a problem. Just ignore it. Everything is fine. I completed the training and the model is fine\n</Comment>\n<Comment by SuperBruceJia at 2024-07-06T05:15:40Z>\n@dacian7 Thank you very much for your quick reply!\r\n\r\nHowever, I have encountered further issues after this problem.\r\n`RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn`\r\n\r\n<img width=\"1533\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/31528604/75d592e1-8089-4eb7-8440-a8bd50451c90\">\n</Comment>\n<Comment by jiazhen-code at 2024-09-15T06:48:46Z>\nThe warning comes from the ViT component, which is already frozen, so you can ignore it. It’s related to gradient checkpointing, which is used to save memory. You’ll notice that the LLM has already called enable_input_require_grads(). After ViT, the MM-projector also needs gradients, so the backward pass for these two components is normal.\r\n\r\nIf you want to remove this warning (though it might not be necessary), you can call vit.enable_input_require_grads() and remove torch.no_grad() in CLIP. \r\n\r\nNote, according to my understanding, if you plan to modify anything related to ViT, those changes are essential.\n</Comment>\n<Comment by LiZhangMing at 2025-03-27T09:15:19Z>\n> The warning comes from the ViT component, which is already frozen, so you can ignore it. It’s related to gradient checkpointing, which is used to save memory. You’ll notice that the LLM has already called enable_input_require_grads(). After ViT, the MM-projector also needs gradients, so the backward pass for these two components is normal.\n> \n> If you want to remove this warning (though it might not be necessary), you can call vit.enable_input_require_grads() and remove torch.no_grad() in CLIP.\n> \n> Note, according to my understanding, if you plan to modify anything related to ViT, those changes are essential.\n\nThank you very much for your response. I added some trainable parameters to the ViT during the LoRA fine-tuning phase, and as you suggested, I set vit.enable_input_require_grads() and removed torch.no_grad() in CLIP. The parameters I added are updated, but I have observed that the parameters in the final layer are not updated, while the parameters in the other layers update normally. Why is that?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1474,
    "state": "open",
    "created_by": "chanangad",
    "created_at": "2024-04-30T06:51:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1474</URL>\n\n<TITLE>Pre-training with MPT-7B went well but fine-tuning it further gives garbled/random outputs</TITLE>\n\n<BODY>### Discussion\r\n\r\nAfter a few bug-fixes, I ran the pre-training code using mosaicml/mpt-7b model.\r\n\r\nThe  pre-training script I used\r\n` deepspeed train_mem.py \\ \r\n    --deepspeed ./scripts/zero2.json \\\r\n    --model_name_or_path mpt-7b \\ \r\n    --version mpt\\\r\n    --data_path LLaVA-Pretrain/blip_laion_cc_sbu_558k.json \\\r\n    --image_folder LLaVA-Pretrain/images \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-mpt-7b-vit-l-pretrain \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 32 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 24000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 1e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True\\\r\n    --dataloader_num_workers 1 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb \r\n`\r\nI ran inference on this pre-trained model using the following script\r\n\r\n`python -m llava.serve.cli \\\r\n    --model-base mpt-7b \\\r\n    --model-path ./checkpoints/llava-mpt-7b-vit-l-pretrain/ \\\r\n    --image-file \"https://cdn.pixabay.com/photo/2024/02/28/07/42/european-shorthair-8601492_1280.jpg\" \\\r\n    --temperature 0.1 `\r\n   \r\nThe output looks like follows, which is good enough for pre-training stage\r\n<img width=\"1384\" alt=\"Screenshot 2024-04-30 at 12 14 11 PM\" src=\"https://github.com/haotian-liu/LLaVA/assets/7700195/b0705e8e-1f20-43f7-8a1c-31d92f6f6c57\">\r\n\r\nAfter this I ran this instruction tuning script:\r\n\r\n`deepspeed train_mem.py \\\r\n--lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path mpt-7b \\\r\n    --version mpt\\\r\n    --data_path LLaVA-InTune/llava_v1_5_mix665k.json \\\r\n    --image_folder LLaVA-InTune/ \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n--pretrain_mm_mlp_adapter ./checkpoints/llava-mpt-7b-vit-l-pretrain/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-mpt-7b-vit-l-lora-fulldata \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 8\\\r\n    --per_device_eval_batch_size 2\\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000\\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True\\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb \\\r\n--image_aspect_ratio pad \\\r\n--group_by_modality_length True`\r\n\r\n\r\nThe training loss graph looked as follows:\r\n<img width=\"1641\" alt=\"Screenshot 2024-04-30 at 12 18 38 PM\" src=\"https://github.com/haotian-liu/LLaVA/assets/7700195/dece1d95-85be-485d-b547-3f52a7d95af2\">\r\n\r\nHowever, now that I try to run inference on this saved checkpoint using this script:\r\n`\r\npython -m llava.serve.cli \\\r\n    --model-base mpt-7b \\\r\n    --model-path ./checkpoints/llava-mpt-7b-vit-l-lora-fulldata/ \\\r\n    --image-file \"https://as1.ftcdn.net/v2/jpg/06/05/37/40/1000_F_605374009_hEUHatmKPzuHTIacg7rLneAgnLHUgegM.jpg\" \\\r\n    --temperature 0.1`\r\n\r\nI get very random output as below:\r\n \r\n2024-04-30 06:35:20,791] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nYou are using a model of type mpt to instantiate a model of type llava_mpt. This is not supported for all configurations of models and can yield errors.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nLoading LLaVA from base model...\r\nLoading checkpoint shards:   0%|                                                                                                                                | 0/2 [00:00<?, ?it/s]/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\nLoading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.63s/it]\r\nSome weights of LlavaMptForCausalLM were not initialized from the model checkpoint at /mnt/localssd/mpt-7b-test and are newly initialized: ['transformer.mm_projector.0.bias', 'transformer.mm_projector.0.weight', 'transformer.mm_projector.2.bias', 'transformer.mm_projector.2.weight']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\nLoading additional LLaVA weights...\r\nLoading LoRA weights...\r\nMerging LoRA weights...\r\nModel is loaded...\r\nuser: describe this image\r\nassistant: \r\n[![Docker](https party/docker/images/docker.png)\r\n\r\n# Docker 容器技朝\r\n\r\n## 前言\r\n\r\nDocker 是一个开源的容器技朝，它可以将一个应用程序的所有依赖项和配置文件打包在一个单一的容器中，从而可以在不同的操作系统上运行。\r\n\r\n.........\r\n\r\n\r\nCan anyone help me understand why is this happening and how can I resolve this?</BODY>\n\n<COMMENTS>\n<Comment by nlpkiddo-2001 at 2024-06-07T11:30:39Z>\n+1, I am also having the same issue. My loss hover around 0.6 - 0.8, and i am training gemma 2b model. Is there any update\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1473,
    "state": "open",
    "created_by": "catssci",
    "created_at": "2024-04-30T00:59:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1473</URL>\n\n<TITLE>[Question] How to batch inference?</TITLE>\n\n<BODY>### Question\n\nHi, I've been extracting information about images using inference examples for some time now, can you tell me how to do batch inference and example code.\r\n\r\nThank you.</BODY>\n\n<COMMENTS>\n<Comment by zsxm1998 at 2024-05-13T05:19:54Z>\nI have the same question\n</Comment>\n<Comment by dreamlychina at 2024-09-18T08:12:32Z>\nI have the same question\n</Comment>\n<Comment by mmervecerit at 2024-10-24T01:47:59Z>\n+1\n</Comment>\n<Comment by wzh125 at 2024-11-20T04:44:01Z>\nthe same question\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1472,
    "state": "closed",
    "created_by": "lukashelff",
    "created_at": "2024-04-29T08:56:21Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1472</URL>\n\n<TITLE>[Question] Convert saved LLaVA checkpoint to SGLang</TITLE>\n\n<BODY>### Question\r\n\r\nHey everyone has anyone figured out how we can transfer learned models to SGLang. I have already made some changes but wasn't able to make it work completely. By now i can successfully launch a server, however, i do receive CUDA error: device-side assert triggered.\r\nUntill now i have made the following changes:\r\n\r\n1. update \"model_type\" in config.json file from \"llava_lama\" to \"llava\"\r\n2. update folder name to llava, otherwise is_multimodal flag is set incorrectly and the server does not launch.\r\n3. The preprocessor_config.json is not saved after running the llava training script (I used the one found in HF repo llava-hf/llava-1.5-13b-hf).\r\n4. I added the dictonary for the image token to the \"added_tokens_decoder\" tokenizer_config.json as it was missing.\r\n5. Also the processor_class was missing inside the tokenizer_config.json. i added \"processor_class\": \"LlavaProcessor\"</BODY>\n\n<COMMENTS>\n<Comment by stephenZkang at 2024-04-29T09:59:35Z>\n![image](https://github.com/haotian-liu/LLaVA/assets/3273104/38e8becb-34bd-4fee-a83e-71956e19ed28)\r\nllava to llava_lama\n</Comment>\n<Comment by lukashelff at 2024-04-29T10:05:51Z>\nhey thanks for the reply. Did you manage to use the model using SGLang? Unfortunately, changing only the model type did not help. I also did the changes above. Having looked more close int it i do receive multiple out of bound errors:\r\ne.g.\r\n```\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [350,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n```\n</Comment>\n<Comment by lukashelff at 2024-04-30T15:20:59Z>\nI was able to load local v1.6 checkpoints as follows:\r\n1. Don't use the local tokenizer. Use the HG tokenizer provided by sglang.\r\n2. Update folder name to llava, otherwise is_multimodal flag is set incorrectly and the server does not launch.\r\n3. Update model config.json: \r\n- `model_type`: \"llava_lama\" -> \"llava\"\r\n- `image_aspect_ratio`: \"pad\" -> \"anyres\"\r\n- `mm_patch_merge_type`: \"flat\" -> \"spatial_unpad\"\r\n- `_name_or_path`: must contain the exact model_name for sglang to select the correct chat template e.g. \"llava-v1.6-34b\" \r\nFor 1.5 checkpoints proceed with the steps 1 and 2 from above. In the config.json of the model adjust the `model_type` and remove entries `mm_patch_merge_type` and `mm_projector_lr`.\n</Comment>\n<Comment by stephenZkang at 2024-05-08T01:29:31Z>\nThanks ,i try to do it\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1471,
    "state": "open",
    "created_by": "enkaranfiles",
    "created_at": "2024-04-29T07:14:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1471</URL>\n\n<TITLE>[Question] How to evaluate pretraining[image-text alignment] performance?</TITLE>\n\n<BODY>### Question\n\nI have trained the vision tower module by replacing another vision encoder and gathering new custom data from another domain. But I wonder how I can evaluate pretraining performance since it is the crucial part for image-text alignment, it must be consider? Anyone who can response, thanks!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1470,
    "state": "open",
    "created_by": "hellangleZ",
    "created_at": "2024-04-29T06:55:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1470</URL>\n\n<TITLE>[Question] FT use 1.5 face a issue that tensor mismatch</TITLE>\n\n<BODY>### Question\n\n[2024-04-29 06:52:01,294] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 295, num_elems = 6.76B\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  3.14it/s]\r\nSome weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at /aml/llama2chat and are newly initialized: ['model.mm_projector.0.bias', 'model.mm_projector.0.weight', 'model.mm_projector.2.bias', 'model.mm_projector.2.weight']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n/aml/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.80s/it]\r\nTraceback (most recent call last):\r\n  File \"/aml/LLaVA-main/llava/train/train_mem.py\", line 5, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"/aml/LLaVA-main/llava/train/train.py\", line 827, in train\r\n    model = LlavaLlamaForCausalLM.from_pretrained(\r\n  File \"/aml/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3850, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\n  **File \"/aml/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 4335, in _load_pretrained_model\r\n    raise RuntimeError(f\"Error(s) in loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\")\r\n**RuntimeError: Error(s) in loading state_dict for LlavaLlamaForCausalLM:\r\n\tsize mismatch for model.embed_tokens.weight: copying a param with shape torch.Size([32000, 4096]) from checkpoint, the shape in current model is torch.Size([32001, 4096]).\r\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([32000, 4096]) from checkpoint, the shape in current model is torch.Size([32001, 4096]).\r\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.****\r\n[2024-04-29 06:52:06,327] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 48707\r\n[2024-04-29 06:52:06,327] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 48708</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1469,
    "state": "open",
    "created_by": "hellangleZ",
    "created_at": "2024-04-29T02:15:59Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1469</URL>\n\n<TITLE>[Question] How to use the pretrain checkpoint</TITLE>\n\n<BODY>### Question\n\nI only found that there are some file like:\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/15274284/cad94ddf-5215-46af-8b1b-d89c15e13fa9)\r\n\r\n\r\nHow can I merge them to base model , or something I should do, any help is very appreciated</BODY>\n\n<COMMENTS>\n<Comment by feizhao19 at 2024-04-29T03:36:15Z>\nI have the same question...\n</Comment>\n<Comment by baichuanzhou at 2024-04-29T08:23:14Z>\nYou can use the `load_pretrained_model` function. Check [here](https://github.com/haotian-liu/LLaVA/blob/main/llava/model/builder.py#L87).\r\n\r\nYou need to:\r\n1. pass a model-base argument, which is the base language model you used for your llava model.\r\n2. pass a model-path argument, which is where your pretrained weights' folder is.\r\n\r\nThis way, you can load the pretrained weight into a base llava model.\n</Comment>\n<Comment by lukashelff at 2024-04-29T09:03:01Z>\nIs it possible to use the pretrained checkpoints with SGLang?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1468,
    "state": "open",
    "created_by": "ShawnAn-WHU",
    "created_at": "2024-04-29T02:09:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1468</URL>\n\n<TITLE>[Question] 4-bit Pretraining</TITLE>\n\n<BODY>### Question\n\nDoes anyone know how to pretrain a 4-bit model during the Pretraining stage. I added the \"--bits 4\" to the \"pretrain.script\", but the GPU VRAM did't decrease. What should I do? Thanks in advance!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1467,
    "state": "open",
    "created_by": "SamuelSchmidgall",
    "created_at": "2024-04-29T00:47:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1467</URL>\n\n<TITLE>[Question] Text only training?</TITLE>\n\n<BODY>### Question\n\nHow do I format the data to do text only training??</BODY>\n\n<COMMENTS>\n<Comment by ZzoomD at 2024-06-05T12:53:26Z>\nhi, bro, have you solved training using only text?\n</Comment>\n<Comment by Tree-Shu-Zhao at 2024-11-27T22:14:53Z>\nHi! May I ask if the performance dropped after fine-tuning on a text-only dataset? I have multimodal/text mixed samples. After fine-tuning, the performance of text-only queries significantly dropped. I'd like to know if you got similar results.\n</Comment>\n<Comment by SamuelSchmidgall at 2024-11-27T23:13:15Z>\nHey, @ZzoomD I ended up switching to prismatic-vlm which supports the mixed training.\r\n\r\n@Tree-Shu-Zhao I did see improvements on both my text and vision problems, the performance did not get worse\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1466,
    "state": "open",
    "created_by": "liuhui0401",
    "created_at": "2024-04-29T00:46:56Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1466</URL>\n\n<TITLE>Unable to determine the device handle for GPU0000:4F:00.0: Unknown Error</TITLE>\n\n<BODY>### Question\n\nWhen I finetuned the llava, I met such a problem \"Unable to determine the device handle for GPU0000:4F:00.0: Unknown Error\".  I finetuned on eight A100 gpu. This problem usually occured after three hours of finetuning. I finetuned on two different servers and this problem occurred. Can anyone please tell me what causes this?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1465,
    "state": "open",
    "created_by": "AliAbdulRehman",
    "created_at": "2024-04-28T22:58:45Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1465</URL>\n\n<TITLE>Multiple Images or Video as Input</TITLE>\n\n<BODY>### Question\n\nCan the chatbot have multiple sequential images as input? I'm trying to predict the pedestrian trajectory and my inputs are multiple frames of a video. How can the model understand sequential images?</BODY>\n\n<COMMENTS>\n<Comment by ElliottDyson at 2024-05-02T10:35:58Z>\n> ### Question\n> \n> Can the chatbot have multiple sequential images as input? I'm trying to predict the pedestrian trajectory and my inputs are multiple frames of a video. How can the model understand sequential images?\n\nIt doesn't. Have a look at the video Llava repo\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1464,
    "state": "open",
    "created_by": "stephenZkang",
    "created_at": "2024-04-28T10:35:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1464</URL>\n\n<TITLE>[Question] i use /home/AI1/qiaok/models/llava-v1.6-vicuna-7b \\ ，but the problem is happening</TITLE>\n\n<BODY>### Question\n\ni run python , “python -m llava.serve.cli \\\r\n    --model-path /home/AI1/qiaok/models/llava-v1.6-vicuna-7b \\\r\n    --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n    --load-4bit”\r\n    \r\nbut the error is happening:\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/3273104/43e6731e-f1df-47aa-a45b-0a391c442ee1)</BODY>\n\n<COMMENTS>\n<Comment by stephenZkang at 2024-04-28T10:37:09Z>\ni use local model, why that is ?\r\ni try to use llava-v1.5-7b\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1463,
    "state": "open",
    "created_by": "fisher75",
    "created_at": "2024-04-28T05:17:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1463</URL>\n\n<TITLE>[Question] Can I use current script to finetune or LoRA the 1.6 models?</TITLE>\n\n<BODY>### Question\n\nI tried and it seems working, so I wonder if any of you has any information regarding this question.</BODY>\n\n<COMMENTS>\n<Comment by itay1542 at 2024-05-22T10:54:24Z>\nI don't think its officially supported by the maintainers but there is a pull request #1347 that adds support. It worked for me\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1462,
    "state": "open",
    "created_by": "weixiaochen358",
    "created_at": "2024-04-27T14:39:39Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1462</URL>\n\n<TITLE>[Question] why the layer of the model has not gradient, the loss deceases from 3rd epoch when running the train_mem.py file</TITLE>\n\n<BODY>### Question\n\nwhy the layer of the model has not gradient, the loss deceases from 3rd epoch when running the train_mem.py file.\r\n```python\r\nfrom transformers import TrainerCallback\r\n\r\n    class GradientCheckingCallback(TrainerCallback):\r\n        def on_step_end(self, args, state, control, **kwargs):\r\n            # This method is called at the end of each training step\r\n            model = kwargs['model']  # Access the model\r\n            for name, param in model.named_parameters():\r\n                if 'mm_projector' in name and param.grad is not None:\r\n                    print(f\"Gradient for {name}: {param.grad.norm().item()}\")\r\n                elif 'mm_projector' in name:\r\n                    print(f\"No gradient for {name}\")\r\ntrainer = LLaVATrainer(model=model,\r\n                    tokenizer=tokenizer,\r\n                    args=training_args,\r\n                       callbacks=[GradientCheckingCallback()],\r\n                    **data_module)\r\n```\r\n\r\nNo gradient for model.mm_projector.0.weight\r\nNo gradient for model.mm_projector.0.bias\r\nNo gradient for model.mm_projector.2.weight\r\nNo gradient for model.mm_projector.2.bias\r\n{'loss': 8.1787, 'grad_norm': 126.70487213134766, 'learning_rate': 0.001, 'epoch': 1.0}\r\n 20%|█████████                                    | 1/5 [00:01<00:04,  1.13s/it]No gradient for model.mm_projector.0.weight\r\nNo gradient for model.mm_projector.0.bias\r\nNo gradient for model.mm_projector.2.weight\r\nNo gradient for model.mm_projector.2.bias\r\n{'loss': 8.1787, 'grad_norm': 126.70057678222656, 'learning_rate': 0.00075, 'epoch': 2.0}\r\n 40%|██████████████████                           | 2/5 [00:01<00:02,  1.06it/s]No gradient for model.mm_projector.0.weight\r\nNo gradient for model.mm_projector.0.bias\r\nNo gradient for model.mm_projector.2.weight\r\nNo gradient for model.mm_projector.2.bias\r\n{'loss': 9.7352, 'grad_norm': 19.390901565551758, 'learning_rate': 0.0005, 'epoch': 3.0}\r\n 60%|███████████████████████████                  | 3/5 [00:02<00:01,  1.13it/s]No gradient for model.mm_projector.0.weight\r\nNo gradient for model.mm_projector.0.bias\r\nNo gradient for model.mm_projector.2.weight\r\nNo gradient for model.mm_projector.2.bias\r\n{'loss': 6.939, 'grad_norm': 9.54937744140625, 'learning_rate': 0.00025, 'epoch': 4.0}\r\n 80%|████████████████████████████████████         | 4/5 [00:03<00:00,  1.17it/s]No gradient for model.mm_projector.0.weight\r\nNo gradient for model.mm_projector.0.bias\r\nNo gradient for model.mm_projector.2.weight\r\nNo gradient for model.mm_projector.2.bias\r\n{'loss': 4.7991, 'grad_norm': 5.036875247955322, 'learning_rate': 0.0, 'epoch': 5.0}\r\n{'train_runtime': 4.3987, 'train_samples_per_second': 2.273, 'train_steps_per_second': 1.137, 'train_loss': 7.56612548828125, 'epoch': 5.0}\r\n100%|█████████████████████████████████████████████| 5/5 [00:04<00:00,  1.14it/s]</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1461,
    "state": "open",
    "created_by": "OliverLeeXZ",
    "created_at": "2024-04-27T13:18:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1461</URL>\n\n<TITLE>[Question] I finetuning llava1.5-7b model using lora on llava_mix_665k dataset but get  545 on MME perception score. Has anyone else encountered this issue?</TITLE>\n\n<BODY>### Question\n\nI finetuning llava1.5-7b model using lora on llava_mix_665k dataset on 4*A100-40g. However, model has bad performance on MME benchmark. perception score:545, cognition score 197. Has anyone else encountered this issue?</BODY>\n\n<COMMENTS>\n<Comment by OliverLeeXZ at 2024-04-28T12:35:09Z>\n@haotian-liu My training hyperparameters remain consistent with you provided. Here are my partial train logs and MME results:\r\n100%|██████████| 10396/10396 [22:39:17<00:00,  9.15s/it]                                             \r\n{'train_runtime': 81561.6745, 'train_samples_per_second': 8.157, 'train_steps_per_second': 0.127, 'train_loss': 3.424769993772549, 'epoch': 1.0}\r\n\r\n=========== Perception ===========\r\ntotal score: 545.515306122449 \r\n\r\n\t existence  score: 48.333333333333336\r\n\t count  score: 50.0\r\n\t position  score: 48.333333333333336\r\n\t color  score: 55.00000000000001\r\n\t posters  score: 46.59863945578232\r\n\t celebrity  score: 55.0\r\n\t scene  score: 55.0\r\n\t landmark  score: 68.25\r\n\t artwork  score: 71.5\r\n\t OCR  score: 47.5\r\n\r\n\r\n=========== Cognition ===========\r\ntotal score: 197.5 \r\n\r\n\t commonsense_reasoning  score: 45.0\r\n\t numerical_calculation  score: 57.5\r\n\t text_translation  score: 75.0\r\n\t code_reasoning  score: 20.0\n</Comment>\n<Comment by zjysteven at 2024-06-25T16:16:54Z>\n@OliverLeeXZ Hi just curious have you found out the reason?\n</Comment>\n<Comment by HuangChiEn at 2025-04-07T06:42:25Z>\nnot sure, because of Lora ? (seems finetune will be better)\n</Comment>\n<Comment by OliverLeeXZ at 2025-04-07T08:34:42Z>\nI solving this issue by changing learning rate from 1e-4 to 1e-5. Emm... maybe result by different gpu setting or python packages version?\n</Comment>\n<Comment by HuangChiEn at 2025-04-09T00:09:31Z>\n> I solving this issue by changing learning rate from 1e-4 to 1e-5. Emm... maybe result by different gpu setting or python packages version?\n\nHi, i also curious that does the lower learning rate, which solve MME drop issue, for finetuning stage only, or for pretrain & finetune stage ?\n</Comment>\n<Comment by OliverLeeXZ at 2025-04-09T03:01:11Z>\nFor finetuning stage using LoRA.\n</Comment>\n<Comment by HuangChiEn at 2025-04-09T03:56:46Z>\n> For finetuning stage using LoRA.\n\ngot it! TKS for your reply ~\n</Comment>\n<Comment by junha1125 at 2025-05-14T07:16:39Z>\nHi just curious did you set the total batch as `256` for 1 iteration? for example 4 GPU x 16 batch x 4 gradient cumulation.\n</Comment>\n<Comment by HuangChiEn at 2025-05-15T02:47:21Z>\n> Hi just curious did you set the total batch as `256` for 1 iteration? for example 4 GPU x 16 batch x 4 gradient cumulation.\n\nUn...no. I just exactly follows llava v1.5 paper, the appendix have provide batch size (and other hyper-params).\nIn finetune stage, my global bz is 128. \n(the way you calculate bz is correct BTW, i can not fit batch in GPU, accumulate grad is required~)\n![Image](https://github.com/user-attachments/assets/00ae02ea-73ac-4787-8d89-2d315359f8b4)\n</Comment>\n<Comment by junha1125 at 2025-05-15T07:32:20Z>\n> > Hi just curious did you set the total batch as `256` for 1 iteration? for example 4 GPU x 16 batch x 4 gradient cumulation.\n> \n\n\nThank you for a reply. I thoght 256 bz is needed for finetuning, but 128 is right.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1460,
    "state": "open",
    "created_by": "wjdghks950",
    "created_at": "2024-04-26T17:49:27Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1460</URL>\n\n<TITLE>[Usage] RuntimeError: Connection reset by peer during Pre-training</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nHi, I've been experiencing issue with the pre-training of LLaVA-1.5 model and my set up is as follows:\r\n- 4x A100 80G\r\n- CUDA 11.7\r\n- Torch 2.0.1+cu117\r\n\r\nCommand:\r\n```\r\n./pretrain.sh\r\n```\r\n\r\nPre-training Script:\r\n```\r\n#!/bin/bash\r\n\r\ndeepspeed --include localhost:4,5,6,7 ../../llava/train/train_mem.py \\\r\n    --master_port 29600 \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --model_name_or_path lmsys/vicuna-7b-v1.5 \\\r\n    --version plain \\\r\n    --data_path ./playground/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json \\\r\n    --image_folder ./playground/data/LLaVA-Pretrain/images \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b-dual_channel-pretrain \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 4 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 24000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 1e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nLog: \r\n```\r\n[2024-04-26 12:45:45,997] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-04-26 12:46:03,757] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\n[2024-04-26 12:46:03,758] [INFO] [runner.py:555:main] cmd = /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None ../../llava/train/train_mem.py --master_port 29600 --deepspeed ./scripts/zero2.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version plain --data_path ./playground/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json --image_folder ./playground/data/LLaVA-Pretrain/images --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --tune_mm_mlp_adapter True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir ./checkpoints/llava-v1.5-7b-dual_channel-pretrain --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 24000 --save_total_limit 1 --learning_rate 1e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb\r\n[2024-04-26 12:46:06,317] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-04-26 12:46:17,428] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [4, 5, 6, 7]}\r\n[2024-04-26 12:46:17,428] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0\r\n[2024-04-26 12:46:17,428] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\r\n[2024-04-26 12:46:17,428] [INFO] [launch.py:163:main] dist_world_size=4\r\n[2024-04-26 12:46:17,428] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=4,5,6,7\r\n[2024-04-26 12:46:29,234] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-04-26 12:46:29,252] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-04-26 12:46:29,295] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-04-26 12:46:29,338] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-04-26 12:46:31,397] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2024-04-26 12:46:31,397] [INFO] [comm.py:594:init_distributed] cdb=None\r\nTraceback (most recent call last):\r\n  File \"/shared/nas/data/m1/jk100/code/ecole/LLaVA/scripts/v1_5/../../llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/shared/nas/data/m1/jk100/code/ecole/LLaVA/llava/train/train.py\", line 776, in train\r\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\r\n  File \"/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/transformers/hf_argparser.py\", line 338, in parse_args_into_dataclasses\r\n    obj = dtype(**inputs)\r\n  File \"<string>\", line 126, in __init__\r\n  File \"/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/transformers/training_args.py\", line 1372, in __post_init__\r\n    and (self.device.type != \"cuda\")\r\n  File \"/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/transformers/training_args.py\", line 1795, in device\r\n    return self._setup_devices\r\n  File \"/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/transformers/utils/generic.py\", line 54, in __get__\r\n    cached = self.fget(obj)\r\n  File \"/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/transformers/training_args.py\", line 1735, in _setup_devices\r\n    self.distributed_state = PartialState(timeout=timedelta(seconds=self.ddp_timeout))\r\n  File \"/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/accelerate/state.py\", line 170, in __init__\r\n    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\r\n  File \"/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/comm/comm.py\", line 627, in init_distributed\r\n    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\r\n  File \"/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/comm/torch.py\", line 71, in __init__\r\n    self.init_process_group(backend, timeout, init_method, rank, world_size)\r\n  File \"/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/comm/torch.py\", line 97, in init_process_group\r\n    torch.distributed.init_process_group(backend,\r\n  File \"/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 900, in init_process_group\r\n    store, rank, world_size = next(rendezvous_iterator)\r\n  File \"/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/distributed/rendezvous.py\", line 245, in _env_rendezvous_handler\r\n    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\r\n  File \"/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/distributed/rendezvous.py\", line 176, in _create_c10d_store\r\n    return TCPStore(\r\nRuntimeError: Connection reset by peer\r\n[2024-04-26 12:46:31,413] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2024-04-26 12:46:31,413] [INFO] [comm.py:594:init_distributed] cdb=None\r\n```\r\n\r\nI'm trying to fix the issue but the `RuntimeError: Connection reset by peer` error keeps appearing and I would like to know if anyone else is experiencing the same error</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1459,
    "state": "open",
    "created_by": "JesseZZZZZ",
    "created_at": "2024-04-26T03:07:28Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1459</URL>\n\n<TITLE>[Question] How to use multi GPUs with LlavaLlamaForCausalLM.from_pretrained</TITLE>\n\n<BODY>Hi! Thank you for your great work, and I want to realize it on my own server. However, I currently only have 8 24G-RTX 4090, and running the inference process on one 4090 will cause OOM error. After viewing previous issues, I see that there are some methods to use multi gpus, but I failed to use them in my own case.` LlavaLlamaForCausalLM.from_pretrained` does not supported the parameter` device_map`, so I'm a bit confused. Thanks!</BODY>\n\n<COMMENTS>\n<Comment by hktk07 at 2024-10-24T12:39:40Z>\nI have the same problem, do you sovle it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1456,
    "state": "closed",
    "created_by": "fisher75",
    "created_at": "2024-04-25T11:05:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1456</URL>\n\n<TITLE>[Question] with default setting, LoRA has no improvement is normal? 默认设置下，LoRA 完全没效果正常吗？</TITLE>\n\n<BODY>### Question\n\nHi, I used Lora for a 1.6-7b model, the training samples are about 100,000 images, and after I Lora I did an inference with the new Lora model. But it turned out to be no improvement at all (almost, like only a 1% improvement). I think this is absolutely not normal cause in the same time I also used ViT and CLIIP as baselines to compare and are all trained too, they both have shown very significant improvement and until like 90%+ accuracy. So I want know is this normal and what might the problem be. And one more question, does the LoRA only affect on LLM or on whole VLM including the CLIP part?\r\n\r\nHere I show my train command and inference command:\r\n```\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path model/llava-v1.6-vicuna-7b \\\r\n    --version v1 \\\r\n    --data_path train/lora.json \\\r\n    --image_folder ./playground/data \\\r\n    --vision_tower model/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir lava-v1.6-vicuna-7b-ora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nand\r\n\r\n```\r\npython -m llava.eval.model_vqa \\\r\n    --model-path lora/llava-v1.6-vicuna-7b-lora \\\r\n    --model-base model/llava-v1.6-vicuna-7b \\\r\n    --question-file questions.jsonl \\\r\n    --image-folder data \\\r\n    --answers-file lora_answers.jsonl \\\r\n    --temperature 0 \\\r\n    --conv-mode vicuna_v1\r\n```\r\n\r\nThanks for your attention.</BODY>\n\n<COMMENTS>\n<Comment by GabrielGan1996 at 2024-04-29T08:11:57Z>\nHi, I also encountered a similar problem, after using LoRA fine-tuning, almost no effect. Have you solved your problem?\n</Comment>\n<Comment by fisher75 at 2024-04-29T08:19:12Z>\n> Hi, I also encountered a similar problem, after using LoRA fine-tuning, almost no effect. Have you solved your problem?\r\n\r\nIn my case, I inspected all the steps from the beginning, turn out that I made the wrong json for training.\n</Comment>\n<Comment by GabrielGan1996 at 2024-04-29T08:26:07Z>\nOh. Thanks. I used the same json data and got very good results with full fine-tuning, but with LorA, it had no effect at all.\n</Comment>\n<Comment by Camellia-hz at 2024-08-01T07:12:31Z>\n@GabrielGan1996 Hello, May I ask if your problem is solved?\r\nI am guessing if it is because the weights obtained from lora are not merged with the weights from LLM?Actually, this is just my guess, I have not yet verified he\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1455,
    "state": "open",
    "created_by": "lizaibeim",
    "created_at": "2024-04-25T09:23:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1455</URL>\n\n<TITLE>[LLaVa demo server down]</TITLE>\n\n<BODY>### Describe the issue\n\nHi, I am trying to explore the functionality of LLaVa on the demo webpage but it seems like the service is down now.\r\n\r\n## How to reproduce the issue \r\ntype in any words or upload any images\r\n\r\n## Issues\r\nNETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.</BODY>\n\n<COMMENTS>\n<Comment by rokopi-byte at 2024-06-03T08:25:40Z>\nRight now the demo server is not reachable at all, error 1033\n</Comment>\n<Comment by qqaatw at 2024-07-25T20:24:03Z>\nSame issue: `NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.`\n</Comment>\n<Comment by Coldermyy at 2024-12-12T01:55:30Z>\nHave you solved the problem yet?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1454,
    "state": "open",
    "created_by": "a1wj1",
    "created_at": "2024-04-25T06:54:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1454</URL>\n\n<TITLE>[Usage] change GPU</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: I run llava/eval/run_llava.py. By default, cuda:0 is used. How can I change it to cuda:1\r\n\r\nThe following is my modification, adding options：to(args.device), but an error was reported, how can I solve it?\r\n![1714028017586](https://github.com/haotian-liu/LLaVA/assets/38905689/9b7d062e-a9cc-4f08-84c6-8f9ab81baee1)\r\n\r\nLog: \r\n```\r\n  File \"/datasets/xcm/LLaVA-main/llava/model/multimodal_encoder/clip_encoder.py\", line 56, in forward\r\n    image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\r\n  File \"/home/cg/anaconda3/envs/lmdrive/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/cg/anaconda3/envs/lmdrive/lib/python3.8/site-packages/accelerate/hooks.py\", line 166, in new_forward\r\n    output = module._old_forward(*args, **kwargs)\r\n  File \"/home/cg/anaconda3/envs/lmdrive/lib/python3.8/site-packages/transformers/models/clip/modeling_clip.py\", line 917, in forward\r\n    return self.vision_model(\r\n  File \"/home/cg/anaconda3/envs/lmdrive/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/cg/anaconda3/envs/lmdrive/lib/python3.8/site-packages/transformers/models/clip/modeling_clip.py\", line 841, in forward\r\n    hidden_states = self.embeddings(pixel_values)\r\n  File \"/home/cg/anaconda3/envs/lmdrive/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/cg/anaconda3/envs/lmdrive/lib/python3.8/site-packages/accelerate/hooks.py\", line 166, in new_forward\r\n    output = module._old_forward(*args, **kwargs)\r\n  File \"/home/cg/anaconda3/envs/lmdrive/lib/python3.8/site-packages/transformers/models/clip/modeling_clip.py\", line 182, in forward\r\n    patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]\r\n  File \"/home/cg/anaconda3/envs/lmdrive/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/cg/anaconda3/envs/lmdrive/lib/python3.8/site-packages/accelerate/hooks.py\", line 166, in new_forward\r\n    output = module._old_forward(*args, **kwargs)\r\n  File \"/home/cg/anaconda3/envs/lmdrive/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 463, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n  File \"/home/cg/anaconda3/envs/lmdrive/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\r\n    return F.conv2d(input, weight, bias, self.stride,\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument weight in method wrapper_CUDA__cudnn_convolution)\r\n\r\n```</BODY>\n\n<COMMENTS>\n<Comment by Yonggie at 2024-05-21T07:28:46Z>\nSame. The args doesn't take effect.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1453,
    "state": "closed",
    "created_by": "zht8506",
    "created_at": "2024-04-25T03:12:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1453</URL>\n\n<TITLE>have problem when resume model</TITLE>\n\n<BODY>Hi, thank you excellent work. i want to run llava-v1-80k with lora. The model can be trained normally.\r\nhowever, when i resume the checkpoints from work_dir (work_dir/checkpoint-xx), i meeting some error.\r\nI found that my ssh to cluster was killed. \r\n![image](https://github.com/haotian-liu/LLaVA/assets/81303574/42713ea0-2905-473d-8f8a-d671d1a1ec20)\r\nAlso, i found that Memory usage exceeds upper limit\r\n![image](https://github.com/haotian-liu/LLaVA/assets/81303574/62168702-0a47-42a6-b9eb-c01161f5e883)\r\nSo, how to solve this problem, how to correctly resume model.\r\nthank you very much.</BODY>\n\n<COMMENTS>\n<Comment by xing0047 at 2024-05-10T08:30:20Z>\nHi, may I ask if there is any update over the checkpoint resuming issue?\n</Comment>\n<Comment by zht8506 at 2024-05-11T03:54:44Z>\n> Hi, may I ask if there is any update over the checkpoint resuming issue?\r\n\r\ni find that it is because the cpu memory exceed the limitation. I solve this problem by only using 6 gpu (CUDA_VISIBLE_DEVICE=0,1,2,3,4,5) in a 8gpu machine.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1452,
    "state": "open",
    "created_by": "cocoshe",
    "created_at": "2024-04-24T23:29:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1452</URL>\n\n<TITLE>[Question] llama version for bias weight of llava-v1</TITLE>\n\n<BODY>### Question\n\nThanks for you great work!\r\n\r\n\r\nI have a question about the version of **llama** when using the legacy delta weight in llava-v1 in [model zoo](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#legacy-models-delta-weights)\r\n\r\nAs the instruction, we need to use a script to merge the original llama weight and a delta weight, take `Vicuna-13B-v1.1` as base LLM for example, the delta weight can be downloaded in huggingface: https://huggingface.co/liuhaotian/LLaVA-13b-delta-v1-1\r\n\r\nAnd we need to submit a application to Meta for llama weight.(in progress)\r\n\r\nSo the question is about the version of llama corresponding to the  `Vicuna-13B-v1.1` based delta weight. llama 1, or 2?\r\n\r\nAnd another question: Is the projector part of llava is in delta weight?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1451,
    "state": "open",
    "created_by": "rohithbojja",
    "created_at": "2024-04-24T11:45:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1451</URL>\n\n<TITLE>Cannot finetune bitsandbytes-4bit-model Ex: panoyo9829/llava-v1.6-mistral-7b-bnb-4bit</TITLE>\n\n<BODY>### Describe the issue\n\n\r\n#!/bin/bash\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 16 --lora_alpha 32 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --model_name_or_path /home/rohith/llava-v1.6-mistral-7b-bnb-4bit/ \\\r\n    --version mistral_instruct \\\r\n    --data_path /home/rohith/Desktop/vqa/vqa/images/filtered_dataset.json \\\r\n    --image_folder /home/rohith/Desktop/vqa/vqa/images/ \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --mm_patch_merge_type spatial_unpad \\\r\n    --image_aspect_ratio anyres \\\r\n    --group_by_modality_length False \\\r\n    --bf16 False \\\r\n    --fp16 True \\\r\n    --output_dir /home/rohith/LLaVA-1.6-ft/llava_lora_mistral_med/ \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 4 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 500 \\\r\n    --save_total_limit 5 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.05 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 4096 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb \\\r\n    \r\n    \r\n    \r\n    \r\n    error:\r\n    ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1450,
    "state": "open",
    "created_by": "yfthu",
    "created_at": "2024-04-24T02:16:14Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1450</URL>\n\n<TITLE>[Question] When will the finetune code for LLaVA 1.6 be released?</TITLE>\n\n<BODY>### Question\n\nHello, thank you for your wonderful work! I have tried the LLaVA1.5 train code, but the image resolution of LLaVA1.5 is too small, and it does not support AnyRes. I wonder when will the finetune code for LLaVA 1.6 be released, so we can do some downstream tasks based on LLaVA 1.6.\r\nThank you very much.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1449,
    "state": "open",
    "created_by": "Lin-Tianwei",
    "created_at": "2024-04-23T20:36:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1449</URL>\n\n<TITLE>[Question] How to update the gradient of vit?</TITLE>\n\n<BODY>### Question\n\nWhen I set vit's attribute requires_grad to True, when loss performs gradient backpass, vit's grad is still None. I think this is because the function truncates the gradient postback when organizing text and images as input. Can someone tell me how to update the gradient of vit? Thank you!</BODY>\n\n<COMMENTS>\n<Comment by MrPetrichor at 2024-05-21T04:14:33Z>\nI have the same question, after set ViT parameters grad  to True and the parameters  weren't updated : (\n</Comment>\n<Comment by Lin-Tianwei at 2024-05-21T06:03:31Z>\n> I have the same question, after set ViT parameters grad to True and the parameters weren't updated : (\r\n\r\nI have solved it! You can delete the decorator **decorator @torch.no_grad()** of **forward** method in **clip_encoder.py**.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1448,
    "state": "open",
    "created_by": "segalinc",
    "created_at": "2024-04-23T17:31:29Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1448</URL>\n\n<TITLE>[Usage] TypeError: LlavaLlamaForCausalLM.forward() got an unexpected keyword argument 'cache_position'</TITLE>\n\n<BODY>### Describe the issue\n\nReference issue https://github.com/huggingface/transformers/issues/29426\r\n\r\nI also encountered this error with transformers >4.38.2 when trying new LLavaLama3 from https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-hf/discussions/1\r\nI need to use tranformers > 4.39.2 for my work and llava requires 4.37 which is quite old by now. \r\nwould it be possible to adjust the code and update it to latest versions?</BODY>\n\n<COMMENTS>\n<Comment by zucchini-nlp at 2024-05-20T07:30:17Z>\nHey! This should be solvable by popping the `cache_position` from `inputs` in [this method](https://github.com/haotian-liu/LLaVA/blob/c121f0432da27facab705978f83c4ada465e46fd/llava/model/language_model/llava_llama.py#L144). \r\n\r\n`inputs.pop(\"cache_position\")`\r\n\r\nThe error is raised because calling \"super()\" returns kwargs that are not used in the custom model's forward.\n</Comment>\n<Comment by SWHL at 2024-05-30T01:57:31Z>\n@zucchini-nlp  It works!\n</Comment>\n<Comment by tseven at 2024-07-11T16:14:04Z>\nThis fixed the issue for me as well, thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1447,
    "state": "open",
    "created_by": "yspch2022",
    "created_at": "2024-04-23T14:13:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1447</URL>\n\n<TITLE>[Question] I have a question about capture to a custom data image.</TITLE>\n\n<BODY>### Question\n\nHello, I'm making instruction-following data while looking at papers and reference documents using my custom images. \r\nBut I have a question. \r\nWhen constructing instruction-following data, 'Context type 1: Captions' is used to make captions to describe the image, \r\nhow do I make captions for the image? I think it's made by putting it in a model such as BLIP, llava, or GPT4, right?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1445,
    "state": "open",
    "created_by": "FuZening",
    "created_at": "2024-04-23T08:12:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1445</URL>\n\n<TITLE>When I want to install Mini-Gemini, this error was returned! Please help me!</TITLE>\n\n<BODY>### Question\n\n<img width=\"568\" alt=\"截屏2024-04-23 17 12 17\" src=\"https://github.com/haotian-liu/LLaVA/assets/36889775/419fcdee-ecbf-4e8d-80b4-65fe2ea96512\"></BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1441,
    "state": "open",
    "created_by": "AmazDeng",
    "created_at": "2024-04-22T12:28:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1441</URL>\n\n<TITLE>[Question] preprocess_plain，no question part</TITLE>\n\n<BODY>### Question\n\nHi,authors, Thank you for your great contribution\r\n\r\nI've noticed that during the pretraining phase, the preprocess_plain method was used. This method discards the question part and directly concatenates <Image> with the answer. Could you explain the rationale behind this approach? Why is the question discarded instead of being retained?\r\n\r\n`def preprocess_plain(\r\n    sources: Sequence[str],\r\n    tokenizer: transformers.PreTrainedTokenizer,\r\n) -> Dict:\r\n    # add end signal and concatenate together\r\n    conversations = []\r\n    for source in sources:\r\n        assert len(source) == 2\r\n        assert DEFAULT_IMAGE_TOKEN in source[0]['value']\r\n        source[0]['value'] = DEFAULT_IMAGE_TOKEN\r\n        conversation = source[0]['value'] + source[1]['value'] + conversation_lib.default_conversation.sep\r\n        conversations.append(conversation)\r\n    # tokenize conversations\r\n    input_ids = [tokenizer_image_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations]\r\n    targets = copy.deepcopy(input_ids)\r\n    for target, source in zip(targets, sources):\r\n        tokenized_len = len(tokenizer_image_token(source[0]['value'], tokenizer))\r\n        target[:tokenized_len] = IGNORE_INDEX\r\n\r\n    return dict(input_ids=input_ids, labels=targets)`</BODY>\n\n<COMMENTS>\n<Comment by daixiangzi at 2024-04-23T16:43:48Z>\n+1+1，I meet this question today\n</Comment>\n<Comment by jzhzhang at 2024-04-25T15:38:45Z>\nSame question. I noticed that in the paper, the pertaining stage uses question X_q. Why pertaining uses `preprocess_plain ` function?\r\n![image](https://github.com/haotian-liu/LLaVA/assets/54199992/ab75275e-5ef7-4dda-8553-12452fc7e6a0)\n</Comment>\n<Comment by AmazDeng at 2024-04-26T12:01:52Z>\nI have found the answer. Look at these two sites. \r\nhttps://github.com/haotian-liu/LLaVA/issues/615\r\nhttps://github.com/haotian-liu/LLaVA/releases/tag/v1.0.1\r\n\r\nThe author says:\r\nPretraining. We simplified the pretraining prompts by removing additional instructions like Describe the image details, which we find to allow the zero-shot inference and can slightly improve the training speed.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1440,
    "state": "open",
    "created_by": "hx-Tang",
    "created_at": "2024-04-22T07:24:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1440</URL>\n\n<TITLE>[bug] When using multi-GPU / multi-node for pre-training, the initialized mm-projector weights & bias are different across each rank</TITLE>\n\n<BODY>### Question\n\nThank you for your contribution :)\r\n\r\nI notice this issue when I'm trying to reproduce your work and I found that there are some difference on the evaluation results of the models obtained using multi-node training and single-node training.\r\n\r\nThen I dig in to the code and found that the newly initialized mm-projector weights & bias are different across each rank.\r\n\r\nInsert this line before trainer.train() and I get different parameter accross ranks.\r\n\r\n`\r\nprint(f\"local_rank:{local_rank}, {[{i[0]:torch.mean(i[1].detach())} for i in trainer.model.get_model().mm_projector.named_parameters()]}\")\r\n`\r\n\r\ne.g. for 4 node trainning I get:\r\n`\r\nlocal_rank:0, [{'0.weight': tensor(-6.6163e-06)}, {'0.bias': tensor(-0.0003)}, {'2.weight': tensor(-1.6939e-06)}, {'2.bias': tensor(-2.5525e-05)}]\r\nlocal_rank:1, [{'0.weight': tensor(3.6777e-06)}, {'0.bias': tensor(-0.0004)}, {'2.weight': tensor(9.7775e-07)}, {'2.bias': tensor(-0.0002)}]\r\nlocal_rank:2, [{'0.weight': tensor(-9.8941e-07)}, {'0.bias': tensor(-4.4031e-05)}, {'2.weight': tensor(-7.1693e-07)}, {'2.bias': tensor(0.0001)}]\r\nlocal_rank:3, [{'0.weight': tensor(-8.1538e-06)}, {'0.bias': tensor(0.0003)}, {'2.weight': tensor(7.1939e-07)}, {'2.bias': tensor(4.7969e-05)}]\r\n`\r\n\r\nI try to set torch.manual_seed(seed) at the beginning of the train.py and this problem solved. \r\n\r\nI notice that there are many other LLaVA related work adopted your training code without reporting any issues on this, so I'm wondering whether I might be misunderstanding something, or if indeed there is a bug.\r\n\r\nThank you :)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1438,
    "state": "closed",
    "created_by": "fisher75",
    "created_at": "2024-04-22T03:12:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1438</URL>\n\n<TITLE>[Question] Failed to satart LoRA because of TypeError: Accelerator.__init__() got an unexpected keyword argument 'use_seedable_sampler'</TITLE>\n\n<BODY>### Question\n\nHi I got problems encountered when trying to LoRA.\r\n\r\nSeems like the main problem is: `TypeError: Accelerator.__init__() got an unexpected keyword argument 'use_seedable_sampler'`\r\n\r\nThe logging part:\r\n```\r\n[2024-04-22 03:43:13,255] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-04-22 03:43:28,314] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\nDetected CUDA_VISIBLE_DEVICES=0,1,2,3: setting --include=localhost:0,1,2,3\r\n[2024-04-22 03:43:28,314] [INFO] [runner.py:571:main] model/lora/llava-v1.6-vicuna-7b-DSLLM-lora --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb\r\n[2024-04-22 03:43:31,655] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-04-22 03:43:35,116] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\r\n[2024-04-22 03:43:35,116] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0\r\n[2024-04-22 03:43:35,116] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\r\n[2024-04-22 03:43:35,116] [INFO] [launch.py:163:main] dist_world_size=4\r\n[2024-04-22 03:43:35,116] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\r\n[2024-04-22 03:43:45,842] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-04-22 03:43:45,843] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-04-22 03:43:45,844] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-04-22 03:43:45,844] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-04-22 03:43:56,196] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2024-04-22 03:43:56,196] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2024-04-22 03:43:56,196] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n[2024-04-22 03:43:56,196] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2024-04-22 03:43:56,196] [INFO] [comm.py:637:init_distributed] cdb=None\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nanaconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\nanaconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\nanaconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\nanaconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\n[2024-04-22 03:44:17,345] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 687, num_elems = 7.06B\r\n\r\nLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\r\nLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\r\nLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\r\nLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\r\nLoading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.82s/it]\r\nLoading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.83s/it]\r\nLoading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.84s/it]\r\nLoading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.04s/it]\r\nLoading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.04s/it]\r\nLoading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.04s/it]\r\nLoading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.65s/it]\r\nLoading checkpoint shards:  67%|██████▋   | 2/3 [00:09<00:04,  4.30s/it]\r\nLoading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  5.52s/it]\r\nLoading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.56s/it]\r\n\r\nLoading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  5.52s/it]\r\nLoading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.56s/it]\r\n\r\nLoading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  5.52s/it]\r\nLoading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.56s/it]\r\n\r\nLoading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.64s/it]\r\nLoading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.69s/it]\r\nAdding LoRA adapters...\r\nopenai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.\r\nopenai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.\r\nopenai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.\r\nopenai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.\r\nFormatting inputs...Skip in lazy mode\r\nTraceback (most recent call last):\r\n  File \"VLM/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"VLM/LLaVA/llava/train/train.py\", line 961, in train\r\n    trainer = LLaVATrainer(model=model,\r\n  File \"anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 367, in __init__\r\n    self.create_accelerator_and_postprocess()\r\n  File \"anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 4127, in create_accelerator_and_postprocess\r\n    self.accelerator = Accelerator(\r\nTypeError: Accelerator.__init__() got an unexpected keyword argument 'use_seedable_sampler'\r\nTraceback (most recent call last):\r\n  File \"VLM/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"VLM/LLaVA/llava/train/train.py\", line 961, in train\r\n    trainer = LLaVATrainer(model=model,\r\n  File \"anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 367, in __init__\r\n    self.create_accelerator_and_postprocess()\r\n  File \"anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 4127, in create_accelerator_and_postprocess\r\n    self.accelerator = Accelerator(\r\nTypeError: Accelerator.__init__() got an unexpected keyword argument 'use_seedable_sampler'\r\nTraceback (most recent call last):\r\n  File \"VLM/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"VLM/LLaVA/llava/train/train.py\", line 961, in train\r\n    trainer = LLaVATrainer(model=model,\r\n  File \"anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 367, in __init__\r\n    self.create_accelerator_and_postprocess()\r\n  File \"anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 4127, in create_accelerator_and_postprocess\r\n    self.accelerator = Accelerator(\r\nTypeError: Accelerator.__init__() got an unexpected keyword argument 'use_seedable_sampler'\r\nTraceback (most recent call last):\r\n  File \"VLM/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"VLM/LLaVA/llava/train/train.py\", line 961, in train\r\n    trainer = LLaVATrainer(model=model,\r\n  File \"anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 367, in __init__\r\n    self.create_accelerator_and_postprocess()\r\n  File \"anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 4127, in create_accelerator_and_postprocess\r\n    self.accelerator = Accelerator(\r\nTypeError: Accelerator.__init__() got an unexpected keyword argument 'use_seedable_sampler'\r\n```</BODY>\n\n<COMMENTS>\n<Comment by ashmalvayani at 2024-05-20T06:33:11Z>\n@fisher75 what was the solution of this error?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1437,
    "state": "open",
    "created_by": "jirvin16",
    "created_at": "2024-04-22T01:06:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1437</URL>\n\n<TITLE>Cannot resume from checkpoint when 8-bit LoRA fine-tuning with deepspeed zero2_offload</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nThe error below occurs when trying to resume 8-bit LoRA fine-tuning using deepspeed zero2_offload. Any help to resolve this would be really appreciated. Note that the checkpoint at `./checkpoints/output_dir/checkpoint-10000` exists when running this command. Thank you!\r\n\r\nCommand:\r\n```\r\nHF_DATASETS_OFFLINE=0 TRANSFORMERS_OFFLINE=0  deepspeed llava/train/train.py \\\r\n    --bits 8 \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero2_offload.json \\\r\n    --model_name_or_path lmsys/vicuna-7b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path ./path/to/our/data.json \\\r\n    --image_folder ./path/to/image/folder \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-7b-pretrain/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/output_dir \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 10000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 48\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/username/LLaVA/llava/train/train.py\", line 1165, in <module>\r\n    train()\r\n  File \"/home/username/LLaVA/llava/train/train.py\", line 1141, in train\r\n    trainer.train(resume_from_checkpoint=True)\r\n  File \"/home/username/miniconda3/envs/llava/lib/python3.9/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/home/username/miniconda3/envs/llava/lib/python3.9/site-packages/transformers/trainer.py\", line 1676, in _inner_training_loop\r\n    deepspeed_load_checkpoint(self.model_wrapped, resume_from_checkpoint)\r\n  File \"/home/username/miniconda3/envs/llava/lib/python3.9/site-packages/transformers/deepspeed.py\", line 383, in deepspeed_load_checkpoint\r\n    load_path, _ = deepspeed_engine.load_checkpoint(\r\n  File \"/home/username/miniconda3/envs/llava/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 2752, in load_checkpoint\r\n    load_path, client_states = self._load_checkpoint(load_dir,\r\n  File \"/home/username/miniconda3/envs/llava/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 2837, in _load_checkpoint\r\n    self.load_module_state_dict(checkpoint=checkpoint,\r\n  File \"/home/username/miniconda3/envs/llava/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 2615, in load_module_state_dict\r\n    self.module.load_state_dict(\r\n  File \"/home/username/miniconda3/envs/llava/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 2139, in load_state_dict\r\n    load(self, state_dict)\r\n  File \"/home/username/miniconda3/envs/llava/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 2127, in load\r\n    load(child, child_state_dict, child_prefix)\r\n  File \"/home/username/miniconda3/envs/llava/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 2127, in load\r\n    load(child, child_state_dict, child_prefix)\r\n  File \"/home/username/miniconda3/envs/llava/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 2127, in load\r\n    load(child, child_state_dict, child_prefix)\r\n  [Previous line repeated 5 more times]\r\n  File \"/home/username/miniconda3/envs/llava/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 2121, in load\r\n    module._load_from_state_dict(\r\n  File \"/home/username/miniconda3/envs/llava/lib/python3.9/site-packages/bitsandbytes/nn/modules.py\", line 406, in _load_from_state_dict\r\n    super()._load_from_state_dict(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys,\r\n  File \"/home/username/miniconda3/envs/llava/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1991, in _load_from_state_dict\r\n    hook(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\r\n  File \"/home/username/miniconda3/envs/llava/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 72, in __call__\r\n    return self.hook(*args, **kwargs)\r\n  File \"/home/username/miniconda3/envs/llava/lib/python3.9/site-packages/bitsandbytes/nn/modules.py\", line 356, in maybe_rearrange_weight\r\n    tile_indices = get_tile_inds(weight_format, weight.device)\r\n  File \"/home/username/miniconda3/envs/llava/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py\", line 247, in get_tile_inds\r\n    return get_inverse_transform_indices(transform, _get_tile_size(format)).to(device)\r\n  File \"/home/username/miniconda3/envs/llava/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py\", line 79, in get_inverse_transform_indices\r\n    permuted_tile_i = transform_tile(sample_tile_i)\r\n  File \"/home/username/miniconda3/envs/llava/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py\", line 245, in <lambda>\r\n    transform = lambda x: F.transform(x.to(device), from_order=\"row\", to_order=format)[0].to(x.device)\r\n  File \"/home/username/miniconda3/envs/llava/lib/python3.9/site-packages/bitsandbytes/functional.py\", line 2080, in transform\r\n    prev_device = pre_call(A.device)\r\n  File \"/home/username/miniconda3/envs/llava/lib/python3.9/site-packages/bitsandbytes/functional.py\", line 416, in pre_call\r\n    torch.cuda.set_device(device)\r\n  File \"/home/username/miniconda3/envs/llava/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 406, in set_device\r\n    device = _get_device_index(device)\r\n  File \"/home/username/miniconda3/envs/llava/lib/python3.9/site-packages/torch/cuda/_utils.py\", line 34, in _get_device_index\r\n    raise ValueError(f\"Expected a cuda device, but got: {device}\")\r\nValueError: Expected a cuda device, but got: cpu\r\n```</BODY>\n\n<COMMENTS>\n<Comment by 20191864218 at 2024-04-23T01:55:10Z>\ntry to use zero2？\n</Comment>\n<Comment by jirvin16 at 2024-05-01T22:50:30Z>\n> try to use zero2？\r\n\r\nSame error with this unfortunately!\n</Comment>\n<Comment by amandalmia14 at 2024-09-20T21:25:29Z>\nDid anyone got the solution ? \r\nI'm stuck for the past few days !!\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path /cache/hub/llava-v1.5-7b \\\r\n    --version llava_llama_2 \\\r\n    --data_path /sample.json \\\r\n    --image_folder /images \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir /zero3-llava-lora-sample \\\r\n    --num_train_epochs 10 \\\r\n    --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 8 \\\r\n    --gradient_accumulation_steps 8 \\\r\n    --evaluation_strategy 'no' \\\r\n    --save_strategy 'epoch' \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 10 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\n</Comment>\n<Comment by ShreelekhaR at 2025-03-25T22:48:41Z>\nHaving the same issue! Any updates?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1436,
    "state": "open",
    "created_by": "devxsss",
    "created_at": "2024-04-21T15:47:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1436</URL>\n\n<TITLE>network error issue</TITLE>\n\n<BODY>### Question\n\nHi the demo has network error issue</BODY>\n\n<COMMENTS>\n<Comment by dansbecker at 2024-04-22T14:27:44Z>\nTo add some detail here, I tried the demo at https://llava.hliu.cc/ on April 22.\r\n\r\nWhen uploading an image, the file drop zone looks as follows\r\n![image](https://github.com/haotian-liu/LLaVA/assets/1390442/1e5be895-e13f-45dd-858d-d4d4f74deacd)\r\n\r\nThis is true for both png and jpeg files, and it happens with an image file that has worked with this demo in the past. The error message in the Chrome console is\r\n![image](https://github.com/haotian-liu/LLaVA/assets/1390442/34494299-2b61-4564-b5b9-d2be75e89b2c)\r\n\r\nI did this from Chrome version 121.0.6167.160 on Mac (which is reasonably up-to-date) and a colleague had this problem on Windows.\r\n\r\nI looked at the underlying source briefly and didn't see anything obvious as an explanation. It makes me wonder if something (e.g. permissions or space available) changed in the blob storage the image file goes to\n</Comment>\n<Comment by ddemillard at 2024-04-22T18:29:50Z>\nSame problem, I've also had the network error issue last night but was able to upload images\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1435,
    "state": "open",
    "created_by": "Carboxy",
    "created_at": "2024-04-21T10:15:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1435</URL>\n\n<TITLE>[Question] Reproduce LLaVA with Mistral backened</TITLE>\n\n<BODY>### Question\n\nThank you for your excellent work. I am reproducing LLaVA with Mistral backened，but there is always a performance gap. Could you provide the training scripts (Pretrain and Finetune) for LLaVA-Mistral ？Thanks a lot!</BODY>\n\n<COMMENTS>\n<Comment by lvaleriu at 2024-04-23T14:17:43Z>\n- did you use **mistralai/Mistral-7B-v0.1** or **mistralai/Mistral-7B-Instruct-v0.1/v0.2** ?\r\n- What conv_template are you using? (**plain**, **mistral_direct**, **mistral_instruct** or any of the **conv_llava_XXX**)\r\n>  --version plain\n</Comment>\n<Comment by chanangad at 2024-07-05T09:16:18Z>\ntry using llava_llama_2 conv_template and  mistralai/Mistral-7B-Instruct-v0.2. It works\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1434,
    "state": "open",
    "created_by": "kefaslungu",
    "created_at": "2024-04-20T01:04:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1434</URL>\n\n<TITLE>having problems reading the outputs of images.</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI'm trying to use the [gradio_client](https://pypi.org/project/gradio-client/) python library to run LLaVa\r\n## Command:\r\n```\r\nfrom gradio_client import Client, file\r\nimagePath = \"\"# a path to an image on your computer, or a url to an image\r\nclient = Client(\"https://llava.hliu.cc/\")\r\njob = client.submit(\"what image is this, file(imagePath), \"Crop\", api_name=\"/add_text\")\r\n\r\nreturn(job.result())# it returns a tuple, instead of a byte, string \r\n```\r\n\r\n## Log/output: \r\n```\r\n([['<img src=\"data:image/jpeg;base64,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\" alt=\"user upload image\" />what image is this', None]], '', None)                                     \r\n```\r\n\r\n### How I tried to fix the issue:\r\nI used this function\r\n```\r\nimport base64\r\ndef encodeImage(image_path):\r\n\twith open(image_path, \"rb\") as image_file:\r\n\t\treturn base64.b64encode(image_file.read()).decode('utf-8')\r\n```\r\nthen I tried to decode the image using the above function before submitting it to gradio, but it didn't work\r\n\r\nafter that, I did this:\r\n\r\n```\r\ndef decodeBytes(byte_data):\r\n    return base64.b64decode(byte_data).decode('utf-8')\r\n```\r\nThis function in the other hand, is being called after gradio's request, but I got an error like this:\r\n```Traceback (most recent call last):                                                                                                                                          \r\n  File \"C:\\Users\\kefas\\python programs\\gradio.py\", line 9, in <module>                                                                                                      \r\n    decoded_result = base64.b64decode(job.result()).decode('utf-8')                                                                                                         \r\n  File \"C:\\Program Files\\Python310\\lib\\base64.py\", line 80, in b64decode                                                                                                    \r\n    s = _bytes_from_decode_data(s)                                                                                                                                          \r\n  File \"C:\\Program Files\\Python310\\lib\\base64.py\", line 45, in _bytes_from_decode_data                                                                                      \r\n    raise TypeError(\"argument should be a bytes-like object or ASCII \"                                                                                                      \r\nTypeError: argument should be a bytes-like object or ASCII string, not 'tuple'\r\n```\r\nWhat can I do?</BODY>\n\n<COMMENTS>\n<Comment by MarinXue at 2024-05-07T09:43:34Z>\n> ### Describe the issue\r\n> Issue: I'm trying to use the [gradio_client](https://pypi.org/project/gradio-client/) python library to run LLaVa\r\n> \r\n> ## Command:\r\n> ```\r\n> from gradio_client import Client, file\r\n> imagePath = \"\"# a path to an image on your computer, or a url to an image\r\n> client = Client(\"https://llava.hliu.cc/\")\r\n> job = client.submit(\"what image is this, file(imagePath), \"Crop\", api_name=\"/add_text\")\r\n> \r\n> return(job.result())# it returns a tuple, instead of a byte, string \r\n> ```\r\n> \r\n> ## Log/output:\r\n> ```\r\n> ([['<img src=\"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAJkAxgDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoqK5uYLOBp7iVYolxlmOAO1OilSaJJYmDxuoZWHQg96AH0VTudVsbObybi5SOTAbaeuPX9DUX9vaX/AM/sf607MV0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFZ39vaX/wA/sf60f29pf/P7H+tFmF0aNFUrfV9Pup1ghuo3lbJCg8nHWrtKw7hRVe0vbW+jaS1nSZFbaShzg+lWKACiiigAooooAKKKKACiiigCC7tIL62a3uYxJE2MqSR0OR0rF17XLfw7p62tqgNwIsQxAHCKAcE8dBjpWhrd9Lp+nNLAivKzBF3HABPfpXmFzZXt1LNcXDpJK6lmYsMn5WP932rSEb7mc5W2LVjNPc3d1NKWeR8Fmw/95vQVew/91vyk/wAKbovh+7uJbgJ5GVAzlx/ecf3D6Vr/APCK3/pbf9/B/wDGq0bVzNJ2MrD/AN1vyk/wow/91vyk/wAK1f8AhFb/ANLb/v4P/jVH/CK3/pbf9/B/8apcyHysysP/AHW/KT/CjD/3W/KT/CtX/hFb/wBLb/v4P/jVH/CK3/pbf9/B/wDGqOZBysysP/db8pP8KMP/AHW/KT/CtX/hFb/0tv8Av4P/AI1R/wAIrf8Apbf9/B/8ao5kHKzKw/8Adb8pP8KMP/db8pP8K1f+EVv/AEtv+/g/+NUf8Irf+lt/38H/AMao5kHKzKw/91vyk/wow/8Adb8pP8K1f+EVv/S2/wC/g/8AjVH/AAit/wClt/38H/xqjmQcrMrD/wB1vyk/wow/91vyk/wrV/4RW/8AS2/7+D/41R/wit/6W3/fwf8AxqjmQcrMrD/3W/KT/CjD/wB1vyk/wrV/4RW/9Lb/AL+D/wCNUf8ACK3/AKW3/fwf/GqOZBysysP/AHW/KT/CjD/3W/KT/CtX/hFb/wBLb/v4P/jVH/CK3/pbf9/B/wDGqOZBysysP/db8pP8KMP/AHW/KT/CtX/hFb/0tv8Av4P/AI1R/wAIrf8Apbf9/B/8ao5kHKzKw/8Adb8pP8KMP/db8pP8K1f+EVv/AEtv+/g/+NUf8Irf+lt/38H/AMao5kHKzKw/91vyk/wow/8Adb8pP8K1f+EVv/S2/wC/g/8AjVH/AAit/wClt/38H/xqjmQcrMrD/wB1vyk/wow/91vyk/wrV/4RW/8AS2/7+D/41R/wit/6W3/fwf8AxqjmQcrMrD/3W/KT/CjD/wB1vyk/wrV/4RW/9Lb/AL+D/wCNUf8ACK3/AKW3/fwf/GqOZBysysP/AHW/KT/CjD/3W/KT/CtX/hFb/wBLb/v4P/jVH/CK3/pbf9/B/wDGqOZBysysP/db8pP8KMP/AHW/KT/CtX/hFb/0tv8Av4P/AI1R/wAIrf8Apbf9/B/8ao5kHKzKw/8Adb8pP8KMP/db8pP8K1f+EVv/AEtv+/g/+NUf8Irf+lt/38H/AMao5kHKzKw/91vyk/wow/8Adb8pP8K1f+EVv/S2/wC/g/8AjVH/AAit/wClt/38H/xqjmQcrMrD/wB1vyk/wow/91vyk/wrV/4RW/8AS2/7+D/41R/wit/6W3/fwf8AxqjmQcrMrD/3W/KT/CjD/wB1vyk/wrV/4RW/9Lb/AL+D/wCNUf8ACK3/AKW3/fwf/GqOZBysysP/AHW/KT/CjD/3W/KT/CtX/hFb/wBLb/v4P/jVH/CK3/pbf9/B/wDGqOZBysysP/db8pP8KMP/AHW/KT/CtX/hFb/0tv8Av4P/AI1R/wAIrf8Apbf9/B/8ao5kHKzKw/8Adb8pP8KMP/db8pP8K1f+EVv/AEtv+/g/+NUf8Irf+lt/38H/AMao5kHKzKw/91vyk/wow/8Adb8pP8K1f+EVv/S2/wC/g/8AjVH/AAit/wClt/38H/xqjmQcrMrD/wB1vyk/wow/91vyk/wrV/4RW/8AS2/7+D/41R/wit/6W3/fwf8AxqjmQcrMrD/3W/KT/CjD/wB1vyk/wrV/4RW/9Lb/AL+D/wCNUf8ACK3/AKW3/fwf/GqOZBysysP/AHW/KT/CjD/3W/KT/CtX/hFb/wBLb/v4P/jVH/CK3/pbf9/B/wDGqOZBysysP/db8pP8KMP/AHW/KT/CtX/hFb/0tv8Av4P/AI1R/wAIrf8Apbf9/B/8ao5kHKzKw/8Adb8pP8KMP/db8pP8K1f+EVv/AEtv+/g/+NUf8Irf+lt/38H/AMao5kHKzKw/91vyk/wow/8Adb8pP8K1f+EVv/S2/wC/g/8AjVH/AAit/wClt/38H/xqjmQcrMrD/wB1vyk/wow/91vyk/wrV/4RW/8AS2/7+D/41R/wit/6W3/fwf8AxqjmQcrMrD/3W/KT/CjD/wB1vyk/wrV/4RW/9Lb/AL+D/wCNUf8ACK3/AKW3/fwf/GqOZBysysP/AHW/KT/CjD/3W/KT/CtX/hFb/wBLb/v4P/jVH/CK3/pbf9/B/wDGqOZBysysP/db8pP8KMP/AHW/KT/CtX/hFb/0tv8Av4P/AI1R/wAIrf8Apbf9/B/8ao5kHKzKw/8Adb8pP8KMP/db8pP8K1f+EVv/AEtv+/g/+NUf8Irf+lt/38H/AMao5kHKzKw/91vyk/wow/8Adb8pP8K1f+EVv/S2/wC/g/8AjVH/AAit/wClt/38H/xqjmQcrMrD/wB1vyk/wow/91vyk/wrV/4RW/8AS2/7+D/41R/wit/6W3/fwf8AxqjmQcrMrD/3W/KT/CjD/wB1vyk/wrV/4RW/9Lb/AL+D/wCNUf8ACK3/AKW3/fwf/GqOZBysysP/AHW/KT/CjD/3W/KT/CtX/hFb/wBLb/v4P/jVH/CK3/pbf9/B/wDGqOZBysysP/db8pP8KMP/AHW/KT/CtX/hFb/0tv8Av4P/AI1R/wAIrf8Apbf9/B/8ao5kHKzKw/8Adb8pP8KMP/db8pP8K1f+EVv/AEtv+/g/+NUf8Irf+lt/38H/AMao5kHKzKw/91vyk/wow/8Adb8pP8K1f+EVv/S2/wC/g/8AjVH/AAit/wClt/38H/xqjmQcrMrD/wB1vyk/wow/91vyk/wrV/4RW/8AS2/7+D/41R/wit/6W3/fwf8AxqjmQcrOb1NpYntpELI6sSrDfkHHvXa+GfEi6vCILnC3qDkAEBwMfMOOOo4rl9c8P3dsLff5HzFsbXHp/uCsmDTruGWOWJkSRGBVgwyDuT/Z96bSkhJuLPV7HT7TTYmitIREjNuIBJyfx+gqzWV4f1C41DTFe6VRMmEYqchvlU56DHXpWrWD31N1toFFFFIYUUUUAFFFFABRRRQBjeJ/+QXH/wBd0/nXGN/q5P8Arkf/AEW9dn4n/wCQXH/13T+dcY3+rk/65H/0W9bU9jGe51Hhf/j4vP8AdX/0OSukrm/C/wDx8Xn+6v8A6HJXSVnPc0hsFFFFSUFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBznivpZf7z/yrmI+i/7w/wDQoq6fxX0sv95/5VzEfRf94f8AoUVbQ2MZ7nW+Ff8AkGy/9dF/9FJW7WF4V/5Bsv8A10X/ANFJW7WctzSOwUUUVJQUUUUAFFFFABRRRQBjeJ/+QXH/ANd0/nXGN/q5P+uR/wDRb12fif8A5Bcf/XdP51xjf6uT/rkf/Rb1tT2MZ7nUeF/+Pi8/3V/9DkrpK5vwv/x8Xn+6v/ocldJWc9zSGwUUUVJQUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABWVrviTSPDdn9p1W9jt0P3VJyzn0VRyaxPF/jN9Ini0bRoBe69crmOHPyQr3kkPZRXI+H/AAve63qB1Q3Qvrtj+81u6j3op/u2kR4wOm8jHoDXfRwkeT2td2j+L/yX9JMly6I0b/4ia5eQefpOjwabYMcJf61MIVf3WP7x/DNZqX3i3Ufn/wCEn1GUH+HS9EPl/g8gXP1ruH0jQfCtjcazdKJriFCz3l7JvlcgcAM3TPQAYHtVjw94rsvFGkTajpUM8kcfyhZE2FnxkqM+mcZ6ZrdV4wg5UaSstLtJ/nf9PQmz6s4ExeLIxu/t3xXH/tPpUMg/JWJog8VeLtOnWIazpWpN0FtqNu9hM3sCwCk/ia6bwV47m8Wanqdq2ly26WshxIzLhV6AHnk5VuRkcda0pPE3h3VNdfwzNsmu8P5sE8XyjGODu65zkYzkVc6tWMnTq0k7K7slovkv1QrdmZmmfEuwa8TT/EFlcaFftwq3Y/dP/uydD+ldurBlDKQQRkEd64vWPAULWTxaV5Rtjy2mXmZLZ/8Ad/iiPuvHtXH6VrOpeBJ5VjS7uNFgI+2aZcHdc6cCeHRv+WkXoRx9DWLw1HEK+H0fb+tvxXnfQpSa3PZaKrafqFrqlhDe2UyTW0yh45EOQRVmvNaadmWFFFFIDnPFfSy/3n/lXMR9F/3h/wChRV0/ivpZf7z/AMq5iPov+8P/AEKKtobGM9zrfCv/ACDZf+ui/wDopK3awvCv/INl/wCui/8AopK3azluaR2CiiipKCiiigAooooAKKKKAMbxP/yC4/8Arun864xv9XJ/1yP/AKLeuz8T/wDILj/67p/OuMb/AFcn/XI/+i3ransYz3Oo8L/8fF5/ur/6HJXSVzfhf/j4vP8AdX/0OSukrOe5pDYKKKKkoKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiig9KAMTxF4t0bwta+fqt4kRI+SJfmkf6L1/pXnGofEvxX9p0/VIdGSx0O4uVt41uh+8n3d/YY544+tT+AND0/W/GPinVNUtxeXdpqTxwNOS4Rct0B447enar/wAY/wDkG6B/2FIv617uHo4eliI4dx5pPdvZXV9F+rIbbVz0wdK4vxN8SdI0K4/s+0WTVNWY7Vs7UbiG9GI6fTk+1dn/AA/hXl/wmtbdtU8V3JhjM66nIglKjcFyTjPpXn4WlScJ1aiuo202vd2Kd9ix4O8YeJ77xtc6H4hs7a3LW32pIo/vQjjCk5OeDzXpNeaWn/JwF/8A9gtf/Za9LqswUFOMoRSvFOy80KIUVznjvxJP4S8G3+t20Ec8tsEKxyEhTlwvOPrWxpd21/pNneOoVp4ElKjoCyg4/WuAot0VyC+L7lviq/hH7LD9mXTvtnn5O/dnGMdMVF8SPGN/4N0nT7nTrKC7uLy9S1WOZio+YMRyPcCgDtKK8vu/HPxA0CE32u+Bon06MZmewvBI8a92285A/D613+ha5YeI9FttW0ybzbS4XcjYwR2II7EHIIoA0aK5jx/4uTwV4Tn1YRLNcb1it4WOBJIx6ce2T+FP8C+Kk8Z+ErTWBGsUz7knhU58uRTgj+R+hFAHSUVna7rlh4b0W51bU5vKtLddztjJPYADuScACvP7Txp8RtetxqWh+DLSLTHG6EX91tllXsQMjGf8k0Aeo0Vzng/xHf8AiLT7h9U0K60e8tpfJlhn5VmwDlG/iHPX+dU/Eni+50Txt4Y0KK1hkh1d5VkkcndHtAIx+fegDr6KxfE93r9npSy+G9Nt9QvvNUGG4l8tQmDk5yOc4/OvOZviF8RbfxPbeHJPCmlDU7iA3EcX2zIKDOTuzgdDxQB7BRXPeFL3xRe29y3ifSbTTplcCBbecSh1xyTycc1leGfG91feLNU8K6/aQ2WrWp8y38piUuoezrnnPfH+BoA7aiuJ1jxtdDx7YeEtAtIbu7K+dqMspOy0i49P4iO3uvrXbUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVz/jLxLH4W8Py3uzzbpyIrWAdZZW4Uf1/CugryXxNqE2sePp2t8SJoapb2aHlXvpzhSR32D5v+A114KiqtS8tlq/8AL5vQmTsh3hTwpNqV3dx6jKZmeTdrV1nm5m6/ZlPaNON2Op46A16JrdteN4eubXR1SO6aIxwESeWIzjAIIBxjr07VNo2lw6NpNtp8GSkKYLN1durMfckkn61eoxGJlUq83Rbf1/XYFHQ4zwRY6/c+HLiLxcqTy3DscyHLbfu7SuML0yMetdBovh/S/D1n9l0u0S3jIUNt6uQMZPqferMmpWUUQle6iWMhiGLDHynB/I8Ug1SxaaKEXcXmzLujQtyw9h+BrOrXnUcnsnrZbArImitYICDFDHGQuwFVA+X0+lYcXgrQofEba6lmpvWB3M/z5YkHfznDDGMjtWsdUsRIifa4tzp5ijd1Xnn6cH8qDqliruhu4g0aeY43fdXGcn2xURqVI35W1fcehxmn3HjRviJcQ3VuRoKfvFQSKdoYFR82MtyrHb2yOTxXR+IvD66vClxbOtvqduCba4K5HPVHH8SN0I/qBWrbXdvew+dbTJLHnG5DkZ9KmrWeIbmpRio2VtP19RW0PIPC+rHwh4gjtmja30PU7hoHtnbP9nXo6x5/uNwVPcEH1r1+vPfiB4eiu5uAEi1ZRaSt/cuFy1vJ7HcChPow9K2/AGuya/4Ps7m5J+2RZt7kHqJEODn68H8a6cWlWpLELfZ/o/wa+SfUUdHY6eiiivNLOc8V9LL/AHn/AJVzEfRf94f+hRV0/ivpZf7z/wAq5iPov+8P/Qoq2hsYz3Ot8K/8g2X/AK6L/wCikrdrC8K/8g2X/rov/opK3azluaR2CiiipKCiiigAooooAKKKKAMbxP8A8guP/run864xv9XJ/wBcj/6Leuz8T/8AILj/AOu6fzrjG/1cn/XI/wDot62p7GM9zqPC/wDx8Xn+6v8A6HJXSVzfhf8A4+Lz/dX/ANDkrpKznuaQ2CiiipKCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoPSijtQB5p8LP8AkN+NP+wq382o+MwKaJo1w3EUOpxM7f3RzzUfgqdNA+JfijQbs+XJfTC9tS3/AC0U5JA/P9DXc+JNBtfE2gXWk3eRHOuA4HKMOQw+hr2KtVUcfGtL4bRfyskQleNjUUhowykEEZBFea/CX/j78W/9hZ/5mqem+M9Z8AwLo3i/Tbma2gGy31O2XesiDoG9+nv7d65bwV43udOtdVtdH0q41DXNUvHnSNV/dxA9Cx/P0HvW1HLq6oVVHVO1nfTe97/mDkro7XTiLj4+6q8R3LBpqJIR2Py8frXplcd4A8I3Hh20ur7VZhca1qL+bdyA5C+ig+2T/kCuxrzcdUhKoowd1FJX72RUTg/jMpb4Ua3gE4WMn/v4tdR4adZPC2kOjBlayhII6EbBVrUtOtdX02506+iEtrcxmOVD3UjBrzaz8B+PfDUDab4Z8Y239kgnyI7+23yQA9gcHP6D2rjGLCQ/7SlztO7ZoYD4/hO4dfzH50vxu/5BXhn/ALDkH8mroPBHgOPwpJe6jeahLqmt6gQbq+lGCR/dUdh/gPQCm/EfwdfeM9J0+206+gs7izvUulkmQsPlBA4HuRQB2L7PLbzNuzB3bumO+a8v+Bf/ACLOteT/AMg/+2J/smOnl4Xp7U678C+PtfhNjr3jmNNOkGJo7CzEbyL3XdxgH8fpXf6Jolj4c0S30rS4BFa26bY1Jzk9SSe5J5JoA8d+JPjDRm+LGi6dq9wy6Vof+lzpHGZDLcEZRMD0+Xr6mnfCrxRZRfEbXNKs4bq30nWXa+sEuovLPmDlwB0wRnp2UV3fgnwH/wAI/Lqmo6y9rqGsajdtcSXIi4QHoq7uQBz+npUvjfwbc+I77Q9V0u7htNU0m6E0csqkq8Z+8hxzzgfr60Ac78cv+QBoTXGf7NGsQm99NnPX2616jGUMamPbswNu3pjtiqOt6JYeItGudK1OETWlwm11zgj0IPYg8g157aeCPiH4ft/7N0HxpayaYg2wfb7XfLCvYA4Ocfl7CgD1KvLfH/8AyV34ef8AXW4/ktdV4M8J3XhmG8l1DXLzV9QvXElxNOcICBgBE/hH+A9Kg8SeELnW/GvhnXYrqKKLSHlaSJlJaTcAOPyoA6+vL9W/5OO0H/sDS/zkr1CuRvPCFzc/FHTvFi3US21rYvatAVO9iS3IPTHzD8qAOurzb4v6EW0KPxbp8/2TWtBPnwzgfeTPzIfUc5/MdzXpNYni/RJfEnhHU9GgmSGW8hMayOCQp98UAc38JvDi6Z4XXXLqX7Vq+uYvbu5bqd3KqPYZ/Mn2rv6y/DmlyaJ4a0zS5ZFlktLaOFnQYDFVAyK1KACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAa7BI2Y9ACTXkfw8iOpahYXsvzNd3d7qshI64Ihj/Lc9er3n/HlPjr5bfyrzL4VY8vSPT+x2x9ftL5/pXo4b3cNUkt7pfhIiW6PVKKx76xll1y2uXtzc26R7VXeB5T7s78E88fjx71kpoV/LYS27xhJHvPMZnClWXc5BODlhyODiuBJDbZeuvDbTf2jsuQBcY8lSDiLLBn6H+JhmpToTyXtvdyTjfCI/kyzKSpbOcnJPzcE9CKrJok22OQw/v109oS7SZbzcADnvxnmmnRb4tfzfL5kkcSRjPzcKm75s8DIPFO/mTbyLUPh8xq2bli32MWw2khf4uSM8/e/So28Pz5vVFwmy5t1iGd3ykIFzjOD09M0z+zr6OPUhFbA3tw7hbtpBgozcd8javbH8PHWlttGnMemx3cEcjWczLv3k5iCts68nBK9fTNF/MdvI1NOsZLV7maaRHnuZA7+Wu1RhQoAH0HWr1YOj6de22m30Lgw3MpfZIdp5OcHIJJxkdaoLot/Jp8sEdubbfHCjq8ofzHVwXfrzxnrgmlbXcd7LY0fGFq134T1FY+JY4TPEfR4/nU/morlvh7cJF4s8U2UXEE8kOowjsBMm4/rXb3MKwaHNC20qluynauB909u1ed/DzP8Awmdznr/YVhu+uz/Cu/DO+Gqxfa/4oT3R6lRRRXnFnOeK+ll/vP8AyrmI+i/7w/8AQoq6fxX0sv8Aef8AlXMR9F/3h/6FFW0NjGe51vhX/kGy/wDXRf8A0UlbtYXhX/kGy/8AXRf/AEUlbtZy3NI7BRRRUlBRRRQAUUUUAFFFFAGN4n/5Bcf/AF3T+dcY3+rk/wCuR/8ARb12fif/AJBcf/XdP51xjf6uT/rkf/Rb1tT2MZ7nUeF/+Pi8/wB1f/Q5K6Sub8L/APHxef7q/wDocldJWc9zSGwUUUVJQUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAHK+MvBNt4rignjney1W0O61vIvvIeuD6jP5VzsV/8VNIT7LNpGnawF4W5SYRlh6kEj+VemUV2UsZKEFTnFSitr9PRqzE0eWTeEPGPjeZP+EtvodO0xGDfYLI5Ln3PI/U/Su/0Pw5pPhyzFrpdnHbp/EQMs59WY8k1qUUq2Mq1Y+z2iui0X9eoJJBRRRXIMKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigBrqHjZT0IINeR/DyU6bqFjZS/K1pdXulSAnoSRNH+e169eryXxNYTaN4+nW3wia4qXFm54Vb6A5VSe28fL/wKvRwLU4zovqr/AHX/AEbfyIl0Z6yc4OOtcvFeXo0pZZLq7Fw0yC8zDzbqc52Db06DPPHNbmkanBrGk21/b5CTJna3VD0Kn3ByD7irtcDTi2mimrnJtqurQC1mVZp4AZgQYsNKu4LGx445I9OMmn2Umu3E1gjTPlEDXDsoVGxIwbjbySBxgjHWupoo5vIXL5nKzapeSaXYFJpVl88pctsKkDDHB+Q45A6CrczX8N1fslzcuivAIlKggBmG7Hy84GfpW/RRcOU5U3+p+bMFluGnxcebD5PywhQfLKnHJOF7nOTWhotzqEt3PBfB90EMQ3lMLIx3EsPqNuR2IraoouCRieL7prTwnqTR8yyQmGIDu7/Io/NhXK/D23SXxZ4pvYjmCCSHToT2IhTaf1qT4geIYrOTHDRaUovJV/vznK28f13Zcj0QetbXgDQpNA8H2dtcg/bJc3FyT1MjnJz9OB+FehFexwbb3lp8tH+i+8W8jp6KKK80s5zxX0sv95/5VzEfRf8AeH/oUVdP4r6WX+8/8q5iPov+8P8A0KKtobGM9zrfCv8AyDZf+ui/+ikrdrC8K/8AINl/66L/AOikrdrOW5pHYKKKKkoKKKKACiiigAooooAxvE//ACC4/wDrun864xv9XJ/1yP8A6Leuz8T/APILj/67p/OuMb/Vyf8AXI/+i3ransYz3Oo8L/8AHxef7q/+hyV0lc34X/4+Lz/dX/0OSukrOe5pDYKKKKkoKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACuf8Y+Go/FPh+Wy3+TdIRNazjrFKvKn+n410FFXTqSpzU47oHqeUeE/FcunXV22oxmBkk26xbY5tZun2lR3ifjdjoeehJr0PXJ74+H7qTR4vtF28R8jZIq8kcNk8cdfesPxf4MfWJ4tZ0edbLXrZcRzY+SZe8cg7qa5Lw/4qvNEvzpT2v2O6U/Pol1JsU/7VpKeNp6+Wxx6EV6cqcMRatRWq3j/AF08/vtu89tGdd4C1bXdQ0I3XiaI29xI6mElVRGQgbcDOck5yD68V1VvcQ3UQlt5UljJIDIcgkHB5+oNcpqd3pPjfRrjQRftYXMwAlt54tk6AHPCt9PvDI9Kt+EfDM3hjwummC/klnVT87HckbHP3B2HfFc1eMJJ1H7sm/htpbuUrnS1XN/aC4e3NzEJkKhoy4DDd93j37VxfgVfGP8AaupN4jbZayN50CmMfPn5eDk7AAoO0/3utNv/AAPDF46XxbPrTQxQKrYuSrKGywIycBV2njuCSaTw1OFSUJzWi0a1TfYLu1w07XPFkvxEuNOudOlTRU/eKxEZZVYYUlgcbdyscD5hxmuk8Q6+mjwJDAiz6lcZFtb7sZI6sx/hRepbt9SKxdX8eW8dnJNpfkmBeG1K7Jjtk/3T1lPsmfqK4/S9H1Px5cSuj3dvos5H2zU7hdlxqAB4SNf+WcXsPxya6o0FUtVqxUIxVvX/AIPlv+Yr20RN4Y0o+MPEKXTyNcaJptwZ5LllwNRvT1fH9xeAo7AAetevVWsLC10uwhsrKBIbaFQkcaDAAqzXFisR7aemkVt/XdlRVgooormGc54r6WX+8/8AKuYj6L/vD/0KKun8V9LL/ef+VcxH0X/eH/oUVbQ2MZ7nW+Ff+QbL/wBdF/8ARSVu1heFf+QbL/10X/0UlbtZy3NI7BRRRUlBRRRQAUUUUAFFFFAGN4n/AOQXH/13T+dcY3+rk/65H/0W9dn4n/5Bcf8A13T+dcY3+rk/65H/ANFvW1PYxnudR4X/AOPi8/3V/wDQ5K6Sub8L/wDHxef7q/8AocldJWc9zSGwUUUVJQUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABWVrvhvSPEln9m1WyjuEH3SRhkPqrDkVq0VUJyg+aLswPMr/AOHWuWcIh0nV4NSsU5Sx1qHzgnssg+ZfwxWcLXxhpoEf/CN6nGB30zWtyfgku7FevUV3RzGdrVIqX4fk1+JHIuh5Gbrxe4x/Ynip/ZtRgQfmqZoi8LeL9TnWQ6Rpmnt1FxqVy9/MvuAxKg/gK9cop/2hZe5TS+9/m7Byd2cPpnw1sFvE1DxBe3Gu36cq12f3Uf8Aux9B+tduqqihVACgYAHalorjq16lV3m7/wBdEUklsFFFFZDCiiigDnPFfSy/3n/lXMR9F/3h/wChRV0/ivpZf7z/AMq5iPov+8P/AEKKtobGM9zrfCv/ACDZf+ui/wDopK3awvCv/INl/wCui/8AopK3azluaR2CiiipKCiiigAooooAKKKKAMbxP/yC4/8Arun864xv9XJ/1yP/AKLeuz8T/wDILj/67p/OuMb/AFcn/XI/+i3ransYz3Oo8L/8fF5/ur/6HJXSVzfhf/j4vP8AdX/0OSukrOe5pDYKKKKkoKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiims4Xr19KAHUgIPQ1A0hb2FICQciq5QLNFRrLnhuPepKlqwBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRQSAOaiaXsv500rgS5oqsSSck05ZSODyKfKBPRSKwYcGlqQCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDnPFfSy/3n/lXMR9F/3h/wChRV0/ivpZf7z/AMq5iPov+8P/AEKKtobGM9zrfCv/ACDZf+ui/wDopK3awvCv/INl/wCui/8AopK3azluaR2CiiipKCiiigAooooAKKKKAMbxP/yC4/8Arun864xv9XJ/1yP/AKLeuz8T/wDILj/67p/OuMb/AFcn/XI/+i3ransYz3Oo8L/8fF5/ur/6HJXSVzfhf/j4vP8AdX/0OSukrOe5pDYKKKKkoKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoooJx1oAKQsFHJqNpey/nUZJJyTmqUQHtKTwOBUdFFWlYAop6xlvYUw8E0XAKni+4KgqeL7gqZbAPoooqACiiigAooooAKKKKACiims6r9fSgB1RtKBwOTUbOzfT0ptWo9wFZix5NJRSgEnAFUAlFOZCuM9TTaAHxff/AAqeoIvv/hU9RLcAoooqQCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAOc8V9LL/ef+VcxH0X/eH/oUVdP4r6WX+8/8q5iPov8AvD/0KKtobGM9zrfCv/INl/66L/6KSt2sLwr/AMg2X/rov/opK3azluaR2CiiipKCiiigAooooAKKKKAMbxP/AMguP/run864xv8AVyf9cj/6Leuz8T/8guP/AK7p/OuMb/Vyf9cj/wCi3ransYz3Oo8L/wDHxef7q/8AocldJXN+F/8Aj4vP91f/AEOSukrOe5pDYKKKKkoKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACims4Xr19KhaQt7CmlcCVpAOByahZix5NJRVpWAKKACTgCpVi7t+VDdgI1Ut0FTLGF68mn9KKhyuAVWPU/WrNVj1P1pxASp4vuCoKni+4KctgH0UUVABRRRQAUUUUAFIzBRyaSRiq5FVyc8mqSuA9pSenAplFFWlYAop6xlvYVKqBelJysBGsRPLcVKAAMAUtFQ3cCKbqKiqWbqKiq47APi+/wDhU9QRff8AwqepluAUUUVIBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAHOeK+ll/vP8AyrmI+i/7w/8AQoq6fxX0sv8Aef8AlXMR9F/3h/6FFW0NjGe51vhX/kGy/wDXRf8A0UlbtYXhX/kGy/8AXRf/AEUlbtZy3NI7BRRRUlBRRRQAUUUUAFFFFAGN4n/5Bcf/AF3T+dcY3+rk/wCuR/8ARb12fif/AJBcf/XdP51xjf6uT/rkf/Rb1tT2MZ7nUeF/+Pi8/wB1f/Q5K6Sub8L/APHxef7q/wDocldJWc9zSGwUUUVJQUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVHKxXAHepKim6imtwIqKKVVLHArQBKkWInrwKkVAv19adUOXYBAoUYApaKKkAooooAKrHqfrVmqx6n61UQEqeL7gqCp4vuCnLYB9FFFQAUUUUAFFFFADJfufjUFTy/c/GoKuOwAOTip1jC9eTUK/eH1qzRJgFFFFQAUUUUARTdRUVSzdRUVaR2AfF9/8KnqCL7/AOFT1MtwCiiipAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA5zxX0sv95/5VzEfRf94f8AoUVdP4r6WX+8/wDKuYj6L/vD/wBCiraGxjPc63wr/wAg2X/rov8A6KSt2sLwr/yDZf8Arov/AKKSt2s5bmkdgoooqSgooooAKKKKACiiigDG8T/8guP/AK7p/OuMb/Vyf9cj/wCi3rs/E/8AyC4/+u6fzrjG/wBXJ/1yP/ot62p7GM9zqPC//Hxef7q/+hyV0lc34X/4+Lz/AHV/9DkrpKznuaQ2CiiipKCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACopuoqWopuopx3Aip8X3/wplPi+/8AhVvYCeiiiswCiiigAooooAKrHqfrVmqx6n61UQEqeL7gqCp4vuCnLYB9FFFQAUUUUAFFFFADJfufjUFTy/c/GoKuOwCr94fWrNVl+8PrVmlIAoooqQCiiigCKbqKiqWbqKirSOwD4vv/AIVPUEX3/wAKnqZbgFFFFSAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBznivpZf7z/wAq5iPov+8P/Qoq6fxX0sv95/5VzEfRf94f+hRVtDYxnudb4V/5Bsv/AF0X/wBFJW7WF4V/5Bsv/XRf/RSVu1nLc0jsFFFFSUFFFFABRRRQAUUUUAY3if8A5Bcf/XdP51xjf6uT/rkf/Rb12fif/kFx/wDXdP51xjf6uT/rkf8A0W9bU9jGe51Hhf8A4+Lz/dX/ANDkrpK5vwv/AMfF5/ur/wChyV0lZz3NIbBRRRUlBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABVPVtQTStIvNQdDIltC8zIpwWCjOP0q5WJ4x/wCRL1v/AK8Zv/QDTW4nsYy+N9Tj0qPV7rwrdppbxibz4rmOVljIBDFAQcYOfbvXW2d5BqFlDeWsgkgmQSRuO4IyK89sfFLTeBLPSdO0XVbu+fTo7df9FKxZ8sLuLnjb796uSw3/AIK0Lww7XTG1tJFttRRT8hWTjf8ARWIx9atxJUjvarPqFpFqMOnvMou5kaRIuclR1P61g+Kr+7/tDRdG06d4bm+ud0sidVgjG5/pngVg6xolzc/E2zRNbv4DPZyyK0ZXMQDD5FyOh96lRvuNy7HZazrUOirYmWJ5DeXkVmgXHys5xk+wwa064bxxa3Nvo3hy2iu2luk1i1VLi4G4l/mwzAYzz6YpniK31rwrph1+HxBe3ptnRrq2uVTy5ULAEKFUbTzQo3Dmsd5RXMeKNWv0vdJ0TSZVhvdTd/8ASSobyYkGWYA9Tzxms7WRqngu2i1ldavNRsI5UW9gvdjHYzBd6FVBBBI46c0KIcx3FFcd4lu9Vfxjoel6bqJs47yGcysED8KAcgHjd1wTxz0NRXC6n4V1/RydZvNQ0/ULj7JNFebWZHYEqysFGBkdP8g5Q5jtq5PU/FN6niOfRtN0OS/mt4llkf7SkQAbp97rXWV5tc6zBo3xR1iSe3u5hJZwKBbQNKRx3A6U6auwkzotE8TR6tfXOnXFnPYalbKHktpsHKn+JWHDDpz710EX3/writEhvNZ8bT+JJbG4sbNLIWcCXK7JJfm3FivYdevtUOi22s+I5tb3+IbuzhtdTuILdbdVDDa3G4kHKjIAAx9auSEpM9Corktd1DU4LzRvDen3uL+9VjNfvGpKRoPmYL03E9OMCqmrHVPBS22qf2xealpvnLFeQ3uxmRWON6MAMYOODkc1nyj5juKK5TxBqOo3fiOy8NaVdfY3lga6uroKGaOIHaAoPGS3Ge1ZuqR6x4e1rQLeLXry7sry+WOVLoKZOAeAwAyp7gjsPehRDmO9oritVm1a++Iq6LaarLZWbaSLiXy1DNnzSuVzkBjkDODwK3LPTb3S9HuoH1yW4k+Zorq8RWMIx3xjdjrkmlYdzZqsep+teb6r4itdCMd9p/jabU7mOVBPZyvG6TIWAYKEUbSMk9e1b3iy71OLWdAtNNvRatdzyxuxTeMbM5K9yOo98VpGLFzHU1PF9wV57rcWu+GbjT57PXri6jvrpLOVb5FkEbPwJFChcYI6VPrkWr+E7GLXY9evb2OGaNby2ughR42YKdoVRtOSKJRuHMd9RXJeLLvVE8QeHbHTL77IbySdJGKbxgR5zt7kcke+M1na3Fr3hi602Sz8QXF1HqF0llIt+iyCN5PuyLtC4xjp0qFEHI76qN5e3FvfWMENhLcR3DsssyMAsAAyC2eua5LXYdX8JQ22tR69fX8KXEaXlvdBCjozBSUCqNpBI/z11PEOoXdr4r8L20E7xw3U0yzIOjgJkZ/GjlDmOnorjNRk1fUPiBJo9pq0ljZ/2ak8nlorPnzGHyk8KTxzg9KdZPqXh/xlaaPcapcajYahBI8JutpkikTBPzADIIP+e5yhzHXS/c/GoK5W8udS8SeJ9Q0q01CbT9N01Y1nltwvmzSuM4DEHaAPbr+kGnPqun+PY9IudVmvbL+z3nj81VD53qPmIADEc4OO9XGOgcx2S/eH1rl7fxtfX8t2NO8M3t3FbXD27SpNGAWU88Eg+n511A+8PrXm3hPWtY05dbhsPDk+ownVrhjMlwiAHI+XB+g/Oi1wk7HoGj6je6jFK97pM+nMjAKk0isXHqNpNaVYx1ya28Lz6zqWnyWUkEUkr2rSBmG3OBkcc4H51iabpGv6xpMOrXXiS8tL65jEsUFuieRCGGVUoQS2O/NRYLnaUVwzeJr6++G+r3zEW2q2Amt5jF0WVOpX8CD+Nb0t3cDwO94JW+0DTjL5nff5ec/nScWh3NabqKirhtV1rVIfhfpGpQ3bC/lFtulbncWIzn2Pen67p+uaLo8+tw+I7ue7tV86SGVEEEij7yhAMrxnuT/OtYx0FzHcxff/AAqpr+tQ+HtEuNUnjeSOHaCiYySzBR19yKwtd8QXcVhpUOk7Uv8AWHWOB5BuEKkbmcjvgdqwvHehalp3gy8nPiG9vYy0P2mK7CFW/eLgphQV+bHGSMZqeW7Vwcux6ZRWR4hbZp6s+trpEAf97PhAxGPuqzcKffBrk9D8Qw2/jOz0mx8SSa3ZX0cuRMyu8EiDdncAMgjPHtUqN0DlY7p9QtE1GPT2mUXckZlWLnJQEAn8zUepXtxZJbG3sJbwyzpE4jYDy1PVznsP61w2qaJdXPxQhjTXNQgM1hJKrxlcxDeBsXI+73ra8S3F7o9j4cggvp2d9VtbaaZyN0qEkMG474o5dgudZRXK69qOoXniWz8NaXdGzeS3a7urpVDOkQbaAgPGSeMnpVO8l1PwfqmlvJqtzqWlX1ytpMt5tMkLtna6soHHqCKOUOY7aiuM1SXV734hf2PZ6q9laHSlnk2IGbPmsuVzwCeBkg8Ci2fU/DvjLTtKn1W41HT9Ujl8v7VtMkUkY3H5gBkEdqOUOY7OivPr2/nvfG2o6bqniG60SGFY/sEcLJEJwR8zbmB3HPGP/r13VnFLBaRRTXLXMirhpmUKX9yBgflSasNO5PRRRSGFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAHOeK+ll/vP8AyrmI+i/7w/8AQoq6fxX0sv8Aef8AlXMR9F/3h/6FFW0NjGe51vhX/kGy/wDXRf8A0UlbtYXhX/kGy/8AXRf/AEUlbtZy3NI7BRRRUlBRRRQAUUUUAFFFFAGN4n/5Bcf/AF3T+dcY3+rk/wCuR/8ARb12fif/AJBcf/XdP51xjf6uT/rkf/Rb1tT2MZ7nUeF/+Pi8/wB1f/Q5K6Sub8L/APHxef7q/wDocldJWc9zSGwUUUVJQUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVkeKbea68J6vb28bSzS2cqIijJZipAAFa9FC0BmV4ZgltvCukQTxtHNFZQo6MMFWCAEH3zU2taXFrWiXmmzY2XETJk/wnsfwOD+FX6ytbstWvYUTStWTT2wQ7NbiXIPccjBFNbi6HJ/D8X+r6jc6zqsZWaxgTSo8nOWTmVvqWxz9a0vEQutN8Y6VriWF3eWiW8ttKLSIyPGSQVO0ckcVvaHo9voOj2+m2xZkhHLt952JyzH3JJNaNU5a3Eo6HJeJ4rrV7Lw5Pb2NyCur2s8kTp88SAnJYDOMd6s+PbO5v8AwRqdraQPPPIihI4xlm+dTwK6SipuOxyniXTL8X2ja/ptv9pudMLrLbA4aWJ1wwXP8Q6gVQ12e+8a2keh2mkahZ2k0qNe3N7D5QSNWDbVB+8xIHTiu6op8wuU5bVbG5k+IPh67it5GtoILlZJFX5UJUYBPbNP8XWdzd3Ph1reCSUQatFLKUXOxArZY+g5FdNRRzDsFcfaWN1H8SdYvXt5FtZbSFI5SvysR1ANdhUM3UURdmDRHXPeCrO5szrouYJIvO1e5lj3rjcjEYYexroafF9/8Kt7BY5zxNpt9FrWl+I9NtzdS2IeOe1U4aWJxztz/EOuO9Z+tTXvjWO30a20i/tLFpkkvbi+h8oBFIbYoPLEkDnoK7qioUhOJyGv2d/pviyy8TWFnJexLbGzvLeHHmeWW3BkH8RB6j/Ix9c1mfWfEnhZU0m+tbdNQDeZeR+UzttPCoecAZyTiu21exvr2KM6fqkmnzxtkMIlkRx6Mp6/gRWVYeF7w63BrGuasdRurVWW1jSARRQ7hhjjJJJHc001uxNPoM+w3X/C1ft/2eT7J/Yvk+dt+Xf5+dufXHOKk8eabf6r4Qu7XTkMk5KMYQ2PNUMCVz7gV0tFTfW5VtLHnHiS+u9e8HzaRo/hfU7cERs4mtxCsYV1bCjqxyAMAY6nPFbPiGzubjxN4bnhgd4oLiVpXVchAYyAT6c111Vj1P1q4yFynNeMrO5vLbSVtoJJTFqtvK4Rc7UDHLH2FS+PLO51DwNfWtnBJPO7RbY41yxxKhPH0BNb9TxfcFDdrDaOc16yuZ/F/ha4igkeG3luDM6rkRgxYGT2yaPGVlc3q6D9mgkl8nWbaaTYudiKTlj7CumoqLhY5nx9Z3N/4Qube0gknmaWEhI1ySBKpPH0BpniKyurnxb4WuIbeSSG3nmMzquRGDHgZPbmupopqVgaOZjsrkfEye9MEn2U6SkQm2/KX80nbn1xzRq9lczePPDl3HBI1vBFdCWUL8qFlXGT2zXTUUrhY4idL7wt4r1O/XTrq+0vVdkjG0j8ySCVRg5UckHrkVS07UJ9T+KCTyafcWcY0lxGtwAsjL5o+YqCdvOQAeeM11Otabq1xMtzpetNZkLsaGSBZYm64OOCDz1z6cVT0Pw6+m311qV/fvqGpXKhHnaMRqqDoqqOg/HmtYvQmzubw+8PrXO+BLG6sLPWFu7eSFpdWuJUEi43IduGHsa6JfvD61ZqJPoVbUo61pqaxot7pzttW5haPd/dJHB/A1zOna/qmj6PBpl94e1KfUbaMQq1tFvhm2jCt5nRQeM5xj0rtKKlPoDRx2m+FLtfAuqabePGNR1QzzzlfupLIOB7gYH5VRi1nVp/Cp0E+GtTGpi0Nq7PGFtwdu3d5mcEY54z6e9d/RT5u4uU861TSr9/hdo1ilnM13ELbfCEO5drDOR7V0fiu3mu/Ceq29vE0s0lq6oiDJYkdBW7N2qKrUg5Tj9W0fUZdH8P39hAZNQ0jy5fszHaZF2gOnsaq+LdV1HxN4YuNN07w5qyyOY2la5g8oIFcNhcnLnIHTjGTniu9i+/+FT0nKzDlOK8Y2lyniDQ9XbTJ9U06z8wTW0CeY6uwAVwn8WP0qGWXUNc8deG9Qj0W9tdOsjcq0tymxiXiIyU6qvAAJxknpxXd0VPMHKcdr5utL8b6brS6deXln9jktZPskRkeNiwYEqOcHHWpfFMN1qtn4cmt7K4yur2txLGyfPEgJJLAZxjPNdZRSuFjk9csr7TvFVp4msLR72MWxs7y2ix5nl7tyug7kHqPT9Kl8174y1TS7ePS72y0uyukvLia9i8ppGTO1FU8kE9T/k9vRT5g5TmRZXP/Cz2vvIk+yf2MIfO2/Lv84nbn1xzRrllcz+N/Ct1FBI8Fu1150irlY90WFye2TxXTUUrjscxr+o2btLY6p4Z1G/gH+raOzFwj8dsElT9cdKf4F0++0zwxFb36SRN5jtFDI+5oYicqhPqBXSUUX0sFtbhRRRSGFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAHOeK+ll/vP/KuYj6L/ALw/9Cirp/FfSy/3n/lXMR9F/wB4f+hRVtDYxnudb4V/5Bsv/XRf/RSVu1heFf8AkGy/9dF/9FJW7WctzSOwUUUVJQUUUUAFFFFABRRRQBjeJ/8AkFx/9d0/nXGN/q5P+uR/9FvXZ+J/+QXH/wBd0/nXGN/q5P8Arkf/AEW9bU9jGe51Hhf/AI+Lz/dX/wBDkrpK5vwv/wAfF5/ur/6HJXSVnPc0hsFFFFSUFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFRTdRUtRTdRTjuBFT4vv/hTKfF9/8Kt7AT0UUVmAUUUUAFFFFABVY9T9as1WP3j9aqICVPF9wVBU8X3BTlsA+iiioAKKKKACiiigBkv3PxqCp5fufjUFXHYBV+8PrVmqy/eH1qzSkAUUUVIBRRRQBFN1FRVLN1FRVpHYB8X3/wAKnqCL7/4VPUy3AKKKKkAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDnPFfSy/3n/lXMR9F/3h/6FFXT+K+ll/vP/KuYj6L/ALw/9CiraGxjPc63wr/yDZf+ui/+ikrdrC8K/wDINl/66L/6KSt2s5bmkdgoooqSgooooAKKKKACiiigDG8T/wDILj/67p/OuMb/AFcn/XI/+i3rs/E//ILj/wCu6fzrjG/1cn/XI/8Aot62p7GM9zqPC/8Ax8Xn+6v/AKHJXSVzfhf/AI+Lz/dX/wBDkrpKznuaQ2CiiipKCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAObnW41nxNeWBv7m0trGGJlS2cI0jvu+YnHQBcAdM5zS+MJ7iy8Obree4WbzYY98RAkYGRVOD0yQT+daGo6DZalcpdSGeG5VPL862maJmTOdpKkZGaXU9LtdQ05bCcSeQNpGyRlYFSCpDZzkEDmrTV0TZkGiwvFanzDqGWkJ237q8g7duMVy/hnUr66120jW6v5Gdrprtbg/uTErsqGPPcHaPl7ZzXW2Fimnxskc91KGbdm5naUj6FjwPamWmiWFvJamKJlNq8jxHecgyElwfUEnofQelV0CzM7xHaz/2xpRh1TULdLy68iSOGYKoURO3AxwcqKua/NPp2k2Yt7iUMLy2iaQnLMpkVTk+4zmtO5sbe7mtZZlLPay+bEQxGG2lc+/DGi9sbfUIUiuULIkqSqAxHzKwZTx7gVF9h2MXxVNcpNpMED3oWe4ZZFsmVZGAjc4BPGMgH8Ks3LTWPg+7kjlu1mjtJZFe5YNKrbSRkjjIrSnsoLi4tp5VJktnLxHJGCVKn68E0+6tory0mtZlLRTI0bgHGVIwefxpXCxhanqN3b+E7OWGby7q6NtB9oYA+WZGVS+OmRkn64rU03Tjp0bp9tvLoMQc3Mm8r9DjvT5tNs7nTDps8IktDGIzGxPKjpz17daZpulw6YjrFNdTF8Za4uGlIA6AbjwKL6B1OZTxKT45MX9owmyaY6cLTeNwlC7/ADMdeuU/AVa8YRzx2KXVvf3lrIs8MWIJNqsHlVTkY64JxWqPDulrYLZi2HlrIJQ+Tv379+7d1zu561NqNjBfxeRcoWj3pJgMR8ysGHT3Aq01cVnYytbE2k+ENSa3u7hpoLaRknkfdJnqDmrHhuKVbeR5TquW28ahIrduq7T05q3e2kOoWU9ncqWgnQpIAcEg9eR0pmlaXDprSeVcXsu4AYuLp5QMegY8UPYdtTSdtiM2M4GcDvXEQXWoRaBpviVtTuJZ7qWFpbUkeSUlcL5arjgqGGD1yOetdzWND4X0uC8S4SOXbHIZo7czMYY5DnLKmdoPJ7cZ4qE0DTKXiqa5W90i3ge+CTSSeYli6rIwCEjluMZrd0+MxafAjNcEhBk3DBpP+BEcZpZrKC4u7a6kUmW2LGI5IxuGD9eKsUN6AkYvia8ubaztILWYwSXt3HamcAExqxOSM8ZwMD3Iq9p9gdPhaL7XdXIJyDcyb2X2Bxmn6hp9rqllJaXkQkhfGRkggg5BBHIIIyCKZp+mxabE6RzXMxc5Z7idpWP4seKOgdTjnvNQPhg+Kv7SuPP83f8AZNw8ny/N2eVtx1x367vyq9401C/02302bTpGDi8DSRj/AJaxqjMyH6hfzrTk8MaYLo3PlzY87z/s/nN5Pm5zv8vOM55+vPWrVzZQXkttJOhZraTzY+cYbaV59eCa0i0Kzsc9a6zcX/j6KK3uCdJFq6qo6SSgIxb8A4H50zxHcai2vXdvYSakbmOzie0S1/1YlLuMyZ4wcDOewNbWnaBpumNafY4DH9lWRIvnJwJGBbOevIH0rUWygS/lvVU+fLGsbNk8qpJHH/AjSk0mFnYzNfurm1j0ny5TG8uoQxS7OjKc5H0q7rM0lvoWoTQuUkjtpHRh2IUkGjVNKttXtkguvNCxyLKjRSNGysOhBBzT006FdLbT3aaaB0aNjNKzuwbOcsTnvUDMnwqkxtfPnfVSzxoT9ukVgSRkldp4/Gqs9jO/jVLUavqSW8lq90Y1mAUMJFAA4+7gnitnTNFg0onyLi9kXYECXF08qqB6BicVZNjbnUl1AoftKwmENk42EgkY6dQKd9RW0M7VriaLXtFhSRlimaYSKOjYjJGfxqjefaNT8TNpYvbi0toLRbhvsz7HldnKj5vRdvQdSea2bm1guL23uZFJltixiOcY3DB478VV1DR7XUpYppTPFPECqTW8zROFPVcr1BwOKpbBZlXQr66n0K7a4m82e0kuLfzwADJ5bEBuOM9M+4NYuj69qUg8O2F9ck3jyI0jgY+0wNC7K+PqMH3X3FdfY6da2mnixt4RHbKhQICeh689STnrUf8AYGm+Zpkn2f8AeaYu21fccoNu3Ge4x60m1cLM0qKKKgoKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA5zxX0sv8Aef8AlXMR9F/3h/6FFXT+K+ll/vP/ACrmI+i/7w/9CiraGxjPc63wr/yDZf8Arov/AKKSt2sLwr/yDZf+ui/+ikrdrOW5pHYKKKKkoKKKKACiiigAooooAxvE/wDyC4/+u6fzrjG/1cn/AFyP/ot67PxP/wAguP8A67p/OuMb/Vyf9cj/AOi3ransYz3Oo8L/APHxef7q/wDocldJXN+F/wDj4vP91f8A0OSukrOe5pDYKKKKkoKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKim6ipaim6inHcCKnxff8AwplPi+/+FW9gJ6KKKzAKKKKACiiigAqKYcA1LSMNykU1oBWpVO1ga57UtcubPxHbWsaxmwj2JfOR8yNKSsWD25HPswqb/hJ7AXdtbSLLHJPNND8wGI2i67vQHjHrkVpa4ro6SiucXxdax/2f5lvKkV6IyjM6Ar5hwuU3bj1GSBxmpB4pi8q5nbT7xbeGUwJJhf30ok8vYg3ZyW6E4FRysLo36K5VPFkkEt8L6ykikS6S2t7YsgdmMQc5bdtx1Oc9Pepz4thntUbT7O4up2iklaJNuYgjFTuJbBO4EAAnODijlYcyOjorn7vXbqz8H2mrrai5uJY4GZFIUZcqCeT/ALVPuvE8do8oewumW1jWS9ZNpFsGGeefmIHJ25wOaVmF0bUv3PxqCppGDRBgcg4IIqGqjsMVfvD61Zqsv3h9as0pAFFFFSAU122qTTqgkbccDoKaV2AyiinIu5gK0AmjGEFOoorIAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA5zxX0sv95/5VzEfRf94f8AoUVdP4r6WX+8/wDKuYj6L/vD/wBCiraGxjPc63wr/wAg2X/rov8A6KSt2sLwr/yDZf8Arov/AKKSt2s5bmkdgoooqSgooooAKKKKACiiigDG8T/8guP/AK7p/OuMb/Vyf9cj/wCi3rs/E/8AyC4/+u6fzrjG/wBXJ/1yP/ot62p7GM9zqPC//Hxef7q/+hyV0lc34X/4+Lz/AHV/9DkrpKznuaQ2CiiipKCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACopuxqWmSj5PpTW4EFPiPzimUqnDA1o9gLNFFFZAFFFFABRRRQAUUUUAc5feD9KvDfTXMTS3F4+9p2PzxnAC7D224GKhvPCOn3z6g8stwJL4RiRlYAoUxyvHBbAz611OMjBqu67D7dq0jJ7C5UYFz4Xtp9Re6W5mhSRoXeBFTaTFjZyRkDgZANXm0G1n0mSwEsyBrhrlZVI3xyGTzAy8Y4bpn8av0AkHI603qFkYz+Eop45HuL+4mvHuFuRcOifK4TZwu3bt28YIqWXwzvELxalcW9wsLQSSwpGvmRk5xt27QQehAyPettJA3B4NPqG2FkZs+iwTaAmkeZIsMcaRo4I3DZgqemCcqKp3fhlLx5i1/cot3Gsd6iBQLgKMZPHykjg7cZFb1FK7CyIpAFjVVGAOABUVSTH5gPSo6uOwxyDLirFQRDL59BU9TLcAooqJ5ey/nSSuASPj5R171FRRVpWAKnjXauT1NMjTJyelTVMn0AKKKKkAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA5zxX0sv8Aef8AlXMR9F/3h/6FFXT+K+ll/vP/ACrmI+i/7w/9CiraGxjPc63wr/yDZf8Arov/AKKSt2sLwr/yDZf+ui/+ikrdrOW5pHYKKKKkoKKKKACiiigAooooAxvE/wDyC4/+u6fzrjG/1cn/AFyP/ot67PxP/wAguP8A67p/OuMb/Vyf9cj/AOi3ransYz3Oo8L/APHxef7q/wDocldJXN+F/wDj4vP91f8A0OSukrOe5pDYKKKKkoKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKRhlSKWigCrRT5Fw/saZWqAsRtuQe1OqCNtrexqes2rMAoooBz0pAFFFFABRSMwVSzEADkk9qFYMoZSCDyCO9AC0jKGGDS0UAV2UqefzptWSARg1C8ZXkcirUgGU9ZGX3FMoqmgJxKp68U7IxnNVqKnlAUnLE+tJRRVASxkKpJI5pTKB05qGilygOZy3U/hTaKKYBT0j3cnpTki7t+VS1Ll2AOlFFFQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAHOeK+ll/vP8AyrmI+i/7w/8AQoq6fxX0sv8Aef8AlXMR9F/3h/6FFW0NjGe51vhX/kGy/wDXRf8A0UlbtYXhX/kGy/8AXRf/AEUlbtZy3NI7BRRRUlBRRRQAUUUUAFFFFAGN4n/5Bcf/AF3T+dcY3+rk/wCuR/8ARb12fif/AJBcf/XdP51xjf6uT/rkf/Rb1tT2MZ7nUeF/+Pi8/wB1f/Q5K6Sub8L/APHxef7q/wDocldJWc9zSGwUUUVJQUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUANkXcvvVerVQypg7h0qovoBHU0b5GD1qGjODkVTVwOf+I93c2PgHVLi0nkgmURhZI2KsAZFBwR7Eiue0Hx1pHhrwRoUF409zeTR8W9soeTljyQSOv1ya2Pie+74caqD1/c/+jkrzb4fi28N+J9MuNXto2h1SAGzun/5ZPnH0HIxntkHgE1cYpw1MpNqR7Ff+J7HSNBTV9VSaxjfGIZlHmlj0XCk88dM8d8ViaT8T9C1TUobFor2yknIELXcQVZCegBBPWuW+NP2hbzQpCQLVTJ8zLlQ+V6jvx2+tJrvhjxJ4ltLJtT8UaHJFvBtnX5NzNwApC8544pKEbJsHJ3shnxk8SxyeV4eh89JYnWac4ARwV+UZzk9ehFb3w3vfDcGjapc6S2ow2kG17j7c4KphSSVCkgdCT3rM+NKumgaMsjBpBMQxHc7Oa7fxXrf/CO+D7rUhbrcMkaosb/dYsQvze3PPr0ofwJIF8TbMBvi/wCG1u0iMWoCB2wLkwAJ9eu7H4Z9q6fXvE2meHNJGpX0xMLkCIRDc0hIyAv4fhXjnii813U/AdpqWp65ZtbXMqmDTYIEBXGRnI5GAOnPWu31Tw9aeJ/hr4dtrnUIrK4FtbtbyykYdzEPl5PORnpzxQ4RVgUm7m14d8faX4j1E6fDbX9tdbDIEuYNu5fXIJAH1xXVV5j4b8S+IdM8a2/hXXp7W/MkR8u5h5ZflLDJAHZecjPQ59fTjUTjZlxd0eV/CW9uL5fEN5fXE1xKJIyzuxZiAH45rrfD3iyx8TaVdajZQ3KQ2zFXEyqGJC7uMMR0PrXF/BxS+n+I1UZJdAB+D1X+Feq2Np4Q1y3ubuCGQO0m2SQKSDHjv7itZLV/IiMrJHb6J4z07XtEvdVtYbpILPd5iyqoY7V3HGGI6e9Ycnxe8NpaxzLHfO7kgwiJdyAHqfmxz7E1hfDj/knHiP8A7bf+iRVz4ZW0Fx8ONVSWJHWSaZXBHUeWtPlSuHNJ2O60XxFp2vaP/alnMRbjPmeYNpjI5Ib0wOa5v/ha3h43TRrFftbI21rsQfuh7nndj8K4bwy1wPhH4nEG4sJVyB/dOzd/47muq8LXWlQ/B6bfLAqiCdZwxGTIS2AR6kYx+FNxSBTbOm0zxnpureHL/XLeG6FrZFxIroodtihjtG7HQjqRT9P8XWGpeF7jxBDDcraQLIzI6qJCEGTgBsfrXCeA4nm+EPiRUGWLXOPf9wlHhPUrBPg3rNvJdQJcLHcJ5byAMSy/LgH1zipcUCmz0fw3rlr4o0ldSs45o4C7JiZQGyOvQkfrW2qhegrhfhD/AMiHF/18SfzrvKynu0aRd1cKKKKgYUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBznivpZf7z/yrmI+i/7w/wDQoq6fxX0sv95/5VzEfRf94f8AoUVbQ2MZ7nW+Ff8AkGy/9dF/9FJW7WF4V/5Bsv8A10X/ANFJW7WctzSOwUUUVJQUUUUAFFFFABRRRQBjeJ/+QXH/ANd0/nXGN/q5P+uR/wDRb12fif8A5Bcf/XdP51xjf6uT/rkf/Rb1tT2MZ7nUeF/+Pi8/3V/9DkrpK5vwv/x8Xn+6v/ocldJWc9zSGwUUUVJQUUUUAFFFFABRRRQAUUUUAFFFFABRRSMwUZIP4AmgBaKZ5q+j/wDfB/wo81fR/wDvg/4UAPopnmr6P/3wf8KPNX0f/vg/4UAPopnmr6P/AN8H/CjzV9H/AO+D/hQA+imeavo//fB/wo81fR/++D/hQA+imeavo/8A3wf8KPNX0f8A74P+FAD6KZ5q+j/98H/CjzV9H/74P+FAD6KZ5q+j/wDfB/wo81fR/wDvg/4UAPoIyMGmeavo/wD3wf8ACjzV9H/74P8AhQBE6FD7U2pjIhGCr/8AfB/wqFiF7Pj/AHDVqQFLVtKtNb0ybTr6MyW023eoYqThgw5HuBWfdeDdAv8ARbPSLqzZrSzJMAEjBlznPzZz3/lW15i+jf8AfJo8xfRv++TVXFZMivdC07U9IXTL6AXVqqhQJWLMMDAO7ru985rC0/4Y+FdOvku4rBpJI23IJpWdVP0PB/HNdGJtvTf/AN8mpBdDurfgpqfeWzCyM/X/AAzpXiaCGHVIGlSFiyBZCuCRjsavX2nWmp6dLYXkKzWsq7XjbuP896kFzH/t/wDfBp3np/tf98mpuwsjkIfhX4ShimjNg8nm8bnmYlRkH5Tnjp161sX/AIS0XVNGtdKvLMS2tqipACxDRhQAMMDnoB9a1/PT/a/75NJ9ojH97/vg/wCFPmkw5UYGgeBNA8N3bXen2rfaSCollcuVB6gZ4H866SoDdIOiuf8AgJpjXG7sw/4CaNXuCSWxnaH4b0nwwtwulwNH9oYNIGkLZIzjqeOprFm+G/hefVGv30872fzGjEjeWWznO3+nT2rqPMX0b/vk0eYvo3/fJq02uocqMrTvDGlaVp95Y2duY7a8LGZN5OdwwcenHpT9I8O6boemS6dYQtHbSszMpcsSSADyfYCtLzAez/8AfJqRcdWD/TYf8KHILIxtG8MaXoFjcWunWjeTMS0kbuX3nGMfMfSvJ9Qj0G1FyieCdVg1p1ZYoH3PAjnI3Dn5sZyBjHSvdhIgGAr/APfBo81f7r/98H/CkqjRLjc5H4a+HrnQPB4ttQi2T3MrTyRN/CCAoB98KPzp0Pww8JwX5uxppY5yInkZowf90n9DXW+avo//AHwf8KPNX0f/AL4P+FTzu9x8qtYo6Lodh4f04WGnRNHbhi4VnLcnrya0aZ5q+j/98H/Ck81fR/8Avg/4VO5RJRTPNX0f/vg/4Ueavo//AHwf8KAH0UzzV9H/AO+D/hR5q+j/APfB/wAKAH0UzzV9H/74P+FHmr6P/wB8H/CgB9FM81fR/wDvg/4Ueavo/wD3wf8ACgB9FM81fR/++D/hR5q+j/8AfB/woAfRTPNX0f8A74P+FHmr6P8A98H/AAoAfRTPNX0f/vg/4Ueavo//AHwf8KAH0UzzV9H/AO+D/hR5q+j/APfB/wAKAH0UzzV9H/74P+FHmr6P/wB8H/CgB9FNVwxwA34qRTqACiiigAooooAKKKKACiiigAooooAKKKKAOc8V9LL/AHn/AJVzEfRf94f+hRV0/ivpZf7z/wAq5iPov+8P/Qoq2hsYz3Ot8K/8g2X/AK6L/wCikrdrC8K/8g2X/rov/opK3azluaR2CiiipKCiiigAooooAKKKKAMbxP8A8guP/run864xv9XJ/wBcj/6Leuz8T/8AILj/AOu6fzrjG/1cn/XI/wDot62p7GM9zqPC/wDx8Xn+6v8A6HJXSVzfhf8A4+Lz/dX/ANDkrpKznuaQ2CiiipKCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAImi7r+VRkEHBFWaQgEYIzVKQFaipjCOxxTDGw7Z+lVzIBlFBBHUUUwCiiigAopQjHoDTxCe5xSugI6esbN14FSqir0HNOqXLsA1UC9BTqKKkAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA5zxX0sv95/5VzEfRf8AeH/oUVdP4r6WX+8/8q5iPov+8P8A0KKtobGM9zrfCv8AyDZf+ui/+ikrdrC8K/8AINl/66L/AOikrdrOW5pHYKKKKkoKKKKACiiigAooooAxvE//ACC4/wDrun864xv9XJ/1yP8A6Leuz8T/APILj/67p/OuMb/Vyf8AXI/+i3ransYz3Oo8L/8AHxef7q/+hyV0lc34X/4+Lz/dX/0OSukrOe5pDYKKKKkoKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigApMD0FLRQAm0egpaKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDnPFfSy/3n/lXMR9F/3h/6FFXT+K+ll/vP/KuYj6L/ALw/9CiraGxjPc63wr/yDZf+ui/+ikrdrC8K/wDINl/66L/6KSt2s5bmkdgoooqSgooooAKKKKACiiigDG8T/wDILj/67p/OuMb/AFcn/XI/+i3rs/E//ILj/wCu6fzrjG/1cn/XI/8Aot62p7GM9zqPC/8Ax8Xn+6v/AKHJXSVzfhf/AI+Lz/dX/wBDkrpKznuaQ2CiiipKCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKCQBknFFABRTZJEijaSR1RFGSzHAH41m/wBu28xK2EM982cboF+T/vs4X8iaANSisr/ieXPQWdkp9czOP/QQP1pDos02DdaxfyHuInWFf/HAD+tAGtRWM3hXR3OZraSZvWa4kkP/AI8xpreENAf/AJhsSn1Qsp/MGgDborl7KF7LxYtnpdxcSWEcDG9illMkcLnHlhSxJDHklc4xg8cV1FABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBznivpZf7z/yrmI+i/7w/wDQoq6fxX0sv95/5VzEfRf94f8AoUVbQ2MZ7nW+Ff8AkGy/9dF/9FJW7WF4V/5Bsv8A10X/ANFJW7WctzSOwUUUVJQUUUUAFFFFABRRRQBjeJ/+QXH/ANd0/nXGN/q5P+uR/wDRb12fif8A5Bcf/XdP51xjf6uT/rkf/Rb1tT2MZ7nUeF/+Pi8/3V/9DkrpK5vwv/x8Xn+6v/ocldJWc9zSGwUUUVJQUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRUc8y28DyvnagycdT7UAVr2SeWRLO1k8uRxuklxkxp7e56D8T2rEurSBPFek21mZftUQe5uZjKzHydpUK2TzuYjH+6fStG8v00PSpL66RpLiVgBDHy0sjcLGvqeg/M+tVbCIeH9Lu9W1iRWvrj99dMnIGB8saZ/hUcD8T3NMR0NFVLfUIptLh1CUNbRSRLIRPhSgIzhvQ1WN5e6hxp8fkQH/AJep16/7idT9TgfWkMq+INStLe6sLK6uEiSR/Pk3HkpHggAdTlyvA96lF9qmo8WNoLOA/wDLxeKdxHqsQ5/76I+lVtA0m1a9udbYPPcTny4Z523v5S8ZHZQxycDAxitaTU7ZZDFGxnlHWOEbyPrjgfjimIrR6DbNIst88l/MOQ1ycqp/2UHyj8s+9agAAAAwB0Aqkz6lLgxR28C/9NSXb8lwP1NNNze2kqfa1gkgdgnmRZUoScDIJPGcDOe9IZoUU1nVWVWYBmOFBPXvxTqAILq8t7KMSXEqxgnCg8lj6ADkn2FUy1/qS4UPYWx/ibHnMPYdE/HJ9hVmHTraG7kughe4cnMshLMB6DPQewq1QBXs7K3sLcQW0YSMEn1JJ6kk8kn1NWKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDnPFfSy/3n/lXMR9F/wB4f+hRV0/ivpZf7z/yrmI+i/7w/wDQoq2hsYz3Ot8K/wDINl/66L/6KSt2sLwr/wAg2X/rov8A6KSt2s5bmkdgoooqSgooooAKKKKACiiigDG8T/8AILj/AOu6fzrjG/1cn/XI/wDot67PxP8A8guP/run864xv9XJ/wBcj/6LetqexjPc6jwv/wAfF5/ur/6HJXSVzfhf/j4vP91f/Q5K6Ss57mkNgoooqSgooooAKKKKACiiigAooooAKKKKACiiigAoqrdalZ2bBJ7hFkPSMcufoo5NQC/vJ/8Aj102Tb/z0uW8ofly36CgDRorO+y6nNjztQSEd1tohn/vps/yFN/sK0kH+kvc3R/6bTsR/wB8ggfpQBclvbS3JE1zDGfRnANczP4z0u51WW3i+0zWtiA88sVs7qZD91Bgdec/984rR1i80vwrpMl8LSJXGEhiijAeaQ8Ki45JJqroWnyW6Wk+syRxXsxMiWu8Y81hl2P99+o9FAwPWmIr2kmo6jq51e70S8Hk5SxglaNBEp4Mhy2d7fTgcdzXO3ur6/4u8SGxstCV9O06crM81xiGSRexYDkA4yAD0I71teJ/E9rOLjQYrieC7mk8ktbhnkjQD5nKoCQOy9Mn2rG0LxPc6bZ3uj6H4cvntNNlfdNdERiGLG853EFn5JxkcYzTQju4NP2ol3q0yT3SDcSTiKI99inp9Tz71T13ULi8tk03SQTc3u5BO2QkcY+++ep4OBjuw5rBTXpbjRLPWJ4JFN/fQQ2AucFzGzDLlB8q5G4juBjmln8QGeDU9ctrpIIxN9jtriRQy7UPJXPYncxPooA56Kw7nTw6IWjRL66kmjRQq28f7qFQOg2jk/8AAia04oYoIxHDGkaDoqKAB+Fcsvia9a6afylS0iiV5IHTEg35EQzniRjg7ccA8nJrore/t7i7mtEkVrm3VTOi5IjLDIBPTPfHXH1oBFqsPxDclnsdMiYCW5nWSQ5/1cMZDux9BwF+rCta7u4LGzlu7lwkMSl3Y9gK88uxfavdXJm8y2jnK/2hMBloogf3dnH6yNnL46FselCBnT6NfQa7rNzfrMrLaqIreEdUVuTI3oWxwOygf3q6KuCmK2t1JZ6Wy2q7THNGknBnZVJ3P2WNFBYjn5gM5NXdLee20GCO1meKK4C29gr/AHljAJads9yNzY7DaO5osFzsKKqabe2+oWKXFrIZYDkJKf8AloBxuB7g4696t0hhRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAc54r6WX+8/8q5iPov+8P8A0KKun8V9LL/ef+VcxH0X/eH/AKFFW0NjGe51vhX/AJBsv/XRf/RSVu1heFf+QbL/ANdF/wDRSVu1nLc0jsFFFFSUFFFFABRRRQAUUUUAY3if/kFx/wDXdP51xjf6uT/rkf8A0W9dn4n/AOQXH/13T+dcY3+rk/65H/0W9bU9jGe51Hhf/j4vP91f/Q5K6Sub8L/8fF5/ur/6HJXSVnPc0hsFFFFSUFFFFABRRRQAUUUUAFFFFABRSEhQSSAB3NZvmHVfuyGOx9VbDT/Q9l/U/TqATSaiGlaGziNzKpwxU4RD/tN/QZPtTfsVzcc3d0wX/nlb5Rfxb7x/MVbiEMMaxxBERRhVXAAp29P7y/nQBFbWVtZqRbwRx56lV5P1PU1PTd6f3l/Ojen95fzoAdUF1PLCiiG3aaRjgAEAD3Y9h+dS70/vL+dZ2t6i9lYbbUq17cOILZT08w9z7KMsfYUAczIlrJ4huvEesXO+z0NGRXP+rE5+/sX/AGRhQepYn0AqzoOm3l7qj+JtXi26jOhSws3PFnAfX/bbqx/CtlNA0s6LDpVzDHdW8Y5E3O9jnLn3JJOfU1csLO1022EFuW2DvJKXY/VmJJp3FY5X+0o/D/hk38TJeazqk5jR9uDNOzbFyOypwMdgPU0l3HHHBZaAHUabJKy6lfSOF859pdkB7liDuPYHH06v7DYb5H8iHMud+QOcnJ/M8097ezkhSF4YHiQgqjKCAR0IFFwscf4mt5/EdsulWNle20kkyRLctDtjhgVgzOrdMkKAB16cUt5oG/U9Js1sJP7GsD5cMCrkMygEM/opPf8A2P8Aartd6f3l/Ojen95fzouFjhYPD/iR4rNpvsi3CajLc3TM5ZZWOQkigdkGAEOOQM4xXSeHNHk0SwktZWWZ2laV7ktl7hmOS7jHB6DAyOK1t6f3l/Ojen95fzouFjH8Radf6jbILKWJXj+aNJPu+bkbXb1CcsB3IHpSw6H9mSCOFwyWcJFuJTndMQcyv6nn/wAeY96196f3l/Ojen95fzpDOXuPBUNxo7WZupIpXhkSRoflEjuwYk99uQMjPIGDWtcaLb6nHZtqMSmWBSCsLsEOQNy44ypwOD6Vpb0/vL+dG9P7y/nQKxU07S7fS1kjtTIsDHKQFspF7IOw9qu03en95fzo3p/eX86BjqKbvT+8v50b0/vL+dADqKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDjIPEWoR/FG90e4lDaW8ESW67ADHOUZ8Zxk7lRzz02cdTTfDfii/1O/wDEt1KrXFjbvG1hBEEVvKww3BmKg7tm/LHgMOlXNX8HpqNxqtwmpPa3N6bdopUQFrZ4dwDLk85DEH2JqO68E2ctlf2tvdLDDcLaLFGYwyRi3IKqVyN6nABHGRkUAWbPxnZXaxk28sbfblsZh5kbiJ3TcjFkYgqxKqCD1YAgYONbTtUi1KW+WGNwlpcm2MjYxIygFivsCSv1U1y2paC1j4f1sO5vLzUdhiSwtRGIpkUCIqu47QGVTljgHqcV0mgaeuj6FaWLzLJNGmZpAf8AWSsS0j/ixY/jQBz8+o6lofiMxS6vPqluLG4vLuCSGJBaquDGVKKCN3zLhixO0kdDUem6rrVofDV9qWofaYtdYRy2vlIq2sjQtMvllQGKjYVO4sTkHI6Ve0Lw3e6Y90L/AFm21GO8Z3ui1nskmZhgZbeeAOAAMADApuneFZ7SfTRd619rtNJVhp8RhCsh2GNWlbcfMZUJUEBepJyaAG20mr2njKPSzrc2pRS2cs1yssESizbcoiKlFHDfONrFidhOeDl+iz6pb+LrzSptWm1W1itElllmhiQ28xbiMGNVByuWwckYHPzUmieHdV0eO8D+ILe4lut8jztZYlaZhhXY+YQQvAC4AwABirPhjRL3QIGt7nVoL2I5dmFt5ckkhOWkd97bifoO3YYoA6Kim70/vL+dG9P7y/nQA6im70/vL+dG9P7y/nQA6im70/vL+dG9P7y/nQA6im70/vL+dG9P7y/nQA6im70/vL+dG9P7y/nQA6im70/vL+dG9P7y/nQA6im70/vL+dG9P7y/nQA6im70/vL+dKGU9CD9DQAtFFFABRRRQAUUUUAFFFFABRRRQBznivpZf7z/AMq5iPov+8P/AEKKun8V9LL/AHn/AJVzEfRf94f+hRVtDYxnudb4V/5Bsv8A10X/ANFJW7WF4V/5Bsv/AF0X/wBFJW7WctzSOwUUUVJQUUUUAFFFFABRRRQBjeJ/+QXH/wBd0/nXGN/q5P8Arkf/AEW9dn4n/wCQXH/13T+dcY3+rk/65H/0W9bU9jGe51Hhf/j4vP8AdX/0OSukrm/C/wDx8Xn+6v8A6HJXSVnPc0hsFFFFSUFFFFABRRRQAUUUUAFFFFAFC+jN9Mtj/wAsSN9x7r2X8T19gfWrghiAAESADgfKKr2LiU3Mvczsv/fPy/0q3QAzyYv+eaf98ijyYv8Anmn/AHyKfRQAzyYv+eaf98ijyYv+eaf98in0UARtHCqlmSMADJJA4rE0mJNWv31p41FsFMVgpXqn8Uv/AAIgY/2QPU1Fczt4nvZNOtSf7Jgbbe3Cnidh1hQ+n94/8B9cdGqqihVAVQMAAYAFAhvkxf8APNP++RR5MX/PNP8AvkVQt9btru/e0gjuH2O0bTCE+UGXqN3TNImtwyanJYx2127RyeW8qwkxq2AeW+hFOzC6NDyYv+eaf98ijyYv+eaf98ismXxNYwXs1vJHchYJFilmEJMaM2MAt/wIfnVrUNXs9MmtIrpyrXUnlx4GefU+g6c+9FmF0XPJi/55p/3yKPJi/wCeaf8AfIqnqOrW+mtDHIk0s0xPlxQRl2bHJOB2FRahr1vptnFdT212YXXcWSEnZ0+96HmizC6NHyYv+eaf98ijyYv+eaf98iqX9rwLb2sssVxD9pmEKJJHtYMc4yOw4pbvV7Oy1C0sZ3InuyRGAMjj1Pb0FFmF0XPJi/55p/3yKPJi/wCeaf8AfIqle61Z6fqNnY3Lskt2SIjt+XI7E9s5GKntb6G7luY4t263l8p8jHzYB4/AiizC6JvJi/55p/3yKPJi/wCeaf8AfIrN1LXoNLuI4ZrW8cyEKjRQllZjnCg+vBqyNSg+1WlsyypNdRtIisuCAuMg+h5FFmF0WfJi/wCeaf8AfIo8mL/nmn/fIqhLrlnFp15fN5nk2kjRyYXncpwcDvzS2mtWt5FcuEnie2GZYpoyrqMZBwfUUWYXRe8mL/nmn/fIo8mL/nmn/fIqmur2rRafIC+2/IEPy+qlufTgUyLXbGXVLzTQ7C5tUDupXGQRnI9eo/OizC6NLpRUFldxX9lDdw7vKmQOu4YOCM1PSGFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFADWjRjlkUn1IpPJi/55p/3yK5zWZpE16ON5HjsXgQXLqcFVLNj6AnALdgfxDZL2TS9d1KclngfESxZ48wRKyADtn5x9cVXKRzHS+TF/wA80/75FHkxf880/wC+RXJ2s82kadFf3Mzy/Z7i4iuGJ+9ljg/99qoH+9U2npcxXVto1xK7yxyC7kYsTuTaCef+upP4CjlDnOm8mL/nmn/fIo8mL/nmn/fIrlrl59On1NLfbAzWLywiKQyAhTzI2ejfMMDnPqccXJ7K2S7t7WzdlS8glWXZITuTaMPnPUEgZ/2qOUOY3fJi/wCeaf8AfIo8mL/nmn/fIrkV1WZZE1m4dxFZoLW5QE4Em0l+O/z+WPzq3aLqdnfQQxRRTzfYlaYTXDRgOXYsRhWzyTRyhznR+TF/zzT/AL5FHkxf880/75Fc7ossz3tj5pwxhuyyhywyJ1HXjOPpXS0mrFJ3GeTF/wA80/75FHkxf880/wC+RT6KQxnkxf8APNP++RR5MX/PNP8AvkU+igBnkxf880/75FHkxf8APNP++RT6KAGeTF/zzT/vkUeTF/zzT/vkU+igBnkxf880/wC+RR5MX/PNP++RT6KAGeTF/wA80/75FHkxf880/wC+RT6KAGeTF/zzT/vkUqxopyqKD7CnUUAFFFFABRRRQAUUUUAFFFFABRRRQBznivpZf7z/AMq5iPov+8P/AEKKun8V9LL/AHn/AJVzEfRf94f+hRVtDYxnudb4V/5Bsv8A10X/ANFJW7WF4V/5Bsv/AF0X/wBFJW7WctzSOwUUUVJQUUUUAFFFFABRRRQBjeJ/+QXH/wBd0/nXGN/q5P8Arkf/AEW9dn4n/wCQXH/13T+dcY3+rk/65H/0W9bU9jGe51Hhf/j4vP8AdX/0OSukrm/C/wDx8Xn+6v8A6HJXSVnPc0hsFFFFSUFFFFABRRRQAUUUUAFFFFAGZZsLTVLuzc485jcw5/iBwGA9w3P/AAIVp1VvrCHUIRHLvVkO6OSNtrxt6qexqgE8QWnypLZX8Y6GXdBIfqVDKfyFAGzRWKdQ17OBoMWfX7cMf+g0oHiO5OGOnWKHuu+dx+e0fzoA1Li5gtIHnuZkhhQZZ3YAD8TWDK994m/dW3nWOkH/AFlwQUmuR6IOqKf7x5PbHWr0Gg24nS5vZZb+5Q5V7kgqh9VQYVfqBn3rVoAhtbWCxtYrW1hSGCJQqRoMBRU1FFAHMwWN/H4kW4tLOWytmkdrvdOrRTDBwyoDwxODnj3zRa6fcweJbq5exuWSW43pMl1iMLsA5TdzyD29K6aiq5ieU4m80LUZdX1CSKzkJnu45oZzcARKFC8tHn5uh7elXtX0C/1vVrmU3P2WCO28iAhVffu5Y8/d5Cj14rqKKOdi5Ec3qdteXul2LTaZPJfomfMt7lY3gkwAcHPIP4/SpdQsNSvPB32OYpNqLRxiQqQAzBgT/I1v0Ucw+Uy9Zs57ttNMKBvJvUlfnGFAbJ/WsbU9A1TVNQ1G9W5+zMoRLSParbwnzgk/w5f9BXW0UlJoHFM57U9Gl1q6tWuodkLWciS4YbopGKFce4IPPtT/AAxZ6laQXp1VU+0S3BbchBDgKqhvbO3OK3qKfM7WDlV7mVrNnPdyaYYEDCC9SWTnGFAYE/qKr6xoz6nrenyt5q20Mcod4pjGwJ245BB7Gt2ikm0NxTOTbQbxPCmrabHGxkmuJGhDS5LKWBBLHvj1q5pukXNpJq1tKXlS5AMV5K+52BXGxv8Ad7exroKKfMxcqOVsLDVJDottc2S28emcvN5oYSYQoNoHPOc84xTdQ0C+luNSvrVUS9W4E1oxYYkXy1Vkb0BwR+RrrKKOZhyIoaJbS2eh2NtOu2WKBEdc5wQOeav0UVL1KQUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUARvbwyFi8MbF02MWUHcvofb2pn2O2xj7NDjcrY2DquNp+owMemKnooAiNtA0TRGCMxs25lKDBbOckeuead5UfnGby180rt345x6Z9KfRQBXtrCzsy5trWCEv8AeMcYXd9cdajt9Ls7S6a4tbdIHZdriJQofnIyB1I559zVyii4rIhNrbmJ4zBEY3beylBhmznJHc55qTy4/N83Yvmbdu/HOPTPpTqKBlOPTootQ+1qzDajIkQChU3EMxGBnJIB596uUUUBYKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA5zxX0sv95/5VzEfRf8AeH/oUVdP4r6WX+8/8q5iPov+8P8A0KKtobGM9zrfCv8AyDZf+ui/+ikrdrC8K/8AINl/66L/AOikrdrOW5pHYKKKKkoKKKKACiiigAooooAxvE//ACC4/wDrun864xv9XJ/1yP8A6Leuz8T/APILj/67p/OuMb/Vyf8AXI/+i3ransYz3Oo8L/8AHxef7q/+hyV0lc34X/4+Lz/dX/0OSukrOe5pDYKKKKkoKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAMHxeGbQgixmUvcwL5fmbN4Mqjbu7Z6VnK15aSw6Xa+RoamCW6YswnDEMBjLYGMHJ79OldZJFHMoWWNXUEMAwzyDkH8DUV1Y2l8qrd2sNwqnKiVAwB/GmKxyieK7x9E1C9fyIpYdOt7qNSOA7hs9TyMgYqe51PVlv9QnjvIktbK9ggFuYQd6uI92WzkH5zjHpzmt660bTL6ZJrrT7aeSNdqtJEGIHpz2qybeA78wx/OwZ/lHzEYwT6ngflQBxdzqF9d3emXU13CYW1d4Us1TDp5YkA5zkk7ckY70lp4i8QPoM+qyxReW9i9ym5UCxyDBVRtcsy4JByAQR74rrv7J077Ybz7DbfaS27zvKG7PrnrmkTSNNjkndLC2V5wRKREoMgPUHjmi4WHWPnRQrDeXcdxdkb22qE4J7L6DpmuRvrzy/CnilDcFbhbyZEXfhwWK7AO/ORjHrXYrZW6ag98E/wBIeJYi5P8AACSAPTkmmS6Vp818l7LY2z3SY2zNEC4x0wcZoA5HW7+/u/tJ+1wxW0GrW1oLUp8zfvIzu3Zzkk8DGMCnv4ru472SeEtc2UkN1JCJIlQEwjouGLEZBBLAe1dVNpWn3F0Lqaxt5LgY/etEC3ByOfali0rT4Ll7iGxt453JLSLEAxJ68470XCxgma/VdJNxfQXUl1KzLIkIURfuHYbQDzyO/as628Q6jpPh7Try9uVvUu7DMLFMMbnAKISDzuyR9RXX2+l2Foc29lbwkPvGyMDDYxnjvgkVRn8PW8t1aeX5UFjbzfaTaxQgB5hnDE+3XGOoHNFwNS1E62kIuWVrgIPMZBgFsc4HpmpaKKQwooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDnPFfSy/3n/lXMR9F/wB4f+hRV0/ivpZf7z/yrmI+i/7w/wDQoq2hsYz3Ot8K/wDINl/66L/6KSt2sLwr/wAg2X/rov8A6KSt2s5bmkdgoooqSgooooAKKKKACiiigDG8T/8AILj/AOu6fzrjG/1cn/XI/wDot67PxP8A8guP/run864xv9XJ/wBcj/6LetqexjPc6jwv/wAfF5/ur/6HJXSVzfhf/j4vP91f/Q5K6Ss57mkNgoooqSgooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAOc8V9LL/ef+VcxH0X/eH/oUVdP4r6WX+8/8q5iPov8AvD/0KKtobGM9zrfCv/INl/66L/6KSt2sLwr/AMg2X/rov/opK3azluaR2CiiipKCiiigAooooAKKKKAMbxP/AMguP/run864xv8AVyf9cj/6Leuy8UELpSEkAeenJPvXFNJH5cn7xP8AVH+If883ransYz3Or8L/APHxef7q/wDocldJXL+GZ4UnvN00Yyq9WH9+Sui+12//AD8Rf99is57mkdiaioftdv8A8/EX/fYo+12//PxF/wB9ipKuTUVD9rt/+fiL/vsUfa7f/n4i/wC+xQFyaioftdv/AM/EX/fYo+12/wDz8Rf99igLk1FQ/a7f/n4i/wC+xR9rt/8An4i/77FAXJqKh+12/wDz8Rf99ij7Xb/8/EX/AH2KAuTUVD9rt/8An4i/77FH2u3/AOfiL/vsUBcmoqH7Xb/8/EX/AH2KPtdv/wA/EX/fYoC5NRUP2u3/AOfiL/vsUfa7f/n4i/77FAXJqKh+12//AD8Rf99ij7Xb/wDPxF/32KAuTUVD9rt/+fiL/vsUfa7f/n4i/wC+xQFyaioftdv/AM/EX/fYo+12/wDz8Rf99igLk1FQ/a7f/n4i/wC+xR9rt/8An4i/77FAXJqKh+12/wDz8Rf99ij7Xb/8/EX/AH2KAuTUVD9rt/8An4i/77FH2u3/AOfiL/vsUBcmoqH7Xb/8/EX/AH2KPtdv/wA/EX/fYoC5NRUP2u3/AOfiL/vsUfa7f/n4i/77FAXJqKh+12//AD8Rf99ij7Xb/wDPxF/32KAuTUVD9rt/+fiL/vsUfa7f/n4i/wC+xQFyaioftdv/AM/EX/fYo+12/wDz8Rf99igLk1FQ/a7f/n4i/wC+xR9rt/8An4i/77FAXJqKh+12/wDz8Rf99ij7Xb/8/EX/AH2KAuTUVD9rt/8An4i/77FH2u3/AOfiL/vsUBcmoqH7Xb/8/EX/AH2KPtdv/wA/EX/fYoC5NRUP2u3/AOfiL/vsUfa7f/n4i/77FAXJqKh+12//AD8Rf99ij7Xb/wDPxF/32KAuTUVD9rt/+fiL/vsUfa7f/n4i/wC+xQFyaioftdv/AM/EX/fYo+12/wDz8Rf99igLk1FQ/a7f/n4i/wC+xR9rt/8An4i/77FAXJqKh+12/wDz8Rf99ij7Xb/8/EX/AH2KAuTUVD9rt/8An4i/77FH2u3/AOfiL/vsUBcmoqH7Xb/8/EX/AH2KPtdv/wA/EX/fYoC5NRUP2u3/AOfiL/vsUfa7f/n4i/77FAXJqKh+12//AD8Rf99ij7Xb/wDPxF/32KAuTUVD9rt/+fiL/vsUfa7f/n4i/wC+xQFyaioftdv/AM/EX/fYo+12/wDz8Rf99igLk1FQ/a7f/n4i/wC+xR9rt/8An4i/77FAXJqKh+12/wDz8Rf99ij7Xb/8/EX/AH2KAuTUVD9rt/8An4i/77FH2u3/AOfiL/vsUBcmoqH7Xb/8/EX/AH2KPtdv/wA/EX/fYoC5NRUP2u3/AOfiL/vsUfa7f/n4i/77FAXJqKh+12//AD8Rf99ij7Xb/wDPxF/32KAuTUVD9rt/+fiL/vsUfa7f/n4i/wC+xQFzC8V9LL/ef+VcxH0X/eH/AKFFXR+KZ4XFntmjOGfow9K5qOSPC/vE+8P4h/ejreGxjLc6/wAK/wDINl/66L/6KSt2sHwmQ2mSlSCPMXkH/pmlb1ZS3NI7BRRRUlBRRRQAUUUUAFFFFAENzbQ3lu8FxGskbjBVhkV5f4j8OPokzOq77NwfLkI+7w3yk+tel6m97HYSNp8SSXQxsV+h5Ge47Z70qwNe6YsOoRJvli2zIOgJHIH61cJOOpE4qR5Xpixb5s+X0HUD+83vWjth/wCmX5D/ABrUfwvfWF9OLRJJ7dwCjCTBXljg5kGcZHNO/sfVv+fOb/v9/wDba1ckzJRaMnbD/wBMvyH+NG2H/pl+Q/xrW/sfVv8Anzm/7/f/AG2j+x9W/wCfOb/v9/8AbaOZDszJ2w/9MvyH+NG2H/pl+Q/xrW/sfVv+fOb/AL/f/baP7H1b/nzm/wC/3/22jmQWZk7Yf+mX5D/GjbD/ANMvyH+Na39j6t/z5zf9/v8A7bR/Y+rf8+c3/f7/AO20cyCzMnbD/wBMvyH+NG2H/pl+Q/xrW/sfVv8Anzm/7/f/AG2j+x9W/wCfOb/v9/8AbaOZBZmTth/6ZfkP8aNsP/TL8h/jWt/Y+rf8+c3/AH+/+20f2Pq3/PnN/wB/v/ttHMgszJ2w/wDTL8h/jRth/wCmX5D/ABrW/sfVv+fOb/v9/wDbaP7H1b/nzm/7/f8A22jmQWZk7Yf+mX5D/GjbD/0y/If41rf2Pq3/AD5zf9/v/ttH9j6t/wA+c3/f7/7bRzILMydsP/TL8h/jRth/6ZfkP8a1v7H1b/nzm/7/AH/22j+x9W/585v+/wB/9to5kFmZO2H/AKZfkP8AGjbD/wBMvyH+Na39j6t/z5zf9/v/ALbR/Y+rf8+c3/f7/wC20cyCzMnbD/0y/If40bYf+mX5D/Gtb+x9W/585v8Av9/9to/sfVv+fOb/AL/f/baOZBZmTth/6ZfkP8aNsP8A0y/If41rf2Pq3/PnN/3+/wDttH9j6t/z5zf9/v8A7bRzILMydsP/AEy/If40bYf+mX5D/Gtb+x9W/wCfOb/v9/8AbaP7H1b/AJ85v+/3/wBto5kFmZO2H/pl+Q/xo2w/9MvyH+Na39j6t/z5zf8Af7/7bR/Y+rf8+c3/AH+/+20cyCzMnbD/ANMvyH+NG2H/AKZfkP8AGtb+x9W/585v+/3/ANto/sfVv+fOb/v9/wDbaOZBZmTth/6ZfkP8aNsP/TL8h/jWt/Y+rf8APnN/3+/+20f2Pq3/AD5zf9/v/ttHMgszJ2w/9MvyH+NG2H/pl+Q/xrW/sfVv+fOb/v8Af/baP7H1b/nzm/7/AH/22jmQWZk7Yf8Apl+Q/wAaNsP/AEy/If41rf2Pq3/PnN/3+/8AttH9j6t/z5zf9/v/ALbRzILMydsP/TL8h/jRth/6ZfkP8a1v7H1b/nzm/wC/3/22j+x9W/585v8Av9/9to5kFmZO2H/pl+Q/xo2w/wDTL8h/jWt/Y+rf8+c3/f7/AO20f2Pq3/PnN/3+/wDttHMgszJ2w/8ATL8h/jRth/6ZfkP8a1v7H1b/AJ85v+/3/wBto/sfVv8Anzm/7/f/AG2jmQWZk7Yf+mX5D/GjbD/0y/If41rf2Pq3/PnN/wB/v/ttH9j6t/z5zf8Af7/7bRzILMydsP8A0y/If40bYf8Apl+Q/wAa1v7H1b/nzm/7/f8A22j+x9W/585v+/3/ANto5kFmZO2H/pl+Q/xo2w/9MvyH+Na39j6t/wA+c3/f7/7bR/Y+rf8APnN/3+/+20cyCzMnbD/0y/If40bYf+mX5D/Gtb+x9W/585v+/wB/9to/sfVv+fOb/v8Af/baOZBZmTth/wCmX5D/ABo2w/8ATL8h/jWt/Y+rf8+c3/f7/wC20f2Pq3/PnN/3+/8AttHMgszJ2w/9MvyH+NG2H/pl+Q/xrW/sfVv+fOb/AL/f/baP7H1b/nzm/wC/3/22jmQWZk7Yf+mX5D/GjbD/ANMvyH+Na39j6t/z5zf9/v8A7bR/Y+rf8+c3/f7/AO20cyCzMnbD/wBMvyH+NG2H/pl+Q/xrW/sfVv8Anzm/7/f/AG2j+x9W/wCfOb/v9/8AbaOZBZmTth/6ZfkP8aNsP/TL8h/jWt/Y+rf8+c3/AH+/+20f2Pq3/PnN/wB/v/ttHMgszJ2w/wDTL8h/jRth/wCmX5D/ABrW/sfVv+fOb/v9/wDbaP7H1b/nzm/7/f8A22jmQWZk7Yf+mX5D/GjbD/0y/If41rf2Pq3/AD5zf9/v/ttH9j6t/wA+c3/f7/7bRzILMydsP/TL8h/jRth/6ZfkP8a1v7H1b/nzm/7/AH/22j+x9W/585v+/wB/9to5kFmZO2H/AKZfkP8AGjbD/wBMvyH+Na39j6t/z5zf9/v/ALbR/Y+rf8+c3/f7/wC20cyCzMnbD/0y/If40bYf+mX5D/Gtb+x9W/585v8Av9/9to/sfVv+fOb/AL/f/baOZBZmTth/6ZfkP8aNsP8A0y/If41rf2Pq3/PnN/3+/wDttH9j6t/z5zf9/v8A7bRzILMydsP/AEy/If40bYf+mX5D/Gtb+x9W/wCfOb/v9/8AbaP7H1b/AJ85v+/3/wBto5kFmZO2H/pl+Q/xo2w/9MvyH+Na39j6t/z5zf8Af7/7bR/Y+rf8+c3/AH+/+20cyCzMnbD/ANMvyH+NG2H/AKZfkP8AGtb+x9W/585v+/3/ANto/sfVv+fOb/v9/wDbaOZBZmTth/6ZfkP8aNsP/TL8h/jWt/Y+rf8APnN/3+/+20f2Pq3/AD5zf9/v/ttHMgszJ2w/9MvyH+NG2H/pl+Q/xrW/sfVv+fOb/v8Af/baP7H1b/nzm/7/AH/22jmQWZk7Yf8Apl+Q/wAaNsP/AEy/If41rf2Pq3/PnN/3+/8AttH9j6t/z5zf9/v/ALbRzILMydsP/TL8h/jRth/6ZfkP8a1v7H1b/nzm/wC/3/22j+x9W/585v8Av9/9to5kFmczqixYhx5fVugHp9ak8P8Ah+XW7kBQFtkb95Lj3U7R74rebwxqOoXVuk6SW8Kli8jSbscdAPMNdpaWkFjapbW6bIkGFGc/rSlUsrII07u7Es7O30+1S2to1jiQAAAYzx1PqferFZ+kS6nLbSHVII4phIQgQjBXA56nvmtCsWbIKKKKQwooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA//9k=\" alt=\"user upload image\" />what image is this', None]], '', None)                                     \r\n> ```\r\n> \r\n> ### How I tried to fix the issue:\r\n> I used this function\r\n> \r\n> ```\r\n> import base64\r\n> def encodeImage(image_path):\r\n> \twith open(image_path, \"rb\") as image_file:\r\n> \t\treturn base64.b64encode(image_file.read()).decode('utf-8')\r\n> ```\r\n> \r\n> then I tried to decode the image using the above function before submitting it to gradio, but it didn't work\r\n> \r\n> after that, I did this:\r\n> \r\n> ```\r\n> def decodeBytes(byte_data):\r\n>     return base64.b64decode(byte_data).decode('utf-8')\r\n> ```\r\n> \r\n> This function in the other hand, is being called after gradio's request, but I got an error like this:\r\n> \r\n> ```\r\n>   File \"C:\\Users\\kefas\\python programs\\gradio.py\", line 9, in <module>                                                                                                      \r\n>     decoded_result = base64.b64decode(job.result()).decode('utf-8')                                                                                                         \r\n>   File \"C:\\Program Files\\Python310\\lib\\base64.py\", line 80, in b64decode                                                                                                    \r\n>     s = _bytes_from_decode_data(s)                                                                                                                                          \r\n>   File \"C:\\Program Files\\Python310\\lib\\base64.py\", line 45, in _bytes_from_decode_data                                                                                      \r\n>     raise TypeError(\"argument should be a bytes-like object or ASCII \"                                                                                                      \r\n> TypeError: argument should be a bytes-like object or ASCII string, not 'tuple'\r\n> ```\r\n> \r\n> What can I do?\r\n\r\nsame problem +1\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1433,
    "state": "open",
    "created_by": "yushuinanrong",
    "created_at": "2024-04-19T17:02:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1433</URL>\n\n<TITLE>A possible bug in deepspeed configuration - zero3_offload.json</TITLE>\n\n<BODY>### Describe the issue\n\nHi, \r\n\r\nFirst of all, great work!\r\nI noticed in the zero3_offload.jon config, it has set the learning rate scheduler to WarmupLR ([this line](https://github.com/haotian-liu/LLaVA/blob/main/scripts/zero3_offload.json#L22)), and it will override the input scheduler (cosine in pre-training and fine-tuning). In our experiments, it shows that WarmupLR will cause the loss hover around 1 and leads to poor performance. \r\n\r\nThanks,\r\nJC</BODY>\n\n<COMMENTS>\n<Comment by yqy2001 at 2024-04-28T03:36:14Z>\n+1. I delete the [`scheduler`](https://github.com/haotian-liu/LLaVA/blob/3e337ad269da3245643a2724a1d694b5839c37f9/scripts/zero3_offload.json#L22) field in [zero3_offload.json](https://github.com/haotian-liu/LLaVA/blob/main/scripts/zero3_offload.json), which makes the lr scheduler bahave properly\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1432,
    "state": "open",
    "created_by": "prhbrt",
    "created_at": "2024-04-19T14:26:55Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1432</URL>\n\n<TITLE>Llava becomes repetitive on OCR tasks</TITLE>\n\n<BODY>### Describe the issue\n\nOCR might not be the target task of Llava, but data is data and I still wanted to make a quick report on this.\r\n\r\nI tried OCR on these two images:\r\n\r\n![patent-smaller-cut](https://github.com/haotian-liu/LLaVA/assets/7011110/79e05735-b77d-4728-b3e5-1a02525ee824)\r\n\r\n![patent-smaller-cut-fraktur](https://github.com/haotian-liu/LLaVA/assets/7011110/4927dfb6-4daa-413c-881d-8f62564560b9)\r\n\r\nResult:\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/7011110/ce15cc2d-664a-4fb6-933d-65efb712dba6)\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/7011110/8842197f-77e4-438d-8ae6-e2a3d8b98f41)\r\n\r\n\r\nCode (copied from the [cli file](https://github.com/haotian-liu/LLaVA/blob/main/llava/serve/cli.py)):\r\n\r\n```\r\nimport torch\r\nimport requests\r\nimport argparse\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\nfrom transformers import TextStreamer\r\n\r\n\r\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\r\nfrom llava.conversation import conv_templates, SeparatorStyle\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.utils import disable_torch_init\r\nfrom llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.mm_utils import get_model_name_from_path\r\nfrom llava.eval.run_llava import eval_model\r\n\r\ndisable_torch_init()\r\nmodel_base = None\r\n# model_path = \"liuhaotian/llava-v1.5-7b\"\r\n# model_path = \"liuhaotian/llava-v1.5-13b-lora\"\r\nmodel_path = \"liuhaotian/llava-v1.6-vicuna-13b\"\r\nmodel_name = get_model_name_from_path(model_path)\r\nload_8bit=False\r\nload_4bit=False\r\ndevice='cuda:0'\r\n\r\nif \"llama-2\" in model_name.lower():\r\n    conv_mode = \"llava_llama_2\"\r\nelif \"mistral\" in model_name.lower():\r\n    conv_mode = \"mistral_instruct\"\r\nelif \"v1.6-34b\" in model_name.lower():\r\n    conv_mode = \"chatml_direct\"\r\nelif \"v1\" in model_name.lower():\r\n    conv_mode = \"llava_v1\"\r\nelif \"mpt\" in model_name.lower():\r\n    conv_mode = \"mpt\"\r\nelse:\r\n    conv_mode = \"llava_v0\"\r\n\r\n\r\nmodel_name = get_model_name_from_path(model_path)\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(model_path, model_base, model_name, load_8bit, load_4bit, device=device)\r\n\r\nimage_file = \"patent-smaller-cut-fraktur.png\"\r\nimage = Image.open(image_file).convert('RGB')\r\nimage_size = image.size\r\n\r\nimage_tensor = process_images([image], image_processor, model.config)\r\nif type(image_tensor) is list:\r\n  image_tensor = [image.to(model.device, dtype=torch.float16) for image in image_tensor]\r\nelse:\r\n  image_tensor = image_tensor.to(model.device, dtype=torch.float16)\r\n\r\n# if model.config.mm_use_im_start_end:\r\nprefix = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n'\r\n# else:\r\n#     prefix = DEFAULT_IMAGE_TOKEN + '\\n'\r\n\r\nprompt = \"Transcribe this page the best you can. It has 3 columns.\"\r\nmax_new_tokens = 4096\r\ntemperature = 0\r\n\r\nconv = conv_templates[conv_mode].copy()\r\nif \"mpt\" in model_name.lower():\r\n  roles = ('user', 'assistant')\r\nelse:\r\n  roles = conv.roles\r\n\r\n\r\nconv.append_message(conv.roles[0], prefix + prompt)\r\nconv.append_message(conv.roles[1], None)\r\n\r\nprompt = conv.get_prompt()\r\n\r\ninput_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)\r\nstop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\r\nkeywords = [stop_str]\r\nstreamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\r\n\r\n\r\nwith torch.inference_mode():\r\n  output_ids = model.generate(\r\n      input_ids,\r\n      images=image_tensor,\r\n      image_sizes=[image_size],\r\n      do_sample=True if temperature > 0 else False,\r\n      temperature=temperature,\r\n      max_new_tokens=max_new_tokens,\r\n      # streamer=streamer,\r\n      use_cache=True)\r\n\r\noutputs = tokenizer.decode(output_ids[0]).strip()\r\nconv.messages[-1][-1] = outputs\r\noutputs\r\n```\r\n\r\nInstalled llava from github:\r\n\r\n```\r\ncommit 3e337ad269da3245643a2724a1d694b5839c37f9 (HEAD -> main, origin/main, origin/HEAD)\r\nAuthor: ZhaoyangLi <43194342+ZhaoyangLi-nju@users.noreply.github.com>\r\nDate:   Fri Apr 19 03:11:11 2024 +0800\r\n\r\n    Update Evaluation.md (#1358)\r\n    \r\n    update the new path with VizWiz VQA Challenge 2024\r\n```</BODY>\n\n<COMMENTS>\n<Comment by Yonggie at 2024-05-21T07:36:37Z>\nHappens to me a lot. It'll somehow get into a nuts repeat.\n</Comment>\n<Comment by Veason-silverbullet at 2024-08-27T07:39:59Z>\n@Yonggie Yes, it happens a lot, not only LLaVA but also to other multimodal LLMs including MiniGPT4 from my empirical experiment observation. Do you have any insight on this phenomenon (and resovle it)?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1431,
    "state": "closed",
    "created_by": "tian969",
    "created_at": "2024-04-19T14:18:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1431</URL>\n\n<TITLE>[Question] Tokenizer.pad_token_id is NoneType, which requires float</TITLE>\n\n<BODY>### Question\n\nHello, I want to change the LLaVA base model from llama2 to llama3, but I encountered error during executing these lines:\r\n\r\ninput_ids = torch.nn.utils.rnn.pad_sequence(\r\ninput_ids,\r\nbatch_first=True,\r\npadding_value=self.tokenizer.pad_token_id)\r\n\r\nthe llama3 tokenizer's pad_token_id is None, which can not be a valid input in this method. How can I resolve this problem?\r\nThe model and tokenizer is correctly loaded.\r\n<img width=\"932\" alt=\"model-arch\" src=\"https://github.com/haotian-liu/LLaVA/assets/102512042/30568d47-e07f-4cf7-8e1e-e6892a2cd358\">\r\n<img width=\"1425\" alt=\"tokenizer-arch\" src=\"https://github.com/haotian-liu/LLaVA/assets/102512042/51a2f469-b341-4f17-b177-7974cd0c6209\">\r\n<img width=\"1143\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/102512042/17c54570-f2c3-47e0-a1bf-866e37fa2816\"></BODY>\n\n<COMMENTS>\n<Comment by chanangad at 2024-04-20T03:56:46Z>\nTry setting pad_token_id to eos_token_id\n</Comment>\n<Comment by tian969 at 2024-04-24T15:05:39Z>\nThanks to your help. It works!\n</Comment>\n<Comment by mmaaz60 at 2024-04-26T19:02:17Z>\nHi @tian969 , @chanangad,\r\n\r\nA better work-around for `pad` token in `LLaMA-3` would be to add a special token to tokenizer and then save it along with the model configs. For example, you may use the following code to achieve this,\r\n\r\n```\r\ndef smart_tokenizer_and_embedding_resize(\r\n    special_tokens_dict: Dict,\r\n    tokenizer: transformers.PreTrainedTokenizer,\r\n    model: transformers.PreTrainedModel,\r\n):\r\n    \"\"\"Resize tokenizer and embedding.\r\n\r\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\r\n    \"\"\"\r\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\r\n    model.resize_token_embeddings(len(tokenizer))\r\n\r\n    if num_new_tokens > 0:\r\n        input_embeddings = model.get_input_embeddings().weight.data\r\n        output_embeddings = model.get_output_embeddings().weight.data\r\n\r\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\r\n            dim=0, keepdim=True)\r\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\r\n            dim=0, keepdim=True)\r\n\r\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\r\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg\r\n```\r\n```\r\nif tokenizer.pad_token is None:\r\n    print(f\"Adding pad token as '<pad>'\")\r\n    smart_tokenizer_and_embedding_resize(\r\n        special_tokens_dict=dict(pad_token=\"<pad>\"),\r\n        tokenizer=tokenizer,\r\n        model=model,\r\n    )\r\n```\r\nAnd finally,\r\n\r\n```\r\nmodel.config.pad_token_id = tokenizer.pad_token_id\r\n```\r\n\r\nHere you add `<pad>` token, resize the embeddings and finally save this information in your model config.\r\n\r\n---\r\n\r\nFollowing above, we train LLaMA-3 based LLaVA-v1.5 model and achieve very good results. All the codes (fully supported with official LLaVA framework), pretrained chekcpoints and evaluation results are available on our GitHub Repo at [LLaVA++](https://github.com/mbzuai-oryx/LLaVA-pp).\r\n\r\nThe pretrained models are available at [Hugging Face](https://huggingface.co/collections/MBZUAI/llava-662b38b972e3e3e4d8f821bb).\r\n\r\nNote that in addition to LLaMA-3, we also support `Phi-3-Mini-3.8B` model which is as good as `LLaVA-v1.5-13B` model on vision-language benchmarks.\r\n\r\nI hope it will be helpful. Thanks and do let me know if you have any questions.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1430,
    "state": "open",
    "created_by": "HelloWorldBeginner",
    "created_at": "2024-04-19T09:16:45Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1430</URL>\n\n<TITLE>[Question] Do you want to support Ascend NPU?</TITLE>\n\n<BODY>### Question\r\n\r\nLLava is a great work, I have adapted llava to Ascend NPU hardware, enabling pre-training, inference, and evaluation on the Ascend NPU. I'm wondering if NPU is also welcomed in LLava ?  If yes, I would be interested in doing this work and submitting a PR.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1429,
    "state": "open",
    "created_by": "chanangad",
    "created_at": "2024-04-19T09:13:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1429</URL>\n\n<TITLE>Cannot perform inference using the 'liuhaotian/LLaVA-Lightning-MPT-7B-preview' checkpoint</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: We loaded the model 'liuhaotian/LLaVA-Lightning-MPT-7B-preview' but we are unable to perform inference.\r\n\r\nCommand:\r\n\r\n```\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.mm_utils import get_model_name_from_path\r\nfrom llava.eval.run_llava import eval_model\r\n\r\nmodel_path = \"liuhaotian/LLaVA-Lightning-MPT-7B-preview\"\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    model_path=model_path,\r\n    model_base=None,\r\n    model_name=get_model_name_from_path(model_path)\r\n)\r\n\r\nprompt = \"Describe this image\"\r\nimage_file = \"https://llava-vl.github.io/static/images/view.jpg\"\r\n\r\nargs = type('Args', (), {\r\n    \"model_path\": model_path,\r\n    \"model_base\": None,\r\n    \"model_name\": get_model_name_from_path(model_path),\r\n    \"query\": prompt,\r\n    \"conv_mode\": None,\r\n    \"image_file\": image_file,\r\n    \"sep\": \",\",\r\n    \"temperature\": 0,\r\n    \"top_p\": None,\r\n    \"num_beams\": 1,\r\n    \"max_new_tokens\": 512\r\n})()\r\n\r\neval_model(args)\r\n```\r\n\r\nLog: \r\n```\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nLoading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.90s/it]\r\n/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\r\n  warnings.warn(\r\n/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\r\n  warnings.warn(\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[2], line 19\r\n      3 image_file = \"https://llava-vl.github.io/static/images/view.jpg\"\r\n      5 args = type('Args', (), {\r\n      6     \"model_path\": model_path,\r\n      7     \"model_base\": None,\r\n   (...)\r\n     16     \"max_new_tokens\": 512\r\n     17 })()\r\n---> 19 eval_model(args)\r\n\r\nFile /sensei-fs/users/achandhok/LLaVA/llava/eval/run_llava.py:115, in eval_model(args)\r\n    108 input_ids = (\r\n    109     tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\r\n    110     .unsqueeze(0)\r\n    111     .cuda()\r\n    112 )\r\n    114 with torch.inference_mode():\r\n--> 115     output_ids = model.generate(\r\n    116         input_ids,\r\n    117         images=images_tensor,\r\n    118         image_sizes=image_sizes,\r\n    119         do_sample=True if args.temperature > 0 else False,\r\n    120         temperature=args.temperature,\r\n    121         top_p=args.top_p,\r\n    122         num_beams=args.num_beams,\r\n    123         max_new_tokens=args.max_new_tokens,\r\n    124         use_cache=True,\r\n    125     )\r\n    127 outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\r\n    128 print(outputs)\r\n\r\nFile ~/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py:115, in context_decorator.<locals>.decorate_context(*args, **kwargs)\r\n    112 @functools.wraps(func)\r\n    113 def decorate_context(*args, **kwargs):\r\n    114     with ctx_factory():\r\n--> 115         return func(*args, **kwargs)\r\n\r\nFile ~/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py:1307, in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\r\n   1305 model_kwargs = generation_config.update(**kwargs)  # All unused kwargs must be model kwargs\r\n   1306 generation_config.validate()\r\n-> 1307 self._validate_model_kwargs(model_kwargs.copy())\r\n   1309 # 2. Set generation parameters if not already defined\r\n   1310 logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\r\n\r\nFile ~/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py:1122, in GenerationMixin._validate_model_kwargs(self, model_kwargs)\r\n   1119         unused_model_args.append(key)\r\n   1121 if unused_model_args:\r\n-> 1122     raise ValueError(\r\n   1123         f\"The following `model_kwargs` are not used by the model: {unused_model_args} (note: typos in the\"\r\n   1124         \" generate arguments will also show up in this list)\"\r\n   1125     )\r\n\r\nValueError: The following `model_kwargs` are not used by the model: ['image_sizes'] (note: typos in the generate arguments will also show up in this list)\r\n```</BODY>\n\n<COMMENTS>\n<Comment by chanangad at 2024-04-19T09:21:46Z>\n@haotian-liu can you please have a look at this? This is in the latest code base\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1428,
    "state": "open",
    "created_by": "wuwu-C",
    "created_at": "2024-04-19T08:18:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1428</URL>\n\n<TITLE>[Question] loss is 0 when I finetune on my custom dataset,and runtime error</TITLE>\n\n<BODY>### Question\r\n\r\nwhen I run v1.5/finetune_lora.sh,it raise error about mm projecoter:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/96038200/1c66b0e0-aaf4-462b-8b7c-0154fe974548)\r\n\r\nAccording to the script of geochat (which also works with llava's fine-tuning), a few modifications were made and it worked fine\r\n![image](https://github.com/haotian-liu/LLaVA/assets/96038200/234ca9e9-391b-4899-958e-4154c6dcc376)\r\nhere is my scripts\r\nQuestion 1: Why change finetune_lora.sh so that no error is reported?\r\n\r\n```\r\n#!/bin/bash\r\n#set PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:6144\r\n\r\n################## VICUNA ##################\r\nPROMPT_VERSION=v1\r\nCUDA_HOME='/data/anaconda3/envs/llava'\r\n\r\nMODEL_VERSION=\"vicuna-v1.5-7b\"\r\n################## VICUNA ##################\r\n#export CUDA_VISIBLE_DEVICES='4'\r\n#\r\n#GPUS_PER_NODE=1\r\n#export NCCL_IB_DISABLE=1\r\n#export NCCL_IBEXT_DISABLE=1\r\n deepspeed --master_port=25888 --include localhost:4,6 llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --lora_enable True \\\r\n    --model_name_or_path /data/user4/cww/LLaVA/checkpoint/llava-v1.5-7b \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path /data/user4/cww/MobileVLM_main/data/finetune_data/mnist/LLAVA_FT.json \\\r\n    --image_folder /data/user4/cww/MobileVLM_main/data/finetune_data/CIFAR_FS/base  \\\r\n    --vision_tower /data/user4/cww/LLaVA/checkpoint/visualer/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --pretrain_mm_mlp_adapter /data/user4/cww/LLaVA/checkpoint/llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5/mm_projector.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --bf16 True \\\r\n    --output_dir /data/user4/cww/LLaVA/checkpoint/geochat-LLAVA15-7B-Vicuna-finetune_lora \\\r\n    --num_train_epochs 10 \\\r\n    --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 4 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"epoch\" \\\r\n    --save_steps 1000 \\\r\n    --save_total_limit 3 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 1024 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --dataloader_num_workers 4 \\\r\n    --report_to wandb\r\n```\r\n\r\n\r\nBut when I try to run finetune_task_lora.sh,the loss is 0 at the begining,it is somtimes said that UserWarning: None of the inputs have requires_grad=True. Gradients will be None\r\n![image](https://github.com/haotian-liu/LLaVA/assets/96038200/3e82889f-ee5b-45bd-a136-233e2be9383a)\r\n\r\n\r\nQuestion 2: Why does my training start with zero loss\r\nhere is my scripts:\r\n```\r\n#!/bin/bash\r\nMODEL_VERSION=llava-v1.5-7b\r\nPretrain_MODEL_VERSION=llava-336px-pretrain-vicuna-7b-v1.3\r\nfinetune_data_dir=/data/user4/cww/MobileVLM_main/data/finetune_data/mnist\r\nPROMPT_VERSION=v1\r\ndeepspeed --include localhost:5 llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path /data/user4/cww/LLaVA/checkpoint/$MODEL_VERSION \\\r\n    --version v1 \\\r\n    --data_path $finetune_data_dir/LLAVA_FT.json \\\r\n    --image_folder $finetune_data_dir/base \\\r\n    --vision_tower /data/user4/cww/LLaVA/checkpoint/visualer/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir /data/user4/cww/LLaVA/checkpoint/$MODEL_VERSION-$Pretrain_MODEL_VERSION-mnist-task-lora \\\r\n    --num_train_epochs 10 \\\r\n    --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 4 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"epoch\" \\\r\n    --save_total_limit 3 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 10 \\\r\n    --tf32 True \\\r\n    --model_max_length 512 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n\r\n```\r\n![image](https://github.com/haotian-liu/LLaVA/assets/96038200/4e76ce18-e59f-4451-979e-1ceab7362569)</BODY>\n\n<COMMENTS>\n<Comment by eslambakr at 2024-04-27T22:55:56Z>\nIncreasing the model max length solve my issue.\r\nAs mentioned here #375\n</Comment>\n<Comment by wuwu-C at 2024-04-28T06:11:44Z>\n@eslambakr  Yes，when I set max length 1024 ，this issue solved.But I got another problem.\r\nDo you train LLaVA in more epoch? Can you save the immediate \"non_trainable_lora.bin\"such file?\r\nAnd when I test my finetuned checkpoint,its output is always same that is very bad and wrong.\r\nAnd sometimes it has no output\n</Comment>\n<Comment by subiaansari at 2024-07-10T18:08:50Z>\nhttps://github.com/haotian-liu/LLaVA/issues/375#issuecomment-1781869446 -> solved my issue\n</Comment>\n<Comment by WojciechKusa at 2024-07-16T09:13:17Z>\nsame here, increasing ```--model_max_length``` from 512 to 1024 solved the issue for me\n</Comment>\n<Comment by MonkeyNi at 2024-09-28T10:40:58Z>\nWhen I use the wrong 'PROMPT_VERSION', I also observe that the loss is 0.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1427,
    "state": "open",
    "created_by": "awzhgw",
    "created_at": "2024-04-19T07:11:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1427</URL>\n\n<TITLE>Could you please support Llama3 in Llava ?</TITLE>\n\n<BODY>### feature\n\nCould you please support Llama3 in Llava ?</BODY>\n\n<COMMENTS>\n<Comment by mmaaz60 at 2024-04-26T19:04:23Z>\nHi @awzhgw & @everyone,\r\n\r\nI hope you are doing well. We have just released our project `LLaVA++: Extending Visual Capabilities with LLaMA-3 and Phi-3`, which features `LLaMA-3` and `Phi-3-Mini` based `LLaVA` models. Please have a look at it at [LLaVA++](https://github.com/mbzuai-oryx/LLaVA-pp).\r\n\r\n- We have released the codes required to support both LLaMA-3 & Phi-3-Mini models in `LLaVA framework`. The chat formats and corresponding preprocess methods are available at our GitHub repo.\r\n- We released all the checkpoints on [Hugging Face](https://huggingface.co/collections/MBZUAI/llava-662b38b972e3e3e4d8f821bb)\r\n- On our GitHub repository we have provided `.py` files that needs to be replaced/added to official LLaVA repository to train and infer LLaMA-3 & Phi-3-Mini based models.\r\n\r\nI hope this would be helpful. Please let me know if you have any questions. Thanks\n</Comment>\n<Comment by hhaAndroid at 2024-04-28T11:37:20Z>\nHere:\r\n\r\nhttps://github.com/InternLM/xtuner/tree/main/xtuner/configs/llava/phi3_mini_4k_instruct_clip_vit_large_p14_336\r\nhttps://github.com/InternLM/xtuner/tree/main/xtuner/configs/llava/llama3_8b_instruct_clip_vit_large_p14_336\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1426,
    "state": "open",
    "created_by": "thesby",
    "created_at": "2024-04-19T05:46:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1426</URL>\n\n<TITLE>[Feature request] Support Llama3</TITLE>\n\n<BODY>### feature\n\nCould you please support Llama3 in Llava ?</BODY>\n\n<COMMENTS>\n<Comment by awzhgw at 2024-04-19T07:12:11Z>\n+1\n</Comment>\n<Comment by HarryHsing at 2024-04-19T07:49:03Z>\n+1\n</Comment>\n<Comment by iMountTai at 2024-04-19T08:20:11Z>\n+1\n</Comment>\n<Comment by GoGoJoestar at 2024-04-19T08:21:04Z>\n+1\n</Comment>\n<Comment by manbehindthemadness at 2024-04-20T18:51:24Z>\n+1\n</Comment>\n<Comment by dingtine at 2024-04-22T03:10:29Z>\ni have trained llava with llama3 model, but the generate result is not correct.\n</Comment>\n<Comment by Namzakku at 2024-04-22T08:55:26Z>\n@dingtine can you specify more about the result? also, which base model did you train on?\n</Comment>\n<Comment by manbehindthemadness at 2024-04-22T16:47:55Z>\n@dingtine there is some mention of an abnormality regarding an end/termination token mentioned here: https://x.com/bartowski1182/status/1782206933719515467?s=46&t=iIhAbXdfE1VCk7vAgMnlRQ as this came out just now, it might affect your results.\n</Comment>\n<Comment by mmaaz60 at 2024-04-26T18:53:49Z>\nHi @thesby @awzhgw , @Namzakku @manbehindthemadness,\r\n\r\nI hope you are doing well. We have just released our project `LLaVA++: Extending Visual Capabilities with LLaMA-3 and Phi-3`, which features LLaMA-3 and Phi-3-Mini based LLaVA models. Please have a look at this at [LLaVA++](https://github.com/mbzuai-oryx/LLaVA-pp).\r\n\r\n- We have released the codes required to support both LLaMA-3 & Phi-3-Mini models in LLaVA framework. The chat formats and corresponding preprocess methods are available at our GitHub repo.\r\n- We released all the checkpoints on [Hugging Face](https://huggingface.co/collections/MBZUAI/llava-662b38b972e3e3e4d8f821bb)\r\n- On our GitHub repository we have provided `.py` files that needs to be replaced/added to official LLaVA repository to train and infer LLaMA-3 & Phi-3-Mini based models.\r\n\r\nFurther, as pointed out by @manbehindthemadness, the issues related to generation have been fixed in the recent update of `generation_config.json` and `tokenizer.json` at `meta-llama/Meta-Llama-3-8B-Instruct`.\r\n\r\nIn case if you face any issue in running/training LLaMA-3 or Phi-3-Mini based LLaVA models, please let me know.\n</Comment>\n<Comment by manbehindthemadness at 2024-04-27T15:43:13Z>\nFantastic!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1425,
    "state": "open",
    "created_by": "FreddieGe",
    "created_at": "2024-04-19T03:18:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1425</URL>\n\n<TITLE>[Question] Can I write question on command run?</TITLE>\n\n<BODY>### Question\n\nI can run this command perfectly: python -m llava.serve.cli     --model-path {/root/Research/LLaVA/llava-v1.5-7b/}     --image-file \"autodl-tmp/download.jpg\"     --load-4bit, is there any way I can ask question in command and then I get result as output?\r\nFor example: --p \"What are the things I should be cautious about when I visit this place?\"\r\n\r\nThank you!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1424,
    "state": "open",
    "created_by": "chanangad",
    "created_at": "2024-04-18T16:03:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1424</URL>\n\n<TITLE>Serve LLaVA MPT-7b  model after pretraining</TITLE>\n\n<BODY>### Question\n\nI've pulled the latest code on 18th April, 2024 and tried to train LLaVA MPT-7B model from scratch.\r\n\r\nAfter running pretraining using the following script:\r\n\r\n```\r\ndeepspeed train_mem.py \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --model_name_or_path mosaicml/mpt-7b \\\r\n    --version mpt\\\r\n    --data_path LLaVA-Pretrain/blip_laion_cc_sbu_558k.json \\\r\n    --image_folder LLaVA-Pretrain/images \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-mpt-7b-pretrain \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 32 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 24000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 1e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True\\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb \r\n\r\n```\r\n\r\nI was able to get the pre-trained MPT model weights. I'm trying to serve it using the following script:\r\n\r\n```\r\npython -m llava.serve.cli \\\r\n    --model-base mosaicml/mpt-7b \\\r\n    --model-path ./checkpoints/llava-mpt-7b-pretrain/ \\\r\n    --image-file ./PXL_20230304_023217466.jpg \\\r\n    --temperature 0.1 \r\n```\r\n\r\nI run into the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/user/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/user/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/sensei-fs/users/achandhok/LLaVA/llava/serve/cli.py\", line 120, in <module>\r\n    main(args)\r\n  File \"/sensei-fs/users/achandhok/LLaVA/llava/serve/cli.py\", line 98, in main\r\n    output_ids = model.generate(input_ids,images=image_tensor,image_size=[image_size],do_sample=True if args.temperature > 0 else False,temperature=args.temperature,max_new_tokens=args.max_new_tokens,streamer=streamer,use_cache=True)\r\n  File \"/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1307, in generate\r\n    self._validate_model_kwargs(model_kwargs.copy())\r\n  File \"/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1122, in _validate_model_kwargs\r\n    raise ValueError(\r\nValueError: The following `model_kwargs` are not used by the model: ['image_size'] (note: typos in the generate arguments will also show up in this list)\r\n```\r\n\r\nIf I remove the image_size argument and run it, I get this error:\r\n\r\n```\r\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\r\nassistant: Traceback (most recent call last):\r\n  File \"/home/user/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/user/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/sensei-fs/users/achandhok/LLaVA/llava/serve/cli.py\", line 127, in <module>\r\n    main(args)\r\n  File \"/sensei-fs/users/achandhok/LLaVA/llava/serve/cli.py\", line 97, in main\r\n    output_ids = model.generate(\r\n  File \"/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1525, in generate\r\n    return self.sample(\r\n  File \"/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2622, in sample\r\n    outputs = self(\r\n  File \"/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sensei-fs/users/achandhok/LLaVA/llava/model/language_model/llava_mpt.py\", line 76, in forward\r\n    return super().forward(\r\n  File \"/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/mpt/modeling_mpt.py\", line 593, in forward\r\n    transformer_outputs = self.transformer(\r\n  File \"/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/mpt/modeling_mpt.py\", line 473, in forward\r\n    outputs = block(\r\n  File \"/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/mpt/modeling_mpt.py\", line 213, in forward\r\n    attn_outputs, attn_weights, past_key_value = self.attn(\r\n  File \"/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/user/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/mpt/modeling_mpt.py\", line 142, in forward\r\n    attention_scores = attention_scores.masked_fill(attention_mask, torch.finfo(query_states.dtype).min)\r\nRuntimeError: The size of tensor a (64) must match the size of tensor b (319) at non-singleton dimension 3\r\n```\r\n\r\nDid anyone run into this issue?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1423,
    "state": "open",
    "created_by": "leechangdong",
    "created_at": "2024-04-18T13:31:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1423</URL>\n\n<TITLE>[Usage] finetune_task_lora.sh checkpoints usage</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\n\r\n\btrain args:\r\nfinetune_task_lora.sh\r\n```\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path liuhaotian/llava-v1.5-7b \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/epoch_test.json\\\r\n    --image_folder ./playground/data \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/epoch_test \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 0.05 \\\r\n    --save_total_limit 3 \\\r\n    --learning_rate 2e-6 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\nWith that training setup, I fine tune using my custom data, and checkpoints were saved for each step.\r\n\r\n<img width=\"401\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/115126427/cb9d16d9-6110-47f4-9e16-69b6f801ffdd\">\r\n\r\nThe folder contents of those \"checkpoints\" are as shown in the photo above, and in order to use those checkpoints for inference, I think need to run merge_lora. \"scripts/merge_lora_weights.py\" to use the above checkpoints for inference. and i did.\r\n\r\ncommand:\r\n```\r\npython scripts/merge_lora_weights.py \\\r\n--model-path ./checkpoints/llava-v1.5-7b-task-lora-13/checkpoint-3-lora \\\r\n--model-base liuhaotian/llava-v1.5-7b \\\r\n--save-model-path ./checkpoints/llava-v1.5-7b-task-lora-13/merged/llava-v1.5-7b-epoch-test\r\n```\r\nBut I get an error like this\r\n\r\n```\r\nValueError: Unrecognized configuration class <class 'transformers.models.llava.configuration_llava.LlavaConfig'> for this kind of AutoModel: AutoModelForCausalLM.\r\nModel type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, ElectraConfig, ErnieConfig, FalconConfig, FuyuConfig, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, LlamaConfig, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MptConfig, MusicgenConfig, MvpConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, LlavaConfig, LlavaMptConfig, LlavaMistralConfig.\r\n```\r\n\r\nMy purpose is to use the checkpoints I saved while training \"finetune_task_lora.sh\" for inference.\r\nDoes anyone know if what I did above is correct and how to fix this error?\r\n\r\n* my transformer version - transformers==4.37.2</BODY>\n\n<COMMENTS>\n<Comment by wuwu-C at 2024-04-20T12:38:12Z>\nI also got the problem when I try to use merge_lora_weights.\r\n```\r\n`OSError: /data/user4/cww/LLaVA/checkpoint/LLaVA.finetune/geochat-LLAVA15-7B-Vicuna-finetune_lora/checkpoint-2234 does not appear to have a file named config.json. Checkout 'https://huggingface.co//data/user4/cww/LLaVA/checkpoint/LLaVA.finetune/geochat-LLAVA15-7B-Vicuna-finetune_lora/checkpoint-2234/main' for available files.\r\n`\r\n```\r\nDid you solve?\n</Comment>\n<Comment by leechangdong at 2024-04-22T02:04:13Z>\n#844 \r\n\r\nCheck this out\n</Comment>\n<Comment by CHENGY12 at 2024-04-26T18:07:52Z>\nHi [leechangdong](https://github.com/leechangdong), did you modify the code beyond https://github.com/haotian-liu/LLaVA/issues/844 . Currently, I change the trainer as https://github.com/haotian-liu/LLaVA/issues/844, but only have\r\n<img width=\"341\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/17920133/debd32d0-9d8d-4bea-9f39-388c1767b40a\"> \r\n\r\nI don't have the gradient information like the folder of \"global_step*\" as you show.\n</Comment>\n<Comment by wuwu-C at 2024-04-28T06:14:56Z>\n@leechangdong  Thank you! But I have another problem.I follow https://github.com/haotian-liu/LLaVA/issues/844 modify my code, but it not save files as expected.And when I examine what errors happened, I find any \"print\" or \"rank0_print\"would not successfule print,it seems that it does not execute my local train.py/LLaVATrainer.py\n</Comment>\n<Comment by user074 at 2024-05-01T20:45:18Z>\nNot for task but for LoRA, I followed #729 to save it. Able to load the LoRA weights after following #1200\n</Comment>\n<Comment by GaoZhitao121 at 2024-12-02T02:17:48Z>\n> Hi [leechangdong](https://github.com/leechangdong), did you modify the code beyond #844 . Currently, I change the trainer as #844, but only have <img alt=\"image\" width=\"341\" src=\"https://private-user-images.githubusercontent.com/17920133/326069512-debd32d0-9d8d-4bea-9f39-388c1767b40a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzMxMDU5NTUsIm5iZiI6MTczMzEwNTY1NSwicGF0aCI6Ii8xNzkyMDEzMy8zMjYwNjk1MTItZGViZDMyZDAtOWQ4ZC00YmVhLTlmMzktMzg4YzE3NjdiNDBhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEyMDIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMjAyVDAyMTQxNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWUwODhkNTQzMWJhZDAwNmMwZTVhZWMyY2YzOWFhZjYwMjk5NzBjN2UyYWRhMzQ5MDQ3MGFkYjJkZDU4ZTM2MTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.se3WLhSA_nhA2SC7yw3LC6u1tmhgO0emnr5cZt9sQgE\">\r\n> \r\n> I don't have the gradient information like the folder of \"global_step*\" as you show.\r\n\r\nI have the same issue, did you fix it？\n</Comment>\n<Comment by larryfans at 2024-12-30T12:27:25Z>\nmodel-path's name need include 'llava'\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1422,
    "state": "closed",
    "created_by": "yanan1989",
    "created_at": "2024-04-18T11:23:10Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1422</URL>\n\n<TITLE>RuntimeError: deepspeed(zero3_offload) doesnt work on LLaVA1.6 finetuning</TITLE>\n\n<BODY>When I run deepspeed with zero3_offload, I got the following runtime error.\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:xx and cpu!\r\nDoes anyone successfully use zero3_offload while finetuning LLaVA1.6_13B/34B?</BODY>\n\n<COMMENTS>\n<Comment by yanan1989 at 2024-04-19T09:03:39Z>\nDeepSpeed v1.4.0 worked with zero3_offload. \r\n```\r\ngit clone https://github.com/microsoft/DeepSpeed.git -b v0.14.0\r\ncd ./DeepSpeed\r\npip install .\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1421,
    "state": "open",
    "created_by": "NanAlbert",
    "created_at": "2024-04-18T07:28:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1421</URL>\n\n<TITLE>[Usage] Discrepancy in Expected Image Count and Actual Size of LLaVA-CC3M-Pretrain-595K/images.zip</TITLE>\n\n<BODY>### Describe the issue\n\nI recently downloaded the LLaVA-CC3M-Pretrain-595K/images.zip from the provided link, expecting it to contain approximately 595K images as suggested by the name. However, I noticed that the zip file is only around 6GB, which seems too small to include all 595K images. Could you please clarify if the zip file is supposed to contain all 595K images? If not, could you provide guidance on where to find the complete dataset or if there are any additional steps required to access all images?\r\n\r\nThank you for your assistance.</BODY>\n\n<COMMENTS>\n<Comment by NanAlbert at 2024-04-18T07:58:54Z>\nhttps://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K/tree/main\n</Comment>\n<Comment by dragen1860 at 2024-05-14T10:17:02Z>\nhi, i have download and unzip all images. the total number is 595375. no problems at all.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1420,
    "state": "open",
    "created_by": "arthurwolf",
    "created_at": "2024-04-17T18:01:59Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1420</URL>\n\n<TITLE>[Question] Contributing images/captions.</TITLE>\n\n<BODY>### Question\n\nIs it possible to contribute images/captions to add to https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K ?\r\n\r\nI find llava could be better at understanding comic book pages/panels, so I figured if I contributed some creative-common or public-domain comic book pages and their captions, it would result in better performance the next time you train a model on the dataset? No?\r\n\r\nIs that sort of contribution something you are open to?\r\n\r\nCheers.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1419,
    "state": "open",
    "created_by": "fisher75",
    "created_at": "2024-04-17T15:22:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1419</URL>\n\n<TITLE>[Question] Anyone could explain to me what does pretrain_mm_mlp_adapter means in lora file? 请问pretrain_mm_mlp_adapter是干什么用的啊？</TITLE>\n\n<BODY>### Question\n\nHi, guys, \r\n(1) does anyone know what pretrain_mm_mlp_adapter means in the Lora file? \r\n(2) btw I'm trying to lora the 1.6-7b models, which file should I use?  finetune_task_lora.sh or finetune_lora.sh?\r\n\r\nThanks!</BODY>\n\n<COMMENTS>\n<Comment by bkuster0 at 2024-04-29T13:06:47Z>\n(this is speculation/my understanding, not 100% accurate answer)\r\n1) The \"pretrain_mlp_adapter\" is the file for the multi-layer perceptron weights. (the output tokens of the CLIP encoder are converted into \"visual\" tokens that are same dimensionality as the \"text\" tokens, by the MLP adapter.\r\nThe adapter can be fine-tuned in 1.5, however in 1.6:\r\n\r\n2) I do believe you should use the finetune_task_lora, since (from my knowledge), in LLaVa-1.6 models, there is no separate adapter weights file.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1418,
    "state": "open",
    "created_by": "Zhangwenyao1",
    "created_at": "2024-04-17T15:18:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1418</URL>\n\n<TITLE>[Usage] CUDA error: the provided PTX was compiled with an unsupported toolchain.</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1417,
    "state": "open",
    "created_by": "ShawnAn-WHU",
    "created_at": "2024-04-17T13:19:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1417</URL>\n\n<TITLE>[Question] LLaVA Pretraining with Mixtral 8×7B</TITLE>\n\n<BODY>### Question\n\nDoes anyone have carried out the pretraining with Mixtral 8×7B? When I run the petraining script, one problem occured like the figure shown below. I just add a llava_mixtral.py to the llava/model/language_model and some necessary supplementary code.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/77612468/5607c88f-0e33-403b-bc04-f6e2d8d25643)</BODY>\n\n<COMMENTS>\n<Comment by martinakaduc at 2024-04-18T03:08:27Z>\nI have not faced this issue. Can you give me the reproducing command.\n</Comment>\n<Comment by ShawnAn-WHU at 2024-04-18T04:13:25Z>\n@martinakaduc Thank you very much for your prompt reply! Below is my pretraining script. The --model_name_or_path is the model I downloaded from HF mistralai/Mixtral-8x7B-v0.1. Despite the warnings, running this script will produce a mm_projector.bin file. When pretraining, the loss decreases from ~15 to ~6 and does not decrease any more. Can you figure out the problem?\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/77612468/e77ba3ed-08f7-4305-99c1-2ec7ba48b24e)\n</Comment>\n<Comment by martinakaduc at 2024-04-18T06:36:17Z>\nHave you merged my pull request about adding mixtral? If not, you can use my modified repo here: https://github.com/martinakaduc/LLaVA\r\n\r\nMy pretraining script:\r\n`deepspeed llava/train/train_mem.py\r\n    --deepspeed ./scripts/zero3_offload.json\r\n    --model_name_or_path mistralai/Mixtral-8x7B-Instruct-v0.1\r\n    --version plain\r\n    --data_path ./playground/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json\r\n    --image_folder ./playground/data/LLaVA-Pretrain/images\r\n    --vision_tower openai/clip-vit-large-patch14-336\r\n    --mm_projector_type mlp2x_gelu\r\n    --tune_mm_mlp_adapter True\r\n    --mm_vision_select_layer -2\r\n    --mm_use_im_start_end False\r\n    --mm_use_im_patch_token False\r\n    --bf16 True\r\n    --output_dir ./checkpoints/Mixtral-pt\r\n    --num_train_epochs 1\r\n    --per_device_train_batch_size 32\r\n    --per_device_eval_batch_size 4\r\n    --gradient_accumulation_steps 2\r\n    --evaluation_strategy \"no\"\r\n    --save_strategy \"steps\"\r\n    --save_steps 200\r\n    --save_total_limit 2\r\n    --learning_rate 1e-3\r\n    --weight_decay 0.\r\n    --warmup_ratio 0.03\r\n    --lr_scheduler_type \"cosine\"\r\n    --logging_steps 1\r\n    --tf32 True\r\n    --model_max_length 32768\r\n    --gradient_checkpointing True\r\n    --dataloader_num_workers 4\r\n    --lazy_preprocess True\r\n    --report_to neptune`\r\n\r\n\r\nAnd fine-tuning script:\r\n`deepspeed llava/train/train_mem.py\r\n    --deepspeed ./scripts/zero3_offload.json\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5\r\n    --model_name_or_path mistralai/Mixtral-8x7B-Instruct-v0.1\r\n    --version mistral_instruct\r\n    --data_path ./playground/data/llava_v1_5_mix665k.json\r\n    --image_folder ./playground/data\r\n    --vision_tower openai/clip-vit-large-patch14-336\r\n    --pretrain_mm_mlp_adapter ./checkpoints/Mixtral-pt/checkpoint-400/mm_projector.bin\r\n    --mm_projector_type mlp2x_gelu\r\n    --mm_vision_select_layer -2\r\n    --mm_use_im_start_end False\r\n    --mm_use_im_patch_token False\r\n    --image_aspect_ratio pad\r\n    --group_by_modality_length True\r\n    --bf16 True\r\n    --output_dir ./checkpoints/Mixtral-sft\r\n    --num_train_epochs 1\r\n    --per_device_train_batch_size 4\r\n    --per_device_eval_batch_size 4\r\n    --gradient_accumulation_steps 8\r\n    --evaluation_strategy \"no\"\r\n    --save_strategy \"steps\"\r\n    --save_steps 200\r\n    --save_total_limit 2\r\n    --learning_rate 2e-5\r\n    --weight_decay 0.\r\n    --warmup_ratio 0.03\r\n    --lr_scheduler_type \"cosine\"\r\n    --logging_steps 1\r\n    --tf32 True\r\n    --model_max_length 32768\r\n    --gradient_checkpointing True\r\n    --dataloader_num_workers 4\r\n    --lazy_preprocess True\r\n    --report_to neptune`\n</Comment>\n<Comment by ShawnAn-WHU at 2024-04-18T06:50:54Z>\n@martinakaduc Thank you! I will try it now and find out the problem!\n</Comment>\n<Comment by accupham at 2024-04-18T15:21:50Z>\nInteresting, would pretraining on mixtral-8x-22b also be possible?\n</Comment>\n<Comment by martinakaduc at 2024-04-18T16:06:34Z>\nI think it is possible. However I have not tested yet.\n</Comment>\n<Comment by ShawnAn-WHU at 2024-04-25T11:40:39Z>\n@martinakaduc Hi, I'm using your pretrained MixSUraV model downloaded from HF to finetune on my own dataset. The script I use is like Figure 1, is it correct? If correct, I found it infeasible when using 8 3090 GPUs (24G) even with 4-bit quantification (set --bits 4, like the red rectangle in the figure). The code for model loading is like Figure 2. However, when I use the code shown in Figure 3, only 3 GPUs are more than enough (may be 1 is ok). Is there any difference between these two codes? And could you please tell me your finetuning script and computational resources needed if you have done this? Thank tou so much!\r\n![image](https://github.com/haotian-liu/LLaVA/assets/77612468/f95fa4b4-41d3-4371-8264-f7e9fb22a7a1)\r\n![image](https://github.com/haotian-liu/LLaVA/assets/77612468/586dd259-a5c8-42fc-b53d-4ed5de4ffb0e)\r\n![image](https://github.com/haotian-liu/LLaVA/assets/77612468/7590cd60-1fff-4a8b-afc6-b6282de38444)\n</Comment>\n<Comment by fisher75 at 2024-04-25T13:12:45Z>\nHi, how do you know the training was effecitve? Did you use the default training setting? I LoRA with default parameters and basically no improvement.\n</Comment>\n<Comment by ShawnAn-WHU at 2024-04-26T07:06:49Z>\n> Hi, how do you know the training was effecitve? Did you use the default training setting? I LoRA with default parameters and basically no improvement.\r\n\r\n@fisher75 I have LoRA finetuned with my own dataset using LLaVA-v1.5 and the qualitative results are better than the original LLaVA-v1.5.\n</Comment>\n<Comment by fisher75 at 2024-04-26T07:11:36Z>\n> ShawnAn-WHU\r\n\r\nHi @ShawnAn-WHU thanks for your reply. I am also working on this, may I ask is the improvement is very obvious? May I see the training and inference scripts(mostly I am curious about the parameter settings), btw if possible, may I add your WeChat? Could be very helpful to share some details.\n</Comment>\n<Comment by ShawnAn-WHU at 2024-04-26T09:01:06Z>\n@fisher75 Sure, e-mail me your WeChat ID is ok.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1416,
    "state": "closed",
    "created_by": "rohithbojja",
    "created_at": "2024-04-17T12:34:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1416</URL>\n\n<TITLE>How to merge lora which is of safetensors type(adapter_model.safetensor) for LLaVA-v1.6</TITLE>\n\n<BODY>Command:\r\n\r\npython scripts/merge_lora_weights.py --model-path /home/rohith/Documents/mistral-llava/  --model-base /media/rohith/CC2A79902A7977F2/models/llava-v1.6-mistral-7b/  --save-model-path /home/rohith/Desktop/mistral-llava-merge\r\n\r\nLog: \r\n\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.30it/s]\r\nTraceback (most recent call last):\r\n  File \"/home/rohith/LLaVA-1.6-ft/scripts/merge_lora_weights.py\", line 22, in <module>\r\n    merge_lora(args)\r\n  File \"/home/rohith/LLaVA-1.6-ft/scripts/merge_lora_weights.py\", line 8, in merge_lora\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, device_map='cpu')\r\n  File \"/home/rohith/LLaVA-1.6-ft/llava/model/builder.py\", line 112, in load_pretrained_model\r\n    mm_projector_weights = torch.load(os.path.join(model_path, 'mm_projector.bin'), map_location='cpu')\r\n  File \"/home/rohith/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py\", line 986, in load\r\n    with _open_file_like(f, 'rb') as opened_file:\r\n  File \"/home/rohith/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py\", line 435, in _open_file_like\r\n    return _open_file(name_or_buffer, mode)\r\n  File \"/home/rohith/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py\", line 416, in __init__\r\n    super().__init__(open(name, mode))\r\nFileNotFoundError: [Errno 2] No such file or directory: '/home/rohith/Documents/mistral-llava/mm_projector.bin'</BODY>\n\n<COMMENTS>\n<Comment by rohithbojja at 2024-04-17T12:43:04Z>\nhad to include lora in model name\r\nthats all fixed\n</Comment>\n<Comment by DebaratiD at 2024-04-25T21:09:18Z>\nHi Rohith,\r\n\r\nI am trying to use the med-vqa dataset you uploaded on hugging face but need help on that. Please let me know how I can find the entire image dataset for the same.\r\n\r\nRegards,\r\nDebarati\n</Comment>\n<Comment by rohithbojja at 2024-04-27T17:11:23Z>\n> Hi Rohith,\r\n> \r\n> I am trying to use the med-vqa dataset you uploaded on hugging face but need help on that. Please let me know how I can find the entire image dataset for the same.\r\n> \r\n> Regards, Debarati\r\n\r\n@DebaratiD \r\nhttps://huggingface.co/datasets/rbojja/medical-vqa/tree/main/vqa-rad/images\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1415,
    "state": "closed",
    "created_by": "chanangad",
    "created_at": "2024-04-17T11:20:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1415</URL>\n\n<TITLE>Testing LLaVA-MPT model after pre-training</TITLE>\n\n<BODY>### Question\n\nI ran pre-training for the the LLaVA model using the following command:\r\n\r\n```\r\ntorchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \\\r\n     train_mem.py \\\r\n    --model_name_or_path customMPTmodel \\\r\n    --version mpt\\\r\n    --data_path /mnt/localssd/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json \\\r\n    --image_folder /mnt/localssd/LLaVA-Pretrain/images \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-pretrain \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 32 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 24000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 1e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n    \r\nI get a lot of Tokenizer mismatch messages on every other iteration but the loss keeps decreasing and the pre-training completes. Now I want to test out the pre-trained model on a sample image before starting the instruction tuning.\r\n\r\nI run this command:\r\n\r\n```\r\npython -m llava.serve.cli \\\r\n    --model-base  customMPTmodel \\\r\n    --model-path ./checkpoints/llava-v1.5-pretrain/ \\\r\n    --image-file ./PXL_20230304_023217466.jpg \\\r\n    --temperature 0.1\r\n```\r\n\r\nWhen I give the prompt as: \"Describe the image\"\r\n\r\nI get this runtime error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/opt/conda/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/sensei-fs/users/achandhok/LLaVA-1.1.3-adobeone/llava/serve/cli.py\", line 120, in <module>\r\n    main(args)\r\n  File \"/sensei-fs/users/achandhok/LLaVA-1.1.3-adobeone/llava/serve/cli.py\", line 97, in main\r\n    output_ids=model.generate(input_ids,images=image_tensor,do_sample=True,temperature=args.temperature,max_new_tokens=args.max_new_tokens,streamer=streamer,use_cache=True,stopping_criteria=[stopping_criteria])\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1588, in generate\r\n    return self.sample(\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2679, in sample\r\n    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n```\r\n\r\n\r\nI saw a couple other people facing similar issue but there wasn't any clear solution posted for the same. I'd appreciate it a lot if someone can point me in the correct direction.</BODY>\n\n<COMMENTS>\n<Comment by asunaperisi at 2024-04-21T14:01:39Z>\nsame problem but after finetuning with my own data. Did you solved it?\n</Comment>\n<Comment by chanangad at 2024-04-21T14:47:37Z>\nI was using the older version of the code (v1.1.3 to be specific). Didn't get this after switching to the latest code base\n</Comment>\n<Comment by asunaperisi at 2024-04-21T22:09:45Z>\nI switched with latest code base tried it again with cli but I got same error. Did you finetune it again with new code base or just switched the code base and worked for inference ?Thank you so much for your quick answer.\n</Comment>\n<Comment by chanangad at 2024-04-22T05:21:31Z>\nyes, I pre-trained again. I was working with the MPT model. If you get a tokeniser mismatch warning while training, you need to rectify that as it can lead to bad learning during pretraining\n</Comment>\n<Comment by zihui-debug at 2024-06-02T14:43:22Z>\n> yes, I pre-trained again. I was working with the MPT model. If you get a tokeniser mismatch warning while training, you need to rectify that as it can lead to bad learning during pretraining\r\n\r\nHello, I encounter the same error, how did you solve the mismatch problem?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1413,
    "state": "open",
    "created_by": "JJJYmmm",
    "created_at": "2024-04-17T07:51:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1413</URL>\n\n<TITLE>[Question] Calculation about `shortest_edge`</TITLE>\n\n<BODY>### Question\n\nIn llava/conversation.py, to limit image's size, shortest_edge is determined by `int(min(max_len / aspect_ratio, min_len, min_hw)) `\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/4e2277a060da264c4f21b364c867cc622c945874/llava/conversation.py#L133-L143\r\n\r\n① If `max(image.size) > max_len`, `max_len / aspect_ratio` is always smaller than min_hw, so shortest_edge can be simplified to `int(min(max_len / aspect_ratio, min_len)`\r\n> max(image.size) > max_len ⇒ max_hw > max_len ⇒ max_hw / min_hw > max_len / min_hw ⇒ min_hw >max_len / aspect_ratio\r\n\r\n② I'm also confused about min_len, in `int(min(max_len / aspect_ratio, min_len)`, the constraint about max_len is larger than min_len(the shortest_edge can be smaller than min_len). The only thing min_len does is to resize image smaller when `max_len / aspect_ratio > min_len`\r\n\r\nI think `shortest_edge = int(max_len / aspect_ratio)` is ok.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1412,
    "state": "closed",
    "created_by": "AtsuMiyai",
    "created_at": "2024-04-17T02:05:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1412</URL>\n\n<TITLE>Multi-turn question (The second turn is only text question.)</TITLE>\n\n<BODY>### Question\n\n<img width=\"806\" alt=\"スクリーンショット 2024-04-17 11 02 49\" src=\"https://github.com/haotian-liu/LLaVA/assets/65175432/68a96c80-7149-4250-b505-c40607385789\">\r\n\r\nThanks for your work! \r\n\r\nThis is a quick question.\r\nI'll try multi-turn questions. Especially, the second turn is only a text question.\r\nCould someone tell me how to implement this?\r\n\r\nThanks for your cooperation!</BODY>\n\n<COMMENTS>\n<Comment by Debolena7 at 2024-04-18T21:41:43Z>\n> ### Question\r\n> <img alt=\"スクリーンショット 2024-04-17 11 02 49\" width=\"806\" src=\"https://private-user-images.githubusercontent.com/65175432/323046252-68a96c80-7149-4250-b505-c40607385789.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTM0NzY3MTEsIm5iZiI6MTcxMzQ3NjQxMSwicGF0aCI6Ii82NTE3NTQzMi8zMjMwNDYyNTItNjhhOTZjODAtNzE0OS00MjUwLWI1MDUtYzQwNjA3Mzg1Nzg5LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE4VDIxNDAxMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWRlNzFiNGZjZDVmYmIxYzk4YjBiMTcxYzY3Y2FhMzYyMTkyYWQ4OWNmYzk1YmYwYzNhMzYwMGU1ZWZiNDkxNWEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.lOaOQakPBNVdtZhE89ZnPlkVdQv_hms1pHukwqJv4yY\">\r\n> Thanks for your work!\r\n> \r\n> This is a quick question. I'll try multi-turn questions. Especially, the second turn is only a text question. Could someone tell me how to implement this?\r\n> \r\n> Thanks for your cooperation!\r\n\r\ncould you please tell me how to do this in python? the second turn is only a text question with reference to the earlier image.\n</Comment>\n<Comment by Debolena7 at 2024-04-19T23:05:06Z>\nAny update on this, please?\n</Comment>\n<Comment by AtsuMiyai at 2024-04-22T09:35:32Z>\n@not-hermione Sorry for the late reply.\r\nI think that the simplest way is to put your previous conversations into text input.\r\n\r\ntext_input = {USER: <image>\\nWhat color is the car?\\nAssistant: The car in this image is yellow. \\nUSER:What color is the person's clothes?\\nAssistant: }\n</Comment>\n<Comment by Debolena7 at 2024-04-22T09:39:27Z>\nthanks a lot for your reply. for the second turn, do we have to input the image again for context?\n</Comment>\n<Comment by AtsuMiyai at 2024-04-22T09:41:46Z>\nI think so.\n</Comment>\n<Comment by Debolena7 at 2024-04-23T14:41:55Z>\n> I think so.\r\n\r\nthanks.. I also found this: https://huggingface.co/docs/transformers/en/model_doc/llava#transformers.LlavaForConditionalGeneration:~:text=For%20multiple%20turns%20conversation%3A\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1411,
    "state": "open",
    "created_by": "awzhgw",
    "created_at": "2024-04-17T01:36:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1411</URL>\n\n<TITLE>Is the LLaVA model suitable for building a personalized search and recommendation system based on product images in the e-commerce field?</TITLE>\n\n<BODY>### Describe the issue\n\ns the LLaVA model suitable for building a personalized search and recommendation system based on product images in the e-commerce field? The training data includes: product images, user portraits, customer service chat records for products, product price information, etc.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1410,
    "state": "open",
    "created_by": "Leon1207",
    "created_at": "2024-04-16T15:01:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1410</URL>\n\n<TITLE>Why I can't save ckpt  when I resmue the model in finetune stage?</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1409,
    "state": "open",
    "created_by": "OliverLeeXZ",
    "created_at": "2024-04-16T13:58:50Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1409</URL>\n\n<TITLE>[Question] model.generate question</TITLE>\n\n<BODY>### Question\n\nHelp!! I use two different methods to eval model:\r\n1:        with torch.inference_mode():\r\n            output_ids = model.generate(\r\n                input_ids,\r\n                images=image_tensor.unsqueeze(0).half().cuda(),\r\n                do_sample=True,\r\n                temperature=0.2,\r\n                top_p=None,\r\n                num_beams=1,\r\n                # no_repeat_ngram_size=3,\r\n                max_new_tokens=1024,\r\n                use_cache=True)\r\n2:         with torch.inference_mode():\r\n            output_ids = model.generate(\r\n                input_ids,\r\n                images=image_tensor.to(dtype=torch.float16, device='cuda', non_blocking=True),\r\n                do_sample=True if args.temperature > 0 else False,\r\n                temperature=args.temperature,\r\n                top_p=args.top_p,\r\n                num_beams=args.num_beams,\r\n                max_new_tokens=args.max_new_tokens,\r\n                use_cache=True)\r\nwhy these two outputs are different？</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1408,
    "state": "closed",
    "created_by": "benihime91",
    "created_at": "2024-04-16T09:55:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1408</URL>\n\n<TITLE>[Usage] v1.5 task LoRA inference is broken ?</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: \r\n\r\nHi, I trained a task LoRA on custom dataset using the script under `scripts/v1_5/finetune_task_lora.sh`. The model has been trained but now i am unable to do any inference.\r\n\r\nCommand:\r\nThe following is the final script i used to train the task lora\r\n\r\n\r\n```bash\r\n#!/bin/bash\r\nexport TIMESTAMP=$(date +\"%Y-%m-%d_%H-%M-%S\")\r\nexport WANDB_PROJECT=...\r\n\r\ndeepspeed --include localhost:1,2 /mnt/data1/ayushman/projects/dash-diffusion/LLaVA/llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed  ./scripts/zero3.json \\\r\n    --model_name_or_path liuhaotian/llava-v1.5-13b \\\r\n    --version v1 \\\r\n    --data_path ... \\\r\n    --image_folder ... \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/$TIMESTAMP \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\n\r\n\r\nThe following is the command i use to launch the model worker\r\n\r\n\r\n```bash\r\nCUDA_VISIBLE_DEVICES=1 CUDA_LAUNCH_BLOCKING=1 python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ${MODEL_PATH} --model-base lmsys/vicuna-13b-v1.5\r\n```\r\n\r\nI am adding the model_worker logs below\r\n\r\nLog: \r\n```bash\r\n2024-04-16 02:52:43 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='/checkpoints/llava-v1.5-13b-task-lora/2024-04-16_01-38-57/', model_base='lmsys/vicuna-13b-v1.5', model_name=None, device='cuda', multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=False, use_flash_attn=False)\r\n2024-04-16 02:52:43 | INFO | model_worker | Loading the model 2024-04-16_01-38-57 on worker dcf422 ...\r\n2024-04-16 02:52:44 | ERROR | stderr | \r\nLoading checkpoint shards:   0%|                                                                                        | 0/3 [00:00<?, ?it/s]\r\n2024-04-16 02:52:44 | ERROR | stderr | /home/ayushman/miniforge3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n2024-04-16 02:52:44 | ERROR | stderr |   return self.fget.__get__(instance, owner)()\r\n2024-04-16 02:52:46 | ERROR | stderr | \r\nLoading checkpoint shards:  33%|██████████████████████████▋                                                     | 1/3 [00:01<00:03,  1.92s/it]\r\n2024-04-16 02:52:47 | ERROR | stderr | \r\nLoading checkpoint shards:  67%|█████████████████████████████████████████████████████▎                          | 2/3 [00:03<00:01,  1.50s/it]\r\n2024-04-16 02:52:48 | ERROR | stderr | \r\nLoading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.14s/it]\r\n2024-04-16 02:52:48 | ERROR | stderr | \r\nLoading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.28s/it]\r\n2024-04-16 02:52:48 | ERROR | stderr | \r\n2024-04-16 02:52:48 | ERROR | stderr | /home/ayushman/miniforge3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\r\n2024-04-16 02:52:48 | ERROR | stderr |   warnings.warn(\r\n2024-04-16 02:52:48 | ERROR | stderr | /home/ayushman/miniforge3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\r\n2024-04-16 02:52:48 | ERROR | stderr |   warnings.warn(\r\n2024-04-16 02:52:48 | INFO | stdout | Loading LoRA weights from /mnt/data1/ayushman/projects/dash-diffusion/checkpoints/llava-v1.5-13b-task-lora/2024-04-16_01-38-57\r\n2024-04-16 02:52:52 | INFO | stdout | Merging weights\r\n2024-04-16 02:52:53 | INFO | stdout | Convert to FP16...\r\n2024-04-16 02:52:53 | INFO | model_worker | Register to controller\r\n2024-04-16 02:52:53 | ERROR | stderr | \u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m3105957\u001b[0m]\r\n2024-04-16 02:52:53 | ERROR | stderr | \u001b[32mINFO\u001b[0m:     Waiting for application startup.\r\n2024-04-16 02:52:53 | ERROR | stderr | \u001b[32mINFO\u001b[0m:     Application startup complete.\r\n2024-04-16 02:52:53 | ERROR | stderr | \u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:40000\u001b[0m (Press CTRL+C to quit)\r\n2024-04-16 02:52:53 | INFO | stdout | \u001b[32mINFO\u001b[0m:     127.0.0.1:56118 - \"\u001b[1mPOST /worker_get_status HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\r\n2024-04-16 02:53:08 | INFO | model_worker | Send heart beat. Models: ['2024-04-16_01-38-57']. Semaphore: None. global_counter: 0\r\n2024-04-16 02:53:18 | INFO | model_worker | Send heart beat. Models: ['2024-04-16_01-38-57']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n2024-04-16 02:53:18 | INFO | stdout | \u001b[32mINFO\u001b[0m:     127.0.0.1:39104 - \"\u001b[1mPOST /worker_generate_stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\r\n2024-04-16 02:53:18 | ERROR | stderr | /home/ayushman/miniforge3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py:1295: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\r\n2024-04-16 02:53:18 | ERROR | stderr |   warnings.warn(\r\n2024-04-16 02:53:19 | ERROR | stderr | Exception in thread Thread-4 (generate):\r\n2024-04-16 02:53:19 | ERROR | stderr | Traceback (most recent call last):\r\n2024-04-16 02:53:19 | ERROR | stderr |   File \"/home/ayushman/miniforge3/envs/llava/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\r\n2024-04-16 02:53:19 | ERROR | stderr |     self.run()\r\n2024-04-16 02:53:19 | ERROR | stderr |   File \"/home/ayushman/miniforge3/envs/llava/lib/python3.10/threading.py\", line 953, in run\r\n2024-04-16 02:53:19 | ERROR | stderr |     self._target(*self._args, **self._kwargs)\r\n2024-04-16 02:53:19 | ERROR | stderr |   File \"/home/ayushman/miniforge3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n2024-04-16 02:53:19 | ERROR | stderr |     return func(*args, **kwargs)\r\n2024-04-16 02:53:19 | ERROR | stderr |   File \"/home/ayushman/miniforge3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1525, in generate\r\n2024-04-16 02:53:19 | ERROR | stderr |     return self.sample(\r\n2024-04-16 02:53:19 | ERROR | stderr |   File \"/home/ayushman/miniforge3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2622, in sample\r\n2024-04-16 02:53:19 | ERROR | stderr |     outputs = self(\r\n2024-04-16 02:53:19 | ERROR | stderr |   File \"/home/ayushman/miniforge3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n2024-04-16 02:53:19 | ERROR | stderr |     return self._call_impl(*args, **kwargs)\r\n2024-04-16 02:53:19 | ERROR | stderr |   File \"/home/ayushman/miniforge3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n2024-04-16 02:53:19 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2024-04-16 02:53:19 | ERROR | stderr |   File \"/home/ayushman/miniforge3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1183, in forward\r\n2024-04-16 02:53:19 | ERROR | stderr |     outputs = self.model(\r\n2024-04-16 02:53:19 | ERROR | stderr |   File \"/home/ayushman/miniforge3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n2024-04-16 02:53:19 | ERROR | stderr |     return self._call_impl(*args, **kwargs)\r\n2024-04-16 02:53:19 | ERROR | stderr |   File \"/home/ayushman/miniforge3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n2024-04-16 02:53:19 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2024-04-16 02:53:19 | ERROR | stderr |   File \"/home/ayushman/miniforge3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1027, in forward\r\n2024-04-16 02:53:19 | ERROR | stderr |     inputs_embeds = self.embed_tokens(input_ids)\r\n2024-04-16 02:53:19 | ERROR | stderr |   File \"/home/ayushman/miniforge3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n2024-04-16 02:53:19 | ERROR | stderr |     return self._call_impl(*args, **kwargs)\r\n2024-04-16 02:53:19 | ERROR | stderr |   File \"/home/ayushman/miniforge3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n2024-04-16 02:53:19 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2024-04-16 02:53:19 | ERROR | stderr |   File \"/home/ayushman/miniforge3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/sparse.py\", line 162, in forward\r\n2024-04-16 02:53:19 | ERROR | stderr |     return F.embedding(\r\n2024-04-16 02:53:19 | ERROR | stderr |   File \"/home/ayushman/miniforge3/envs/llava/lib/python3.10/site-packages/torch/nn/functional.py\", line 2233, in embedding\r\n2024-04-16 02:53:19 | ERROR | stderr |     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\n2024-04-16 02:53:19 | ERROR | stderr | RuntimeError: CUDA error: device-side assert triggered\r\n2024-04-16 02:53:19 | ERROR | stderr | Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n2024-04-16 02:53:19 | ERROR | stderr | \r\n2024-04-16 02:53:23 | INFO | model_worker | Send heart beat. Models: ['2024-04-16_01-38-57']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n2024-04-16 02:53:33 | INFO | stdout | Caught Unknown Error\r\n2024-04-16 02:53:33 | INFO | model_worker | Send heart beat. Models: ['2024-04-16_01-38-57']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n2024-04-16 02:53:38 | INFO | model_worker | Send heart beat. Models: ['2024-04-16_01-38-57']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n\r\n```\r\n\r\nAlso adding my env for better visibility\r\n\r\n```txt\r\nPackage                   Version     Editable project location\r\n------------------------- ----------- -------------------------------------------------\r\naccelerate                0.21.0\r\naiofiles                  23.2.1\r\nalbumentations            1.4.3\r\naltair                    5.3.0\r\nannotated-types           0.6.0\r\nanyio                     4.3.0\r\nappdirs                   1.4.4\r\nasttokens                 2.4.1\r\nattrs                     23.2.0\r\nbitsandbytes              0.43.1\r\nblis                      0.7.11\r\nbraceexpand               0.1.7\r\ncachetools                5.3.3\r\ncatalogue                 2.0.10\r\ncertifi                   2024.2.2\r\ncharset-normalizer        3.3.2\r\nclick                     8.1.7\r\ncloudpathlib              0.16.0\r\ncolorama                  0.4.6\r\ncomm                      0.2.2\r\nconfection                0.1.4\r\ncontourpy                 1.2.1\r\ncycler                    0.12.1\r\ncymem                     2.0.8\r\ndataclasses               0.6\r\ndebugpy                   1.8.1\r\ndecorator                 5.1.1\r\ndeepspeed                 0.12.6\r\ndocker-pycreds            0.4.0\r\neinops                    0.6.1\r\neinops-exts               0.0.4\r\nexceptiongroup            1.2.0\r\nexecuting                 2.0.1\r\nExifRead-nocycle          3.0.1\r\nfastai                    2.7.14\r\nfastapi                   0.110.1\r\nfastcore                  1.5.29\r\nfastdownload              0.0.7\r\nfastprogress              1.0.3\r\nffmpy                     0.3.2\r\nfilelock                  3.13.4\r\nfire                      0.5.0\r\nflash-attn                2.5.7\r\nfonttools                 4.51.0\r\nfsspec                    2024.3.1\r\ngitdb                     4.0.11\r\nGitPython                 3.1.43\r\ngradio                    4.16.0\r\ngradio_client             0.8.1\r\nh11                       0.14.0\r\nhjson                     3.1.0\r\nhttpcore                  0.17.3\r\nhttpx                     0.24.0\r\nhuggingface-hub           0.22.2\r\nidna                      3.7\r\nimageio                   2.34.0\r\nimg2dataset               1.45.0\r\nimportlib_metadata        7.1.0\r\nimportlib_resources       6.4.0\r\nipykernel                 6.29.4\r\nipython                   8.23.0\r\njedi                      0.19.1\r\nJinja2                    3.1.3\r\njoblib                    1.4.0\r\njsonschema                4.21.1\r\njsonschema-specifications 2023.12.1\r\njupyter_client            8.6.1\r\njupyter_core              5.7.2\r\nkiwisolver                1.4.5\r\nlangcodes                 3.3.0\r\nlazy_loader               0.4\r\nllava                     1.2.2.post1 /mnt/data1/ayushman/projects/dash-diffusion/LLaVA\r\nmarkdown-it-py            3.0.0\r\nmarkdown2                 2.4.13\r\nMarkupSafe                2.1.5\r\nmatplotlib                3.8.4\r\nmatplotlib-inline         0.1.6\r\nmdurl                     0.1.2\r\nmpmath                    1.3.0\r\nmurmurhash                1.0.10\r\nnest_asyncio              1.6.0\r\nnetworkx                  3.3\r\nninja                     1.11.1.1\r\nnumpy                     1.26.4\r\nnvidia-cublas-cu12        12.1.3.1\r\nnvidia-cuda-cupti-cu12    12.1.105\r\nnvidia-cuda-nvrtc-cu12    12.1.105\r\nnvidia-cuda-runtime-cu12  12.1.105\r\nnvidia-cudnn-cu12         8.9.2.26\r\nnvidia-cufft-cu12         11.0.2.54\r\nnvidia-curand-cu12        10.3.2.106\r\nnvidia-cusolver-cu12      11.4.5.107\r\nnvidia-cusparse-cu12      12.1.0.106\r\nnvidia-ml-py              12.535.133\r\nnvidia-nccl-cu12          2.18.1\r\nnvidia-nvjitlink-cu12     12.4.127\r\nnvidia-nvtx-cu12          12.1.105\r\nnvitop                    1.3.2\r\nopencv-python-headless    4.9.0.80\r\norjson                    3.10.1\r\npackaging                 24.0\r\npandas                    2.2.2\r\nparso                     0.8.4\r\npeft                      0.10.0\r\npexpect                   4.9.0\r\npickleshare               0.7.5\r\npillow                    10.3.0\r\npip                       24.0\r\nplatformdirs              4.2.0\r\npreshed                   3.0.9\r\nprompt-toolkit            3.0.43\r\nprotobuf                  4.25.3\r\npsutil                    5.9.8\r\nptyprocess                0.7.0\r\npure-eval                 0.2.2\r\npy-cpuinfo                9.0.0\r\npyarrow                   15.0.2\r\npydantic                  2.7.0\r\npydantic_core             2.18.1\r\npydub                     0.25.1\r\nPygments                  2.17.2\r\npynvml                    11.5.0\r\npyparsing                 3.1.2\r\npython-dateutil           2.9.0.post0\r\npython-multipart          0.0.9\r\npytz                      2024.1\r\nPyYAML                    6.0.1\r\npyzmq                     26.0.0\r\nreferencing               0.34.0\r\nregex                     2023.12.25\r\nrequests                  2.31.0\r\nrich                      13.7.1\r\nrpds-py                   0.18.0\r\nruff                      0.3.7\r\nsafetensors               0.4.3\r\nscikit-image              0.23.1\r\nscikit-learn              1.2.2\r\nscipy                     1.13.0\r\nsemantic-version          2.10.0\r\nsentencepiece             0.1.99\r\nsentry-sdk                1.45.0\r\nsetproctitle              1.3.3\r\nsetuptools                69.5.1\r\nshellingham               1.5.4\r\nshortuuid                 1.0.13\r\nsix                       1.16.0\r\nsmart-open                6.4.0\r\nsmmap                     5.0.1\r\nsniffio                   1.3.1\r\nspacy                     3.7.4\r\nspacy-legacy              3.0.12\r\nspacy-loggers             1.0.5\r\nsrsly                     2.4.8\r\nstack-data                0.6.3\r\nstarlette                 0.37.2\r\nsvgwrite                  1.4.3\r\nsympy                     1.12\r\ntermcolor                 2.4.0\r\nthinc                     8.2.3\r\nthreadpoolctl             3.4.0\r\ntifffile                  2024.2.12\r\ntimm                      0.6.13\r\ntokenizers                0.15.1\r\ntomlkit                   0.12.0\r\ntoolz                     0.12.1\r\ntorch                     2.1.2\r\ntorchvision               0.16.2\r\ntornado                   6.4\r\ntqdm                      4.66.2\r\ntraitlets                 5.14.2\r\ntransformers              4.37.2\r\ntriton                    2.1.0\r\ntyper                     0.9.4\r\ntyping_extensions         4.11.0\r\ntzdata                    2024.1\r\nurllib3                   2.2.1\r\nuvicorn                   0.29.0\r\nwandb                     0.16.6\r\nwasabi                    1.1.2\r\nwavedrom                  2.0.3.post3\r\nwcwidth                   0.2.13\r\nweasel                    0.3.4\r\nwebdataset                0.2.86\r\nwebsockets                11.0.3\r\nwheel                     0.43.0\r\nzipp                      3.17.0\r\n\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by feiyangsuo at 2024-07-15T09:46:06Z>\nHi, did you find the solution or any other way to load the trained lora for inference?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1407,
    "state": "open",
    "created_by": "Linjyan00",
    "created_at": "2024-04-16T08:54:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1407</URL>\n\n<TITLE>Although per_device_train_batch_size*gradient_accumulation_steps is consistent, the loss decline curve is inconsistent</TITLE>\n\n<BODY>For the training stage of llava1.5 (LLaVA/scripts/v1_5/finetune_lora.sh), I tried the following two different examples and found something wrong (2*A100)\r\n\r\n--per_device_train_batch_size 4 \\\r\n--gradient_accumulation_steps 16 \\\r\n\r\n\r\n--per_device_train_batch_size 1 \\\r\n--gradient_accumulation_steps 64 \\\r\n\r\nIn these two Settings, their first-iteration loss is not the same, but by definition, they should be. Why?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1406,
    "state": "closed",
    "created_by": "HuizaiVictorYao",
    "created_at": "2024-04-16T07:21:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1406</URL>\n\n<TITLE>[Usage] Merging and Finetuning llava 1.5 13b</TITLE>\n\n<BODY></BODY>\n\n<COMMENTS>\n<Comment by HuizaiVictorYao at 2024-04-16T16:08:40Z>\nNVM. I've just mistaken some of the checkpoint paths.  model_vqa.py is able to directly work on lora unmerged path.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1405,
    "state": "closed",
    "created_by": "fisher75",
    "created_at": "2024-04-15T15:25:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1405</URL>\n\n<TITLE>[Question] Batch Inference Lora Model + SGLang Supported LLaVa models 请问批量推理还支持哪些LLaVa的model？Lora后的模型如何批量推理？</TITLE>\n\n<BODY>### Question\n\nI saw at README saying those models are supported, got two questions:\r\n(1) what if I wanna use `llava-v1.6-vicuna-13b` or any other LLaVa models, is that possible? Thanks!\r\n(2) After I fine-tune or lora the existing models, how can I do a batch inference with it since in SGLang it looks that I need --model-path and --tokenizer-path to do batch inference.\r\n\r\n> LLaVA\r\n> python3 -m sglang.launch_server --model-path liuhaotian/llava-v1.5-7b --tokenizer-path llava-hf/llava-1.5-7b-hf --chat-template vicuna_v1.1 --port 30000\r\n> python3 -m sglang.launch_server --model-path liuhaotian/llava-v1.6-vicuna-7b --tokenizer-path llava-hf/llava-1.5-7b-hf --chat-template vicuna_v1.1 --port 30000\r\n> python3 -m sglang.launch_server --model-path liuhaotian/llava-v1.6-34b --tokenizer-path liuhaotian/llava-v1.6-34b-tokenizer --port 3000</BODY>\n\n<COMMENTS>\n<Comment by fengluo233 at 2024-05-19T05:44:17Z>\nHello! I have the same question. Have you solved it?\n</Comment>\n<Comment by anas-zafar at 2024-09-10T21:15:34Z>\nHi @fengluo233 @fisher75 did you solve it? Thanks\n</Comment>\n<Comment by fengluo233 at 2024-10-12T15:50:47Z>\n> Hi @fengluo233 @fisher75 did you solve it? Thanks嗨，您解决了吗？谢谢\r\n\r\nHi, I solved it by using SGLang. It works.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1404,
    "state": "closed",
    "created_by": "Hesh0629",
    "created_at": "2024-04-15T13:57:57Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1404</URL>\n\n<TITLE>[Question] How to use pretrained projector in finetuning?</TITLE>\n\n<BODY>### Question\n\nHello,\r\n\r\nI'm trying to finetune llava-v1.57b with my pretrained projector.\r\nHowever, when i use --pretrain_mm_mlp_adapter option in finetune_task_lora.sh, i met error message below.\r\n\r\n![Screenshot 2024-04-15 at 10 57 31 PM](https://github.com/haotian-liu/LLaVA/assets/57738176/ecdac0b7-1b5d-4f59-ac43-db4d998f2449)\r\n\r\nhow can i use my projector in finetuning...?</BODY>\n\n<COMMENTS>\n<Comment by Hesh0629 at 2024-04-17T04:53:58Z>\nI just solved this by switching projector file in model directory.\r\nBefore that, I downloaded model from hugging face hub.\n</Comment>\n<Comment by fangxin2github at 2024-06-03T08:52:46Z>\nI met same problem. Can you explain it in detail？ thank u.\n</Comment>\n<Comment by hvgupta at 2024-07-20T04:00:09Z>\n> I just solved this by switching projector file in model directory. Before that, I downloaded model from hugging face hub.\r\n\r\ncan you elaborate?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1402,
    "state": "open",
    "created_by": "leechangdong",
    "created_at": "2024-04-15T08:58:59Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1402</URL>\n\n<TITLE>[Question]  Getting the model after one epoch is too long. Is there any way to save the model in between 1 epoch?</TITLE>\n\n<BODY>### Question\n\nI'm \bfine-tune on my own custom data\r\nGetting the model after one epoch is too long. Is there any way to save the model in between 1 epoch?</BODY>\n\n<COMMENTS>\n<Comment by Jayantverma2 at 2024-04-15T11:52:15Z>\n--save_strategy \"epoch\" \\\r\n    --save_steps .3 \\\n</Comment>\n<Comment by leechangdong at 2024-04-16T09:42:28Z>\n@Jayantverma2 \r\nThank you for your response.\r\nBut in my fine_tune_task_lora.sh\r\n\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\nto\r\n \r\n    --save_strategy \"epoch\" \\\r\n    --save_steps .3 \\\r\n    --save_total_limit 3 \\\r\n\r\nand It didn't save while the training was still in progress, but after the training was over, the checkpoints were split and saved.\r\n\r\nIs there any way to get it to save while the lesson is in progress?\n</Comment>\n<Comment by Jayantverma2 at 2024-04-16T18:28:13Z>\ngo for \r\n--save_strategy \"steps\" \\\r\n--save_steps .2\r\n --save_total_limit 1\r\n    if decimal it will save at 20% of total steps , save_total_limit will make ensure only running loss weights are saved , int his way u can save intemediate, try this this works for me\n</Comment>\n<Comment by leechangdong at 2024-04-17T16:20:58Z>\n```\r\nfrom llava.eval.run_llava import eval_model\r\nfrom llava.mm_utils import get_model_name_from_path\r\n\r\nargs = type('Args', (), {\r\n    \"model_path\": model_path,\r\n    \"model_base\": model_base,\r\n    \"model_name\": get_model_name_from_path(model_path),\r\n    \"query\": prompt,\r\n    \"conv_mode\": None,\r\n    \"image_file\": image_file,\r\n    \"sep\": \",\",\r\n    \"temperature\": 0,\r\n    \"top_p\": None,\r\n    \"num_beams\": 1,\r\n    \"max_new_tokens\": 512\r\n})()\r\n\r\ninference = eval_model(args)\r\n```\r\n\r\nOSError: [llava-v1.5-7b-task-lora-13/checkpoint-6](/llava-v1.5-7b-task-lora-13/checkpoint-6) does not appear to have a file named config.json. Checkout 'https://huggingface.co//llava-v1.5-7b-task-lora-13/checkpoint-6/main' for available files.\r\n\r\nI tried to fix the problem by roughly copying the \"config.json\", but the \"non_lora_trainables.bin\" file is not in the folder, so I get the another error\r\n\r\nHFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name':\r\n\r\n\r\n<img width=\"258\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/115126427/229b90af-3c12-4811-9e4a-8161a8255dae\">\r\n\r\n@Jayantverma2  Sorry.\r\n\r\nI am getting an error when running the inference with the above code because the \"config.json\" and \"non_lora_trainables.bin\" file is not saved in the checkpoint folder saved with the above settings.\r\n\r\nMy purpose is to inference with checkpoints saved during training\r\n\r\nDoes anyone know how to fix this issue?\n</Comment>\n<Comment by yinyuanzhang at 2025-04-05T13:52:32Z>\nmark\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1400,
    "state": "open",
    "created_by": "py4",
    "created_at": "2024-04-12T20:48:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1400</URL>\n\n<TITLE>[Question] SGlang seems to support AWQ/GPTQ quantization but readme mentions 4 bit quantization is not supported in sglang.</TITLE>\n\n<BODY>### Question\n\n@haotian-liu hey. SGLang project mentions they support AWQ/GPTQ quantization. But Readme of llava mention 4 bit quantization is not support in sglang. two questions:\r\n\r\n1. what about 8 bit quantization?\r\n2. given that sglang  mentions they support AWQ/GPTQ, how come you mention 4 bit quant is not supported?\r\n\r\nThanks!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1399,
    "state": "closed",
    "created_by": "yukiarimo",
    "created_at": "2024-04-12T20:00:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1399</URL>\n\n<TITLE>[Question] How does the model works?</TITLE>\n\n<BODY>### Question\n\nCan somebody please explain how the model works? For example, if my context window is 1024 tokens, and the model uses 724 for the image (or whatever), how? The example image is 378x378 (it’s a lot of pixels). How do you put an image into a text using such a small amount of tokens?  (Please no math)</BODY>\n\n<COMMENTS>\n<Comment by yukiarimo at 2025-03-16T22:46:21Z>\nGot it\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1398,
    "state": "open",
    "created_by": "ogimgio",
    "created_at": "2024-04-12T17:16:30Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1398</URL>\n\n<TITLE>[Question] Why Vision encoder frozen?</TITLE>\n\n<BODY>### Question\n\nHi,\r\nI would like to ask you what is the reason for keeping the vision encoder frozen. I am asking this because OCR perfomance of vision encoder from CLIP can be improved and thus why wouldn't we need to also fine-tune the vision encoder? is there any particular reason?\r\n\r\nThanks,\r\nGioele</BODY>\n\n<COMMENTS>\n<Comment by FanWangRIT at 2024-04-18T18:31:43Z>\nSame question since LLM is not frozen. Is there any reason to keep the vision encoder frozen?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1397,
    "state": "open",
    "created_by": "20191864218",
    "created_at": "2024-04-12T13:55:29Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1397</URL>\n\n<TITLE>[Question] Lora微调之后，合并权重，再进行Lora</TITLE>\n\n<BODY>### Question\n\n各位大佬，我想请问一下能不能Lora微调之后，合并权重，再进行Lora微调在合并权重呀</BODY>\n\n<COMMENTS>\n<Comment by awzhgw at 2024-04-12T19:16:54Z>\n我也有这个问题。。你先试试呗\n</Comment>\n<Comment by 20191864218 at 2024-04-13T01:52:48Z>\n> 我也有这个问题。。你先试试呗\r\n\r\n因为得租卡微调，一次一百多块钱，成本有点多/(ㄒoㄒ)/~~\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1396,
    "state": "open",
    "created_by": "shengyuwoo",
    "created_at": "2024-04-12T06:20:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1396</URL>\n\n<TITLE>LLaVA-1.6 training dataset and training code.</TITLE>\n\n<BODY>### Describe the issue\n\nWhen will the llava-1.6 training dataset and training code be open-sourced?\r\nHello, I'm glad to see that the performance of llava-1.6 has improved so significantly. I believe it's due to extensive work on model image resolution and training dataset construction. As for the release of llava-1.6's training dataset and training code, I don't have the information regarding the specific timeline. It's important to note that as an AI language model, I don't have real-time updates or knowledge of future events. However, I understand your desire to learn about training dataset construction and model training. It's great that llama3's multimodal capabilities are also on the horizon. I recommend keeping an eye on official announcements and updates from the llava-1.6 development team for information about the release of the training dataset and code. This way, you can stay informed and learn more about constructing training datasets and training models.</BODY>\n\n<COMMENTS>\n<Comment by bhuvanl at 2024-04-15T15:00:53Z>\n+1   \r\n\r\nWhen will be training dataset and training code be open-sourced for llava-1.6.\n</Comment>\n<Comment by 50Bytes-dev at 2024-05-20T02:11:51Z>\n+1\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1395,
    "state": "open",
    "created_by": "awzhgw",
    "created_at": "2024-04-12T05:43:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1395</URL>\n\n<TITLE>[Usage] The optimization of the slow training speed for pure text training.</TITLE>\n\n<BODY>### Describe the issue\n\n\r\nI encountered an issue during the training process. I am using the LoRA fine-tuning method, and my data consists of two parts:\r\n\r\n1. Image-question-answering dialogues.\r\n2. Pure question-answering dialogues.\r\n\r\nDuring training, I found that the training speed for the second part of the dataset is very slow, and it is as slow as the first part. After investigation, I found that the reason is:\r\n\r\n在train.py的LazySupervisedDataset 类的__getitem__ 方法：\r\n\r\n```python\r\n            if 'image' in self.list_data_dict[i]:\r\n                data_dict['image'] = image\r\n                data_dict['image_size'] = image_size\r\n            elif self.data_args.is_multimodal:\r\n                # image does not exist in the data, but the model is multimodal\r\n                crop_size = self.data_args.image_processor.crop_size\r\n                data_dict['image'] = torch.zeros(3, crop_size['height'], crop_size['width'])\r\n                data_dict['image_size'] = crop_size\r\n            return data_dict\r\n```\r\n\r\nDue to the large amount of pure text data, can I remove the following parameter from the training script:\r\n\r\n--vision_tower ${VISION_TOWER}\r\n\r\nand then perform fine-tuning for the pure text stage?\r\n\r\n\r\n\r\nEv0entually my training will become: 1) A stage with pictures-text question and answer, plus --vision_tower ${VISION_TOWER} for lora  fine-tuning. . . 2) In the stage of plain text question and answer, remove -vision_tower ${VISION_TOWER} for lora fine-tuning\r\n\r\n\r\n\r\nCan I do this? ? ? Finally trained a good llava model</BODY>\n\n<COMMENTS>\n<Comment by awzhgw at 2024-04-12T05:52:10Z>\n@haotian-liu\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1394,
    "state": "closed",
    "created_by": "Ben81828",
    "created_at": "2024-04-12T02:20:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1394</URL>\n\n<TITLE>[Discussion] Long estimated running time for finetuning Visual Instruction Tuning on colab with 4 T4 GPUs</TITLE>\n\n<BODY>### Discussion\r\n\r\n\r\nHi,\r\n\r\nI am trying to fine-tune Visual Instruction Tuning on colab with 4 T4 GPUs. I tried to modify the finetune_lora.sh script with the following commands to avoid OOM:\r\n\r\n```\r\n!deepspeed llava/train/train_xformers.py \\\r\n    --lora_enable True --lora_r 32 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3_offload.json \\\r\n    --model_name_or_path lmsys/vicuna-7b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/llava_v1_5_mix665k.json \\\r\n    --image_folder ./playground/data \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-7b/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 False \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 32 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 False \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\n\r\nHowever, the estimated running time is currently showing more than 700 hours.\r\n\r\nI would like to ask if anyone has similar equipment and fine-tuning experience. Is this time normal? Or is there something wrong with my settings?\r\n\r\nAny help would be greatly appreciated.\r\n\r\nThanks.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1393,
    "state": "open",
    "created_by": "samuruph",
    "created_at": "2024-04-10T14:34:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1393</URL>\n\n<TITLE>Inference errors when multiple questions</TITLE>\n\n<BODY></BODY>\n\n<COMMENTS>\n<Comment by Z1zs at 2024-06-03T06:42:52Z>\nHi Ruffino, it maybe somehow irrelevant to your question, but I'm really curious how did you construct the *Autonomous Driving Scene Discussion*. It looks so great.\r\nDid you build it by yourself or borrow it from some existing papers?\r\nThank you very much!!!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1392,
    "state": "open",
    "created_by": "samuruph",
    "created_at": "2024-04-10T13:20:14Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1392</URL>\n\n<TITLE>Model v1.6-mistral-7b --load4bit</TITLE>\n\n<BODY>### Describe the issue\n\nHello,\r\n\r\nI am currently using model v1.6-mistral-7b with 4-bit quantization. I am getting this warning and I was wondering whether i shoudl take care of it or it is not relevant.\r\n```bash\r\n2024-04-10 15:03:09 | ERROR | stderr |   warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\r\n2024-04-10 15:03:09 | ERROR | stderr | /home/samuele/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\r\n2024-04-10 15:03:09 | ERROR | stderr |   warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\r\n2024-04-10 15:03:09 | ERROR | stderr | /home/samuele/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\r\n2024-04-10 15:03:09 | ERROR | stderr |   warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\r\n2024-04-10 15:03:09 | ERROR | stderr | /home/samuele/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\r\n2024-04-10 15:03:09 | ERROR | stderr |   warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\r\n2024-04-10 15:03:09 | ERROR | stderr | /home/samuele/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\r\n2024-04-10 15:03:09 | ERROR | stderr |   warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\r\n2024-04-10 15:03:09 | ERROR | stderr | /home/samuele/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\r\n2024-04-10 15:03:09 | ERROR | stderr |   warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\r\n2024-04-10 15:03:09 | ERROR | stderr | /home/samuele/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\r\n2024-04-10 15:03:09 | ERROR | stderr |   warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\r\n2024-04-10 15:03:09 | ERROR | stderr | /home/samuele/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\r\n2024-04-10 15:03:09 | ERROR | stderr |   warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\r\n2024-04-10 15:03:09 | ERROR | stderr | /home/samuele/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\r\n2024-04-10 15:03:09 | ERROR | stderr |   warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\r\n2024-04-10 15:03:09 | ERROR | stderr | /home/samuele/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\r\n2024-04-10 15:03:09 | ERROR | stderr |   warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\r\n2024-04-10 15:03:09 | ERROR | stderr | /home/samuele/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\r\n2024-04-10 15:03:09 | ERROR | stderr |   warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\r\n2024-04-10 15:03:09 | ERROR | stderr | /home/samuele/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\r\n2024-04-10 15:03:09 | ERROR | stderr |   warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\r\n2024-04-10 15:03:09 | ERROR | stderr | /home/samuele/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\r\n2024-04-10 15:03:09 | ERROR | stderr |   warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\r\n2024-04-10 15:03:09 | ERROR | stderr | /home/samuele/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\r\n2024-04-10 15:03:09 | ERROR | stderr |   warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\r\n2024-04-10 15:03:09 | ERROR | stderr | /home/samuele/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\r\n2024-04-10 15:03:09 | ERROR | stderr |   warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\r\n2024-04-10 15:03:09 | ERROR | stderr | /home/samuele/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\r\n2024-04-10 15:03:09 | ERROR | stderr |   warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\r\n2024-04-10 15:03:09 | ERROR | stderr | /home/samuele/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\r\n2024-04-10 15:03:09 | ERROR | stderr |   warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\r\n2024-04-10 15:03:09 | ERROR | stderr | /home/samuele/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\r\n2024-04-10 15:03:09 | ERROR | stderr |   warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\r\n2024-04-10 15:03:09 | ERROR | stderr | /home/samuele/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\r\n2024-04-10 15:03:09 | ERROR | stderr |   warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\r\n2024-04-10 15:03:09 | ERROR | stderr | /home/samuele/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\r\n2024-04-10 15:03:09 | ERROR | stderr |   warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\r\n2024-04-10 15:03:09 | ERROR | stderr | /home/samuele/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\r\n```\r\nI get it several times actually, not only the ones reported here.\r\n\r\nThank you for your help :)</BODY>\n\n<COMMENTS>\n<Comment by kdlsw at 2024-04-16T07:59:44Z>\nsame here, any ideas?\n</Comment>\n<Comment by Aesc-zzh at 2024-04-19T08:31:43Z>\nme too. Can anyone help?\n</Comment>\n<Comment by Ramzes30765 at 2025-06-19T13:04:34Z>\nSame for me\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1391,
    "state": "open",
    "created_by": "jiezhangGt",
    "created_at": "2024-04-10T12:25:28Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1391</URL>\n\n<TITLE>Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\n\r\nCommand:\r\n```\r\nCUDA_VISIBLE_DEVICES=1,2,3,4,5,6,7 python3 -m sglang.launch_server --model-path liuhaotian/llava-v1.6-34b --tokenizer-path llava-hf/llava-v1.6-34b-hf --port 30000 --tp 7\r\n```\r\n\r\n\r\nthe error is\r\n\r\n[2024-04-10 20:18:11,995] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\npreprocessor_config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 754/754 [00:00<00:00, 5.15MB/s]\r\ntokenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.86k/1.86k [00:00<00:00, 16.8MB/s]\r\ntokenizer.model: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.03M/1.03M [00:00<00:00, 4.17MB/s]\r\nadded_tokens.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23.0/23.0 [00:00<00:00, 228kB/s]\r\nspecial_tokens_map.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 748/748 [00:00<00:00, 7.38MB/s]\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nserver started on [0.0.0.0]:10005\r\nserver started on [0.0.0.0]:10006\r\nserver started on [0.0.0.0]:10007\r\nserver started on [0.0.0.0]:10008\r\nserver started on [0.0.0.0]:10009\r\nserver started on [0.0.0.0]:10010\r\nserver started on [0.0.0.0]:10011\r\naccepted ('127.0.0.1', 29117) with fd 93\r\nwelcome ('127.0.0.1', 29117)\r\naccepted ('127.0.0.1', 38123) with fd 85\r\nwelcome ('127.0.0.1', 38123)\r\naccepted ('127.0.0.1', 48207) with fd 86\r\nwelcome ('127.0.0.1', 48207)\r\naccepted ('127.0.0.1', 14646) with fd 95\r\nwelcome ('127.0.0.1', 14646)\r\naccepted ('127.0.0.1', 44617) with fd 97\r\nwelcome ('127.0.0.1', 44617)\r\naccepted ('127.0.0.1', 38550) with fd 99\r\nwelcome ('127.0.0.1', 38550)\r\naccepted ('127.0.0.1', 24387) with fd 95\r\nwelcome ('127.0.0.1', 24387)\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nrouter init state: Traceback (most recent call last):\r\n  File \"HOME/miniconda3/envs/llava/lib/python3.10/site-packages/sglang/srt/managers/router/manager.py\", line 68, in start_router_process\r\n    model_client = ModelRpcClient(server_args, port_args)\r\n  File \"HOME/miniconda3/envs/llava/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 640, in __init__\r\n    rets = [obtain(x) for x in executor.map(init_model, range(tp_size))]\r\n  File \"miniconda3/envs/llava/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 640, in <listcomp>\r\n    rets = [obtain(x) for x in executor.map(init_model, range(tp_size))]\r\n  File \"miniconda3/envs/llava/lib/python3.10/concurrent/futures/_base.py\", line 621, in result_iterator\r\n    yield _result_or_cancel(fs.pop())\r\n  File \"HOME/miniconda3/envs/llava/lib/python3.10/concurrent/futures/_base.py\", line 319, in _result_or_cancel\r\n    return fut.result(timeout)\r\n  File \"miniconda3/envs/llava/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\r\n    return self.__get_result()\r\n  File \"miniconda3/envs/llava/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\r\n    raise self._exception\r\n  File \"miniconda3/envs/llava/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"HOME/miniconda3/envs/llava/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 638, in init_model\r\n    return self.model_servers[i].init_model(i, server_args, port_args)\r\n  File \"HOME/miniconda3/envs/llava/lib/python3.10/site-packages/rpyc/core/netref.py\", line 239, in __call__\r\n    return syncreq(_self, consts.HANDLE_CALL, args, kwargs)\r\n  File \"HOME/miniconda3/envs/llava/lib/python3.10/site-packages/rpyc/core/netref.py\", line 63, in syncreq\r\n    return conn.sync_request(handler, proxy, *args)\r\n  File \"HOME/miniconda3/envs/llava/lib/python3.10/site-packages/rpyc/core/protocol.py\", line 744, in sync_request\r\n    return _async_res.value\r\n  File \"HOME/miniconda3/envs/llava/lib/python3.10/site-packages/rpyc/core/async_.py\", line 111, in value\r\n    raise self._obj\r\n_get_exception_class.<locals>.Derived: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\r\n\r\n========= Remote Traceback (1) =========\r\nTraceback (most recent call last):\r\n  File \"HOME/miniconda3/envs/llava/lib/python3.10/site-packages/rpyc/core/protocol.py\", line 369, in _dispatch_request\r\n    res = self._HANDLERS[handler](self, *args)\r\n  File \"HOME/miniconda3/envs/llava/lib/python3.10/site-packages/rpyc/core/protocol.py\", line 863, in _handle_call\r\n    return obj(*args, **dict(kwargs))\r\n  File \"HOME/miniconda3/envs/llava/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 70, in exposed_init_model\r\n    self.model_runner = ModelRunner(\r\n  File \"HOME/miniconda3/envs/llava/lib/python3.10/site-packages/sglang/srt/managers/router/model_runner.py\", line 271, in __init__\r\n    torch.cuda.set_device(self.tp_rank)\r\n  File \"miniconda3/envs/llava/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 404, in set_device\r\n    torch._C._cuda_setDevice(device)\r\n  File \"miniconda3/envs/llava/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 284, in _lazy_init\r\n    raise RuntimeError(\r\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\r\n\r\n\r\ndetoken init state: init ok\r\ngoodbye ('127.0.0.1', 14646)\r\ngoodbye ('127.0.0.1', 24387)\r\ngoodbye ('127.0.0.1', 29117)\r\ngoodbye ('127.0.0.1', 48207)\r\ngoodbye ('127.0.0.1', 38550)\r\ngoodbye ('127.0.0.1', 38123)\r\ngoodbye ('127.0.0.1', 44617)</BODY>\n\n<COMMENTS>\n<Comment by MengSunS at 2024-04-12T21:20:42Z>\nset start method to be spawn : https://stackoverflow.com/questions/61939952/mp-set-start-methodspawn-triggered-an-error-saying-the-context-is-already-be\n</Comment>\n<Comment by Poeroz at 2024-04-15T10:31:49Z>\nsame problem when running llava-v1.6-vicuna-13b with sglang:\r\n\r\n```Shell\r\nCUDA_VISIBLE_DEVICES=1,2 python -m sglang.launch_server --model-path llava-v1.6-vicuna-13b --tokenizer-path llava-v1.6-vicuna-13b-hf/ --port 30000 --tp 2\r\n```\r\n```\r\nTraceback (most recent call last):\r\n  File \"/data/***/anaconda3/envs/llava/lib/python3.10/site-packages/rpyc/core/protocol.py\", line 369, in _dispatch_request\r\n    res = self._HANDLERS[handler](self, *args)\r\n  File \"/data/***/anaconda3/envs/llava/lib/python3.10/site-packages/rpyc/core/protocol.py\", line 863, in _handle_call\r\n    return obj(*args, **dict(kwargs))\r\n  File \"/data/***/anaconda3/envs/llava/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 70, in exposed_init_model\r\n    self.model_runner = ModelRunner(\r\n  File \"/data/***/anaconda3/envs/llava/lib/python3.10/site-packages/sglang/srt/managers/router/model_runner.py\", line 271, in __init__\r\n    torch.cuda.set_device(self.tp_rank)\r\n  File \"/data/***/anaconda3/envs/llava/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 404, in set_device\r\n    torch._C._cuda_setDevice(device)\r\n  File \"/data/***/anaconda3/envs/llava/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 284, in _lazy_init\r\n    raise RuntimeError(\r\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\r\n```\n</Comment>\n<Comment by trimbilrepo at 2024-05-16T03:59:46Z>\nI have the same error\r\n`RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method`\r\n\r\nis there any workaround how to solve this error?\n</Comment>\n<Comment by chrisx599 at 2024-05-22T12:36:51Z>\ni modified the sglang's code, and it worked for me\r\nadd this in sglang>srt>server.py line 143\r\n```Python\r\ntry:\r\n    mp.set_start_method('spawn', force=True)\r\n    print(\"spawned\")\r\nexcept RuntimeError:\r\n    pass\r\n```\n</Comment>\n<Comment by endNone at 2024-08-12T06:09:50Z>\n> i modified the sglang's code, and it worked for me add this in sglang>srt>server.py line 143\r\n> \r\n> ```python\r\n> try:\r\n>     mp.set_start_method('spawn', force=True)\r\n>     print(\"spawned\")\r\n> except RuntimeError:\r\n>     pass\r\n> ```\r\nIt does not work for me\n</Comment>\n<Comment by Lin-sudo at 2024-09-14T03:05:33Z>\nit worked for me, but I add the above code in func `launch_server`, line 279, sglang v0.3.0\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1390,
    "state": "open",
    "created_by": "GuanlinLee",
    "created_at": "2024-04-10T07:28:12Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1390</URL>\n\n<TITLE>[Usage] How to craft a custom dataset</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: It is confusing when I make a custom dataset, referring to the example provided in [the link](https://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md). Because the roles ('human' and 'gpt') do not match the conversation templates used in inference phase ('USER' and 'ASSISTANT').\r\n\r\nConversation template:\r\n```\r\n    conv_llava_v1 = Conversation(\r\n        system='',\r\n        roles=(\"USER\", \"ASSISTANT\"),\r\n        version=\"v1\",\r\n        messages=[],\r\n        offset=0,\r\n        sep_style=SeparatorStyle.TWO,\r\n        sep=\" \",\r\n        sep2=\"</s>\",\r\n    )\r\n```\r\n\r\nIs it important to use the same name for roles? How to make a correct format for llava-1.6?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1389,
    "state": "open",
    "created_by": "xinghedyc",
    "created_at": "2024-04-10T03:20:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1389</URL>\n\n<TITLE>[Usage] multiple inference getting wrong results</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: When I run multiple inference for some images, the result of first image is good but the following results for other image are poor. I use the python script from https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/model_vqa.py\r\nHow can I fix this problem, thanks a lot!\r\nCommand:\r\n```\r\nimport argparse\r\nimport glob\r\nimport torch\r\nimport os\r\nimport json\r\nfrom tqdm import tqdm\r\nimport shortuuid\r\n\r\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\r\nfrom llava.conversation import conv_templates, SeparatorStyle\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.utils import disable_torch_init\r\nfrom llava.mm_utils import tokenizer_image_token, process_images, get_model_name_from_path\r\n\r\nfrom PIL import Image\r\nimport math\r\n\r\n\r\ndef split_list(lst, n):\r\n    \"\"\"Split a list into n (roughly) equal-sized chunks\"\"\"\r\n    chunk_size = math.ceil(len(lst) / n)  # integer division\r\n    return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]\r\n\r\n\r\ndef get_chunk(lst, n, k):\r\n    chunks = split_list(lst, n)\r\n    return chunks[k]\r\n\r\n\r\ndef eval_model(args):\r\n    # Model\r\n    disable_torch_init()\r\n    model_path = os.path.expanduser(args.model_path)\r\n    model_name = get_model_name_from_path(model_path)\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, args.model_base, model_name, cache_dir=\"/data/model_cache\")\r\n\r\n    # questions = [json.loads(q) for q in open(os.path.expanduser(args.question_file), \"r\")]\r\n    # questions = get_chunk(questions, args.num_chunks, args.chunk_idx)\r\n    # answers_file = os.path.expanduser(args.answers_file)\r\n    # os.makedirs(os.path.dirname(answers_file), exist_ok=True)\r\n    # ans_file = open(answers_file, \"w\")\r\n    prompt = \"描述一下图片内容\"\r\n    # for line in tqdm(questions):\r\n    #     idx = line[\"question_id\"]\r\n    #     image_file = line[\"image\"]\r\n    #     qs = line[\"text\"]\r\n    for idx, image_file in enumerate(os.listdir(args.image_folder)):\r\n        qs = \"描述一下图片内容\"\r\n        cur_prompt = qs\r\n        if model.config.mm_use_im_start_end:\r\n            qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + qs\r\n        else:\r\n            qs = DEFAULT_IMAGE_TOKEN + '\\n' + qs\r\n\r\n        conv = conv_templates[args.conv_mode].copy()\r\n        conv.append_message(conv.roles[0], qs)\r\n        conv.append_message(conv.roles[1], None)\r\n        prompt = conv.get_prompt()\r\n\r\n        input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\r\n\r\n        image = Image.open(os.path.join(args.image_folder, image_file)).convert('RGB')\r\n        image_tensor = process_images([image], image_processor, model.config)[0]\r\n\r\n        with torch.inference_mode():\r\n            output_ids = model.generate(\r\n                input_ids,\r\n                images=image_tensor.unsqueeze(0).half().cuda(),\r\n                image_sizes=[image.size],\r\n                do_sample=True if args.temperature > 0 else False,\r\n                temperature=args.temperature,\r\n                top_p=args.top_p,\r\n                num_beams=args.num_beams,\r\n                # no_repeat_ngram_size=3,\r\n                max_new_tokens=512,\r\n                use_cache=True)\r\n\r\n        outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\r\n        print(outputs)\r\n    #     ans_id = shortuuid.uuid()\r\n    #     ans_file.write(json.dumps({\"question_id\": idx,\r\n    #                                \"prompt\": cur_prompt,\r\n    #                                \"text\": outputs,\r\n    #                                \"answer_id\": ans_id,\r\n    #                                \"model_id\": model_name,\r\n    #                                \"metadata\": {}}) + \"\\n\")\r\n    #     ans_file.flush()\r\n    # ans_file.close()\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--model-path\", type=str, default=\"liuhaotian/llava-v1.6-34b\")\r\n    parser.add_argument(\"--model-base\", type=str, default=None)\r\n    parser.add_argument(\"--image-folder\", type=str, default=\"../Qwen/test_imgs/\")\r\n    parser.add_argument(\"--question-file\", type=str, default=\"tables/question.jsonl\")\r\n    parser.add_argument(\"--answers-file\", type=str, default=\"answer.jsonl\")\r\n    parser.add_argument(\"--conv-mode\", type=str, default=\"chatml_direct\")\r\n    parser.add_argument(\"--num-chunks\", type=int, default=1)\r\n    parser.add_argument(\"--chunk-idx\", type=int, default=0)\r\n    parser.add_argument(\"--temperature\", type=float, default=0.2)\r\n    parser.add_argument(\"--top_p\", type=float, default=0.7)\r\n    parser.add_argument(\"--num_beams\", type=int, default=1)\r\n    args = parser.parse_args()\r\n\r\n    eval_model(args)\r\n```\r\n\r\nLog: \r\n```\r\n# 第一个结果\r\n张图片显示的是一个室内空间，看起来像是一个地下停车场的入口。我们可以看到：\r\n\r\n1. 入口处的地面是斜坡，方便车辆进入地下。\r\n2. 入口处的左侧有灭火器箱，这是为了应对火灾的紧急情况。\r\n3. 入口处的右侧有指示牌，上面有箭头和文字，可能是指示车辆行驶方向或提供其他信息。\r\n4. 天花板上布满了各种管道和电缆，这是建筑物的内部设施，用于通风、照明、电力等。\r\n5. 入口处的左侧有蓝色的墙壁，上面有白色的条纹，可能是为了增加视觉效果或区分不同的区域。\r\n6. 入口处的右侧有白色的墙壁，上面有蓝色的条纹，与左侧的墙壁相呼应。\r\n7. 入口处的顶部有白色的管道，可能是用于通风或空调系统。\r\n8. 入口处的顶部有红色的管道，可能是用于消防系统。\r\n9. 入口处的顶部有白色的管道，可能是用于电力系统。\r\n10. 入口处的顶部有白色的管道，可能是用于通信系统。\r\n\r\n总的来说，这张图片展示了一个现代化的地下停车场入口，设计简洁，功能齐全。\r\n# 第二个结果\r\n这张图片看起来像是一个抽象的、色彩丰富的艺术作品。它由许多重复的形状和颜色组成，这些形状和颜色交织在一起，形成了一个复杂的、几乎无法分辨具体内容的图案。这些形状可能是几何图\r\n形，如三角形、矩形和圆形，它们以不同的颜色和大小出现，并被叠加在一起，创造出一种视觉上的深度和复杂性。\r\n\r\n由于这张图片的抽象性质，它可能没有具体的主题或意义，而是更多地关注于色彩、形状和构图的视觉效果。它可能是一种实验性的艺术作品，旨在挑战观众的感知和想象力，或者它可能是一种视\r\n觉游戏，要求观众去发现隐藏在其中的细节和模式。\r\n# 第三个结果\r\n这张图片看起来像是一个抽象的、重复的图案，由许多细小的、类似羽毛或波浪形状的元素组成。这些元素以不同的颜色和透明度排列，形成了一个复杂而密集的视觉效果。由于图片的模糊和重复\r\n，很难确定具体的细节或主题。它可能是一种艺术作品，旨在创造一种视觉上的错觉或幻觉。\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by MengSunS at 2024-04-16T23:53:02Z>\nsame here\n</Comment>\n<Comment by CongYep at 2024-08-14T12:11:52Z>\nSome question. I used the gradio web to infer. Only the first image get the right answer, the later images I uploaded all got wrong answers. Do you know how to solve this problem?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1388,
    "state": "open",
    "created_by": "Pansf2023",
    "created_at": "2024-04-09T14:30:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1388</URL>\n\n<TITLE>[Question] Does it support Chinese fine-tuning?</TITLE>\n\n<BODY>### Question\n\nDoes it support Chinese fine-tuning?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1387,
    "state": "open",
    "created_by": "YatingPan",
    "created_at": "2024-04-09T14:02:10Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1387</URL>\n\n<TITLE>[Question] LLaVa variants for longer input token length</TITLE>\n\n<BODY>### Question\n\nHi, are there any LLaVA variants that can take a relatively longer input token length, like 10k or 32k?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1386,
    "state": "open",
    "created_by": "UknowSth",
    "created_at": "2024-04-09T09:36:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1386</URL>\n\n<TITLE>[Usage]  Error in load liuhaotian/llava-v1.6-34b checkpoint.</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: When loading weights for llava-v1.6-34b, it says model parameter mismatch.\r\n\r\nCommand:\r\n```\r\n    model_path = \"liuhaotian/llava-v1.6-34b\"\r\n    with warnings.catch_warnings():\r\n        warnings.simplefilter(\"ignore\")  # Pytorch non-meta copying warning fills out the console\r\n        tokenizer, model, image_processor, context_len = load_pretrained_model(\r\n            model_path=model_path,\r\n            model_base=None,\r\n            model_name=get_model_name_from_path(model_path),\r\n        )\r\n```\r\n\r\nLog: \r\n```\r\nLoading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]\r\nTraceback (most recent call last):\r\n  File \"/mnt/petrelfs/wuxiaoxue/Llava/caption_llava.py\", line 359, in <module>\r\n    main(args)\r\n  File \"/mnt/petrelfs/wuxiaoxue/.conda/envs/panda/lib/python3.9/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/mnt/petrelfs/wuxiaoxue/Llava/caption_llava.py\", line 269, in main\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(\r\n  File \"/mnt/petrelfs/wuxiaoxue/Llava/llava/model/builder.py\", line 114, in load_pretrained_model\r\n    model = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\n  File \"/mnt/petrelfs/wuxiaoxue/.conda/envs/panda/lib/python3.9/site-packages/transformers/modeling_utils.py\", line 2795, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\n  File \"/mnt/petrelfs/wuxiaoxue/.conda/envs/panda/lib/python3.9/site-packages/transformers/modeling_utils.py\", line 3123, in _load_pretrained_model\r\n    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\r\n  File \"/mnt/petrelfs/wuxiaoxue/.conda/envs/panda/lib/python3.9/site-packages/transformers/modeling_utils.py\", line 698, in _load_state_dict_into_meta_model\r\n    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)\r\n  File \"/mnt/petrelfs/wuxiaoxue/.conda/envs/panda/lib/python3.9/site-packages/accelerate/utils/modeling.py\", line 348, in set_module_tensor_to_device\r\n    raise ValueError(\r\nValueError: Trying to set a tensor of shape torch.Size([1024, 7168]) in \"weight\" (which has shape torch.Size([7168, 7168])), this look incorrect.\r\n```\r\n\r\nI downloaded model weights from https://hf-mirror.com/liuhaotian/llava-v1.6-34b/tree/main, and it dosen't work. I wonder if the model weights is wrong or something wrong with code version (I use llava-1.2.0), or package version (I use python 3.9, torch 1.12.1+cu113, torchvision 0.13.1+cu113).</BODY>\n\n<COMMENTS>\n<Comment by Athicbliss at 2024-05-10T02:03:43Z>\nIs there a solution to this problem\n</Comment>\n<Comment by YangYangTx at 2024-05-14T07:22:05Z>\ncan anyone help?\n</Comment>\n<Comment by RichardSunnyMeng at 2024-07-08T13:21:16Z>\nI also met this problem when I try to use llama3-llava-next-8b\n</Comment>\n<Comment by RichardSunnyMeng at 2024-07-09T02:00:17Z>\nMaybe you  can check whether you load the right model. I solve it by it.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1385,
    "state": "open",
    "created_by": "samuruph",
    "created_at": "2024-04-09T08:41:57Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1385</URL>\n\n<TITLE>[Usagv.16-mistral-7b conv-mode and max_length</TITLE>\n\n<BODY>### Describe the issue\n\nHello, I am using liuhaotian/llava-v1.5-7b (4-bit version) and I am getting this warning: \r\n```bash\r\nToken indices sequence length is longer than the specified maximum sequence length for this model (4789 > 2048). Running this sequence through the model will result in indexing errors\r\n```\r\nI have computed some info from a perception model and I am trying to embed this information into text (I both tried to append this info to system message or directly to USER message). Anyway, both are really slow and I am not quite sure about the correct procedure.\r\nDo you have any suggestion on how to deal with this problem (maybe the best practice to embed this into the MLLM). I have 8GB memory so i am quite constraint.\r\nI think trying v1.6-mistral-7b with 8-bit qunatization would also work, maybe.\r\n\r\nMoreover, which conv-mode I should choose to properly use v1.6-mistral-7b locally?\r\nThank you so much!!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1384,
    "state": "open",
    "created_by": "Pansf2023",
    "created_at": "2024-04-09T07:54:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1384</URL>\n\n<TITLE>[Question] fine tuning error</TITLE>\n\n<BODY>### Question\n\nUse my own data set with a 3090, and use lora to fine-tune llava-v1.5-7b. After the fine-tuning is successful, use python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost: 10000 --port 40000 --worker http://localhost:40000 --model-path /mnt/LLaVA-main/checkpoints/llava-v1.5-7b-task-lora --model-base /mnt/vicuna- 7b-v1.5 --load-4bit, \r\nthe following error is reported when the command is started: \r\n2024-04-09 05:37:12 | ERROR | stderr | model.load_state_dict(non_lora_trainables, strict=False)\r\n2024-04-09 05:37:12 | ERROR | stderr | File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2152, in load_state_dict\r\n2024-04-09 05:37:12 | ERROR | stderr | raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\n2024-04-09 05:37:12 | ERROR | stderr | RuntimeError: Error(s) in loading state_dict for LlavaLlamaForCausalLM:\r\n2024-04-09 05:37:12 | ERROR | stderr | size mismatch for model.mm_projector.0.weight: copying a param with shape torch.Size([4096, 1024]) from checkpoint, the shape in current model is torch.Size([2097152, 1]).\r\n2024-04-09 05:37:12 | ERROR | stderr | size mismatch for model.mm_projector.2.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([8388608, 1]).</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1383,
    "state": "open",
    "created_by": "awzhgw",
    "created_at": "2024-04-09T01:34:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1383</URL>\n\n<TITLE>[Question] Great work! Is it possible to release the training data for the 1.6 version?</TITLE>\n\n<BODY>### Question\n\nGreat work! Is it possible to release the training data for the 1.6 version?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1380,
    "state": "open",
    "created_by": "GohioAC",
    "created_at": "2024-04-08T11:09:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1380</URL>\n\n<TITLE>[Question] find_all_linear_names function for LoRA does not behave as expected</TITLE>\n\n<BODY>### Question\n\nhttps://github.com/haotian-liu/LLaVA/blob/4e2277a060da264c4f21b364c867cc622c945874/llava/train/train.py#L169-L182\r\n\r\nSince the code splits the module names and adds only the last portion, the output for LLaVA-v1.6-34b is `['down_proj', 'gate_proj', 'v_proj', 'up_proj', 'o_proj', 'q_proj', 'k_proj']`. These attention layers are present in the CLIPTower, thus applying LoRA adapters to the vision_tower.\r\n\r\nThis is applicable when loading pre-trained weights for instruction tuning and does not seem like expected behaviour.\r\n\r\n@haotian-liu please clarify.</BODY>\n\n<COMMENTS>\n<Comment by srnangi at 2024-07-09T21:27:14Z>\nI was wondering about the same thing. Any update on this one?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1379,
    "state": "open",
    "created_by": "459737087",
    "created_at": "2024-04-08T09:16:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1379</URL>\n\n<TITLE>It's so strange, accelerate==0.21.0, It said cannot import name 'LlavaLlamaForCausalLM' from 'llava.model' when I change accelerate==0.28.0. It worked</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.controller --host 0.0.0.0 --port 10000\r\n```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/runpy.py\", line 187, in _run_module_as_main\r\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\r\n  File \"/usr/local/lib/python3.10/runpy.py\", line 110, in _get_module_details\r\n    __import__(pkg_name)\r\n  File \"/data/yanxin/LLaVA-main/llava/__init__.py\", line 1, in <module>\r\n    from .model import LlavaLlamaForCausalLM\r\nImportError: cannot import name 'LlavaLlamaForCausalLM' from 'llava.model'\r\n```\r\n\r\nwhen I use accelerate==0.28.0\r\nrun \r\n```\r\npython -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload --port 8080\r\n``` \r\nERROR | stderr | /usr/local/lib/python3.10/site-packages/gradio/components/dropdown.py:163: UserWarning: The value passed into gr.Dropdown() is not in the list of choices. Please update the list of choices to include:  or set allow_custom_value=True.\r\n\r\n\r\nand I can't curl the http</BODY>\n\n<COMMENTS>\n<Comment by nj159 at 2024-04-11T12:20:12Z>\nHave you solved this problem? How was it resolved?thanks\n</Comment>\n<Comment by 459737087 at 2024-04-12T07:01:27Z>\nNo, I'm waiting for somebody solving it.\r\n@nj159\n</Comment>\n<Comment by wentaoyuan at 2024-05-10T17:34:44Z>\nAlso have the same problem.\n</Comment>\n<Comment by MismatchQuest at 2024-05-19T12:28:57Z>\nI am having the same issue.\n</Comment>\n<Comment by Hameed92 at 2024-11-10T06:52:06Z>\nI had the same problem, it worked when I ran `pip install accelerate -U`\n</Comment>\n<Comment by xing0047 at 2025-02-27T02:46:56Z>\nsame issue.\n</Comment>\n<Comment by 1835969208 at 2025-03-02T09:11:50Z>\n卧槽 牛逼啊\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1378,
    "state": "closed",
    "created_by": "Lexarymade",
    "created_at": "2024-04-07T17:07:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1378</URL>\n\n<TITLE>[Usage] Cannot load llava-v1.6-13B</TITLE>\n\n<BODY>the model I download from the https://huggingface.co/liuhaotian/ works</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1377,
    "state": "closed",
    "created_by": "huilin66",
    "created_at": "2024-04-06T08:49:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1377</URL>\n\n<TITLE>[Question] how to use finetune weight to inference ?</TITLE>\n\n<BODY>### Question\r\n\r\nTo finetune llava-v1.5-7b, I followed scripts/v1_5/finetune_lora.sh and used the following instruction:\r\n\r\n`#!#!/bin/bash\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path ./ckpt/vicuna-7b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path /nfsv4/23039356r/data/defect_caption/llava_v1_5_vqa320.json \\\r\n    --image_folder /nfsv4/23039356r/data/defect_caption \\\r\n    --vision_tower ./ckpt/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter ./ckpt/llava-v1.5-7b/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n\r\n`\r\n\r\n\r\nIt seems that the training results of llava-v1.5-7b should contain \"pytorch_model-00001-of-00002.bin\" and \"pytorch_model-00001-of-00002.bin\", which are about 13.5G.\r\n\r\nBut my finetune results only contain \"non_lora_trainables.bin\", 42MB.\r\n\r\nCan anyone help me with this problem? Thanks very much!\r\n\r\n![c899c83d7d7d57d6d70489fc5989edc](https://github.com/haotian-liu/LLaVA/assets/143692339/5c1e0b22-663e-4fcc-a609-18b9841f5fe5)</BODY>\n\n<COMMENTS>\n<Comment by huilin66 at 2024-04-09T01:37:00Z>\nI use merge_lora_weights.py to merge the results, thanks.\n</Comment>\n<Comment by wuwu-C at 2024-04-20T12:57:41Z>\nHey! I have met some errors when I run merge_lora_weights.py.\r\n`OSError: /data/user4/cww/LLaVA/checkpoint/LLaVA.finetune/geochat-LLAVA15-7B-Vicuna-finetune_lora/checkpoint-2234 does not appear to have a file named config.json. Checkout 'https://huggingface.co//data/user4/cww/LLaVA/checkpoint/LLaVA.finetune/geochat-LLAVA15-7B-Vicuna-finetune_lora/checkpoint-2234/main' for available files.`\r\nMy lora finetuned checkpoint do not have config.json, but I saw you have this file.can you help me?\r\n![image](https://github.com/haotian-liu/LLaVA/assets/96038200/5eb59d9e-ce29-4939-bcb1-2b8516d3e927)\n</Comment>\n<Comment by ShawnAn-WHU at 2024-05-09T03:20:20Z>\n@wuwu-C Have you solved this problem? I used the finetune_lora.sh and get the same files as yours.\n</Comment>\n<Comment by keanshi-nlp at 2025-04-20T14:04:23Z>\nHi, the merge_lora script seems not load mm_projector's weight. It warns me that the mm_projector's weights is newly initialized when I tried to merge lora's weight to pretrained llava. Can you describe the detail of how to merge them? Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1375,
    "state": "open",
    "created_by": "awzhgw",
    "created_at": "2024-04-06T00:57:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1375</URL>\n\n<TITLE>[Question]  loss 不能下降</TITLE>\n\n<BODY>### Question\n\nWhen I fine-tune with Lora, the loss will drop to about 0.8 in the first 20k steps, and then increase to 6.2 over time, and then keep oscillating here. May I ask why this problem occurs? How to solve it? Translate this passage into English</BODY>\n\n<COMMENTS>\n<Comment by Sprinter1999 at 2024-04-19T02:46:31Z>\nHi friend, I guess you may consider switching a lower learning rate when conducting your SFT.\n</Comment>\n<Comment by OliverLeeXZ at 2024-04-29T05:07:19Z>\nSame question\n</Comment>\n<Comment by Vignesh-Valaboju at 2024-05-01T19:08:30Z>\nSame problem too! Curious to know if lower learning rate helped you\n</Comment>\n<Comment by OliverLeeXZ at 2024-05-02T16:57:09Z>\nI change learning rate from 2e-4 to 2e-5. It is really work!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1374,
    "state": "open",
    "created_by": "bug-fixed",
    "created_at": "2024-04-05T14:29:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1374</URL>\n\n<TITLE>[Question] any updates on the v1.6</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1373,
    "state": "open",
    "created_by": "St3p99",
    "created_at": "2024-04-05T09:42:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1373</URL>\n\n<TITLE>[Question] projector not saved in the finetuning stage</TITLE>\n\n<BODY>### Question\n\nUsing the following script for the FT stage both projector and LLM will be trained. However since tune_mm_mlp_adapter is set to False, projector weights ('mm_projector.bin') will not be saved. As you can see in [_save_checkpoint()](https://github.com/haotian-liu/LLaVA/blob/main/llava/train/llava_trainer.py#L230) - [LLaVATrainer](https://github.com/haotian-liu/LLaVA/blob/main/llava/train/llava_trainer.py).\r\n\r\n[https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_lora.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_lora.sh)</BODY>\n\n<COMMENTS>\n<Comment by lzn87 at 2024-10-14T18:41:53Z>\nHaving the same issue here :( Did you figure out how to fix it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1372,
    "state": "open",
    "created_by": "JaejinCho",
    "created_at": "2024-04-05T00:37:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1372</URL>\n\n<TITLE>[Question] What are the samples that include \"model\"-key and \"\"-value (i.e., empty string) pairs in the fine-tuning dataset llava_v1_5_mix665k.json?</TITLE>\n\n<BODY>### Question\n\nThank you for sharing this great work.\r\n\r\nWhat are the samples that include \"model\"-key and \"\"-value (i.e., empty string) pairs in the fine-tuning dataset json llava_v1_5_mix665k.json? For example,\r\n\r\n...\r\n...\r\n{\r\n  \"id\": \"Da9D8J4_0\",\r\n  \"model\": \"\",\r\n  ...\r\n  ...\r\n  ...\r\n  \r\n  I found ~40k samples around the end of the file.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1371,
    "state": "closed",
    "created_by": "SirLPS",
    "created_at": "2024-04-04T21:15:02Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1371</URL>\n\n<TITLE>[Question] the output does not follow the query instruction and is too short when running run_llava.py</TITLE>\n\n<BODY>### Question\r\n\r\nFinetuned the model with some local data and found the output is independent of the query description. Like when the query is empyt or not, the results is always the same. For example, given a picture of food, the output is no problem but always be like  'breakfast buffet at the hotel'. And another issue is the output can be different under multi testing\r\n\r\n`python3 run_llava.py --model-path models/mpt-pretrained-model --image-file test.jpg --query \"Describe this image and its style in a very detailed manner.\" `\r\n\r\n![screenshot-20240404-141419](https://github.com/haotian-liu/LLaVA/assets/23416925/c911629b-0689-4541-a1bd-eac39ab7d794)</BODY>\n\n<COMMENTS>\n<Comment by SirLPS at 2024-04-05T19:04:33Z>\nFixed. Not load the instruction tuning ckpt successfully. The final output will be like: 'The image features a white plate filled with a delicious breakfast meal. The plate contains a variety of food items, including eggs, bacon, sausage, and ham. There are also several slices of orange on the plate, adding a touch of color and freshness to the meal. In addition to the main plate, there are two bowls placed nearby, one on the left side and the other on the right side of the image. A cup can be seen on the left side of the plate, and a spoon is located near the top right corner of the image. The overall scene presents a well-rounded and appetizing breakfast meal.'\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1370,
    "state": "open",
    "created_by": "comfilcon",
    "created_at": "2024-04-04T19:31:14Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1370</URL>\n\n<TITLE>[Question] Training Data</TITLE>\n\n<BODY>### Question\n\nGreat work! Is it possible to release the training data for the 1.6 version?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1369,
    "state": "open",
    "created_by": "Ahnhojin1223",
    "created_at": "2024-04-04T13:20:57Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1369</URL>\n\n<TITLE>[Question]  Error during this command: pip install flash-attn --no-build-isolation</TITLE>\n\n<BODY>### Question\r\n\r\nCommand:\r\n`pip install flash-attn --no-build-isolation`\r\n\r\nLog:\r\n\r\n```\r\nCollecting flash-attn\r\n  Downloading flash_attn-2.5.6.tar.gz (2.5 MB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/2.5 MB 11.4 MB/s eta 0:00:00\r\n  Preparing metadata (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  × python setup.py egg_info did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> [12 lines of output]\r\n      fatal: not a git repository (or any of the parent directories): .git\r\n      Traceback (most recent call last):\r\n        File \"<string>\", line 2, in <module>\r\n        File \"<pip-setuptools-caller>\", line 34, in <module>\r\n        File \"/tmp/pip-install-9u5e9dng/flash-attn_e362cbbd46404df8a4978593d8bb899c/setup.py\", line 114, in <module>\r\n          raise RuntimeError(\r\n      RuntimeError: FlashAttention is only supported on CUDA 11.6 and above.  Note: make sure nvcc has a supported version by running nvcc -V.\r\n      \r\n      \r\n      torch.__version__  = 2.1.2+cu121\r\n      \r\n      \r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n× Encountered error while generating package metadata.\r\n╰─> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details.\r\n```\r\n\r\nEnviroment:\r\n`Ubuntu 20.04`\r\n\r\nMy Cuda version is:\r\n`Cuda compilation tools, release 10.1, V10.1.243`\r\n\r\n**How can I do to install flash-attn????**\r\n\r\n![1](https://github.com/haotian-liu/LLaVA/assets/63521098/acd80c26-f04f-4bf4-922f-0b374ea3a1c0)\r\n\r\n![2](https://github.com/haotian-liu/LLaVA/assets/63521098/5abf0060-f7a5-4858-a745-547fedc9586f)\r\n\r\n![3](https://github.com/haotian-liu/LLaVA/assets/63521098/b5490987-5170-4933-96c0-0cb9199195d0)</BODY>\n\n<COMMENTS>\n<Comment by iFe1er at 2024-04-10T17:20:53Z>\nsame question\n</Comment>\n<Comment by FuZening at 2024-04-17T08:53:23Z>\nSame question! If you find any solution, please tell me! Thank you very much in advance!\n</Comment>\n<Comment by JonathonYan1993 at 2024-04-29T06:22:29Z>\ntry to install the version of flash-attention compiled with torch which you can download from https://github.com/Dao-AILab/flash-attention/releases/tag/v2.5.8  will be useful\n</Comment>\n<Comment by zs-zhong at 2024-05-06T15:31:30Z>\n`pip install flash-attn==2.5.5 --no-build-isolation`\n</Comment>\n<Comment by 2416602906 at 2024-11-11T04:18:36Z>\n同样的问题！请问您解决了吗？希望能得到最新的消息，万分感谢！\n</Comment>\n<Comment by lwwdww at 2025-04-18T08:11:13Z>\nsame question\n</Comment>\n<Comment by riderexin at 2025-07-26T11:46:13Z>\nSame problem, solved by downgrading to 2.7.3 with `pip install flash-attn==2.7.3 --no-build-isolation`\n\nhttps://github.com/Dao-AILab/flash-attention/issues/1503#issuecomment-2676484761\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1368,
    "state": "open",
    "created_by": "aagrawal1-chwy",
    "created_at": "2024-04-04T13:14:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1368</URL>\n\n<TITLE>[Question] Discrepancy between demo and cli/local responses/generated text</TITLE>\n\n<BODY>### Question\n\nI am trying to identify pet breeds using the Oxford-IIIT pet dataset. I am noticing in many cases demo identifies the breed correctly but the cli or local implementation doesn't. I use the same prompt and 1.6-34b model for both. Any idea what could be happening ?\r\n\r\nInput: Image of a Russian Blue cat\r\nPrompt: What's the breed of the cat?\r\n\r\nCLI:  The cat in the image appears to be a Sphynx cat, which is a breed known for its lack of a fur coat. Sphynx cats are known for their distinctive appearance, with a muscular body, large ears, and expressive eyes. They are also known for their friendly and social nature.\r\n\r\nDemo: The cat in the image appears to be a Russian Blue, which is a breed known for its short, dense, and silvery-blue coat, and its striking green eyes. Russian Blues are medium-sized cats with a muscular build and a wedge-shaped head.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1366,
    "state": "open",
    "created_by": "Get-David",
    "created_at": "2024-04-03T09:17:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1366</URL>\n\n<TITLE>[Question] ImportError:transformers.models.llama.modeling_llama</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nThis is my error message:\r\n```python\r\n(llava) zdw@ai-gpu-server149:~/LLaVA$ python test.py\r\nTraceback (most recent call last):\r\n  File \"/data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1364, in _get_module\r\n    return importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"/data/share8/zdw/miniconda3/envs/llava/lib/python3.10/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 55, in <module>\r\n    from flash_attn import flash_attn_func, flash_attn_varlen_func\r\n  File \"/data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages/flash_attn/__init__.py\", line 3, in <module>\r\n    from flash_attn.flash_attn_interface import (\r\n  File \"/data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py\", line 10, in <module>\r\n    import flash_attn_2_cuda as flash_attn_cuda\r\nImportError: /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages/flash_attn_2_cuda.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN2at4_ops15sum_IntList_out4callERKNS_6TensorEN3c1016OptionalArrayRefIlEEbSt8optionalINS5_10ScalarTypeEERS2_\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/zdw/LLaVA/test.py\", line 1, in <module>\r\n    from llava.model.builder import load_pretrained_model\r\n  File \"/home/zdw/LLaVA/llava/__init__.py\", line 1, in <module>\r\n    from .model import LlavaLlamaForCausalLM\r\n  File \"/home/zdw/LLaVA/llava/model/__init__.py\", line 2, in <module>\r\n    from .language_model.llava_llama import LlavaLlamaForCausalLM, LlavaConfig\r\n  File \"/home/zdw/LLaVA/llava/model/language_model/llava_llama.py\", line 21, in <module>\r\n    from transformers import AutoConfig, AutoModelForCausalLM, \\\r\n  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\r\n  File \"/data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1355, in __getattr__\r\n    value = getattr(module, name)\r\n  File \"/data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1354, in __getattr__\r\n    module = self._get_module(self._class_to_module[name])\r\n  File \"/data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1366, in _get_module\r\n    raise RuntimeError(\r\nRuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\r\n/data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages/flash_attn_2_cuda.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN2at4_ops15sum_IntList_out4callERKNS_6TensorEN3c1016OptionalArrayRefIlEEbSt8optionalINS5_10ScalarTypeEERS2_\r\n(llava) zdw@ai-gpu-server149:~/LLaVA$ \r\n```\r\nI tried installing but without any problems:\r\n```python\r\n(llava) zdw@ai-gpu-server149:~/LLaVA$ pip install flash-attn --no-build-isolation --no-cache-dir\r\nLooking in indexes: http://registry.xtrfr.cn/repository/pypi-test/simple/\r\nRequirement already satisfied: flash-attn in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (2.5.6)\r\nRequirement already satisfied: torch in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from flash-attn) (2.1.2)\r\nRequirement already satisfied: einops in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from flash-attn) (0.6.1)\r\nRequirement already satisfied: packaging in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from flash-attn) (24.0)\r\nRequirement already satisfied: ninja in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from flash-attn) (1.11.1.1)\r\nRequirement already satisfied: filelock in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from torch->flash-attn) (3.13.3)\r\nRequirement already satisfied: typing-extensions in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from torch->flash-attn) (4.10.0)\r\nRequirement already satisfied: sympy in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from torch->flash-attn) (1.12)\r\nRequirement already satisfied: networkx in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from torch->flash-attn) (3.2.1)\r\nRequirement already satisfied: jinja2 in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from torch->flash-attn) (3.1.3)\r\nRequirement already satisfied: fsspec in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from torch->flash-attn) (2024.3.1)\r\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)\r\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)\r\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)\r\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from torch->flash-attn) (8.9.2.26)\r\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from torch->flash-attn) (12.1.3.1)\r\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from torch->flash-attn) (11.0.2.54)\r\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from torch->flash-attn) (10.3.2.106)\r\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from torch->flash-attn) (11.4.5.107)\r\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from torch->flash-attn) (12.1.0.106)\r\nRequirement already satisfied: nvidia-nccl-cu12==2.18.1 in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from torch->flash-attn) (2.18.1)\r\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)\r\nRequirement already satisfied: triton==2.1.0 in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from torch->flash-attn) (2.1.0)\r\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.4.99)\r\nRequirement already satisfied: MarkupSafe>=2.0 in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.5)\r\nRequirement already satisfied: mpmath>=0.19 in /data/share8/zdw/miniconda3/envs/llava/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\r\n(llava) zdw@ai-gpu-server149:~/LLaVA$ \r\n```\r\ntest.py The content is as follows：\r\n```python\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.mm_utils import get_model_name_from_path\r\nfrom llava.eval.run_llava import eval_model\r\n\r\nmodel_path = \"/home/zdw/Open-Sora/pre_training/llava-v1.6-34b\"\r\n\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    model_path=model_path,\r\n    model_base=None,\r\n    model_name=get_model_name_from_path(model_path)\r\n)\r\n\r\nprompt = \"What are the things I should be cautious about when I visit here?\"\r\nimage_file = \"https://llava-vl.github.io/static/images/view.jpg\"\r\n\r\nargs = type('Args', (), {\r\n    \"model_path\": model_path,\r\n    \"model_base\": None,\r\n    \"model_name\": get_model_name_from_path(model_path),\r\n    \"query\": prompt,\r\n    \"conv_mode\": None,\r\n    \"image_file\": image_file,\r\n    \"sep\": \",\",\r\n    \"temperature\": 0,\r\n    \"top_p\": None,\r\n    \"num_beams\": 1,\r\n    \"max_new_tokens\": 512\r\n})()\r\n\r\neval_model(args)\r\n```</BODY>\n\n<COMMENTS>\n<Comment by Joe-J at 2024-04-08T07:02:05Z>\nI have the same problem\n</Comment>\n<Comment by KatameRonin at 2024-04-18T00:41:55Z>\nI have a similar issue. \r\nMy error looks like this:\r\n`RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\r\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\r\ncannot import name 'OutOfResources' from partially initialized module 'triton.runtime.autotuner' (most likely due to a circular import)`\r\n\r\nI tried reinstalling several times too. Does anyone have a solution?\n</Comment>\n<Comment by eware-godaddy at 2024-04-19T03:33:42Z>\nI solved this by:\r\n\r\n```\r\npip uninstall flash-attn\r\npip install flash-attn --no-build-isolation --no-cache-dir\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1364,
    "state": "open",
    "created_by": "uniquehou",
    "created_at": "2024-04-02T12:58:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1364</URL>\n\n<TITLE>[Discussion] More data leading to lower indicators?</TITLE>\n\n<BODY>### Discussion\n\nHi, I tried LLaVA-1.5(13B), training an image classification task, with good results, but now I'm running into a problem.\r\nBefore I used 250k data (finetune lora), the precision went up with more training data. Later, we added 200k data and the indicator dropped by 6-7% instead.\r\nWe are now basically sure that the new data distribution is the same as before, so what else could be the reason for the decline?</BODY>\n\n<COMMENTS>\n<Comment by awzhgw at 2024-04-06T00:58:34Z>\nsame too.  how to resolve it ?\n</Comment>\n<Comment by awzhgw at 2024-04-06T01:10:34Z>\n@uniquehou  how to resolve it ?  i meet same problem\n</Comment>\n<Comment by uniquehou at 2024-04-07T10:45:01Z>\n> @uniquehou how to resolve it ? i meet same problem\r\n\r\nNot yet. We're checking for overfitting.\n</Comment>\n<Comment by AnnaGao0827 at 2024-10-22T13:18:35Z>\nHi, i am dealing with finetuning overfit. I wonder if your 250k data follows the standard fine-tuning data format? And do you change your loss to classification loss,or keep the language modeling loss? Thanks a lot!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1363,
    "state": "open",
    "created_by": "clownrat6",
    "created_at": "2024-04-02T08:00:29Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1363</URL>\n\n<TITLE>🐛 [BUG] llava-v1.6-mistral-7b fail to generate right response via 'mistral_instruct' template</TITLE>\n\n<BODY># Description\r\n\r\nI write a inference script like this:\r\n```python\r\nimport torch\r\nfrom PIL import Image\r\n\r\nimport sys\r\nsys.path.append('./')\r\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\r\nfrom llava.conversation import conv_templates, SeparatorStyle\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.utils import disable_torch_init\r\nfrom llava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\r\n\r\n\r\ndef main():\r\n    disable_torch_init()\r\n    image = 'llava/serve/examples/extreme_ironing.jpg'\r\n    inp = 'What is unusual about this image?'\r\n    model_path = 'liuhaotian/llava-v1.6-mistral-7b'\r\n    conv_mode = sys.argv[1]\r\n\r\n    model_name = get_model_name_from_path(model_path)\r\n    tokenizer, model, processor, _ = load_pretrained_model(model_path, None, model_name)\r\n    conv = conv_templates[conv_mode].copy()\r\n    roles = conv.roles\r\n\r\n    image = Image.open(image)\r\n    image_tensor = processor.preprocess(image, return_tensors='pt')['pixel_values']\r\n    if type(image_tensor) is list:\r\n        tensor = [image.to(model.device, dtype=torch.float16) for image in image_tensor]\r\n    else:\r\n        tensor = image_tensor.to(model.device, dtype=torch.float16)\r\n\r\n    print(f\"{roles[0]}: {inp}\")\r\n    inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\r\n    conv.append_message(conv.roles[0], inp)\r\n    conv.append_message(conv.roles[1], None)\r\n    prompt = conv.get_prompt()\r\n\r\n    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\r\n    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\r\n    keywords = [stop_str]\r\n    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\r\n\r\n    with torch.inference_mode():\r\n        output_ids = model.generate(\r\n            input_ids,\r\n            images=tensor,\r\n            do_sample=True,\r\n            temperature=0.2,\r\n            max_new_tokens=1024,\r\n            use_cache=True,\r\n            stopping_criteria=[stopping_criteria])\r\n\r\n    outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()\r\n    print(outputs)\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nIf I conduct this command `python inference.py mistral_instruct`, this code will generate empty output.\r\nIf I conduct this command `python inference.py llava_v1`, this code will generate normal output:\r\n```\r\ncity setting with traffic. It is also not typical to see someone standing on the back of a vehicle, as it can be dangerous and is generally not allowed. The man's actions are likely intended to be humorous or to draw attention to a specific cause or event. </s>\r\n```</BODY>\n\n<COMMENTS>\n<Comment by paralym at 2024-04-14T15:18:41Z>\nI tried to finetune llava-v1.6-mistral-7b with `mistral_instruct` template, but the output was not in the expected format. Have you figured out what template llava-v1.6-mistral-7b uses?\n</Comment>\n<Comment by cooleel at 2024-05-14T15:09:10Z>\n> I tried to finetune llava-v1.6-mistral-7b with `mistral_instruct` template, but the output was not in the expected format. Have you figured out what template llava-v1.6-mistral-7b uses?\r\n\r\nDid you solve it?  what version did you use in pretraining and finetuning btw?\n</Comment>\n<Comment by paralym at 2024-05-15T14:45:31Z>\n> > I tried to finetune llava-v1.6-mistral-7b with `mistral_instruct` template, but the output was not in the expected format. Have you figured out what template llava-v1.6-mistral-7b uses?\r\n> \r\n> Did you solve it? what version did you use in pretraining and finetuning btw?\r\n\r\nThe codebase does not support llava 1.6 training and I didn't solve it, but I'm going to work on this in the coming days. I use the latest code in finetuning llava-v1.6-mistral-7b\n</Comment>\n<Comment by chanangad at 2024-06-06T08:52:04Z>\nI think llava-v1.6-mistral-7b model uses llava_llama_2 conversation template. You can try it out!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1362,
    "state": "open",
    "created_by": "Lin-Tianwei",
    "created_at": "2024-04-02T07:50:09Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1362</URL>\n\n<TITLE>[Question] Cannot reproduce lora-mme</TITLE>\n\n<BODY>### Question\n\nFor the **llava1.5-7b** model, using the lora weight on hugging face, my score on mme is only **1344.9**, and the result of my own training score is **1372.7**, which is far lower than the reported score **1510.7**. Can someone explain the situation? Thanks</BODY>\n\n<COMMENTS>\n<Comment by OliverLeeXZ at 2024-04-28T06:08:59Z>\nUsing lora finetuing on llava_mix_665k?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1361,
    "state": "open",
    "created_by": "zousss",
    "created_at": "2024-04-02T06:44:30Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1361</URL>\n\n<TITLE>[Question] Finetune with 2 T4, error Expected is_sm80 || is_sm90 to be true, but got false.</TITLE>\n\n<BODY>### Question\n\nTraceback (most recent call last):\r\n  File \"/kaggle/working/LLaVA/llava/train/train_xformers.py\", line 13, in <module>\r\n    train()\r\n  File \"/kaggle/working/LLaVA/llava/train/train.py\", line 969, in train\r\n    trainer.train()\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1869, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2781, in training_step\r\n    self.accelerator.backward(loss)\r\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1847, in backward\r\n    self.deepspeed_engine_wrapped.backward(loss, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/deepspeed.py\", line 167, in backward\r\n    self.engine.backward(loss, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1955, in backward\r\n    self.optimizer.backward(loss, retain_graph=retain_graph)\r\n  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 2019, in backward\r\n    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\r\n  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 63, in backward\r\n    scaled_loss.backward(retain_graph=retain_graph)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 492, in backward\r\n    torch.autograd.backward(\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 251, in backward\r\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py\", line 288, in apply\r\n    return user_fn(self, *args)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 288, in backward\r\n    torch.autograd.backward(outputs_with_grad, args_with_grad)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 251, in backward\r\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\nRuntimeError: Expected is_sm80 || is_sm90 to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)\r\n\r\ntorch==2.1.2 torchvision==0.16.2\r\n\r\nwhat should i do to solve this problem?</BODY>\n\n<COMMENTS>\n<Comment by Ben81828 at 2024-04-09T02:18:24Z>\nHello, I've encountered the same issue as well. Have you managed to solve it?\n</Comment>\n<Comment by zousss at 2024-04-09T02:43:51Z>\nNot yet solved\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1360,
    "state": "open",
    "created_by": "coder4nlp",
    "created_at": "2024-04-02T02:52:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1360</URL>\n\n<TITLE>[Question] When will LLaVA-NeXT be open sourced?</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by awzhgw at 2024-04-06T00:59:36Z>\nsame too request\n</Comment>\n<Comment by hunterheiden at 2024-05-14T16:45:42Z>\nWould be nice to hear about this training code, if only to hear about whether we can expect to see it in the next few months or father out than that\n</Comment>\n<Comment by AmazDeng at 2024-05-22T07:07:02Z>\n+1\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1359,
    "state": "closed",
    "created_by": "aagrawal1-chwy",
    "created_at": "2024-04-01T15:39:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1359</URL>\n\n<TITLE>[Question] Demo page not working. Is server down ?</TITLE>\n\n<BODY>### Question\n\nHi - noticed the demo page is down. Just wanted to flag it.</BODY>\n\n<COMMENTS>\n<Comment by ddemillard at 2024-04-01T18:50:26Z>\nIt is down for me too\n</Comment>\n<Comment by AlephZr at 2024-04-02T02:49:16Z>\nMe too, can't use the demo so far.\n</Comment>\n<Comment by haotian-liu at 2024-04-02T16:06:41Z>\nIt's up running now. Thanks for reporting.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1357,
    "state": "open",
    "created_by": "Dinghaoxuan",
    "created_at": "2024-04-01T08:32:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1357</URL>\n\n<TITLE>[Question] How to use in-context learning in LLaVA?</TITLE>\n\n<BODY>### Question\n\nHello, I want to input some in-context examples to LLaVA. But I can not find any guidance about how to insert images in input prompt. Could you give me some templates about multi-image input prompt? Thank you very much.</BODY>\n\n<COMMENTS>\n<Comment by iamjudit at 2024-04-05T12:03:58Z>\nI'm also trying to send multiple images for a few-shot request via the pipeline. Thanks in advance\n</Comment>\n<Comment by Deep1994 at 2024-04-07T07:28:44Z>\n> I'm also trying to send multiple images for a few-shot request via the pipeline. Thanks in advance\r\n\r\nhi, I want to know if you guys have found a solution?\n</Comment>\n<Comment by Dinghaoxuan at 2024-04-18T05:37:14Z>\n> > I'm also trying to send multiple images for a few-shot request via the pipeline. Thanks in advance\r\n> \r\n> hi, I want to know if you guys have found a solution?\r\n\r\nI attempt to enclose each question and answer with symbols <s> your sentences </s>, but the in-context learning ability of LLaVA is poor. The in-context answers disturb the answer for query question.\n</Comment>\n<Comment by Dinghaoxuan at 2024-04-18T05:37:33Z>\n> I'm also trying to send multiple images for a few-shot request via the pipeline. Thanks in advance\r\n\r\nI attempt to enclose each question and answer with symbols <s> your sentences </s>, but the in-context learning ability of LLaVA is poor. The in-context answers disturb the answer for query question.\n</Comment>\n<Comment by ajaymin28 at 2024-06-18T19:06:26Z>\n> ### Question\r\n> Hello, I want to input some in-context examples to LLaVA. But I can not find any guidance about how to insert images in input prompt. Could you give me some templates about multi-image input prompt? Thank you very much.\r\n\r\nOk, so I have got it done by making changes like this, its limited to two images, and maybe I'm not sure how many images can be added to this.\r\n\r\n\r\n```\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\r\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX\r\n\r\nfrom llava.conversation import conv_templates, SeparatorStyle\r\n\r\nfrom PIL import Image\r\nimport requests\r\nimport copy\r\nimport torch\r\n\r\n\r\npretrained = \"lmms-lab/llama3-llava-next-8b\"\r\nmodel_name = \"llava_llama_3\"\r\ndevice = \"cuda\"\r\ndevice_map = \"auto\"\r\ntokenizer, model, image_processor, max_length = load_pretrained_model(pretrained, None, model_name, device_map=device_map) # Add any other thing you want to pass in llava_model_args\r\n\r\nmodel.eval()\r\nmodel.tie_weights()\r\n\r\nurl = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\r\nimage = Image.open(requests.get(url, stream=True).raw)\r\n\r\nimage_tensor = process_images([image], image_processor, model.config)\r\nimage_tensor_list = [_image.to(dtype=torch.float16, device=device) for _image in image_tensor] # Jai: replace image_tensor by image_tensor_list that will be used to append second image\r\n\r\n## Jai: add second image [\"digitally altered image of a person standing in the water\"]\r\nimage2 = Image.open(\"./LLaVA-NeXT/inputs/9f776e16-0d07-40d7-b2fd-45e23267f79b.jpg\")\r\nimage_tensor2 = process_images([image2], image_processor, model.config)\r\nfor _image in image_tensor2:\r\n    image_tensor_list.append(_image.to(dtype=torch.float16, device=device))\r\n\r\n\r\nInstruction_COT = \"\"\"There are two different images provided as an input, describe each of them independently\"\"\" \r\nconv_template = \"llava_llama_3\" # Make sure you use correct chat template for different models\r\n\r\n \r\nquestion = DEFAULT_IMAGE_TOKEN + DEFAULT_IMAGE_TOKEN + f\"\\n{Instruction_COT}\\n\" # Jai: add second Image token in the question. By default there is only one.\r\nconv = copy.deepcopy(conv_templates[conv_template]) \r\nconv.append_message(conv.roles[0], question)\r\nconv.append_message(conv.roles[1], None)\r\nprompt_question = conv.get_prompt()\r\n\r\ninput_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0) \r\nimage_sizes = [image.size, image2.size] # Jai: add second image size here.\r\n\r\n\r\ncont = model.generate(\r\n    input_ids,\r\n    images=image_tensor_list,\r\n    image_sizes=image_sizes,\r\n    do_sample=False,\r\n    temperature=0,\r\n    max_new_tokens=2024,\r\n)\r\ntext_outputs = tokenizer.batch_decode(cont, skip_special_tokens=True)\r\nprint(text_outputs)\r\n```\r\n\r\n\r\n\r\nHere's the output,\r\n\r\n\r\n> '\\nThe image on the left appears to be a radar chart, also known as a spider chart or a web chart. This type of chart is used to display multivariate data in the form of a two-dimensional chart of three or more quantitative variables represented on axes starting from the same point. Each axis represents a different variable, and the values are plotted along each axis and connected to form a polygon.\\n\\nThe radar chart in the image is labeled with various acronyms such as \"MM-Vet,\" \"LLaVA-Bench,\" \"SEED-Bench,\" \"MMBench-CN,\" \"MMBench,\" \"TextVQA,\" \"POPE,\" \"BLIP-2,\" \"InstructionBLIP,\" \"Owen-VL-Chat,\" and \"LLaVA-1.5.\" These labels likely represent different benchmarks or models used in a particular context, possibly in the field of natural language processing or a related area of artificial intelligence.\\n\\nThe radar chart is color-coded, with different colors representing different models or benchmarks. The chart is overlaid with a blue background that seems to be a stylized representation of water, giving the impression that the radar chart is underwater.\\n\\nThe image on the right shows a person standing in what appears to be a body of water, possibly a pool or a shallow sea, given the clear visibility and the presence of bubbles. The person is wearing a black shirt and dark pants, and they are looking directly at the camera with a neutral expression. The water around them is a bright blue, and there are bubbles visible, suggesting that the water is clear and possibly that the person is underwater. The image is a photograph and has a realistic style.'\n</Comment>\n<Comment by ys-zong at 2024-11-29T22:56:30Z>\nI implemented ICL inference with LLaVA and many other models in this repo: https://github.com/ys-zong/VL-ICL. You might find it useful.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1356,
    "state": "open",
    "created_by": "xffzzt",
    "created_at": "2024-04-01T07:05:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1356</URL>\n\n<TITLE>[Question] google.protobuf.message.DecodeError: Error parsing message with type 'sentencepiece.ModelProto'    May I ask how to solve this problem?</TITLE>\n\n<BODY>### Question\n\ngoogle.protobuf.message.DecodeError: Error parsing message with type 'sentencepiece.ModelProto'\r\nTo add, the version of my protobuf is 3.20.3\r\n![出现的问题2](https://github.com/haotian-liu/LLaVA/assets/65904481/ba75a941-eb41-4e1e-8f03-f43590ff8476)</BODY>\n\n<COMMENTS>\n<Comment by xjiaf at 2024-07-29T12:27:26Z>\nsame\n</Comment>\n<Comment by Veeraja-Veeraesh at 2025-03-09T05:28:25Z>\nAny update or solution??\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1355,
    "state": "closed",
    "created_by": "xffzzt",
    "created_at": "2024-04-01T07:03:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1355</URL>\n\n<TITLE>[Question] google.protobuf.message.DecodeError: Error parsing message with type 'sentencepiece.ModelProto'    May I ask how to solve this problem?</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1354,
    "state": "closed",
    "created_by": "20191864218",
    "created_at": "2024-04-01T02:39:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1354</URL>\n\n<TITLE>[Question] Some weights of LlavaTaiyiForCausalLM were not initialized from the model checkpoint，When I use MLP after pretrain for inference, I get an error</TITLE>\n\n<BODY>### Question\n\n`Some weights of LlavaTaiyiForCausalLM were not initialized from the model checkpoint at /root/autodl-tmp/Taiyi and are newly initialized: ['transformer.mm_projector.0.bias', 'transformer.mm_projector.0.weight', 'transformer.mm_projector.2.bias', 'transformer.mm_projector.2.weight']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/root/miniconda3/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/root/LLaVA/llava/serve/cli.py\", line 137, in <module>\r\n    main(args)\r\n  File \"/root/LLaVA/llava/serve/cli.py\", line 34, in main\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n  File \"/root/LLaVA/llava/model/builder.py\", line 124, in load_pretrained_model\r\n    model.load_state_dict(mm_projector_weights, strict=False)\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2152, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for LlavaTaiyiForCausalLM:\r\n        size mismatch for transformer.mm_projector.0.weight: copying a param with shape torch.Size([4096, 512]) from checkpoint, the shape in current model is torch.Size([1048576, 1]).\r\n        size mismatch for transformer.mm_projector.2.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([8388608, 1]).`\r\n        \r\nI would be grateful if someone could help me</BODY>\n\n<COMMENTS>\n<Comment by haozhang1234 at 2025-02-17T09:16:41Z>\nI also have this problem. How did you solve it？If I could receive your help, I would be extremely grateful！\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1353,
    "state": "open",
    "created_by": "Nyandwi",
    "created_at": "2024-03-31T21:50:50Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1353</URL>\n\n<TITLE>[Question] LLaVA-1.6 Training Data</TITLE>\n\n<BODY>### Question\n\nHello, thanks for the awesome works you have been doing in making MLLMs better and accessible to the community. Is there a plan to release the training data(and code) for LLaVA-1.6? We are eagerly waiting for the data :-)</BODY>\n\n<COMMENTS>\n<Comment by AhmedMasryKU at 2024-07-07T03:46:10Z>\nAny updates on this?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1352,
    "state": "open",
    "created_by": "jinluyang",
    "created_at": "2024-03-29T11:41:56Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1352</URL>\n\n<TITLE>[Question] How to load an old model using newer version of  transformers where llava is integrated?</TITLE>\n\n<BODY>### Question\n\nI am deploying a checkpoint model obviously trained for transformers==4.31.0 and I'm deploying it in an environment of transformers==4.36.1. (tensorrt_llm expects a newer version)  In the new version of transformers, they have integrated the llava class, and modified its expected state_dict key names. So when I use\r\n`from transformers import LlavaForConditionalGeneration\r\nLlavaForConditionalGeneration.from_pretrained(...)\r\n`\r\nit gives warning about uninitialized parameters:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/15670327/242f2c51-888a-41dc-84a7-60fcabb25bb7)\r\nIt seems in the older version, the keys are: model.layers.8.self_attn.k_proj.weight\r\nand in newer transformers version, the expected keys are: language_model.model.layers.8.self_attn.k_proj.weight\r\n\r\nSo that my model cannot be loaded correctly.\r\n\r\nIs there any way to load the model correctly? Do I have to modify the file llava-v1.5-13b/pytorch_model.bin.index.json ?</BODY>\n\n<COMMENTS>\n<Comment by jinluyang at 2024-04-10T07:19:08Z>\nseems tensorrtllm just support the huggingface version of llava, and I am using that version of model instead.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1350,
    "state": "open",
    "created_by": "ydygxu123",
    "created_at": "2024-03-29T01:30:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1350</URL>\n\n<TITLE>[Usage] Problems that arise when using cli.py/llava.serve.model_worker for inference</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nWhen I tried to use cli.py or llava.serve.model_worker locally to perform inference on lava-v1.5-7b, I was prompted that the checkpoint could not be loaded, but in fact both files existed.\r\nCommand:\r\n```\r\npython -m llava.serve.cli \\\r\n    --model-path ./llava-v1.5-7b \\\r\n    --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n    --load-4bit\r\n\r\nor\r\n\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./llava-v1.5-7b\r\n```\r\n\r\nLog: \r\n```\r\nUnicodeDecodeError: 'utf-8' codec can‘t decode byte 0x80 in position 256: invalid start byte\r\n```\r\n\r\nScreenshots:\r\n![img_v3_029e_f589e834-7bdc-4cb5-9b17-6a6dfce9d27g](https://github.com/haotian-liu/LLaVA/assets/56526929/e4e90447-ed69-4327-abe8-58871b83f96f)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1349,
    "state": "open",
    "created_by": "Yingyue-L",
    "created_at": "2024-03-28T09:03:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1349</URL>\n\n<TITLE>MME Results[Question]</TITLE>\n\n<BODY>### Question\n\nCan you provide the detailed MME results of the `llava-v1.5-7b` or the `jsonl` answer just as the llava-v1.5-13b?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1348,
    "state": "open",
    "created_by": "Bobby-youngking",
    "created_at": "2024-03-28T07:21:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1348</URL>\n\n<TITLE>about the model_name_or_path</TITLE>\n\n<BODY>### Question\n\nwhen i'm finetuning the model, the finetune_lora.sh script shows \"model_name_or_path\" and \"pretrain_mm_mlp_adapter\"， what is the difference?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1346,
    "state": "closed",
    "created_by": "baoshenghe0321",
    "created_at": "2024-03-27T18:51:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1346</URL>\n\n<TITLE>[BUG] is unpad_image a bug?</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nreading through the code, for the function https://github.com/haotian-liu/LLaVA/blob/4e2277a060da264c4f21b364c867cc622c945874/llava/model/llava_arch.py#L100C5-L100C16\r\nI think you might want to drop the patches within the padding area; however, the function checks the padding area in pixel unit, instead of the patch. Since later image_feature is partitioned in patches, so I think the function could drop more than needed.\r\n\r\nIs this expected or it is a bug?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1345,
    "state": "open",
    "created_by": "NimrodShabtay",
    "created_at": "2024-03-27T16:22:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1345</URL>\n\n<TITLE>[Question] Load LLaVA1.5 with a linear mmprojector</TITLE>\n\n<BODY>### Question\n\nHi @haotian-liu \r\nI was wondering if it's possible to load LLaVA 1.5 from \"liuhaotian/llava-v1.5-7b\" with a linear mmprojector?\r\n\r\nSince the default configuration is with \"mlp2x_gelu\" I am getting the following warning:\r\n\r\n```\r\nSome weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['model.mm_projector.bias', 'model.mm_projector.weight']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n```\r\n\r\nWhere can I find the trained weights for a linear mmprojector?\r\nThanks in advance!</BODY>\n\n<COMMENTS>\n<Comment by Yinhance at 2025-03-16T02:00:37Z>\nsame question, have you solved it? :)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1341,
    "state": "open",
    "created_by": "cozheyuanzhangde",
    "created_at": "2024-03-26T21:33:26Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1341</URL>\n\n<TITLE>[Question] Does LLaVA use ViT-only feature or CLIP feature?</TITLE>\n\n<BODY>### Question\n\nSorry, it might sound like a superfacial question. From what I see in the code, it seems LLaVA only use the ViT to encode the image without the CLIP projection layer (to language space). I know that LLaVA has a projection layer which projects the visual features to LM embedding space. However, why we use the \"CLIP encoder\" in the paper, not the \"ViT encoder\" because it will makes it sound like CLIP features?\r\n\r\nThanks in advance!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1340,
    "state": "open",
    "created_by": "BakingBrains",
    "created_at": "2024-03-26T11:06:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1340</URL>\n\n<TITLE>Issue with resource consumption while inferencing 7B model</TITLE>\n\n<BODY>I was using ```liuhaotian/llava-v1.5-7b```. Below is my inference script, the GPU consumption for 7B with the below inference script is around 32GB. Not sure why it is consuming that much. Any suggestions or changes in the script?\r\n\r\n\r\n```from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\r\nfrom llava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\r\nfrom transformers import TextStreamer\r\nfrom transformers import AutoTokenizer, BitsAndBytesConfig\r\nfrom llava.model import LlavaLlamaForCausalLM\r\nfrom llava.conversation import conv_templates, SeparatorStyle\r\nfrom llava.utils import disable_torch_init\r\nimport torch\r\nimport requests\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\n\r\n\r\nmodel_path = \"liuhaotian/llava-v1.5-7b\"\r\nkwargs = {\"device_map\": \"auto\"}\r\n\r\nmodel = LlavaLlamaForCausalLM.from_pretrained(model_path, **kwargs)\r\ntokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n\r\nvision_tower = model.get_vision_tower()\r\nif not vision_tower.is_loaded:\r\n    vision_tower.load_model()\r\nvision_tower.to(device='cpu')\r\nimage_processor = vision_tower.image_processor\r\n\r\n\r\ndef load_image(image_file):\r\n    if image_file.startswith('http') or image_file.startswith('https'):\r\n        response = requests.get(image_file)\r\n        image = Image.open(BytesIO(response.content)).convert('RGB')\r\n    else:\r\n        image = Image.open(image_file).convert('RGB')\r\n    return image\r\n\r\n\r\ndef generate(img_url, inp):\r\n    disable_torch_init()\r\n    conv_mode = \"llava_v0\"\r\n    conv = conv_templates[conv_mode].copy()\r\n    roles = conv.roles\r\n\r\n    image = load_image(img_url)\r\n    image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].cuda()\r\n\r\n    print(f\"{roles[1]}: \", end=\"\")\r\n\r\n    if image is not None:\r\n        # first message\r\n        if model.config.mm_use_im_start_end:\r\n            inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + inp\r\n        else:\r\n            inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\r\n        conv.append_message(conv.roles[0], inp)\r\n        image = None\r\n    else:\r\n        # later messages\r\n        conv.append_message(conv.roles[0], inp)\r\n    conv.append_message(conv.roles[1], None)\r\n    prompt = conv.get_prompt()\r\n\r\n    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\r\n    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\r\n    keywords = [stop_str]\r\n    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\r\n    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\r\n\r\n    with torch.inference_mode():\r\n        output_ids = model.generate(\r\n            input_ids,\r\n            images=image_tensor,\r\n            do_sample=True,\r\n            temperature=0.2,\r\n            max_new_tokens=2048,\r\n            streamer=streamer,\r\n            use_cache=True,\r\n            stopping_criteria=[stopping_criteria])\r\n\r\n    outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()\r\n    conv.messages[-1][-1] = outputs\r\n\r\n    print(\"\\n\", {\"prompt\": prompt, \"outputs\": outputs}, \"\\n\")\r\n\r\n\r\nimage_path = \"\"\r\nprompts = \"\"\r\n\r\ngenerate(image_path, prompts)\r\n```</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1339,
    "state": "open",
    "created_by": "ashu0013",
    "created_at": "2024-03-26T08:20:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1339</URL>\n\n<TITLE>OCR Text Output Error in Document QA Project[Usage]</TITLE>\n\n<BODY>### Describe the issue\n\nI'm exploring options to address OCR text output errors in my project Info extraction from documents. Firstly, I'm considering replacing the OCR engine, but I need guidance on how to proceed with this. Alternatively, I'm contemplating providing OCR text along with images in the prompt for information extraction. Could you advise on the feasibility and implementation of this approach?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1338,
    "state": "open",
    "created_by": "ScottishFold007",
    "created_at": "2024-03-26T07:09:27Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1338</URL>\n\n<TITLE>IndexError: Dimension out of range (expected to be in range of [-2, 1], but got 2)</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nI am using the script here （https://github.com/haotian-liu/LLaVA/tree/main/scripts/v1_5） for pre-training and fine-tuning, the pre-training is fine, but during the fine-tuning phase, the following error occurs:\r\n```\r\n[2024-03-26 14:56:28,712] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-03-26 14:56:30,375] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\n[2024-03-26 14:56:30,375] [INFO] [runner.py:570:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero3.json --do_eval False --model_name_or_path /mnt/data/users/gck/Qwen1.5_14B_SFT/Qwen1.5_14B_SFT_SftLoss_bz8/checkpoint-4600 --version qwen_2 --data_path /mnt/data/users/gck/Llava_train_worshop/datasets/llava-v1.5_datasets/final_valid_ds_954260.json --image_folder /mnt/data/users/gck/Llava_train_worshop/datasets/llava-v1.5_datasets/images --vision_tower /mnt/data/users/gck/Llava_train_worshop/clip_models/chinese-clip-vit-large-patch14-336px --pretrain_mm_mlp_adapter /mnt/data/users/gck/Llava_train_worshop/Llava_Qwen2/checkpoint-20000/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --fp16 False --torch_compile True --output_dir /mnt/data/users/gck/Llava_train_worshop/Llava_Qwen2_FT --num_train_epochs 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 500 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0.0 --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 100 --tf32 True --neftune_noise_alpha 5 --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 1 --lazy_preprocess True --report_to wandb\r\n[2024-03-26 14:56:32,296] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-03-26 14:56:33,953] [INFO] [launch.py:138:main] 0 NCCL_IB_TC=136\r\n[2024-03-26 14:56:33,953] [INFO] [launch.py:138:main] 0 NCCL_NET_PLUGIN=accl-lite\r\n[2024-03-26 14:56:33,953] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.18.3\r\n[2024-03-26 14:56:33,953] [INFO] [launch.py:138:main] 0 NCCL_NVLS_ENABLE=0\r\n[2024-03-26 14:56:33,953] [INFO] [launch.py:138:main] 0 NCCL_DEBUG=WARN\r\n[2024-03-26 14:56:33,953] [INFO] [launch.py:138:main] 0 NCCL_IB_GID_INDEX=3\r\n[2024-03-26 14:56:33,953] [INFO] [launch.py:138:main] 0 NCCL_IB_QPS_PER_CONNECTION=4\r\n[2024-03-26 14:56:33,953] [INFO] [launch.py:138:main] 0 NCCL_MIN_CTAS=4\r\n[2024-03-26 14:56:33,953] [INFO] [launch.py:138:main] 0 NCCL_IB_TIMEOUT=22\r\n[2024-03-26 14:56:33,953] [INFO] [launch.py:138:main] 0 NCCL_IB_SL=5\r\n[2024-03-26 14:56:33,954] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\r\n[2024-03-26 14:56:33,954] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0\r\n[2024-03-26 14:56:33,954] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\r\n[2024-03-26 14:56:33,954] [INFO] [launch.py:163:main] dist_world_size=4\r\n[2024-03-26 14:56:33,954] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\r\nwandb: Currently logged in as: gaochangkuan. Use `wandb login --relogin` to force relogin\r\nwandb: Currently logged in as: gaochangkuan. Use `wandb login --relogin` to force relogin\r\nwandb: Currently logged in as: gaochangkuan. Use `wandb login --relogin` to force relogin\r\nwandb: Currently logged in as: gaochangkuan. Use `wandb login --relogin` to force relogin\r\nwandb: wandb version 0.16.5 is available!  To upgrade, please run:\r\nwandb:  $ pip install wandb --upgrade\r\nwandb: Tracking run with wandb version 0.16.2\r\nwandb: wandb version 0.16.5 is available!  To upgrade, please run:\r\nwandb:  $ pip install wandb --upgrade\r\nwandb: Tracking run with wandb version 0.16.2\r\nwandb: Run data is saved locally in /mnt/workspace/users/gck/Llava_train_worshop/LLaVA/wandb/run-20240326_145642-oie9hysm\r\nwandb: Run `wandb offline` to turn off syncing.\r\nwandb: Run data is saved locally in /mnt/workspace/users/gck/Llava_train_worshop/LLaVA/wandb/run-20240326_145642-o5ly0dt5\r\nwandb: Run `wandb offline` to turn off syncing.\r\nwandb: Syncing run LlavaQwen2_2024-03-26\r\nwandb: ⭐️ View project at https://wandb.ai/gaochangkuan/LlavaQwen2-SFT\r\nwandb: 🚀 View run at https://wandb.ai/gaochangkuan/LlavaQwen2-SFT/runs/oie9hysm\r\nwandb: Syncing run LlavaQwen2_2024-03-26\r\nwandb: ⭐️ View project at https://wandb.ai/gaochangkuan/LlavaQwen2-SFT\r\nwandb: 🚀 View run at https://wandb.ai/gaochangkuan/LlavaQwen2-SFT/runs/o5ly0dt5\r\nwandb: wandb version 0.16.5 is available!  To upgrade, please run:\r\nwandb:  $ pip install wandb --upgrade\r\nwandb: Tracking run with wandb version 0.16.2\r\nwandb: Run data is saved locally in /mnt/workspace/users/gck/Llava_train_worshop/LLaVA/wandb/run-20240326_145642-ofy262iu\r\nwandb: Run `wandb offline` to turn off syncing.\r\nwandb: Syncing run LlavaQwen2_2024-03-26\r\nwandb: ⭐️ View project at https://wandb.ai/gaochangkuan/LlavaQwen2-SFT\r\nwandb: 🚀 View run at https://wandb.ai/gaochangkuan/LlavaQwen2-SFT/runs/ofy262iu\r\nwandb: wandb version 0.16.5 is available!  To upgrade, please run:\r\nwandb:  $ pip install wandb --upgrade\r\nwandb: Tracking run with wandb version 0.16.2\r\nwandb: Run data is saved locally in /mnt/workspace/users/gck/Llava_train_worshop/LLaVA/wandb/run-20240326_145642-rjefxr37\r\nwandb: Run `wandb offline` to turn off syncing.\r\nwandb: Syncing run LlavaQwen2_2024-03-26\r\nwandb: ⭐️ View project at https://wandb.ai/gaochangkuan/LlavaQwen2-SFT\r\nwandb: 🚀 View run at https://wandb.ai/gaochangkuan/LlavaQwen2-SFT/runs/rjefxr37\r\n[2024-03-26 14:56:46,526] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-03-26 14:56:46,526] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-03-26 14:56:46,586] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-03-26 14:56:46,732] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2024-03-26 14:56:46,732] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2024-03-26 14:56:46,788] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2024-03-26 14:56:46,789] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n[2024-03-26 14:56:47,014] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-03-26 14:56:47,222] [INFO] [comm.py:637:init_distributed] cdb=None\r\nYou are using a model of type qwen2 to instantiate a model of type llava_qwen2. This is not supported for all configurations of models and can yield errors.\r\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nYou are using a model of type qwen2 to instantiate a model of type llava_qwen2. This is not supported for all configurations of models and can yield errors.\r\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\ndsw-3824-dfd867448-6rrft:580075:580075 [0] NCCL INFO Bootstrap : Using net0:10.15.7.126<0>\r\ndsw-3824-dfd867448-6rrft:580075:580075 [0] NCCL INFO Plugin name set by env to libnccl-net-accl-lite.so\r\nYou are using a model of type qwen2 to instantiate a model of type llava_qwen2. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type qwen2 to instantiate a model of type llava_qwen2. This is not supported for all configurations of models and can yield errors.\r\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\ndsw-3824-dfd867448-6rrft:580075:580075 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\r\ndsw-3824-dfd867448-6rrft:580075:580075 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\r\ndsw-3824-dfd867448-6rrft:580075:580075 [0] NCCL INFO cudaDriverVersion 12020\r\nNCCL version 2.18.1+cuda12.1\r\ndsw-3824-dfd867448-6rrft:580075:580075 [0] NCCL INFO NCCL_MIN_CTAS set by environment to 4.\r\ndsw-3824-dfd867448-6rrft:580076:580076 [1] NCCL INFO cudaDriverVersion 12020\r\ndsw-3824-dfd867448-6rrft:580076:580076 [1] NCCL INFO Bootstrap : Using net0:10.15.7.126<0>\r\ndsw-3824-dfd867448-6rrft:580076:580076 [1] NCCL INFO Plugin name set by env to libnccl-net-accl-lite.so\r\ndsw-3824-dfd867448-6rrft:580076:580076 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\r\ndsw-3824-dfd867448-6rrft:580076:580076 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\r\ndsw-3824-dfd867448-6rrft:580076:580076 [1] NCCL INFO NCCL_MIN_CTAS set by environment to 4.\r\ndsw-3824-dfd867448-6rrft:580077:580077 [2] NCCL INFO cudaDriverVersion 12020\r\ndsw-3824-dfd867448-6rrft:580078:580078 [3] NCCL INFO cudaDriverVersion 12020\r\ndsw-3824-dfd867448-6rrft:580078:580078 [3] NCCL INFO Bootstrap : Using net0:10.15.7.126<0>\r\ndsw-3824-dfd867448-6rrft:580077:580077 [2] NCCL INFO Bootstrap : Using net0:10.15.7.126<0>\r\ndsw-3824-dfd867448-6rrft:580078:580078 [3] NCCL INFO Plugin name set by env to libnccl-net-accl-lite.so\r\ndsw-3824-dfd867448-6rrft:580077:580077 [2] NCCL INFO Plugin name set by env to libnccl-net-accl-lite.so\r\ndsw-3824-dfd867448-6rrft:580078:580078 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\r\ndsw-3824-dfd867448-6rrft:580078:580078 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\r\ndsw-3824-dfd867448-6rrft:580077:580077 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\r\ndsw-3824-dfd867448-6rrft:580077:580077 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\r\ndsw-3824-dfd867448-6rrft:580078:580078 [3] NCCL INFO NCCL_MIN_CTAS set by environment to 4.\r\ndsw-3824-dfd867448-6rrft:580077:580077 [2] NCCL INFO NCCL_MIN_CTAS set by environment to 4.\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO NET/accl-lite : Using [0]mlx5_0:1/RoCE [1]mlx5_1:1/RoCE [RO]; OOB net0:10.15.7.126<0>\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Using network accl-lite\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO NET/accl-lite : Using [0]mlx5_0:1/RoCE [1]mlx5_1:1/RoCE [RO]; OOB net0:10.15.7.126<0>\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Using network accl-lite\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO NET/accl-lite : Using [0]mlx5_0:1/RoCE [1]mlx5_1:1/RoCE [RO]; OOB net0:10.15.7.126<0>\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Using network accl-lite\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO NET/accl-lite : Using [0]mlx5_0:1/RoCE [1]mlx5_1:1/RoCE [RO]; OOB net0:10.15.7.126<0>\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Using network accl-lite\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO NCCL_NVLS_ENABLE set by environment to 0.\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ffffffff\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO NCCL_NVLS_ENABLE set by environment to 0.\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Setting affinity for GPU 2 to ffff,ffffffff\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO NCCL_NVLS_ENABLE set by environment to 0.\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Setting affinity for GPU 3 to ffff,ffffffff\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO NCCL_NVLS_ENABLE set by environment to 0.\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Channel 00/12 :    0   1   2   3\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] -1/-1/-1->3->2 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] -1/-1/-1->3->2 [11] -1/-1/-1->3->2\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO P2P Chunksize set to 524288\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Channel 01/12 :    0   1   2   3\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO P2P Chunksize set to 524288\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO P2P Chunksize set to 524288\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Channel 02/12 :    0   1   2   3\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Channel 03/12 :    0   1   2   3\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Channel 04/12 :    0   1   2   3\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Channel 05/12 :    0   1   2   3\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Channel 06/12 :    0   1   2   3\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Channel 07/12 :    0   1   2   3\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Channel 08/12 :    0   1   2   3\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Channel 09/12 :    0   1   2   3\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Channel 10/12 :    0   1   2   3\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Channel 11/12 :    0   1   2   3\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO P2P Chunksize set to 524288\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Channel 00/0 : 3[40] -> 0[10] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Channel 00/0 : 0[10] -> 1[20] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Channel 00/0 : 2[30] -> 3[40] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Channel 00/0 : 1[20] -> 2[30] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Channel 01/0 : 3[40] -> 0[10] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Channel 01/0 : 0[10] -> 1[20] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Channel 01/0 : 2[30] -> 3[40] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Channel 01/0 : 1[20] -> 2[30] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Channel 02/0 : 3[40] -> 0[10] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Channel 02/0 : 0[10] -> 1[20] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Channel 02/0 : 2[30] -> 3[40] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Channel 02/0 : 1[20] -> 2[30] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Channel 03/0 : 3[40] -> 0[10] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Channel 03/0 : 0[10] -> 1[20] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Channel 03/0 : 2[30] -> 3[40] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Channel 03/0 : 1[20] -> 2[30] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Channel 04/0 : 3[40] -> 0[10] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Channel 04/0 : 0[10] -> 1[20] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Channel 04/0 : 2[30] -> 3[40] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Channel 04/0 : 1[20] -> 2[30] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Channel 05/0 : 3[40] -> 0[10] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Channel 05/0 : 0[10] -> 1[20] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Channel 05/0 : 2[30] -> 3[40] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Channel 05/0 : 1[20] -> 2[30] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Channel 06/0 : 3[40] -> 0[10] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Channel 06/0 : 0[10] -> 1[20] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Channel 06/0 : 2[30] -> 3[40] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Channel 06/0 : 1[20] -> 2[30] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Channel 07/0 : 3[40] -> 0[10] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Channel 07/0 : 0[10] -> 1[20] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Channel 07/0 : 2[30] -> 3[40] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Channel 07/0 : 1[20] -> 2[30] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Channel 08/0 : 3[40] -> 0[10] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Channel 08/0 : 0[10] -> 1[20] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Channel 08/0 : 2[30] -> 3[40] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Channel 08/0 : 1[20] -> 2[30] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Channel 09/0 : 3[40] -> 0[10] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Channel 09/0 : 0[10] -> 1[20] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Channel 09/0 : 2[30] -> 3[40] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Channel 09/0 : 1[20] -> 2[30] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Channel 10/0 : 3[40] -> 0[10] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Channel 10/0 : 0[10] -> 1[20] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Channel 10/0 : 2[30] -> 3[40] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Channel 10/0 : 1[20] -> 2[30] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Channel 11/0 : 3[40] -> 0[10] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Channel 11/0 : 2[30] -> 3[40] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Channel 11/0 : 1[20] -> 2[30] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Channel 11/0 : 0[10] -> 1[20] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Connected all rings\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Connected all rings\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Channel 00/0 : 3[40] -> 2[30] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Connected all rings\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Connected all rings\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Channel 01/0 : 3[40] -> 2[30] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Channel 02/0 : 3[40] -> 2[30] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Channel 03/0 : 3[40] -> 2[30] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Channel 04/0 : 3[40] -> 2[30] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Channel 05/0 : 3[40] -> 2[30] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Channel 06/0 : 3[40] -> 2[30] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Channel 07/0 : 3[40] -> 2[30] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Channel 08/0 : 3[40] -> 2[30] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Channel 09/0 : 3[40] -> 2[30] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Channel 10/0 : 3[40] -> 2[30] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Channel 11/0 : 3[40] -> 2[30] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Channel 00/0 : 2[30] -> 1[20] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Channel 00/0 : 1[20] -> 0[10] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Channel 01/0 : 2[30] -> 1[20] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Channel 01/0 : 1[20] -> 0[10] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Channel 02/0 : 2[30] -> 1[20] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Channel 02/0 : 1[20] -> 0[10] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Channel 03/0 : 2[30] -> 1[20] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Channel 03/0 : 1[20] -> 0[10] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Channel 04/0 : 2[30] -> 1[20] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Channel 04/0 : 1[20] -> 0[10] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Channel 05/0 : 2[30] -> 1[20] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Channel 05/0 : 1[20] -> 0[10] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Channel 06/0 : 2[30] -> 1[20] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Channel 06/0 : 1[20] -> 0[10] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Channel 07/0 : 2[30] -> 1[20] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Channel 07/0 : 1[20] -> 0[10] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Channel 08/0 : 2[30] -> 1[20] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Channel 08/0 : 1[20] -> 0[10] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Channel 09/0 : 2[30] -> 1[20] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Channel 09/0 : 1[20] -> 0[10] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Channel 10/0 : 2[30] -> 1[20] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Channel 10/0 : 1[20] -> 0[10] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Channel 11/0 : 1[20] -> 0[10] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO Connected all trees\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO 12 coll channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Channel 11/0 : 2[30] -> 1[20] via P2P/IPC/read\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO Connected all trees\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO 12 coll channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO Connected all trees\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO 12 coll channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO Connected all trees\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO 12 coll channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer\r\ndsw-3824-dfd867448-6rrft:580077:580336 [2] NCCL INFO comm 0x55a4b06c0710 rank 2 nranks 4 cudaDev 2 busId 30 commId 0x120db2d6d528920 - Init COMPLETE\r\ndsw-3824-dfd867448-6rrft:580075:580333 [0] NCCL INFO comm 0x562690ba00b0 rank 0 nranks 4 cudaDev 0 busId 10 commId 0x120db2d6d528920 - Init COMPLETE\r\ndsw-3824-dfd867448-6rrft:580076:580334 [1] NCCL INFO comm 0x5582582cf3f0 rank 1 nranks 4 cudaDev 1 busId 20 commId 0x120db2d6d528920 - Init COMPLETE\r\ndsw-3824-dfd867448-6rrft:580078:580335 [3] NCCL INFO comm 0x562b90151f80 rank 3 nranks 4 cudaDev 3 busId 40 commId 0x120db2d6d528920 - Init COMPLETE\r\n[2024-03-26 14:56:53,704] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 483, num_elems = 14.16B\r\nLoading checkpoint shards: 100%|██████████████████| 6/6 [00:21<00:00,  3.55s/it]\r\nLoading checkpoint shards: 100%|██████████████████| 6/6 [00:21<00:00,  3.54s/it]\r\nLoading checkpoint shards: 100%|██████████████████| 6/6 [00:21<00:00,  3.55s/it]\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nYou are using a model of type clip_vision_model to instantiate a model of type chinese_clip_vision_model. This is not supported for all configurations of models and can yield errors.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nYou are using a model of type clip_vision_model to instantiate a model of type chinese_clip_vision_model. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type clip_vision_model to instantiate a model of type chinese_clip_vision_model. This is not supported for all configurations of models and can yield errors.\r\n/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\n/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\n/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\nLoading checkpoint shards: 100%|██████████████████| 6/6 [00:24<00:00,  4.03s/it]\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nYou are using a model of type clip_vision_model to instantiate a model of type chinese_clip_vision_model. This is not supported for all configurations of models and can yield errors.\r\n/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\n[2024-03-26 14:57:18,720] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 874, num_elems = 14.47B\r\nFormatting inputs...Skip in lazy mode\r\nDetected kernel version 4.19.91, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\r\nUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\r\nUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\r\nDetected CUDA files, patching ldflags\r\nEmitting ninja build file /root/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...\r\nBuilding extension module fused_adam...\r\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\nninja: no work to do.\r\nLoading extension module fused_adam...\r\nTime to load fused_adam op: 0.20271062850952148 seconds\r\n/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\r\n  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\r\nLoading extension module fused_adam...\r\nTime to load fused_adam op: 0.20226621627807617 seconds\r\n/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\r\n  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\r\nUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\r\nDetected CUDA files, patching ldflags\r\nEmitting ninja build file /root/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...\r\nBuilding extension module fused_adam...\r\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\nUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\r\nninja: no work to do.\r\nLoading extension module fused_adam...\r\nTime to load fused_adam op: 0.15857577323913574 seconds\r\n/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\r\n  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\r\nLoading extension module fused_adam...\r\nTime to load fused_adam op: 0.1020195484161377 seconds\r\n/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\r\n  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\r\nParameter Offload: Total persistent parameters: 1363968 in 448 params\r\n  0%|                                                 | 0/59641 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n  warnings.warn(\r\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n  warnings.warn(\r\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n  warnings.warn(\r\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\r\n  warnings.warn(\r\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\r\n  warnings.warn(\r\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\r\n  warnings.warn(\r\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n  warnings.warn(\r\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\r\n  warnings.warn(\r\nTraceback (most recent call last):\r\n  File \"/mnt/workspace/users/gck/Llava_train_worshop/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"/mnt/workspace/users/gck/Llava_train_worshop/LLaVA/llava/train/train.py\", line 930, in train\r\n    trainer.train()\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1780, in train\r\n    return inner_training_loop(\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2118, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3036, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3059, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py\", line 1818, in forward\r\n    loss = self.module(*inputs, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/mnt/workspace/users/gck/Llava_train_worshop/LLaVA/llava/model/language_model/llava_qwen2.py\", line 65, in forward\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/mnt/workspace/users/gck/Llava_train_worshop/LLaVA/llava/model/llava_arch.py\", line 251, in prepare_inputs_labels_for_multimodal\r\n    cur_input_embeds = self.get_model().embed_tokens(torch.cat(cur_input_ids_noim))\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1581, in _call_impl\r\n    hook_result = hook(self, args, result)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_utils.py\", line 128, in neftune_post_forward_hook\r\n    dims = torch.tensor(output.size(1) * output.size(2))\r\nIndexError: Dimension out of range (expected to be in range of [-2, 1], but got 2)\r\nTraceback (most recent call last):\r\n  File \"/mnt/workspace/users/gck/Llava_train_worshop/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"/mnt/workspace/users/gck/Llava_train_worshop/LLaVA/llava/train/train.py\", line 930, in train\r\n    trainer.train()\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1780, in train\r\n    return inner_training_loop(\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2118, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3036, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3059, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py\", line 1818, in forward\r\n    loss = self.module(*inputs, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/mnt/workspace/users/gck/Llava_train_worshop/LLaVA/llava/model/language_model/llava_qwen2.py\", line 65, in forward\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/mnt/workspace/users/gck/Llava_train_worshop/LLaVA/llava/model/llava_arch.py\", line 251, in prepare_inputs_labels_for_multimodal\r\n    cur_input_embeds = self.get_model().embed_tokens(torch.cat(cur_input_ids_noim))\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1581, in _call_impl\r\n    hook_result = hook(self, args, result)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_utils.py\", line 128, in neftune_post_forward_hook\r\n    dims = torch.tensor(output.size(1) * output.size(2))\r\nIndexError: Dimension out of range (expected to be in range of [-2, 1], but got 2)\r\nTraceback (most recent call last):\r\n  File \"/mnt/workspace/users/gck/Llava_train_worshop/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"/mnt/workspace/users/gck/Llava_train_worshop/LLaVA/llava/train/train.py\", line 930, in train\r\n    trainer.train()\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1780, in train\r\n    return inner_training_loop(\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2118, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3036, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3059, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py\", line 1818, in forward\r\n    loss = self.module(*inputs, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/mnt/workspace/users/gck/Llava_train_worshop/LLaVA/llava/model/language_model/llava_qwen2.py\", line 65, in forward\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/mnt/workspace/users/gck/Llava_train_worshop/LLaVA/llava/model/llava_arch.py\", line 251, in prepare_inputs_labels_for_multimodal\r\n    cur_input_embeds = self.get_model().embed_tokens(torch.cat(cur_input_ids_noim))\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1581, in _call_impl\r\n    hook_result = hook(self, args, result)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_utils.py\", line 128, in neftune_post_forward_hook\r\n    dims = torch.tensor(output.size(1) * output.size(2))\r\nIndexError: Dimension out of range (expected to be in range of [-2, 1], but got 2)\r\nTraceback (most recent call last):\r\n  File \"/mnt/workspace/users/gck/Llava_train_worshop/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"/mnt/workspace/users/gck/Llava_train_worshop/LLaVA/llava/train/train.py\", line 930, in train\r\n    trainer.train()\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1780, in train\r\n    return inner_training_loop(\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2118, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3036, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3059, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py\", line 1818, in forward\r\n    loss = self.module(*inputs, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/mnt/workspace/users/gck/Llava_train_worshop/LLaVA/llava/model/language_model/llava_qwen2.py\", line 65, in forward\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/mnt/workspace/users/gck/Llava_train_worshop/LLaVA/llava/model/llava_arch.py\", line 251, in prepare_inputs_labels_for_multimodal\r\n    cur_input_embeds = self.get_model().embed_tokens(torch.cat(cur_input_ids_noim))\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1581, in _call_impl\r\n    hook_result = hook(self, args, result)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_utils.py\", line 128, in neftune_post_forward_hook\r\n    dims = torch.tensor(output.size(1) * output.size(2))\r\nIndexError: Dimension out of range (expected to be in range of [-2, 1], but got 2)\r\n```\r\nMay I ask what causes this? Is it because I've added dialog data without images in it (similar to sharegpt format)?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1337,
    "state": "open",
    "created_by": "yash0307",
    "created_at": "2024-03-25T23:53:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1337</URL>\n\n<TITLE>[Usage] Instructions for evaluating Mistral based model</TITLE>\n\n<BODY>### Describe the issue\n\nHi,\r\n\r\nCould you point me to the instructions for evaluating Mistral based model on VQA datasets (such as GQA, TextVQA, etc)?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1336,
    "state": "open",
    "created_by": "nishgop",
    "created_at": "2024-03-25T17:40:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1336</URL>\n\n<TITLE>Does LLaVA 1.6 have the same commercial usage conditions as 1.5?</TITLE>\n\n<BODY>### Question\n\nReferring to https://github.com/haotian-liu/LLaVA/issues/659. Does LLaVA 1.6 have the same commercial usage conditions as 1.5?</BODY>\n\n<COMMENTS>\n<Comment by JVP15 at 2024-03-28T16:29:27Z>\nFrom their [blog](https://llava-vl.github.io/blog/2024-01-30-llava-next/), they say that they used [ShareGPT4V](https://sharegpt4v.github.io//) which is licensed under CC BY NC 4.0 so I have to imagine that it would share the same commercial conditions (i.e. no commercial usage).\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1335,
    "state": "open",
    "created_by": "adabadaramola",
    "created_at": "2024-03-25T17:24:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1335</URL>\n\n<TITLE>issue Finetuning llava-v1.6-34b model</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI am finetuning llava-v1.6-34b on a some data, about 75,866 images with a resolution of 750X750 per image. I have tried finetuuning with  a100 80GB (6 devices, 8 devices) and the h100 80GB (2, devices, 6 devices and 8 devices)\r\nwhen training it just exits and runs into out of memory error, or it just gives another error which I will provide. \r\n\r\nwhats are the specifications to successfully train the llava-v1.6-34b model, or is there any reason for the issue\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path liuhaotian/llava-v1.6-34b \\\r\n    --version v1 \\\r\n    --data_path ./training001/metadata.json \\\r\n    --image_folder ./ \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.6-34b-task \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n1.\r\n/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\r\n  warnings.warn(\r\nParameter Offload: Total persistent parameters: 1213440 in 369 params\r\nTraceback (most recent call last):\r\n  File \"/workspace/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"/workspace/LLaVA/llava/train/train.py\", line 969, in train\r\n    trainer.train()\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1687, in _inner_training_loop\r\n    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 1198, in prepare\r\n    result = self._prepare_deepspeed(*args)\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 1537, in _prepare_deepspeed\r\n    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/__init__.py\", line 171, in initialize\r\n    engine = DeepSpeedEngine(args=args,\r\n  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py\", line 304, in __init__\r\n    self._configure_optimizer(optimizer, model_parameters)\r\n  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py\", line 1234, in _configure_optimizer\r\n    self.optimizer = self._configure_zero_optimizer(basic_optimizer)\r\n  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py\", line 1563, in _configure_zero_optimizer\r\n    optimizer = DeepSpeedZeroOptimizer_Stage3(\r\n  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/stage3.py\", line 362, in __init__\r\n    self._setup_for_real_optimizer()\r\n  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/stage3.py\", line 465, in _setup_for_real_optimizer\r\n    self._create_fp32_partitions()\r\n  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/stage3.py\", line 854, in _create_fp32_partitions\r\n    self.device).clone().float().detach())\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.16 GiB. GPU 0 has a total capacty of 79.15 GiB of which 1.16 GiB is free. Process 3400995 has 77.98 GiB memory in use. Of the allocated memory 74.87 GiB is allocated by PyTorch, and 2.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n[2024-03-21 10:19:08,590] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1464\r\n[2024-03-21 10:19:08,591] [ERROR] [launch.py:321:sigkill_handler] ['/usr/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.6-34b', '--version', 'v1', '--data_path', './training001/metadata.json', '--image_folder', './training001', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.6-34b-task', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = 1\r\n\r\n2.\r\n[2024-03-25 14:13:32,240] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3772\r\n[2024-03-25 14:13:32,979] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3773\r\n[2024-03-25 14:13:32,981] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3774\r\n[2024-03-25 14:13:33,154] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3775\r\n[2024-03-25 14:13:33,156] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3776\r\n[2024-03-25 14:13:33,156] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3777\r\n[2024-03-25 14:13:33,157] [ERROR] [launch.py:321:sigkill_handler] ['/usr/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=5', '--de\r\nepspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.6-34b', '--version', 'v1', '--data_path', './training001/metadat\r\na.json', '--image_folder', './', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_selec\r\nt_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_lengt\r\nh', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.6-34b-task', '--num_train_epochs', '1', '--per_device_train_batch_size',\r\n'4', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--s\r\nave_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_ty\r\npe', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_wo\r\nrkers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = 1\r\nroot@04b9ed35b384:/workspace/LLaVA#\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by Vish2427 at 2024-03-26T04:02:36Z>\n@adabadaramola I have a bit different issue. Can you please help me with it.\r\nI had followed the same fine tune script, i am getting error about not able to import llava module while executing train_mem.py file.\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/98541876/f067f8a8-8e52-417a-865f-940d9738da07)\r\n\r\nIf you can help me with the code/script which you have used to start with finetning it will be helpfull to me.\r\n\r\nThanks\n</Comment>\n<Comment by spillai at 2024-03-26T18:02:11Z>\n@adabadaramola I didn't realize that v1.6 was fine-tunable yet. Were you able to fine-tune a smaller 7b model instead?\n</Comment>\n<Comment by anidh at 2024-03-26T18:48:36Z>\n> @adabadaramola I have a bit different issue. Can you please help me with it. I had followed the same fine tune script, i am getting error about not able to import llava module while executing train_mem.py file.\r\n> \r\n> ![image](https://private-user-images.githubusercontent.com/98541876/316722551-f067f8a8-8e52-417a-865f-940d9738da07.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTE0NzkxNDAsIm5iZiI6MTcxMTQ3ODg0MCwicGF0aCI6Ii85ODU0MTg3Ni8zMTY3MjI1NTEtZjA2N2Y4YTgtOGU1Mi00MTdhLTg2NWYtOTQwZDk3MzhkYTA3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzI2VDE4NDcyMFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc0NjVjZDk5NjQ2ZWU5MWUzNWU4YmM0ZmM3ZTU4ODgwMjM5NmI4OWE0Njc3MzMzOTYyYWViZDQzNmY1OGMxNDAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.u3wFarIHIDVa5Ntd6hrZsYgZbvsYQNl_ebcbswZLnmE)\r\n> \r\n> If you can help me with the code/script which you have used to start with finetning it will be helpfull to me.\r\n> \r\n> Thanks\r\n\r\nYou need to do pip install from the llava folder using the command \r\n`pip install -e .`\n</Comment>\n<Comment by arielnlee at 2024-03-27T20:29:19Z>\nI used 3 A100 80GB gpus for 1.6-34b and 1 A100 80GB for 1.6-mistral-7b. note: I've only tried this for low rank fine-tuning, not full!\r\nhttps://github.com/arielnlee/LLaVA-1.6-ft\n</Comment>\n<Comment by adabadaramola at 2024-03-27T23:01:30Z>\n> @adabadaramola I didn't realize that v1.6 was fine-tunable yet. Were you able to fine-tune a smaller 7b model instead?\r\n\r\nyeah I saw a guy finetune v1.6 on youtube\r\nfor the smaller 7b, no I did not try that, felt the 34b was more suitable for my usecase\n</Comment>\n<Comment by adabadaramola at 2024-03-27T23:01:49Z>\n> I used 3 A100 80GB gpus for 1.6-34b and 1 A100 80GB for 1.6-mistral-7b. note: I've only tried this for low rank fine-tuning, not full! https://github.com/arielnlee/LLaVA-1.6-ft\r\n\r\nThanks for this, I will try it out\n</Comment>\n<Comment by adabadaramola at 2024-03-31T05:51:35Z>\n> I used 3 A100 80GB gpus for 1.6-34b and 1 A100 80GB for 1.6-mistral-7b. note: I've only tried this for low rank fine-tuning, not full! https://github.com/arielnlee/LLaVA-1.6-ft\r\n\r\nthanks the script works, I have been trying to evaluate after training I have been getting errors, please can you help or tell me how you go about it, I already have the checkpoints in the output dir\n</Comment>\n<Comment by Iven2132 at 2024-03-31T14:19:52Z>\nHey, @arielnlee Do you have a notebook for fine-tuning 1.6-34b?\n</Comment>\n<Comment by arielnlee at 2024-03-31T14:33:37Z>\n> > I used 3 A100 80GB gpus for 1.6-34b and 1 A100 80GB for 1.6-mistral-7b. note: I've only tried this for low rank fine-tuning, not full! https://github.com/arielnlee/LLaVA-1.6-ft\r\n> \r\n> thanks the script works, I have been trying to evaluate after training I have been getting errors, please can you help or tell me how you go about it, I already have the checkpoints in the output dir\r\n\r\n\r\nDo you mean the model is outputting errors or when you try to run you get errors? Before evaluating, I merge the LoRA weights back onto the base model. Then I eval on the “merged” fine-tune. There’s a python file in scripts that you can use to merge.\n</Comment>\n<Comment by arielnlee at 2024-03-31T14:34:08Z>\n> Hey, @arielnlee Do you have a notebook for fine-tuning 1.6-34b?\r\n\r\nI don’t, but I can throw one together this week!\n</Comment>\n<Comment by Iven2132 at 2024-03-31T14:43:05Z>\n> > Hey, @arielnlee Do you have a notebook for fine-tuning 1.6-34b?\r\n> \r\n> I don’t, but I can throw one together this week!\r\n\r\nIt would be great, I'd love to chat with you. What's your email? Btw I contacted you on your website :)\n</Comment>\n<Comment by Linziyang1999 at 2024-04-03T00:30:11Z>\n> ### Describe the issue\r\n> Issue: I am finetuning llava-v1.6-34b on a some data, about 75,866 images with a resolution of 750X750 per image. I have tried finetuuning with a100 80GB (6 devices, 8 devices) and the h100 80GB (2, devices, 6 devices and 8 devices) when training it just exits and runs into out of memory error, or it just gives another error which I will provide.\r\n> \r\n> whats are the specifications to successfully train the llava-v1.6-34b model, or is there any reason for the issue\r\n> \r\n> Command:\r\n> \r\n> ```\r\n> PASTE THE COMMANDS HERE.\r\n> deepspeed llava/train/train_mem.py \\\r\n>     --deepspeed ./scripts/zero3.json \\\r\n>     --model_name_or_path liuhaotian/llava-v1.6-34b \\\r\n>     --version v1 \\\r\n>     --data_path ./training001/metadata.json \\\r\n>     --image_folder ./ \\\r\n>     --vision_tower openai/clip-vit-large-patch14-336 \\\r\n>     --mm_projector_type mlp2x_gelu \\\r\n>     --mm_vision_select_layer -2 \\\r\n>     --mm_use_im_start_end False \\\r\n>     --mm_use_im_patch_token False \\\r\n>     --image_aspect_ratio pad \\\r\n>     --group_by_modality_length True \\\r\n>     --bf16 True \\\r\n>     --output_dir ./checkpoints/llava-v1.6-34b-task \\\r\n>     --num_train_epochs 1 \\\r\n>     --per_device_train_batch_size 4 \\\r\n>     --per_device_eval_batch_size 4 \\\r\n>     --gradient_accumulation_steps 1 \\\r\n>     --evaluation_strategy \"no\" \\\r\n>     --save_strategy \"steps\" \\\r\n>     --save_steps 50000 \\\r\n>     --save_total_limit 1 \\\r\n>     --learning_rate 2e-5 \\\r\n>     --weight_decay 0. \\\r\n>     --warmup_ratio 0.03 \\\r\n>     --lr_scheduler_type \"cosine\" \\\r\n>     --logging_steps 1 \\\r\n>     --tf32 True \\\r\n>     --model_max_length 2048 \\\r\n>     --gradient_checkpointing True \\\r\n>     --dataloader_num_workers 4 \\\r\n>     --lazy_preprocess True \\\r\n>     --report_to wandb\r\n> ```\r\n> \r\n> Log:\r\n> \r\n> ```\r\n> PASTE THE LOGS HERE.\r\n> 1.\r\n> /usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\r\n>   warnings.warn(\r\n> Parameter Offload: Total persistent parameters: 1213440 in 369 params\r\n> Traceback (most recent call last):\r\n>   File \"/workspace/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n>     train(attn_implementation=\"flash_attention_2\")\r\n>   File \"/workspace/LLaVA/llava/train/train.py\", line 969, in train\r\n>     trainer.train()\r\n>   File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1539, in train\r\n>     return inner_training_loop(\r\n>   File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1687, in _inner_training_loop\r\n>     model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 1198, in prepare\r\n>     result = self._prepare_deepspeed(*args)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 1537, in _prepare_deepspeed\r\n>     engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/deepspeed/__init__.py\", line 171, in initialize\r\n>     engine = DeepSpeedEngine(args=args,\r\n>   File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py\", line 304, in __init__\r\n>     self._configure_optimizer(optimizer, model_parameters)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py\", line 1234, in _configure_optimizer\r\n>     self.optimizer = self._configure_zero_optimizer(basic_optimizer)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py\", line 1563, in _configure_zero_optimizer\r\n>     optimizer = DeepSpeedZeroOptimizer_Stage3(\r\n>   File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/stage3.py\", line 362, in __init__\r\n>     self._setup_for_real_optimizer()\r\n>   File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/stage3.py\", line 465, in _setup_for_real_optimizer\r\n>     self._create_fp32_partitions()\r\n>   File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/stage3.py\", line 854, in _create_fp32_partitions\r\n>     self.device).clone().float().detach())\r\n> torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.16 GiB. GPU 0 has a total capacty of 79.15 GiB of which 1.16 GiB is free. Process 3400995 has 77.98 GiB memory in use. Of the allocated memory 74.87 GiB is allocated by PyTorch, and 2.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n> [2024-03-21 10:19:08,590] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1464\r\n> [2024-03-21 10:19:08,591] [ERROR] [launch.py:321:sigkill_handler] ['/usr/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.6-34b', '--version', 'v1', '--data_path', './training001/metadata.json', '--image_folder', './training001', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.6-34b-task', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = 1\r\n> \r\n> 2.\r\n> [2024-03-25 14:13:32,240] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3772\r\n> [2024-03-25 14:13:32,979] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3773\r\n> [2024-03-25 14:13:32,981] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3774\r\n> [2024-03-25 14:13:33,154] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3775\r\n> [2024-03-25 14:13:33,156] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3776\r\n> [2024-03-25 14:13:33,156] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3777\r\n> [2024-03-25 14:13:33,157] [ERROR] [launch.py:321:sigkill_handler] ['/usr/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=5', '--de\r\n> epspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.6-34b', '--version', 'v1', '--data_path', './training001/metadat\r\n> a.json', '--image_folder', './', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_selec\r\n> t_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_lengt\r\n> h', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.6-34b-task', '--num_train_epochs', '1', '--per_device_train_batch_size',\r\n> '4', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--s\r\n> ave_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_ty\r\n> pe', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_wo\r\n> rkers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = 1\r\n> root@04b9ed35b384:/workspace/LLaVA#\r\n> ```\r\n> \r\n> Screenshots: You may attach screenshots if it better explains the issue.\r\n\r\nHi, base on my experience in finetuning yi 34b. It seems like your need use batch size1 and zero3_offload\n</Comment>\n<Comment by Linziyang1999 at 2024-04-03T00:34:10Z>\n> I used 3 A100 80GB gpus for 1.6-34b and 1 A100 80GB for 1.6-mistral-7b. note: I've only tried this for low rank fine-tuning, not full!\r\n> https://github.com/arielnlee/LLaVA-1.6-ft\r\n\r\nHi! LLaVA train vit parameters in 2 training strategy, but they didn't release their ViT parameters. Could you teach me how to solve this promble. Or just using raw clip parameter. Thanks!\n</Comment>\n<Comment by Iven2132 at 2024-04-05T15:48:27Z>\n> > Hey, @arielnlee Do you have a notebook for fine-tuning 1.6-34b?\r\n> \r\n> I don’t, but I can throw one together this week!\r\n\r\nHey, @arielnlee Let me know if you got something :)\n</Comment>\n<Comment by jsm69 at 2024-04-07T10:49:43Z>\n> > Hey, @arielnlee Do you have a notebook for fine-tuning 1.6-34b?\r\n> \r\n> I don’t, but I can throw one together this week!\r\n\r\n@arielnlee How did you fine-tuned LLaVA 1.6 34b? Do you have any resources for this?\n</Comment>\n<Comment by arielnlee at 2024-04-08T01:37:57Z>\n> > > Hey, @arielnlee Do you have a notebook for fine-tuning 1.6-34b?\r\n> > \r\n> > \r\n> > I don’t, but I can throw one together this week!\r\n> \r\n> Hey, @arielnlee Let me know if you got something :)\r\n\r\nApologies, the week got away from me, but it's still on my list. In the meantime it should work by using the repo!\n</Comment>\n<Comment by arielnlee at 2024-04-08T01:42:10Z>\n> > > > Hey, @arielnlee Do you have a notebook for fine-tuning 1.6-34b?\r\n> > > \r\n> > > \r\n> > > I don’t, but I can throw one together this week!\r\n> > \r\n> > \r\n> > @arielnlee How did you fine-tuned LLaVA 1.6 34b? Do you have any resources for this?\r\n> \r\n> I have a question. I've been trying to fine-tune llava 7b, everything works but the results did not change at all. I just wanted it to label one image to what I fine tune it with, it still recognize the image as something else.\r\n\r\nHow big is your fine-tuning dataset? And what's the task? For my specific use-case, the scripts work well. The size of my dataset is ~20k.\n</Comment>\n<Comment by Iven2132 at 2024-04-08T13:56:33Z>\n> > > > Hey, @arielnlee Do you have a notebook for fine-tuning 1.6-34b?\r\n> > > \r\n> > > \r\n> > > I don’t, but I can throw one together this week!\r\n> > \r\n> > \r\n> > Hey, @arielnlee Let me know if you got something :)\r\n> \r\n> Apologies, the week got away from me, but it's still on my list. In the meantime it should work by using the repo!\r\n\r\n@arielnlee I'm curious - About how many lines of examples to fine-tune LLaVA to get results? Also, can you make a notebook this week?\n</Comment>\n<Comment by moaldeen at 2024-04-10T22:59:38Z>\n> > > > > Hey, @arielnlee Do you have a notebook for fine-tuning 1.6-34b?\r\n> > > > \r\n> > > > \r\n> > > > I don’t, but I can throw one together this week!\r\n> > > \r\n> > > \r\n> > > Hey, @arielnlee Let me know if you got something :)\r\n> > \r\n> > \r\n> > Apologies, the week got away from me, but it's still on my list. In the meantime it should work by using the repo!\r\n> \r\n> @arielnlee I'm curious - About how many lines of examples to fine-tune LLaVA to get results? Also, can you make a notebook this week?\r\n\r\nI am  trying to feed it the same image that i used for finetuning but it still predict it wrong .. am i getting anything wrong\n</Comment>\n<Comment by fisher75 at 2024-04-25T13:12:06Z>\nHi, how do you know the training was effective? Did you use the default training setting? I LoRA with default parameters and basically no improvement.\n</Comment>\n<Comment by songchx24 at 2024-04-25T16:59:20Z>\n> Hi, how do you know the training was effective? Did you use the default training setting? I LoRA with default parameters and basically no improvement.\r\n\r\nI don't think the training script for 1.5 works for 1.6 at the moment. I looked into the llava/train/train.py, llava/model/builder.py, and llava/model/langauge_model, and noticed that they are not compatible with training 1.6. For example, I found that even though I tried to finetune llava 1.6 Mistral, the training file initiated a llava llama for me, because in the train.py, only llama and mpt instance were told to be initiated. I think if you want to fine-tune 1.6, you need to change many of the files manually.\n</Comment>\n<Comment by arielnlee at 2024-04-25T17:03:46Z>\n> > Hi, how do you know the training was effective? Did you use the default training setting? I LoRA with default parameters and basically no improvement.\r\n> \r\n> I don't think the training script for 1.5 works for 1.6 at the moment. I looked into the llava/train/train.py, llava/model/builder.py, and llava/model/langauge_model, and noticed that they are not compatible with training 1.6. For example, I found that even though I tried to finetune llava 1.6 Mistral, the training file initiated a llava llama for me, because in the train.py, only llama and mpt instance were told to be initiated. I think if you want to fine-tune 1.6, you need to change many of the files manually.\r\n\r\n@songchx24 \r\nCheck here: https://github.com/arielnlee/LLaVA-1.6-ft\n</Comment>\n<Comment by fisher75 at 2024-04-25T17:43:40Z>\n> > > Hi, how do you know the training was effective? Did you use the default training setting? I LoRA with default parameters and basically no improvement.\r\n> > \r\n> > \r\n> > I don't think the training script for 1.5 works for 1.6 at the moment. I looked into the llava/train/train.py, llava/model/builder.py, and llava/model/langauge_model, and noticed that they are not compatible with training 1.6. For example, I found that even though I tried to finetune llava 1.6 Mistral, the training file initiated a llava llama for me, because in the train.py, only llama and mpt instance were told to be initiated. I think if you want to fine-tune 1.6, you need to change many of the files manually.\r\n> \r\n> @songchx24 Check here: https://github.com/arielnlee/LLaVA-1.6-ft\r\n\r\nSo this you have successfully changed code to fine-tune the 1.6? Nice! Thanks for the info!\r\n\r\nBtw, may I ask it can support all 1.6 version lora or just Mistral?\n</Comment>\n<Comment by fisher75 at 2024-04-25T17:44:44Z>\n> > Hi, how do you know the training was effective? Did you use the default training setting? I LoRA with default parameters and basically no improvement.\r\n> \r\n> I don't think the training script for 1.5 works for 1.6 at the moment. I looked into the llava/train/train.py, llava/model/builder.py, and llava/model/langauge_model, and noticed that they are not compatible with training 1.6. For example, I found that even though I tried to finetune llava 1.6 Mistral, the training file initiated a llava llama for me, because in the train.py, only llama and mpt instance were told to be initiated. I think if you want to fine-tune 1.6, you need to change many of the files manually.\r\n\r\nOK that explains a lot, because I tried my 1.6 it basically has no improvement after lora with main repo. Btw @arielnlee has shown a repo I think maybe it already has someone who changed the code to make it suitable for 1.6 finetune.\n</Comment>\n<Comment by babuus at 2024-07-17T13:59:29Z>\nIf someone has fine-tuned models with sizes of 1.6B( 7B, and 13B ), can you mention the minimum hardware requirements?\n</Comment>\n<Comment by Yuanyuan-Shen at 2024-07-24T21:49:35Z>\n> If someone has fine-tuned models with sizes of 1.6B( 7B, and 13B ), can you mention the minimum hardware requirements?\r\n\r\n@babuus Not sure the minimum requirements, but seems 1 A100 80G works referring to https://github.com/haotian-liu/LLaVA/issues/1335#issuecomment-2023922331\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1334,
    "state": "open",
    "created_by": "CinKKKyo",
    "created_at": "2024-03-25T10:58:10Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1334</URL>\n\n<TITLE>[Usage] minimum GPU memory for LLaVA-7B</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:  What is the minimum GPU memory for pretraining & fine-tuning the LLaVA-7B? Is there a memory requirement per GPU card?</BODY>\n\n<COMMENTS>\n<Comment by anas-zafar at 2024-06-08T11:55:16Z>\n@CinKKKyo did you get to know? Thanks\n</Comment>\n<Comment by nauyan at 2024-08-24T16:46:14Z>\n@CinKKKyo @anas-zafar any ideas?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1333,
    "state": "open",
    "created_by": "guyuliang7",
    "created_at": "2024-03-25T10:47:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1333</URL>\n\n<TITLE>NPU eval MME The result is C++ code.</TITLE>\n\n<BODY>### Question\n\n![image](https://github.com/haotian-liu/LLaVA/assets/163366727/6f8f825a-c8e4-472c-ade2-deb6da33428e)</BODY>\n\n<COMMENTS>\n<Comment by guyuliang7 at 2024-03-26T02:51:28Z>\n这个评测 是GPU才可以评测吗\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1332,
    "state": "open",
    "created_by": "jeevanmac011769",
    "created_at": "2024-03-25T04:59:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1332</URL>\n\n<TITLE>NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE. (error_code: 1)</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: I am encountering this error when i launch my model and run it in the gradio.\r\nNETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE. (error_code: 1)\r\nplease help me.\r\n\r\n\r\nScreenshots:\r\nI have attached the screenshots here\r\n![Screenshot 2024-03-22 120004](https://github.com/haotian-liu/LLaVA/assets/149673062/3c3be544-4be3-4fae-99aa-359fbe5c11d7)\r\n![Screenshot 2024-03-25 102401](https://github.com/haotian-liu/LLaVA/assets/149673062/cc697e9d-3a90-491e-a1d4-6940d190bd60)\r\n![Screenshot 2024-03-25 102423](https://github.com/haotian-liu/LLaVA/assets/149673062/4c59800a-1979-4888-a2cd-a4cb292e4028)\r\n![Screenshot 2024-03-25 102457](https://github.com/haotian-liu/LLaVA/assets/149673062/66249e31-90d2-4ed7-b165-ea4fc89795e3)</BODY>\n\n<COMMENTS>\n<Comment by jiuge4010 at 2024-04-09T06:13:48Z>\nI am encountering this error,please are you solve this problem?\n</Comment>\n<Comment by samuruph at 2024-04-09T09:07:04Z>\nI am having the same issue for now\n</Comment>\n<Comment by foundwant at 2024-04-24T07:23:40Z>\nsame error! the latest code should have bugs, step by the readme, will meet this bug always.\n</Comment>\n<Comment by caixiaoniweimar at 2024-05-13T09:22:06Z>\nsame\n</Comment>\n<Comment by seasoncool at 2024-05-16T22:56:40Z>\nsame +1\n</Comment>\n<Comment by Z500-RAY at 2024-05-20T03:54:49Z>\nsame +1\n</Comment>\n<Comment by LivioGama at 2024-05-30T01:00:26Z>\nI have the same error. Gonna try to dig. Does anyone know if older versions were working?\n</Comment>\n<Comment by LivioGama at 2024-06-01T07:22:38Z>\nAlright guys, after investigating a bit, I think I understood the problem. Although the error message is not very clear, it simply look like you cannot run and have the workers working on a VPS without GPU. I was assuming that GPU was a plus and it would run on the rest in \"best effort\" mode, but it's unlikely.... The error message was laying there since the beginning :/ and I thought it was a warning.\r\n![CleanShot 2024-06-01 at 09 21 12](https://github.com/haotian-liu/LLaVA/assets/6930498/5d3a0249-0042-4f1d-a0c7-79e067e585f3)\n</Comment>\n<Comment by shirley-wu at 2024-07-23T08:25:44Z>\nHi, for me the problem is resolved by running gradio demo **AFTER** the backend is setup, rather than in the second step right after controller is launched as in README. So my actually order would be\r\n```bash\r\npython -m llava.serve.controller --host 0.0.0.0 --port 10000\r\nCUDA_VISIBLE_DEVICES=0,1 python3 -m sglang.launch_server --model-path liuhaotian/llava-v1.6-34b --tokenizer-path liuhaotian/llava-v1.6-34b-tokenizer --port 30000 --tp 2\r\npython -m llava.serve.sglang_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --sgl-endpoint http://127.0.0.1:30000\r\npython -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload --share\r\n```\n</Comment>\n<Comment by HAOYON-666 at 2024-10-14T03:36:48Z>\nI have a same error,please tell me how to deal with it ? thanks!\r\nNETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.\n</Comment>\n<Comment by jiajiao272727 at 2024-12-02T21:59:57Z>\nHi, I have the same issue, may I ask have you solved it yet?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1331,
    "state": "closed",
    "created_by": "nixsui",
    "created_at": "2024-03-25T03:47:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1331</URL>\n\n<TITLE>[Question]  about 1.6 training</TITLE>\n\n<BODY>First of all, thank you for sharing such excellent work and code. I have some questions about the training of 1.6. I tried to modify the code to train version 1.6 myself. I have some doubts about some details. \r\n1. Regarding the issue of resolution, I saw in the report that it has reached 672, is this dynamic high resolution? \r\n<img width=\"929\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/42858061/fe782376-2042-4dbc-94fc-a7cd0c25d95d\">\r\n\r\n2. I noticed in the report that the full model was fine-tuned in the second stage. Does this include the image encoder?\r\n<img width=\"671\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/42858061/b0df2588-368d-45e2-a8dd-cc1eded8bcd8\"></BODY>\n\n<COMMENTS>\n<Comment by nixsui at 2024-03-25T10:13:15Z>\n3. Was dynamic high resolution also used in the pretraining stage?\r\n![image](https://github.com/haotian-liu/LLaVA/assets/42858061/0c22c068-12b6-4974-9b71-f0cc4fd88832)\n</Comment>\n<Comment by nixsui at 2024-03-28T05:45:17Z>\n1、During my debugging, I found that the token length of dynamic high resolution has reached 2000+, so I believe the authors did not use a larger resolution.\r\n2、I looked at the weights of LLaVA 1.6 and saw that the config includes the learning rate of the vision encoder, so I believe that it includes the vision encoder here.\r\n3、I will conduct the experiment for verification and synchronize the results afterward.\n</Comment>\n<Comment by BlueBlueFF at 2024-04-08T08:18:19Z>\n> 1、During my debugging, I found that the token length of dynamic high resolution has reached 2000+, so I believe the authors did not use a larger resolution. 2、I looked at the weights of LLaVA 1.6 and saw that the config includes the learning rate of the vision encoder, so I believe that it includes the vision encoder here. 3、I will conduct the experiment for verification and synchronize the results afterward.\r\n\r\nAny update？\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1330,
    "state": "open",
    "created_by": "Bobby-youngking",
    "created_at": "2024-03-25T02:19:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1330</URL>\n\n<TITLE>is llava-1.6 version can be finetuned? Or the scripts to finetune v1.6 is released or not ?</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1329,
    "state": "open",
    "created_by": "fisher75",
    "created_at": "2024-03-24T16:46:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1329</URL>\n\n<TITLE>[Question] How to implement an Adapter or P-Tuning into LLava? 类似P-Tuning或者Adapter这种微调操作怎么加入Llava中？</TITLE>\n\n<BODY>### Question\n\nHow to implement an Adapter or P-Tuning into LLava? Any example?\r\n\r\n类似P-Tuning或者Adapter这种微调操作怎么加入Llava中？有没有傻瓜式的例子可以参考？</BODY>\n\n<COMMENTS>\n<Comment by zhipeixu at 2024-06-01T09:38:17Z>\nI have the same problem. Have you found a solution?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1328,
    "state": "open",
    "created_by": "deadpipe",
    "created_at": "2024-03-24T12:54:50Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1328</URL>\n\n<TITLE>[Usage] CPU offloading \"llm_int8_enable_fp32_cpu_offload = True\"</TITLE>\n\n<BODY>### Describe the issue\n\nI am trying to load the **llava-1.6-vicuna-13b** on RTX 3060 12GB, using [8bit with CPU offloading.\r\n](https://huggingface.co/docs/transformers/main/en/main_classes/quantization#transformers.BitsAndBytesConfig.llm_int8_enable_fp32_cpu_offload)\r\n\r\nI added the parameter \"llm_int8_enable_fp32_cpu_offload = True\" in [builder.py at line 26](https://github.com/haotian-liu/LLaVA/blob/7440ec9ee37b0374c6b5548818e89878e38f3353/llava/model/builder.py#L26).\r\n\r\nAnd When I try to load the model, I get this error :\r\n\r\n```\r\nLoading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:21<00:00,  3.65s/it]\r\nUSER: hi\r\nTraceback (most recent call last):\r\n  File \"/home/badapadda/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/badapadda/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/badapadda/llava/llava/serve/cli.py\", line 134, in <module>\r\n    main(args)\r\n  File \"/home/badapadda/llava/llava/serve/cli.py\", line 104, in main\r\n    output_ids = model.generate(\r\n  File \"/home/badapadda/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/badapadda/llava/llava/model/language_model/llava_llama.py\", line 125, in generate\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/home/badapadda/llava/llava/model/llava_arch.py\", line 157, in prepare_inputs_labels_for_multimodal\r\n    image_features = self.encode_images(concat_images)\r\n  File \"/home/badapadda/llava/llava/model/llava_arch.py\", line 141, in encode_images\r\n    image_features = self.get_model().get_vision_tower()(images)\r\n  File \"/home/badapadda/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/badapadda/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/badapadda/.local/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/badapadda/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/badapadda/llava/llava/model/multimodal_encoder/clip_encoder.py\", line 54, in forward\r\n    image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\r\n  File \"/home/badapadda/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/badapadda/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/badapadda/.local/lib/python3.10/site-packages/accelerate/hooks.py\", line 160, in new_forward\r\n    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\r\n  File \"/home/badapadda/.local/lib/python3.10/site-packages/accelerate/hooks.py\", line 290, in pre_forward\r\n    return send_to_device(args, self.execution_device), send_to_device(\r\n  File \"/home/badapadda/.local/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 151, in send_to_device\r\n    return honor_type(\r\n  File \"/home/badapadda/.local/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 83, in honor_type\r\n    return type(obj)(generator)\r\n  File \"/home/badapadda/.local/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 152, in <genexpr>\r\n    tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\r\n  File \"/home/badapadda/.local/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 167, in send_to_device\r\n    return tensor.to(device, non_blocking=non_blocking)\r\nNotImplementedError: Cannot copy out of meta tensor; no data!\r\n```\r\n\r\n\r\n\r\n**I have tried loading in 4bit but I get Cuda OOM error.**\r\n\r\n**I am thinking since I have plenty of System RAM (32 GB) so offloading Weights to CPU can make the 13billion run in[ 8bit with CPU offloading](https://huggingface.co/docs/transformers/main/en/main_classes/quantization#transformers.BitsAndBytesConfig.llm_int8_enable_fp32_cpu_offload) on a 3060 12GB**\r\n\r\n\r\nSystem Info:\r\ni7-7700k\r\nRTX 3060 12GB\r\n32GB RAM\r\nWSL on Windows 10, and a dual booted Ubuntu 22.04 (I have tried on both)\r\n\r\n\r\n\r\nCan you please suggest a possible fix or a workaround?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1327,
    "state": "open",
    "created_by": "danielje679",
    "created_at": "2024-03-24T11:04:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1327</URL>\n\n<TITLE>[Question] Which base model to use for finetuning on vqav2 dataset.</TITLE>\n\n<BODY>### Question\n\nHi,\r\nI want to finetune the model specifically on the the vqav2 dataset and want to compare it to a zero-shot result. However, as I understood, the llava model has already been finetuned on the dataset during the instruction tuning step. \r\n\r\nSo I am wondering, which base model should I best use to finetune on this data set and for comparison with just a pretrained model? Should I use one of the projectors as a base, that are mentioned in the [MODEL_ZOO.md](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md)? Or what are they used for?\r\n\r\nThanks in advance!</BODY>\n\n<COMMENTS>\n<Comment by Minato-Zackie at 2024-06-07T03:54:08Z>\nHello, if you want to finetune llava-1.5-7B, you can choose [vicuna-7B-1.5 ](https://huggingface.co/lmsys/vicuna-7b-v1.5)as the base model. For the projector weights, you can choose [llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5](https://huggingface.co/liuhaotian/llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5)\n</Comment>\n<Comment by zhw0516 at 2025-05-11T10:09:48Z>\n@danielje679 \nHi, do you know the zero-shot results on the VQA v2 dataset? I’d appreciate it if you could share any related information or references. Thanks in advance!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1326,
    "state": "open",
    "created_by": "xushilin1",
    "created_at": "2024-03-24T07:02:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1326</URL>\n\n<TITLE>[Usage] When will the eval script for llava v1.6 be available?</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: I cannot get the result \r\n\r\nCommand:\r\n```\r\nsh scripts/v1_5/eval/textvqa.sh checkpoints/llava-v1.6-vicuna-7b\r\n```\r\n\r\nI received 61.35 instead of the 64.9 you reported. Is there a difference between v1.6 and v1.5 during evaluation?\r\n\r\nPlease release the evaluation scripts promptly.</BODY>\n\n<COMMENTS>\n<Comment by ywh187 at 2024-06-17T15:40:21Z>\n61.35 too\n</Comment>\n<Comment by ymy-k at 2024-10-30T01:55:52Z>\n61.4 too\n</Comment>\n<Comment by ymy-k at 2024-10-30T11:58:07Z>\nIn model_vqa_loader.py, class CustomDataset, it should be modified as this: \r\n```\r\nif self.model_config.mm_use_im_start_end:\r\n    qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + qs\r\nelse:\r\n    qs = DEFAULT_IMAGE_TOKEN + '\\n' + qs\r\nqs = qs.split('\\n')  # [DEFAULT_IMAGE_TOKEN, question, ocr token, format prompt]\r\nqs = '\\n'.join([qs[0], qs[1], qs[3]])\r\n```\r\nThen, I raise the score from 61.4 to 64.7 on 3090, from 61.3 to 64.9 on A100.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1324,
    "state": "open",
    "created_by": "rak55",
    "created_at": "2024-03-23T07:28:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1324</URL>\n\n<TITLE>[Usage] problem with process_images function in then mm_utils.py</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue: The function is not returning tensors for some images (based on size). I have two images (one for demonstration and one query image) and I get different number of patches for each image. As a result, a list is returned instead of a tensor from torch.stack function. How do I resolve this?\r\n\r\n\r\nLog: \r\n\r\nAttributeError: 'list' object has no attribute 'to'</BODY>\n\n<COMMENTS>\n<Comment by SakuraTroyChen at 2024-04-16T03:15:15Z>\nI have the same problem too.\n</Comment>\n<Comment by HuangZhen02 at 2024-04-27T04:07:42Z>\nsame problem\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1322,
    "state": "open",
    "created_by": "20191864218",
    "created_at": "2024-03-22T15:57:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1322</URL>\n\n<TITLE>[Question] 运行cli.py文件时出现ValueError: `bos_token_id` has to be defined when no `input_ids` are provided.</TITLE>\n\n<BODY>### Question\n\n我在运行cli.py文件进行推理时，出现以下错误：\r\n`Traceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/root/miniconda3/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/root/LLaVA/llava/serve/cli.py\", line 137, in <module>\r\n    main(args)\r\n  File \"/root/LLaVA/llava/serve/cli.py\", line 106, in main\r\n    output_ids = model.generate(\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/root/LLaVA/llava/model/language_model/llava_Taiyi.py\", line 133, in generate\r\n    return super().generate(\r\n  File \"/root/LLaVA/llava/model/language_model/Taiyi/modeling_qwen.py\", line 1111, in generate\r\n    return super().generate(\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1330, in generate\r\n    inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py\", line 402, in _prepare_model_inputs\r\n    model_kwargs[\"input_ids\"] = self._maybe_initialize_input_ids_for_generation(\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py\", line 431, in _maybe_initialize_input_ids_for_generation\r\n    raise ValueError(\"`bos_token_id` has to be defined when no `input_ids` are provided.\")\r\nValueError: `bos_token_id` has to be defined when no `input_ids` are provided.`\r\n\r\n但是input_ids打印出来长这个样子\r\n![image](https://github.com/haotian-liu/LLaVA/assets/56297762/3a91b508-a75b-4ac6-8953-488d581c2cc6)\r\n\r\n\r\ncli.py文件内容为：\r\n`import argparse\r\nimport torch\r\n\r\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\r\nfrom llava.conversation import conv_templates, SeparatorStyle\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.utils import disable_torch_init\r\nfrom llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\r\n\r\nfrom PIL import Image\r\n\r\nimport requests\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\nfrom transformers import TextStreamer\r\n\r\n\r\ndef load_image(image_file):\r\n    if image_file.startswith('http://') or image_file.startswith('https://'):\r\n        response = requests.get(image_file)\r\n        image = Image.open(BytesIO(response.content)).convert('RGB')\r\n    else:\r\n        image = Image.open(image_file).convert('RGB')\r\n    return image\r\n\r\n\r\ndef main(args):\r\n    # Model\r\n    disable_torch_init()\r\n\r\n    model_name = get_model_name_from_path(args.model_path)\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n    if \"llama-2\" in model_name.lower():\r\n        conv_mode = \"llava_llama_2\"\r\n    elif \"mistral\" in model_name.lower():\r\n        conv_mode = \"mistral_instruct\"\r\n    elif \"v1.6-34b\" in model_name.lower():\r\n        conv_mode = \"chatml_direct\"\r\n    elif \"v1\" in model_name.lower():\r\n        conv_mode = \"llava_v1\"\r\n    elif \"mpt\" in model_name.lower():\r\n        conv_mode = \"mpt\"\r\n    else:\r\n        conv_mode = \"llava_v0\"\r\n\r\n    if args.conv_mode is not None and conv_mode != args.conv_mode:\r\n        print('[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}'.format(conv_mode, args.conv_mode, args.conv_mode))\r\n    else:\r\n        args.conv_mode = conv_mode\r\n\r\n    conv = conv_templates[args.conv_mode].copy()\r\n    if \"mpt\" in model_name.lower():\r\n        roles = ('user', 'assistant')\r\n    else:\r\n        roles = conv.roles\r\n\r\n    image = load_image(args.image_file)\r\n    image_size = image.size\r\n    # Similar operation in model_worker.py\r\n    image_tensor = process_images([image], image_processor, model.config)\r\n    if type(image_tensor) is list:\r\n        image_tensor = [image.to(model.device, dtype=torch.float16) for image in image_tensor]\r\n    else:\r\n        image_tensor = image_tensor.to(model.device, dtype=torch.float16)\r\n\r\n    while True:\r\n        try:\r\n            inp = input(f\"{roles[0]}: \")\r\n        except EOFError:\r\n            inp = \"\"\r\n        if not inp:\r\n            print(\"exit...\")\r\n            break\r\n\r\n        print(f\"{roles[1]}: \", end=\"\")\r\n\r\n        if image is not None:\r\n            # first message\r\n            if model.config.mm_use_im_start_end:\r\n                inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + inp\r\n            else:\r\n                inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\r\n            conv.append_message(conv.roles[0], inp)\r\n            image = None\r\n        else:\r\n            # later messages\r\n            conv.append_message(conv.roles[0], inp)\r\n        conv.append_message(conv.roles[1], None)\r\n        prompt = conv.get_prompt()\r\n        print('prompt:', prompt)\r\n\r\n        input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)\r\n        print('input_ids:', input_ids)\r\n        print('input_ids.shape:', input_ids.shape)\r\n        print('input_ids.shape[1]:', input_ids.shape[1])\r\n        # bos_token_id = torch.tensor([[tokenizer.bos_token_id]], dtype=torch.long).to(model.device)\r\n        # eos_token_id = torch.tensor([[tokenizer.eos_token_id]], dtype=torch.long).to(model.device)\r\n        # user_input_ids = torch.concat([bos_token_id,input_ids, eos_token_id], dim=1).to(model.device)\r\n        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\r\n        print('stop_str:', stop_str)\r\n        keywords = [stop_str]\r\n        stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\r\n        streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\r\n\r\n        with torch.inference_mode():\r\n            output_ids = model.generate(\r\n                input_ids,\r\n                images=image_tensor,\r\n                image_sizes=[image_size],\r\n                do_sample=True if args.temperature > 0 else False,\r\n                temperature=args.temperature,\r\n                max_new_tokens=args.max_new_tokens,\r\n                streamer=streamer,\r\n                stopping_criteria=[stopping_criteria])\r\n\r\n        outputs = tokenizer.decode(output_ids[0]).strip()\r\n        conv.messages[-1][-1] = outputs\r\n        \r\n        # 如果启用了调试模式（args.debug为真），则打印出提示和生成的输出，方便调试和检查模型的行为。\r\n        if args.debug:\r\n            print(\"\\n\", {\"prompt\": prompt, \"outputs\": outputs}, \"\\n\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--model-path\", type=str, default=\"facebook/opt-350m\")\r\n    parser.add_argument(\"--model-base\", type=str, default=None)\r\n    parser.add_argument(\"--image-file\", type=str, required=True)\r\n    parser.add_argument(\"--device\", type=str, default=\"cuda\")\r\n    parser.add_argument(\"--conv-mode\", type=str, default=None)\r\n    parser.add_argument(\"--temperature\", type=float, default=0.2)\r\n    parser.add_argument(\"--max-new-tokens\", type=int, default=512)\r\n    parser.add_argument(\"--load-8bit\", action=\"store_true\")\r\n    parser.add_argument(\"--load-4bit\", action=\"store_true\")\r\n    parser.add_argument(\"--debug\", action=\"store_true\")\r\n    args = parser.parse_args()\r\n    main(args)\r\n`\r\n请问各位大佬，为什么出现这个错误，感谢大佬的回复</BODY>\n\n<COMMENTS>\n<Comment by JosephPai at 2024-06-08T08:56:22Z>\nSame problem...\n</Comment>\n<Comment by TheSweetestGirlInTheUniverse at 2024-12-05T10:34:45Z>\nYou can try to use transformers==4.40.2 and tokenizer==0.19.0\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1321,
    "state": "open",
    "created_by": "dongdql",
    "created_at": "2024-03-22T10:11:35Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1321</URL>\n\n<TITLE>支持使用中文数据微调么[Question]</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1320,
    "state": "open",
    "created_by": "samuruph",
    "created_at": "2024-03-22T09:06:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1320</URL>\n\n<TITLE>Iterate and run inference over images independently [Usage]</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nI would like to run LlaVa independently over images. Do you think that simply removing \r\n`conv.messages[-1][-1] = outputs` would work? Or the model keeps state and memory in some other way?\r\nI have also thought about reinitialize the oncversation, maybe thta would be better or not adding the user message is enough?\r\n\r\nMoreover, I do not need the TextStreamer, since i will simply save json files), but apparently without the TextStream it is quite slower (I was expectring the opposite actually).</BODY>\n\n<COMMENTS>\n<Comment by xinghedyc at 2024-04-09T14:12:15Z>\n+1\n</Comment>\n<Comment by samuruph at 2024-04-12T10:00:51Z>\nI am keeping getting strange behavious even when not appending the previous answer, actually. How can i fix it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1319,
    "state": "open",
    "created_by": "yfpeng1234",
    "created_at": "2024-03-22T09:04:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1319</URL>\n\n<TITLE>[Usage] RuntimeError: CUDA error: device-side assert triggered</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\ntest llava 1.5 on the \"get start code\", but failed with \"RuntimeError: CUDA error: device-side assert triggered\"\r\nCommand:\r\n```\r\ndemo.py:\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.mm_utils import get_model_name_from_path\r\nfrom llava.eval.run_llava import eval_model\r\nimport os\r\n\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\r\n\r\nmodel_path = \"./ckpt/llava-v1.5-7b\"\r\nprompt = \"What are the things I should be cautious about when I visit here?\"\r\nimage_file = \"https://llava-vl.github.io/static/images/view.jpg\"\r\n\r\nargs = type('Args', (), {\r\n    \"model_path\": model_path,\r\n    \"model_base\": None,\r\n    \"model_name\": get_model_name_from_path(model_path),\r\n    \"query\": prompt,\r\n    \"conv_mode\": None,\r\n    \"image_file\": image_file,\r\n    \"sep\": \",\",\r\n    \"temperature\": 0,\r\n    \"top_p\": None,\r\n    \"num_beams\": 1,\r\n    \"max_new_tokens\": 512\r\n})()\r\n\r\neval_model(args)\r\n\r\npython demo.py\r\n```\r\n\r\nLog: \r\n```\r\n[2024-03-22 17:04:06,640] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nLoading checkpoint shards:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]/home/zlihm/anaconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.16s/it]\r\n/home/zlihm/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\r\n  warnings.warn(\r\n/home/zlihm/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\r\n  warnings.warn(\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [64,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [65,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [66,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [67,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [68,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [69,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [70,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [71,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [72,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [73,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [74,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [75,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [76,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [77,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [78,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [79,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [80,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [81,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [82,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [83,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [84,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [85,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [86,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [87,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [88,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [89,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [90,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [91,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [92,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [93,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [94,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [50,0,0], thread: [95,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\nTraceback (most recent call last):\r\n  File \"/home/zlihm/yf/LLaVA/demo.py\", line 27, in <module>\r\n    eval_model(args)\r\n  File \"/home/zlihm/yf/LLaVA/llava/eval/run_llava.py\", line 115, in eval_model\r\n    output_ids = model.generate(\r\n  File \"/home/zlihm/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/zlihm/yf/LLaVA/llava/model/language_model/llava_llama.py\", line 137, in generate\r\n    return super().generate(\r\n  File \"/home/zlihm/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/zlihm/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1479, in generate\r\n    return self.greedy_search(\r\n  File \"/home/zlihm/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2340, in greedy_search\r\n    outputs = self(\r\n  File \"/home/zlihm/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/zlihm/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/zlihm/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/zlihm/yf/LLaVA/llava/model/language_model/llava_llama.py\", line 91, in forward\r\n    return super().forward(\r\n  File \"/home/zlihm/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1183, in forward\r\n    outputs = self.model(\r\n  File \"/home/zlihm/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/zlihm/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/zlihm/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1070, in forward\r\n    layer_outputs = decoder_layer(\r\n  File \"/home/zlihm/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/zlihm/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/zlihm/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/zlihm/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 798, in forward\r\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n  File \"/home/zlihm/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/zlihm/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/zlihm/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/zlihm/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 706, in forward\r\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\r\n  File \"/home/zlihm/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 234, in apply_rotary_pos_emb\r\n    q_embed = (q * cos) + (rotate_half(q) * sin)\r\n  File \"/home/zlihm/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 208, in rotate_half\r\n    return torch.cat((-x2, x1), dim=-1)\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n```\r\n\r\nScreenshots:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/135833420/5bcff429-76a3-485b-811d-bacad3311588)</BODY>\n\n<COMMENTS>\n<Comment by linzy5 at 2024-03-25T03:43:22Z>\nI encounter the same problem when I run the llava-v1.6 scripts in huggingface. The problem occurs when I load the model with device_map='auto' and load_in_8bits. If I load the model to cuda:0, everything is fine. I cannot find solution for this problem too.\n</Comment>\n<Comment by yfpeng1234 at 2024-03-25T07:11:02Z>\nHi, I also found that only if I load model to a single GPU, the CLI inference and the \"get start code\" can run successfully. I guess this transcript only supports single GPU inference\n</Comment>\n<Comment by linzy5 at 2024-03-25T07:15:31Z>\n@yfpeng1234 Maybe....The “quickstart\" code use device_map=\"auto\", so I thought the model support multi-gpu inference...\n</Comment>\n<Comment by ShyFoo at 2024-03-28T06:50:07Z>\nHave you solved this issue? Hope for your help.\n</Comment>\n<Comment by yfpeng1234 at 2024-03-28T14:52:34Z>\nHi, I found that using a single GPU for inference can solve this problem, then also change your cuda version to 1.8\n</Comment>\n<Comment by ShyFoo at 2024-03-31T02:22:15Z>\n> Hi, I found that using a single GPU for inference can solve this problem, then also change your cuda version to 1.8\r\n\r\nThanks for your advice. I'll try it.\n</Comment>\n<Comment by bigbrother001 at 2025-02-20T13:33:32Z>\n> Hi, I found that using a single GPU for inference can solve this problem, then also change your cuda version to 1.8\n\nis it 11.8?\n</Comment>\n<Comment by yfpeng1234 at 2025-02-21T07:59:48Z>\n> > Hi, I found that using a single GPU for inference can solve this problem, then also change your cuda version to 1.8\n> \n> is it 11.8?\n\nyep! sorry for the typo\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1318,
    "state": "open",
    "created_by": "Vish2427",
    "created_at": "2024-03-22T08:49:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1318</URL>\n\n<TITLE>[Usage] Not able to import llava file while fine tunning</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nWhile running the fine tune script its not able to import the llava file when running the train_mem.py file.\r\nCommand:\r\n```\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --bits 4 \\\r\n    --deepspeed {DEEPSPEED_JSON} \\\r\n    --model_name_or_path {MODEL_NAME} \\\r\n    --version v1 \\\r\n    --data_path {DATA_PATH} \\\r\n    --image_folder {IMAGE_FOLDER} \\\r\n    --vision_tower {VISION_TOWER} \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir {OUTPUT_DIR} \\\r\n    --num_train_epochs 5 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 250 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\n\r\n\r\nScreenshots:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/98541876/cfac3c15-3e7d-4051-b186-1637974fc40a)</BODY>\n\n<COMMENTS>\n<Comment by Bobby-youngking at 2024-03-28T07:17:54Z>\nchange the import path, there are so many wrong paths in the code\n</Comment>\n<Comment by moaldeen at 2024-04-10T23:02:02Z>\n> change the import path, there are so many wrong paths in the code\r\ni finished finetuning but when i feed it the same image i used for fine tuning it still predict it wrong\n</Comment>\n<Comment by devsangho at 2024-06-27T03:01:23Z>\n```Shell\r\npip install -e .\r\n```\r\nYou can use the command as provided in Readme.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1317,
    "state": "closed",
    "created_by": "hollyaoyaozi",
    "created_at": "2024-03-22T07:09:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1317</URL>\n\n<TITLE>[Usage] Is there a python script to run infer ?</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nHi deer author，\r\n\r\nI have 2 questions:\r\n1) I found that there is a **predict.py** in the root dir and **run_llava.py** in LLaVA/llava/eval dir. I wonder which can be used to run infer ?\r\n2) Refering to the guide of installation on windows, when i ran 'pip install -e .'，pytorch 2.1.2 cpu version will be installed, which is conflicted with the step of installation of pytorch 2.0.1 gpu version (pip install torch==2.0.1+cu117 torchvision==0.15.2+cu117 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu117). So which version is required exactly ？\r\n\r\nThanks !</BODY>\n\n<COMMENTS>\n<Comment by prokaryote-hgy at 2024-05-09T13:05:45Z>\n@hollyaoyaozi Hi, which script did you use for inference?\n</Comment>\n<Comment by anas-zafar at 2024-09-10T21:16:33Z>\nHi @hollyaoyaozi @prokaryote-hgy did you figure out which script to use? Thanks\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1315,
    "state": "open",
    "created_by": "fisher75",
    "created_at": "2024-03-22T06:34:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1315</URL>\n\n<TITLE>[Question] In-Context-Learning for Batch Inference 上下文学习怎么批量推理？</TITLE>\n\n<BODY>### Question\n\n我看这个SGLang的例子中的questions的形式类似于zero-shot，请问我要做上下文学习有没有合适的例子呢？也就是我先给模型在之前的对话中提供多张图片并且分别告诉他们图片的含义，之后再让他推理另外的图片。这个功能怎么实现呢？\r\n\r\nI think the form of the questions in this SGLang example is similar to zero-shot. Is there any suitable example for context learning? That is, I first provide the model with multiple pictures in the previous conversation and tell them the meaning of the pictures, and then let it reason about other pictures. How to implement this function?</BODY>\n\n<COMMENTS>\n<Comment by ys-zong at 2024-07-14T14:48:10Z>\nYou could use our implemented codebase for in-context learning. https://github.com/ys-zong/VL-ICL\n</Comment>\n<Comment by ZihaoZheng98 at 2024-12-02T13:41:32Z>\n> You could use our implemented codebase for in-context learning. https://github.com/ys-zong/VL-ICL\r\n\r\n哈哈 老哥在llava的github里打广告 说明工作很硬，我follow一波。\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1313,
    "state": "open",
    "created_by": "sayedmohamedscu",
    "created_at": "2024-03-22T03:50:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1313</URL>\n\n<TITLE>[Usage]  can not apply inference with  a merged mistral model after finetune it</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nI have finetuned mistral llava model with a sample dataset and the training was well \r\n\r\nhere is the commands of training\r\n\r\n```\r\ndeepspeed llava/train/train_mem.py --deepspeed scripts/zero2.json\r\n --lora_enable True --lora_r 128 \r\n--lora_alpha 256 --mm_projector_lr 2e-5 --model_name_or_path liuhaotian/llava-v1.6-mistral-7b \r\n--version llava_llama_2 --data_path final_dataset/train/dataset.json --image_folder final_dataset/images/ \r\n--vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad \r\n--group_by_modality_length True --bf16 True --output_dir llama-2-7b-chat-task-qlora \r\n--num_train_epochs 50 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 \r\n--gradient_accumulation_steps 1 --evaluation_strategy \"no\"\r\n --save_strategy \"steps\" --save_steps 50 --save_total_limit 1 \r\n--learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 \r\n--lr_scheduler_type \"cosine\" --logging_steps 1 --tf32 True \r\n--model_max_length 2048 --gradient_checkpointing True \r\n--dataloader_num_workers 4 --lazy_preprocess True\r\n```\r\n\r\nafter merging using \r\n\r\n```\r\npython scripts/merge_lora_weights.py  --model-path llama-2-7b-chat-task-qlora/checkpoint-200 --model-base liuhaotian/llava-v1.6-mistral-7b --save-model-path outputfinalm\r\n```\r\n\r\noutput when apply inferecse with cli with the finetuned  model \r\n\r\n```\r\npython -m llava.serve.cli  --model-path 'outputfinalm'  --image-file \"1.jpg\"\r\n```\r\n\r\n```\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [02:11<00:00, 32.79s/it]\r\nTraceback (most recent call last):\r\n  File \"/home/sayed/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/sayed/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/mnt/e/llava2/LLaVA/llava/serve/cli.py\", line 128, in <module>\r\n    main(args)\r\n  File \"/mnt/e/llava2/LLaVA/llava/serve/cli.py\", line 61, in main\r\n    image_tensor = process_images([image], image_processor, model.config)\r\n  File \"/mnt/e/llava2/LLaVA/llava/mm_utils.py\", line 176, in process_images\r\n    image = process_anyres_image(image, image_processor, model_cfg.image_grid_pinpoints)\r\n  File \"/mnt/e/llava2/LLaVA/llava/mm_utils.py\", line 138, in process_anyres_image\r\n    patches = divide_to_patches(image_padded, processor.crop_size['height'])\r\nAttributeError: 'NoneType' object has no attribute 'crop_size'```\r\n\r\n```\r\n\r\nnote \r\n```\r\n\r\npython -m llava.serve.cli  --model-path 'liuhaotian/llava-v1.6-mistral-7b'  --image-file \"1.jpg\"\r\n\r\n```\r\nworks well \r\n\r\nI think the  problem in something during training process as I have used this \r\nis there anything wrong about this ?\r\n\r\n --model_name_or_path liuhaotian/llava-v1.6-mistral-7b \r\n--version llava_llama_2</BODY>\n\n<COMMENTS>\n<Comment by fisher75 at 2024-04-25T13:10:39Z>\nHi, how do you know the training was well? Did you use the default training setting? I LoRA with default parameters and basically no improvement.\n</Comment>\n<Comment by yiyiwwang at 2024-05-29T03:09:36Z>\n> ### Describe the issue\r\n> Issue: I have finetuned mistral llava model with a sample dataset and the training was well\r\n> \r\n> here is the commands of training\r\n> \r\n> ```\r\n> deepspeed llava/train/train_mem.py --deepspeed scripts/zero2.json\r\n>  --lora_enable True --lora_r 128 \r\n> --lora_alpha 256 --mm_projector_lr 2e-5 --model_name_or_path liuhaotian/llava-v1.6-mistral-7b \r\n> --version llava_llama_2 --data_path final_dataset/train/dataset.json --image_folder final_dataset/images/ \r\n> --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad \r\n> --group_by_modality_length True --bf16 True --output_dir llama-2-7b-chat-task-qlora \r\n> --num_train_epochs 50 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 \r\n> --gradient_accumulation_steps 1 --evaluation_strategy \"no\"\r\n>  --save_strategy \"steps\" --save_steps 50 --save_total_limit 1 \r\n> --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 \r\n> --lr_scheduler_type \"cosine\" --logging_steps 1 --tf32 True \r\n> --model_max_length 2048 --gradient_checkpointing True \r\n> --dataloader_num_workers 4 --lazy_preprocess True\r\n> ```\r\n> \r\n> after merging using\r\n> \r\n> ```\r\n> python scripts/merge_lora_weights.py  --model-path llama-2-7b-chat-task-qlora/checkpoint-200 --model-base liuhaotian/llava-v1.6-mistral-7b --save-model-path outputfinalm\r\n> ```\r\n> \r\n> output when apply inferecse with cli with the finetuned model\r\n> \r\n> ```\r\n> python -m llava.serve.cli  --model-path 'outputfinalm'  --image-file \"1.jpg\"\r\n> ```\r\n> \r\n> ```\r\n> Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [02:11<00:00, 32.79s/it]\r\n> Traceback (most recent call last):\r\n>   File \"/home/sayed/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n>     return _run_code(code, main_globals, None,\r\n>   File \"/home/sayed/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n>     exec(code, run_globals)\r\n>   File \"/mnt/e/llava2/LLaVA/llava/serve/cli.py\", line 128, in <module>\r\n>     main(args)\r\n>   File \"/mnt/e/llava2/LLaVA/llava/serve/cli.py\", line 61, in main\r\n>     image_tensor = process_images([image], image_processor, model.config)\r\n>   File \"/mnt/e/llava2/LLaVA/llava/mm_utils.py\", line 176, in process_images\r\n>     image = process_anyres_image(image, image_processor, model_cfg.image_grid_pinpoints)\r\n>   File \"/mnt/e/llava2/LLaVA/llava/mm_utils.py\", line 138, in process_anyres_image\r\n>     patches = divide_to_patches(image_padded, processor.crop_size['height'])\r\n> AttributeError: 'NoneType' object has no attribute 'crop_size'```\r\n> ```\r\n> \r\n> note\r\n> \r\n> ```\r\n> \r\n> python -m llava.serve.cli  --model-path 'liuhaotian/llava-v1.6-mistral-7b'  --image-file \"1.jpg\"\r\n> ```\r\n> \r\n> works well\r\n> \r\n> I think the problem in something during training process as I have used this is there anything wrong about this ?\r\n> \r\n> --model_name_or_path liuhaotian/llava-v1.6-mistral-7b --version llava_llama_2\r\n\r\nI have meet similar problem. I find the reason lies in the model name, try adding \"llava-\" in your merged model name, and then run again.\r\n\r\nWithout \"llava\" in the model name, the image processor will not be loaded.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/166595333/7a594704-c3e0-4b66-8e8c-ff31c1da7cbe)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1312,
    "state": "open",
    "created_by": "guoqianghao9",
    "created_at": "2024-03-22T03:28:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1312</URL>\n\n<TITLE>[Usage] deploy a llava model on AWS SageMaker use HuggingFaceModel</TITLE>\n\n<BODY>### Describe the issue\n\nimport:\r\n```\r\nimport sagemaker\r\nfrom sagemaker.huggingface.model import HuggingFaceModel \r\n```\r\ncode:\r\n```\r\nsagemaker_session = sagemaker.Session()\r\nrole = sagemaker.get_execution_role()\r\n\r\nhub = { \r\n  'HF_MODEL_ID' : \"llava-hf/llava-1.5-7b-hf\" , # model_id from hf.co/models \r\n  'HF_TASK' : 'image-to-text'                            # NLP task you want to use for predictions\r\n}\r\n\r\n# create Hugging Face Model Class\r\nhuggingface_model = HuggingFaceModel(\r\n   env=hub,                                                # configuration for loading model from Hub\r\n   role=role,                                              # IAM role with permissions to create an endpoint\r\n   transformers_version=\"4.26\",                             # Transformers version used\r\n   pytorch_version=\"1.13\",                                  # PyTorch version used\r\n   py_version='py39',                                      # Python version used\r\n)\r\n\r\n\r\n```\r\n\r\ndeploy model to SageMaker Inference\r\n```\r\npredictor = huggingface_model.deploy(\r\n   initial_instance_count=1,\r\n   instance_type=\"ml.m5.4xlarge\"\r\n)\r\n```\r\nrequest:\r\n```\r\ndata_url = {'inputs':'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'}\r\npredictor .predict(data_url)\r\n```\r\n\r\nerror:\r\n```\r\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message \"{\r\n  \"code\": 400,\r\n  \"type\": \"InternalServerException\",\r\n  \"message\": \"\\u0027llava\\u0027\"\r\n}\r\n\". See https://ap-southeast-1.console.aws.amazon.com/cloudwatch/home?region=ap-southeast-1#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-2024-03-16-14-04-39-322 in account 601804951188 for more information.\r\n```\r\n#Attempted Solutions\r\nConsulted\r\n[https://github.com/sungeuns/gen-ai-sagemaker/blob/main/MultiModal/02-llava-sagemaker-endpoint.ipynb](url)\r\n[https://github.com/haotian-liu/LLaVA/issues/600](url)\r\n[https://github.com/haotian-liu/LLaVA/issues/907](url)\r\n\r\n\r\n![图片](https://github.com/haotian-liu/LLaVA/assets/30927867/bcb1f9cc-0989-4453-82fe-20d0e06cd00c)</BODY>\n\n<COMMENTS>\n<Comment by guoqianghao9 at 2024-03-27T05:45:30Z>\n？\n</Comment>\n<Comment by mujammilk-servify at 2024-06-11T12:29:42Z>\ndid you get any solution for this?\n</Comment>\n<Comment by mujammilk-servify at 2024-06-12T05:24:16Z>\nhttps://huggingface.co/llava-hf/llava-v1.6-vicuna-13b-hf/discussions/3\r\nthis should help\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1310,
    "state": "open",
    "created_by": "Alexey234432",
    "created_at": "2024-03-21T16:24:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1310</URL>\n\n<TITLE>[Question] ONNX / TFLite export</TITLE>\n\n<BODY>### Question\n\nHi all, Does anyone know how can we get an onnx / TFLite model for further tests? Thank you.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1309,
    "state": "open",
    "created_by": "Yxxxb",
    "created_at": "2024-03-21T08:57:56Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1309</URL>\n\n<TITLE>[Usage] The checkpoints saved.</TITLE>\n\n<BODY>### Describe the issue\n\nI conducted further SFT on the officially provided pre-training model, but when I conducted many experiments, I found that sometimes the saved checkpoint content was missing. For example, in the following two sets of experiments, the former is more than the latter. added_tokens.json model.safetensors.index.json special_tokens_map.json tokenizer_config.json tokenizer.model training_args.bin. I know that there will be additional files saved after I add the special token, but the necessary files (such as tokenizer.model ) is missing, I'd like to know why, thank you.\r\n\r\n（我在官方所提供的预训练模型上进行进一步的SFT，但是我进行多次试验的时候发现有的时候保存的checkpoint的内容是有缺失的，例如下面这两组实验，其中前者比后者多了added_tokens.json model.safetensors.index.json special_tokens_map.json tokenizer_config.json tokenizer.model training_args.bin这些文件，我知道我增加了特殊token后会有额外的文件保存，但是必要的文件（例如tokenizer.model）是缺失的，我想知道这是为什么，感谢您。）\r\nckpt1\r\n![image](https://github.com/haotian-liu/LLaVA/assets/64196230/51fea75d-ed43-4a6a-87fa-761795b0d0fa)\r\nckpt2\r\n![image](https://github.com/haotian-liu/LLaVA/assets/64196230/8a1fde34-ff90-41d6-be55-479bf89d9498)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1308,
    "state": "open",
    "created_by": "abrahamhwj",
    "created_at": "2024-03-21T06:43:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1308</URL>\n\n<TITLE>[Usage] Training process interrupted without error log</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\ntry training on a A100 VM，interrupted without error log\r\n\r\nCommand:\r\n```\r\ndeepspeed --master_port=18001 --include localhost:0 \\\r\n    llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero1.json \\\r\n    --model_name_or_path /opt/tmp/project/LLaVA/models/llava-v1.5-7b \\\r\n    --version v1 \\\r\n    --data_path /opt/tmp/project/LLaVA/datasets/llava_v1_5_IVA_v1.json \\\r\n    --image_folder /opt/tmp/project/LLaVA/datasets \\\r\n    --vision_tower clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter /opt/tmp/project/LLaVA/models/llava-v1.5-7b/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length False \\\r\n    --bf16 False \\\r\n    --fp16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7B-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 8 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 200 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-6 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 False \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to none >./train_logs/llava-v1.5-7B-lora.log 2>&1 &```\r\n\r\nLog: \r\n```\r\n[llava-v1.5-7B-lora.log](https://github.com/haotian-liu/LLaVA/files/14687076/llava-v1.5-7B-lora.log)\r\n\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by ZhaoyangLi-nju at 2024-04-10T06:37:29Z>\n+1. Have you fixed this problem?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1307,
    "state": "open",
    "created_by": "rnjsspfld",
    "created_at": "2024-03-21T05:23:14Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1307</URL>\n\n<TITLE>[Usage] kwargs issue</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue: Since I don't have GPU access, I am running the LLaVa in google colab premium. \r\nfor **llava-v1.6-mistral-7b and llava-v1.6-vicuna-13b**,\r\n\r\nfrom transformers import AutoTokenizer, BitsAndBytesConfig\r\nfrom llava.model import LlavaLlamaForCausalLM\r\nimport torch\r\n\r\n#model_path = \"liuhaotian/llava-v1.6-mistral-7b\"\r\nmodel_path = \"liuhaotian/llava-v1.6-vicuna-13b\"\r\n#model_path = \"liuhaotian/llava-v1.6-34b\"\r\n\r\nkwargs = {\"device_map\": \"auto\"}\r\nkwargs['load_in_4bit'] = True\r\nkwargs['quantization_config'] = BitsAndBytesConfig(\r\n    load_in_4bit=True,\r\n    bnb_4bit_compute_dtype=torch.float16,\r\n    bnb_4bit_use_double_quant=True,\r\n    bnb_4bit_quant_type='nf4'\r\n)\r\nmodel = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\ntokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n\r\n**This code worked well.** \r\nHowever, when I changed the model to **\"liuhaotian/llava-v1.6-34b\"**\r\nI got this error.\r\n\r\nValueError    Traceback (most recent call last)\r\n<ipython-input-4-8a7283d83ee5> in <cell line: 19>()\r\n     17     bnb_4bit_quant_type='nf4'\r\n     18 )\r\n---> 19 model = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\n     20 tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n\r\n/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\r\n   2840                 }\r\n   2841                 if \"cpu\" in device_map_without_lm_head.values() or \"disk\" in device_map_without_lm_head.values():\r\n-> 2842                     raise ValueError(\r\n   2843                         \"\"\"\r\n   2844                         Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\r\n\r\nValueError: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check\r\n                  https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1306,
    "state": "open",
    "created_by": "gapjialin",
    "created_at": "2024-03-21T05:06:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1306</URL>\n\n<TITLE>[Usage] Question about fine-tuning Custom_Data</TITLE>\n\n<BODY>### Describe the issue\n\nHello, there is currently a self built dataset for 80K object detection, which is used to detect the position of objects in the image. The image size is 1920x1080. When I use this data for fine-tuning, it is difficult to detect the position of objects in the image. What is the problem? The machine I am using is 8xA40, and Lora is used for fine-tuning. The other training parameters remain unchanged.       My dataset example is as follows:      \r\nhuman: Verify if there is a presence of people in the image.\r\ngpt: There are 1 people.\r\nhuman: Pinpoint and describe the exact spots where each person can be found in this picture.\r\ngpt: person 1's bounding box coordinate of the region is [0.32, 0.65, 0.34, 0.75].</BODY>\n\n<COMMENTS>\n<Comment by tctrautman at 2024-03-21T17:53:12Z>\nIt sounds like you might need to resize your images -- take a look at \"Increasing the input image resolution\" improvement point in the LLaVA NeXT blog post: https://llava-vl.github.io/blog/2024-01-30-llava-next/\n</Comment>\n<Comment by gapjialin at 2024-03-24T12:54:55Z>\n> It sounds like you might need to resize your images -- take a look at \"Increasing the input image resolution\" improvement point in the LLaVA NeXT blog post: https://llava-vl.github.io/blog/2024-01-30-llava-next/\r\n\r\nThank you! I noticed that during fine-tuning, there is a description of the coordinates in the image in the dataset. Does this coordinate correspond to the image before or after compression resolution? I really want to know this question.\n</Comment>\n<Comment by Linziyang1999 at 2024-04-03T00:52:44Z>\n> > It sounds like you might need to resize your images -- take a look at \"Increasing the input image resolution\" improvement point in the LLaVA NeXT blog post: https://llava-vl.github.io/blog/2024-01-30-llava-next/\r\n> \r\n> Thank you! I noticed that during fine-tuning, there is a description of the coordinates in the image in the dataset. Does this coordinate correspond to the image before or after compression resolution? I really want to know this question.\r\n\r\nI think it is no different because you using range 0~1， just make true you using image_radio = spatial unpad.\n</Comment>\n<Comment by moaldeen at 2024-04-08T00:46:52Z>\n> > It sounds like you might need to resize your images -- take a look at \"Increasing the input image resolution\" improvement point in the LLaVA NeXT blog post: https://llava-vl.github.io/blog/2024-01-30-llava-next/\r\n> \r\n> Thank you! I noticed that during fine-tuning, there is a description of the coordinates in the image in the dataset. Does this coordinate correspond to the image before or after compression resolution? I really want to know this question.\r\n\r\n\r\nI want to ask you what image size or resolution did you end up doing and did it work.\n</Comment>\n<Comment by gapjialin at 2025-01-10T09:29:57Z>\n> > > It sounds like you might need to resize your images -- take a look at \"Increasing the input image resolution\" improvement point in the LLaVA NeXT blog post: https://llava-vl.github.io/blog/2024-01-30-llava-next/\r\n> > \r\n> > \r\n> > Thank you! I noticed that during fine-tuning, there is a description of the coordinates in the image in the dataset. Does this coordinate correspond to the image before or after compression resolution? I really want to know this question.\r\n> \r\n> I want to ask you what image size or resolution did you end up doing and did it work.\r\n\r\nI found that fine tuning revealed no detectable effect.\n</Comment>\n<Comment by Linziyang1999 at 2025-01-20T02:24:16Z>\n> > > > It sounds like you might need to resize your images -- take a look at \"Increasing the input image resolution\" improvement point in the LLaVA NeXT blog post: https://llava-vl.github.io/blog/2024-01-30-llava-next/\n> \n> > > \n> \n> > > \n> \n> > > Thank you! I noticed that during fine-tuning, there is a description of the coordinates in the image in the dataset. Does this coordinate correspond to the image before or after compression resolution? I really want to know this question.\n> \n> > \n> \n> > I want to ask you what image size or resolution did you end up doing and did it work.\n> \n> \n> \n> I found that fine tuning revealed no detectable effect.\n\nYou can check my repository Llava gui and vision gui assistant, which full finetune the llava1.6 and get promising grounding capability. I released my tuning scripts and code and dataset.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1305,
    "state": "open",
    "created_by": "siddharthshah52",
    "created_at": "2024-03-20T18:15:50Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1305</URL>\n\n<TITLE>[Usage] How to do batch image inference using run_llava and LLAVA 1.6 model?</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nWhen using multiple images with run_llava, many times the output for some images is empty text. When the image is run in isolation, the output obtained is correct. Firstly, run_llava currently uses only one input_id (correspondingly prompt) even if multiple images are provided instead of repeating the input_ids to match the number of images. On repeating the input_ids, I often get empty text as output for some of the images.\r\n\r\nCommand:\r\n```\r\npython3 run_llava -- --model-path models/llava-v1.6-34b --image-file test.jpg,test2.jpg,test3.jpg --query \"Describe this image and its style in a very detailed manner.\" --load-4bit\r\n```</BODY>\n\n<COMMENTS>\n<Comment by ZichengDuan at 2024-04-03T12:55:53Z>\nsame\n</Comment>\n<Comment by xinghedyc at 2024-04-09T14:57:04Z>\nsame problem\n</Comment>\n<Comment by Starry-lei at 2024-04-14T15:51:05Z>\nsame problem\n</Comment>\n<Comment by lukashelff at 2024-04-16T14:28:23Z>\nsame problem here\n</Comment>\n<Comment by alwaysPKU at 2024-04-23T13:48:56Z>\nsame\n</Comment>\n<Comment by collinmccarthy at 2024-09-28T19:52:32Z>\nSee #1720 for the fix for this.\n</Comment>\n<Comment by Juny-Chen at 2024-12-20T13:26:56Z>\nsame\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1304,
    "state": "open",
    "created_by": "Jintao-Huang",
    "created_at": "2024-03-20T09:34:12Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1304</URL>\n\n<TITLE>[Discussion] ms-swift对于Llava系列微调(finetune)的支持</TITLE>\n\n<BODY>### Discussion\n\nLlava仓库本身已经提供了非常优秀的微调脚本.\r\n\r\nms-swift多模态大模型微调框架集成了Llava的推理与微调, 并书写了最佳实践: https://github.com/modelscope/swift/blob/main/docs/source/Multi-Modal/llava%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md\r\n\r\n如果有感兴趣的小伙伴, 可以来使用😊\r\n\r\nThe Llava repository itself provides excellent fine-tuning scripts. \r\n\r\nThe ms-swift multi-modal large model fine-tuning framework integrates Llava's inference and fine-tuning and documents best practices: https://github.com/modelscope/swift/blob/main/docs/source/Multi-Modal/llava%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md\r\n\r\nIf any interested individuals, feel free to use it! 😊</BODY>\n\n<COMMENTS>\n<Comment by wxxu-cnic at 2024-10-14T12:49:06Z>\n支持多图微调和推理吗\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1303,
    "state": "closed",
    "created_by": "bjzhb666",
    "created_at": "2024-03-20T09:17:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1303</URL>\n\n<TITLE>[Question] Can we use a higher torch version?</TITLE>\n\n<BODY>### Question\n\n[Question] Can we use a higher torch version? (like 2.2.0) Thanks.</BODY>\n\n<COMMENTS>\n<Comment by bjzhb666 at 2024-03-24T03:27:51Z>\nYES\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1301,
    "state": "closed",
    "created_by": "QAQdev",
    "created_at": "2024-03-20T03:10:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1301</URL>\n\n<TITLE>[Question] Loss suddenly increases during fine-tuning with LoRA</TITLE>\n\n<BODY>### Question\n\n<img width=\"2190\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/73159641/c95195b5-92a5-4bb9-a682-b25c613bba45\">\r\nHi, community, I encountered a strange sudden increase of loss during fine-tuning llava-v1.5-7b with LoRA, using my custom dataset. This case does not happen when I set the epoch to 1 or 2.\r\nI would like to know the possible reasons for this happening, thanks for help in advance.</BODY>\n\n<COMMENTS>\n<Comment by The-kamisato at 2024-03-30T07:04:34Z>\nDo you find out why? I am faced with the same problem\n</Comment>\n<Comment by QAQdev at 2024-03-30T07:24:34Z>\n> Do you find out why? I am faced with the same problem\r\n\r\nyeah, roughly because of learning_rate, i decreased it by 10x lower\n</Comment>\n<Comment by bang123-box at 2024-10-21T11:32:10Z>\nhello, when we decrease the lr, how dose the performance change?\n</Comment>\n<Comment by QAQdev at 2024-10-21T11:35:09Z>\n> hello, when we decrease the lr, how dose the performance change?\r\n\r\nHi, I remembered that the performance increased because the loss looked much better\n</Comment>\n<Comment by HangerYang at 2025-03-05T00:33:36Z>\nDid you decrease the learning rate for just the llm, or both llm and the projector? Thanks!\n</Comment>\n<Comment by euyis1019 at 2025-04-03T10:14:02Z>\nCan you guys share the learning rate strategy? I encounter the similar question now. Thanks a lot!!!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1300,
    "state": "closed",
    "created_by": "siddharthshah52",
    "created_at": "2024-03-19T22:18:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1300</URL>\n\n<TITLE>[Usage] LLAVA 1.6 inference doesn't work when the best resolution is not a multiple of 224</TITLE>\n\n<BODY>Issue:\r\nI ran the run_llava script with an image of dimensions 640 x 1316. The best resolution selected by the [select_best_resolution](https://github.com/haotian-liu/LLaVA/blob/7440ec9ee37b0374c6b5548818e89878e38f3353/llava/mm_utils.py#L12) function is (336, 1008). The supported resolutions for LLAVA 1.6 can be found in it's [config.json](https://huggingface.co/liuhaotian/llava-v1.6-34b/blob/main/config.json#L16-L37) file. However, this results in tensor mismatch errors. This is due to mismatch in [divide_to_patches](https://github.com/haotian-liu/LLaVA/blob/7440ec9ee37b0374c6b5548818e89878e38f3353/llava/mm_utils.py#L77) and [get_anyres_image_grid_shape](https://github.com/haotian-liu/LLaVA/blob/7440ec9ee37b0374c6b5548818e89878e38f3353/llava/mm_utils.py#L99) functions. Since, 336 and 1008 are not perfectly divisible by CLIPImageProcessor's 224 patch size, the divide_to_patches produces a patch at the edge with padding done by PIL Image crop function. However, the get_anyres_image_grid_shape function takes the floor value on number of patches in width and height thus giving tensor mismatch error during inference. I tried two things (separately):\r\n\r\n1. Take ceil instead of floor in get_anyres_image_grid_shape function. However, this returned empty text.\r\n2. Don't take the patch at the edge in divide_to_patches. However, this returned gibberish text.\r\n\r\nRunning inference on square images (or when best resolution is square) works fine.\r\n\r\nCommand:\r\n```\r\npython3 run_llava -- --model-path models/llava-v1.6-34b --image-file test.jpg --query \"Describe this image and its style in a very detailed manner.\" --load-4bit\r\n```\r\n\r\nLog: \r\n```\r\nRuntimeError: The expanded size of the tensor (17920) must match the existing size (7168) at non-singleton dimension 0.  Target sizes: [17920, 32, 1].  Tensor sizes: [7168, 1, 1]\r\n```</BODY>\n\n<COMMENTS>\n<Comment by siddharthshah52 at 2024-03-20T04:27:30Z>\nBoth of my solutions work. Just that the observed empty text or gibberish text seems to happen when using batch size > 1. This happens irrespective of the resolutions of the images. Will open a separate issue for this.\n</Comment>\n<Comment by boyugou at 2024-03-25T01:23:48Z>\nisn't LLaVa 1.6 using 336 to patchify? https://huggingface.co/liuhaotian/llava-v1.6-34b/blob/main/config.json#L16-L37:~:text=1008%2C,336\r\n\r\nLLaVA-1.5-HD used 224\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1299,
    "state": "open",
    "created_by": "hylq66",
    "created_at": "2024-03-19T13:24:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1299</URL>\n\n<TITLE>[Question]How to get image and text features in batch</TITLE>\n\n<BODY>### Question\n\nHello, what part of the code should I change if I want to use your model to generate image and text features for me in batches.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1298,
    "state": "closed",
    "created_by": "fisher75",
    "created_at": "2024-03-19T11:48:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1298</URL>\n\n<TITLE>如何批量化推理 How to implement batch reasoning in LLava? [Question]</TITLE>\n\n<BODY>### Question\n\nI recently used LLava to do batch inference, using thousands of images in my data set, and then recording the results. Is this functionality already available in llava? If not, what part of the code should I modify in llava? I feel like where should I write a for loop?</BODY>\n\n<COMMENTS>\n<Comment by bjzhb666 at 2024-03-20T09:39:25Z>\nSame question, how to solve it?\n</Comment>\n<Comment by fisher75 at 2024-03-20T09:40:57Z>\n> Same question, how to solve it?\r\n\r\nTo my knowledge should use SGLang, but don't know about details...\n</Comment>\n<Comment by fisher75 at 2024-03-20T09:42:02Z>\n> Same question, how to solve it?\r\n\r\nbtw bro, could leave a WeChat? can discuss\n</Comment>\n<Comment by bjzhb666 at 2024-03-20T09:46:24Z>\nok, wxid_b4d5n9oqkr2f22\n</Comment>\n<Comment by merrymercy at 2024-03-27T01:36:13Z>\nhttps://github.com/sgl-project/sglang/tree/main/benchmark/llava_bench\n</Comment>\n<Comment by fisher75 at 2024-03-28T07:32:34Z>\n> https://github.com/sgl-project/sglang/tree/main/benchmark/llava_bench\r\n\r\n我想做一个多轮对话的批量推理，请问格式应该是什么样子的呢？我做这个的目的是为了实现某种CoT的思想，请问可行吗？\n</Comment>\n<Comment by merrymercy at 2024-03-28T22:53:14Z>\n可行。按照这个格式写多轮对话的SGLang function https://github.com/sgl-project/sglang/blob/2af565b3bb22cb8ba06acc17a2bbfa8d0ade0145/examples/quick_start/srt_example_chat.py#L10-L13\n</Comment>\n<Comment by fisher75 at 2024-03-29T04:09:31Z>\n> 可行。按照这个格式写多轮对话的SGLang function https://github.com/sgl-project/sglang/blob/2af565b3bb22cb8ba06acc17a2bbfa8d0ade0145/examples/quick_start/srt_example_chat.py#L10-L13\r\n\r\n感谢回复啊老哥，但是我看了之后觉得可能你误解了，你贴的代码我看是给LLM的多轮对话推理用的，而我想做的是用VLM的能力也就是用LLava，具体来说就是我想知道假如可以用多轮对话的话，在https://github.com/sgl-project/sglang/tree/main/benchmark/llava_bench 中提供的questions的jsonl应该以什么形式怎么改？\r\n例如怎么把这个改成多轮对话：\r\n`{\"image\": \"001.jpg\", \"text\": \"What is the name of this famous sight in the photo?\", \"category\": \"conv\", \"question_id\": 0}`\n</Comment>\n<Comment by zeyuwang-zju at 2024-04-26T05:10:36Z>\n你好，如果我想对一批图片进行推理，但是每个图片只需要一个问题，问题都是一样的，这样的话还可以吗\n</Comment>\n<Comment by fisher75 at 2024-04-26T05:34:48Z>\n> 你好，如果我想对一批图片进行推理，但是每个图片只需要一个问题，问题都是一样的，这样的话还可以吗\r\n\r\n肯定可以，这个听起来是最简单的\n</Comment>\n<Comment by zeyuwang-zju at 2024-04-26T06:31:10Z>\n好的，已经解决了，谢谢！\n\n2024-04-26 13:35:09 \"Haozhuang Chi\" ***@***.***> 写道：\n\n你好，如果我想对一批图片进行推理，但是每个图片只需要一个问题，问题都是一样的，这样的话还可以吗\n\n肯定可以，这个听起来是最简单的\n\n—\nReply to this email directly, view it on GitHub, or unsubscribe.\nYou are receiving this because you commented.Message ID: ***@***.***>\n</Comment>\n<Comment by gehong-coder at 2024-05-04T02:31:12Z>\n> 好的，已经解决了，谢谢！ 2024-04-26 13:35:09 \"Haozhuang Chi\" ***@***.***> 写道： 你好，如果我想对一批图片进行推理，但是每个图片只需要一个问题，问题都是一样的，这样的话还可以吗 肯定可以，这个听起来是最简单的 — Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you commented.Message ID: ***@***.***>\r\n\r\n这个实现已经有了吗，我的需求和你一样，请求支援，感谢\n</Comment>\n<Comment by hadolop at 2024-05-09T06:30:26Z>\n> 你好，如果我想对一批图片进行推理，但是每个图片只需要一个问题，问题都是一样的，这样的话还可以吗\r\n\r\n同样的问题，请教如何实现？\n</Comment>\n<Comment by fengluo233 at 2024-05-19T05:11:33Z>\n> > 你好，如果我想对一批图片进行推理，但是每个图片只需要一个问题，问题都是一样的，这样的话还可以吗\r\n> \r\n> 同样的问题，请教如何实现？\r\n\r\nHello, I got the same question! Have you already solved it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1296,
    "state": "open",
    "created_by": "20191864218",
    "created_at": "2024-03-18T08:40:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1296</URL>\n\n<TITLE>Encountered error during Inference</TITLE>\n\n<BODY>### Question\n\n``After replacing both LLM and vision encoder, I proceeded to pretrain and finetune_lora the model. However, when I attempted to perform model inference following the steps, I encountered this error:\r\n\r\n2024-03-18 15:19:15 | ERROR | stderr | Exception in thread Thread-6 (generate):\r\n2024-03-18 15:19:15 | ERROR | stderr | Traceback (most recent call last):\r\n2024-03-18 15:19:15 | ERROR | stderr |   File \"/root/miniconda3/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\r\n2024-03-18 15:19:15 | ERROR | stderr |     self.run()\r\n2024-03-18 15:19:15 | ERROR | stderr |   File \"/root/miniconda3/lib/python3.10/threading.py\", line 953, in run\r\n2024-03-18 15:19:15 | ERROR | stderr |     self._target(*self._args, **self._kwargs)\r\n2024-03-18 15:19:15 | ERROR | stderr |   File \"/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n2024-03-18 15:19:15 | ERROR | stderr |     return func(*args, **kwargs)\r\n2024-03-18 15:19:15 | ERROR | stderr |   File \"/root/LLaVA/llava/model/language_model/llava_Taiyi.py\", line 138, in generate\r\n2024-03-18 15:19:15 | ERROR | stderr |     return super().generate(\r\n2024-03-18 15:19:15 | ERROR | stderr |   File \"/root/LLaVA/llava/model/language_model/Taiyi/modeling_qwen.py\", line 1111, in generate\r\n2024-03-18 15:19:15 | ERROR | stderr |     return super().generate(\r\n2024-03-18 15:19:15 | ERROR | stderr |   File \"/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n2024-03-18 15:19:15 | ERROR | stderr |     return func(*args, **kwargs)\r\n2024-03-18 15:19:15 | ERROR | stderr |   File \"/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1330, in generate\r\n2024-03-18 15:19:15 | ERROR | stderr |     inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(\r\n2024-03-18 15:19:15 | ERROR | stderr |   File \"/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py\", line 402, in _prepare_model_inputs\r\n2024-03-18 15:19:15 | ERROR | stderr |     model_kwargs[\"input_ids\"] = self._maybe_initialize_input_ids_for_generation(\r\n2024-03-18 15:19:15 | ERROR | stderr |   File \"/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py\", line 431, in _maybe_initialize_input_ids_for_generation\r\n2024-03-18 15:19:15 | ERROR | stderr |     raise ValueError(\"`bos_token_id` has to be defined when no `input_ids` are provided.\")\r\n2024-03-18 15:19:15 | ERROR | stderr | ValueError: `bos_token_id` has to be defined when no `input_ids` are provided.\r\n\r\nThis is how it looks during inference:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/56297762/743d21cc-eeca-45f2-8e0e-51dcbc3d4531)\r\n\r\nThis is the content of my LLM file:\r\n`\r\nclass LlavaTaiyiConfig(QWenConfig):\r\n    model_type = \"llava_Taiyi\"\r\n\r\n\r\nclass LlavaTaiyiModel(LlavaMetaModel, QWenModel):\r\n    config_class = LlavaTaiyiConfig\r\n\r\n    def __init__(self, config: QWenConfig):\r\n        super(LlavaTaiyiModel, self).__init__(config)\r\n\r\n    def embed_tokens(self, x):\r\n        return self.wte(x)\r\n\r\nclass LlavaTaiyiForCausalLM(QWenLMHeadModel, LlavaMetaForCausalLM):\r\n    config_class = LlavaTaiyiConfig\r\n\r\n    def __init__(self, config):\r\n        super(QWenLMHeadModel, self).__init__(config)\r\n        self.transformer = LlavaTaiyiModel(config)\r\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\r\n\r\n        # Initialize weights and apply final processing\r\n        self.post_init()\r\n\r\n    def get_model(self):\r\n        return self.transformer\r\n\r\n    def forward(\r\n        self,\r\n        input_ids: Optional[torch.LongTensor] = None,\r\n        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\r\n        attention_mask: Optional[torch.FloatTensor] = None,\r\n        token_type_ids: Optional[torch.LongTensor] = None,\r\n        position_ids: Optional[torch.LongTensor] = None,\r\n        head_mask: Optional[torch.FloatTensor] = None,\r\n        inputs_embeds: Optional[torch.FloatTensor] = None,\r\n        encoder_hidden_states: Optional[torch.Tensor] = None,\r\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\r\n        labels: Optional[torch.LongTensor] = None,\r\n        use_cache: Optional[bool] = None,\r\n        output_attentions: Optional[bool] = None,\r\n        output_hidden_states: Optional[bool] = None,\r\n        images: Optional[torch.FloatTensor] = None,\r\n        image_sizes: Optional[List[List[int]]] = None,\r\n        return_dict: Optional[bool] = None,\r\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\r\n\r\n        \r\n        \r\n        if inputs_embeds is None:\r\n            (\r\n                input_ids,\r\n                position_ids,\r\n                attention_mask,\r\n                past_key_values,\r\n                inputs_embeds,\r\n                labels\r\n            ) = self.prepare_inputs_labels_for_multimodal(\r\n                input_ids,\r\n                position_ids,\r\n                attention_mask,\r\n                past_key_values,\r\n                labels,\r\n                images,\r\n                image_sizes\r\n            )\r\n\r\n        return super().forward(\r\n            input_ids=input_ids,\r\n            past_key_values=past_key_values,\r\n            attention_mask=attention_mask,\r\n            token_type_ids=token_type_ids,\r\n            position_ids=position_ids,\r\n            head_mask=head_mask,\r\n            inputs_embeds=inputs_embeds,\r\n            encoder_hidden_states=encoder_hidden_states,\r\n            encoder_attention_mask=encoder_attention_mask,\r\n            labels=labels,\r\n            use_cache=use_cache,\r\n            output_attentions=output_attentions,\r\n            output_hidden_states=output_hidden_states,\r\n            return_dict=return_dict\r\n        )\r\n\r\n    @torch.no_grad()\r\n    def generate(\r\n        self,\r\n        inputs: Optional[torch.Tensor] = None,\r\n        images: Optional[torch.Tensor] = None,\r\n        image_sizes: Optional[torch.Tensor] = None,\r\n        **kwargs,\r\n    ) -> Union[GenerateOutput, torch.LongTensor]:\r\n        position_ids = kwargs.pop(\"position_ids\", None)\r\n        attention_mask = kwargs.pop(\"attention_mask\", None)\r\n        if \"inputs_embeds\" in kwargs:\r\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\r\n\r\n        if images is not None:\r\n            (\r\n                inputs,\r\n                position_ids,\r\n                attention_mask,\r\n                _,\r\n                inputs_embeds,\r\n                _\r\n            ) = self.prepare_inputs_labels_for_multimodal(\r\n                inputs,\r\n                position_ids,\r\n                attention_mask,\r\n                None,\r\n                None,\r\n                images,\r\n                image_sizes=image_sizes\r\n            )\r\n        else:\r\n            inputs_embeds = self.get_model().embed_tokens(inputs)\r\n\r\n        return super().generate(\r\n            position_ids=position_ids,\r\n            attention_mask=attention_mask,\r\n            inputs_embeds=inputs_embeds,\r\n            **kwargs\r\n        )\r\n\r\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None,\r\n                                      inputs_embeds=None, **kwargs):\r\n        images = kwargs.pop(\"images\", None)\r\n        image_sizes = kwargs.pop(\"image_sizes\", None)\r\n        inputs = super().prepare_inputs_for_generation(\r\n            input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs\r\n        )\r\n        if images is not None:\r\n            inputs['images'] = images\r\n        if image_sizes is not None:\r\n            inputs['image_sizes'] = image_sizes\r\n        return inputs\r\n\r\n\r\nAutoConfig.register(\"llava_Taiyi\", LlavaTaiyiConfig)\r\nAutoModelForCausalLM.register(LlavaTaiyiConfig, LlavaTaiyiForCausalLM)\r\n\r\n`\r\n\r\nIf you can help me, I will be extremely grateful!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1295,
    "state": "open",
    "created_by": "tzjtatata",
    "created_at": "2024-03-18T08:34:23Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1295</URL>\n\n<TITLE>[Question] Recipe for full finetune llava-v1.5-7b without LoRA in 10 hours on 8xA100(40G)</TITLE>\n\n<BODY>### Question\n\nThanks for your great works. \r\nRecently, I am trying to train my own llava-v1.5-7b on 8xA100(40G). But there is always warning like \"4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance.\"\r\nAnd I only use 257K data for finetune but the time consumption is about 40 hours with zero3 and per_device_train_bs=8, gradient_accumulation_steps=2. I try to lower the per_device_train_bs=4, but it is useless.\r\n\r\nCan you provide the recipe for full finetune llava-v1.5-7b without LoRA and can be done in 10 hours? As noted in the Readme.md. Thank you very much.</BODY>\n\n<COMMENTS>\n<Comment by fisher75 at 2024-04-25T13:09:45Z>\nany recipe? I got no improvement from my LoRA\n</Comment>\n<Comment by xing0047 at 2024-05-02T08:08:32Z>\n> 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance.\r\n\r\nFor eliminating this warning, I install `deepspeed` from source and add `torch.cuda.empty_cache()` before `optimizer.step()`. The warning goes away.\n</Comment>\n<Comment by ZitianTang at 2024-05-09T04:22:32Z>\n> > 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance.\r\n> \r\n> For eliminating this warning, I install `deepspeed` from source and add `torch.cuda.empty_cache()` before `optimizer.step()`. The warning goes away.\r\n\r\nThank you! Could you tell me where the `optimizer.step()` is?\n</Comment>\n<Comment by xing0047 at 2024-05-09T05:38:12Z>\nFor `deepspeed-0.12.6`, it can be specified in `deepspeed/runtime/zero/stage3.py`, in function `_optimizer_step` of `class DeepSpeedZeroOptimizer_Stage3`. Hope this may help you.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1294,
    "state": "closed",
    "created_by": "zhangbaijin",
    "created_at": "2024-03-18T07:41:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1294</URL>\n\n<TITLE>ScienceQA-results-44.8%</TITLE>\n\n<BODY>### Question\r\n\r\nhello,haotian,when i  run the scripts `python -m llava.eval.model_vqa_science \\\r\n    --model-path  /mnt/xiaofeng.zxf/models/llava-v1.5-7b \\\r\n    --question-file /mnt/xiaofeng.zxf/code/LLaVA/ScienceQA_DATA/llava_test_QCM-LEA.json \\\r\n    --image-folder /mnt/xiaofeng.zxf/code/LLaVA/ScienceQA_DATA/test \\\r\n    --answers-file vqa/results/ScienceQA/test_llava-7b.jsonl \\\r\n    --conv-mode llava_v1`\r\nThe results is :Total: 4241, Correct: 988, Accuracy: 23.30%, IMG-Accuracy: 46.06%\r\n\r\nThe environment is below:\r\n\r\n1. bsl-py                       2.1.0\r\n2. accelerate                    0.27.2\r\n3. addict                        2.4.0\r\n4. aiohttp                       3.9.3\r\n5. aiosignal                     1.3.1\r\n6. aliyun-python-sdk-core        2.14.0\r\n7. aliyun-python-sdk-kms         2.16.2\r\n8. annotated-types               0.6.0\r\n9. antlr4-python3-runtime        4.9.3\r\n10. anyio                         4.3.0\r\n11. asttokens                     2.4.1\r\n12. async-timeout                 4.0.3\r\n13. attrs                         23.2.0\r\n14. braceexpand                   0.1.7\r\n15. certifi                       2024.2.2\r\n16. cffi                          1.16.0\r\n17. charset-normalizer            3.3.2\r\n18. comm                          0.2.1\r\n19. contourpy                     1.2.0\r\n20. crcmod                        1.7\r\n21. cryptography                  42.0.4\r\n22. cycler                        0.12.1\r\n23. datasets                      2.17.1\r\n24. debugpy                       1.6.7\r\n25. decorator                     5.1.1\r\n26. decord                        0.6.0\r\n27. dill                          0.3.8\r\n28. distro                        1.9.0\r\n29. einops                        0.7.0\r\n30. entrypoints                   0.4\r\n31. exceptiongroup                1.2.0\r\n32. executing                     2.0.1\r\n33. filelock                      3.13.1\r\n34. fire                          0.5.0\r\n35. fonttools                     4.49.0\r\n36. frozenlist                    1.4.1\r\n37. fsspec                        2023.10.0\r\n38. gast                          0.5.4\r\n39. grpcio                        1.60.1\r\n40. h11                           0.14.0\r\n41. httpcore                      1.0.4\r\n42. httpx                         0.27.0\r\n43. huggingface-hub               0.20.3\r\n44. idna                          3.6\r\n45. importlib-metadata            7.0.1\r\n46. iopath                        0.1.10\r\n47. ipykernel                     6.29.2\r\n48. ipython                       8.21.0\r\n49. jedi                          0.19.1\r\n50. Jinja2                        3.1.3\r\n51. jmespath                      0.10.0\r\n52. jupyter-client                7.3.4\r\n53. jupyter_core                  5.7.1\r\n54. kiwisolver                    1.4.5\r\n55. Markdown                      3.5.2\r\n56. MarkupSafe                    2.1.5\r\n57. matplotlib                    3.8.3\r\n58. matplotlib-inline             0.1.6\r\n59. modelscope                    1.12.0\r\n60. mpmath                        1.3.0\r\n61. multidict                     6.0.5\r\n62. multiprocess                  0.70.16\r\n63. nest_asyncio                  1.6.0\r\n64. networkx                      3.2.1\r\n65. numpy                         1.26.4\r\n66. nvidia-cublas-cu12            12.1.3.1\r\n67. nvidia-cuda-cupti-cu12        12.1.105\r\n68. nvidia-cuda-nvrtc-cu12        12.1.105\r\n69. nvidia-cuda-runtime-cu12      12.1.105\r\n70. nvidia-cudnn-cu12             8.9.2.26\r\n71. nvidia-cufft-cu12             11.0.2.54\r\n72. nvidia-curand-cu12            10.3.2.106\r\n73. nvidia-cusolver-cu12          11.4.5.107\r\n74. nvidia-cusparse-cu12          12.1.0.106\r\n75. nvidia-nccl-cu12              2.19.3\r\n76. nvidia-nvjitlink-cu12         12.3.101\r\n77. nvidia-nvtx-cu12              12.1.105\r\n78. omegaconf                     2.3.0\r\n79. openai                        1.13.3\r\n80. opencv-python                 4.9.0.80\r\n81. oss2                          2.18.4\r\n82. packaging                     23.2\r\n83. pandas                        2.2.0\r\n84. parso                         0.8.3\r\n85. pexpect                       4.9.0\r\n86. pickleshare                   0.7.5\r\n87. pillow                        10.2.0\r\n88. pip                           23.3.1\r\n89. platformdirs                  4.2.0\r\n90. portalocker                   2.8.2\r\n91. prompt-toolkit                3.0.42\r\n92. protobuf                      4.25.3\r\n93. psutil                        5.9.0\r\n94. ptyprocess                    0.7.0\r\n95. pure-eval                     0.2.2\r\n96. pyarrow                       15.0.0\r\n97. pyarrow-hotfix                0.6\r\n98. pycparser                     2.21\r\n99. pycryptodome                  3.20.0\r\n100. pydantic                      2.6.3\r\n101. pydantic_core                 2.16.3\r\n102. Pygments                      2.17.2\r\n103. pyparsing                     3.1.1\r\n104. python-dateutil               2.8.2\r\n105. pytz                          2024.1\r\n106. PyYAML                        6.0.1\r\n107. pyzmq                         25.1.2\r\n108. regex                         2023.12.25\r\n109. requests                      2.31.0\r\n110. safetensors                   0.4.2\r\n111. scipy                         1.12.0\r\n112. seaborn                       0.13.2\r\n113. sentencepiece                 0.2.0\r\n114. setuptools                    68.2.2\r\n115. shortuuid                     1.0.12\r\n116. simplejson                    3.19.2\r\n117. six                           1.16.0\r\n118. sniffio                       1.3.1\r\n119. sortedcontainers              2.4.0\r\n120. stack-data                    0.6.2\r\n121. sympy                         1.12\r\n122. tensorboard                   2.16.2\r\n123. tensorboard-data-server       0.7.2\r\n124. termcolor                     2.4.0\r\n125. tiktoken                      0.6.0\r\n126. timm                          0.9.16\r\n127. tokenizers                    0.13.3\r\n128. tomli                         2.0.1\r\n129. torch                         2.2.0\r\n130. torchvision                   0.17.0\r\n131. tornado                       6.1\r\n132. tqdm                          4.66.2\r\n133. traitlets                     5.14.1\r\n134. transformers                  4.29.2     /mnt/xiaofeng.zxf/code/OPERA/transformers-4.29.2\r\n135. transformers-stream-generator 0.0.4\r\n136. triton                        2.2.0\r\n137. typing_extensions             4.9.0\r\n138. tzdata                        2024.1\r\n139. urllib3                       2.2.1\r\n140. wcwidth                       0.2.13\r\n141. webdataset                    0.2.86\r\n142. Werkzeug                      3.0.1\r\n143. wheel                         0.41.2\r\n144. xxhash                        3.4.1\r\n145. yapf                          0.40.2\r\n146. yarl                          1.9.4\r\n147. zipp                          3.17.0</BODY>\n\n<COMMENTS>\n<Comment by zhangbaijin at 2024-03-19T06:46:53Z>\nThe problem is solved, when the input format is QCM-A, the accuracy is 44.8%, when the input format is CQM-A, the accuracy is 66.8%.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1293,
    "state": "open",
    "created_by": "charismaticchiu",
    "created_at": "2024-03-18T07:01:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1293</URL>\n\n<TITLE>[Question] What is VizWiz submission site?</TITLE>\n\n<BODY>### Question\n\nThe link to VizWiz submission site is broken, saying `the challenge does not exist`. Does anyone have the correct link?</BODY>\n\n<COMMENTS>\n<Comment by XpracticeYSKM at 2024-03-20T10:46:58Z>\nme too\n</Comment>\n<Comment by SachinG007 at 2024-03-21T16:56:57Z>\nmaybe this \r\nhttps://eval.ai/web/challenges/challenge-page/2182/overview\n</Comment>\n<Comment by LuFan31 at 2024-03-27T05:05:00Z>\nme too\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1292,
    "state": "open",
    "created_by": "PzWHU",
    "created_at": "2024-03-18T06:47:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1292</URL>\n\n<TITLE>process multi images</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nHow can I input multi images</BODY>\n\n<COMMENTS>\n<Comment by SakuraTroyChen at 2024-03-29T07:24:43Z>\nConcat the images path with ','\n</Comment>\n<Comment by Dinghaoxuan at 2024-04-01T08:34:32Z>\n> Concat the images path with ','\r\n\r\nWhat about the prompt? How to edit prompt when multi-images are input? Thank you for your response.\n</Comment>\n<Comment by Sprinter1999 at 2024-04-01T08:52:31Z>\nI'm also wondering.\n</Comment>\n<Comment by SakuraTroyChen at 2024-04-01T13:41:28Z>\n> > Concat the images path with ','\r\n> \r\n> What about the prompt? How to edit prompt when multi-images are input? Thank you for your response.\r\n\r\nThat's a task-specific problem. Maybe you can add some captions and instruct LLaVA in the appropriate order?\n</Comment>\n<Comment by yuejunpeng at 2024-06-01T15:05:34Z>\n@SakuraTroyChen I tried it but failed. In detail, I input two images as '--image-file' and cancat them with ',', and the task is to output the image captioning about the two images. However, the model only answer the captioning about the first image. The second image is almost invisible for the model. Maybe I think it's necessary to split the two image tokens with some special token.\n</Comment>\n<Comment by anas-zafar at 2024-07-03T06:36:04Z>\nHi @yuejunpeng were you able to solve it? Thanks\n</Comment>\n<Comment by SakuraTroyChen at 2024-07-03T06:41:42Z>\n> @SakuraTroyChen I tried it but failed. In detail, I input two images as '--image-file' and cancat them with ',', and the task is to output the image captioning about the two images. However, the model only answer the captioning about the first image. The second image is almost invisible for the model. Maybe I think it's necessary to split the two image tokens with some special token. \r\n\r\nSince the models are not trained on multi-image datasets, the real performance is unpredictable. I think you might need to train the model on specific multi-image datasets before using the multi-image inputs.\n</Comment>\n<Comment by anas-zafar at 2024-07-03T10:19:04Z>\n@SakuraTroyChen can you please guide me how can I train on multi-image inputs? Thanks\n</Comment>\n<Comment by SakuraTroyChen at 2024-07-03T14:34:23Z>\n> @SakuraTroyChen can you please guide me how can I train on multi-image inputs? Thanks\r\n\r\nYou can take a look at this [tutorial](https://zhuanlan.zhihu.com/p/693416852).\r\n(It was written in Chinese though)\n</Comment>\n<Comment by SakuraTroyChen at 2024-07-05T17:30:23Z>\nDataset format:\r\n```\r\n{\r\n        \"id\": \"1\",\r\n        \"image\": [\r\n            \"images/french_toast/278421.jpg\",\r\n            \"images/onion_rings/1074382.jpg\",\r\n            \"images/spring_rolls/419151.jpg\",\r\n            \"images/carrot_cake/263995.jpg\",\r\n            \"images/eggs_benedict/44135.jpg\"\r\n        ],\r\n        \"conversations\": [\r\n            {\r\n                \"from\": \"human\",\r\n                \"value\": \"<image>\\n<image>\\n<image>\\n<image>\\n<image>\\nWhat food are these five pictures about?\"\r\n            },\r\n            {\r\n                \"from\": \"gpt\",\r\n                \"value\": \"french toast, onion rings, spring rolls, carrot cake, eggs benedict\"\r\n            }\r\n        ]\r\n    }\r\n```\r\nStep 1: Modify `__getitem__` in `LazySupervisedDataset `.\r\n```python\r\ndef __getitem__(self, i) -> Dict[str, torch.Tensor]:\r\n        sources = self.list_data_dict[i]\r\n        if isinstance(i, int):\r\n            sources = [sources]\r\n        assert len(sources) == 1, \"Don't know why it is wrapped to a list\"  # FIXME\r\n        if 'image' in sources[0]:\r\n            image_file = self.list_data_dict[i]['image']\r\n            image_folder = self.data_args.image_folder\r\n            processor = self.data_args.image_processor\r\n​\r\n            if isinstance(image_file, list):\r\n                image = []\r\n                for img_file in image_file:\r\n                    image.append(Image.open(os.path.join(image_folder, img_file)).convert('RGB'))\r\n            else:\r\n                image = Image.open(os.path.join(image_folder, image_file)).convert('RGB')\r\n            \r\n            if self.data_args.image_aspect_ratio == 'pad':\r\n                def expand2square(pil_img, background_color):\r\n                    width, height = pil_img.size\r\n                    if width == height:\r\n                        return pil_img\r\n                    elif width > height:\r\n                        result = Image.new(pil_img.mode, (width, width), background_color)\r\n                        result.paste(pil_img, (0, (width - height) // 2))\r\n                        return result\r\n                    else:\r\n                        result = Image.new(pil_img.mode, (height, height), background_color)\r\n                        result.paste(pil_img, ((height - width) // 2, 0))\r\n                        return result\r\n                if isinstance(image, list):\r\n                    image = [expand2square(img, tuple(int(x*255) for x in processor.image_mean)) for img in image]\r\n                    image = [processor.preprocess(img, return_tensors='pt')['pixel_values'][0] for img in image]\r\n                else:\r\n                    image = expand2square(image, tuple(int(x*255) for x in processor.image_mean))\r\n                    image = processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\r\n            else:\r\n                if isinstance(image, list):\r\n                    image = [processor.preprocess(img, return_tensors='pt')['pixel_values'][0] for img in image]\r\n                else:\r\n                    image = processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\r\n            \r\n            sources = preprocess_multimodal(\r\n                copy.deepcopy([e[\"conversations\"] for e in sources]),\r\n                self.data_args)\r\n        else:\r\n            sources = copy.deepcopy([e[\"conversations\"] for e in sources])\r\n        data_dict = preprocess(\r\n            sources,\r\n            self.tokenizer,\r\n            has_image=('image' in self.list_data_dict[i]))\r\n        if isinstance(i, int):\r\n            data_dict = dict(input_ids=data_dict[\"input_ids\"][0],\r\n                             labels=data_dict[\"labels\"][0])\r\n​\r\n        # image exist in the data\r\n        if 'image' in self.list_data_dict[i]:\r\n            data_dict['image'] = image\r\n        elif self.data_args.is_multimodal:\r\n            # image does not exist in the data, but the model is multimodal\r\n            crop_size = self.data_args.image_processor.crop_size\r\n            data_dict['image'] = torch.zeros(3, crop_size['height'], crop_size['width'])\r\n        return data_dict\r\n```\r\nStep 2: Modify `DataCollatorForSupervisedDataset`.\r\n```python\r\nclass DataCollatorForSupervisedDataset(object):\r\n    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\r\n​\r\n    tokenizer: transformers.PreTrainedTokenizer\r\n​\r\n    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\r\n        input_ids, labels = tuple([instance[key] for instance in instances]\r\n                                  for key in (\"input_ids\", \"labels\"))\r\n        input_ids = torch.nn.utils.rnn.pad_sequence(\r\n            input_ids,\r\n            batch_first=True,\r\n            padding_value=self.tokenizer.pad_token_id)\r\n        labels = torch.nn.utils.rnn.pad_sequence(labels,\r\n                                                 batch_first=True,\r\n                                                 padding_value=IGNORE_INDEX)\r\n        input_ids = input_ids[:, :self.tokenizer.model_max_length]\r\n        labels = labels[:, :self.tokenizer.model_max_length]\r\n        batch = dict(\r\n            input_ids=input_ids,\r\n            labels=labels,\r\n            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\r\n        )\r\n​\r\n        if 'image' in instances[0]:\r\n            \r\n            images = [instance['image'] for instance in instances]\r\n            if isinstance(images[0], list):\r\n                images = torch.stack([torch.stack(img, dim=0) for img in images], dim = 0)\r\n                batch['images'] = images\r\n            else:\r\n                if all(x is not None and x.shape == images[0].shape for x in images):\r\n                    batch['images'] = torch.stack(images)\r\n                else:\r\n                    batch['images'] = images\r\n​\r\n        return batch\r\n```\n</Comment>\n<Comment by FengLi-ust at 2024-07-05T18:25:07Z>\nHi, LLaVA-Next-Interleave version is out, which naturally supports multi-image interleaved inputs. Please refer to [this evaluation code](https://github.com/LLaVA-VL/LLaVA-NeXT/blob/inference/llava/eval/model_vqa.py) for the input format. It can directly handle the input format you provide.\r\n\r\nYou can also try our [model demo](http://semantic-sam.xyzou.net:6123/) and see the details of the model in [this blog](https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/).\r\n\r\n> Dataset format:\r\n> \r\n> ```\r\n> {\r\n>         \"id\": \"1\",\r\n>         \"image\": [\r\n>             \"images/french_toast/278421.jpg\",\r\n>             \"images/onion_rings/1074382.jpg\",\r\n>             \"images/spring_rolls/419151.jpg\",\r\n>             \"images/carrot_cake/263995.jpg\",\r\n>             \"images/eggs_benedict/44135.jpg\"\r\n>         ],\r\n>         \"conversations\": [\r\n>             {\r\n>                 \"from\": \"human\",\r\n>                 \"value\": \"<image>\\n<image>\\n<image>\\n<image>\\n<image>\\nWhat food are these five pictures about?\"\r\n>             },\r\n>             {\r\n>                 \"from\": \"gpt\",\r\n>                 \"value\": \"french toast, onion rings, spring rolls, carrot cake, eggs benedict\"\r\n>             }\r\n>         ]\r\n>     }\r\n> ```\r\n> \r\n> Step 1: Modify `__getitem__` in `LazySupervisedDataset `.\r\n> \r\n> ```python\r\n> def __getitem__(self, i) -> Dict[str, torch.Tensor]:\r\n>         sources = self.list_data_dict[i]\r\n>         if isinstance(i, int):\r\n>             sources = [sources]\r\n>         assert len(sources) == 1, \"Don't know why it is wrapped to a list\"  # FIXME\r\n>         if 'image' in sources[0]:\r\n>             image_file = self.list_data_dict[i]['image']\r\n>             image_folder = self.data_args.image_folder\r\n>             processor = self.data_args.image_processor\r\n> ​\r\n>             if isinstance(image_file, list):\r\n>                 image = []\r\n>                 for img_file in image_file:\r\n>                     image.append(Image.open(os.path.join(image_folder, img_file)).convert('RGB'))\r\n>             else:\r\n>                 image = Image.open(os.path.join(image_folder, image_file)).convert('RGB')\r\n>             \r\n>             if self.data_args.image_aspect_ratio == 'pad':\r\n>                 def expand2square(pil_img, background_color):\r\n>                     width, height = pil_img.size\r\n>                     if width == height:\r\n>                         return pil_img\r\n>                     elif width > height:\r\n>                         result = Image.new(pil_img.mode, (width, width), background_color)\r\n>                         result.paste(pil_img, (0, (width - height) // 2))\r\n>                         return result\r\n>                     else:\r\n>                         result = Image.new(pil_img.mode, (height, height), background_color)\r\n>                         result.paste(pil_img, ((height - width) // 2, 0))\r\n>                         return result\r\n>                 if isinstance(image, list):\r\n>                     image = [expand2square(img, tuple(int(x*255) for x in processor.image_mean)) for img in image]\r\n>                     image = [processor.preprocess(img, return_tensors='pt')['pixel_values'][0] for img in image]\r\n>                 else:\r\n>                     image = expand2square(image, tuple(int(x*255) for x in processor.image_mean))\r\n>                     image = processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\r\n>             else:\r\n>                 if isinstance(image, list):\r\n>                     image = [processor.preprocess(img, return_tensors='pt')['pixel_values'][0] for img in image]\r\n>                 else:\r\n>                     image = processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\r\n>             \r\n>             sources = preprocess_multimodal(\r\n>                 copy.deepcopy([e[\"conversations\"] for e in sources]),\r\n>                 self.data_args)\r\n>         else:\r\n>             sources = copy.deepcopy([e[\"conversations\"] for e in sources])\r\n>         data_dict = preprocess(\r\n>             sources,\r\n>             self.tokenizer,\r\n>             has_image=('image' in self.list_data_dict[i]))\r\n>         if isinstance(i, int):\r\n>             data_dict = dict(input_ids=data_dict[\"input_ids\"][0],\r\n>                              labels=data_dict[\"labels\"][0])\r\n> ​\r\n>         # image exist in the data\r\n>         if 'image' in self.list_data_dict[i]:\r\n>             data_dict['image'] = image\r\n>         elif self.data_args.is_multimodal:\r\n>             # image does not exist in the data, but the model is multimodal\r\n>             crop_size = self.data_args.image_processor.crop_size\r\n>             data_dict['image'] = torch.zeros(3, crop_size['height'], crop_size['width'])\r\n>         return data_dict\r\n> ```\r\n> \r\n> Step 2: Modify `DataCollatorForSupervisedDataset`.\r\n> \r\n> ```python\r\n> class DataCollatorForSupervisedDataset(object):\r\n>     \"\"\"Collate examples for supervised fine-tuning.\"\"\"\r\n> ​\r\n>     tokenizer: transformers.PreTrainedTokenizer\r\n> ​\r\n>     def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\r\n>         input_ids, labels = tuple([instance[key] for instance in instances]\r\n>                                   for key in (\"input_ids\", \"labels\"))\r\n>         input_ids = torch.nn.utils.rnn.pad_sequence(\r\n>             input_ids,\r\n>             batch_first=True,\r\n>             padding_value=self.tokenizer.pad_token_id)\r\n>         labels = torch.nn.utils.rnn.pad_sequence(labels,\r\n>                                                  batch_first=True,\r\n>                                                  padding_value=IGNORE_INDEX)\r\n>         input_ids = input_ids[:, :self.tokenizer.model_max_length]\r\n>         labels = labels[:, :self.tokenizer.model_max_length]\r\n>         batch = dict(\r\n>             input_ids=input_ids,\r\n>             labels=labels,\r\n>             attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\r\n>         )\r\n> ​\r\n>         if 'image' in instances[0]:\r\n>             \r\n>             images = [instance['image'] for instance in instances]\r\n>             if isinstance(images[0], list):\r\n>                 images = torch.stack([torch.stack(img, dim=0) for img in images], dim = 0)\r\n>                 batch['images'] = images\r\n>             else:\r\n>                 if all(x is not None and x.shape == images[0].shape for x in images):\r\n>                     batch['images'] = torch.stack(images)\r\n>                 else:\r\n>                     batch['images'] = images\r\n> ​\r\n>         return batch\r\n> ```\n</Comment>\n<Comment by anas-zafar at 2024-07-06T11:54:21Z>\nThanks @FengLi-ust .\n</Comment>\n<Comment by SakuraTroyChen at 2024-07-07T15:03:03Z>\n> Thanks @FengLi-ust , will look into it. Thanks @SakuraTroyChen, can you guide me please how do I test this model after the Lora weights have been merged?\r\n\r\nI think it is enough to follow these 2 steps. In the tests I conducted, the source code performed better than the one with step 3. However, you can still use the Step 3 code as follows:\r\n```python\r\nclass PloyLlavaMetaForCausalLM(LlavaMetaForCausalLM, ABC):\r\n    def __init__(self):\r\n        super().__init__()\r\n​\r\n    def prepare_inputs_labels_for_multimodal(\r\n        self, input_ids, position_ids, attention_mask, past_key_values, labels,\r\n        images, image_sizes=None\r\n    ):\r\n        vision_tower = self.get_vision_tower()\r\n        if vision_tower is None or images is None or input_ids.shape[1] == 1:\r\n            return input_ids, position_ids, attention_mask, past_key_values, None, labels\r\n        if images.ndim == 5:\r\n            image_features = torch.stack([self.encode_images(img) for img in images], dim = 0)\r\n        else:\r\n            image_features = self.encode_images(images)\r\n        # image_features = self.encode_images(images)\r\n​\r\n        # TODO: image start / end is not implemented here to support pretraining.\r\n        if getattr(self.config, 'tune_mm_mlp_adapter', False) and getattr(self.config, 'mm_use_im_start_end', False):\r\n            raise NotImplementedError\r\n​\r\n        # Let's just add dummy tensors if they do not exist,\r\n        # it is a headache to deal with None all the time.\r\n        # But it is not ideal, and if you have a better idea,\r\n        # please open an issue / submit a PR, thanks.\r\n        _labels = labels\r\n        _position_ids = position_ids\r\n        _attention_mask = attention_mask\r\n        if attention_mask is None:\r\n            attention_mask = torch.ones_like(input_ids, dtype=torch.bool)\r\n        else:\r\n            attention_mask = attention_mask.bool()\r\n        if position_ids is None:\r\n            position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device)\r\n        if labels is None:\r\n            labels = torch.full_like(input_ids, IGNORE_INDEX)\r\n​\r\n        # remove the padding using attention_mask -- FIXME\r\n        _input_ids = input_ids\r\n        input_ids = [cur_input_ids[cur_attention_mask] for cur_input_ids, cur_attention_mask in zip(input_ids, attention_mask)]\r\n        labels = [cur_labels[cur_attention_mask] for cur_labels, cur_attention_mask in zip(labels, attention_mask)]\r\n​\r\n        new_input_embeds = []\r\n        new_labels = []\r\n        \r\n        for batch_idx, cur_input_ids in enumerate(input_ids):\r\n            cur_image_idx = 0\r\n            num_images = (cur_input_ids == IMAGE_TOKEN_INDEX).sum()\r\n            if image_features.ndim == 4:\r\n                batch_image_features = image_features[batch_idx]\r\n            else:\r\n                batch_image_features = image_features\r\n            if num_images == 0:\r\n                cur_image_features = batch_image_features[cur_image_idx]\r\n                cur_input_embeds_1 = self.get_model().embed_tokens(cur_input_ids)\r\n                cur_input_embeds = torch.cat([cur_input_embeds_1, cur_image_features[0:0]], dim=0)\r\n                new_input_embeds.append(cur_input_embeds)\r\n                new_labels.append(labels[batch_idx])\r\n                cur_image_idx += 1\r\n                continue\r\n​\r\n            image_token_indices = [-1] + torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)[0].tolist() + [cur_input_ids.shape[0]]\r\n            cur_input_ids_noim = []\r\n            cur_labels = labels[batch_idx]\r\n            cur_labels_noim = []\r\n            for i in range(len(image_token_indices) - 1):\r\n                cur_input_ids_noim.append(cur_input_ids[image_token_indices[i]+1:image_token_indices[i+1]])\r\n                cur_labels_noim.append(cur_labels[image_token_indices[i]+1:image_token_indices[i+1]])\r\n            split_sizes = [x.shape[0] for x in cur_labels_noim]\r\n            cur_input_embeds = self.get_model().embed_tokens(torch.cat(cur_input_ids_noim))\r\n            cur_input_embeds_no_im = torch.split(cur_input_embeds, split_sizes, dim=0)\r\n            cur_new_input_embeds = []\r\n            cur_new_labels = []\r\n​\r\n            for i in range(num_images + 1):\r\n                cur_new_input_embeds.append(cur_input_embeds_no_im[i])\r\n                cur_new_labels.append(cur_labels_noim[i])\r\n                if i < num_images:\r\n                    cur_image_features = batch_image_features[cur_image_idx]\r\n                    cur_image_idx += 1\r\n                    cur_new_input_embeds.append(cur_image_features)\r\n                    cur_new_labels.append(torch.full((cur_image_features.shape[0],), IGNORE_INDEX, device=cur_labels.device, dtype=cur_labels.dtype))\r\n​\r\n            cur_new_input_embeds = [x.to(self.device) for x in cur_new_input_embeds]\r\n​\r\n            cur_new_input_embeds = torch.cat(cur_new_input_embeds)\r\n            cur_new_labels = torch.cat(cur_new_labels)\r\n​\r\n            new_input_embeds.append(cur_new_input_embeds)\r\n            new_labels.append(cur_new_labels)\r\n​\r\n        # Truncate sequences to max length as image embeddings can make the sequence longer\r\n        tokenizer_model_max_length = getattr(self.config, 'tokenizer_model_max_length', None)\r\n        if tokenizer_model_max_length is not None:\r\n            new_input_embeds = [x[:tokenizer_model_max_length] for x in new_input_embeds]\r\n            new_labels = [x[:tokenizer_model_max_length] for x in new_labels]\r\n​\r\n        # Combine them\r\n        max_len = max(x.shape[0] for x in new_input_embeds)\r\n        batch_size = len(new_input_embeds)\r\n​\r\n        new_input_embeds_padded = []\r\n        new_labels_padded = torch.full((batch_size, max_len), IGNORE_INDEX, dtype=new_labels[0].dtype, device=new_labels[0].device)\r\n        attention_mask = torch.zeros((batch_size, max_len), dtype=attention_mask.dtype, device=attention_mask.device)\r\n        position_ids = torch.zeros((batch_size, max_len), dtype=position_ids.dtype, device=position_ids.device)\r\n​\r\n        for i, (cur_new_embed, cur_new_labels) in enumerate(zip(new_input_embeds, new_labels)):\r\n            cur_len = cur_new_embed.shape[0]\r\n            if getattr(self.config, 'tokenizer_padding_side', 'right') == \"left\":\r\n                new_input_embeds_padded.append(torch.cat((\r\n                    torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device),\r\n                    cur_new_embed\r\n                ), dim=0))\r\n                if cur_len > 0:\r\n                    new_labels_padded[i, -cur_len:] = cur_new_labels\r\n                    attention_mask[i, -cur_len:] = True\r\n                    position_ids[i, -cur_len:] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\r\n            else:\r\n                new_input_embeds_padded.append(torch.cat((\r\n                    cur_new_embed,\r\n                    torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device)\r\n                ), dim=0))\r\n                if cur_len > 0:\r\n                    new_labels_padded[i, :cur_len] = cur_new_labels\r\n                    attention_mask[i, :cur_len] = True\r\n                    position_ids[i, :cur_len] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\r\n​\r\n        new_input_embeds = torch.stack(new_input_embeds_padded, dim=0)\r\n​\r\n        if _labels is None:\r\n            new_labels = None\r\n        else:\r\n            new_labels = new_labels_padded\r\n​\r\n        if _attention_mask is None:\r\n            attention_mask = None\r\n        else:\r\n            attention_mask = attention_mask.to(dtype=_attention_mask.dtype)\r\n​\r\n        if _position_ids is None:\r\n            position_ids = None\r\n​\r\n        return None, position_ids, attention_mask, past_key_values, new_input_embeds, new_labels\r\n​\r\n​\r\n​\r\nclass PloyLlavaLlamaForCausalLM(LlamaForCausalLM, PloyLlavaMetaForCausalLM):\r\n    config_class = LlavaConfig\r\n​\r\n    def __init__(self, config):\r\n        super(LlamaForCausalLM, self).__init__(config)\r\n        self.model = LlavaLlamaModel(config)\r\n        self.pretraining_tp = config.pretraining_tp\r\n        self.vocab_size = config.vocab_size\r\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\r\n​\r\n        # Initialize weights and apply final processing\r\n        self.post_init()\r\n​\r\n    def get_model(self):\r\n        return self.model\r\n​\r\n    def forward(\r\n        self,\r\n        input_ids: torch.LongTensor = None,\r\n        attention_mask: Optional[torch.Tensor] = None,\r\n        position_ids: Optional[torch.LongTensor] = None,\r\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\r\n        inputs_embeds: Optional[torch.FloatTensor] = None,\r\n        labels: Optional[torch.LongTensor] = None,\r\n        use_cache: Optional[bool] = None,\r\n        output_attentions: Optional[bool] = None,\r\n        output_hidden_states: Optional[bool] = None,\r\n        images: Optional[torch.FloatTensor] = None,\r\n        image_sizes: Optional[List[List[int]]] = None,\r\n        return_dict: Optional[bool] = None,\r\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\r\n​\r\n        if inputs_embeds is None:\r\n            (\r\n                input_ids,\r\n                position_ids,\r\n                attention_mask,\r\n                past_key_values,\r\n                inputs_embeds,\r\n                labels\r\n            ) = self.prepare_inputs_labels_for_multimodal(\r\n                input_ids,\r\n                position_ids,\r\n                attention_mask,\r\n                past_key_values,\r\n                labels,\r\n                images,\r\n                image_sizes\r\n            )\r\n​\r\n        return super().forward(\r\n            input_ids=input_ids,\r\n            attention_mask=attention_mask,\r\n            position_ids=position_ids,\r\n            past_key_values=past_key_values,\r\n            inputs_embeds=inputs_embeds,\r\n            labels=labels,\r\n            use_cache=use_cache,\r\n            output_attentions=output_attentions,\r\n            output_hidden_states=output_hidden_states,\r\n            return_dict=return_dict\r\n        )\r\n​\r\n    @torch.no_grad()\r\n    def generate(\r\n        self,\r\n        inputs: Optional[torch.Tensor] = None,\r\n        images: Optional[torch.Tensor] = None,\r\n        image_sizes: Optional[torch.Tensor] = None,\r\n        **kwargs,\r\n    ) -> Union[GenerateOutput, torch.LongTensor]:\r\n        position_ids = kwargs.pop(\"position_ids\", None)\r\n        attention_mask = kwargs.pop(\"attention_mask\", None)\r\n        if \"inputs_embeds\" in kwargs:\r\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\r\n​\r\n        if images is not None:\r\n            (\r\n                inputs,\r\n                position_ids,\r\n                attention_mask,\r\n                _,\r\n                inputs_embeds,\r\n                _\r\n            ) = self.prepare_inputs_labels_for_multimodal(\r\n                inputs,\r\n                position_ids,\r\n                attention_mask,\r\n                None,\r\n                None,\r\n                images,\r\n                image_sizes=image_sizes\r\n            )\r\n        else:\r\n            inputs_embeds = self.get_model().embed_tokens(inputs)\r\n​\r\n        return super().generate(\r\n            position_ids=position_ids,\r\n            attention_mask=attention_mask,\r\n            inputs_embeds=inputs_embeds,\r\n            **kwargs\r\n        )\r\n​\r\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None,\r\n                                      inputs_embeds=None, **kwargs):\r\n        images = kwargs.pop(\"images\", None)\r\n        image_sizes = kwargs.pop(\"image_sizes\", None)\r\n        inputs = super().prepare_inputs_for_generation(\r\n            input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs\r\n        )\r\n        if images is not None:\r\n            inputs['images'] = images\r\n        if image_sizes is not None:\r\n            inputs['image_sizes'] = image_sizes\r\n        return inputs\r\n```\n</Comment>\n<Comment by SakuraTroyChen at 2024-07-07T17:37:39Z>\n> Got it . @SakuraTroyChen How do I test this model after merging the lora weights?\r\n\r\nLoad it and test it on your downstream task. It's task-specific. You can follow the evaluation tutorial in  https://github.com/haotian-liu/LLaVA\n</Comment>\n<Comment by anas-zafar at 2024-07-14T17:54:42Z>\n@SakuraTroyChen when add more than 2 images in the prompt I get the following error. Can you please guide me how to solve this? Thanks\r\n\r\n```\r\n[/content/LLaVA/llava/model/llava_arch.py](https://localhost:8080/#) in prepare_inputs_labels_for_multimodal(self, input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes)\r\n    258                 cur_new_labels.append(cur_labels_noim[i])\r\n    259                 if i < num_images:\r\n--> 260                     cur_image_features = image_features[cur_image_idx]\r\n    261                     cur_image_idx += 1\r\n    262                     cur_new_input_embeds.append(cur_image_features)\r\n\r\nIndexError: index 3 is out of bounds for dimension 0 with size 3\r\n```\n</Comment>\n<Comment by lqqyyy at 2024-08-03T10:09:38Z>\n@SakuraTroyChen 您好，只修改了第一二步后 用lora微调模型，出现loss值快速下降 几个step就到0的情况，用了2w作用的数据量。请问我是否遗漏了哪些步骤？\n</Comment>\n<Comment by SakuraTroyChen at 2024-08-04T17:18:12Z>\n> @SakuraTroyChen 您好，只修改了第一二步后 用lora微调模型，出现loss值快速下降 几个step就到0的情况，用了2w作用的数据量。请问我是否遗漏了哪些步骤？\r\n\r\n是用的llava原先的lora微调脚本吗？我是没出现这种情况，可能得看看你的learning rate之类的超参？\n</Comment>\n<Comment by anas-zafar at 2024-08-05T05:27:27Z>\nhi @SakuraTroyChen, sorry for pinging you again. I am trying to test the LoRA merged model. I am kind of stuck on this. \r\n\r\n```\r\nargs = type('Args', (), {\r\n    \"model_path\": fine_tuned_model_path,\r\n    \"model_base\": None,\r\n    \"model_name\": get_model_name_from_path(fine_tuned_model_path),\r\n    \"query\": prompt,\r\n    \"conv_mode\": None,\r\n    \"image_file\": \"test_6859.png,test_6843.png\",\r\n    \"sep\": \",\",\r\n    \"temperature\": 0,\r\n    \"top_p\": None,\r\n    \"num_beams\": 1,\r\n    \"max_new_tokens\": 512\r\n})() \r\n```\r\n`prompt = \"USER: <image>\\n The diagram represents a factory floor. <image> \\n How many actions are depicted in the diagram? ASSISTANT:\"`\r\n\r\n```\r\nIndexError                                Traceback (most recent call last)\r\n[<ipython-input-5-4a73c0203166>](https://localhost:8080/#) in <cell line: 38>()\r\n     36 \r\n     37 \r\n---> 38 eval_model(args)\r\n     39 # print(\"args.image_file before eval_model:\", args.image_file)\r\n     40 # eval_model(args)\r\n\r\n3 frames\r\n[/content/LLaVA/llava/model/llava_arch.py](https://localhost:8080/#) in prepare_inputs_labels_for_multimodal(self, input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes)\r\n    258                 cur_new_labels.append(cur_labels_noim[i])\r\n    259                 if i < num_images:\r\n--> 260                     cur_image_features = image_features[cur_image_idx]\r\n    261                     cur_image_idx += 1\r\n    262                     cur_new_input_embeds.append(cur_image_features)\r\n\r\nIndexError: index 2 is out of bounds for dimension 0 with size 2\r\n```\n</Comment>\n<Comment by SakuraTroyChen at 2024-08-05T06:11:50Z>\n@anas-zafar I didn't come across this situation. Maybe you need to use ipdb to print the vectors and debug here.\n</Comment>\n<Comment by lxr-1204 at 2024-11-14T12:48:18Z>\n> > @SakuraTroyChen can you please guide me how can I train on multi-image inputs? Thanks您能指导我如何训练多图像输入吗？谢谢\r\n> \r\n> You can take a look at this [tutorial](https://zhuanlan.zhihu.com/p/693416852).你可以看看这个[教程](https://zhuanlan.zhihu.com/p/693416852)。 (It was written in Chinese though)（虽然是用中文写的）\r\n\r\nHello, I have a question I’d like to ask. I’m working with a dataset that contains multiple images per data element, and I’ve encountered an issue. Due to this line of code:\r\n\r\n`images = torch.stack([torch.stack(img, dim=0) for img in images], dim=0)`\r\n\r\nit seems that the dataset needs to meet one of two conditions: either each data element must contain a fixed number N of images, or the batchsize must be set to 1. However, the number of images per element in my dataset varies, which causes errors during execution. Do you have any suggestions for handling this situation?\n</Comment>\n<Comment by SakuraTroyChen at 2024-11-14T13:21:20Z>\n@lxr-1204 I manually revise the datasets:\r\n\\# for single-image\r\nimg: img.jpg,\r\n\\# for multi-image\r\nimg:  [img1.jpg, img2.jpg, ......, imgn.jpg]\r\nOr you can add a type-checking here to make the code more adaptive.\n</Comment>\n<Comment by bjzhb666 at 2025-02-19T06:04:22Z>\n> [@SakuraTroyChen](https://github.com/SakuraTroyChen) 您好，只修改了第一二步后 用lora微调模型，出现loss值快速下降 几个step就到0的情况，用了2w作用的数据量。请问我是否遗漏了哪些步骤？\n@lqqyyy 请问这是什么情况。找到原因了吗？我也遇到了类似的情况，谢谢\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1291,
    "state": "closed",
    "created_by": "Pompey21",
    "created_at": "2024-03-17T18:26:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1291</URL>\n\n<TITLE>[Usage] ImportError: cannot import name 'LlavaLlamaForCausalLM' from 'llava.model'</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nHi, I've been trying to setup the project on my local machine and after downloading/installing all necessary pip libraries I tried running the exemplary test script (see below) but am running into problems.\r\nI created a new python file in the home directory of the project - where llava folder is also present.\r\n\r\nAlso please note that I made a fork and names it deep-llava. Nothing has been changed yet.\r\n\r\nCommand:\r\n```\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.mm_utils import get_model_name_from_path\r\nfrom llava.eval.run_llava import eval_model\r\n\r\nmodel_path = \"liuhaotian/llava-v1.5-7b\"\r\n\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    model_path=model_path,\r\n    model_base=None,\r\n    model_name=get_model_name_from_path(model_path)\r\n)\r\n```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/path-on-my-machine/deep-llava/test.py\", line 1, in <module>\r\n    from llava.model.builder import load_pretrained_model\r\n  File \"/path-on-my-machine/deep-llava/llava/__init__.py\", line 1, in <module>\r\n    from .model import LlavaLlamaForCausalLM\r\nImportError: cannot import name 'LlavaLlamaForCausalLM' from 'llava.model' (/path-on-my-machine/deep-llava/llava/model/__init__.py)\r\n```</BODY>\n\n<COMMENTS>\n<Comment by zhaohm14 at 2024-03-18T01:39:33Z>\nAs discussed in [issue #1101](https://github.com/haotian-liu/LLaVA/issues/1101), I've tried the following package versions and they seem to work fine in my environment:\r\n\r\n- `transformers==4.37.2`\r\n- `accelerate==0.28.0`\r\n- `torch==2.1.2`\n</Comment>\n<Comment by Pompey21 at 2024-03-18T14:10:01Z>\nThank you, this was really helpful :)\n</Comment>\n<Comment by wentaoyuan at 2024-05-10T17:36:59Z>\n> As discussed in [issue #1101](https://github.com/haotian-liu/LLaVA/issues/1101), I've tried the following package versions and they seem to work fine in my environment:\r\n> \r\n> * `transformers==4.37.2`\r\n> * `accelerate==0.28.0`\r\n> * `torch==2.1.2`\r\n\r\nI have exactly the same versions but still running into `ImportError: cannot import name 'LlavaLlamaForCausalLM' from 'llava.model'`\n</Comment>\n<Comment by zhaohm14 at 2024-05-11T04:11:22Z>\n@wentaoyuan Maybe you can comment out the `try...except...` block in `llava/model/__init__.py`\r\n\r\n```python\r\n# try:\r\nfrom .language_model.llava_llama import LlavaLlamaForCausalLM, LlavaConfig\r\nfrom .language_model.llava_mpt import LlavaMptForCausalLM, LlavaMptConfig\r\nfrom .language_model.llava_mistral import LlavaMistralForCausalLM, LlavaMistralConfig\r\n# except:\r\n#     pass\r\n```\r\nto find out more detailed error information.\r\nhttps://github.com/haotian-liu/LLaVA/issues/1101#issuecomment-1933385973\n</Comment>\n<Comment by CrispyFeSo4 at 2025-02-26T02:39:52Z>\nSad that I‘ve tried all the advice above but it doesn't work.\nHowever, I replaced `from .model import LlavaLlamaForCausalLM` with `from .model.language_model.llava_llama import LlavaLlamaForCausalLM`, and it worked.\n\nMaybe you can try it first.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1290,
    "state": "open",
    "created_by": "lwpyh",
    "created_at": "2024-03-16T21:55:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1290</URL>\n\n<TITLE>[Question] How to output the possibility of the token</TITLE>\n\n<BODY>### Question\n\nThanks for this great work, just wondering how to output the possibility of each token in the last layer, so that I can know better about how the model works.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1289,
    "state": "open",
    "created_by": "zhaohm14",
    "created_at": "2024-03-16T07:15:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1289</URL>\n\n<TITLE>Error when launching SGLang worker with llava-v1.6-34b</TITLE>\n\n<BODY>Thank you for your wonderful work! I have been following the demo instructions and successfully launched the controller and the Gradio web server. However, I encountered an issue when trying to launch an SGLang worker with local llava-v1.6-34b model.\r\n\r\nHere's the command I used:\r\n\r\n```bash\r\n$ CUDA_VISIBLE_DEVICES=1,2,3,4 python -m sglang.launch_server --model-path models/llava-v1.6-34b --tokenizer-path models/llava-v1.6-34b-tokenizer --port 30000 --tp 4\r\n```\r\n\r\nAnd here's the terminal output\r\n\r\n```bash\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nserver started on [0.0.0.0]:10011\r\nserver started on [0.0.0.0]:10010\r\nserver started on [0.0.0.0]:10012\r\nserver started on [0.0.0.0]:10013\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\naccepted ('127.0.0.1', 54758) with fd 41\r\nwelcome ('127.0.0.1', 54758)\r\naccepted ('127.0.0.1', 50916) with fd 32\r\nwelcome ('127.0.0.1', 50916)\r\naccepted ('127.0.0.1', 34778) with fd 35\r\nwelcome ('127.0.0.1', 34778)\r\naccepted ('127.0.0.1', 46910) with fd 33\r\nwelcome ('127.0.0.1', 46910)\r\nRank 0: load weight begin.\r\nRank 1: load weight begin.\r\nRank 2: load weight begin.\r\nRank 3: load weight begin.\r\n/home/zhaohm14/anaconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\n/home/zhaohm14/anaconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\n/home/zhaohm14/anaconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\n/home/zhaohm14/anaconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\nRank 0: load weight end.\r\nRank 1: load weight end.\r\nRank 3: load weight end.\r\nRank 2: load weight end.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nRank 0: max_total_num_token=12849, max_prefill_num_token=4096, context_len=4096, \r\ndisable_radix_cache=False, enable_flashinfer=False, disable_regex_jump_forward=False, disable_disk_cache=False, attention_reduce_in_fp32=False\r\nRank 3: max_total_num_token=12849, max_prefill_num_token=4096, context_len=4096, \r\ndisable_radix_cache=False, enable_flashinfer=False, disable_regex_jump_forward=False, disable_disk_cache=False, attention_reduce_in_fp32=False\r\nRank 1: max_total_num_token=12849, max_prefill_num_token=4096, context_len=4096, \r\ndisable_radix_cache=False, enable_flashinfer=False, disable_regex_jump_forward=False, disable_disk_cache=False, attention_reduce_in_fp32=False\r\nRank 2: max_total_num_token=12849, max_prefill_num_token=4096, context_len=4096, \r\ndisable_radix_cache=False, enable_flashinfer=False, disable_regex_jump_forward=False, disable_disk_cache=False, attention_reduce_in_fp32=False\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nINFO:     Started server process [83740]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)\r\nINFO:     127.0.0.1:41876 - \"GET /get_model_info HTTP/1.1\" 200 OK\r\nnew fill batch. #seq: 1. #cached_token: 0. #new_token: 8. #remaining_req: 0. #running_req: 0. tree_cache_hit_rate: 0.00%.\r\npython: /project/lib/Analysis/Allocation.cpp:40: std::pair<llvm::SmallVector<unsigned int>, llvm::SmallVector<unsigned int> > mlir::triton::getCvtOrder(mlir::Attribute, mlir::Attribute): Assertion `!(srcMmaLayout && dstMmaLayout) && \"Unexpected mma -> mma layout conversion\"' failed.\r\npython: /project/lib/Analysis/Allocation.cpp:40: std::pair<llvm::SmallVector<unsigned int>, llvm::SmallVector<unsigned int> > mlir::triton::getCvtOrder(mlir::Attribute, mlir::Attribute): Assertion `!(srcMmaLayout && dstMmaLayout) && \"Unexpected mma -> mma layout conversion\"' failed.\r\npython: /project/lib/Analysis/Allocation.cpp:40: std::pair<llvm::SmallVector<unsigned int>, llvm::SmallVector<unsigned int> > mlir::triton::getCvtOrder(mlir::Attribute, mlir::Attribute): Assertion `!(srcMmaLayout && dstMmaLayout) && \"Unexpected mma -> mma layout conversion\"' failed.\r\npython: /project/lib/Analysis/Allocation.cpp:40: std::pair<llvm::SmallVector<unsigned int>, llvm::SmallVector<unsigned int> > mlir::triton::getCvtOrder(mlir::Attribute, mlir::Attribute): Assertion `!(srcMmaLayout && dstMmaLayout) && \"Unexpected mma -> mma layout conversion\"' failed.\r\nProcess Process-1:\r\nTraceback (most recent call last):\r\n  File \"/home/zhaohm14/anaconda3/envs/llava/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/home/zhaohm14/anaconda3/envs/llava/lib/python3.10/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/zhaohm14/anaconda3/envs/llava/lib/python3.10/site-packages/sglang/srt/managers/router/manager.py\", line 79, in start_router_process\r\n    loop.run_until_complete(router.loop_for_forward())\r\n  File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\r\n  File \"/home/zhaohm14/anaconda3/envs/llava/lib/python3.10/site-packages/sglang/srt/managers/router/manager.py\", line 38, in loop_for_forward\r\n    out_pyobjs = await self.model_client.step(next_step_input)\r\n  File \"/home/zhaohm14/anaconda3/envs/llava/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 648, in _func\r\n    await asyncio.gather(*[asyncio.to_thread(t.wait) for t in tasks])\r\n  File \"/home/zhaohm14/anaconda3/envs/llava/lib/python3.10/asyncio/threads.py\", line 25, in to_thread\r\n    return await loop.run_in_executor(None, func_call)\r\n  File \"/home/zhaohm14/anaconda3/envs/llava/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/home/zhaohm14/anaconda3/envs/llava/lib/python3.10/site-packages/rpyc/core/async_.py\", line 51, in wait\r\n    self._conn.serve(self._ttl, waiting=self._waiting)\r\n  File \"/home/zhaohm14/anaconda3/envs/llava/lib/python3.10/site-packages/rpyc/core/protocol.py\", line 464, in serve\r\n    data = self._channel.poll(timeout) and self._channel.recv()\r\n  File \"/home/zhaohm14/anaconda3/envs/llava/lib/python3.10/site-packages/rpyc/core/channel.py\", line 55, in recv\r\n    header = self.stream.read(self.FRAME_HEADER.size)\r\n  File \"/home/zhaohm14/anaconda3/envs/llava/lib/python3.10/site-packages/rpyc/core/stream.py\", line 280, in read\r\n    raise EOFError(\"connection closed by peer\")\r\nEOFError: connection closed by peer\r\nHTTPConnectionPool(host='127.0.0.1', port=30000): Read timed out. (read timeout=60)\r\n```\r\n\r\nCould you please help me understand what might be causing this issue? I am eager to get the worker up and running and would greatly appreciate any assistance you can provide.</BODY>\n\n<COMMENTS>\n<Comment by jiezhangGt at 2024-04-11T03:34:32Z>\nHello, I also made a mistake in this step, but it is more advanced than your mistake, my error is：\r\n\r\n\r\n[2024-04-11 10:31:54,306] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n[2024-04-11 10:32:22,116] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-04-11 10:32:22,117] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n[2024-04-11 10:32:42,473] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-04-11 10:32:42,473] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nrouter init state: Traceback (most recent call last):\r\n  File \"/ssd11/exec/zhangjie07/HOME/miniconda3/envs/llava/lib/python3.10/site-packages/sglang/srt/managers/router/manager.py\", line 68, in start_router_process\r\n    model_client = ModelRpcClient(server_args, port_args)\r\n  File \"/ssd11/exec/zhangjie07/HOME/miniconda3/envs/llava/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 633, in __init__\r\n    self.model_servers = [x[0] for x in rets]\r\n  File \"/ssd11/exec/zhangjie07/HOME/miniconda3/envs/llava/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 633, in <listcomp>\r\n    self.model_servers = [x[0] for x in rets]\r\n  File \"/ssd11/exec/zhangjie07/HOME/miniconda3/envs/llava/lib/python3.10/concurrent/futures/_base.py\", line 621, in result_iterator\r\n    yield _result_or_cancel(fs.pop())\r\n  File \"/ssd11/exec/zhangjie07/HOME/miniconda3/envs/llava/lib/python3.10/concurrent/futures/_base.py\", line 319, in _result_or_cancel\r\n    return fut.result(timeout)\r\n  File \"/ssd11/exec/zhangjie07/HOME/miniconda3/envs/llava/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\r\n    return self.__get_result()\r\n  File \"/ssd11/exec/zhangjie07/HOME/miniconda3/envs/llava/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\r\n    raise self._exception\r\n  File \"/ssd11/exec/zhangjie07/HOME/miniconda3/envs/llava/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/ssd11/exec/zhangjie07/HOME/miniconda3/envs/llava/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 683, in start_model_process\r\n    raise RuntimeError(\"init rpc env error!\")\r\nRuntimeError: init rpc env error!\r\n\r\ndetoken init state: init ok\r\n\r\nHave you ever encountered this error？\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1288,
    "state": "open",
    "created_by": "LTtt456c",
    "created_at": "2024-03-16T02:45:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1288</URL>\n\n<TITLE>[Usage] After startup, the prompt NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nAfter startup, the prompt NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n1. python -m llava.serve.controller --host 0.0.0.0 --port 4006\r\n2. python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:4006 --port 2006 --worker http://localhost:2006 --model-path G:\\AI\\LLaVA\\weights\\llava-v1.5-7b\r\n3. python -m llava.serve.gradio_web_server --controller http://localhost:4006  --model-list-mode reload\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\n2024-03-16 10:40:04 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=2006, worker_address='http://localhost:2006', controller_address='http://localhost:4006', model_path='G:\\\\AI\\\\LLaVA\\\\weights\\\\llava-v1.5-7b', model_base=None, model_name=None, device='cuda', multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=False, use_flash_attn=False)\r\n2024-03-16 10:40:04 | INFO | model_worker | Loading the model G:\\AI\\LLaVA\\weights\\llava-v1.5-7b on worker 3157da ...\r\n2024-03-16 10:40:04 | WARNING | transformers.configuration_utils | You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\n2024-03-16 10:40:04 | WARNING | transformers.configuration_utils | You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\n2024-03-16 10:40:05 | ERROR | stderr | \r\nLoading checkpoint shards:   0%|                                                                 | 0/2 [00:00<?, ?it/s]\r\n2024-03-16 10:40:05 | ERROR | stderr | D:\\Program Files\\Python310\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n2024-03-16 10:40:05 | ERROR | stderr |   return self.fget.__get__(instance, owner)()\r\n2024-03-16 10:40:40 | ERROR | stderr | \r\nLoading checkpoint shards:  50%|████████████████████████████▌                            | 1/2 [00:35<00:35, 35.59s/it]\r\n2024-03-16 10:40:41 | ERROR | stderr | \r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:36<00:00, 15.28s/it]\r\n2024-03-16 10:40:41 | ERROR | stderr | \r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:36<00:00, 18.33s/it]\r\n2024-03-16 10:40:41 | ERROR | stderr | \r\n2024-03-16 10:40:48 | INFO | model_worker | Register to controller\r\n2024-03-16 10:40:50 | ERROR | stderr | \u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m7016\u001b[0m]\r\n2024-03-16 10:40:50 | ERROR | stderr | \u001b[32mINFO\u001b[0m:     Waiting for application startup.\r\n2024-03-16 10:40:50 | ERROR | stderr | \u001b[32mINFO\u001b[0m:     Application startup complete.\r\n2024-03-16 10:40:50 | ERROR | stderr | \u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:2006\u001b[0m (Press CTRL+C to quit)\r\n2024-03-16 10:40:58 | INFO | stdout | \u001b[32mINFO\u001b[0m:     127.0.0.1:51758 - \"\u001b[1mPOST /worker_get_status HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\r\n2024-03-16 10:41:05 | INFO | model_worker | Send heart beat. Models: ['G:\\\\AI\\\\LLaVA\\\\weights\\\\llava-v1.5-7b']. Semaphore: None. global_counter: 0\r\n2024-03-16 10:41:11 | INFO | model_worker | Send heart beat. Models: ['G:\\\\AI\\\\LLaVA\\\\weights\\\\llava-v1.5-7b']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n2024-03-16 10:41:13 | INFO | stdout | \u001b[32mINFO\u001b[0m:     127.0.0.1:51777 - \"\u001b[1mPOST /worker_generate_stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\r\n2024-03-16 10:41:22 | INFO | model_worker | Send heart beat. Models: ['G:\\\\AI\\\\LLaVA\\\\weights\\\\llava-v1.5-7b']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n2024-03-16 10:41:28 | INFO | stdout | Caught Unknown Error\r\n2024-03-16 10:41:28 | INFO | model_worker | Send heart beat. Models: ['G:\\\\AI\\\\LLaVA\\\\weights\\\\llava-v1.5-7b']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n\r\n\r\n\r\n\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.\r\n\r\n![ab7a3612807729c8f352bfbbb69c3a6](https://github.com/haotian-liu/LLaVA/assets/53040718/f3237f95-3588-4900-8518-9d37c6de58ce)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1287,
    "state": "open",
    "created_by": "lezhang7",
    "created_at": "2024-03-15T16:48:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1287</URL>\n\n<TITLE>[Question] Evaluation time ov tasks, say ScienceQA, GQA?</TITLE>\n\n<BODY>### Question\n\nHey, \r\n\r\nI was evaluating lora finetuned 7b model on ScienceQA and tqdm shows it takes around 5 hours to finish testing on 4241 samples, is this normal or same goes wrong and I didn't notice? BTW, how much time do you expect on different tasks?\r\n<img width=\"894\" alt=\"Screenshot 2024-03-15 at 12 47 52 PM\" src=\"https://github.com/haotian-liu/LLaVA/assets/57939690/1bfd19f5-4be7-4e72-8e13-11f4f100f844\"></BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1286,
    "state": "closed",
    "created_by": "fisher75",
    "created_at": "2024-03-15T09:34:23Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1286</URL>\n\n<TITLE>[Usage] CLI stuck at \"User:\"</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nCan't run this command cause stuck at \"User:\"\r\nCommand:\r\n```\r\nI run the example demo CLI.\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/83761274/68ef5132-11f3-4b31-8464-c14a33af5ff2)</BODY>\n\n<COMMENTS>\n<Comment by fisher75 at 2024-03-17T05:14:21Z>\nSolved\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1285,
    "state": "closed",
    "created_by": "CloudedLeopard17",
    "created_at": "2024-03-15T09:10:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1285</URL>\n\n<TITLE>[Question] Using variable image sizes for fine tuning ?</TITLE>\n\n<BODY>### Question\r\n\r\nDo we have to resize the images to a fixed resolution/size or the model does it by itself? I am getting the error. \r\nMy data contains some small images, I resized the images to 336x336. \r\n\r\n![Screenshot from 2024-03-15 14-39-55](https://github.com/haotian-liu/LLaVA/assets/33505589/6f69a619-c2dd-47a8-9164-bf3c3ea554aa)</BODY>\n\n<COMMENTS>\n<Comment by CloudedLeopard17 at 2024-03-18T14:21:36Z>\nIt seems greater than 336x336 images are supported.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1284,
    "state": "closed",
    "created_by": "yanbai1993",
    "created_at": "2024-03-15T07:38:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1284</URL>\n\n<TITLE>The LLAVA-V1.6-34B outputs garbled sentences.</TITLE>\n\n<BODY>### Question\n\nI tested the 34B model with textvqa, but the results are chaotic. Could there be a configuration error?\r\n`python -m llava.eval.model_vqa_loader \\\r\n    --model-path liuhaotian/llava-v1.6-34b \\\r\n    --question-file ./playground/data/eval/textvqa/llava_textvqa_val_v051_ocr.jsonl \\\r\n    --image-folder ./playground/data/eval/textvqa/train_images \\\r\n    --answers-file ./playground/data/eval/textvqa/answers/llava-v1.6-34b.jsonl \\\r\n    --temperature 0 \\\r\n    --conv-mode chatml_direct （）\r\n\r\npython -m llava.eval.eval_textvqa \\\r\n    --annotation-file ./playground/data/eval/textvqa/TextVQA_0.5.1_val.json \\\r\n    --result-file ./playground/data/eval/textvqa/answers/llava-v1.6-34b.jsonl`\r\n\r\nResult:\r\n{\"question_id\": \"b9dc400eb20bad64\", \"prompt\": \"What does the small white text spell?\\nReference OCR token: D, RUPALCON, PALCON, COPENHAGEN\\nAnswer the question using a single word or phrase.\", \"text\": \"Drupalcon\", \"answer_id\": \"DhmWWncmmMSJZxcYp58cMC\", \"model_id\": \"llava-v1.6-34b\", \"metadata\": {}}\r\n{\"question_id\": \"2b538a43dd933fc1\", \"prompt\": \"What kind of beer is this?\\nReference OCR token: NINK, NK, BOWING, CC, STON, SUE, ED, Sublimely, SELF, ELF-RICHEE, swAaVd, KGy, ALE\\nAnswer the question using a single word or phrase.\", \"text\": \"SelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelfSelf\", \"answer_id\": \"cMbXZuYAuqZHyhyebAPEVr\", \"model_id\": \"llava-v1.6-34b\", \"metadata\": {}}\r\n{\"question_id\": \"831bcec304a17054\", \"prompt\": \"What brand liquor is on the right?\\nReference OCR token: IGNET, N, MORANGIE, MORANGIE, MORANGIE, CHLANDS, ISLAY, OWMOR, OR, SINGL, MALT, WHISKY, SINGLE, MALT, HISKY, SCOTCH, WHITS, LASANTA, NON, CHILL-F, MATURED, RARE, SHERRY, CASKS, BatckK, Reliasen, CAKTYPE, STFILLBOURSON, BATCHNS, TOUE, CISTILLER, 55.6%ac, LONGHO, ATOL, BLAY, SCORLAND, AGED, 10\\nAnswer the question using a single word or phrase.\", \"text\": \"Bowmore Islay Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie Morangie\", \"answer_id\": \"4gtX3Xz9wwuaGn3PkaFKxS\", \"model_id\": \"llava-v1.6-34b\", \"metadata\": {}}\r\n{\"question_id\": \"831bcec304a17054\", \"prompt\": \"How long has the drink on the right been aged?\\nReference OCR token: IGNET, N, MORANGIE, MORANGIE, MORANGIE, CHLANDS, ISLAY, OWMOR, OR, SINGL, MALT, WHISKY, SINGLE, MALT, HISKY, SCOTCH, WHITS, LASANTA, NON, CHILL-F, MATURED, RARE, SHERRY, CASKS, BatckK, Reliasen, CAKTYPE, STFILLBOURSON, BATCHNS, TOUE, CISTILLER, 55.6%ac, LONGHO, ATOL, BLAY, SCORLAND, AGED, 10\\nAnswer the question using a single word or phrase.\", \"text\": \"10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\", \"answer_id\": \"FPeDSeeENBSMtLWAMBbMTN\", \"model_id\": \"llava-v1.6-34b\", \"metadata\": {}}\r\n{\"question_id\": \"94ad4aad01e27a32\", \"prompt\": \"What number is on the player's jersey?\\nReference OCR token: 22\\nAnswer the question using a single word or phrase.\", \"text\": \"22222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\", \"answer_id\": \"XiSeaMejABN5tbNeTWpiwa\", \"model_id\": \"llava-v1.6-34b\", \"metadata\": {}}\r\n{\"question_id\": \"181f00d3ee2b2076\", \"prompt\": \"What is the time?\\nReference OCR token: N, u, g0\\nAnswer the question using a single word or phrase.\", \"text\": \"The time is 1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\", \"answer_id\": \"7GUvvBADuw2NKYXyPXwrpR\", \"model_id\": \"llava-v1.6-34b\", \"metadata\": {}}\r\n\r\n\r\nModifying chatml_direct to vicuna_v1 also has issues.</BODY>\n\n<COMMENTS>\n<Comment by yanbai1993 at 2024-03-15T11:35:25Z>\nI have solved the issue by updating transformer to 4.36.2.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1283,
    "state": "open",
    "created_by": "jihuishan",
    "created_at": "2024-03-15T06:36:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1283</URL>\n\n<TITLE>[Question] How are the candidate answers in VQA datasets used?</TITLE>\n\n<BODY>### Question\n\nAs OKVQA, A-OKVQA, VQAv2, .etc, contain ten candidate answers. So how are these answers used in training? Are all candidate answers involved? Or, only the main answer is used? Thank you.</BODY>\n\n<COMMENTS>\n<Comment by zealousChinese at 2024-03-15T09:31:10Z>\nDoes the OKVQA dataset have main answers? Which attribute in the annotation indicates the main answer?\n</Comment>\n<Comment by jihuishan at 2024-03-15T11:22:14Z>\n> swer?\r\n\r\nOps, it doesn't. I usually regard the most common answer as the main one.\n</Comment>\n<Comment by jihuishan at 2024-03-15T11:25:44Z>\nYet the main concern here is how the answers are used. A traditional way I know is to compute the loss by comparing the generated text towards each candidate answer and then average or sum the results.\n</Comment>\n<Comment by jihuishan at 2024-03-15T12:22:05Z>\n> Yet the main concern here is how the answers are used. A traditional way I know is to compute the loss by comparing the generated text towards each candidate answer and then average or sum the results.\r\n\r\nWhich probably not the llava way here, as me observed.\n</Comment>\n<Comment by zealousChinese at 2024-03-15T13:13:38Z>\n> > Yet the main concern here is how the answers are used. A traditional way I know is to compute the loss by comparing the generated text towards each candidate answer and then average or sum the results.\r\n> \r\n> Which probably not the llava way here, as me observed.\r\n\r\nYes, I didn't find the performance of LLaVA on OK-VQA in the paper. I also want to use LLaVA for the OK-VQA dataset.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1282,
    "state": "open",
    "created_by": "ATing0203",
    "created_at": "2024-03-15T06:16:59Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1282</URL>\n\n<TITLE>[Usage] train() got an unexpected keyword argument 'attn_implementation'</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nwhen I run the pretrain.sh, it reports an error.\r\nCommand:\r\n```\r\nnohup sh scripts/v1_5/pretrain.sh > pretrain_log.txt 2>&1 &\r\n```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/data/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n  File \"/data/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n  File \"/data/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\nTraceback (most recent call last):\r\n  File \"/data/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n            train(attn_implementation=\"flash_attention_2\")train(attn_implementation=\"flash_attention_2\")train(attn_implementation=\"flash_attention_2\")\r\n\r\nTypeError    TypeError: TypeErrortrain(attn_implementation=\"flash_attention_2\"): train() got an unexpected keyword argument 'attn_implementation': \r\ntrain() got an unexpected keyword argument 'attn_implementation'\r\ntrain() got an unexpected keyword argument 'attn_implementation'\r\nTypeError\r\n: train() got an unexpected keyword argument 'attn_implementation'\r\n```\r\n\r\nwith CUDA11.7,  pytorch=2.0.1, torchvision=0.15.2, transformers=4.37.2,  llava=1.2.2.post1, flash-attn=2.5.5</BODY>\n\n<COMMENTS>\n<Comment by LALBJ at 2024-04-08T13:49:57Z>\nIn my environment, this problem is caused by invoking the llava package in the pip environment. So, I solved this problem by adding the local file llava to the python environment through the addition of the `PYTHONPATH` parameter when executing bash in the terminal.\r\n\r\nHere is an example:\r\n`PYTHONPATH=/path/to/your/llava:$PYTHONPATH bash your_script.sh`\n</Comment>\n<Comment by JJJYmmm at 2024-04-21T09:51:34Z>\n> In my environment, this problem is caused by invoking the llava package in the pip environment. So, I solved this problem by adding the local file llava to the python environment through the addition of the `PYTHONPATH` parameter when executing bash in the terminal.\r\n> \r\n> Here is an example: `PYTHONPATH=/path/to/your/llava:$PYTHONPATH bash your_script.sh`\r\n\r\nExcume me, I have a little problem. I find `attn_implementation` is used here in `train.py`. \r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/3e337ad269da3245643a2724a1d694b5839c37f9/llava/train/train.py#L827-L830\r\n\r\nBut it seems that the argument is not used in `LlavaLlamaForCausalLM.from_pretrained`. Could you point out somewhere the argument is actually used?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1280,
    "state": "open",
    "created_by": "birchmi",
    "created_at": "2024-03-15T04:03:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1280</URL>\n\n<TITLE>[Usage]</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 64 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3_offload.json \\\r\n    --model_name_or_path /home/projects/LLaVA/weight/models--lmsys--vicuna-7b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path /home/projects/LLaVA/data/instruct_tuning/json/llava_instruct_150k.json \\\r\n    --image_folder /home/projects/LLaVA/data/instruct_tuning/images/coco/train2017 \\\r\n    --vision_tower /home/projects/LLaVA/weight/models--openai--clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter /home/projects/LLaVA/weight/llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --fp16 True \\\r\n    --output_dir /home/projects/LLaVA/weight/finetuning \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 4 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nLog: \r\n```\r\n[2024-03-15 11:54:54,850] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-03-15 11:55:47,809] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\n[2024-03-15 11:55:47,810] [INFO] [runner.py:571:main] cmd = /home/miniconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --lora_enable True --lora_r 64 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero3_offload.json --model_name_or_path /home/mijunhua/docker/projects/LLaVA/weight/models--lmsys--vicuna-7b-v1.5 --version v1 --data_path /home/projects/LLaVA/data/instruct_tuning/json/llava_instruct_150k.json --image_folder /home/projects/LLaVA/data/instruct_tuning/images/coco/train2017 --vision_tower /home/projects/LLaVA/weight/models--openai--clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter /home/projects/LLaVA/weight/llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --fp16 True --output_dir /home/projects/LLaVA/weight/finetuning --num_train_epochs 1 --per_device_train_batch_size 4 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb\r\n[2024-03-15 11:55:50,140] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-03-15 11:55:51,629] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}\r\n[2024-03-15 11:55:51,629] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0\r\n[2024-03-15 11:55:51,630] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\r\n[2024-03-15 11:55:51,630] [INFO] [launch.py:163:main] dist_world_size=1\r\n[2024-03-15 11:55:51,630] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0\r\n[2024-03-15 11:55:54,827] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-03-15 11:56:02,936] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2024-03-15 11:56:02,936] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\nYou are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nYou are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\r\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nFlash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.\r\nFlash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.\r\n[2024-03-15 11:56:13,147] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B\r\nLoading checkpoint shards:   0%|                                                                                                                                                | 0/2 [00:00<?, ?it/s]\r\nTraceback (most recent call last):\r\n  File \"/home/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 531, in load_state_dict\r\n    return torch.load(\r\n  File \"/home/miniconda3/envs/llava/lib/python3.10/site-packages/torch/serialization.py\", line 1004, in load\r\n    overall_storage = torch.UntypedStorage.from_file(f, False, size)\r\nRuntimeError: unable to mmap 9976634558 bytes from file </home/projects/LLaVA/weight/models--lmsys--vicuna-7b-v1.5/pytorch_model-00001-of-00002.bin>: Cannot allocate memory (12)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 540, in load_state_dict\r\n    if f.read(7) == \"version\":\r\n  File \"/home/miniconda3/envs/llava/lib/python3.10/codecs.py\", line 322, in decode\r\n    (result, consumed) = self._buffer_decode(data, self.errors, final)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 128: invalid start byte\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/projects/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"/home/projects/LLaVA/llava/train/train.py\", line 828, in train\r\n    model = LlavaLlamaForCausalLM.from_pretrained(\r\n  File \"/home/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3850, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\n  File \"/home/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 4259, in _load_pretrained_model\r\n    state_dict = load_state_dict(shard_file)\r\n  File \"/home/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 552, in load_state_dict\r\n    raise OSError(\r\nOSError: Unable to load weights from pytorch checkpoint file for '/home/projects/LLaVA/weight/models--lmsys--vicuna-7b-v1.5/pytorch_model-00001-of-00002.bin' at '/home/projects/LLaVA/weight/models--lmsys--vicuna-7b-v1.5/pytorch_model-00001-of-00002.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1278,
    "state": "open",
    "created_by": "lezhang7",
    "created_at": "2024-03-14T18:49:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1278</URL>\n\n<TITLE>[Question] `nan` in finetuned model weight</TITLE>\n\n<BODY>### Question\r\n\r\nHi, \r\n\r\nI have successfully pretrained the mm_projector, and finish the finetune stage with following script:\r\n\r\n```\r\n################## LLaMA-2 ##################\r\nPROMPT_VERSION=\"llava_llama_2\"\r\nMODEL_VERSION=\"llama-2-7b-chat\"\r\n################## LLaMA-2 ##################\r\n\r\ndeepspeed --num_gpus=4 llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --model_name_or_path meta-llama/Llama-2-7b-chat-hf \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path ~/scratch/datasets/llava/llava_instruct_80k.json \\\r\n    --image_folder ~/scratch/datasets/coco/2017/train2017 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava-llama-2-7b-chat-pretrain-baseline/mm_projector.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-$MODEL_VERSION-finetune \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nHowever, when I evaluate on the task, I always find the output to be empty and inference become quiet slow, so I debug step by step, and find that the weight of `llava-llama-2-7b-chat-finetune/model-0000x-of-00003.safetensors ` seems contain many nans as shown follows: \r\n\r\n```\r\n{'lm_head.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\r\n         [nan, nan, nan,  ..., nan, nan, nan],\r\n         [nan, nan, nan,  ..., nan, nan, nan],\r\n         ...,\r\n         [nan, nan, nan,  ..., nan, nan, nan],\r\n         [nan, nan, nan,  ..., nan, nan, nan],\r\n         [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.bfloat16),\r\n 'model.layers.23.input_layernorm.weight': tensor([nan, nan, nan,  ..., nan, nan, nan], dtype=torch.bfloat16),\r\n 'model.layers.23.mlp.down_proj.weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\r\n         [nan, nan, nan,  ..., nan, nan, nan],\r\n         [nan, nan, nan,  ..., nan, nan, nan],\r\n         ...,\r\n         [nan, nan, nan,  ..., nan, nan, nan],\r\n         [nan, nan, nan,  ..., nan, nan, nan],\r\n         [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.bfloat16),\r\n...\r\n...\r\n...\r\n```\r\n\r\nI follow the official pretraining and finetuning script. Any idea why this happends and how to fix it?</BODY>\n\n<COMMENTS>\n<Comment by pipixin321 at 2024-03-15T02:17:04Z>\nI met the same question when I run finetune_lora.sh, the loss suddenly increases during training.The only modification I made was to use half of the llava-v1_5_mix665k samples.\r\n<img width=\"1034\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/66454973/e96a61dd-f3d6-41b1-b0a5-2e467f95c34b\">\n</Comment>\n<Comment by lezhang7 at 2024-03-15T16:46:30Z>\nI use lora and it works without this issue, but still wonder why this happened when full-parameters finetuning.\n</Comment>\n<Comment by sunwhw at 2024-08-06T07:33:53Z>\nhi, have you solved the problem?  I also met the problem when finetune the videollava sourced from llava...\n</Comment>\n<Comment by ghazalsaheb at 2024-08-06T19:43:04Z>\nI had the same issue and I figured it was because I was using hugging face's \"llava-hf/llava-1.5-7b-hf\" as the base model. I switched the base to \"liuhaotian/llava-v1.5-7b\" and it resolved the NaN issue. Plus, the training performance got much better.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1277,
    "state": "open",
    "created_by": "tibetgao",
    "created_at": "2024-03-14T08:27:39Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1277</URL>\n\n<TITLE>[Usage] request explaination of process related to instruction length</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nHi there!\r\n\r\nI have noticed that when you calculate the instruction length, you have subtracted 2 from the original length. I would like to know what is the purpose for that (like what are the 2 special tokens that to be ignored?)\r\n\r\nThanks!\r\nCommand:\r\n```\r\ninstruction_len = len(tokenizer_image_token(parts[0], tokenizer)) - 2\r\n```</BODY>\n\n<COMMENTS>\n<Comment by Isaachhh at 2024-03-14T16:24:07Z>\na bias (because `cur_len` starts from 1) and last space\r\n\r\nyou can print the unmasked target and masked target, and decode them\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1276,
    "state": "closed",
    "created_by": "CSEEduanyu",
    "created_at": "2024-03-14T03:42:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1276</URL>\n\n<TITLE>The quality of the code is poor, the readability is poor, nobody says? Not at the level of an open source project</TITLE>\n\n<BODY>### Describe the issue\n\nThe quality of the code is poor, the readability is poor, nobody says? Not at the level of an open source project</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1275,
    "state": "open",
    "created_by": "charliedream1",
    "created_at": "2024-03-14T02:36:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1275</URL>\n\n<TITLE>[Example request] could you provide an example of sending image with RestFul API: llava\\serve\\test_message.py</TITLE>\n\n<BODY>### feature\n\ncould you provide and example of sending image with restful API in example: llava\\serve\\test_message.py</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1274,
    "state": "open",
    "created_by": "charliedream1",
    "created_at": "2024-03-13T10:27:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1274</URL>\n\n<TITLE>[Usage] SGLang Launch: AttributeError: 'str' object has no attribute 'eos_token_id'</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nrouter init state: Traceback (most recent call last):\r\n  File \"/home/a/miniconda3/envs/py310/lib/python3.10/site-packages/sglang/srt/managers/router/manager.py\", line 68, in start_router_process\r\n    model_client = ModelRpcClient(server_args, port_args)\r\n  File \"/home/a/miniconda3/envs/py310/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 619, in __init__\r\n    self.model_server.exposed_init_model(0, server_args, port_args)\r\n  File \"/home/a/miniconda3/envs/py310/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 137, in exposed_init_model\r\n    self.regex_fsm_cache = FSMCache(\r\n  File \"/home/a/miniconda3/envs/py310/lib/python3.10/site-packages/sglang/srt/constrained/fsm_cache.py\", line 8, in __init__\r\n    self.outlines_tokenizer = TransformerTokenizer(\r\n  File \"/home/a/miniconda3/envs/py310/lib/python3.10/site-packages/outlines/models/transformers.py\", line 63, in __init__\r\n    self.eos_token_id = self.tokenizer.eos_token_id\r\nAttributeError: 'str' object has no attribute 'eos_token_id'\r\n\r\n\r\nCommand:\r\n```\r\nCUDA_VISIBLE_DEVICES=0 python3 -m sglang.launch_server --model-path liuhaotian/llava-v1.5-7b --tokenizer-path llava-hf/llava-1.5-7b-hf --port 30000\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by tfcoe-depop at 2024-03-14T14:43:57Z>\nhttps://github.com/sgl-project/sglang/issues/292 This has been raised and a temporary solution addressed in sglang project.\n</Comment>\n<Comment by butchmarshall at 2024-03-20T04:42:29Z>\nfyi https://github.com/haotian-liu/LLaVA/pull/1302 works around the issue\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1273,
    "state": "open",
    "created_by": "Rishbhu",
    "created_at": "2024-03-13T00:19:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1273</URL>\n\n<TITLE>Output not being generated</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: When I insert a picture, its not generating a output. I already refreshed and reloaded the page multiple times but same message keeps showing. Attached a screenshot below\r\n![Screenshot 2024-03-12 191919](https://github.com/haotian-liu/LLaVA/assets/99981600/b822e1d4-36e1-493a-b32e-7b1b948e9b39)\r\n\r\nAppreciate it a lot if you could fix this!\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1272,
    "state": "open",
    "created_by": "sreejank",
    "created_at": "2024-03-12T18:39:10Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1272</URL>\n\n<TITLE>[Question] Finetuning on task-specific data: how much?</TITLE>\n\n<BODY>### Question\n\nI was wondering what's a good estimate for the amount of data to finetune on task-specific data? Either from LoRA or the full-model?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1271,
    "state": "open",
    "created_by": "AmericanPresidentJimmyCarter",
    "created_at": "2024-03-12T14:11:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1271</URL>\n\n<TITLE>[Feature request] Add FastV optimizations for effective 2x speedup</TITLE>\n\n<BODY>### feature\n\nPaper\r\nhttps://arxiv.org/abs/2403.06764\r\n\r\nCode\r\nhttps://github.com/pkunlp-icler/FastV</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1270,
    "state": "open",
    "created_by": "DafengChi",
    "created_at": "2024-03-12T03:41:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1270</URL>\n\n<TITLE>[Question]  Is the LLaVA-1.6 training/fine-tuning code ready?</TITLE>\n\n<BODY>### Question\n\nwhen i load the “llava-v1.5-7b”， the training process is ok \r\nbut when i load the “llava-v1.6-vicuna-7b” \r\nthe error is \r\nTraceback (most recent call last):\r\n  File \"/home/ma-user/work/chidafeng/Embodied_AI_Agent/llava/train/train_xformers.py\", line 13, in <module>\r\ntrain()\r\n  File \"/home/ma-user/work/chidafeng/EmbodiedAgent/llava/train/train.py\", line 970, in train\r\ntrainer.train()\r\n  File \"/home/ma-user/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(    \r\ntrainer.train()\r\n  File \"/home/ma-user/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1687, in _inner_training_loop\r\nmodel, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\r\n  File \"/home/ma-user/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1198, in prepare\r\n    result = self._prepare_deepspeed(*args)\r\n      File \"/home/ma-user/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1537, in _prepare_deepspeed\r\nmodel, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\r\nengine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\r\n  File \"/home/ma-user/anaconda3/envs/llava/lib/python3.10/site-packages/deepspeed/__init__.py\", line 171, in initialize\r\n    engine = DeepSpeedEngine(args=args,    \r\nengine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\r\n  File \"/home/ma-user/anaconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 304, in __init__\r\n  File \"/home/ma-user/anaconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1234, in _configure_optimizer\r\n    self._configure_optimizer(optimizer, model_parameters)    \r\nself.optimizer = self._configure_zero_optimizer(basic_optimizer)\r\n  File \"/home/ma-user/anaconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1563, in _configure_zero_optimizer\r\n    self.optimizer = self._configure_zero_optimizer(basic_optimizer)\r\noptimizer = DeepSpeedZeroOptimizer_Stage3(\r\n  File \"/home/ma-user/anaconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 314, in __init__\r\n    self._create_fp16_partitions_with_defragmentation(self.trainable_param_groups)    \r\noptimizer = DeepSpeedZeroOptimizer_Stage3(\r\n  File \"/home/ma-user/anaconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 687, in _create_fp16_partitions_with_defragmentation\r\ndevice_buffer = __class__.defragment(parameter_partitions)  File \"/home/ma-user/anaconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 687, in _create_fp16_partitions_with_defragmentation\r\n\r\n  File \"/home/ma-user/anaconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 522, in defragment\r\n    assert len(set(t.dtype for t in tensors)) == 1    \r\ndevice_buffer = __class__.defragment(parameter_partitions)\r\nAssertionError</BODY>\n\n<COMMENTS>\n<Comment by pengwangucla at 2024-03-12T20:38:39Z>\n+1\n</Comment>\n<Comment by joaomsimoes at 2024-03-13T05:04:58Z>\n+1\n</Comment>\n<Comment by yinincanada at 2024-03-15T13:41:40Z>\n+1\n</Comment>\n<Comment by linkboyx at 2024-03-19T12:52:19Z>\n+1\n</Comment>\n<Comment by LoFiApostasy at 2024-03-22T01:33:47Z>\n∞ +1\n</Comment>\n<Comment by markmywords-tech at 2024-03-23T09:16:07Z>\n+1\n</Comment>\n<Comment by drogozhang at 2024-03-25T19:44:46Z>\n+1\n</Comment>\n<Comment by jsm69 at 2024-04-07T14:15:00Z>\ndid anyone get something?\n</Comment>\n<Comment by PzWHU at 2024-05-09T07:02:01Z>\n+1\n</Comment>\n<Comment by NicoZenith at 2024-06-05T13:53:33Z>\nany update?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1269,
    "state": "open",
    "created_by": "yinincanada",
    "created_at": "2024-03-11T18:47:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1269</URL>\n\n<TITLE>[Question] How to evaluate pretrained model effectively</TITLE>\n\n<BODY>### Question\n\n@haotian-liu  I am wondering how you evaluate the effectiveness of feature alignment in pretraining before finetune kick-in? or you always targeted on evaluating the final instruction-tuned model.  thanks!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1268,
    "state": "open",
    "created_by": "zhyhome",
    "created_at": "2024-03-11T14:47:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1268</URL>\n\n<TITLE>RuntimeError: Error(s) in loading state_dict for LlavaLlamaForCausalLM:[Question]</TITLE>\n\n<BODY>### Question\n\n**when I finetuned llava-v1.5-13b by :**\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3_offload.json \\\r\n    --model_name_or_path llava-v1.5-13b \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/llava-1.5-13B-lora-sacarasm.json \\\r\n    --image_folder ./playground/data/twitter/ \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir llava-v1.5-13b-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n\r\n**Then, I tried many ways to use the finetuned model,such as run_llava.py, but there's a problem with all of them when inference:**\r\n\r\n[2024-03-11 22:31:41,166] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nLoading LLaVA from base model...\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:06<00:00,  2.31s/it]\r\nLoading additional LLaVA weights...\r\nTraceback (most recent call last):\r\n  File \"/root/autodl-tmp/LLaVA/llava/eval/run_llava.py\", line 149, in <module>\r\n    eval_model(args)\r\n  File \"/root/autodl-tmp/LLaVA/llava/eval/run_llava.py\", line 57, in eval_model\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(\r\n  File \"/root/autodl-tmp/LLaVA/llava/model/builder.py\", line 79, in load_pretrained_model\r\n    model.load_state_dict(non_lora_trainables, strict=False)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2152, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for LlavaLlamaForCausalLM:\r\n        size mismatch for model.mm_projector.0.weight: copying a param with shape torch.Size([5120, 1024]) from checkpoint, the shape in current model is torch.Size([2621440, 1]).\r\n        size mismatch for model.mm_projector.2.weight: copying a param with shape torch.Size([5120, 5120]) from checkpoint, the shape in current model is torch.Size([13107200, 1]).\r\n\r\n**I tried 4-bit,do-sample,temperature and so on to handle this. But this model can't generate anything. This code can be inferred on the base model: llava-v1.5-13b. I print the output_ids[0] below:**\r\n![1](https://github.com/haotian-liu/LLaVA/assets/94852722/0839b734-b659-4610-bf1a-f81bbb602bd4)\r\nThis is a normal result based on llava-v1.5-13b\r\n![2](https://github.com/haotian-liu/LLaVA/assets/94852722/8403baa5-32f4-4c5d-a7be-566c044a0470)\r\nThis is a abnormal result based on finetuned model, whether merge or unmerged.</BODY>\n\n<COMMENTS>\n<Comment by sreejank at 2024-03-12T20:19:03Z>\nHaving the same issue\n</Comment>\n<Comment by 20191864218 at 2024-04-01T03:24:21Z>\nDid you manage to resolve it\n</Comment>\n<Comment by awzhgw at 2024-04-08T06:14:06Z>\nHaving the same issue\n</Comment>\n<Comment by Xuefei98 at 2024-05-22T01:30:58Z>\nsame issue here\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1267,
    "state": "open",
    "created_by": "DafengChi",
    "created_at": "2024-03-11T12:26:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1267</URL>\n\n<TITLE>[Question] Language Task Generation</TITLE>\n\n<BODY>### Question\n\nargs = type('Args', (), {\r\n    \"model_path\": model_path,\r\n    \"model_base\": None,\r\n    \"model_name\": get_model_name_from_path(model_path),\r\n    \"query\": prompt,\r\n    \"conv_mode\": None,\r\n    \"image_file\": image_file,\r\n    \"sep\": \",\",\r\n    \"temperature\": 0,\r\n    \"top_p\": None,\r\n    \"num_beams\": 1,\r\n    \"max_new_tokens\": 512\r\n})()\r\neval_model(args)\r\n\r\nif i use the script, it is failed to execute text only task</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1266,
    "state": "open",
    "created_by": "created-Bi",
    "created_at": "2024-03-11T08:35:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1266</URL>\n\n<TITLE>[Usage] When using merge_lora_weights.py, \"Supplied state dict for model.layers.0.self_attn.q_proj.weight does not contain `bitsandbytes__*` and possibly other `quantized_stats` components.\" occured</TITLE>\n\n<BODY>### Describe the issue\n\n[Usage] When using merge_lora_weights.py, \"Supplied state dict for model.layers.0.self_attn.q_proj.weight does not contain `bitsandbytes__*` and possibly other `quantized_stats` components.\" occurred\r\n\r\nversion:\r\npeft                      0.9.0\r\naccelerate                0.27.2 \r\nbitsandbytes              0.43.0\r\ntransformers              4.38.2\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/28759055/d0ae88eb-73a2-4161-aeb6-5478fc0ef2ba)\r\n\r\n\r\nHow to fix the problem, thx</BODY>\n\n<COMMENTS>\n<Comment by created-Bi at 2024-03-11T11:57:44Z>\nwhen finetuning using peft with codes like: \r\nbnb_model_from_pretrained_args.update(dict(\r\n            device_map={\"\": training_args.device},\r\n            load_in_4bit=training_args.bits == 4,\r\n            load_in_8bit=training_args.bits == 8,\r\n            quantization_config=BitsAndBytesConfig(\r\n                load_in_4bit=training_args.bits == 4,\r\n                load_in_8bit=training_args.bits == 8,\r\n                llm_int8_skip_modules=[\"mm_projector\"],\r\n                llm_int8_threshold=6.0,\r\n                llm_int8_has_fp16_weight=False,\r\n                bnb_4bit_compute_dtype=compute_dtype,\r\n                bnb_4bit_use_double_quant=training_args.double_quant,\r\n                bnb_4bit_quant_type=training_args.quant_type # {'fp4', 'nf4'}\r\n            )\r\n, an error will occur as ValueError: You can't pass `load_in_4bit`or `load_in_8bit` as a kwarg when passing `quantization_config` argument at the same time.\n</Comment>\n<Comment by created-Bi at 2024-03-11T11:58:00Z>\n@haotian-liu\n</Comment>\n<Comment by Karan-IceApple at 2024-03-21T10:56:18Z>\n@created-Bi facing same issue, let me know if you found any solutions!\n</Comment>\n<Comment by created-Bi at 2024-03-22T03:00:29Z>\n@Karan-IceApple  pip uninstall transformers and pip install transformers==4.36.2\n</Comment>\n<Comment by nasheed-uniphore at 2024-04-11T17:12:48Z>\nSo here are my two cents. You downloaded a model using `from_pretrained` with some form of 8bit/4bit quant and then saved it with `save_pretrained`. Ideally, you should always save an unquantized model.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1265,
    "state": "open",
    "created_by": "joaomsimoes",
    "created_at": "2024-03-11T06:16:57Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1265</URL>\n\n<TITLE>[Question] LLaVA-1.6 Training/Finetunning Code</TITLE>\n\n<BODY>### Question\n\nAny updates when is going to be release? Thanks!</BODY>\n\n<COMMENTS>\n<Comment by jweihe at 2024-03-11T08:14:18Z>\nHope to publish it as soon as possible\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1264,
    "state": "open",
    "created_by": "XuecaiHu",
    "created_at": "2024-03-11T06:00:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1264</URL>\n\n<TITLE>When use multi-nodes to train the model, training loss  rises abnormally</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nWhen use multi-nodes to train the model, training loss  rises abnormally\r\n\r\nwe use two nodes, each nodes has 8 A100s(40)\r\nCommand:\r\n```\r\n#!/bin/bash\r\n\r\ndeepspeed --hostfile=scripts/v1_5/hostfile.txt llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --model_name_or_path /path_to_data/LLaVa/Models/vicuna-13b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path /path_to_data/LLaVa/Data/LLaVA-Instruct-150K/llava_v1_5_mix665k.json \\\r\n    --image_folder /path_to_data/LLaVa/Data/ \\\r\n    --vision_tower /path_to_data/LLaVa/Models/clip-vit-large-patch14-336/ \\\r\n    --pretrain_mm_mlp_adapter /path_to_data/trained_model/checkpoints/llava-v1.5-13b-pretrain//mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir /path_to_data/trained_model/checkpoints/llava-v1.5-13b-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 2 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to \"none\" 2>&1 | tee  llava_v1.5_0311.log\r\n\r\n```\r\n\r\nLog: \r\n```\r\n192.168.50.171:   total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])\r\n192.168.50.170: {'loss': 1.2992, 'learning_rate': 1.282051282051282e-06, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.2859, 'learning_rate': 2.564102564102564e-06, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.2685, 'learning_rate': 3.846153846153847e-06, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.3461, 'learning_rate': 5.128205128205128e-06, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.3198, 'learning_rate': 6.41025641025641e-06, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.2998, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.3204, 'learning_rate': 8.974358974358976e-06, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.2842, 'learning_rate': 1.0256410256410256e-05, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.349, 'learning_rate': 1.153846153846154e-05, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.2635, 'learning_rate': 1.282051282051282e-05, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.2919, 'learning_rate': 1.4102564102564104e-05, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.2695, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.2735, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.2399, 'learning_rate': 1.794871794871795e-05, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.2478, 'learning_rate': 1.923076923076923e-05, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.2887, 'learning_rate': 2.0512820512820512e-05, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.2083, 'learning_rate': 2.1794871794871795e-05, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.227, 'learning_rate': 2.307692307692308e-05, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.2554, 'learning_rate': 2.435897435897436e-05, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.1559, 'learning_rate': 2.564102564102564e-05, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.2511, 'learning_rate': 2.6923076923076923e-05, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.2031, 'learning_rate': 2.8205128205128207e-05, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.1509, 'learning_rate': 2.948717948717949e-05, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.2433, 'learning_rate': 3.0769230769230774e-05, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.202, 'learning_rate': 3.205128205128206e-05, 'epoch': 0.0}\r\n192.168.50.170: {'loss': 1.1598, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.01}\r\n192.168.50.170: {'loss': 1.2317, 'learning_rate': 3.461538461538462e-05, 'epoch': 0.01}\r\n192.168.50.170: {'loss': 1.1928, 'learning_rate': 3.58974358974359e-05, 'epoch': 0.01}\r\n192.168.50.170: {'loss': 1.1584, 'learning_rate': 3.717948717948718e-05, 'epoch': 0.01}\r\n192.168.50.170: {'loss': 1.1689, 'learning_rate': 3.846153846153846e-05, 'epoch': 0.01}\r\n192.168.50.170: {'loss': 1.2189, 'learning_rate': 3.974358974358974e-05, 'epoch': 0.01}\r\n192.168.50.170: {'loss': 1.142, 'learning_rate': 4.1025641025641023e-05, 'epoch': 0.01}\r\n192.168.50.170: {'loss': 1.1915, 'learning_rate': 4.230769230769231e-05, 'epoch': 0.01}\r\n192.168.50.170: {'loss': 1.151, 'learning_rate': 4.358974358974359e-05, 'epoch': 0.01}\r\n192.168.50.170: {'loss': 1.0986, 'learning_rate': 4.4871794871794874e-05, 'epoch': 0.01}\r\n192.168.50.170: {'loss': 1.1711, 'learning_rate': 4.615384615384616e-05, 'epoch': 0.01}\r\n192.168.50.170: {'loss': 0.2248, 'learning_rate': 4.7435897435897435e-05, 'epoch': 0.01}\r\n192.168.50.170: {'loss': 1.1816, 'learning_rate': 4.871794871794872e-05, 'epoch': 0.01}\r\n192.168.50.170: {'loss': 1.1495, 'learning_rate': 5e-05, 'epoch': 0.01}\r\n......\r\n192.168.50.170: {'loss': 6.3472, 'learning_rate': 0.0001999475152949459, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 8.034, 'learning_rate': 0.00019994547742763934, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 9.2512, 'learning_rate': 0.00019994340075805725, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 9.6909, 'learning_rate': 0.00019994128528700583, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 11.2645, 'learning_rate': 0.00019993913101530635, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 11.2146, 'learning_rate': 0.00019993693794379525, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 10.875, 'learning_rate': 0.0001999347060733239, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 10.65, 'learning_rate': 0.00019993243540475877, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 10.288, 'learning_rate': 0.00019993012593898146, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 11.3354, 'learning_rate': 0.00019992777767688854, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 11.0331, 'learning_rate': 0.00019992539061939175, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 9.385, 'learning_rate': 0.00019992296476741776, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 11.1739, 'learning_rate': 0.00019992050012190843, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 8.9434, 'learning_rate': 0.00019991799668382058, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 8.43, 'learning_rate': 0.00019991545445412613, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 8.0869, 'learning_rate': 0.0001999128734338121, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 8.9871, 'learning_rate': 0.00019991025362388044, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 10.097, 'learning_rate': 0.00019990759502534834, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 10.3712, 'learning_rate': 0.00019990489763924797, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 7.6627, 'learning_rate': 0.00019990216146662646, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 8.6533, 'learning_rate': 0.00019989938650854616, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 8.7901, 'learning_rate': 0.00019989657276608437, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 9.1619, 'learning_rate': 0.00019989372024033352, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 9.0868, 'learning_rate': 0.000199890828932401, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 9.3259, 'learning_rate': 0.00019988789884340936, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 9.7941, 'learning_rate': 0.00019988492997449615, 'epoch': 0.04}\r\n192.168.50.170: {'loss': 10.0934, 'learning_rate': 0.00019988192232681397, 'epoch': 0.05}\r\n192.168.50.170: {'loss': 9.4644, 'learning_rate': 0.00019987887590153055, 'epoch': 0.05}\r\n192.168.50.170: {'loss': 8.8681, 'learning_rate': 0.00019987579069982855, 'epoch': 0.05}\r\n192.168.50.170: {'loss': 8.274, 'learning_rate': 0.00019987266672290575, 'epoch': 0.05}\r\n192.168.50.170: {'loss': 8.434, 'learning_rate': 0.00019986950397197503, 'epoch': 0.05}\r\n192.168.50.170: {'loss': 8.905, 'learning_rate': 0.00019986630244826426, 'epoch': 0.05}\r\n192.168.50.170: {'loss': 9.5568, 'learning_rate': 0.00019986306215301637, 'epoch': 0.05}\r\n192.168.50.170: {'loss': 9.7974, 'learning_rate': 0.00019985978308748937, 'epoch': 0.05}\r\n192.168.50.170: {'loss': 10.1468, 'learning_rate': 0.00019985646525295632, 'epoch': 0.05}\r\n192.168.50.170: {'loss': 9.6746, 'learning_rate': 0.0001998531086507053, 'epoch': 0.05}\r\n192.168.50.170: {'loss': 9.2765, 'learning_rate': 0.00019984971328203946, 'epoch': 0.05}\r\n\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by manushree635 at 2024-04-08T20:28:04Z>\nwere you able to resolve this\n</Comment>\n<Comment by fisher75 at 2024-04-25T13:15:21Z>\nHi, how do you know the training was effecitve? Did you use the default training setting? I LoRA with default parameters and basically no improvement.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1263,
    "state": "open",
    "created_by": "20191864218",
    "created_at": "2024-03-11T03:13:35Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1263</URL>\n\n<TITLE>the loss curve during the pretraining phase</TITLE>\n\n<BODY>### Question\n\nI replaced the vision encoder and LLM (Qwen7B-base) with LLaVA's original training data for pretraining. Here is the loss curve.\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/56297762/3bff5f87-d660-4dd2-8d7b-5f1a49db19fd)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1262,
    "state": "open",
    "created_by": "zzxslp",
    "created_at": "2024-03-10T21:34:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1262</URL>\n\n<TITLE>[Usage] Errors when launching demo</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nErrors occur when launching demo:\r\nCommand:\r\n```\r\npython -m llava.serve.sglang_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --sgl-endpoint http://127.0.0.1:30000\r\n```\r\nand \r\n```\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b\r\n```\r\n\r\nLog: \r\n```\r\nImportError: cannot import name 'build_regex_from_object' from 'outlines.fsm.json_schema' (/usr/local/lib/python3.11/site-packages/outlines/fsm/json_schema.py)\r\noutlines-server-1 exited with code 1\r\n```\r\nand \r\n```\r\nTypeError: LllavaLlamaForCausalLM.forward() got an unexpected keyword argument 'cache_position'\r\n```\r\n\r\nFix (temporary): \r\n`pip install \"sglang[all]\" \"outlines<=0.0.30\"`\r\n`pip install transformers==4.36.2`</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1261,
    "state": "open",
    "created_by": "rohithbojja",
    "created_at": "2024-03-10T16:59:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1261</URL>\n\n<TITLE>KeyError: 'llava_llama'</TITLE>\n\n<BODY>(llavalora) PS F:\\finetune_LLaVA> python llava\\eval\\run_llava.py --model-path C:\\Users\\rohit\\Downloads\\Compressed\\llama-2-7b-chat-task-qlora_5\\workspace\\LLaVA\\model-lora\\llama-2-7b-chat-task-qlora --model-base C:\\models\\llava-v1.5-7b --image-file \"C:\\datasets\\evqa-rad\\images\\34bc8f62-7dc1-4491-ab51-5b521f2acfe5.jpg\" --query \"describe this image\"\r\nbin C:\\ProgramData\\miniconda3\\envs\\llavalora\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\r\nTraceback (most recent call last):\r\n  File \"F:\\finetune_LLaVA\\llava\\eval\\run_llava.py\", line 157, in <module>\r\n    eval_model(args)\r\n  File \"F:\\finetune_LLaVA\\llava\\eval\\run_llava.py\", line 56, in eval_model\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\llavalora\\lib\\site-packages\\llava\\model\\builder.py\", line 50, in load_pretrained_model\r\n    lora_cfg_pretrained = AutoConfig.from_pretrained(model_path)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\llavalora\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\", line 998, in from_pretrained\r\n    config_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\llavalora\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\", line 710, in __getitem__\r\n    raise KeyError(key)\r\nKeyError: 'llava_llama'</BODY>\n\n<COMMENTS>\n<Comment by awzhgw at 2024-03-26T06:06:11Z>\ni get same error . how toresolve it ?\n</Comment>\n<Comment by Celtic-sf at 2024-05-26T03:25:37Z>\nI solved it by installing latest llava and transformers\n</Comment>\n<Comment by dhruv10xd at 2024-09-17T06:54:54Z>\nI am facing the same error when i try to load my custom finetuned 7b lora model. Were you able to resolve it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1260,
    "state": "closed",
    "created_by": "Yang-bug-star",
    "created_at": "2024-03-10T13:39:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1260</URL>\n\n<TITLE>[Question] 原始llava论文里用于提示GPT4的5条关于图片的caption句子是如何得到的，CC3M数据集每张图片只对应一个caption</TITLE>\n\n<BODY>### Question\n\n如何能控制每张图片都配对5条句子呢，是用gpt4还是其它模型caption 5 次得到的吗？</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2024-03-11T15:51:50Z>\nIt comes from the COCO captions annotations.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1259,
    "state": "closed",
    "created_by": "erjiaxiao",
    "created_at": "2024-03-10T05:28:02Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1259</URL>\n\n<TITLE>[Question] how could I use llava-v1.6 in command line?</TITLE>\n\n<BODY>### Question\n\nHello! Could I directly replace the model in the following command to use llava-v1.6?\r\n\r\n`python -m llava.serve.cli --model-path liuhaotian/llava-v1.6-34b --image-file \"https://llava-vl.github.io/static/images/view.jpg\" --load-4bit`\r\n\r\nThank you!</BODY>\n\n<COMMENTS>\n<Comment by phuchm at 2024-03-11T10:34:07Z>\nOf course, you can do it, but remember that use liuhaotian/llava-v1.6-34b will take so much resource! Please refer to https://github.com/haotian-liu/LLaVA/issues/1099 for more detail!\n</Comment>\n<Comment by erjiaxiao at 2024-03-11T14:00:17Z>\nThanks a lot! @phuchm\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1258,
    "state": "open",
    "created_by": "Yang-bug-star",
    "created_at": "2024-03-10T04:10:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1258</URL>\n\n<TITLE>[Question] 请问playground里构造conversation数据的few-shot examples是完全启发式的人工标注还是有应用特定方法的，比如询问gpt4？</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1257,
    "state": "closed",
    "created_by": "sunnychenxiwang",
    "created_at": "2024-03-10T01:11:23Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1257</URL>\n\n<TITLE>多模态语言模型能把LLM单独抽出来使用吗？</TITLE>\n\n<BODY>### Question\n\n我很好奇，是否能把MLLM的LLM分离出来，还能和之前一样和其文本对话。同时此时的LLM比起之前是否能力下降。</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2024-03-11T15:51:35Z>\nWithout the multimodal patches, LLaVA operates as an ordinary LLM. You can try it in our official demo. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1255,
    "state": "closed",
    "created_by": "20191864218",
    "created_at": "2024-03-09T08:00:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1255</URL>\n\n<TITLE>[Question] Visual encoder initialization issue</TITLE>\n\n<BODY>### Question\n\nI replaced the visual model with MedCLIP. During the pretraining phase, the model initializes and loads the checkpoint normally. However, when I'm finetuning with LoRA, I encounter this error. Can anyone help me?\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train()\r\n  File \"/root/LLaVA/llava/train/train.py\", line 1039, in train\r\n    model.get_model().initialize_vision_modules(\r\n  File \"/root/LLaVA/llava/model/llava_arch.py\", line 62, in initialize_vision_modules\r\n    vision_tower = build_vision_tower(model_args)\r\n  File \"/root/LLaVA/llava/model/multimodal_encoder/builder.py\", line 10, in build_vision_tower\r\n    return CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\r\n  File \"/root/LLaVA/llava/model/multimodal_encoder/clip_encoder.py\", line 29, in __init__\r\n    self.load_model()\r\n  File \"/root/LLaVA/llava/model/multimodal_encoder/clip_encoder.py\", line 58, in load_model\r\n    self.vision_tower = MedCLIPModel(vision_cls=MedCLIPVisionModelViT, vision_checkpoint=\"LLaVA/llava/model/multimodal_encoder/medclip-vit-pretrained\").vision_model\r\n  File \"/root/LLaVA/llava/model/multimodal_encoder/good.py\", line 146, in __init__\r\n    self.vision_model = vision_cls(medclip_checkpoint=vision_checkpoint)\r\n  File \"/root/LLaVA/llava/model/multimodal_encoder/good.py\", line 103, in __init__\r\n    self.load_from_medclip(medclip_checkpoint)\r\n  File \"/root/LLaVA/llava/model/multimodal_encoder/good.py\", line 119, in load_from_medclip\r\n    missing_keys, unexpected_keys = self.load_state_dict(new_state_dict, strict=False)\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2152, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for MedCLIPVisionModelViT:\r\n        size mismatch for model.embeddings.patch_embeddings.projection.weight: copying a param with shape torch.Size([96, 3, 4, 4]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.embeddings.patch_embeddings.projection.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.embeddings.norm.weight: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.embeddings.norm.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.layernorm_before.weight: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.layernorm_before.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table: copying a param with shape torch.Size([169, 3]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.attention.self.query.weight: copying a param with shape torch.Size([96, 96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.attention.self.query.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.attention.self.key.weight: copying a param with shape torch.Size([96, 96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.attention.self.key.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.attention.self.value.weight: copying a param with shape torch.Size([96, 96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.attention.self.value.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.attention.output.dense.weight: copying a param with shape torch.Size([96, 96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.attention.output.dense.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.layernorm_after.weight: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.layernorm_after.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.intermediate.dense.weight: copying a param with shape torch.Size([384, 96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.intermediate.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.output.dense.weight: copying a param with shape torch.Size([96, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.output.dense.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.layernorm_before.weight: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.layernorm_before.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table: copying a param with shape torch.Size([169, 3]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.attention.self.query.weight: copying a param with shape torch.Size([96, 96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.attention.self.query.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.attention.self.key.weight: copying a param with shape torch.Size([96, 96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.attention.self.key.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.attention.self.value.weight: copying a param with shape torch.Size([96, 96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.attention.self.value.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.attention.output.dense.weight: copying a param with shape torch.Size([96, 96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.attention.output.dense.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.layernorm_after.weight: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.layernorm_after.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.intermediate.dense.weight: copying a param with shape torch.Size([384, 96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.intermediate.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.output.dense.weight: copying a param with shape torch.Size([96, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.output.dense.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.downsample.reduction.weight: copying a param with shape torch.Size([192, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.downsample.norm.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.downsample.norm.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.layernorm_before.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.layernorm_before.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table: copying a param with shape torch.Size([169, 6]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.attention.self.query.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.attention.self.query.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.attention.self.key.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.attention.self.key.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.attention.self.value.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.attention.self.value.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.attention.output.dense.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.attention.output.dense.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.layernorm_after.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.layernorm_after.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.intermediate.dense.weight: copying a param with shape torch.Size([768, 192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.intermediate.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.output.dense.weight: copying a param with shape torch.Size([192, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.output.dense.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.layernorm_before.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.layernorm_before.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table: copying a param with shape torch.Size([169, 6]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.attention.self.query.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.attention.self.query.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.attention.self.key.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.attention.self.key.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.attention.self.value.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.attention.self.value.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.attention.output.dense.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.attention.output.dense.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.layernorm_after.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.layernorm_after.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.intermediate.dense.weight: copying a param with shape torch.Size([768, 192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.intermediate.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.output.dense.weight: copying a param with shape torch.Size([192, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.output.dense.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.downsample.reduction.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.downsample.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.downsample.norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.layernorm_before.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.layernorm_before.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table: copying a param with shape torch.Size([169, 12]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.attention.self.query.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.attention.self.query.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.attention.self.key.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.attention.self.key.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.attention.self.value.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.attention.self.value.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.attention.output.dense.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.attention.output.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.layernorm_after.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.layernorm_after.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.intermediate.dense.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.intermediate.dense.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.output.dense.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.output.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.layernorm_before.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.layernorm_before.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table: copying a param with shape torch.Size([169, 12]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.attention.self.query.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.attention.self.query.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.attention.self.key.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.attention.self.key.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.attention.self.value.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.attention.self.value.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.attention.output.dense.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.attention.output.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.layernorm_after.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.layernorm_after.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.intermediate.dense.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.intermediate.dense.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.output.dense.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.output.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.layernorm_before.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.layernorm_before.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table: copying a param with shape torch.Size([169, 12]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.attention.self.query.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.attention.self.query.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.attention.self.key.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.attention.self.key.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.attention.self.value.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.attention.self.value.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.attention.output.dense.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.attention.output.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.layernorm_after.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.layernorm_after.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.intermediate.dense.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.intermediate.dense.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.output.dense.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.output.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.layernorm_before.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.layernorm_before.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table: copying a param with shape torch.Size([169, 12]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.attention.self.query.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.attention.self.query.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.attention.self.key.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.attention.self.key.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.attention.self.value.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.attention.self.value.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.attention.output.dense.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.attention.output.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.layernorm_after.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.layernorm_after.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.intermediate.dense.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.intermediate.dense.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.output.dense.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.output.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.layernorm_before.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.layernorm_before.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table: copying a param with shape torch.Size([169, 12]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.attention.self.query.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.attention.self.query.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.attention.self.key.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.attention.self.key.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.attention.self.value.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.attention.self.value.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.attention.output.dense.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.attention.output.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.layernorm_after.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.layernorm_after.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.intermediate.dense.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.intermediate.dense.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.output.dense.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.output.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.layernorm_before.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.layernorm_before.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table: copying a param with shape torch.Size([169, 12]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.attention.self.query.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.attention.self.query.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.attention.self.key.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.attention.self.key.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.attention.self.value.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.attention.self.value.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.attention.output.dense.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.attention.output.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.layernorm_after.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.layernorm_after.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.intermediate.dense.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.intermediate.dense.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.output.dense.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.output.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.downsample.reduction.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.downsample.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.downsample.norm.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.layernorm_before.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.layernorm_before.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table: copying a param with shape torch.Size([169, 24]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.layernorm_after.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.layernorm_after.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.layernorm_before.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.layernorm_before.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table: copying a param with shape torch.Size([169, 24]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.layernorm_after.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.layernorm_after.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.layernorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.layernorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).</BODY>\n\n<COMMENTS>\n<Comment by 2U1 at 2024-04-17T07:14:19Z>\nDid you solved this issue?\r\nI'm having the same issue when chaning the vision model to open-clip\r\nIt's okay when pretraining, however same error occurs when fine-tuning with lora\n</Comment>\n<Comment by 20191864218 at 2024-04-18T00:53:15Z>\n> Did you solved this issue? I'm having the same issue when chaning the vision model to open-clip It's okay when pretraining, however same error occurs when fine-tuning with lora\r\n\r\ndeepspeed 使用 zero2来代替zero3，这样我能够成功运行lora\n</Comment>\n<Comment by 2U1 at 2024-04-18T02:25:15Z>\n> > Did you solved this issue? I'm having the same issue when chaning the vision model to open-clip It's okay when pretraining, however same error occurs when fine-tuning with lora\n> \n> deepspeed 使用 zero2来代替zero3，这样我能够成功运行lora\n\nThanks for your reply. I've solved the issu with using deepspeed zero2.\n</Comment>\n<Comment by LinJM at 2024-07-09T14:45:27Z>\nAfter changing to zero2, the parameters can indeed be loaded. What is the reason for its effectiveness?\n</Comment>\n<Comment by lsnls at 2024-07-14T05:03:07Z>\n> After changing to zero2, the parameters can indeed be loaded. What is the reason for its effectiveness?\r\n\r\nHello, I'm also confused about this question, do you know why?\n</Comment>\n<Comment by ShawnKing98 at 2024-10-05T22:30:38Z>\nThanks, this solved my problem! I'm also willing to know why tho...\n</Comment>\n<Comment by nasazzam at 2024-11-22T08:00:36Z>\n![image](https://github.com/user-attachments/assets/f28b6b74-c9e0-4cbc-ad05-3b6b534c6613)\r\nI think this may be the issue.\n</Comment>\n<Comment by MassEast at 2025-04-17T12:14:41Z>\nI am still very confused why using zero2 would work, and zero3 wouldn't. Because that does not have anything to do with how things models are loaded and just with how they are split up across GPUs, right? \nI also don't see the connection of the post by @Nas-Azzam to this.\nWhich code did you use for MedCLIP @20191864218 resp. for OpenCLIP @2U1?\nThanks!\n</Comment>\n<Comment by 2U1 at 2025-04-18T01:46:34Z>\n@MassEast I've used the original code from here \nhttps://github.com/mlfoundations/open_clip. \n\nI think it's the sharding is the reason for this but, I'm not sure why llava dosen't pretraining works and finetuning dosen't work.\n</Comment>\n<Comment by MassEast at 2025-04-18T09:53:55Z>\nThanks @2U1 !\nHow exactly did you use the code from there? I added a wrapper [here](https://github.com/Bliss-e-V/LLaVA-OpenCLIP/commit/04d79119c1dbc4f2a62536efd4512cbdfedb6882#diff-bcf9111e65f7ac88bb694ae94b394b051415b9578b6517a59478009b79513778), but something seems to be off with the config. I am suspecting it's the config, hidden_size, etc. methods, which I haven't tested yet.\n\nFor me, neither zero2 nor zero3 work with my added OpenCLIP code. And it makes sense that pretraining works and finetuning doesn't, because in the fine tuning some configs from pretrain seem to be loaded from/saved wrong already.\n</Comment>\n<Comment by MassEast at 2025-05-09T12:34:42Z>\nGot it to work, see [this fork](https://github.com/Bliss-e-V/LLaVA-OpenCLIP)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1254,
    "state": "open",
    "created_by": "arthurwolf",
    "created_at": "2024-03-09T04:45:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1254</URL>\n\n<TITLE>[Question] Adding my tasks to the dataset.</TITLE>\n\n<BODY>### Question\n\nI'd like Llava to become better at some tasks I need it to do (in the next version, I guess).\r\n\r\n(the tasks are related to reading/understanding comic book pages/panels, and analyzing their different elements)\r\n\r\nIs it possible to propose additions to the dataset, for them to then be used in the next round of training?\r\n\r\nIf so, what is the correct repository and procedure to do this?\r\n\r\nAny help would be greatly appreciated.\r\n\r\nThank you.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1253,
    "state": "open",
    "created_by": "lywang76",
    "created_at": "2024-03-09T02:00:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1253</URL>\n\n<TITLE>Inconsistent answer[Question]</TITLE>\n\n<BODY>### Question\n\nI run cli.py with liuhaotian/llava-v1.6-34b model.\r\nI set up same parameters as https://llava.hliu.cc/.\r\nFor waterview picture, the demo gives much better answer than the one from cli.py.\r\nWhy is it?</BODY>\n\n<COMMENTS>\n<Comment by phuchm at 2024-03-15T02:25:09Z>\nSame problem with llava-v1.6-mistral-7b.\r\n1 prompt but inconsistent result between runs\n</Comment>\n<Comment by yanshanjing at 2024-03-19T11:40:39Z>\nSame problem ,model is  liuhaotian/llava-v1.6-34b\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1252,
    "state": "closed",
    "created_by": "zzxslp",
    "created_at": "2024-03-09T00:03:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1252</URL>\n\n<TITLE>[Usage] error during finetuning</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: error occurred after finetuning is done and model can not be saved. \r\n\r\nCommand (default v1_5 finetuing script):\r\n```\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path lmsys/vicuna-13b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/llava_v1_5_mix665k.json \\\r\n    --image_folder ./playground/data \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-13b \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nLog: \r\n```\r\nFile \"/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py\", line 354, in validate\r\nraise ValueError(\r\nValueError: do_sample is set to False. However, temperature is set to 0.9 -- this flag is only used in sample-based generation modes. Set do_sample=True or unset temperature to continue.\r\n```\r\nAccording to[ this thread](https://github.com/facebookresearch/llama/issues/649), the error is caused with HF 4.37.2 release. Downgrading to 4.36.2 seems to work.</BODY>\n\n<COMMENTS>\n<Comment by yzbx at 2024-03-14T01:30:26Z>\nafter update transformer from 4.31.0 to 4.37.2, llama2-based model cannot saved after finetune. \r\nthe `do_sample is set to False` conflict with `temperature > 0` and `top_p > 0`\r\n\r\n- view generation/configuration_utils.py\r\n```\r\n# 1. detect sampling-only parameterization when not in sampling mode\r\n        if self.do_sample is False:\r\n            greedy_wrong_parameter_msg = (\r\n                \"`do_sample` is set to `False`. However, `{flag_name}` is set to `{flag_value}` -- this flag is only \"\r\n                \"used in sample-based generation modes. You should set `do_sample=True` or unset `{flag_name}`.\"\r\n                + fix_location\r\n            )\r\n            if self.temperature != 1.0:\r\n                warnings.warn(\r\n                    greedy_wrong_parameter_msg.format(flag_name=\"temperature\", flag_value=self.temperature),\r\n                    UserWarning,\r\n                )\r\n            if self.top_p != 1.0:\r\n                warnings.warn(\r\n                    greedy_wrong_parameter_msg.format(flag_name=\"top_p\", flag_value=self.top_p),\r\n                    UserWarning,\r\n                )\r\n            if self.typical_p != 1.0:\r\n                warnings.warn(\r\n                    greedy_wrong_parameter_msg.format(flag_name=\"typical_p\", flag_value=self.typical_p),\r\n                    UserWarning,\r\n                )\r\n            if self.top_k != 50 and self.penalty_alpha is None:  # contrastive search uses top_k\r\n                warnings.warn(\r\n                    greedy_wrong_parameter_msg.format(flag_name=\"top_k\", flag_value=self.top_k),\r\n                    UserWarning,\r\n                )\r\n            if self.epsilon_cutoff != 0.0:\r\n                warnings.warn(\r\n                    greedy_wrong_parameter_msg.format(flag_name=\"epsilon_cutoff\", flag_value=self.epsilon_cutoff),\r\n                    UserWarning,\r\n                )\r\n            if self.eta_cutoff != 0.0:\r\n                warnings.warn(\r\n                    greedy_wrong_parameter_msg.format(flag_name=\"eta_cutoff\", flag_value=self.eta_cutoff),\r\n                    UserWarning,\r\n                )\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1251,
    "state": "open",
    "created_by": "hantao-zhou",
    "created_at": "2024-03-08T20:21:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1251</URL>\n\n<TITLE>During finetuning the system reports that numpy has no typedict</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by hantao-zhou at 2024-03-08T20:22:04Z>\nFile \"/data/3zhou/Workspace/golf/LLaVA/llava/model/__init__.py\", line 1, in <module>\r\n    from .language_model.llava_llama import LlavaLlamaForCausalLM, LlavaConfig\r\n  File \"/data/3zhou/Workspace/golf/LLaVA/llava/model/language_model/llava_llama.py\", line 21, in <module>\r\n    from transformers import AutoConfig, AutoModelForCausalLM, \\\r\n  File \"<frozen importlib._bootstrap>\", line 1039, in _handle_fromlist\r\n  File \"/homeL/3zhou/.local/lib/python3.8/site-packages/transformers/utils/import_utils.py\", line 1090, in __getattr__\r\n    value = getattr(module, name)\r\n  File \"/homeL/3zhou/.local/lib/python3.8/site-packages/transformers/utils/import_utils.py\", line 1089, in __getattr__\r\n    module = self._get_module(self._class_to_module[name])\r\n  File \"/homeL/3zhou/.local/lib/python3.8/site-packages/transformers/utils/import_utils.py\", line 1101, in _get_module\r\n    raise RuntimeError(\r\nRuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\r\nmodule 'numpy' has no attribute 'typeDict'\r\n[2024-03-08 21:20:45,771] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 101448\r\n[2024-03-08 21:20:45,786] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 101449\r\n[2024-03-08 21:20:45,787] [ERROR] [launch.py:321:sigkill_handler] ['/usr/bin/python3', '-u', 'llava/train/train_mem.py', '--local_rank=1', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-13b', '--version', 'v1', '--data_path', '/homeL/3zhou/data/Workspace/golf/data/output/processed_dataset.json', '--image_folder', '/homeL/3zhou/data/Workspace/golf/data/input', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b-task-lora', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = 1\n</Comment>\n<Comment by hantao-zhou at 2024-03-08T20:38:52Z>\nand this error can be addressed by manually changing numpy version to lower, yet it will then illicit another error\r\n\r\n File \"/homeL/3zhou/.local/lib/python3.8/site-packages/deepspeed/comm/torch.py\", line 196, in broadcast\r\n    return torch.distributed.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)\r\n  File \"/homeL/3zhou/.local/lib/python3.8/site-packages/torch/distributed/c10d_logger.py\", line 47, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/homeL/3zhou/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 1906, in broadcast\r\n    work = default_pg.broadcast([tensor], opts)\r\ntorch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/NCCLUtils.hpp:219, invalid argument, NCCL version 2.14.3\r\nncclInvalidArgument: Invalid value for an argument.\r\nLast error:\r\nInvalid config blocking attribute value -2147483648\r\n[2024-03-08 21:37:15,437] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 116109\r\n[2024-03-08 21:37:15,455] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 116110\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1249,
    "state": "open",
    "created_by": "HeliumCake",
    "created_at": "2024-03-08T10:44:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1249</URL>\n\n<TITLE>[Question] Inconsistency between continuous inference in the same process and single inference in different processes</TITLE>\n\n<BODY>### Question\n\n### Question\r\nI try to use [llava/serve/cli.py](https://github.com/haotian-liu/LLaVA/blob/5d8f1760c08b7dfba3ae97b71cbd4c6f17d12dbd/llava/serve/cli.py) to do inference on multiple images.\r\n\r\nBut I found inconsistencies in the results between **running cli.py multiple times** using a shell script, inputting one image each time, and **running cli.py only once**, using a for loop to input one image each time.\r\n\r\nI use the original [llava/serve/cli.py](https://github.com/haotian-liu/LLaVA/blob/5d8f1760c08b7dfba3ae97b71cbd4c6f17d12dbd/llava/serve/cli.py) and the following shell script to run cli.py multiple times and do single inference in different processes.\r\n\r\n```shell\r\necho -e \"Is the person in the picture sitting in the car?\\n\\n\" | CUDA_VISIBLE_DEVICES=0,1  python llava/serve/cli.py --model-path liuhaotian/llava-v1.6-34b --image-file /root/data/pos/image1.jpg --temperature 0 --max-new-tokens 1024\r\necho -e \"Is the person in the picture sitting in the car?\\n\\n\" | CUDA_VISIBLE_DEVICES=0,1  python llava/serve/cli.py --model-path liuhaotian/llava-v1.6-34b --image-file /root/data/pos/image2.jpg --temperature 0 --max-new-tokens 1024\r\necho -e \"Is the person in the picture sitting in the car?\\n\\n\" | CUDA_VISIBLE_DEVICES=0,1  python llava/serve/cli.py --model-path liuhaotian/llava-v1.6-34b --image-file /root/data/pos/image3.jpg --temperature 0 --max-new-tokens 1024\r\necho -e \"Is the person in the picture sitting in the car?\\n\\n\" | CUDA_VISIBLE_DEVICES=0,1  python llava/serve/cli.py --model-path liuhaotian/llava-v1.6-34b --image-file /root/data/pos/image4.jpg --temperature 0 --max-new-tokens 1024\r\necho -e \"Is the person in the picture sitting in the car?\\n\\n\" | CUDA_VISIBLE_DEVICES=0,1  python llava/serve/cli.py --model-path liuhaotian/llava-v1.6-34b --image-file /root/data/pos/image5.jpg --temperature 0 --max-new-tokens 1024\r\n``` \r\n\r\nIn order to run cli.py only once, I made some changes to cli.py. On the basis of [llava/serve/cli.py](https://github.com/haotian-liu/LLaVA/blob/5d8f1760c08b7dfba3ae97b71cbd4c6f17d12dbd/llava/serve/cli.py), I only made the following changes: \r\n```python\r\n    parser.add_argument(\"--model-path\", type=str, default=\"liuhaotian/llava-v1.6-34b\")\r\n    parser.add_argument(\"--temperature\", type=float, default=0)\r\n    parser.add_argument(\"--max-new-tokens\", type=int, default=1024)\r\n\r\n    image_dir = \"/root/data/pos\"\r\n    image_files = os.listdir(image_dir)\r\n\r\n    for image_file in image_files:\r\n        args.image_file = os.path.join(image_dir, image_file)\r\n        main(args)\r\n``` \r\n\r\nThen for the same image, I got inconsistent results:\r\n\r\n| Images | continuous inference in the same process | single inference in different processes |\r\n|--------|--------|--------|\r\n| image1 | Yes, the person in the picture is sitting in the car. They appear to be in the driver's seat, as indicated by the steering wheel and the position of the person relative to the vehicle's controls. | Yes, the person in the picture is sitting in the car. They appear to be in the driver's seat, as indicated by the steering wheel and the position of the person relative to the vehicle's controls. |\r\n| image2 | The image provided is very blurry and lacks clear details, making it difficult to discern any specific objects or people. If there is a person in the picture, it would be challenging to confirm their presence or actions due to the low quality of the image. | Yes, there is a person sitting in the driver's seat of the vehicle in the picture. |\r\n| image3 | The image you've provided is very blurry and lacks clear details, making it difficult to discern any specific objects or people. If there is a person in the picture, it's not possible to determine their actions or whether they are in a car due to the quality of the image. | Yes, there is a person sitting in the driver's seat of the vehicle in the picture. |\r\n| image4 | The image provided is very blurry and lacks clear details, making it difficult to discern any specific objects or people. If there is a person in the car, it's not possible to confirm that based on this image. | Yes, there is a person sitting in the driver's seat of the vehicle in the picture. |\r\n| image5 | The image you've provided is very blurry and lacks any discernible details. It's not possible to determine if there is a person sitting in a car or any other context based on this image. If you have a clearer image or more information, I might be able to assist you better. | Yes, the person in the picture is sitting inside a vehicle, which appears to be a truck or a van. They are wearing a face mask and are seated in the passenger seat. | \r\n\r\nObviously, the results of the first image are consistent. But the following results are wrong when using the same process for continuous inference. It seems that the previous results have had a effect on the subsequent results.\r\n\r\nBut I only made few changes to [llava/serve/cli.py](https://github.com/haotian-liu/LLaVA/blob/5d8f1760c08b7dfba3ae97b71cbd4c6f17d12dbd/llava/serve/cli.py). And I'm sure that I reload model for each image. The previous results should not have a effect on the subsequent results. The only difference is whether to restart the process.\r\n\r\nSo why are they not consistent?  Is there any way to solve it?\r\n\r\nAnd can I load the model only once to infer multiple images and ensure that they do not affect each other, in order to save time on loading the model?</BODY>\n\n<COMMENTS>\n<Comment by phuchm at 2024-03-15T04:32:16Z>\nSam problem. Is there any update?\n</Comment>\n<Comment by eslambakr at 2024-07-09T18:25:19Z>\nAny updates about this issue?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1248,
    "state": "closed",
    "created_by": "chanangad",
    "created_at": "2024-03-08T09:40:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1248</URL>\n\n<TITLE>Error during this command: pip install flash-attn==2.0.4 --no-build-isolation</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: \r\n\r\nCommand:\r\n```\r\npip install flash-attn==2.0.4 --no-build-isolation\r\n```\r\n\r\nLog: \r\n```\r\n(llava) achandhok@achandhok-0-0:/sensei-fs/users/achandhok/LLaVA$ pip install flash-attn==2.0.4 --no-build-isolation\r\nCollecting flash-attn==2.0.4\r\n  Using cached flash_attn-2.0.4.tar.gz (4.2 MB)\r\n  Preparing metadata (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  × python setup.py egg_info did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> [8 lines of output]\r\n      Traceback (most recent call last):\r\n        File \"<string>\", line 2, in <module>\r\n        File \"<pip-setuptools-caller>\", line 34, in <module>\r\n        File \"/tmp/pip-install-v8_kacfm/flash-attn_d20678f39509460397d2b69d9bc4b015/setup.py\", line 13, in <module>\r\n          import torch\r\n        File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/__init__.py\", line 235, in <module>\r\n          from torch._C import *  # noqa: F403\r\n      ImportError: /opt/conda/envs/llava/lib/python3.10/site-packages/torch/lib/../../nvidia/cusparse/lib/libcusparse.so.12: undefined symbol: __nvJitLinkAddData_12_1, version libnvJitLink.so.12\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n× Encountered error while generating package metadata.\r\n╰─> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details.\r\n```\r\n![Uploading Screenshot 2024-03-08 at 3.10.00 PM.png…]()</BODY>\n\n<COMMENTS>\n<Comment by chanangad at 2024-03-12T05:34:59Z>\nUsed the v1.1.3 release and this issue wasn't there anymore\n</Comment>\n<Comment by Zero-coder at 2025-03-22T03:00:06Z>\n![Image](https://github.com/user-attachments/assets/229b6540-b327-4a26-b70c-0f391dc4ac3f) I don't think so~\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1246,
    "state": "open",
    "created_by": "yqy2001",
    "created_at": "2024-03-08T07:46:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1246</URL>\n\n<TITLE>[Discussion] We contribute the `CapsFusion` dataset, an enhanced version of BLIP-captions (LCS-558K). Welcome to have a try 🎉🎉</TITLE>\n\n<BODY>### Discussion\n\nWe open-source CapsFusion, a high-quality, large-scale image-text pair dataset consisting of 120M samples (accepted by CVPR 2024). The dataset has addressed the severe *Scalability Deficiency* and *World Knowledge Loss* issues of BLIP-synthetic captions (perhaps other synthetic captions also suffer), which are used in the **pretraining stage** of LLaVA.\r\n\r\nThe huggingface dataset provides [an easy inspection](https://huggingface.co/datasets/BAAI/CapsFusion-120M) of the CapsFusion dataset.\r\n\r\nData: https://huggingface.co/datasets/BAAI/CapsFusion-120M\r\nCode: https://github.com/baaivision/CapsFusion\r\nPaper: https://arxiv.org/abs/2310.20550\r\n\r\nExamples are shown below, where ➁ stands for captions used by LLaVA's pretraining (BLIP-generated), and ③ represents Capsfusion captions:\r\n\r\n<img width=\"1296\" alt=\"image6\" src=\"https://github.com/haotian-liu/LLaVA/assets/55196500/05d8b09e-2e78-4d6c-9748-150b6e6cb521\"></BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1245,
    "state": "open",
    "created_by": "lucasjinreal",
    "created_at": "2024-03-08T06:53:59Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1245</URL>\n\n<TITLE>How to test Lora model in checkpoint- middle checkpoints?</TITLE>\n\n<BODY>OSError: ./checkpoints/llava-qwen-4b-finetune-lora/ does not appear to have a file named config.json. Checkout 'https://huggingface.co/./checkpoints/llava-qwen-4b-finetune-lora//main' for available files.</BODY>\n\n<COMMENTS>\n<Comment by sahil02235 at 2024-03-23T14:52:29Z>\n@lucasjinreal \r\nActually you cant, its only saving the Lora weights, whereas u need the config.json file and non_lora_trainables.bin, which are saved after the training is done. \r\nThis is the problem with hugging face trainer, basically it only saves trainable parameters when u use FSDP. If you have any solution for this let me know.\n</Comment>\n<Comment by fisher75 at 2024-04-25T13:15:55Z>\nHi, how do you know the training was effecitve? Did you use the default training setting? I LoRA with default parameters and basically no improvement.\n</Comment>\n<Comment by APTX574 at 2024-11-04T13:35:21Z>\nI have the same question, I try to new a LLava class from the final output and load weight from the pt file in  checkpoint-xxx/global_stepxxx, but I can not get the weight , there are only the weight of the opt .\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1244,
    "state": "open",
    "created_by": "jeffreylm",
    "created_at": "2024-03-08T06:05:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1244</URL>\n\n<TITLE>[Usage] Cloned response via Open-WebUi</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI use OpenWeb UI\r\n\r\nWhen I give LLava an image to describe, it is giving me the exact same cloned description. Even when I restart the computer. I don't even understand how this is possible. What can I do to fix this?\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1243,
    "state": "open",
    "created_by": "JacobLee121",
    "created_at": "2024-03-08T04:10:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1243</URL>\n\n<TITLE>[Question] How to replace LLaVA's LLM with other LLM such as phi or Mistral</TITLE>\n\n<BODY>### Question\n\nHow to replace LLaVA's LLM with other LLM such as phi or Mistral？Does anyone have relevant practice?</BODY>\n\n<COMMENTS>\n<Comment by Isaachhh at 2024-03-09T11:47:23Z>\nhttps://github.com/BAAI-DCAI/Bunny\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1242,
    "state": "open",
    "created_by": "Rishbhu",
    "created_at": "2024-03-08T00:41:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1242</URL>\n\n<TITLE>Image not compiling through</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nWhen I run an image through the software, it shows 'undefined' was wondering if this can be fixed since I need to use LLaVA for a project I'm working on\r\n\r\nThank you\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.\r\n![Screenshot 2024-03-07 184047](https://github.com/haotian-liu/LLaVA/assets/99981600/09533a36-c071-4a03-b271-8b5be2021af2)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1241,
    "state": "open",
    "created_by": "PRSantos-BR",
    "created_at": "2024-03-07T17:49:28Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1241</URL>\n\n<TITLE>ERROR when running the CLI.</TITLE>\n\n<BODY>### Discussion\n\nGood afternoon everybody!\r\n\r\nI'm Paulo Roberto, born in Rio de Janeiro - Brazil\r\n\r\nI really need help.\r\n\r\nhttps://github.com/haotian-liu/LLaVA?tab=readme-ov-file#Demo\r\n\r\n\r\npython -m llava.serve.cli \\\r\n     --model-path liuhaotian/llava-v1.5-7b \\\r\n     --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n     --load-4bit\r\n\r\n\r\nMy environment is WSL2 (Windows 11)\r\n\r\nThe mistake\r\n\r\n(llava) prsantos@LAPTOP-940KNJ7R:~/Desenvolvimentos/ProjetosPython/ProjetosITTech/LLaVA$ python -m llava.serve.cli \\\r\n     --model-path liuhaotian/llava-v1.5-7b \\\r\n     --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n     --load-4bit\r\nTraceback (most recent call last):\r\n   File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n   File \"<frozen runpy>\", line 88, in _run_code\r\n   File \"/home/prsantos/Desenvolvimentos/ProjetosPython/ProjetosITTech/LLaVA/llava/serve/cli.py\", line 128, in <module>\r\n     main(args)\r\n   File \"/home/prsantos/Desenvolvimentos/ProjetosPython/ProjetosITTech/LLaVA/llava/serve/cli.py\", line 32, in main\r\n     tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^^^\r\n   File \"/home/prsantos/Desenvolvimentos/ProjetosPython/ProjetosITTech/LLaVA/llava/model/builder.py\", line 116, in load_pretrained_model\r\n     tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^\r\n   File \"/home/prsantos/miniconda3/envs/llava/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py\", line 814, in from_pretrained\r\n     return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n   File \"/home/prsantos/miniconda3/envs/llava/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2029, in from_pretrained\r\n     return cls._from_pretrained(\r\n            ^^^^^^^^^^^^^^^^^^^^^\r\n   File \"/home/prsantos/miniconda3/envs/llava/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2261, in _from_pretrained\r\n     tokenizer = cls(*init_inputs, **init_kwargs)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n   File \"/home/prsantos/miniconda3/envs/llava/lib/python3.11/site-packages/transformers/models/llama/tokenization_llama.py\", line 178, in __init__\r\n     self.sp_model = self.get_spm_processor(kwargs.pop(\"from_slow\", False))\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^\r\n   File \"/home/prsantos/miniconda3/envs/llava/lib/python3.11/site-packages/transformers/models/llama/tokenization_llama.py\", line 208, in get_spm_processor\r\n     model_pb2 = import_protobuf(f\"The new behavior of {self.__class__.__name__} (with `self.legacy = False`)\")\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n   File \"/home/prsantos/miniconda3/envs/llava/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py\", line 43, in import_protobuf\r\n     raise ImportError(PROTOBUF_IMPORT_ERROR.format(error_message))\r\nImportError:\r\nThe new behavior of LlamaTokenizer (with `self.legacy = False`) requires the protobuf library but it was not found in your environment. Checkout the instructions on the\r\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\r\nthat matches your environment. Please note that you may need to restart your runtime after installation.\r\n\r\n\r\nCan you help me please?\r\n\r\nI need to use LLaVA to analyze several images using a Python script.\r\n\r\nI'm having a lot of trouble getting this result.\r\n\r\nThank you in advance for the feedback.</BODY>\n\n<COMMENTS>\n<Comment by PRSantos-BR at 2024-03-07T18:38:20Z>\nMy e-mail\r\nprsantosbr@gmail.com\r\n\r\nPhone\r\n+55 21 97018-4747\n</Comment>\n<Comment by ubun23 at 2024-05-06T05:45:44Z>\nDo you slove this bug? I also encounter this 🌧️ . \r\nThanks for your kindly reply！ 🌹\n</Comment>\n<Comment by DingYX0731 at 2024-05-10T09:03:16Z>\nHave you try install `protobuf`? Maybe try `pip install protobuf`\n</Comment>\n<Comment by ubun23 at 2024-05-11T07:52:08Z>\nYes, I already sloved it by `pip install protobuf`\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1240,
    "state": "open",
    "created_by": "HelloWorldLTY",
    "created_at": "2024-03-07T15:47:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1240</URL>\n\n<TITLE>[Usage] LLaVA cannot handle the input case with space in the image name</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nIf my input file name contains space, LLaVa will return error. For example:\r\n\r\nMy file name is ' Malignant.png'\r\nusage: run_llava.py [-h] [--model-path MODEL_PATH] [--model-base MODEL_BASE] --image-file IMAGE_FILE --query QUERY\r\n                    [--conv-mode CONV_MODE] [--sep SEP] [--temperature TEMPERATURE] [--top_p TOP_P]\r\n                    [--num_beams NUM_BEAMS] [--max_new_tokens MAX_NEW_TOKENS]\r\nrun_llava.py: error: unrecognized arguments: Malignant.png\r\n\r\nCould you please choose another file name processor? Thanks.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1239,
    "state": "closed",
    "created_by": "20191864218",
    "created_at": "2024-03-07T13:57:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1239</URL>\n\n<TITLE>AttributeError: 'DeepSpeedCPUAdam' object has no attribute 'ds_opt_adam'</TITLE>\n\n<BODY>### Question\r\n\r\nWhen I encountered this error during run finetune_lora.sh, how should I resolve it? Thank you for your assistance\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train()\r\n  File \"/root/LLaVA/llava/train/train.py\", line 1098, in train\r\n    trainer.train()\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/transformers/trainer.py\", line 1690, in _inner_training_loop\r\n    model, self.optimizer, self.lr_scheduler = self.accelerator.prepare(\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1198, in prepare\r\n    result = self._prepare_deepspeed(*args)\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1537, in _prepare_deepspeed\r\n    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/deepspeed/__init__.py\", line 171, in initialize\r\n    engine = DeepSpeedEngine(args=args,\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 304, in __init__\r\n    self._configure_optimizer(optimizer, model_parameters)\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1208, in _configure_optimizer\r\n    basic_optimizer = self._configure_basic_optimizer(model_parameters)\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1279, in _configure_basic_optimizer\r\n    optimizer = DeepSpeedCPUAdam(model_parameters,\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/deepspeed/ops/adam/cpu_adam.py\", line 94, in __init__\r\n    self.ds_opt_adam = CPUAdamBuilder().load()\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/deepspeed/ops/op_builder/builder.py\", line 458, in load\r\n    return self.jit_load(verbose)\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/deepspeed/ops/op_builder/builder.py\", line 490, in jit_load\r\n    cxx_args = self.strip_empty_entries(self.cxx_args())\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/deepspeed/ops/op_builder/builder.py\", line 738, in cxx_args\r\n    CUDA_ENABLE = self.is_cuda_enable()\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/deepspeed/ops/op_builder/builder.py\", line 350, in is_cuda_enable\r\n    assert_no_cuda_mismatch(self.name)\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/deepspeed/ops/op_builder/builder.py\", line 101, in assert_no_cuda_mismatch\r\n    raise CUDAMismatchException(\r\ndeepspeed.ops.op_builder.builder.CUDAMismatchException: >- DeepSpeed Op Builder: Installed CUDA version 11.8 does not match the version torch was compiled with 12.1, unable to compile cuda/cpp extensions without a matching cuda version.\r\nException ignored in: <function DeepSpeedCPUAdam.__del__ at 0x7f7e8ecc7be0>\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/deepspeed/ops/adam/cpu_adam.py\", line 102, in __del__\r\n    self.ds_opt_adam.destroy_adam(self.opt_id)\r\nAttributeError: 'DeepSpeedCPUAdam' object has no attribute 'ds_opt_adam'\r\n[2024-03-07 21:33:13,113] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 38255</BODY>\n\n<COMMENTS>\n<Comment by happywinder at 2024-07-25T07:19:19Z>\nhello，could you share your recipe to solve this problem ，i met the same bug\n</Comment>\n<Comment by YanzhaoDrew at 2024-10-11T08:21:21Z>\n> hello，could you share your recipe to solve this problem ，i met the same bug\r\n\r\n```\r\nimport subprocess\r\nimport sys\r\nimport os\r\n\r\ndef install_deepspeed():\r\n    # Set environment variables\r\n    os.environ['DS_BUILD_CPU_ADAM'] = '1'\r\n    os.environ['BUILD_UTILS'] = '1'\r\n\r\n    # Construct the pip command\r\n    pip_command = [sys.executable, '-m', 'pip', 'install', 'deepspeed==0.14.4', '-U']\r\n\r\n    # Run the pip command\r\n    try:\r\n        subprocess.check_call(pip_command)\r\n        print(\"DeepSpeed installed successfully!\")\r\n    except subprocess.CalledProcessError as e:\r\n        print(f\"Error installing DeepSpeed: {e}\")\r\n\r\nif __name__ == \"__main__\":\r\n    install_deepspeed()\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1238,
    "state": "open",
    "created_by": "jotazu77",
    "created_at": "2024-03-07T09:39:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1238</URL>\n\n<TITLE>[Question] License</TITLE>\n\n<BODY>### Question\n\nWhich vision-to-text models are free to train and commercial-friendly?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1237,
    "state": "open",
    "created_by": "459737087",
    "created_at": "2024-03-07T05:02:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1237</URL>\n\n<TITLE>got nothing after finetune the llava-v1.5-13b [Usage]</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\ngot nothing after finetune the llava-v1.5-13b \r\npredict \"-------------------------\"\r\nor blank answer\r\n\r\nCommand:\r\n```\r\nPROMPT_VERSION=v1\r\nMODEL_VERSION=\"llava-v1.5-13b\"\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --lora_enable True \\\r\n    --model_name_or_path /output/$MODEL_VERSION \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path /output/data.json \\\r\n    --image_folder /output/train_data/input1/ \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter /output/$MODEL_VERSION/mm_projector.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/$MODEL_VERSION-finetune_lora_0306 \\\r\n    --num_train_epochs 100 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 5000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --dataloader_num_workers 4 \\\r\n    --report_to wandb\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/54131759/33c36101-10d4-407f-a604-8315f36f6a6e)</BODY>\n\n<COMMENTS>\n<Comment by ykzqjyhhh at 2024-11-23T07:14:26Z>\nI have the same problem as well. Have you resolved it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1236,
    "state": "closed",
    "created_by": "Rishbhu",
    "created_at": "2024-03-07T03:34:10Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1236</URL>\n\n<TITLE>Error when compiling pictures</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nWebsite not loading\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.\r\n![Screenshot 2024-03-06 213259](https://github.com/haotian-liu/LLaVA/assets/99981600/67240576-4872-4f1a-a2dd-2323d09cdfd0)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1235,
    "state": "open",
    "created_by": "thiner",
    "created_at": "2024-03-06T16:29:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1235</URL>\n\n<TITLE>[Question] How to load local model files</TITLE>\n\n<BODY>### Question\n\nI have dowloaded model files to local hard drive. I set the `model_path` value pointing to the local model folder, but seems the model worker still trying to download model files from huggingface.</BODY>\n\n<COMMENTS>\n<Comment by deadpipe at 2024-03-06T21:27:26Z>\nwhat command message are you using?\n</Comment>\n<Comment by thiner at 2024-03-07T01:46:27Z>\n@deadpipe I ran the model worker with below command:\r\n`python3 -m llava.serve.model_worker --port 40000 --model-path /opt/models/llava/llava-v1.6-34b --controller http://localhost:10000 --worker http://localhost:40000 --load-8bit `\r\n\r\nthe `/opt/models/llava/llava-v1.6-34b` is the local folder on server which contains the Llava-1.6-34b model files.\n</Comment>\n<Comment by deadpipe at 2024-03-14T21:16:49Z>\n@thiner  I earlier tried to load files from a different directory and i was facing a similar issue\r\nTry creating a folder \"liuhaotian\" within the LLaVA directory and keep the model \"llava/llava-v1.6-34b\" inside the folder\r\n\r\nthen run this command - \r\n\r\n`python3 -m llava.serve.model_worker --port 40000 --model-path liuhaotian/llava-v1.6-34b --controller http://localhost:10000 --worker http://localhost:40000 --load-8bit`\r\n\r\nThis works for me\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1234,
    "state": "open",
    "created_by": "20191864218",
    "created_at": "2024-03-06T14:25:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1234</URL>\n\n<TITLE>[Question] 请问有适用于LLaVA框架的医学视觉编码器吗，比如medclip或者medsam</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by YangQiuEric at 2024-03-06T23:58:03Z>\nI just changed the vision_tower to microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224, and the finetune can be done. But the model_vqa.py has errors, and can not recognize this model. I tried to download all the files from Hugging Face, but it does not have preprocess_config.json and config.json. How to fix it?\n</Comment>\n<Comment by 20191864218 at 2024-03-07T13:52:16Z>\n> I just changed the vision_tower to microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224, and the finetune can be done. But the model_vqa.py has errors, and can not recognize this model. I tried to download all the files from Hugging Face, but it does not have preprocess_config.json and config.json. How to fix it?\r\n\r\n我也是遇到了这个问题，我用的是medclip，出现以下错误\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train()\r\n  File \"/root/LLaVA/llava/train/train.py\", line 1039, in train\r\n    model.get_model().initialize_vision_modules(\r\n  File \"/root/LLaVA/llava/model/llava_arch.py\", line 71, in initialize_vision_modules\r\n    vision_tower = build_vision_tower(model_args)\r\n  File \"/root/LLaVA/llava/model/multimodal_encoder/builder.py\", line 11, in build_vision_tower\r\n    return CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\r\n  File \"/root/LLaVA/llava/model/multimodal_encoder/clip_encoder.py\", line 23, in __init__\r\n    self.load_model()\r\n  File \"/root/LLaVA/llava/model/multimodal_encoder/clip_encoder.py\", line 40, in load_model\r\n    self.vision_tower1 = MedCLIPModel(vision_cls=MedCLIPVisionModelViT, vision_checkpoint=\"LLaVA/llava/model/multimodal_encoder/medclip\").vision_model\r\n  File \"/root/LLaVA/llava/model/multimodal_encoder/good.py\", line 141, in __init__\r\n    self.vision_model = vision_cls(medclip_checkpoint=vision_checkpoint)\r\n  File \"/root/LLaVA/llava/model/multimodal_encoder/good.py\", line 103, in __init__\r\n    self.load_from_medclip(medclip_checkpoint)\r\n  File \"/root/LLaVA/llava/model/multimodal_encoder/good.py\", line 114, in load_from_medclip\r\n    missing_keys, unexpected_keys = self.load_state_dict(new_state_dict, strict=False)\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2152, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for MedCLIPVisionModelViT:\r\n        size mismatch for model.embeddings.patch_embeddings.projection.weight: copying a param with shape torch.Size([96, 3, 4, 4]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.embeddings.patch_embeddings.projection.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.embeddings.norm.weight: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.embeddings.norm.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.layernorm_before.weight: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.layernorm_before.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table: copying a param with shape torch.Size([169, 3]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.attention.self.query.weight: copying a param with shape torch.Size([96, 96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.attention.self.query.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.attention.self.key.weight: copying a param with shape torch.Size([96, 96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.attention.self.key.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.attention.self.value.weight: copying a param with shape torch.Size([96, 96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.attention.self.value.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.attention.output.dense.weight: copying a param with shape torch.Size([96, 96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.attention.output.dense.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.layernorm_after.weight: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.layernorm_after.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.intermediate.dense.weight: copying a param with shape torch.Size([384, 96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.intermediate.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.output.dense.weight: copying a param with shape torch.Size([96, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.0.output.dense.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.layernorm_before.weight: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.layernorm_before.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table: copying a param with shape torch.Size([169, 3]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.attention.self.query.weight: copying a param with shape torch.Size([96, 96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.attention.self.query.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.attention.self.key.weight: copying a param with shape torch.Size([96, 96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.attention.self.key.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.attention.self.value.weight: copying a param with shape torch.Size([96, 96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.attention.self.value.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.attention.output.dense.weight: copying a param with shape torch.Size([96, 96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.attention.output.dense.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.layernorm_after.weight: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.layernorm_after.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.intermediate.dense.weight: copying a param with shape torch.Size([384, 96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.intermediate.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.output.dense.weight: copying a param with shape torch.Size([96, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.blocks.1.output.dense.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.downsample.reduction.weight: copying a param with shape torch.Size([192, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.downsample.norm.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.0.downsample.norm.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.layernorm_before.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.layernorm_before.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table: copying a param with shape torch.Size([169, 6]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.attention.self.query.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.attention.self.query.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.attention.self.key.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.attention.self.key.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.attention.self.value.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.attention.self.value.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.attention.output.dense.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.attention.output.dense.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.layernorm_after.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.layernorm_after.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.intermediate.dense.weight: copying a param with shape torch.Size([768, 192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.intermediate.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.output.dense.weight: copying a param with shape torch.Size([192, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.0.output.dense.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.layernorm_before.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.layernorm_before.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table: copying a param with shape torch.Size([169, 6]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.attention.self.query.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.attention.self.query.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.attention.self.key.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.attention.self.key.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.attention.self.value.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.attention.self.value.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.attention.output.dense.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.attention.output.dense.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.layernorm_after.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.layernorm_after.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.intermediate.dense.weight: copying a param with shape torch.Size([768, 192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.intermediate.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.output.dense.weight: copying a param with shape torch.Size([192, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.blocks.1.output.dense.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.downsample.reduction.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.downsample.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.1.downsample.norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.layernorm_before.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.layernorm_before.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table: copying a param with shape torch.Size([169, 12]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.attention.self.query.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.attention.self.query.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.attention.self.key.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.attention.self.key.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.attention.self.value.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.attention.self.value.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.attention.output.dense.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.attention.output.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.layernorm_after.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.layernorm_after.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.intermediate.dense.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.intermediate.dense.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.output.dense.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.0.output.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.layernorm_before.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.layernorm_before.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table: copying a param with shape torch.Size([169, 12]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.attention.self.query.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.attention.self.query.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.attention.self.key.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.attention.self.key.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.attention.self.value.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.attention.self.value.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.attention.output.dense.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.attention.output.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.layernorm_after.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.layernorm_after.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.intermediate.dense.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.intermediate.dense.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.output.dense.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.1.output.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.layernorm_before.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.layernorm_before.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table: copying a param with shape torch.Size([169, 12]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.attention.self.query.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.attention.self.query.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.attention.self.key.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.attention.self.key.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.attention.self.value.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.attention.self.value.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.attention.output.dense.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.attention.output.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.layernorm_after.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.layernorm_after.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.intermediate.dense.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.intermediate.dense.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.output.dense.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.2.output.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.layernorm_before.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.layernorm_before.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table: copying a param with shape torch.Size([169, 12]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.attention.self.query.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.attention.self.query.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.attention.self.key.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.attention.self.key.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.attention.self.value.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.attention.self.value.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.attention.output.dense.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.attention.output.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.layernorm_after.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.layernorm_after.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.intermediate.dense.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.intermediate.dense.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.output.dense.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.3.output.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.layernorm_before.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.layernorm_before.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table: copying a param with shape torch.Size([169, 12]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.attention.self.query.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.attention.self.query.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.attention.self.key.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.attention.self.key.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.attention.self.value.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.attention.self.value.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.attention.output.dense.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.attention.output.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.layernorm_after.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.layernorm_after.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.intermediate.dense.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.intermediate.dense.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.output.dense.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.4.output.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.layernorm_before.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.layernorm_before.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table: copying a param with shape torch.Size([169, 12]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.attention.self.query.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.attention.self.query.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.attention.self.key.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.attention.self.key.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.attention.self.value.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.attention.self.value.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.attention.output.dense.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.attention.output.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.layernorm_after.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.layernorm_after.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.intermediate.dense.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.intermediate.dense.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.output.dense.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.blocks.5.output.dense.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.downsample.reduction.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.downsample.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.2.downsample.norm.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.layernorm_before.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.layernorm_before.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table: copying a param with shape torch.Size([169, 24]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.layernorm_after.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.layernorm_after.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.0.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.layernorm_before.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.layernorm_before.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table: copying a param with shape torch.Size([169, 24]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.layernorm_after.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.layernorm_after.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.encoder.layers.3.blocks.1.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.layernorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\r\n        size mismatch for model.layernorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([0]).\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1233,
    "state": "open",
    "created_by": "criminact",
    "created_at": "2024-03-06T14:22:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1233</URL>\n\n<TITLE>[Question] How to swap a Mistral finetuned LLM into LLava1.6</TITLE>\n\n<BODY>### Question\n\nI want to switch the language model in llava1.6 to a custom mistral finteuned model, Is this possible and how?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1232,
    "state": "open",
    "created_by": "bf-yang",
    "created_at": "2024-03-06T14:15:50Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1232</URL>\n\n<TITLE>[Question] Which part of model parameters is trained if I use LoRA fine-tuning?</TITLE>\n\n<BODY>### Question\n\nHi, thanks for you great work! \r\nI have a question about the LoRA fine-tuning. If I use LoRA technique to fine-tune LLaVA, which parameters of LLaVA is fine-tuned? e.g., the LLM attention? or projection layer? Thanks!</BODY>\n\n<COMMENTS>\n<Comment by fisher75 at 2024-04-25T13:15:36Z>\nMy question too\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1231,
    "state": "open",
    "created_by": "lucasjinreal",
    "created_at": "2024-03-06T06:25:21Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1231</URL>\n\n<TITLE>[Question] The loss first normal, but suddenly become 0</TITLE>\n\n<BODY>### Question\n\n```\r\n{'loss': 1.0176, 'learning_rate': 1.6180582605257102e-05, 'epoch': 0.62}                                                                                                                                                        \r\n{'loss': 0.9101, 'learning_rate': 1.6180267518266783e-05, 'epoch': 0.62}                                                                                                                                                        \r\n{'loss': 0.8977, 'learning_rate': 1.6179952421348435e-05, 'epoch': 0.62}                                                                                                                                                        \r\n{'loss': 0.948, 'learning_rate': 1.6179637314502555e-05, 'epoch': 0.62}                                                                                                                                                         \r\n{'loss': 1.2777, 'learning_rate': 1.6179322197729653e-05, 'epoch': 0.62}                                                                                                                                                        \r\n{'loss': 1.3493, 'learning_rate': 1.6179007071030236e-05, 'epoch': 0.62}                                                                                                                                                        \r\n{'loss': 1.3082, 'learning_rate': 1.6178691934404813e-05, 'epoch': 0.62}                                                                                                                                                        \r\n{'loss': 1.0649, 'learning_rate': 1.6178376787853884e-05, 'epoch': 0.62}                                                                                                                                                        \r\n{'loss': 1.4151, 'learning_rate': 1.6178061631377956e-05, 'epoch': 0.62}                                                                                                                                                        \r\n{'loss': 1.0117, 'learning_rate': 1.6177746464977546e-05, 'epoch': 0.62}                                                                                                                                                        \r\n{'loss': 0.975, 'learning_rate': 1.617743128865314e-05, 'epoch': 0.62}                                                                                                                                                          \r\n{'loss': 0.9362, 'learning_rate': 1.6177116102405265e-05, 'epoch': 0.62}                                                                                                                                                        \r\n{'loss': 1.0906, 'learning_rate': 1.6176800906234415e-05, 'epoch': 0.62}                                                                                                                                                        \r\n{'loss': 0.873, 'learning_rate': 1.61764857001411e-05, 'epoch': 0.62}                                                                                                                                                           \r\n{'loss': 1.0242, 'learning_rate': 1.617617048412583e-05, 'epoch': 0.62}                                                                                                                                                         \r\n{'loss': 0.8448, 'learning_rate': 1.6175855258189102e-05, 'epoch': 0.62}                                                                                                                                                        \r\n{'loss': 1.0232, 'learning_rate': 1.6175540022331428e-05, 'epoch': 0.62}                                                                                                                                                        \r\n{'loss': 0.7128, 'learning_rate': 1.6175224776553317e-05, 'epoch': 0.62}                                                                                                                                                        \r\n{'loss': 0.9428, 'learning_rate': 1.6174909520855272e-05, 'epoch': 0.62}                                                                                                                                                        \r\n{'loss': 0.9259, 'learning_rate': 1.61745942552378e-05, 'epoch': 0.62}                                                                                                                                                          \r\n{'loss': 0.8517, 'learning_rate': 1.617427897970141e-05, 'epoch': 0.62}                                                                                                                                                         \r\n{'loss': 0.0, 'learning_rate': 1.61739636942466e-05, 'epoch': 0.62}                                                                                                                                                             \r\n{'loss': 0.0, 'learning_rate': 1.6173648398873888e-05, 'epoch': 0.62}                                                                                                                                                           \r\n{'loss': 0.0, 'learning_rate': 1.617333309358377e-05, 'epoch': 0.62}                                                                                                                                                            \r\n{'loss': 0.0, 'learning_rate': 1.6173017778376762e-05, 'epoch': 0.62}                                                                                                                                                           \r\n{'loss': 0.0, 'learning_rate': 1.6172702453253365e-05, 'epoch': 0.62} \r\n```\r\n\r\nthe loss first normal, but suddenly got 0, training on stage2. Base model is qwen2\r\n\r\nit was actually worked on model at 0.5 epoch, but got 0 later.\r\n\r\nWhy</BODY>\n\n<COMMENTS>\n<Comment by patrick-tssn at 2024-03-18T01:49:04Z>\nwhen I change zero3 to zero2-offload, I meet the same problem, and my base model is Vicuna.\n</Comment>\n<Comment by patrick-tssn at 2024-03-18T06:16:46Z>\nIn my case, setting the 'overlap_comm' parameter to false resulted in the disappearance of the phenomenon. This issue may originate from DeepSpeed, although I cannot confirm this with certainty. Inspired by [this issue](https://github.com/microsoft/DeepSpeed/issues/4298)\n</Comment>\n<Comment by lucasjinreal at 2024-03-18T06:18:50Z>\n@patrick-tssn Hi, you mean, change:\r\n\r\n```\r\n\"zero_optimization\": {\r\n        \"stage\": 2,\r\n        \"overlap_comm\": true,\r\n        \"contiguous_gradients\": true,\r\n        \"sub_group_size\": 1e9,\r\n        \"reduce_bucket_size\": \"auto\"\r\n    }\r\n```\r\n\r\nthis to false? \r\n\r\nHave u tested if the training can result normal loss and normal result?\n</Comment>\n<Comment by patrick-tssn at 2024-03-18T06:25:02Z>\n> @patrick-tssn Hi, you mean, change:\r\n> \r\n> ```\r\n> \"zero_optimization\": {\r\n>         \"stage\": 2,\r\n>         \"overlap_comm\": true,\r\n>         \"contiguous_gradients\": true,\r\n>         \"sub_group_size\": 1e9,\r\n>         \"reduce_bucket_size\": \"auto\"\r\n>     }\r\n> ```\r\n> \r\n> this to false?\r\n> \r\n> Have u tested if the training can result normal loss and normal result?\r\n\r\nYes, setting `\"overlap_comm\": false` worked for me; I am now getting the normal loss. However, I haven't evaluated it yet as I am still in the process of training.\n</Comment>\n<Comment by lucasjinreal at 2024-03-18T07:46:41Z>\nI am tested same zero2 config on finetuning whole model directly with SAM & a Opt model except `overlap_comm`, the loss with `overlap_comm` set to True always exceed to maximum value (Nan) while false loss seems can decrease now.\r\n\r\nAnd the first iter loss doubled if set to True.\r\n\r\nAny reason for the differences? Looks like for any full-model task, if using zero2, it should overlap_coomn to False\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1230,
    "state": "open",
    "created_by": "deadpipe",
    "created_at": "2024-03-05T20:57:23Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1230</URL>\n\n<TITLE>ERROR : Connection Refused by SGL backend when launching with Llava-1.6-34b</TITLE>\n\n<BODY>Hello\r\n\r\nI am trying to launch the **34b model** on **WSL2 (ubuntu 22.04)**\r\n\r\nI have launched 4 instances of command prompt and entered the following commands: \r\n\r\n1. `python -m llava.serve.controller --host 0.0.0.0 --port 10000`\r\n\r\n2. `python -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload`\r\n\r\n3. `CUDA_VISIBLE_DEVICES=0,1 python3 -m sglang.launch_server --model-path \"/mnt/s/Models/llava-v1.6-34b\" --tokenizer-path liuhaotian/llava-v1.6-34b-tokenizer --port 30000 --tp 2`\r\n\r\n4. `python -m llava.serve.sglang_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --sgl-endpoint http://127.0.0.1:30000`\r\n\r\n\r\nafter the 4th command, i get error : \r\n\r\n(llava) badapadda@DESKTOP-UHHHGJ4:~/llava/LLaVA$ python -m llava.serve.sglang_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --sgl-endpoint http://localhost:30000\r\n2024-03-06 02:16:59 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_name=None, sgl_endpoint='http://localhost:30000', limit_model_concurrency=5, stream_interval=1, no_register=False)\r\n2024-03-06 02:16:59 | ERROR | stderr | Traceback (most recent call last):\r\n2024-03-06 02:16:59 | ERROR | stderr |   File \"/home/badapadda/miniconda3/envs/llava/lib/python3.10/urllib/request.py\", line 1348, in do_open\r\n2024-03-06 02:16:59 | ERROR | stderr |     h.request(req.get_method(), req.selector, req.data, headers,\r\n2024-03-06 02:16:59 | ERROR | stderr |   File \"/home/badapadda/miniconda3/envs/llava/lib/python3.10/http/client.py\", line 1283, in request\r\n2024-03-06 02:16:59 | ERROR | stderr |     self._send_request(method, url, body, headers, encode_chunked)\r\n2024-03-06 02:16:59 | ERROR | stderr |   File \"/home/badapadda/miniconda3/envs/llava/lib/python3.10/http/client.py\", line 1329, in _send_request\r\n2024-03-06 02:16:59 | ERROR | stderr |     self.endheaders(body, encode_chunked=encode_chunked)\r\n2024-03-06 02:16:59 | ERROR | stderr |   File \"/home/badapadda/miniconda3/envs/llava/lib/python3.10/http/client.py\", line 1278, in endheaders\r\n2024-03-06 02:16:59 | ERROR | stderr |     self._send_output(message_body, encode_chunked=encode_chunked)\r\n2024-03-06 02:16:59 | ERROR | stderr |   File \"/home/badapadda/miniconda3/envs/llava/lib/python3.10/http/client.py\", line 1038, in _send_output\r\n2024-03-06 02:16:59 | ERROR | stderr |     self.send(msg)\r\n2024-03-06 02:16:59 | ERROR | stderr |   File \"/home/badapadda/miniconda3/envs/llava/lib/python3.10/http/client.py\", line 976, in send\r\n2024-03-06 02:16:59 | ERROR | stderr |     self.connect()\r\n2024-03-06 02:16:59 | ERROR | stderr |   File \"/home/badapadda/miniconda3/envs/llava/lib/python3.10/http/client.py\", line 942, in connect\r\n2024-03-06 02:16:59 | ERROR | stderr |     self.sock = self._create_connection(\r\n2024-03-06 02:16:59 | ERROR | stderr |   File \"/home/badapadda/miniconda3/envs/llava/lib/python3.10/socket.py\", line 845, in create_connection\r\n2024-03-06 02:16:59 | ERROR | stderr |     raise err\r\n2024-03-06 02:16:59 | ERROR | stderr |   File \"/home/badapadda/miniconda3/envs/llava/lib/python3.10/socket.py\", line 833, in create_connection\r\n2024-03-06 02:16:59 | ERROR | stderr |     sock.connect(sa)\r\n2024-03-06 02:16:59 | ERROR | stderr | ConnectionRefusedError: [Errno 111] Connection refused\r\n2024-03-06 02:16:59 | ERROR | stderr |\r\n2024-03-06 02:16:59 | ERROR | stderr | During handling of the above exception, another exception occurred:\r\n2024-03-06 02:16:59 | ERROR | stderr |\r\n2024-03-06 02:16:59 | ERROR | stderr | Traceback (most recent call last):\r\n2024-03-06 02:16:59 | ERROR | stderr |   File \"/home/badapadda/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n2024-03-06 02:16:59 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n2024-03-06 02:16:59 | ERROR | stderr |   File \"/home/badapadda/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n2024-03-06 02:16:59 | ERROR | stderr |     exec(code, run_globals)\r\n2024-03-06 02:16:59 | ERROR | stderr |   File \"/home/badapadda/llava/LLaVA/llava/serve/sglang_worker.py\", line 238, in <module>\r\n2024-03-06 02:16:59 | ERROR | stderr |     worker = ModelWorker(args.controller_address,\r\n2024-03-06 02:16:59 | ERROR | stderr |   File \"/home/badapadda/llava/LLaVA/llava/serve/sglang_worker.py\", line 62, in __init__\r\n2024-03-06 02:16:59 | ERROR | stderr |     backend = RuntimeEndpoint(sgl_endpoint)\r\n2024-03-06 02:16:59 | ERROR | stderr |   File \"/home/badapadda/miniconda3/envs/llava/lib/python3.10/site-packages/sglang/backend/runtime_endpoint.py\", line 22, in __init__\r\n2024-03-06 02:16:59 | ERROR | stderr |     res = http_request(\r\n2024-03-06 02:16:59 | ERROR | stderr |   File \"/home/badapadda/miniconda3/envs/llava/lib/python3.10/site-packages/sglang/utils.py\", line 110, in http_request\r\n2024-03-06 02:16:59 | ERROR | stderr |     resp = urllib.request.urlopen(req, data=data)\r\n2024-03-06 02:16:59 | ERROR | stderr |   File \"/home/badapadda/miniconda3/envs/llava/lib/python3.10/urllib/request.py\", line 216, in urlopen\r\n2024-03-06 02:16:59 | ERROR | stderr |     return opener.open(url, data, timeout)\r\n2024-03-06 02:16:59 | ERROR | stderr |   File \"/home/badapadda/miniconda3/envs/llava/lib/python3.10/urllib/request.py\", line 519, in open\r\n2024-03-06 02:16:59 | ERROR | stderr |     response = self._open(req, data)\r\n2024-03-06 02:16:59 | ERROR | stderr |   File \"/home/badapadda/miniconda3/envs/llava/lib/python3.10/urllib/request.py\", line 536, in _open\r\n2024-03-06 02:16:59 | ERROR | stderr |     result = self._call_chain(self.handle_open, protocol, protocol +\r\n2024-03-06 02:16:59 | ERROR | stderr |   File \"/home/badapadda/miniconda3/envs/llava/lib/python3.10/urllib/request.py\", line 496, in _call_chain\r\n2024-03-06 02:16:59 | ERROR | stderr |     result = func(*args)\r\n2024-03-06 02:16:59 | ERROR | stderr |   File \"/home/badapadda/miniconda3/envs/llava/lib/python3.10/urllib/request.py\", line 1377, in http_open\r\n2024-03-06 02:16:59 | ERROR | stderr |     return self.do_open(http.client.HTTPConnection, req)\r\n2024-03-06 02:16:59 | ERROR | stderr |   File \"/home/badapadda/miniconda3/envs/llava/lib/python3.10/urllib/request.py\", line 1351, in do_open\r\n2024-03-06 02:16:59 | ERROR | stderr |     raise URLError(err)\r\n2024-03-06 02:16:59 | ERROR | stderr | urllib.error.URLError: <urlopen error [Errno 111] Connection refused>\r\n\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/142412134/451366b9-2cb3-4357-88a7-a566bd6718a1)\r\n\r\nAny help is highly appreciated \r\n\r\n\r\n\r\n\r\nUPDATE :\r\n\r\nI installed Ubuntu 22 dual boot on my computer and followed the same steps as above and i get the exact same error\r\nSee Screenshot below :</BODY>\n\n<COMMENTS>\n<Comment by EYcab at 2024-03-21T07:07:27Z>\nMet the exact same problem\n</Comment>\n<Comment by deadpipe at 2024-03-21T09:58:43Z>\nhave you found a solution @EYcab ?\n</Comment>\n<Comment by EYcab at 2024-03-22T09:25:08Z>\n@deadpipe ,no,have you?\n</Comment>\n<Comment by deadpipe at 2024-03-22T10:13:40Z>\nno, @EYcab \n\nbut a temporary solution is to not use Sgl backend at all. Use the traditional old way and launch llava-1.6-34b with these three commands -\n\n`python -m llava.serve.controller --host 0.0.0.0 --port 10000`\n\n\n`python -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload`\n\n`CUDA_VISIBLE_DEVICES=0 python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.6-34b --load-4bit`\n\nAnd the model runs on 20GB Vram\n</Comment>\n<Comment by EYcab at 2024-03-25T09:29:31Z>\n@deadpipe ,got you.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1229,
    "state": "open",
    "created_by": "lTloser",
    "created_at": "2024-03-05T16:22:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1229</URL>\n\n<TITLE>[Question] How to stop automated questions</TITLE>\n\n<BODY>### Question\n\nHow to stop automated questions in LLaVA-1.6， any parameter to control？\r\n<img width=\"1063\" alt=\"截屏2024-03-06 00 21 49\" src=\"https://github.com/haotian-liu/LLaVA/assets/55436871/e962e6ed-069f-4a45-a536-db316c04a0b2\"></BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1228,
    "state": "open",
    "created_by": "20191864218",
    "created_at": "2024-03-05T12:54:29Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1228</URL>\n\n<TITLE>[Question] How to convert the LLaVA training data format?</TITLE>\n\n<BODY>### Question\n\nMy dataset consists of an image and a corresponding text description. How can I convert them into the dataset format suitable for training with LLaVA?Thanks！</BODY>\n\n<COMMENTS>\n<Comment by nnethercott at 2024-03-06T12:50:35Z>\nhttps://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1227,
    "state": "closed",
    "created_by": "nixsui",
    "created_at": "2024-03-05T11:54:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1227</URL>\n\n<TITLE>[Question]  cuda out of memory  while i run the script  of finetune_task.sh</TITLE>\n\n<BODY>### Question\n\n\r\ndeepspeed  --include localhost:1  --master_port 29597 llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path /ssd1/suixin02/data/exp/llava/liuhaotian/llava-v1.6-vicuna-7b \\\r\n    --version v1 \\\r\n    --data_path /ssd1/suixin02/data/exp/llava/liuhaotian/LLaVA-Instruct-150K/chinese_and_original.json \\\r\n    --image_folder /ssd1/suixin02/data/exp/llava/liuhaotian/LLaVA-Instruct-150K \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir /ssd1/suixin02/data/exp/llava/checkpoints/llava-v1.5-34b-task-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 2 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 16 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb</BODY>\n\n<COMMENTS>\n<Comment by eslambakr at 2024-04-01T18:18:14Z>\nI am facing the same issue.\r\nDid u know how to solve it?\r\nThanks in advance!\n</Comment>\n<Comment by AngelAlita at 2024-04-20T11:06:56Z>\nWhen i finetune LLaVA-1.5-7B in 4 3090-24GB,I meet the same probelm.\r\nHow did you solve it? Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1226,
    "state": "open",
    "created_by": "aymenabid-lab",
    "created_at": "2024-03-05T08:14:23Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1226</URL>\n\n<TITLE>How to add model from PC into gradio web server</TITLE>\n\n<BODY>### Question\n\nThe model is in local folder as figured in the images\r\ncontroller, gradio web server & model worker (Multiple GPUs, when GPU VRAM <= 24GB) are loaded as follow in the images\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/63906728/b060efe9-4f8d-4298-bd68-065b5413c89d)\r\n![image](https://github.com/haotian-liu/LLaVA/assets/63906728/5372c2d8-6749-4543-8568-78fcc57fa81b)\r\n![image](https://github.com/haotian-liu/LLaVA/assets/63906728/de0876aa-36bc-4f7e-9189-1d1c9e1c2921)\r\n![image](https://github.com/haotian-liu/LLaVA/assets/63906728/af7c73c6-9def-4f52-9b28-f25a42a1c01f)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1224,
    "state": "open",
    "created_by": "harrytea",
    "created_at": "2024-03-04T19:10:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1224</URL>\n\n<TITLE>save vision tower</TITLE>\n\n<BODY>### Describe the issue\n\nIf i use fsdp, why should i make the vision_tower to be a list?\r\nI am trying to use fsdp to finetune the vision tower, how to save this list file if i use fsdp?\r\nThanks!\r\n\r\nif fsdp is not None and len(fsdp) > 0:\r\n                self.vision_tower = [vision_tower]\r\n            else:\r\n                self.vision_tower = vision_tower</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1222,
    "state": "open",
    "created_by": "Marcophono2",
    "created_at": "2024-03-04T15:02:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1222</URL>\n\n<TITLE>GPU utilized with only about 20%</TITLE>\n\n<BODY>### Describe the issue\n\nHello!\r\n\r\nI am wondering if the inference speed couldn't be improved significantly if the GPU utilization would come near to 100%. My RTX 4090 is only at about 20%. Or is this unusual and I have one or more issues with depencies versions or else?\r\nOn the other hand my CPU utilization is at 100% (used only one core on a 5900x). Or is the CPU the bottleneck here?\r\n\r\nBest regards\r\nMarc</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1221,
    "state": "open",
    "created_by": "airogachev",
    "created_at": "2024-03-04T14:16:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1221</URL>\n\n<TITLE>[Question] Inferencing on an image folder</TITLE>\n\n<BODY>### Question\n\nHow to inference model on a folder of images, using batched inference?\r\nPassing image_tensor as list of images doesn't provide an output for every single image</BODY>\n\n<COMMENTS>\n<Comment by idan-tankel at 2024-03-10T20:27:09Z>\nI had used something similar to what was presented [here](https://github.com/NielsRogge/Transformers-Tutorials/blob/46391b376125160924757efe40c3fbd41ddbc799/LLaVa/Inference_with_LLaVa_for_multimodal_generation.ipynb). Specifically tried that with a list of prompts and a list of images which are the same. It seems to go through without errors, but sometimes the predictions are not that good, and I have something like \"user: user: user:...\" in the output\r\nSo I guess that is the correct method to do batching, and maybe I had missed something on the way through\n</Comment>\n<Comment by idan-tankel at 2024-03-11T06:13:17Z>\nalso adding  pad_token=\"\\<pad\\>\" to the processor seems to help\r\n```python\r\nAutoProcessor.from_pretrained(..., pad_token=\"<pad>\")\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1220,
    "state": "open",
    "created_by": "airogachev",
    "created_at": "2024-03-04T11:56:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1220</URL>\n\n<TITLE>[Question] LLAVA 1.6 inference acceleration</TITLE>\n\n<BODY>### Question\n\nDoes any kind of script that allows to perform accelerated inference of LLAVA 1.6 exists?\r\nThere is a [4-bit quantization example](https://huggingface.co/llava-hf/llava-1.5-7b-hf#4-bit-quantization-through-bitsandbytes-library) for 1.5, but there is no 1.6 model for that hf repo. \r\n\r\nAnyway, I'm looking for the currently fastest way of inference.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1219,
    "state": "closed",
    "created_by": "SizheWei",
    "created_at": "2024-03-04T11:12:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1219</URL>\n\n<TITLE>ValueError `offload_dir` needed. Evaluation llava-v1.5-7b-lora on vqav2 on 4x3090</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI followed the tutorial to train llava-v1.5, after pretaining and finetuning the 7b model via lora, I want to inference it on vqav2 dataset, but I met the ValueError listed below: \r\n\r\nCommand:\r\n```\r\nCUDA_VISIBLE_DEVICES=0,1,2,3 bash ./scripts/v1_5/eval/vqav2.sh \r\n```\r\n\r\nLog: \r\n```\r\nLoading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.64it/s]\r\nLoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.42it/s]\r\nLoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.28it/s]\r\nSome weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at lmsys/vicuna-7b-v1.5 and are newly initialized: ['model.mm_projector.0.bias', 'model.mm_projector.0.weight', 'model.mm_projector.2.bias', 'model.mm_projector.2.weight']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n/data/miniconda/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\r\n  warnings.warn(\r\n/data/miniconda/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\r\n  warnings.warn(\r\n/data/miniconda/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\r\n  warnings.warn(\r\n/data/miniconda/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\r\n  warnings.warn(\r\nTraceback (most recent call last):\r\n  File \"/data/miniconda/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/data/miniconda/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/data/LLaVA/llava/eval/model_vqa_loader.py\", line 144, in <module>\r\n    eval_model(args)\r\n  File \"/data/LLaVA/llava/eval/model_vqa_loader.py\", line 84, in eval_model\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, args.model_base, model_name)\r\n  File \"/data/LLaVA/llava/model/builder.py\", line 83, in load_pretrained_model\r\n    model = PeftModel.from_pretrained(model, model_path)\r\n  File \"/data/miniconda/envs/llava/lib/python3.10/site-packages/peft/peft_model.py\", line 354, in from_pretrained\r\n    model.load_adapter(model_id, adapter_name, is_trainable=is_trainable, **kwargs)\r\n  File \"/data/miniconda/envs/llava/lib/python3.10/site-packages/peft/peft_model.py\", line 728, in load_adapter\r\n    dispatch_model(\r\n  File \"/data/miniconda/envs/llava/lib/python3.10/site-packages/accelerate/big_modeling.py\", line 343, in dispatch_model\r\n    raise ValueError(\r\nValueError: We need an `offload_dir` to dispatch this model according to this `device_map`, the following submodules need to be offloaded: base_model.model.model.layers.22, base_model.model.model.layers.23, base_model.model.model.layers.24, base_model.model.model.layers.25, base_model.model.model.layers.26, base_model.model.model.layers.27, base_model.model.model.layers.28, base_model.model.model.layers.29, base_model.model.model.layers.30, base_model.model.model.layers.31, base_model.model.model.norm, base_model.model.model.vision_tower, base_model.model.model.mm_projector, base_model.model.lm_head.\r\nLoading additional LLaVA weights...\r\nLoading LoRA weights...\r\n```</BODY>\n\n<COMMENTS>\n<Comment by SizheWei at 2024-03-05T06:47:15Z>\nI've resolved the issue. There appeared to be a problem with my GPU card, but after reconnecting to my GPU server, it now operates without any bugs.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1218,
    "state": "open",
    "created_by": "alvinxjm",
    "created_at": "2024-03-04T08:50:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1218</URL>\n\n<TITLE>[Usage] TypeError: LlavaLlamaForCausalLM.forward() got an unexpected keyword argument 'cache_position'</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nModel Worker is showing this error when i chat with it.\r\nTypeError: LlavaLlamaForCausalLM.forward() got an unexpected keyword argument 'cache_position'\r\n\r\nLog: \r\n\r\n2024-03-04 08:41:32 | ERROR | stderr | Exception in thread Thread-3 (generate):\r\n2024-03-04 08:41:32 | ERROR | stderr | Traceback (most recent call last):\r\n2024-03-04 08:41:32 | ERROR | stderr |   File \"/home/ec2-user/anaconda3/envs/llava/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\r\n2024-03-04 08:41:32 | ERROR | stderr |     self.run()\r\n2024-03-04 08:41:32 | ERROR | stderr |   File \"/home/ec2-user/anaconda3/envs/llava/lib/python3.10/threading.py\", line 953, in run\r\n2024-03-04 08:41:32 | ERROR | stderr |     self._target(*self._args, **self._kwargs)\r\n2024-03-04 08:41:32 | ERROR | stderr |   File \"/home/ec2-user/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n2024-03-04 08:41:32 | ERROR | stderr |     return func(*args, **kwargs)\r\n2024-03-04 08:41:32 | ERROR | stderr |   File \"/home/ec2-user/LLaVA/llava/model/language_model/llava_llama.py\", line 138, in generate\r\n2024-03-04 08:41:32 | ERROR | stderr |     return super().generate(\r\n2024-03-04 08:41:32 | ERROR | stderr |   File \"/home/ec2-user/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n2024-03-04 08:41:32 | ERROR | stderr |     return func(*args, **kwargs)\r\n2024-03-04 08:41:32 | ERROR | stderr |   File \"/home/ec2-user/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1592, in generate\r\n2024-03-04 08:41:32 | ERROR | stderr |     return self.sample(\r\n2024-03-04 08:41:32 | ERROR | stderr |   File \"/home/ec2-user/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2696, in sample\r\n2024-03-04 08:41:32 | ERROR | stderr |     outputs = self(\r\n2024-03-04 08:41:32 | ERROR | stderr |   File \"/home/ec2-user/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n2024-03-04 08:41:32 | ERROR | stderr |     return self._call_impl(*args, **kwargs)\r\n2024-03-04 08:41:32 | ERROR | stderr |   File \"/home/ec2-user/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n2024-03-04 08:41:32 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2024-03-04 08:41:32 | ERROR | stderr |   File \"/home/ec2-user/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2024-03-04 08:41:32 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2024-03-04 08:41:32 | ERROR | stderr | TypeError: LlavaLlamaForCausalLM.forward() got an unexpected keyword argument 'cache_position'\r\n\r\nScreenshots:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/80762904/a0c435ce-655a-4465-8681-58a98558a05b)\r\n![image](https://github.com/haotian-liu/LLaVA/assets/80762904/f378b1b6-4606-40b1-b2d5-be455e782075)\r\n![image](https://github.com/haotian-liu/LLaVA/assets/80762904/1ea249d4-b81c-43ed-aa62-1bd58687fb47)</BODY>\n\n<COMMENTS>\n<Comment by londee at 2024-03-04T10:57:43Z>\nI encountered the same error while running \"eval_mode\".\r\n\r\n>>> eval_model(args)\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.27s/it]\r\n/root/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\r\n  warnings.warn(\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/humaodi/code/LLaVA/llava/eval/run_llava.py\", line 115, in eval_model\r\n    output_ids = model.generate(\r\n  File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/humaodi/code/LLaVA/llava/model/language_model/llava_llama.py\", line 137, in generate\r\n    return super().generate(\r\n  File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1544, in generate\r\n    return self.greedy_search(\r\n  File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2404, in greedy_search\r\n    outputs = self(\r\n  File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\nTypeError: LlavaLlamaForCausalLM.forward() got an unexpected keyword argument 'cache_position'\n</Comment>\n<Comment by scp92 at 2024-03-04T15:44:03Z>\nI encountered the same error too. Any ideas?\n</Comment>\n<Comment by zfreeman32 at 2024-03-04T16:14:17Z>\nGot this error as well. Not been able to fix yet. Tracking this Issue.\n</Comment>\n<Comment by shashwat14 at 2024-03-04T21:33:43Z>\nI had the same issue. Fixed it by ensuring the transformers version to be same as the one mentioned in pyproject.toml, i.e. transformers==4.37.2\n</Comment>\n<Comment by FanHengbo at 2024-03-05T01:32:13Z>\n> I had the same issue. Fixed it by ensuring the transformers version to be same as the one mentioned in pyproject.toml, i.e. transformers==4.37.2\r\n\r\nProblem solved. Thanks!\n</Comment>\n<Comment by lixiaoxiangzhi at 2024-03-05T01:34:34Z>\n> Problem solved. Thanks!\n</Comment>\n<Comment by aliencaocao at 2024-03-06T16:28:53Z>\nthis was because transformers 4.38.0 added static cache. So have to use any version below.\n</Comment>\n<Comment by lixiaoxiangzhi at 2024-03-06T16:29:15Z>\n您的来信已收到，祝您每天有个好心情。\n</Comment>\n<Comment by YFCYFC at 2024-03-11T13:32:29Z>\nhttps://github.com/haotian-liu/LLaVA/issues/1218#issuecomment-1977497811\r\nThank you so much, my problem was solved via degrading transformers==4.37.2.The transformers package interfaces changes frequently, which makes me so confused that I have to spend too much time to debug the meaningless bugs.\n</Comment>\n<Comment by RandomInternetPreson at 2024-03-15T23:18:38Z>\n> I had the same issue. Fixed it by ensuring the transformers version to be same as the one mentioned in pyproject.toml, i.e. transformers==4.37.2\r\n\r\nTY <3\n</Comment>\n<Comment by wyxscir at 2024-03-23T06:42:57Z>\n> I had the same issue. Fixed it by ensuring the transformers version to be same as the one mentioned in pyproject.toml, i.e. transformers==4.37.2\r\n\r\nthank you!!!!!!\n</Comment>\n<Comment by segalinc at 2024-04-23T17:59:07Z>\nnew models use transformers>4.39 is there a way to actually fix this ?\n</Comment>\n<Comment by lixiaoxiangzhi at 2024-04-23T17:59:39Z>\n您的来信已收到，祝您每天有个好心情。\n</Comment>\n<Comment by foundwant at 2024-04-24T07:35:10Z>\nTypeError: LlavaLlamaForCausalLM.forward() got an unexpected keyword argument 'cache_position'\r\nmy transformers version is 4.37.2，also have this problem.\n</Comment>\n<Comment by baichuanzhou at 2024-05-16T09:24:57Z>\nHey, adding cache_position=None to the forward method also works. Check [here](https://github.com/LLaVA-VL/LLaVA-NeXT/blob/inference/llava/model/language_model/llava_llama.py#L80C9-L80C23)\n</Comment>\n<Comment by lixiaoxiangzhi at 2024-05-16T09:25:29Z>\n您的来信已收到，祝您每天有个好心情。\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1217,
    "state": "closed",
    "created_by": "wangzhao0217",
    "created_at": "2024-03-04T08:35:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1217</URL>\n\n<TITLE>[Question] Only Tensors of floating point and complex dtype can require gradients</TITLE>\n\n<BODY>### Question\r\n\r\nDear Haotian,\r\n\r\nCongratulations on your outstanding work! I am currently engaged in a project sponsored by the European Space Agency, aiming to utilize satellite imagery and machine learning to determine road surface conditions. I plan to assess the capabilities of the LLaVA model in this scenario by fine-tuning it with our dataset.\r\n\r\nFor the fine-tuning process, I utilized Google Colab, and it was successful. \r\n```\r\n!deepspeed /content/LLaVA/llava/train/train_mem.py \\\r\n    --deepspeed /content/LLaVA/scripts/zero2.json \\\r\n    --lora_enable True \\\r\n    --lora_r 128 \\\r\n    --lora_alpha 256 \\\r\n    --mm_projector_lr 2e-5 \\\r\n    --bits 4 \\\r\n    --model_name_or_path /content/LLaVA/llava-v1.5-7b \\\r\n    --version llava_llama_2 \\\r\n    --data_path /content/drive/MyDrive/ML_llava_train/train/dataset.json \\\r\n    --image_folder /content/drive/MyDrive/ML_llava_train/images \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 False \\\r\n    --output_dir /content/LLaVA/llava/checkpoints/ESA_llava \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 32 \\\r\n    --per_device_eval_batch_size 32 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 100 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\nHowever, I encountered an issue when loading the model.\r\n```\r\n!python /content/LLaVA/llava/eval/run_llava.py --model-path /content/LLaVA/llava/checkpoints/ESA_llava  \\\r\n--model-base /content/LLaVA/llava-v1.5-7b \\\r\n--image-file /content/drive/MyDrive/ML_llava_test/test.jpg \\\r\n--query \"What is the condition of this road\"\r\n```\r\nError:\r\n```\r\n[2024-03-03 12:22:49,116] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n2024-03-03 12:22:50.450603: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-03-03 12:22:50.450653: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-03-03 12:22:50.452294: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-03-03 12:22:51.645656: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\nLoading LLaVA from base model...\r\nLoading checkpoint shards:   0% 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\nLoading checkpoint shards: 100% 2/2 [00:04<00:00,  2.28s/it]\r\nTraceback (most recent call last):\r\n  File \"/content/LLaVA/llava/eval/run_llava.py\", line 145, in <module>\r\n    eval_model(args)\r\n  File \"/content/LLaVA/llava/eval/run_llava.py\", line 55, in eval_model\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(\r\n  File \"/content/LLaVA/llava/model/builder.py\", line 153, in load_pretrained_model\r\n    model.resize_token_embeddings(len(tokenizer))\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 1811, in resize_token_embeddings\r\n    model_embeds = self._resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 1847, in _resize_token_embeddings\r\n    new_lm_head = self._get_resized_lm_head(old_lm_head, new_num_tokens)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 2016, in _get_resized_lm_head\r\n    new_lm_head = nn.Linear(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 96, in __init__\r\n    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parameter.py\", line 39, in __new__\r\n    return torch.Tensor._make_subclass(cls, data, requires_grad)\r\nRuntimeError: Only Tensors of floating point and complex dtype can require gradients\r\n```\r\n\r\nDo you have any suggestions on how to resolve this issue?\r\n\r\nBest wishes,\r\nZhao</BODY>\n\n<COMMENTS>\n<Comment by wangzhao0217 at 2024-03-10T15:04:36Z>\nThe issue solved by adding: --tune_mm_mlp_adapter True\r\n\r\nHowever, got new error :\r\n\r\nCommand:\r\n```\r\n!python /content/LLaVA/llava/eval/run_llava.py  --model-path /content/LLaVA/llava/checkpoints/ESA-llava-qlora \\\r\n--model-base /content/LLaVA/llava-v1.5-7b \\\r\n--image-file \"/content/drive/MyDrive/train_rgb/Bad/R1C1 N1_103.tif\" \\\r\n--query \"What is the condition of this road\"\r\n```\r\n\r\nError:\r\n```\r\n]\r\n!python /content/LLaVA/llava/eval/run_llava.py  --model-path /content/LLaVA/llava/checkpoints/ESA-llava-qlora \\\r\n--model-base /content/LLaVA/llava-v1.5-7b \\\r\n--image-file \"/content/drive/MyDrive/train_rgb/Bad/R1C1 N1_103.tif\" \\\r\n--query \"What is the condition of this road\"\r\n\r\n[2024-03-08 16:07:29,340] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n2024-03-08 16:07:30.682805: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-03-08 16:07:30.682860: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-03-08 16:07:30.684501: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-03-08 16:07:31.916176: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\nLoading LLaVA from base model...\r\nLoading checkpoint shards:   0% 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\nLoading checkpoint shards: 100% 2/2 [00:04<00:00,  2.30s/it]\r\nLoading additional LLaVA weights...\r\nLoading LoRA weights...\r\nMerging LoRA weights...\r\n/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/bnb.py:272: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\r\n  warnings.warn(\r\nModel is loaded...\r\nTraceback (most recent call last):\r\n  File \"/content/LLaVA/llava/eval/run_llava.py\", line 145, in <module>\r\n    eval_model(args)\r\n  File \"/content/LLaVA/llava/eval/run_llava.py\", line 115, in eval_model\r\n    output_ids = model.generate(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/content/LLaVA/llava/model/language_model/llava_llama.py\", line 137, in generate\r\n    return super().generate(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 1525, in generate\r\n    return self.sample(\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 2622, in sample\r\n    outputs = self(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/content/LLaVA/llava/model/language_model/llava_llama.py\", line 91, in forward\r\n    return super().forward(\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 1201, in forward\r\n    logits = self.lm_head(hidden_states)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py\", line 414, in forward\r\n    assert self.weight.shape[1] == 1\r\nAssertionError\r\n```\n</Comment>\n<Comment by wangzhao0217 at 2024-03-11T21:25:56Z>\nsolved using zero3.json\n</Comment>\n<Comment by LordUky at 2024-05-27T19:52:35Z>\nMay I ask did you just changing zero2.json to zero3.json in finetune_qlora.sh? I tried but the ' assert self.weight.shape[1] == 1' error still exists.\n</Comment>\n<Comment by wangzhao0217 at 2024-05-28T10:44:58Z>\n> May I ask did you just changing zero2.json to zero3.json in finetune_qlora.sh? I tried but the ' assert self.weight.shape[1] == 1' error still exists.\r\n\r\nyes, that's work for me\n</Comment>\n<Comment by HadeerArafa at 2024-07-19T12:30:38Z>\ncan you help me , when i used zero3 i got this error : \r\n`ValueError: DeepSpeed Zero-3 is not compatible with low_cpu_mem_usage=True or with passing a device_map.`\n</Comment>\n<Comment by LordUky at 2024-07-19T22:47:30Z>\n> can you help me , when i used zero3 i got this error : `ValueError: DeepSpeed Zero-3 is not compatible with low_cpu_mem_usage=True or with passing a device_map.`\r\n\r\nI think maybe it can be solved by just commenting the 'device_map' parameter in the code, and pass low_cpu_mem_usage=False or commenting it as well.\n</Comment>\n<Comment by HadeerArafa at 2024-07-21T17:36:56Z>\nThank you @LordUky ! I still encountered an assertion error. Do you have a solution for this? Also, could you please tell me the PyTorch version you used?\n</Comment>\n<Comment by LordUky at 2024-07-22T06:43:42Z>\nHi I completely followed the provided commands to set up the environment. Unfortunately, as mentioned in my previous comments, I still got this assertion error after switching to zero3.json. Actually I am not working on this project now, but good luck!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1216,
    "state": "closed",
    "created_by": "sentinel8b",
    "created_at": "2024-03-04T01:14:14Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1216</URL>\n\n<TITLE>[Question] How can I get H_{v} (projected image feature) ?</TITLE>\n\n<BODY>### Question\n\nI'm trying to leverage feature vector from LLaVA for my research. How can I get projected image feature H_{v}?\r\n<img width=\"316\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/58873463/aaa0e069-0a86-4e0b-9e9e-ae392b97e229\"></BODY>\n\n<COMMENTS>\n<Comment by sentinel8b at 2024-03-04T03:24:25Z>\nI found it on llava_arch.py.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1215,
    "state": "open",
    "created_by": "conheaven",
    "created_at": "2024-03-03T14:39:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1215</URL>\n\n<TITLE>[Question] after finetune i run cli.py</TITLE>\n\n<BODY>### Question\n\nafter finetune with finetune_lora.sh, I merge the model with 'python scripts/merge_lora_weights.py \\\r\n    --model-path /data1/khw/output_llava/finetune/llava-v1.5-13b-lora \\\r\n    --model-base /data1/khw/llava \\\r\n    --save-model-path /data1/khw/output_llava/merge_model-lora'\r\n    then i run \r\n    'python -m llava.serve.cli \\\r\n    --model-base /data1/khw/llava \\\r\n    --model-path /data1/khw/output_llava/merge_model-lora \\\r\n    --image-file /data1/khw/img/1.jpg \\'\r\n    it shows \r\n    \"Traceback (most recent call last):\r\n  File \"/home/khw/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/khw/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/khw/llava/LLaVA/llava/serve/cli.py\", line 126, in <module>\r\n    main(args)\r\n  File \"/home/khw/llava/LLaVA/llava/serve/cli.py\", line 32, in main\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n  File \"/home/khw/llava/LLaVA/llava/model/builder.py\", line 122, in load_pretrained_model\r\n    model = AutoModelForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, **kwargs)\r\n  File \"/home/khw/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\r\n    raise ValueError(\r\nValueError: Unrecognized configuration class <class 'transformers.models.llava.configuration_llava.LlavaConfig'> for this kind of AutoModel: AutoModelForCausalLM.\r\nModel type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, ElectraConfig, ErnieConfig, FalconConfig, FuyuConfig, GemmaConfig, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, LlamaConfig, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MptConfig, MusicgenConfig, MvpConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, LlavaConfig, LlavaMptConfig, LlavaMistralConfig.\"\r\nthank you in advance</BODY>\n\n<COMMENTS>\n<Comment by sayedmohamedscu at 2024-03-22T03:58:28Z>\ntry to upgrade transformers\n</Comment>\n<Comment by conheaven at 2024-03-22T06:08:21Z>\n> try to upgrade transformers\r\n\r\nmy transformers is 4.37.2, should i upgrade it？\n</Comment>\n<Comment by Namzakku at 2024-04-09T01:40:25Z>\nI've managed to merge the model by installing llava 1.1.3 from previous releases and change the transformers_version in config.json in the finetuned model to 4.31.0\n</Comment>\n<Comment by DemonsAH at 2024-07-25T06:50:01Z>\n#1567\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1214,
    "state": "closed",
    "created_by": "zwcolin",
    "created_at": "2024-03-02T23:52:10Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1214</URL>\n\n<TITLE>[Question] llava1.6 config.json doesn't match what is described in LLaVA-NeXT blog?</TITLE>\n\n<BODY>### Question\n\nHi,\r\n\r\nThanks for the great work! I am doing research in relevant domains and I had some questions about LLaVA 1.6's training details, in particular:\r\n\r\n(1) the blog says \"It supports three aspect ratios, up to 672x672, 336x1344, 1344x336 resolution\" but in `config.json`, it seems that only up to 1008 is supported. Which one should be correct?\r\n(2) since the visual encoder is kept frozen for both LLaVA and LLaVA 1.5, I assume that it's still kept frozen if it was not mentioned in the LLaVA-NeXT's blog. But I do see in `config.json` that `unfreeze_mm_vision_tower` is set to true and there's a learning rate associated with it. If possible, could you provide more details in how you finetuned the visual encoder?</BODY>\n\n<COMMENTS>\n<Comment by l4b4r4b4b4 at 2024-03-06T02:03:53Z>\n@zwcolin \r\ndid you manage to get fine-tuning to work with llava 1.6?\n</Comment>\n<Comment by zwcolin at 2024-03-06T02:20:08Z>\nImplementation-wise, I think you _at least_ need to (1) build the training pipeline mostly in `train.py` based on the inference pipeline; (2) finetune the visual encoder at some point using some learning rate; and (3) use the correct instruction tuning template depending on the LLM backbone. You might be able to infer some parameters based on `config.json` they put in their model weight folder.\r\n\r\nThat being said we cannot do more than just having this because we don't have the exact data that llava 1.6 is trained on, but say if you have your own data, you could try to have this implemented and see how it performs. Again I'm also a user of this codebase and **my views are completely unofficial**. To make sure everything is correct, I'd suggest waiting for the authors to release the official implementation to train llava 1.6.\n</Comment>\n<Comment by l4b4r4b4b4 at 2024-03-06T21:39:44Z>\nWell I kinda have the feeling the code base has been put together in a haste. \r\nYou sure have to prompt the LLM with its respective prompt template. But the provided code is sure not clear on that, as it uses llama [INST] prompt template for a mistral model, which generally is chatML based... \r\n\r\nBut looking at the date for train.py and llava_trainer.py I feel I can get this working with 1.6 using my experience in fine.tuning and training. \r\n\r\nIll connected on LinkedIn in case you would like to get into the weeds of things ;)\r\n\r\n> Implementation-wise, I think you _at least_ need to (1) build the training pipeline mostly in `train.py` based on the inference pipeline; (2) finetune the visual encoder at some point using some learning rate; and (3) use the correct instruction tuning template depending on the LLM backbone. You might be able to infer some parameters based on `config.json` they put in their model weight folder.\r\n> \r\n> That being said we cannot do more than just having this because we don't have the exact data that llava 1.6 is trained on, but say if you have your own data, you could try to have this implemented and see how it performs. Again I'm also a user of this codebase and **my views are completely unofficial**. To make sure everything is correct, I'd suggest waiting for the authors to release the official implementation to train llava 1.6.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1213,
    "state": "closed",
    "created_by": "RylanSchaeffer",
    "created_at": "2024-03-02T22:40:09Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1213</URL>\n\n<TITLE>[Usage] Unable to Generate with Mistral</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue: I receive the following error when trying to generate using Mistral (`[liuhaotian/llava-v1.6-mistral-7b](https://huggingface.co/liuhaotian/llava-v1.6-mistral-7b)`): `TypeError: bad operand type for unary -: 'NoneType'`\r\n\r\nCommand:\r\n\r\n```\r\n        generated_ids = self.model.generate(\r\n            input_ids,\r\n            images=image_pixel_values.half(),\r\n            do_sample=True if self.generation_kwargs[\"temperature\"] > 0 else False,\r\n            **self.generation_kwargs,\r\n        )\r\n```\r\n\r\nStack trace:\r\n\r\n```\r\n  File \"/lfs/ampere8/0/rschaef/PerezAstraFellowship-Image-DAN-VLM-Attack/submodules/LLaVA/llava/model/language_model/llava_mistral.py\", line 141, in generate\r\n    return super().generate(\r\n  File \"/lfs/ampere8/0/rschaef/miniconda3/envs/universal_vlm_jailbreak_env/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/lfs/ampere8/0/rschaef/miniconda3/envs/universal_vlm_jailbreak_env/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1652, in generate\r\n    return self.sample(\r\n  File \"/lfs/ampere8/0/rschaef/miniconda3/envs/universal_vlm_jailbreak_env/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2734, in sample\r\n    outputs = self(\r\n  File \"/lfs/ampere8/0/rschaef/miniconda3/envs/universal_vlm_jailbreak_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/lfs/ampere8/0/rschaef/miniconda3/envs/universal_vlm_jailbreak_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/lfs/ampere8/0/rschaef/PerezAstraFellowship-Image-DAN-VLM-Attack/submodules/LLaVA/llava/model/language_model/llava_mistral.py\", line 95, in forward\r\n    return super().forward(\r\n  File \"/lfs/ampere8/0/rschaef/miniconda3/envs/universal_vlm_jailbreak_env/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 1045, in forward\r\n    outputs = self.model(\r\n  File \"/lfs/ampere8/0/rschaef/miniconda3/envs/universal_vlm_jailbreak_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/lfs/ampere8/0/rschaef/miniconda3/envs/universal_vlm_jailbreak_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/lfs/ampere8/0/rschaef/miniconda3/envs/universal_vlm_jailbreak_env/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 888, in forward\r\n    attention_mask = self._prepare_decoder_attention_mask(\r\n  File \"/lfs/ampere8/0/rschaef/miniconda3/envs/universal_vlm_jailbreak_env/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 796, in _prepare_decoder_attention_mask\r\n    combined_attention_mask = _make_sliding_window_causal_mask(\r\n  File \"/lfs/ampere8/0/rschaef/miniconda3/envs/universal_vlm_jailbreak_env/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 88, in _make_sliding_window_causal_mask\r\n    mask = torch.triu(mask, diagonal=-sliding_window)\r\nTypeError: bad operand type for unary -: 'NoneType'\r\n```</BODY>\n\n<COMMENTS>\n<Comment by RylanSchaeffer at 2024-03-02T22:52:36Z>\nUpdating `transformers` solved this error for me, following https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/discussions/19.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1212,
    "state": "open",
    "created_by": "rawann31",
    "created_at": "2024-03-01T19:55:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1212</URL>\n\n<TITLE>load_pretrained_model() got an unexpected keyword argument 'offload_folder'</TITLE>\n\n<BODY>### Describe the issue\n\nmodel_path = \"liuhaotian/llava-v1.5-7b\"\r\n\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    model_path=model_path,\r\n    model_base=None,\r\n    model_name=get_model_name_from_path(model_path),\r\n    offload_folder=\"/content/llava_model\"\r\n)\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n[<ipython-input-28-738c62711b77>](https://localhost:8080/#) in <cell line: 3>()\r\n      1 model_path = \"liuhaotian/llava-v1.5-7b\"\r\n      2 \r\n----> 3 tokenizer, model, image_processor, context_len = load_pretrained_model(\r\n      4     model_path=model_path,\r\n      5     model_base=None,\r\n\r\nTypeError: load_pretrained_model() got an unexpected keyword argument 'offload_folder'</BODY>\n\n<COMMENTS>\n<Comment by KansaiTraining at 2024-06-25T14:17:55Z>\nI have a similar problem. Any solution?\r\n```\r\nValueError: The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format.\r\n```\r\nIn other projects, it is said that in [here](https://github.com/haotian-liu/LLaVA/blob/c121f0432da27facab705978f83c4ada465e46fd/llava/model/builder.py#L117) offload_folder has to be provided ([ref](https://github.com/nomic-ai/gpt4all/issues/239))\r\nAny idea here?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1210,
    "state": "closed",
    "created_by": "AL3708",
    "created_at": "2024-03-01T11:29:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1210</URL>\n\n<TITLE>[Question] Is a LLaVA 1.6 the same as LLaVA-NeXT?</TITLE>\n\n<BODY>### Question\n\nhttps://llava-vl.github.io/blog/2024-01-30-llava-next/ compares LLaVA-NeXT to previous version 1.5. Also googling LLaVA 1.6 redirects to LLaVA-NeXT. So is there naming inconsistency and they are same models?</BODY>\n\n<COMMENTS>\n<Comment by NielsRogge at 2024-03-02T14:16:20Z>\nYes\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1209,
    "state": "open",
    "created_by": "m22cs058",
    "created_at": "2024-03-01T06:31:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1209</URL>\n\n<TITLE>[Question] Confidence Score</TITLE>\n\n<BODY>### Question\n\nHow to print the confidence score of the bounding box detections?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1208,
    "state": "open",
    "created_by": "20191864218",
    "created_at": "2024-03-01T04:50:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1208</URL>\n\n<TITLE>[Question] ImportError: cannot import name 'LlavaLlamaForCausalLM' from 'llava.model' (/root/LLaVA/llava/model/__init__.py)</TITLE>\n\n<BODY>### Question\n\nIf I introduce a new package in clip_encoder.py, I get this error. What should I do?Thanks!</BODY>\n\n<COMMENTS>\n<Comment by zzxslp at 2024-03-07T02:05:53Z>\nSee this thread: https://github.com/haotian-liu/LLaVA/issues/1101 \r\nBasically re-install flash-attn can solve this error.\r\n```\r\npip uninstall flash-attn \r\npip install flash-attn --no-build-isolation --no-cache-dir\r\n```\n</Comment>\n<Comment by 20191864218 at 2024-03-07T06:16:41Z>\n> See this thread: #1101 Basically re-install flash-attn can solve this error.\r\n> \r\n> ```\r\n> pip uninstall flash-attn \r\n> pip install flash-attn --no-build-isolation --no-cache-dir\r\n> ```\r\n\r\nthinks！\n</Comment>\n<Comment by hantao-zhou at 2024-03-08T20:31:25Z>\n> > See this thread: #1101 Basically re-install flash-attn can solve this error.\r\n> > ```\r\n> > pip uninstall flash-attn \r\n> > pip install flash-attn --no-build-isolation --no-cache-dir\r\n> > ```\r\n> \r\n> thinks！\r\n\r\n\r\n\r\n> See this thread: #1101 Basically re-install flash-attn can solve this error.\r\n> \r\n> ```\r\n> pip uninstall flash-attn \r\n> pip install flash-attn --no-build-isolation --no-cache-dir\r\n> ```\r\n\r\nthe question still persist after the commands\n</Comment>\n<Comment by SuperStacie at 2024-04-09T17:41:45Z>\nHi hi~ I met the same issue when adding new modules, have you sucessfully solved this problem?\n</Comment>\n<Comment by 20191864218 at 2024-04-10T00:49:21Z>\n> Hi hi~ I met the same issue when adding new modules, have you sucessfully solved this problem?\r\n\r\n不用管__init__.py文件，在需要导入__init__.py的地方直接把相应的文件导入\n</Comment>\n<Comment by hantao-zhou at 2024-04-10T09:04:38Z>\nRecently being tortured by some other tedious job~ just saw the updates\r\nMy issue was due to a path pointing to an environment controlled by the system and causing conflicts, so after several printenv, I solved it by correcting the referrals\n</Comment>\n<Comment by foreverhell at 2024-04-24T06:33:00Z>\nIn llava/__init__.py, I modify the code \r\n     from .model import LlavaLlamaForCausalLM\r\nto\r\n     from .model.language_model.llava_llama import LlavaLlamaForCausalLM\r\nand fix it.\r\nI'm not sure if it makes effort for someone else, but I think I should share it.\n</Comment>\n<Comment by boyugou at 2024-06-21T03:01:25Z>\n> See this thread: #1101 Basically re-install flash-attn can solve this error.\r\n> \r\n> ```\r\n> pip uninstall flash-attn \r\n> pip install flash-attn --no-build-isolation --no-cache-dir\r\n> ```\r\n\r\nthis one addressed my issue (which might be caused also by not creating a separated conda env)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1207,
    "state": "open",
    "created_by": "alexv-cerebras",
    "created_at": "2024-03-01T04:48:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1207</URL>\n\n<TITLE>Issue with SEED benchmark</TITLE>\n\n<BODY>### Describe the issue\n\nI was attempting to evaluate the SEED dataset using the instructions provided in `Evaluation.md`. I downloaded the images and all the videos, then converted the videos into images. However, when I tried to run `./scripts/v1_5/eval/seed.sh`, I encountered a 'file not found' error for some of the images. Upon investigating, I discovered that `./scripts/v1_5/eval/seed.sh ` and `./playground/data/eval/seed_bench/extract_video_frames.py` reference different JSON files. Specifically, seed.sh uses `llava-seed-bench.jsonl` for running predictions, while extract_video_frames.py uses `SEED-Bench.json`, which I had downloaded from the official repository. It appears that `llava-seed-bench.jsonl` contains more images. Could you please guide me on what I might be doing wrong, or if there's possibly a mismatch between these two scripts?</BODY>\n\n<COMMENTS>\n<Comment by dsn01 at 2024-07-16T11:11:46Z>\n@alexv-cerebras Hi! I met the same problem as you did. How do you solve it ?\n</Comment>\n<Comment by Cola-any at 2024-09-21T12:22:05Z>\nme too\n</Comment>\n<Comment by Twi-etn at 2025-02-16T15:01:51Z>\nI'm also having the same problem, there are only over 3,000 images extracted from the video, but more than 5,000 images are required for evaluation according to `llava-seed-bench.jsonl`.\n</Comment>\n<Comment by Twi-etn at 2025-02-17T04:58:32Z>\nHey bro, I've found the problem. The json file we need needs to be downloaded in its HuggingFace history,such as [url](https://huggingface.co/datasets/AILab-CVC/SEED-Bench/commit/8a0beb74cb0ed9d51791a055a89d9679e001d6bc#d2h-689886)\n</Comment>\n<Comment by Twi-etn at 2025-02-17T05:01:27Z>\n> Hey bro, I've found the problem. The json file we need needs to be downloaded in its HuggingFace history,such as [url](https://huggingface.co/datasets/AILab-CVC/SEED-Bench/commit/8a0beb74cb0ed9d51791a055a89d9679e001d6bc#d2h-689886)\n\nThis version of the json file will extract more than 5,000 images, which correspond to our `llava-seed-bench.jsonl`, but later changed to the json file and was modified, and it does not correspond to our `llava-seed-bench.jsonl`.\n</Comment>\n<Comment by ge-bin-hui at 2025-03-20T09:49:59Z>\n@alexv-cerebras Hey bro! Could you tell me how to convert the videos into images.I can not find the extract_video_frames.py as mentioned in Evaluation.md\n</Comment>\n<Comment by HuangChiEn at 2025-04-07T06:24:02Z>\n@ge-bin-hui @Cola-any @dsn01 @alexv-cerebras \nYes, it's mismatch ~ llava-seed-bench.jsonl doesn't match with SEED-Bench.json (officially released by SEED-Bench v1).\n\nSo, just apply SEED-Bench.json, since it's officially released by SEED-Bench v1. While llava-seed-bench.jsonl is released by llava repo which seems not be maintained anymore (at least in this issue).\n\nIf you aims to compare with other method you should also apply SEED-Bench.json or turn to SEED-Bench.v2 something.\nThe following is the modified llava-seed-bench.jsonl (i follows HF SEED-Bench.json to create and keep image part as same in original llava-seed-bench.jsonl..)\n[links](https://drive.google.com/file/d/1lpaZl9gqR5BvJQ1BiIMaGpxKksdAGoZP/view?usp=sharing)\n\nThe only things you need to do is rename the video image as `f\"SEED-Bench-video-image/{quest['question_type_id']}_{quest['question_id']}.png\"`.\nThe image folder layout for video : \n\n![Image](https://github.com/user-attachments/assets/21621c09-5f88-47a6-a98d-24fe5ae2bc8f)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1206,
    "state": "open",
    "created_by": "tangjunjun966",
    "created_at": "2024-03-01T01:40:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1206</URL>\n\n<TITLE>lora训练测试自己数据测试出现重复预测错误？</TITLE>\n\n<BODY>### Question\n\n我使用lora训练，语言模型使用lora与non_trainables微调，训练出现警告几个类似WARNING: tokenization mismatch: 1 vs. 57. (ignored)，查看loss降低有些不正常，果然测试效果极差，请问是什么问题导致？\r\n![1b151268fbbe9afce868e0faa768227](https://github.com/haotian-liu/LLaVA/assets/30024958/98ecc415-37d0-4720-95a4-7ee6f2ef9f70)\r\n\r\n\r\n![5aa004e7bc008c229088d880eeb17f2](https://github.com/haotian-liu/LLaVA/assets/30024958/b8c08ecd-2c12-42c8-90a7-0d760085280e)</BODY>\n\n<COMMENTS>\n<Comment by Mingfeng-Chen at 2024-03-22T02:34:25Z>\n我也遇到同样的问题\r\n![1711074850898](https://github.com/haotian-liu/LLaVA/assets/59349853/4ec0b5ff-2403-4cc7-8e52-8e54cf8da350)\n</Comment>\n<Comment by tangjunjun966 at 2024-03-22T02:39:06Z>\n这个问题在于数据，你的数据是不是太简单了或者数量较少啊\r\n\r\n> 我也遇到同样的问题 ![1711074850898](https://private-user-images.githubusercontent.com/59349853/315866912-4ec0b5ff-2403-4cc7-8e52-8e54cf8da350.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTEwNzUzMzYsIm5iZiI6MTcxMTA3NTAzNiwicGF0aCI6Ii81OTM0OTg1My8zMTU4NjY5MTItNGVjMGI1ZmYtMjQwMy00Y2M3LThlNTItOGU1NGNmOGRhMzUwLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzIyVDAyMzcxNlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTczOGYyNTM1MGRlZjk2YWIwYmZmOGY2OTAzMTAxOGU5MjZkMDZkZTBhMjU3ZjQyYzEyZDNhODUxMzc1NzQwZjgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.CtLN-6dD0sFmGaEOwUtO0QTFc0d9bMR0xVcVAV8INhc)\r\n\r\n这个问题多半是数据，你的数据是不是太简单了或者数量量很少啊\n</Comment>\n<Comment by Mingfeng-Chen at 2024-03-22T02:42:28Z>\n> 这个问题在于数据，你的数据是不是太简单了或者数量较少啊\r\n> \r\n> > 我也遇到同样的问题 ![1711074850898](https://private-user-images.githubusercontent.com/59349853/315866912-4ec0b5ff-2403-4cc7-8e52-8e54cf8da350.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTEwNzUzMzYsIm5iZiI6MTcxMTA3NTAzNiwicGF0aCI6Ii81OTM0OTg1My8zMTU4NjY5MTItNGVjMGI1ZmYtMjQwMy00Y2M3LThlNTItOGU1NGNmOGRhMzUwLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzIyVDAyMzcxNlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTczOGYyNTM1MGRlZjk2YWIwYmZmOGY2OTAzMTAxOGU5MjZkMDZkZTBhMjU3ZjQyYzEyZDNhODUxMzc1NzQwZjgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.CtLN-6dD0sFmGaEOwUtO0QTFc0d9bMR0xVcVAV8INhc)\r\n> \r\n> 这个问题多半是数据，你的数据是不是太简单了或者数量量很少啊\r\n\r\n我用的是llava instruct 150k指令微调数据\n</Comment>\n<Comment by tangjunjun966 at 2024-03-22T02:48:22Z>\n> > 这个问题在于数据，你的数据是不是太简单了或者数量较少啊\r\n> > > 我也遇到同样的问题 ![1711074850898](https://private-user-images.githubusercontent.com/59349853/315866912-4ec0b5ff-2403-4cc7-8e52-8e54cf8da350.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTEwNzUzMzYsIm5iZiI6MTcxMTA3NTAzNiwicGF0aCI6Ii81OTM0OTg1My8zMTU4NjY5MTItNGVjMGI1ZmYtMjQwMy00Y2M3LThlNTItOGU1NGNmOGRhMzUwLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzIyVDAyMzcxNlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTczOGYyNTM1MGRlZjk2YWIwYmZmOGY2OTAzMTAxOGU5MjZkMDZkZTBhMjU3ZjQyYzEyZDNhODUxMzc1NzQwZjgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.CtLN-6dD0sFmGaEOwUtO0QTFc0d9bMR0xVcVAV8INhc)\r\n> > \r\n> > \r\n> > 这个问题多半是数据，你的数据是不是太简单了或者数量量很少啊\r\n> \r\n> 我用的是llava instruct 150k指令微调数据\r\n\r\n有没有可能你预测时候 lora没有merge啊\n</Comment>\n<Comment by Mingfeng-Chen at 2024-03-22T02:50:52Z>\n> > > 这个问题在于数据，你的数据是不是太简单了或者数量较少啊\r\n> > > > 我也遇到同样的问题 ![1711074850898](https://private-user-images.githubusercontent.com/59349853/315866912-4ec0b5ff-2403-4cc7-8e52-8e54cf8da350.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTEwNzUzMzYsIm5iZiI6MTcxMTA3NTAzNiwicGF0aCI6Ii81OTM0OTg1My8zMTU4NjY5MTItNGVjMGI1ZmYtMjQwMy00Y2M3LThlNTItOGU1NGNmOGRhMzUwLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzIyVDAyMzcxNlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTczOGYyNTM1MGRlZjk2YWIwYmZmOGY2OTAzMTAxOGU5MjZkMDZkZTBhMjU3ZjQyYzEyZDNhODUxMzc1NzQwZjgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.CtLN-6dD0sFmGaEOwUtO0QTFc0d9bMR0xVcVAV8INhc)\r\n> > > \r\n> > > \r\n> > > 这个问题多半是数据，你的数据是不是太简单了或者数量量很少啊\r\n> > \r\n> > \r\n> > 我用的是llava instruct 150k指令微调数据\r\n> \r\n> 有没有可能你预测时候 lora没有merge啊\r\n\r\nmerge了，不然没法输出😔😢，请问你是多加了数据训练就解决了问题吗\n</Comment>\n<Comment by tangjunjun966 at 2024-03-22T03:00:38Z>\n> > > > 这个问题在于数据，你的数据是不是太简单了或者数量较少啊\r\n> > > > > 我也遇到同样的问题 ![1711074850898](https://private-user-images.githubusercontent.com/59349853/315866912-4ec0b5ff-2403-4cc7-8e52-8e54cf8da350.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTEwNzUzMzYsIm5iZiI6MTcxMTA3NTAzNiwicGF0aCI6Ii81OTM0OTg1My8zMTU4NjY5MTItNGVjMGI1ZmYtMjQwMy00Y2M3LThlNTItOGU1NGNmOGRhMzUwLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzIyVDAyMzcxNlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTczOGYyNTM1MGRlZjk2YWIwYmZmOGY2OTAzMTAxOGU5MjZkMDZkZTBhMjU3ZjQyYzEyZDNhODUxMzc1NzQwZjgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.CtLN-6dD0sFmGaEOwUtO0QTFc0d9bMR0xVcVAV8INhc)\r\n> > > > \r\n> > > > \r\n> > > > 这个问题多半是数据，你的数据是不是太简单了或者数量量很少啊\r\n> > > \r\n> > > \r\n> > > 我用的是llava instruct 150k指令微调数据\r\n> > \r\n> > \r\n> > 有没有可能你预测时候 lora没有merge啊\r\n> \r\n> merge了，不然没法输出😔😢，请问你是多加了数据训练就解决了问题吗\r\n\r\n我用这个数据llava_v1_5_mix665k跑的，没啥问题。我用我们公司数据在跑了。我后面是数据加大解决了这个问题。突刺问题是使用小点学习率也可暂时解决\n</Comment>\n<Comment by Mingfeng-Chen at 2024-03-22T03:03:40Z>\n> > > > > 这个问题在于数据，你的数据是不是太简单了或者数量较少啊\r\n> > > > > > 我也遇到同样的问题 ![1711074850898](https://private-user-images.githubusercontent.com/59349853/315866912-4ec0b5ff-2403-4cc7-8e52-8e54cf8da350.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTEwNzUzMzYsIm5iZiI6MTcxMTA3NTAzNiwicGF0aCI6Ii81OTM0OTg1My8zMTU4NjY5MTItNGVjMGI1ZmYtMjQwMy00Y2M3LThlNTItOGU1NGNmOGRhMzUwLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzIyVDAyMzcxNlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTczOGYyNTM1MGRlZjk2YWIwYmZmOGY2OTAzMTAxOGU5MjZkMDZkZTBhMjU3ZjQyYzEyZDNhODUxMzc1NzQwZjgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.CtLN-6dD0sFmGaEOwUtO0QTFc0d9bMR0xVcVAV8INhc)\r\n> > > > > \r\n> > > > > \r\n> > > > > 这个问题多半是数据，你的数据是不是太简单了或者数量量很少啊\r\n> > > > \r\n> > > > \r\n> > > > 我用的是llava instruct 150k指令微调数据\r\n> > > \r\n> > > \r\n> > > 有没有可能你预测时候 lora没有merge啊\r\n> > \r\n> > \r\n> > merge了，不然没法输出😔😢，请问你是多加了数据训练就解决了问题吗\r\n> \r\n> 我用这个数据llava_v1_5_mix665k跑的，没啥问题。我用我们公司数据在跑了。我后面是数据加大解决了这个问题。突刺问题是使用小点学习率也可暂时解决\r\n\r\n谢谢，我试一下mix665k👌😘\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1204,
    "state": "closed",
    "created_by": "20191864218",
    "created_at": "2024-02-29T14:27:27Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1204</URL>\n\n<TITLE>Does LLaVA's package dependencies conflict with those of MedCLIP?</TITLE>\n\n<BODY>### Question\n\nWhen I replace the vision encoder of LLaVA with MedCLIP, the following error occurs.\r\n\r\n`RuntimeError: Error(s) in loading state_dict for MedCLIPModel:\r\n        Unexpected key(s) in state_dict: \"text_model.model.embeddings.position_ids\". `</BODY>\n\n<COMMENTS>\n<Comment by SachJbp at 2024-03-14T14:46:45Z>\nWhat was the resolution here?\r\n\r\nI encountered the same issue:\r\n\r\n```\r\nfrom medclip import MedCLIPModel, MedCLIPVisionModelViT\r\n# load MedCLIP-ViT\r\nclip_model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\r\nclip_model.from_pretrained()\r\nRuntimeError: Error(s) in loading state_dict for MedCLIPModel:\r\n\tUnexpected key(s) in state_dict: \"text_model.model.embeddings.position_ids\". \r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1203,
    "state": "closed",
    "created_by": "20191864218",
    "created_at": "2024-02-29T14:27:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1203</URL>\n\n<TITLE>Does LLaVA's package dependencies conflict with those of MedCLIP?When I replace the vision encoder of LLaVA with MedCLIP, the following error occurs</TITLE>\n\n<BODY>### Question\n\nWhen I replace the vision encoder of LLaVA with MedCLIP, the following error occurs.\r\n\r\n`RuntimeError: Error(s) in loading state_dict for MedCLIPModel:\r\n        Unexpected key(s) in state_dict: \"text_model.model.embeddings.position_ids\". `</BODY>\n\n<COMMENTS>\n<Comment by ameer-khu at 2024-07-12T06:59:34Z>\n@20191864218 Hi, can you let me know how you resolved this issue? What changes do we need to make to integrate MedCLIP?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1202,
    "state": "open",
    "created_by": "htluandc2",
    "created_at": "2024-02-29T09:30:09Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1202</URL>\n\n<TITLE>[Usage] How can I implemet few shot learning on LLaVa</TITLE>\n\n<BODY>### Describe the issue\n\nHi there,\r\n\r\nI have some images and some custom explain. \r\nSo I want to implement few shot learning to make summaries of my images.\r\n\r\nThis is my current implement:\r\n```\r\ntemplates = [\r\n    {\r\n        \"url\": \"\",\r\n        \"explain\": \"\"\"\"\"\",\r\n    },\r\n    {\r\n        \"url\": \"\",\r\n        \"explain\": \"\"\"\"\"\",\r\n    },\r\n    {\r\n        \"url\": \"\",\r\n        \"explain\": \"\"\"\"\"\"\r\n    },\r\n    {\r\n        \"url\": \",\r\n        \"explain\": \"\"\"\"\"\"\r\n    },\r\n    {\r\n        \"url\": \"\",\r\n        \"explain\": \"\"\"\"\"\"\r\n    },\r\n]\r\n```\r\n\r\nMy code to build prompt:\r\n```\r\nfrom PIL import Image\r\nimport cv2\r\nimport numpy as np\r\nimport requests\r\n\r\n\"\"\"Make image summary\"\"\"\r\nimg_prompt = \"User: <image>\\n\"+\"\\nASSISTANT:\"\r\n\r\nprompt = (\r\n    \"You are an assistant tasked with summarizing images for retrieval. \"\r\n    \"These summaries will be embedded and used to retrieve the raw image. \"\r\n    \"Give a concise summary of the image that is well optimized for retrieval.\"\r\n)\r\nprint(prompt)\r\n\r\nimages = []\r\n\r\nfor i, temp in enumerate(templates):\r\n    image_i = Image.open(requests.get(temp['url'], stream=True).raw)\r\n    eplain_i  = temp[\"explain\"]\r\n    example_i = f\"\\nUser: <image{i}>\"+\"\\nASSISTANT:\" + eplain_i + \"\\n\"\r\n    prompt += example_i\r\n    images.append(image_i)\r\n\r\nprompt += f\"\\nUser: <image{len(templates)}>\"+\"\\nASSISTANT:\"\r\nprint(prompt)\r\nprint('-'*100)\r\nprint(\"Examples:\", len(images))\r\n```\r\nInference:\r\n```\r\ntarget = Image.open(\"figures/figure-2-5.jpg\")\r\n\r\n\r\nout = model_multi_modals(\r\n    images=images+[target],\r\n    prompt=prompt,\r\n    generate_kwargs={\"max_new_tokens\": 2048})\r\n```\r\n\r\nAnd my error:\r\n```\r\nValueError: The input provided to the model are wrong. The number of image tokens is 0 while the number of image given to the model is 1. This prevents correct indexing and breaks batch generation.\r\n```</BODY>\n\n<COMMENTS>\n<Comment by leeyyi at 2024-03-01T08:50:11Z>\nIn-context learning or fine tuning\n</Comment>\n<Comment by Nomiluks at 2024-03-07T13:40:57Z>\nThat's an excellent question. Similar to OpenAI GPT models, we can enhance them through a few-shot approach. It would be fantastic if we could apply the same method to these pre-trained models. @haotian-liu\n</Comment>\n<Comment by fisher75 at 2024-03-25T10:17:55Z>\nIs it solved? Because I use SGLang for batch inference and I also need this feature for ICL and multiple discussions or few shot.\n</Comment>\n<Comment by Debolena7 at 2024-06-10T12:46:17Z>\n> image{len(templates)}\r\n\r\n\r\n\r\n> ### Describe the issue\r\n> Hi there,\r\n> \r\n> I have some images and some custom explain. So I want to implement few shot learning to make summaries of my images.\r\n> \r\n> This is my current implement:\r\n> \r\n> ```\r\n> templates = [\r\n>     {\r\n>         \"url\": \"\",\r\n>         \"explain\": \"\"\"\"\"\",\r\n>     },\r\n>     {\r\n>         \"url\": \"\",\r\n>         \"explain\": \"\"\"\"\"\",\r\n>     },\r\n>     {\r\n>         \"url\": \"\",\r\n>         \"explain\": \"\"\"\"\"\"\r\n>     },\r\n>     {\r\n>         \"url\": \",\r\n>         \"explain\": \"\"\"\"\"\"\r\n>     },\r\n>     {\r\n>         \"url\": \"\",\r\n>         \"explain\": \"\"\"\"\"\"\r\n>     },\r\n> ]\r\n> ```\r\n> \r\n> My code to build prompt:\r\n> \r\n> ```\r\n> from PIL import Image\r\n> import cv2\r\n> import numpy as np\r\n> import requests\r\n> \r\n> \"\"\"Make image summary\"\"\"\r\n> img_prompt = \"User: <image>\\n\"+\"\\nASSISTANT:\"\r\n> \r\n> prompt = (\r\n>     \"You are an assistant tasked with summarizing images for retrieval. \"\r\n>     \"These summaries will be embedded and used to retrieve the raw image. \"\r\n>     \"Give a concise summary of the image that is well optimized for retrieval.\"\r\n> )\r\n> print(prompt)\r\n> \r\n> images = []\r\n> \r\n> for i, temp in enumerate(templates):\r\n>     image_i = Image.open(requests.get(temp['url'], stream=True).raw)\r\n>     eplain_i  = temp[\"explain\"]\r\n>     example_i = f\"\\nUser: <image{i}>\"+\"\\nASSISTANT:\" + eplain_i + \"\\n\"\r\n>     prompt += example_i\r\n>     images.append(image_i)\r\n> \r\n> prompt += f\"\\nUser: <image{len(templates)}>\"+\"\\nASSISTANT:\"\r\n> print(prompt)\r\n> print('-'*100)\r\n> print(\"Examples:\", len(images))\r\n> ```\r\n> \r\n> Inference:\r\n> \r\n> ```\r\n> target = Image.open(\"figures/figure-2-5.jpg\")\r\n> \r\n> \r\n> out = model_multi_modals(\r\n>     images=images+[target],\r\n>     prompt=prompt,\r\n>     generate_kwargs={\"max_new_tokens\": 2048})\r\n> ```\r\n> \r\n> And my error:\r\n> \r\n> ```\r\n> ValueError: The input provided to the model are wrong. The number of image tokens is 0 while the number of image given to the model is 1. This prevents correct indexing and breaks batch generation.\r\n> ```\r\n---------------\r\n\r\nI think The error is because of the image token. In the prompt, the image token should be given as:\r\n```python\r\n<image>\r\n``` \r\nand not by image id or image index. I got a similar error in my setup for multi-prompt. \r\n\r\nBTW, the model is not capable of performing directly on multiple images and prompts simultaneously, as is evident from the following conversations by the author and others. \r\n\r\nhttps://discuss.huggingface.co/t/llava-multi-image-input-support-for-inference/68458\r\n\r\nhttps://github.com/haotian-liu/LLaVA/issues/197#:~:text=Due%20to%20the%20current%20way%20of%20training%2C%20we%20do%20not%20observe%20the%20model%20having%20very%20good%20capability%20referring%20to%20/%20comparing%20with%20multiple%20images.%20We%20are%20working%20on%20improving%20this%20aspect%20as%20well%2C%20stay%20tuned!\r\n\r\nhttps://github.com/haotian-liu/LLaVA/issues/57#:~:text=Due%20to%20the%20current%20way%20of%20training%2C%20we%20do%20not%20observe%20the%20model%20having%20very%20good%20capability%20referring%20to%20/%20comparing%20with%20multiple%20images.\r\n\r\nhttps://huggingface.co/YouLiXiya/tinyllava-v1.0-1.1b-hf/discussions/1#:~:text=The%20training%20is%20based%20on%20a%20single%20image.%20Multiple%20images%20are%20not%20supported\n</Comment>\n<Comment by ys-zong at 2024-07-14T14:51:15Z>\nHi guys, you can use our implemented codebase for ICL. https://github.com/ys-zong/VL-ICL\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1201,
    "state": "open",
    "created_by": "N4MXN",
    "created_at": "2024-02-29T06:01:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1201</URL>\n\n<TITLE>Source code for LLAVA: v1.6  [Discussion]</TITLE>\n\n<BODY>### Discussion\n\nWhere can I find the source code for LLAVA: v1.6 or even LLAVA: v1.5? I need it to tune the model to my requirements. Any help would be appreciated.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1200,
    "state": "open",
    "created_by": "zengxingchen",
    "created_at": "2024-02-29T03:39:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1200</URL>\n\n<TITLE>[Usage]  `resume_from_checkpoint` fails when finetuning in the lora settings</TITLE>\n\n<BODY>### Describe the issue\n\nI think the code is trying to resume_from_checkpoint like its a full-parameter fine-tunung checkpoint.</BODY>\n\n<COMMENTS>\n<Comment by zengxingchen at 2024-02-29T03:41:02Z>\n<img width=\"1004\" alt=\"Screenshot 2024-02-29 at 11 40 53\" src=\"https://github.com/haotian-liu/LLaVA/assets/63913112/34b832c8-3dbc-41fc-942e-839bf1791921\">\n</Comment>\n<Comment by CynthiaChuang at 2024-03-14T03:57:32Z>\nI have the same issue. Can anyone tell me how to fix it?\n</Comment>\n<Comment by qingyuanxingsi at 2024-03-25T04:47:52Z>\n+1\n</Comment>\n<Comment by sunhm15 at 2024-04-01T08:07:21Z>\n+1\n</Comment>\n<Comment by davidhalladay at 2024-04-08T06:00:41Z>\nI encountered this error while resuming the checkpoint of Lora training. I found that this is basically due to the old version of Transformers that LLaVA is using. Please refer to this issue: https://github.com/huggingface/peft/issues/746. \r\n\r\nIn this issue, the names of keys in the checkpoint saved via deepspeed have some mismatches with those saved via Transformers. Specifically, there is an added \".default.\" in each key of the non-trainable parameters, leading to errors while loading the checkpoint.\r\n\r\nHere is a solution that I found. I have only tested it for Lora training, and it works well. However, I haven't tested it for other features, thus, it may potentially introduce further errors:\r\n\r\n1. This \"mismatch\" problem has been solved in the latest Transformers package. Thus, we need to update the Transformers package to the latest version:\r\n`pip install transformers==4.39.3`\r\n2. And then we need to update Accelerate as well based on the version of Transformers:\r\n`pip install accelerate==0.27.2`\r\n\r\nAgain, this works for me so far only with Lora training. I'm not sure whether this will introduce other errors.\n</Comment>\n<Comment by zhipeixu at 2024-04-25T06:30:33Z>\n> I encountered this error while resuming the checkpoint of Lora training. I found that this is basically due to the old version of Transformers that LLaVA is using. Please refer to this issue: [huggingface/peft#746](https://github.com/huggingface/peft/issues/746).\r\n> \r\n> In this issue, the names of keys in the checkpoint saved via deepspeed have some mismatches with those saved via Transformers. Specifically, there is an added \".default.\" in each key of the non-trainable parameters, leading to errors while loading the checkpoint.\r\n> \r\n> Here is a solution that I found. I have only tested it for Lora training, and it works well. However, I haven't tested it for other features, thus, it may potentially introduce further errors:\r\n> \r\n> 1. This \"mismatch\" problem has been solved in the latest Transformers package. Thus, we need to update the Transformers package to the latest version:\r\n>    `pip install transformers==4.39.3`\r\n> 2. And then we need to update Accelerate as well based on the version of Transformers:\r\n>    `pip install accelerate==0.27.2`\r\n> \r\n> Again, this works for me so far only with Lora training. I'm not sure whether this will introduce other errors.\r\n\r\nbut i meet some problems when pip, how can you solve this :\r\n![image](https://github.com/haotian-liu/LLaVA/assets/87603951/b5d373f8-1689-49f9-b0a4-f5578d3cc33c)\n</Comment>\n<Comment by Linjyan00 at 2024-04-25T11:11:48Z>\n> > I encountered this error while resuming the checkpoint of Lora training. I found that this is basically due to the old version of Transformers that LLaVA is using. Please refer to this issue: [huggingface/peft#746](https://github.com/huggingface/peft/issues/746).\r\n> > In this issue, the names of keys in the checkpoint saved via deepspeed have some mismatches with those saved via Transformers. Specifically, there is an added \".default.\" in each key of the non-trainable parameters, leading to errors while loading the checkpoint.\r\n> > Here is a solution that I found. I have only tested it for Lora training, and it works well. However, I haven't tested it for other features, thus, it may potentially introduce further errors:\r\n> > \r\n> > 1. This \"mismatch\" problem has been solved in the latest Transformers package. Thus, we need to update the Transformers package to the latest version:\r\n> >    `pip install transformers==4.39.3`\r\n> > 2. And then we need to update Accelerate as well based on the version of Transformers:\r\n> >    `pip install accelerate==0.27.2`\r\n> > \r\n> > Again, this works for me so far only with Lora training. I'm not sure whether this will introduce other errors.\r\n> \r\n> but i meet some problems when pip, how can you solve this : ![image](https://private-user-images.githubusercontent.com/87603951/325497731-b5d373f8-1689-49f9-b0a4-f5578d3cc33c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTQwNDA3ODgsIm5iZiI6MTcxNDA0MDQ4OCwicGF0aCI6Ii84NzYwMzk1MS8zMjU0OTc3MzEtYjVkMzczZjgtMTY4OS00OWY5LWIwYTQtZjU1NzhkM2NjMzNjLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDI1VDEwMjEyOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWUzNjE4M2JiZDIxMzU1Mjc5YWI1ODQ2NDkwNzMwNWYzMWQ5OGRiMzlmZjUxZDA4MzJiZTA1MWQ0ZTY1M2IwYmUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.mGrhBGJV7zcncnzFEza51QxBUBp2nssMx29H4qQHxFc)\r\n\r\njust ignore it\n</Comment>\n<Comment by davidhalladay at 2024-04-26T01:17:26Z>\nOn my end, this compatibility issue only causes errors during testing. Therefore, I maintain two separate conda environments: one for training (with transformers==4.39.3) and one for testing (with transformers==4.37.1). While this setup may seem redundant, it offers a quick solution to address the problem.\n</Comment>\n<Comment by user074 at 2024-04-30T02:55:40Z>\n> I encountered this error while resuming the checkpoint of Lora training. I found that this is basically due to the old version of Transformers that LLaVA is using. Please refer to this issue: [huggingface/peft#746](https://github.com/huggingface/peft/issues/746).\r\n> \r\n> In this issue, the names of keys in the checkpoint saved via deepspeed have some mismatches with those saved via Transformers. Specifically, there is an added \".default.\" in each key of the non-trainable parameters, leading to errors while loading the checkpoint.\r\n> \r\n> Here is a solution that I found. I have only tested it for Lora training, and it works well. However, I haven't tested it for other features, thus, it may potentially introduce further errors:\r\n> \r\n> 1. This \"mismatch\" problem has been solved in the latest Transformers package. Thus, we need to update the Transformers package to the latest version:\r\n>    `pip install transformers==4.39.3`\r\n> 2. And then we need to update Accelerate as well based on the version of Transformers:\r\n>    `pip install accelerate==0.27.2`\r\n> \r\n> Again, this works for me so far only with Lora training. I'm not sure whether this will introduce other errors.\r\n\r\nThanks! Solved my issue. I tried to save and load the LoRA checkpoints but had problems for a while\n</Comment>\n<Comment by wenyisir at 2024-05-09T04:42:26Z>\nI fixed this bug by modifying it:  `site-packages/deepspeed/runtime/engine.py line 2675 load_module_strict=Fasle`\n</Comment>\n<Comment by tetsu-kikuchi at 2024-05-09T04:57:06Z>\nI am afraid that `non_lora_trainables.bin` will not be loaded by just setting trainer.train(resume_from_checkpoint=True), because `non_lora_trainables.bin` is a name only specific to LLaVA and is outside the scope of huggingface.\r\nCould anyone clarify this point?\r\n\r\nAdded : It seems that `non_lora_trainables.bin` is not even saved at intermediate saving steps (at every args.save_steps iterations). It is saved only when the whole training schedule is ended.  In any case, I am afraid that `non_lora_trainables.bin` will not be loaded by using huggingface APIs, including other ways such as in #1027\r\n\r\nMaybe we have to insert a code to load `non_lora_trainables.bin` in llava/train/train.py, just as is done, for example, in llava/eval/model_vqa.py.  I would appreciate comments if I am misunderstanding.\n</Comment>\n<Comment by bang123-box at 2024-10-11T11:11:05Z>\n> I am afraid that `non_lora_trainables.bin` will not be loaded by just setting trainer.train(resume_from_checkpoint=True), because `non_lora_trainables.bin` is a name only specific to LLaVA and is outside the scope of huggingface. Could anyone clarify this point?\r\n> \r\n> Added : It seems that `non_lora_trainables.bin` is not even saved at intermediate saving steps (at every args.save_steps iterations). It is saved only when the whole training schedule is ended. In any case, I am afraid that `non_lora_trainables.bin` will not be loaded by using huggingface APIs, including other ways such as in #1027\r\n> \r\n> Maybe we have to insert a code to load `non_lora_trainables.bin` in llava/train/train.py, just as is done, for example, in llava/eval/model_vqa.py. I would appreciate comments if I am misunderstanding.\r\n\r\nhello, I am also confused about this question. We should reload non_lora_trainables.bin to projector, but in the checkpoint files, I cannot find this non_lora_trainables.bin file, so what I need to do to save non_lora_trainables.bin in the saving checkpoint stage?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1198,
    "state": "open",
    "created_by": "YangQiuEric",
    "created_at": "2024-02-28T22:29:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1198</URL>\n\n<TITLE>How to change CLIP to BiomedCLIP</TITLE>\n\n<BODY>### Question\n\nI am doing medical image project, so how can we change CLIP to other models?</BODY>\n\n<COMMENTS>\n<Comment by zsxm1998 at 2024-02-29T07:29:24Z>\nchange the vision_tower parameter\n</Comment>\n<Comment by 20191864218 at 2024-03-01T09:34:04Z>\n> change the vision_tower parameter\r\n\r\n\r\n\r\n> change the vision_tower parameter\r\n\r\nHi, could you please be more specific? Which specific parts do you need to change?Thank you\n</Comment>\n<Comment by YangQiuEric at 2024-03-06T23:57:38Z>\n@zsxm1998 @20191864218 I just change the vision_tower to microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224, and the finetune can be done. But the model_vqa.py has errors, can not recognize this model. I tried to download all the files from hugging face, but it does not have preprocess_config.json and config.json. How to fix?\n</Comment>\n<Comment by ATing0203 at 2024-03-18T03:45:16Z>\n> @zsxm1998 @20191864218 I just change the vision_tower to microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224, and the finetune can be done. But the model_vqa.py has errors, can not recognize this model. I tried to download all the files from hugging face, but it does not have preprocess_config.json and config.json. How to fix?\r\n\r\nHave you solved the problem yet? @YangQiuEric\n</Comment>\n<Comment by Abhiram-kandiyana at 2024-04-08T13:13:49Z>\n> @zsxm1998 @20191864218 I just change the vision_tower to microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224, and the finetune can be done. But the model_vqa.py has errors, can not recognize this model. I tried to download all the files from hugging face, but it does not have preprocess_config.json and config.json. How to fix?\r\n\r\nAny update @YangQiuEric ?\n</Comment>\n<Comment by TommyIX at 2025-02-20T07:57:57Z>\nI have made an implementation myself for llava models to adapt to biomedclip, see [this gist.](https://gist.github.com/TommyIX/681ad23947c3aa7c8482f4d39849df7d)\nStraight forward parameter settings were used to at least get it running... Thanks for understanding\n</Comment>\n<Comment by beidiz at 2025-03-17T09:26:06Z>\n> I have made an implementation myself for llava models to adapt to biomedclip, see [this gist.](https://gist.github.com/TommyIX/681ad23947c3aa7c8482f4d39849df7d)\n> Straight forward parameter settings were used to at least get it running... Thanks for understanding\n\nHi, thanks for providing the code. I was wondering after directly change the vision encoder to biomedclip, can your llava model generate output for questions. The model can run from my side, but it will not generate anything in the output. Did you meet the same situation?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1197,
    "state": "open",
    "created_by": "piperino11",
    "created_at": "2024-02-28T22:22:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1197</URL>\n\n<TITLE>[Usage]Error message when i run finetune_lora.sh. pip install flash-attn --no-build-isolation --no-cache-dir doesn't work</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nTraceback (most recent call last):\r\n  File \"/home/sca/LLaVA/llava/train/train_mem.py\", line 1, in <module>\r\n    from llava.train.train import train\r\n  File \"/home/sca/LLaVA/llava/__init__.py\", line 1, in <module>\r\n    from .model import LlavaLlamaForCausalLM\r\nImportError: cannot import name 'LlavaLlamaForCausalLM' from 'llava.model' (/home/sca/LLaVA/llava/model/__init__.py)\r\n\r\nSTEP:\r\n```\r\ngit clone https://github.com/haotian-liu/LLaVA.git\r\ncd LLaVA\r\nconda create -n llava python=3.10 -y\r\nconda activate llava\r\npip install --upgrade pip \r\npip install -e .\r\npip install -e \".[train]\"\r\npip install flash-attn --no-build-isolation --no-cache-dir\r\n```\r\nCommand:\r\n```\r\nbash my_python_finetune_lora.sh\r\n```\r\nenv: \r\n```\r\n# Name                    Version                   Build  Channel\r\n_libgcc_mutex             0.1                        main    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\n_openmp_mutex             5.1                       1_gnu    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\naccelerate                0.21.0                   pypi_0    pypi\r\naiofiles                  23.2.1                   pypi_0    pypi\r\naltair                    5.2.0                    pypi_0    pypi\r\nannotated-types           0.6.0                    pypi_0    pypi\r\nanyio                     4.3.0                    pypi_0    pypi\r\nappdirs                   1.4.4                    pypi_0    pypi\r\nattrs                     23.2.0                   pypi_0    pypi\r\nbitsandbytes              0.42.0                   pypi_0    pypi\r\nbzip2                     1.0.8                h5eee18b_5    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nca-certificates           2023.12.12           h06a4308_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\ncertifi                   2024.2.2                 pypi_0    pypi\r\ncharset-normalizer        3.3.2                    pypi_0    pypi\r\nclick                     8.1.7                    pypi_0    pypi\r\ncolorama                  0.4.6                    pypi_0    pypi\r\ncontourpy                 1.2.0                    pypi_0    pypi\r\ncuda-cccl                 11.8.89                       0    nvidia/label/cuda-11.8.0\r\ncuda-command-line-tools   11.8.0                        0    nvidia/label/cuda-11.8.0\r\ncuda-compiler             11.8.0                        0    nvidia/label/cuda-11.8.0\r\ncuda-cudart               11.8.89                       0    nvidia/label/cuda-11.8.0\r\ncuda-cudart-dev           11.8.89                       0    nvidia/label/cuda-11.8.0\r\ncuda-cuobjdump            11.8.86                       0    nvidia/label/cuda-11.8.0\r\ncuda-cupti                11.8.87                       0    nvidia/label/cuda-11.8.0\r\ncuda-cuxxfilt             11.8.86                       0    nvidia/label/cuda-11.8.0\r\ncuda-documentation        11.8.86                       0    nvidia/label/cuda-11.8.0\r\ncuda-driver-dev           11.8.89                       0    nvidia/label/cuda-11.8.0\r\ncuda-gdb                  11.8.86                       0    nvidia/label/cuda-11.8.0\r\ncuda-libraries            11.8.0                        0    nvidia/label/cuda-11.8.0\r\ncuda-libraries-dev        11.8.0                        0    nvidia/label/cuda-11.8.0\r\ncuda-memcheck             11.8.86                       0    nvidia/label/cuda-11.8.0\r\ncuda-nsight               11.8.86                       0    nvidia/label/cuda-11.8.0\r\ncuda-nsight-compute       11.8.0                        0    nvidia/label/cuda-11.8.0\r\ncuda-nvcc                 11.8.89                       0    nvidia/label/cuda-11.8.0\r\ncuda-nvdisasm             11.8.86                       0    nvidia/label/cuda-11.8.0\r\ncuda-nvml-dev             11.8.86                       0    nvidia/label/cuda-11.8.0\r\ncuda-nvprof               11.8.87                       0    nvidia/label/cuda-11.8.0\r\ncuda-nvprune              11.8.86                       0    nvidia/label/cuda-11.8.0\r\ncuda-nvrtc                11.8.89                       0    nvidia/label/cuda-11.8.0\r\ncuda-nvrtc-dev            11.8.89                       0    nvidia/label/cuda-11.8.0\r\ncuda-nvtx                 11.8.86                       0    nvidia/label/cuda-11.8.0\r\ncuda-nvvp                 11.8.87                       0    nvidia/label/cuda-11.8.0\r\ncuda-profiler-api         11.8.86                       0    nvidia/label/cuda-11.8.0\r\ncuda-sanitizer-api        11.8.86                       0    nvidia/label/cuda-11.8.0\r\ncuda-toolkit              11.8.0                        0    nvidia/label/cuda-11.8.0\r\ncuda-tools                11.8.0                        0    nvidia/label/cuda-11.8.0\r\ncuda-visual-tools         11.8.0                        0    nvidia/label/cuda-11.8.0\r\ncycler                    0.12.1                   pypi_0    pypi\r\ndeepspeed                 0.12.6                   pypi_0    pypi\r\ndocker-pycreds            0.4.0                    pypi_0    pypi\r\neinops                    0.6.1                    pypi_0    pypi\r\neinops-exts               0.0.4                    pypi_0    pypi\r\nexceptiongroup            1.2.0                    pypi_0    pypi\r\nfastapi                   0.110.0                  pypi_0    pypi\r\nffmpy                     0.3.2                    pypi_0    pypi\r\nfilelock                  3.13.1                   pypi_0    pypi\r\nflash-attn                2.5.5                    pypi_0    pypi\r\nfonttools                 4.49.0                   pypi_0    pypi\r\nfsspec                    2024.2.0                 pypi_0    pypi\r\ngds-tools                 1.4.0.31                      0    nvidia/label/cuda-11.8.0\r\ngitdb                     4.0.11                   pypi_0    pypi\r\ngitpython                 3.1.42                   pypi_0    pypi\r\ngradio                    4.16.0                   pypi_0    pypi\r\ngradio-client             0.8.1                    pypi_0    pypi\r\nh11                       0.14.0                   pypi_0    pypi\r\nhjson                     3.1.0                    pypi_0    pypi\r\nhttpcore                  0.17.3                   pypi_0    pypi\r\nhttpx                     0.24.0                   pypi_0    pypi\r\nhuggingface-hub           0.21.2                   pypi_0    pypi\r\nidna                      3.6                      pypi_0    pypi\r\nimportlib-resources       6.1.2                    pypi_0    pypi\r\njinja2                    3.1.3                    pypi_0    pypi\r\njoblib                    1.3.2                    pypi_0    pypi\r\njsonschema                4.21.1                   pypi_0    pypi\r\njsonschema-specifications 2023.12.1                pypi_0    pypi\r\nkiwisolver                1.4.5                    pypi_0    pypi\r\nld_impl_linux-64          2.38                 h1181459_1    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nlibcublas                 11.11.3.6                     0    nvidia/label/cuda-11.8.0\r\nlibcublas-dev             11.11.3.6                     0    nvidia/label/cuda-11.8.0\r\nlibcufft                  10.9.0.58                     0    nvidia/label/cuda-11.8.0\r\nlibcufft-dev              10.9.0.58                     0    nvidia/label/cuda-11.8.0\r\nlibcufile                 1.4.0.31                      0    nvidia/label/cuda-11.8.0\r\nlibcufile-dev             1.4.0.31                      0    nvidia/label/cuda-11.8.0\r\nlibcurand                 10.3.0.86                     0    nvidia/label/cuda-11.8.0\r\nlibcurand-dev             10.3.0.86                     0    nvidia/label/cuda-11.8.0\r\nlibcusolver               11.4.1.48                     0    nvidia/label/cuda-11.8.0\r\nlibcusolver-dev           11.4.1.48                     0    nvidia/label/cuda-11.8.0\r\nlibcusparse               11.7.5.86                     0    nvidia/label/cuda-11.8.0\r\nlibcusparse-dev           11.7.5.86                     0    nvidia/label/cuda-11.8.0\r\nlibffi                    3.4.4                h6a678d5_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nlibgcc-ng                 11.2.0               h1234567_1    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nlibgomp                   11.2.0               h1234567_1    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nlibnpp                    11.8.0.86                     0    nvidia/label/cuda-11.8.0\r\nlibnpp-dev                11.8.0.86                     0    nvidia/label/cuda-11.8.0\r\nlibnvjpeg                 11.9.0.86                     0    nvidia/label/cuda-11.8.0\r\nlibnvjpeg-dev             11.9.0.86                     0    nvidia/label/cuda-11.8.0\r\nlibstdcxx-ng              11.2.0               h1234567_1    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nlibuuid                   1.41.5               h5eee18b_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nllava                     1.2.2.post1              pypi_0    pypi\r\nmarkdown-it-py            3.0.0                    pypi_0    pypi\r\nmarkdown2                 2.4.13                   pypi_0    pypi\r\nmarkupsafe                2.1.5                    pypi_0    pypi\r\nmatplotlib                3.8.3                    pypi_0    pypi\r\nmdurl                     0.1.2                    pypi_0    pypi\r\nmpmath                    1.3.0                    pypi_0    pypi\r\nncurses                   6.4                  h6a678d5_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nnetworkx                  3.2.1                    pypi_0    pypi\r\nnsight-compute            2022.3.0.22                   0    nvidia/label/cuda-11.8.0\r\nnumpy                     1.26.4                   pypi_0    pypi\r\nnvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\r\nnvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\r\nnvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\r\nnvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\r\nnvidia-cudnn-cu12         8.9.2.26                 pypi_0    pypi\r\nnvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\r\nnvidia-curand-cu12        10.3.2.106               pypi_0    pypi\r\nnvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\r\nnvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\r\nnvidia-nccl-cu12          2.18.1                   pypi_0    pypi\r\nnvidia-nvjitlink-cu12     12.3.101                 pypi_0    pypi\r\nnvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\r\nopenssl                   3.0.13               h7f8727e_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\norjson                    3.9.15                   pypi_0    pypi\r\npackaging                 23.2                     pypi_0    pypi\r\npandas                    2.2.1                    pypi_0    pypi\r\npeft                      0.9.0                    pypi_0    pypi\r\npillow                    10.2.0                   pypi_0    pypi\r\npip                       24.0                     pypi_0    pypi\r\npsutil                    5.9.8                    pypi_0    pypi\r\npy-cpuinfo                9.0.0                    pypi_0    pypi\r\npydantic                  2.6.3                    pypi_0    pypi\r\npydantic-core             2.16.3                   pypi_0    pypi\r\npydub                     0.25.1                   pypi_0    pypi\r\npygments                  2.17.2                   pypi_0    pypi\r\npynvml                    11.5.0                   pypi_0    pypi\r\npyparsing                 3.1.1                    pypi_0    pypi\r\npython                    3.10.13              h955ad1f_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\npython-dateutil           2.8.2                    pypi_0    pypi\r\npython-multipart          0.0.9                    pypi_0    pypi\r\npytz                      2024.1                   pypi_0    pypi\r\npyyaml                    6.0.1                    pypi_0    pypi\r\nreadline                  8.2                  h5eee18b_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nreferencing               0.33.0                   pypi_0    pypi\r\nregex                     2023.12.25               pypi_0    pypi\r\nrequests                  2.31.0                   pypi_0    pypi\r\nrich                      13.7.1                   pypi_0    pypi\r\nrpds-py                   0.18.0                   pypi_0    pypi\r\nruff                      0.2.2                    pypi_0    pypi\r\nsafetensors               0.4.2                    pypi_0    pypi\r\nscikit-learn              1.2.2                    pypi_0    pypi\r\nscipy                     1.12.0                   pypi_0    pypi\r\nsemantic-version          2.10.0                   pypi_0    pypi\r\nsentencepiece             0.1.99                   pypi_0    pypi\r\nsentry-sdk                1.40.6                   pypi_0    pypi\r\nsetproctitle              1.3.3                    pypi_0    pypi\r\nsetuptools                68.2.2          py310h06a4308_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nshellingham               1.5.4                    pypi_0    pypi\r\nshortuuid                 1.0.11                   pypi_0    pypi\r\nsix                       1.16.0                   pypi_0    pypi\r\nsmmap                     5.0.1                    pypi_0    pypi\r\nsniffio                   1.3.1                    pypi_0    pypi\r\nsqlite                    3.41.2               h5eee18b_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nstarlette                 0.36.3                   pypi_0    pypi\r\nsvgwrite                  1.4.3                    pypi_0    pypi\r\nsympy                     1.12                     pypi_0    pypi\r\nthreadpoolctl             3.3.0                    pypi_0    pypi\r\ntimm                      0.6.13                   pypi_0    pypi\r\ntk                        8.6.12               h1ccaba5_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\ntokenizers                0.15.1                   pypi_0    pypi\r\ntomlkit                   0.12.0                   pypi_0    pypi\r\ntoolz                     0.12.1                   pypi_0    pypi\r\ntorch                     2.1.2                    pypi_0    pypi\r\ntorchvision               0.16.2                   pypi_0    pypi\r\ntqdm                      4.66.2                   pypi_0    pypi\r\ntransformers              4.37.2                   pypi_0    pypi\r\ntriton                    2.1.0                    pypi_0    pypi\r\ntyper                     0.9.0                    pypi_0    pypi\r\ntyping-extensions         4.10.0                   pypi_0    pypi\r\ntzdata                    2024.1                   pypi_0    pypi\r\nurllib3                   2.2.1                    pypi_0    pypi\r\nuvicorn                   0.27.1                   pypi_0    pypi\r\nwandb                     0.16.3                   pypi_0    pypi\r\nwavedrom                  2.0.3.post3              pypi_0    pypi\r\nwebsockets                11.0.3                   pypi_0    pypi\r\nwheel                     0.41.2          py310h06a4308_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nxz                        5.4.6                h5eee18b_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nzlib                      1.2.13               h5eee18b_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\n```</BODY>\n\n<COMMENTS>\n<Comment by TobiasJu at 2024-05-03T13:41:16Z>\nHave a look at https://github.com/haotian-liu/LLaVA/issues/1291#issuecomment-2002727430\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1195,
    "state": "open",
    "created_by": "NeilHnxTcc",
    "created_at": "2024-02-28T08:24:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1195</URL>\n\n<TITLE>[Usage] output meanless code when running the cli.py using LLaVA-Lightning-7B-v1-1 as the model</TITLE>\n\n<BODY>### Describe the issue\n\ngreat work! but I met some problem.\r\nusing llava-1.5 as the model the cli.py ran smoothly and correctly but once I change the model to LLaVA-Lightning-7B-v1-1 the answer text is random code. why?\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/130633542/e3b314fe-ab8c-4df4-8c00-b552270661e2)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1194,
    "state": "open",
    "created_by": "charismaticchiu",
    "created_at": "2024-02-28T06:44:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1194</URL>\n\n<TITLE>[Question] Merging LoRA weights with lmsys/vicuna-13b-v1.5 instead of liuhaotian/llava-v1.5-13b</TITLE>\n\n<BODY>### Question\n\nHi I finetune my own model with LoRA, `new-v1.5-13b-lora-665k-custom` using `finetune_lora.sh` but have trouble merging the LoRA weights with `lmsys/vicuna-13b-v1.5` backbone.\r\n\r\nCan anyone shed some light? Thank you!\r\n\r\nThe command I used is\r\n\r\n> `python scripts/merge_lora_weights.py \\\r\n>     --model-path ./checkpoints/new-v1.5-13b-lora-665k-custom \\\r\n>     --model-base lmsys/vicuna-13b-v1.5 \\\r\n>     --save-model-path ./checkpoints/merged/new-v1.5-13b-lora-665k-custom\r\n> `\r\n\r\nAnd error is below\r\n\r\n> Loading LLaVA from base model...\r\nLoading checkpoint shards:   0%|                                                       | 0/3 [00:00<?, ?it/s]/home1/XXX/.conda/envs/llava2/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████| 3/3 [00:19<00:00,  6.36s/it]\r\nSome weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at lmsys/vicuna-13b-v1.5 and are newly initialized: ['model.mm_projector.0.bias', 'model.mm_projector.0.weight', 'model.mm_projector.2.bias', 'model.mm_projector.2.weight']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n/home1/XXX/.conda/envs/llava2/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\r\n  warnings.warn(\r\n/home1/XXX/.conda/envs/llava2/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\r\n  warnings.warn(\r\nLoading additional LLaVA weights...\r\nLoading LoRA weights...\r\nMerging LoRA weights...\r\nModel is loaded...\r\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\r\nNon-default generation parameters: {'max_length': 4096}\r\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\r\nTraceback (most recent call last):\r\n  File \"/home1/XXX/.conda/envs/llava2/lib/python3.10/site-packages/transformers/generation/configuration_utils.py\", line 558, in save_pretrained\r\n    raise ValueError(str([w.message for w in caught_warnings]))\r\nValueError: [UserWarning('`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.'), UserWarning('`do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.')]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/project/LLaVA/scripts/merge_lora_weights.py\", line 22, in <module>\r\n    merge_lora(args)\r\n  File \"/project/LLaVA/scripts/merge_lora_weights.py\", line 10, in merge_lora\r\n    model.save_pretrained(args.save_model_path)\r\n  File \"/home1/XXX/.conda/envs/llava2/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2364, in save_pretrained\r\n    model_to_save.generation_config.save_pretrained(save_directory)\r\n  File \"/home1/XXX/.conda/envs/llava2/lib/python3.10/site-packages/transformers/generation/configuration_utils.py\", line 560, in save_pretrained\r\n    raise ValueError(\r\nValueError: The generation config instance is invalid -- `.validate()` throws warnings and/or exceptions. Fix these issues to save the configuration.\r\n\r\nThrown during validation:\r\n[UserWarning('`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.'), UserWarning('`do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.')]</BODY>\n\n<COMMENTS>\n<Comment by charismaticchiu at 2024-02-29T23:24:35Z>\n@haotian-liu \r\n\r\nDo you think it is caused by the non-default `max_length` or `do_sample=True`? \r\n\r\nShould I manually unset `max_length` or set `do_sample: true`  in the config.json? I actually tried both, but still has the same warning.    Maybe the problem comes from the `lmsys/vicuna-13b-v1.5` checkpoint?\n</Comment>\n<Comment by PointsCoder at 2024-03-06T19:46:16Z>\nSame issue here\n</Comment>\n<Comment by PointsCoder at 2024-03-06T20:11:42Z>\nI solved this issue by downgrading the Transformer version with `pip install git+https://github.com/huggingface/transformers@v4.31-release `\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1193,
    "state": "open",
    "created_by": "shashwat14",
    "created_at": "2024-02-28T03:40:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1193</URL>\n\n<TITLE>[Usage] Unable to reproduce TextVQA results for liuhaotian/llava-v1.5-7b</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\n#!/bin/bash\r\n\r\npython -m llava.eval.model_vqa_loader \\\r\n    --model-path liuhaotian/llava-v1.5-7b \\\r\n    --question-file ./playground/data/eval/textvqa/llava_textvqa_val_v051_ocr.jsonl \\\r\n    --image-folder ./playground/data/eval/textvqa/train_images \\\r\n    --answers-file ./playground/data/eval/textvqa/answers/llava-v1.5-7b.jsonl \\\r\n    --temperature 0 \\\r\n    --conv-mode vicuna_v1\r\n\r\npython -m llava.eval.eval_textvqa \\\r\n    --annotation-file ./playground/data/eval/textvqa/TextVQA_0.5.1_val.json \\\r\n    --result-file ./playground/data/eval/textvqa/answers/llava-v1.5-7b.jsonl\r\n\r\n```\r\n\r\nLog: \r\n```\r\n[2024-02-27 22:45:21,047] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nLoading checkpoint shards:   0%|                                                                                                                       | 0/2 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.75s/it]\r\n  0%|                                                                                                                                               | 0/5000 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\r\n  warnings.warn(\r\n/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\r\n  warnings.warn(\r\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [26:43<00:00,  3.12it/s]\r\n[2024-02-27 23:12:16,324] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nllava-v1.5-7b\r\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:02<00:00, 2099.00it/s]\r\nSamples: 5000\r\nAccuracy: 40.36%\r\n```\r\n\r\nModelZoo.md claims 58.2 on TextVQA task for liuhaotian/llava-v1.5-7b.</BODY>\n\n<COMMENTS>\n<Comment by shashwat14 at 2024-03-04T21:34:25Z>\nAble to replicate on an A100 instance. Wonder if it's the dtype issue.\n</Comment>\n<Comment by 191220042 at 2024-06-03T04:43:55Z>\ncan you tell me where is llava_textvqa_val_v051_ocr.jsonl\n</Comment>\n<Comment by jhuang2026 at 2024-06-04T15:18:44Z>\n@191220042 In [evaluation.md](https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md), there is an eval.zip file that contains the file you're looking for (in the directory textvqa).\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1192,
    "state": "open",
    "created_by": "dengweihuan",
    "created_at": "2024-02-28T02:59:35Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1192</URL>\n\n<TITLE>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1![Question]</TITLE>\n\n<BODY>### Question\n\nI want to launch a model worker (Multiple GPUs, when GPU VRAM <= 24GB) with the code \"CUDA_VISIBLE_DEVICES=0,1 python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b\", raise the problem of \"RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1!\". how to fix it.</BODY>\n\n<COMMENTS>\n<Comment by RylanSchaeffer at 2024-03-10T21:23:14Z>\nI'm having the same error. I posted here as well: https://github.com/huggingface/transformers/pull/28051\r\n\r\nThis is my gist to reproduce: https://gist.github.com/RylanSchaeffer/8198233a445ab3373aff01b038cc4b8b\n</Comment>\n<Comment by dengweihuan at 2024-03-26T13:51:40Z>\nMany thanks\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1191,
    "state": "open",
    "created_by": "RylanSchaeffer",
    "created_at": "2024-02-27T23:22:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1191</URL>\n\n<TITLE>[Question] Which `conv_templates` should be used with which Llava checkpoints?</TITLE>\n\n<BODY>### Question\r\n\r\nThe model zoo helpfully lists all the available checkpoints: https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md e.g.:\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/8942987/1ee8a98a-7ae4-4751-88a3-b00c7d2f92db)\r\n\r\nThe code also lists a number of conversation templates:\r\n\r\n```\r\nconversation_templates = {\r\n    \"default\": conv_vicuna_v0,\r\n    \"v0\": conv_vicuna_v0,\r\n    \"v1\": conv_vicuna_v1,\r\n    \"vicuna_v1\": conv_vicuna_v1,\r\n    \"llama_2\": conv_llama_2,\r\n    \"v0_plain\": conv_llava_plain,\r\n    \"llava_v0\": conv_llava_v0,\r\n    \"v0_mmtag\": conv_llava_v0_mmtag,\r\n    \"llava_v1\": conv_llava_v1,\r\n    \"v1_mmtag\": conv_llava_v1_mmtag,\r\n    \"llava_llama_2\": conv_llava_llama_2,\r\n    \"mpt\": conv_mpt,\r\n}\r\n```\r\n\r\nWhich template should be used with which checkpoint?</BODY>\n\n<COMMENTS>\n<Comment by Reichenbachian at 2024-04-18T18:51:16Z>\nAny answer on this?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1190,
    "state": "open",
    "created_by": "hzgdeerHo",
    "created_at": "2024-02-27T13:40:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1190</URL>\n\n<TITLE>[Usage] return tensor.to(device, non_blocking=non_blocking) NotImplementedError: Cannot copy out of meta tensor; no data!</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.mm_utils import get_model_name_from_path\r\nfrom llava.eval.run_llava import eval_model\r\n\r\nmodel_path = \"liuhaotian/llava-v1.6-34b\"\r\n\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    model_path=model_path,\r\n    model_base=None,\r\n    model_name=get_model_name_from_path(model_path),\r\n    device=\"cuda\"\r\n    \r\n    \r\n)\r\n\r\nmodel_path = \"liuhaotian/llava-v1.6-34b\"\r\nprompt = \"What are the things I should be cautious about when I visit here?\"\r\nimage_file = \"https://llava-vl.github.io/static/images/view.jpg\"\r\n\r\nargs = type('Args', (), {\r\n    \"model_path\": model_path,\r\n    \"model_base\": None,\r\n    \"model_name\": get_model_name_from_path(model_path),\r\n    \"query\": prompt,\r\n    \"conv_mode\": None,\r\n    \"image_file\": image_file,\r\n    \"sep\": \",\",\r\n    \"temperature\": 0,\r\n    \"top_p\": None,\r\n    \"num_beams\": 1,\r\n    \"max_new_tokens\": 512\r\n})()\r\n\r\neval_model(args)\r\n####\r\n\r\nGOT THIS ERROR：raceback (most recent call last):\r\n  File \"/home/ubuntu/Llava-34B-1.6.py\", line 34, in <module>\r\n    eval_model(args)\r\n  File \"/home/ubuntu/LLaVA/llava/eval/run_llava.py\", line 115, in eval_model\r\n    output_ids = model.generate(\r\n  File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/ubuntu/LLaVA/llava/model/language_model/llava_llama.py\", line 125, in generate\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/home/ubuntu/LLaVA/llava/model/llava_arch.py\", line 157, in prepare_inputs_labels_for_multimodal\r\n    image_features = self.encode_images(concat_images)\r\n  File \"/home/ubuntu/LLaVA/llava/model/llava_arch.py\", line 141, in encode_images\r\n    image_features = self.get_model().get_vision_tower()(images)\r\n  File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/ubuntu/LLaVA/llava/model/multimodal_encoder/clip_encoder.py\", line 54, in forward\r\n    image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\r\n  File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 160, in new_forward\r\n    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\r\n  File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 290, in pre_forward\r\n    return send_to_device(args, self.execution_device), send_to_device(\r\n  File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 151, in send_to_device\r\n    return honor_type(\r\n  File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 83, in honor_type\r\n    return type(obj)(generator)\r\n  File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 152, in <genexpr>\r\n    tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\r\n  File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 167, in send_to_device\r\n    return tensor.to(device, non_blocking=non_blocking)\r\nNotImplementedError: Cannot copy out of meta tensor; no data!\r\n#####\r\n4XV100 GPU 32G\r\n\r\n###How Can I run this model to infer ?</BODY>\n\n<COMMENTS>\n<Comment by hzgdeerHo at 2024-02-27T13:42:02Z>\nAnother way to run this model also failed and got OOM on this hardware configuration :CUDA_VISIBLE_DEVICES=0,1,2,3 python3 -m sglang.launch_server --model-path  liuhaotian/llava-v1.6-34b --tokenizer-path liuhaotian/llava-v1.6-34b-tokenizer --port 30000 --tp 2\n</Comment>\n<Comment by SKosinski at 2024-02-29T09:42:56Z>\nI am struggling with similar issue on my device.\r\nWhile running llava-v1.5 works fine, running llava-v1.6 in any configuration (7B, 13B and 34B) yields same errors.\n</Comment>\n<Comment by Larerr at 2024-04-15T09:16:09Z>\nMe too\n</Comment>\n<Comment by linmingyan123 at 2024-05-16T03:55:40Z>\nMe too\n</Comment>\n<Comment by renllll at 2024-05-20T10:00:39Z>\nHow to solve this error\n</Comment>\n<Comment by LanqingL at 2024-08-09T10:00:35Z>\nThis problem may occur if out of memory. Observe trend by \"watch nvidia-smi\".\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1189,
    "state": "open",
    "created_by": "awzhgw",
    "created_at": "2024-02-27T13:13:35Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1189</URL>\n\n<TITLE>When the code and dataset of LLaVA1.6 will be released? [Question]</TITLE>\n\n<BODY>### Question\n\nWhen the code and dataset of LLaVA1.6 will be released?</BODY>\n\n<COMMENTS>\n<Comment by zsxm1998 at 2024-02-29T07:30:01Z>\nsame question\n</Comment>\n<Comment by awzhgw at 2024-02-29T14:11:42Z>\nsame too\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1188,
    "state": "closed",
    "created_by": "arvinxx",
    "created_at": "2024-02-27T11:12:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1188</URL>\n\n<TITLE>Does LLAVA have a logo？</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by HelloWorldLTY at 2024-02-29T15:09:46Z>\nI think so, the volcano is what you are looking for.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1186,
    "state": "open",
    "created_by": "HelloWorldLTY",
    "created_at": "2024-02-27T04:57:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1186</URL>\n\n<TITLE>Where I can find the model with pretrain name?</TITLE>\n\n<BODY>### Describe the issue\n\nHi, in the finetuning code, I notie that the model has name: --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin \\\r\n\r\nHowever, I can only find foder without pretrain notifaction, like llava-v1.5-13b\r\n\r\nAre they same? Thanks.</BODY>\n\n<COMMENTS>\n<Comment by HelloWorldLTY at 2024-02-27T04:59:53Z>\nFurthermore, if I remove the word \"pretrain\", I meet a new bug:\r\n\r\n_pickle.UnpicklingError: invalid load key, 'v'.\r\n\r\nThe finetune_LLava seems outdated since it is still based on transformers=4.31.0\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1185,
    "state": "open",
    "created_by": "lucasjinreal",
    "created_at": "2024-02-27T03:08:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1185</URL>\n\n<TITLE>Why not open vision encoder and LLM in last stage?</TITLE>\n\n<BODY>Yi-VL and many other methods, all using open vision encoder and LLM in last stage training strategy.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1184,
    "state": "open",
    "created_by": "WilTay1",
    "created_at": "2024-02-27T02:38:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1184</URL>\n\n<TITLE>[Usage] Demo is not working, cannot upload image</TITLE>\n\n<BODY>### Describe the issue\n\nalways like this\r\n<img width=\"262\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/133484736/5c9ae7e2-d5f2-425e-a7b2-4858fda5e925\"></BODY>\n\n<COMMENTS>\n<Comment by lh-seu at 2024-02-27T03:33:50Z>\n+1\n</Comment>\n<Comment by HelloWorldLTY at 2024-02-27T13:54:56Z>\nSame question here.\n</Comment>\n<Comment by manas6266 at 2024-03-07T11:55:14Z>\n+1\n</Comment>\n<Comment by edward3862 at 2024-03-08T04:39:50Z>\n+1\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1183,
    "state": "open",
    "created_by": "butterluo",
    "created_at": "2024-02-27T00:58:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1183</URL>\n\n<TITLE>When the code and dataset of LLaVA1.6 will be released?</TITLE>\n\n<BODY>### Question\n\nWhen the code and dataset of LLaVA1.6 will be released?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1182,
    "state": "open",
    "created_by": "Occupying-Mars",
    "created_at": "2024-02-26T23:24:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1182</URL>\n\n<TITLE>How can we get the attention weights of the input tokens during the inference [Discussion]</TITLE>\n\n<BODY>### Discussion\n\nBeen trying to experiment with Llava and i wanted to get attention weights of the input tokens and since it uses llamaspadattetnion + the monkeypatch i wanted to know how we can get the attention weights of the input tokens (both for text and image) during the inference (new and learning about pytorch pardon if this is a dumb issue)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1180,
    "state": "open",
    "created_by": "xjturobocon",
    "created_at": "2024-02-26T12:44:55Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1180</URL>\n\n<TITLE>can pytorch==2.0.1 woks well?</TITLE>\n\n<BODY>### Question\n\nfor some reason, I can not install pytorch>2.0.1 succesfully,  can I just use pytorch==2.0.1? will it decrease performance?</BODY>\n\n<COMMENTS>\n<Comment by Mowenyii at 2024-02-27T02:23:19Z>\nsame question\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1179,
    "state": "open",
    "created_by": "detrin",
    "created_at": "2024-02-26T11:39:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1179</URL>\n\n<TITLE>[Question] Is it possible to run llava-v1.6-mistral-7b with transformers?</TITLE>\n\n<BODY>### Question\n\nComing from https://huggingface.co/liuhaotian/llava-v1.6-mistral-7b I tried to make it work in Google Colab, but without any success. Could you please tell me if it is possible to run it with transformers and show me the code snippet for it please? Or should I use llava-hf/llava-v1.6-mistral-7b-hf ?</BODY>\n\n<COMMENTS>\n<Comment by wanderingweights at 2024-02-27T14:52:33Z>\nSecond this\n</Comment>\n<Comment by aliencaocao at 2024-03-02T15:20:12Z>\nhttps://github.com/haotian-liu/LLaVA/pull/1115\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1178,
    "state": "open",
    "created_by": "20191864218",
    "created_at": "2024-02-26T08:53:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1178</URL>\n\n<TITLE>如何把视觉编码器clip替换成别的视觉编码器How to replace a visual encoder clip with a different visual encoder</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by Amazingren at 2024-08-15T10:09:02Z>\nDid you solved this problem bro?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1177,
    "state": "open",
    "created_by": "CateMrl",
    "created_at": "2024-02-26T08:37:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1177</URL>\n\n<TITLE>RuntimeError: \"addmm_impl_cpu_\" not implemented for 'Half'</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI'm trying to use llava using [CLI Inference](https://github.com/haotian-liu/LLaVA?tab=readme-ov-file#cli-inference)\r\nbut it gives me this error **RuntimeError: \"addmm_impl_cpu_\" not implemented for 'Half'**\r\nhow can I solve this?</BODY>\n\n<COMMENTS>\n<Comment by shashwat14 at 2024-02-29T19:09:46Z>\nSame issue\n</Comment>\n<Comment by Adrian0999 at 2024-03-20T08:16:18Z>\nsame issue\n</Comment>\n<Comment by Alexey234432 at 2024-03-21T16:13:03Z>\nany solutions to this?\n</Comment>\n<Comment by shashwat14 at 2024-03-21T16:14:45Z>\nI used A100s instance and error went away.\r\n\r\nRegards,\r\nShashwat Verma\r\n\r\n\r\nOn Thu, Mar 21, 2024 at 9:13 AM Alexey234432 ***@***.***>\r\nwrote:\r\n\r\n> any solutions to this?\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/haotian-liu/LLaVA/issues/1177#issuecomment-2012830519>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/ABZ6HFK7EAY4WQ32FAXW2ODYZMBKLAVCNFSM6AAAAABDZX7CMWVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMJSHAZTANJRHE>\r\n> .\r\n> You are receiving this because you commented.Message ID:\r\n> ***@***.***>\r\n>\n</Comment>\n<Comment by Alexey234432 at 2024-03-21T16:17:43Z>\ncould you elaborate more please, which instance have you used? is it in AWS? also what was the command?\r\n\r\nI am trying to run - _CUDA_VISIBLE_DEVICES=-1 python -m llava.serve.cli     --model-path liuhaotian/llava-v1.5-7b     --image-file \"https://llava-vl.github.io/static/images/view.jpg\" --device cpu --debug_\r\n\r\nThank you\n</Comment>\n<Comment by shashwat14 at 2024-03-21T23:09:15Z>\nProbably because you are doing CPU inference and fp16 is not allowed? \r\n\r\nhttps://stackoverflow.com/questions/73530569/pytorch-matmul-runtimeerror-addmm-impl-cpu-not-implemented-for-half\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1176,
    "state": "open",
    "created_by": "daijianxin",
    "created_at": "2024-02-26T07:42:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1176</URL>\n\n<TITLE>[Question] Failed to reproduce CC3M-595K</TITLE>\n\n<BODY>### Question\n\nWe extract noun-phrases using Spacy for each caption over the whole CC3M dataset, and count the frequency of each unique noun-phrase.  but   we  obtained  1700K image-text pairs.  (first step , we obtained 890K unique noun-phrase  data instead of 100K)\r\n（1）Did you perform any other filtering operations during processing?\r\n（2）Could you provide the data processing script for CC3M-595K ?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1175,
    "state": "open",
    "created_by": "lh-seu",
    "created_at": "2024-02-26T07:14:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1175</URL>\n\n<TITLE>[Question]  llava-1.6 在自有数据上输出不符合预期</TITLE>\n\n<BODY>### Question\r\n\r\n本地下载llava-v1.6-34b， 并在自有数据集上进行测试， 发现输出和预期不一致\r\n\r\n\r\n\r\n主要有以下几个问题：\r\n1. 输出重复， 同一句话循环输出多次\r\n2. 达非所问\r\n3. 对于一些问题不输出预测结果，不断回车\r\n输出可参考下图：\r\n\r\n<img width=\"876\" alt=\"企业微信截图_17089329946350\" src=\"https://github.com/haotian-liu/LLaVA/assets/39977871/238f8e3b-8d82-4192-a1f4-9b5bc972fb29\">\r\n\r\n\r\n![Peek 2024-02-26 15-41](https://github.com/haotian-liu/LLaVA/assets/39977871/7fafee65-ac7b-46a6-aeed-9ccf2ea2618c)\r\n\r\n\r\n想请问下出现这个问题的原因是啥， 还是我哪里使用错误了</BODY>\n\n<COMMENTS>\n<Comment by WLpub at 2024-02-28T07:39:07Z>\nSame issue, has it been resolved yet?\n</Comment>\n<Comment by lh-seu at 2024-02-28T08:48:13Z>\n> Same issue, has it been resolved yet?\r\n\r\nno\n</Comment>\n<Comment by life2048 at 2024-05-10T03:07:15Z>\nI have the same issue，how to resolve it?\n</Comment>\n<Comment by july-love at 2025-03-31T02:50:32Z>\n你好，现在你解决这个问题了吗\n</Comment>\n<Comment by july-love at 2025-04-02T09:12:47Z>\n> > 同样的问题，解决了吗？\n> \n> 不\n\n你好，这个问题你们解决了吗\n</Comment>\n<Comment by UniqueMike at 2025-04-07T06:21:15Z>\nHello, I have the same issue，how to resolve it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1174,
    "state": "open",
    "created_by": "zhly0",
    "created_at": "2024-02-25T09:08:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1174</URL>\n\n<TITLE>[Feature request] Training code for LLaVA-NeXT-34B</TITLE>\n\n<BODY>### feature\n\nHi，is the training code for LLaVA-NeXT-34B will release in February or March？</BODY>\n\n<COMMENTS>\n<Comment by awzhgw at 2024-02-26T03:31:02Z>\n+1\n</Comment>\n<Comment by Reichenbachian at 2024-02-27T01:15:50Z>\n+1\n</Comment>\n<Comment by loveunk at 2024-02-27T09:41:08Z>\nSame question\n</Comment>\n<Comment by awzhgw at 2024-02-27T13:13:55Z>\nSame question\n</Comment>\n<Comment by Kalmend at 2024-03-05T21:29:47Z>\n+1\n</Comment>\n<Comment by yushuinanrong at 2024-04-02T23:16:02Z>\n+1\n</Comment>\n<Comment by jasonaidm at 2024-04-20T00:55:29Z>\n+ Same question\n</Comment>\n<Comment by chricro at 2024-05-04T17:22:57Z>\nThank you for your efforts, @haotian-liu. Can you please share any progress updates regarding the pretraining code release for the 1.6 series? cheers\n</Comment>\n<Comment by hunterheiden at 2024-05-14T16:47:48Z>\nYes, it would be fantastic to hear if we can expect to see training code before the end of June or if we should expect it later on in the year! Very eager to build on top of this awesome work\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1173,
    "state": "closed",
    "created_by": "zengxingchen",
    "created_at": "2024-02-24T13:40:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1173</URL>\n\n<TITLE>[Usage] How to unfreeze the visual encoder when fine-tuning LLaVA?</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nHow to unfreeze the visual encoder when fine-tuning LLaVA?</BODY>\n\n<COMMENTS>\n<Comment by yushuinanrong at 2024-04-18T23:02:56Z>\nHave you resolved it?\n</Comment>\n<Comment by zengxingchen at 2024-04-20T05:46:36Z>\n> Have you resolved it?\r\n\r\nJust unfreeze the forward process, which is currently no_grad(). Also, set require_grad = True in some places.\n</Comment>\n<Comment by CR400AF-A at 2024-07-10T09:25:40Z>\n> > Have you resolved it?\r\n> \r\n> Just unfreeze the forward process, which is currently no_grad(). Also, set require_grad = True in some places.\r\n\r\nHi, I'm curious about performance trained with visual encoder unfreeze, but I don't have GPU resource now. Does it drop or rise?\n</Comment>\n<Comment by Road2Redemption at 2024-10-10T10:31:20Z>\nCan you provide where to add requires_grad=True?\n</Comment>\n<Comment by Purshow at 2024-11-07T10:21:20Z>\nHi~ I have the same question.Have you sovled it?\n</Comment>\n<Comment by muzairkhattak at 2024-11-11T00:41:13Z>\n@Purshow you can remove no_grad() from here: https://github.com/haotian-liu/LLaVA/blob/c121f0432da27facab705978f83c4ada465e46fd/llava/model/multimodal_encoder/clip_encoder.py#L45\r\n\r\nAdditionally, explicitly set the require grad to true for the vision parameters in trainer.py\r\n\r\nSomething like the below:\r\n`for name, p in model.get_model().vision_tower.named_parameters():\r\n    p.requires_grad = True`\n</Comment>\n<Comment by Purshow at 2024-11-12T09:29:04Z>\n> @Purshow you can remove no_grad() from here:\r\n> \r\n> https://github.com/haotian-liu/LLaVA/blob/c121f0432da27facab705978f83c4ada465e46fd/llava/model/multimodal_encoder/clip_encoder.py#L45\r\n> \r\n> Additionally, explicitly set the require grad to true for the vision parameters in trainer.py\r\n> \r\n> Something like the below: `for name, p in model.get_model().vision_tower.named_parameters(): p.requires_grad = True`\r\nThanks!\n</Comment>\n<Comment by ThisisBillhe at 2025-07-04T07:24:32Z>\nI am curious how many parameters are added to the optimizer? Since we only set requires_grad to the model and do not explicitly control the optimizer.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1172,
    "state": "open",
    "created_by": "awzhgw",
    "created_at": "2024-02-24T09:35:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1172</URL>\n\n<TITLE>[Usage]  finetune my data ，Why is the loss not decreasing?</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nwhen i finetune mydata with lora, the loss not decreasing ,why ,how to resolve it ?\r\n\r\ni use liuhaotian/llava-v1.6-34b this model for base model\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/47099843/9f34d06b-cc18-4cc6-8235-cdf6f5bbd9fd)</BODY>\n\n<COMMENTS>\n<Comment by yuan10li20221130 at 2024-02-26T01:03:05Z>\nthe training code for llava-v1.6-34b seems not open source yet,did you write the training code based on llava-v1.5?\n</Comment>\n<Comment by awzhgw at 2024-02-26T01:54:58Z>\nany help ?\n</Comment>\n<Comment by yuan10li20221130 at 2024-02-26T01:59:13Z>\nHi,maybe your learning rate is too large，if I were training your model,I would try constant learning rate 2e-5.\r\nBy the way,\r\nIs your training code write by yourself or use llava-1.5 to train llava-v1.6-34b?\n</Comment>\n<Comment by awzhgw at 2024-02-26T03:21:07Z>\n> Hi,maybe your learning rate is too large，if I were training your model,I would try constant learning rate 2e-5. By the way, Is your training code write by yourself or use llava-1.5 to train llava-v1.6-34b?\r\n\r\ni use llava-1.5 to finetune my data. \r\nlearing rate is 2e-5 \r\n\r\nhow to resolve it ?\n</Comment>\n<Comment by awzhgw at 2024-02-26T03:27:43Z>\n> Hi,maybe your learning rate is too large，if I were training your model,I would try constant learning rate 2e-5. By the way, Is your training code write by yourself or use llava-1.5 to train llava-v1.6-34b?\r\n\r\nmy start shell is :\r\n```shell\r\nnohup deepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path ${CheckPoint_ROOT}/${ModelName} \\\r\n    --version v1 \\\r\n    --data_path ${JSON_PATH} \\\r\n    --image_folder ${IMAGE_FOLDER} \\\r\n    --vision_tower ${VISION_TOWER} \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ${CheckPoint_ROOT}/${ModelName}-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 2500 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 1 \\\r\n    --lazy_preprocess True \\\r\n    --report_to tensorboard  > ${Local_DATA_ROOT}/llava_tune.log 2>&1 &\r\n```\n</Comment>\n<Comment by awzhgw at 2024-02-29T14:15:02Z>\nany help？？\n</Comment>\n<Comment by tangjunjun966 at 2024-03-05T02:12:47Z>\nI met, how to solve\n</Comment>\n<Comment by OliverLeeXZ at 2024-04-29T05:12:27Z>\nSame question! I also find that on finetine_lora.sh script provided by haotianliu learning rate is 2e-4, so we should change it to 2e-5?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1171,
    "state": "closed",
    "created_by": "Rishbhu",
    "created_at": "2024-02-24T05:27:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1171</URL>\n\n<TITLE>Image error</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nWhenever I ask it to analyze an image, the image doesnt get run through fully and it just says 'undefined'\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.\r\n![Screenshot 2024-02-23 232553](https://github.com/haotian-liu/LLaVA/assets/99981600/17162cbb-3748-4194-b848-d83c61a00330)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2024-02-24T05:28:20Z>\nIt's been fixed now. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1170,
    "state": "open",
    "created_by": "20191864218",
    "created_at": "2024-02-23T16:48:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1170</URL>\n\n<TITLE>[Question] AttributeError: object has no attribute 'embed_tokens'</TITLE>\n\n<BODY>### Question\r\n\r\n各位大佬，我把llava的LLM换成由Qwen，重新预训练的时候出现这个错误，请问怎么解决</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1169,
    "state": "open",
    "created_by": "yinincanada",
    "created_at": "2024-02-22T16:59:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1169</URL>\n\n<TITLE>[Question] inference after first-stage pretraining</TITLE>\n\n<BODY>### Question\r\n\r\nusing script scripts/v1_5/pretrain.sh, I conducted llava pretraining using some task-specific image-text pair, now I am thinking of testing the pretrained model, and wondering how can I do that?\r\n\r\nthe pretraining only output the modality aligner (mm_projector.bin) part and I do not see any script which load in base llm vicuna, visual tower, and pretrained mm_projector.bin and do inference. anything to share for this if it is available?\r\n\r\nanother direction could be, is there a configuration that I can save the whole model after pretrain (like is done after second-stage instructional finetune), with which I can simply leverage the available inference script llava/eval/model_vqa.py?\r\n\r\nthanks!</BODY>\n\n<COMMENTS>\n<Comment by yinincanada at 2024-02-23T14:18:15Z>\nAppreciate if anyone give some advice. I have been trying to change the code a bit attempting to save the whole model, not working so far.\n</Comment>\n<Comment by Shengcao-Cao at 2024-02-23T23:07:44Z>\nHi @yinincanada ,\r\n\r\nI happen to have the same question as yours. I find that `load_pretrained_model` from `llava/model` is able to load the model with a pretrained `mm_projector.bin` (in my case, it is in folder `./save/llava-v1.5-7b-pretrain`). Then I use the simple CLI script to test my pretrained project (along with frozen LLM + Vision Encoder) like this:\r\n\r\n```\r\npython -m llava.serve.cli \\\r\n    --model-base lmsys/vicuna-7b-v1.5 \\\r\n    --model-path ./save/llava-v1.5-7b-pretrain \\\r\n    --image-file ./save/sample/cat-and-dog.jpg \\\r\n    --temperature 0.0\r\n```\r\n\r\nHope this helps!\n</Comment>\n<Comment by yinincanada at 2024-02-23T23:17:48Z>\n@Shengcao-Cao thanks! but I do not see, from what you said, how the vision tower is loaded in for model inference?  my understanding is, we need base vicuna, mm_project.bin, and also vision encoder (clip-vit)\n</Comment>\n<Comment by Shengcao-Cao at 2024-02-23T23:34:00Z>\nYou may check the `load_pretrained_model` function and the vision tower is loaded at the end. The config comes from your pretrain save directory.\n</Comment>\n<Comment by Shengcao-Cao at 2024-02-23T23:40:35Z>\nBTW I forgot to mention, I modified `llava/model/builder.py` a bit due to a bug reported here: https://github.com/haotian-liu/LLaVA/issues/1075.\r\n\r\nMy updated code between Line 96 and Line 100:\r\n```\r\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n                from llava.model.language_model.llava_llama import LlavaConfig\r\n                cfg_pretrained = LlavaConfig.from_pretrained(model_path)\r\n                model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs)\r\n```\r\n\r\nIn my case, `cfg_pretrained` cannot be correctly loaded from `model_path` if it is `AutoConfig`, so I follow https://github.com/haotian-liu/LLaVA/commit/04fb03d4943ed212c24381e44c325525e700884d and load the config with `LlavaConfig` as well.\n</Comment>\n<Comment by yinincanada at 2024-02-26T16:50:27Z>\nThanks! @Shengcao-Cao\n</Comment>\n<Comment by conheaven at 2024-03-03T15:39:18Z>\n> Hi @yinincanada , 你好,\r\n> \r\n> I happen to have the same question as yours. I find that `load_pretrained_model` from `llava/model` is able to load the model with a pretrained `mm_projector.bin` (in my case, it is in folder `./save/llava-v1.5-7b-pretrain`). Then I use the simple CLI script to test my pretrained project (along with frozen LLM + Vision Encoder) like this:我碰巧也有和你一样的问题。我发现from能够用预训练的模型加载(在我的情况下，它在文件夹中)。然后，我使用简单的CLI脚本来测试我的预训练项目(连同冻结LLM视觉编码器)，如下所示:\r\n> \r\n> ```\r\n> python -m llava.serve.cli \\\r\n>     --model-base lmsys/vicuna-7b-v1.5 \\\r\n>     --model-path ./save/llava-v1.5-7b-pretrain \\\r\n>     --image-file ./save/sample/cat-and-dog.jpg \\\r\n>     --temperature 0.0\r\n> ```\r\n> \r\n> Hope this helps! 希望这对你有帮助!\r\n\r\nafter finetune with finetune_lora.sh, I merge the model with 'python scripts/merge_lora_weights.py\r\n--model-path /data1/khw/output_llava/finetune/llava-v1.5-13b-lora\r\n--model-base /data1/khw/llava\r\n--save-model-path /data1/khw/output_llava/merge_model-lora'\r\nthen i run\r\n'python -m llava.serve.cli\r\n--model-base /data1/khw/llava\r\n--model-path /data1/khw/output_llava/merge_model-lora\r\n--image-file /data1/khw/img/1.jpg '\r\nit shows\r\n\"Traceback (most recent call last):\r\nFile \"/home/khw/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\nreturn _run_code(code, main_globals, None,\r\nFile \"/home/khw/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\nexec(code, run_globals)\r\nFile \"/home/khw/llava/LLaVA/llava/serve/cli.py\", line 126, in\r\nmain(args)\r\nFile \"/home/khw/llava/LLaVA/llava/serve/cli.py\", line 32, in main\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\nFile \"/home/khw/llava/LLaVA/llava/model/builder.py\", line 122, in load_pretrained_model\r\nmodel = AutoModelForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, **kwargs)\r\nFile \"/home/khw/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\r\nraise ValueError(\r\nValueError: Unrecognized configuration class <class 'transformers.models.llava.configuration_llava.LlavaConfig'> for this kind of AutoModel: AutoModelForCausalLM.\r\nModel type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, ElectraConfig, ErnieConfig, FalconConfig, FuyuConfig, GemmaConfig, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, LlamaConfig, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MptConfig, MusicgenConfig, MvpConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, LlavaConfig, LlavaMptConfig, LlavaMistralConfig.\"\r\nthank you in advance\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1168,
    "state": "closed",
    "created_by": "fbrand-new",
    "created_at": "2024-02-22T14:19:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1168</URL>\n\n<TITLE>Demo is down</TITLE>\n\n<BODY>Hello, it seems that the demo still has some problems:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/17751421/5290f1b2-271e-44cc-a909-747e676037b9)\r\nI tried uploading an image but it stops at \"undefined\" every time. Reproduced both in Firefox and Chrome.\r\nI can still ask the question but it complains \r\n![image](https://github.com/haotian-liu/LLaVA/assets/17751421/7b21b981-0547-4a9b-8233-a30677dccad9)\r\n\r\n_Originally posted by @fbrand-new in https://github.com/haotian-liu/LLaVA/issues/1151#issuecomment-1959546940_</BODY>\n\n<COMMENTS>\n<Comment by alarmgoose at 2024-02-23T05:46:36Z>\nYup. Would be nice if I could run this exact version in colab.\n</Comment>\n<Comment by haotian-liu at 2024-02-24T05:28:47Z>\nThe demo is fixed now. Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1167,
    "state": "open",
    "created_by": "20191864218",
    "created_at": "2024-02-22T09:38:09Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1167</URL>\n\n<TITLE>TypeError: pad_sequence(): argument 'padding_value' (position 3) must be float, not NoneType</TITLE>\n\n<BODY>我使用llava的框架，把LLM替换成其他的大语言模型，运行过程中报错TypeError: pad_sequence(): argument 'padding_value' (position 3) must be float, not NoneType具体错误如下\r\n![image](https://github.com/haotian-liu/LLaVA/assets/56297762/c2f1efa3-b0a2-4302-aa0b-c95aef8515f0)\r\n\r\n\r\n请问各位大佬具体是哪里的问题</BODY>\n\n<COMMENTS>\n<Comment by mohit-217 at 2024-02-22T11:10:43Z>\nI am also getting the same error which base model you are using?\n</Comment>\n<Comment by 20191864218 at 2024-02-22T15:06:49Z>\n> I am also getting the same error which base model you are using?\r\n\r\nI am using Qwen-7B，Could you please help me with a solution?\n</Comment>\n<Comment by jyC23333 at 2024-02-23T03:30:48Z>\nYou can check whether the pad_token is undefined in your tokenizer.\r\n\r\nWhen I adapt Qwen to llava, the pad_token is None in QwenTokenizer, which caused the same error.\n</Comment>\n<Comment by 20191864218 at 2024-02-23T03:47:54Z>\n> You can check whether the pad_token is undefined in your tokenizer.\r\n> \r\n> When I adapt Qwen to llava, the pad_token is None in QwenTokenizer, which caused the same error.\r\n\r\nHow can I check? If it turns out as you mentioned, how can I solve it? Thank you for your response.\n</Comment>\n<Comment by mohit-217 at 2024-02-23T03:59:18Z>\nI am also using Qwen-7b actually for llama unk_token is <unk> but qwen it is null. SO it is causing error. Pad sequence error.\r\nunk_token == null  for Qwen\r\nunk_token\"<unk>\"\r\n\r\nIf you want to validate it is available in tokenizer_config.json\r\nAfter modifying training initiated but loss is zero.\r\n![W B Chart 2_23_2024, 9_28_14 AM](https://github.com/haotian-liu/LLaVA/assets/51528367/f722de35-789f-465b-8565-642c40bb2b8a)\r\n\r\nAnyone help me to resolve the issue.\n</Comment>\n<Comment by mohit-217 at 2024-02-23T04:00:53Z>\n@haotian-liu  Could you please give your insights how we can train using qwen7b base model and solve the issue?\n</Comment>\n<Comment by jyC23333 at 2024-02-23T05:37:18Z>\nThe pad_token is used to make attention mask, just set the pad_token_id as -100 directly in the training code will solve it.\n</Comment>\n<Comment by mohit-217 at 2024-02-23T06:45:04Z>\nHave you trained model by assigning pad_token_id  to -100, could you please share config if feasible otherwise make comment over accuracy for the model you have trained.\n</Comment>\n<Comment by TuuSiwei at 2024-05-06T11:04:47Z>\nI also try to use llama3-8b as llm, and i add the code \"tokenizer.pad_token_id=-100\", after that when i start pretrain, i get the error: OverflowError: out of range integral type conversion attempted\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/67264710/95ddcd3d-b7d2-49a7-abeb-970b2e581439)\n</Comment>\n<Comment by michaelsaxon at 2024-08-15T19:29:18Z>\nHere's the fix, if you're not using the LLaVA-NeXT codebase:\r\n\r\nAdd the following to line 909 in `train.py` (https://github.com/haotian-liu/LLaVA/blob/main/llava/train/train.py#L909)\r\n\r\n```python3\r\n    if \"llama-3\" in model_args.model_name_or_path.lower():\r\n        # hack for the LLaMA 3 using Qwen tokenizer, by default Qwen doesn't have a pad token but we can assign it EOT as pad. \r\n        # more information: https://github.com/QwenLM/Qwen/blob/main/tokenization_note.md\r\n        logging.warn(f\"Model name: {model_args.model_name_or_path.lower()}\")\r\n        logging.warn(\"Making the new pad token for the tokenizer\")\r\n        # this is the hack you need because later in the logic, it adds the unk token\r\n        tokenizer.pad_token='<|eot_id|>'\r\n        tokenizer.unk_token='<|eot_id|>'\r\n        logging.warn(f\"Tokenizer pad token! {tokenizer.pad_token}\")\r\n```\r\n\r\nThis follows from the suggestion for Qwen adding pad tokens (llama 3 is based on qwen)\n</Comment>\n<Comment by GETUNTUN at 2024-11-26T13:14:20Z>\n在train.py中当--version plain时会强制改变tokenizer.pad_token为tokenizer.unk_token，跟着tokenizer.pad_token_id索引不到对应的值就变为了None，所以报错了，我这么理解的，这里我设置--version v0会保留LLM原来的tokenizer.pad_token，就不报错了\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1166,
    "state": "closed",
    "created_by": "awzhgw",
    "created_at": "2024-02-22T08:57:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1166</URL>\n\n<TITLE>[Usage] loss is 0.0 and tokenization mismatch: 102 vs. 104. (ignored)  on liuhaotian/llava-v1.6-34b</TITLE>\n\n<BODY></BODY>\n\n<COMMENTS>\n<Comment by Leon1207 at 2024-02-24T05:52:45Z>\nI have the same problem, could you please share me how to fix it?\n</Comment>\n<Comment by nixsui at 2024-03-05T06:33:05Z>\nsame problem\n</Comment>\n<Comment by lukashelff at 2024-04-09T10:00:42Z>\ni am facing the same problem\n</Comment>\n<Comment by prashanthsadasivan at 2024-04-17T13:23:32Z>\nI ran into this yesterday. managed to fix it with advice from this  issue https://github.com/haotian-liu/LLaVA/issues/661#issuecomment-1778630921 . depending on if you're finetuning different models you'll need to adjust the calculations there. for me, I was using the `</s>` separator with tiny llama, so i needed to start `cur_len = 1` and make `cur_len += round_len + 1` to account for the separator tokenization in process_mpt\n</Comment>\n<Comment by ticketlearn at 2024-07-11T11:02:55Z>\nface the same problem\n</Comment>\n<Comment by Bleking at 2024-09-30T07:48:37Z>\nPlus, make sure to set the version value as 'mpt' due to the separation token.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1165,
    "state": "open",
    "created_by": "Fire-Liu001",
    "created_at": "2024-02-22T05:23:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1165</URL>\n\n<TITLE>[Question] No matching distribution found for flash-attn</TITLE>\n\n<BODY>### Question\n\nI have a problem with the commond \"pip install flash-attn --no-build-isolation\":\r\nFailed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/flash-attn/\r\nERROR: Could not find a version that satisfies the requirement flash-attn (from versions: none)\r\nERROR: No matching distribution found for flash-attn.\r\nCan you give me some advice？</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1164,
    "state": "open",
    "created_by": "jiadingfang",
    "created_at": "2024-02-22T03:29:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1164</URL>\n\n<TITLE>[Usage] Missing \"trainer_state.json\" when resuming training from saved checkpoints</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: I'm trying to reproduce the training results. As my cluster has 4-hour usage limit per use, I need to save checkpoints and resume training from them. However, resume training failed because of this error.\r\n\r\nCommand:\r\n```\r\n./scripts/v1_5/pretrain.sh\r\n```\r\nwhere I change the save_steps to be 500.\r\n\r\nLog: \r\n```\r\n[Errno 2] No such file or directory: './checkpoints/llava-v1.5-7b-pretrain/checkpoint-1500/trainer_state.json'\r\n```\r\n\r\nScreenshots:\r\nI have \"checkpoint-1500\" folder, but it does not contain such \"trainer_state.json\" file. \r\n![image](https://github.com/haotian-liu/LLaVA/assets/16885585/19f7b641-0e33-4089-86dd-188c87e6049f)</BODY>\n\n<COMMENTS>\n<Comment by sahilqure at 2024-02-27T15:26:29Z>\n@jiadingfang Were you able to solve this issue?\n</Comment>\n<Comment by sahilqure at 2024-02-27T16:05:39Z>\n@haotian-liu Can you please check this issue. I think many people have missed this because nobody is doing the pre-training. Is this the problem with transformers or deepspeed version?\n</Comment>\n<Comment by baochi0212 at 2024-03-08T06:36:17Z>\nadd `self.state.save_to_json(os.path.join(output_dir, TRAINER_STATE_NAME))` in llava/train/llava_trainer.py in _save_checkpoint method\n</Comment>\n<Comment by JiangLinsheng at 2024-03-13T06:58:36Z>\nI also encountered the same problem.Is there any solution?\n</Comment>\n<Comment by git-siddhesh at 2024-03-23T09:01:17Z>\nsame issue, using huggingface trainer, saving the model, and facing the same issue when loading the model from the saved directory.\n</Comment>\n<Comment by ashmalvayani at 2024-05-21T20:47:47Z>\n> self.state.save_to_json(os.path.join(output_dir, TRAINER_STATE_NAME))\r\n\r\n> raise ValueError(f\"Can't find a valid checkpoint at {checkpoint_path}\")\r\n\r\nThis will not work because it also needs the .pth files and optimizer files without which it won't work.\n</Comment>\n<Comment by ashmalvayani at 2024-05-22T10:17:03Z>\nEven if you add trainer_state.json file, it will not resume as it will ask for optimizer files and .pth files which still won't be saved. I think the best way is to comment out their function and simply keep their \"super(LlaVaTrainer, self) ... \" line and let the code run. I have tested this, it does not save the mm_projector.bin file at each stage but it does save the entire weights at each checkpoint.\r\n\r\nYou can either manually extract the mm_projector weights later. If you don't want to do this, don't worry, at the end of training it automatically saves the trainer_state.json, mm_projector.bin and config.json file after the completion of last step.\n</Comment>\n<Comment by lucasjinreal at 2024-06-20T02:31:51Z>\nJust add this single line to the _save_checkpoint function:\r\n\r\n# save all for mm adaptor resume\r\n            self.save_model(output_dir, _internal_call=True)\r\n\r\nThen you can either resume, or using the mmprojector.bin, keep everything else untouched.\n</Comment>\n<Comment by junha1125 at 2024-06-20T14:59:54Z>\n#### Try1 following [issuecomment-2179701269](https://github.com/haotian-liu/LLaVA/issues/1164#issuecomment-2179701269)\r\n1. add this single line `self.save_model(output_dir, _internal_call=True)` under the line, `torch.save(weight_to_save, os.path.join(output_dir, f'mm_projector.bin'))`. \r\n2. delete `def _save(self` in `llava_trainer.py`.\r\n3. It does save the entire weights. But, it dose not save `deepspeed_checkpoint_dirs = sorted(glob.glob(f\"{checkpoint_path}/global_step*\"))`. So I got the error.\r\n\r\n#### Try2 following [issuecomment-2124420477](https://github.com/haotian-liu/LLaVA/issues/1164#issuecomment-2124420477)\r\n1. delete `def _save_checkpoint(self`in `llava_trainer.py`.\r\n2. It also does save the entire weights and the `global_step1` directory.\r\n3. When re-run `train.py`, resume the model weight and the deepspeed-files without errors. 🥳\r\n\r\nFor me, recommend Try2.\n</Comment>\n<Comment by lxysl at 2024-06-20T17:29:35Z>\nThe entire model weights are saved in this way in `safe_save_model_for_hf_trainer()`:\r\n```python\r\n    ...\r\n    if trainer.deepspeed:\r\n        torch.cuda.synchronize()\r\n        trainer.save_model(output_dir)\r\n        return\r\n\r\n    state_dict = trainer.model.state_dict()\r\n    if trainer.args.should_save:\r\n        cpu_state_dict = {\r\n            key: value.cpu()\r\n            for key, value in state_dict.items()\r\n        }\r\n        del state_dict\r\n        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\r\n```\r\nand before this function is called, trainer state has been saved:\r\n```python\r\ntrainer.save_state()\r\n```\r\nSo I wonder whether `torch.cuda.synchronize()` and `trainer.save_state()` should be done when using `save_steps` in hugging face `Trainer` with deepspeed?\r\nWhat's the difference between `trainer.save_model()` and `trainer._save`?\n</Comment>\n<Comment by lxysl at 2024-06-20T17:36:06Z>\nAnd I also wonder why model weights are not checkpointed every save_step? Isn't the default save_step in hugging face Trainer equal to 500? Please @ me if there's any progress.\n</Comment>\n<Comment by lucasjinreal at 2024-06-21T02:53:00Z>\nOh... Ignore all the above noisy comments, just a single line solve all issues.\n</Comment>\n<Comment by lxysl at 2024-06-21T09:16:21Z>\n> Oh... Ignore all the above noisy comments, just a single line solve all issues.\r\n\r\nI will try it later! :)\n</Comment>\n<Comment by StephenQSstarThomas at 2024-06-30T00:08:53Z>\n> Just add this single line to the _save_checkpoint function:\r\n> \r\n> # save all for mm adaptor resume\r\n> ```\r\n>         self.save_model(output_dir, _internal_call=True)\r\n> ```\r\n> \r\n> Then you can either resume, or using the mmprojector.bin, keep everything else untouched.\r\n\r\nbetween which lines should this line be added?\n</Comment>\n<Comment by wanlipeng at 2024-08-22T06:35:52Z>\n> Just add this single line to the _save_checkpoint function:\r\n> \r\n> # save all for mm adaptor resume\r\n> ```\r\n>         self.save_model(output_dir, _internal_call=True)\r\n> ```\r\n> \r\n> Then you can either resume, or using the mmprojector.bin, keep everything else untouched.\r\n\r\n\r\n\r\n> Just add this single line to the _save_checkpoint function:\r\n> \r\n> # save all for mm adaptor resume\r\n> ```\r\n>         self.save_model(output_dir, _internal_call=True)\r\n> ```\r\n> \r\n> Then you can either resume, or using the mmprojector.bin, keep everything else untouched.\r\n\r\nAfter I added this code, there are still no other files in the directory where the training is saved. What's going on?\n</Comment>\n<Comment by wanlipeng at 2024-08-22T06:49:50Z>\n> Even if you add trainer_state.json file, it will not resume as it will ask for optimizer files and .pth files which still won't be saved. I think the best way is to comment out their function and simply keep their \"super(LlaVaTrainer, self) ... \" line and let the code run. I have tested this, it does not save the mm_projector.bin file at each stage but it does save the entire weights at each checkpoint.\r\n> \r\n> You can either manually extract the mm_projector weights later. If you don't want to do this, don't worry, at the end of training it automatically saves the trainer_state.json, mm_projector.bin and config.json file after the completion of last step.\r\n\r\n@ashmalvayani  May I ask which functions should be commented out as mentioned above, or should all functions of this class LLaVATrainer be commented out?\n</Comment>\n<Comment by ShawnKing98 at 2024-08-25T01:40:07Z>\n> Oh... Ignore all the above noisy comments, just a single line solve all issues.\r\n\r\nIt doesn't seem so. I test it out, `self.save_model(output_dir, _internal_call=True)`  will call `self._save()`, which is overloaded by `LLaVATrainer` and would do nothing during pretraining.\r\n\r\nThe correct solution has been mentioned by several other users, which is to let `LLaVATrainer` directly call its parent class's `_save_checkpoint` method, instead of the one overloaded by `LLaVATrainer`. One way of doing this is taking out the `super(LLaVATrainer, self)._save_checkpoint(model, trial, metrics)` line under the `else` part in `llava_trainer.py` and put it right before the `if` block, this way both the trainer state and `mm_projector.bin` can be saved. Just tested it out and it worked.\n</Comment>\n<Comment by mohammadakhoundi1 at 2025-03-29T21:48:57Z>\nwell the solution that worked for me is that if your max_step is 10 so th e10th step is the final state and does not have this file try loading a version before for example v 9 or v8 \n\n![Image](https://github.com/user-attachments/assets/539138d9-46e8-46be-a252-1866296654f5)\n\n![Image](https://github.com/user-attachments/assets/005edb8a-1aa3-4c64-b624-f3755dd5af80)\n</Comment>\n<Comment by Basma2423 at 2025-04-26T20:55:26Z>\n**_This worked for me using the following approach:_**\n\n\n\n```\nclass MyTrainer(Seq2SeqTrainer):  \n    def _save_checkpoint(self, model, trial):  \n        super()._save_checkpoint(model, trial)  # Default saving  \n        trainer_state_path = os.path.join(self.args.output_dir, 'trainer_state.json')  \n        self.state.save_to_json(trainer_state_path)  \n\ntrainer = MyTrainer(...)\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1162,
    "state": "open",
    "created_by": "g-h-chen",
    "created_at": "2024-02-21T16:24:30Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1162</URL>\n\n<TITLE>[Discussion] Check out the 1M high-quality data!</TITLE>\n\n<BODY>### Discussion\n\nHi Haotian,\r\n\r\nWe recently released [ALLaVA-4V](https://huggingface.co/datasets/FreedomIntelligence/ALLaVA-4V), a large dataset with fine-grained caption and complex reasoning QA pairs. Inclusion of our data can significantly boost model performance on reasoning tasks. We are looking forward to the results of adding ALLaVA-4V when training your 34B model!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1161,
    "state": "closed",
    "created_by": "Winter-Dry",
    "created_at": "2024-02-21T14:22:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1161</URL>\n\n<TITLE>[Usage] Why the weight of mm_projector.Liner always in the shape of (4096， 1024)</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nI uses other visual encoder to train the llava v1.5 7b, the saved checkpoint's \"mm_hidden_size\" is 1664， when I evaluate the model, an error occurred:\r\n```\r\nRuntimeError: Error(s) in loading state_dict for LlavaLlamaForCausalLM:\r\n\tsize mismatch for model.mm_projector.0.weight: copying a param with shape torch.Size([4096, 1664]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\r\n```\r\nwhen I checked the parameters in model.mm_projector.0.weight, I wonder why the in_features is 1664, however the weight is 1024?\r\n\r\nScreenshots:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/65168344/6eb0ebaf-cbd3-4015-9989-430b69e33558)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1158,
    "state": "open",
    "created_by": "yjhdhr",
    "created_at": "2024-02-21T03:26:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1158</URL>\n\n<TITLE>[Question] Do we need to modify the grounding (bbox) in the data to adapt to \"anyres\"?</TITLE>\n\n<BODY>### Question\n\nIs the training data of SFT in llava-1.5 also used for llava-next? Do we need to modify the grounding (bbox) in the data to adapt to \"anyres\"?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1156,
    "state": "open",
    "created_by": "RonanKMcGovern",
    "created_at": "2024-02-20T17:58:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1156</URL>\n\n<TITLE>Does the LLM or vision encoder limit performance?</TITLE>\n\n<BODY>### Question\n\nFor Llava 1.6, in the vision transformer or the LLM or the combination limiting performance?\r\n\r\nhard for me to tell as I can’t find eval info on leave 1.6 7B vs 34b.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1155,
    "state": "open",
    "created_by": "YangQiuEric",
    "created_at": "2024-02-20T10:50:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1155</URL>\n\n<TITLE>Difference between pretrain and finetune</TITLE>\n\n<BODY>### Question\n\nCan we use the same dataset for fine-tune and pretrain? For example, use blip_laion_cc_sbu_558k.json?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1154,
    "state": "open",
    "created_by": "annopackage",
    "created_at": "2024-02-20T06:33:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1154</URL>\n\n<TITLE>Extension of larger model_max_length</TITLE>\n\n<BODY>Hi, haotian, thanks for your great work.\r\nCurrently, I want to increase the model max length for larger image size. If we change the parameter, do we need to change '--model_max_length' in the config of vicuna-13b at the same time?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1153,
    "state": "open",
    "created_by": "lucasjinreal",
    "created_at": "2024-02-20T04:55:10Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1153</URL>\n\n<TITLE>[Question] Training with Qwen2 backend got loss 0</TITLE>\n\n<BODY>### Question\n\nI got loss to be 0 when training on Qwen2 backend,\r\n\r\n{'loss': 0.0, 'learning_rate': 0.00015267175572519084, 'epoch': 0.0}                                                                                                            \r\n  0%|▎                                                                                                                                     | 20/8720 [01:38<11:01:39,  4.56s/it]WARNING: tokenization mismatch: 47 vs. 48. (ignored)\r\nWARNING: tokenization mismatch: 54 vs. 55. (ignored)\r\nWARNING: tokenization mismatch: 46 vs. 47. (ignored)\r\nWARNING: tokenization mismatch: 43 vs. 44. (ignored)\r\n\r\nWhat could be the reason caused it?</BODY>\n\n<COMMENTS>\n<Comment by yiyexy at 2024-02-21T02:30:07Z>\nme too\n</Comment>\n<Comment by yiyexy at 2024-02-21T10:11:29Z>\nI found that the reason for this problem is different tokenizer rules.\r\nThe `bos_token` is null and the `eos_token` is set to \"<|endoftext|>\" in the Qwen tokenizer configuration.\r\nSo I added the Qwen tokenizer rule in `/mnt2/yinxie/code/LLaVA/llava/conversation.py` as follows:\r\n```\r\nclass SeparatorStyle(Enum):\r\n    \"\"\"Different separator style.\"\"\"\r\n    SINGLE = auto()\r\n    TWO = auto()\r\n    MPT = auto()\r\n    PLAIN = auto()\r\n    LLAMA_2 = auto()\r\n    QWEN_2 = auto()\r\ndef get_prompt(self):\r\n   elif self.sep_style == SeparatorStyle.QWEN_2:\r\n            seps = [self.sep, self.sep2]\r\n            ret = self.system + seps[0]\r\n            for i, (role, message) in enumerate(messages):\r\n                if message:\r\n                    if type(message) is tuple:\r\n                        message, _, _ = message\r\n                    ret += role + \": \" + message + seps[i % 2]\r\n                else:\r\n                    ret += role + \":\"\r\n\r\nconv_qwen_2 = Conversation(\r\n    system=\"A chat between a curious user and an artificial intelligence assistant. \"\r\n    \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\r\n    roles=(\"USER\", \"ASSISTANT\"),\r\n    version=\"qwen_v2\",\r\n    messages=(),\r\n    offset=0,\r\n    sep_style=SeparatorStyle.QWEN_2,\r\n    sep=\" \",\r\n    sep2=\"<|endoftext|>\",\r\n)\r\nconv_templates = {\r\n    \"default\": conv_vicuna_v0,\r\n    \"v0\": conv_vicuna_v0,\r\n    \"v1\": conv_vicuna_v1,\r\n    \"vicuna_v1\": conv_vicuna_v1,\r\n    \"qwen_2\": conv_qwen_2,\r\n    \"llama_2\": conv_llama_2,\r\n    \"mistral_instruct\": conv_mistral_instruct,\r\n    \"chatml_direct\": conv_chatml_direct,\r\n    \"mistral_direct\": conv_chatml_direct,\r\n\r\n    \"plain\": conv_llava_plain,\r\n    \"v0_plain\": conv_llava_plain,\r\n    \"llava_v0\": conv_llava_v0,\r\n    \"v0_mmtag\": conv_llava_v0_mmtag,\r\n    \"llava_v1\": conv_llava_v1,\r\n    \"v1_mmtag\": conv_llava_v1_mmtag,\r\n    \"llava_llama_2\": conv_llava_llama_2,\r\n\r\n    \"mpt\": conv_mpt,\r\n}\r\n\r\n```\r\nAnd then, I added the method `preprocess_qwen_2` in `train.py`.\r\n```\r\ndef preprocess_qwen_2(\r\n    sources,\r\n    tokenizer: transformers.PreTrainedTokenizer,\r\n    has_image: bool = False\r\n) -> Dict:\r\n    conv = conversation_lib.default_conversation.copy()\r\n    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\r\n\r\n    # Apply prompt templates\r\n    conversations = []\r\n    for i, source in enumerate(sources):\r\n        if roles[source[0][\"from\"]] != conv.roles[0]:\r\n            # Skip the first one if it is not from human\r\n            source = source[1:]\r\n\r\n        conv.messages = []\r\n        for j, sentence in enumerate(source):\r\n            role = roles[sentence[\"from\"]]\r\n            assert role == conv.roles[j % 2], f\"{i}\"\r\n            conv.append_message(role, sentence[\"value\"])\r\n        conversations.append(conv.get_prompt())\r\n\r\n    # Tokenize conversations\r\n\r\n    if has_image:\r\n        input_ids = torch.stack([tokenizer_image_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations], dim=0)\r\n    else:\r\n        input_ids = tokenizer(\r\n            conversations,\r\n            return_tensors=\"pt\",\r\n            padding=\"longest\",\r\n            max_length=tokenizer.model_max_length,\r\n            truncation=True,\r\n        ).input_ids\r\n\r\n    targets = input_ids.clone()\r\n\r\n    assert conv.sep_style == conversation_lib.SeparatorStyle.QWEN_2\r\n\r\n    # Mask targets\r\n    sep = conv.sep + conv.roles[1] + \": \"\r\n    for conversation, target in zip(conversations, targets):\r\n        total_len = int(target.ne(tokenizer.pad_token_id).sum())\r\n\r\n        rounds = conversation.split(conv.sep2)\r\n        rounds_len = len(rounds)\r\n        cur_len = 0\r\n        # target[:cur_len] = IGNORE_INDEX\r\n        for i, rou in enumerate(rounds):\r\n            if rou == \"\":\r\n                break\r\n\r\n            parts = rou.split(sep)\r\n            if len(parts) != 2:\r\n                break\r\n            parts[0] += sep\r\n\r\n            if has_image:\r\n                round_ids = tokenizer_image_token(rou, tokenizer)\r\n                instruction_ids = tokenizer_image_token(parts[0], tokenizer)\r\n                equal_parts = [x == y for x, y in zip(round_ids, instruction_ids)]\r\n\r\n                instruction_len = equal_parts.index(False) if False in equal_parts else len(equal_parts)\r\n                round_len = len(round_ids)\r\n\r\n            else:\r\n                round_ids = tokenizer(rou).input_ids\r\n                instruction_ids = tokenizer(parts[0]).input_ids\r\n                equal_parts = [x == y for x, y in zip(round_ids, instruction_ids)]\r\n            \r\n                instruction_len = equal_parts.index(False) if False in equal_parts else len(equal_parts)\r\n                round_len = len(round_ids)\r\n\r\n            if i != 0 and not tokenizer.legacy and IS_TOKENIZER_GREATER_THAN_0_14:\r\n                round_len += 1\r\n                instruction_len += 1\r\n\r\n            target[cur_len : cur_len + instruction_len] = IGNORE_INDEX\r\n\r\n            cur_len += round_len\r\n        target[cur_len:] = IGNORE_INDEX\r\n\r\n        if cur_len < tokenizer.model_max_length:\r\n            if cur_len != total_len + rounds_len - 2:\r\n                target[:] = IGNORE_INDEX\r\n                print(\r\n                    f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\r\n                    f\" (ignored)\"\r\n                )\r\n\r\n    return dict(\r\n        input_ids=input_ids,\r\n        labels=targets,\r\n    )\r\n\r\ndef preprocess(\r\n    sources: Sequence[str],\r\n    tokenizer: transformers.PreTrainedTokenizer,\r\n    has_image: bool = False\r\n) -> Dict:\r\n    if conversation_lib.default_conversation.sep_style == conversation_lib.SeparatorStyle.PLAIN:\r\n        return preprocess_plain(sources, tokenizer)\r\n    if conversation_lib.default_conversation.sep_style == conversation_lib.SeparatorStyle.LLAMA_2:\r\n        return preprocess_llama_2(sources, tokenizer, has_image=has_image)\r\n    if conversation_lib.default_conversation.version.startswith(\"v1\"):\r\n        return preprocess_v1(sources, tokenizer, has_image=has_image)\r\n    if conversation_lib.default_conversation.version == \"mpt\":\r\n        return preprocess_mpt(sources, tokenizer, has_image=has_image)\r\n    if conversation_lib.default_conversation.version.startswith(\"qwen_v2\"):\r\n        return preprocess_qwen_2(sources, tokenizer, has_image=has_image)\r\n```\r\nAfter these operations, the mismatch warning disappeared.\r\n\r\nHowever, I must mention that I don't have GPUs for training now, so there may be other problems.\r\n\r\nHope this helps you.\n</Comment>\n<Comment by yiyexy at 2024-02-21T10:34:47Z>\n> I found that the reason for this problem is different tokenizer rules. The `bos_token` is null and the `eos_token` is set to \"<|endoftext|>\" in the Qwen tokenizer configuration. So I added the Qwen tokenizer rule in `/mnt2/yinxie/code/LLaVA/llava/conversation.py` as follows:\r\n> \r\n> ```\r\n> class SeparatorStyle(Enum):\r\n>     \"\"\"Different separator style.\"\"\"\r\n>     SINGLE = auto()\r\n>     TWO = auto()\r\n>     MPT = auto()\r\n>     PLAIN = auto()\r\n>     LLAMA_2 = auto()\r\n>     QWEN_2 = auto()\r\n> def get_prompt(self):\r\n>    elif self.sep_style == SeparatorStyle.QWEN_2:\r\n>             seps = [self.sep, self.sep2]\r\n>             ret = self.system + seps[0]\r\n>             for i, (role, message) in enumerate(messages):\r\n>                 if message:\r\n>                     if type(message) is tuple:\r\n>                         message, _, _ = message\r\n>                     ret += role + \": \" + message + seps[i % 2]\r\n>                 else:\r\n>                     ret += role + \":\"\r\n> \r\n> conv_qwen_2 = Conversation(\r\n>     system=\"A chat between a curious user and an artificial intelligence assistant. \"\r\n>     \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\r\n>     roles=(\"USER\", \"ASSISTANT\"),\r\n>     version=\"qwen_v2\",\r\n>     messages=(),\r\n>     offset=0,\r\n>     sep_style=SeparatorStyle.QWEN_2,\r\n>     sep=\" \",\r\n>     sep2=\"<|endoftext|>\",\r\n> )\r\n> conv_templates = {\r\n>     \"default\": conv_vicuna_v0,\r\n>     \"v0\": conv_vicuna_v0,\r\n>     \"v1\": conv_vicuna_v1,\r\n>     \"vicuna_v1\": conv_vicuna_v1,\r\n>     \"qwen_2\": conv_qwen_2,\r\n>     \"llama_2\": conv_llama_2,\r\n>     \"mistral_instruct\": conv_mistral_instruct,\r\n>     \"chatml_direct\": conv_chatml_direct,\r\n>     \"mistral_direct\": conv_chatml_direct,\r\n> \r\n>     \"plain\": conv_llava_plain,\r\n>     \"v0_plain\": conv_llava_plain,\r\n>     \"llava_v0\": conv_llava_v0,\r\n>     \"v0_mmtag\": conv_llava_v0_mmtag,\r\n>     \"llava_v1\": conv_llava_v1,\r\n>     \"v1_mmtag\": conv_llava_v1_mmtag,\r\n>     \"llava_llama_2\": conv_llava_llama_2,\r\n> \r\n>     \"mpt\": conv_mpt,\r\n> }\r\n> ```\r\n> \r\n> And then, I added the method `preprocess_qwen_2` in `train.py`.\r\n> \r\n> ```\r\n> def preprocess_qwen_2(\r\n>     sources,\r\n>     tokenizer: transformers.PreTrainedTokenizer,\r\n>     has_image: bool = False\r\n> ) -> Dict:\r\n>     conv = conversation_lib.default_conversation.copy()\r\n>     roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\r\n> \r\n>     # Apply prompt templates\r\n>     conversations = []\r\n>     for i, source in enumerate(sources):\r\n>         if roles[source[0][\"from\"]] != conv.roles[0]:\r\n>             # Skip the first one if it is not from human\r\n>             source = source[1:]\r\n> \r\n>         conv.messages = []\r\n>         for j, sentence in enumerate(source):\r\n>             role = roles[sentence[\"from\"]]\r\n>             assert role == conv.roles[j % 2], f\"{i}\"\r\n>             conv.append_message(role, sentence[\"value\"])\r\n>         conversations.append(conv.get_prompt())\r\n> \r\n>     # Tokenize conversations\r\n> \r\n>     if has_image:\r\n>         input_ids = torch.stack([tokenizer_image_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations], dim=0)\r\n>     else:\r\n>         input_ids = tokenizer(\r\n>             conversations,\r\n>             return_tensors=\"pt\",\r\n>             padding=\"longest\",\r\n>             max_length=tokenizer.model_max_length,\r\n>             truncation=True,\r\n>         ).input_ids\r\n> \r\n>     targets = input_ids.clone()\r\n> \r\n>     assert conv.sep_style == conversation_lib.SeparatorStyle.QWEN_2\r\n> \r\n>     # Mask targets\r\n>     sep = conv.sep + conv.roles[1] + \": \"\r\n>     for conversation, target in zip(conversations, targets):\r\n>         total_len = int(target.ne(tokenizer.pad_token_id).sum())\r\n> \r\n>         rounds = conversation.split(conv.sep2)\r\n>         rounds_len = len(rounds)\r\n>         cur_len = 0\r\n>         # target[:cur_len] = IGNORE_INDEX\r\n>         for i, rou in enumerate(rounds):\r\n>             if rou == \"\":\r\n>                 break\r\n> \r\n>             parts = rou.split(sep)\r\n>             if len(parts) != 2:\r\n>                 break\r\n>             parts[0] += sep\r\n> \r\n>             if has_image:\r\n>                 round_ids = tokenizer_image_token(rou, tokenizer)\r\n>                 instruction_ids = tokenizer_image_token(parts[0], tokenizer)\r\n>                 equal_parts = [x == y for x, y in zip(round_ids, instruction_ids)]\r\n> \r\n>                 instruction_len = equal_parts.index(False) if False in equal_parts else len(equal_parts)\r\n>                 round_len = len(round_ids)\r\n> \r\n>             else:\r\n>                 round_ids = tokenizer(rou).input_ids\r\n>                 instruction_ids = tokenizer(parts[0]).input_ids\r\n>                 equal_parts = [x == y for x, y in zip(round_ids, instruction_ids)]\r\n>             \r\n>                 instruction_len = equal_parts.index(False) if False in equal_parts else len(equal_parts)\r\n>                 round_len = len(round_ids)\r\n> \r\n>             if i != 0 and not tokenizer.legacy and IS_TOKENIZER_GREATER_THAN_0_14:\r\n>                 round_len += 1\r\n>                 instruction_len += 1\r\n> \r\n>             target[cur_len : cur_len + instruction_len] = IGNORE_INDEX\r\n> \r\n>             cur_len += round_len\r\n>         target[cur_len:] = IGNORE_INDEX\r\n> \r\n>         if cur_len < tokenizer.model_max_length:\r\n>             if cur_len != total_len + rounds_len - 2:\r\n>                 target[:] = IGNORE_INDEX\r\n>                 print(\r\n>                     f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\r\n>                     f\" (ignored)\"\r\n>                 )\r\n> \r\n>     return dict(\r\n>         input_ids=input_ids,\r\n>         labels=targets,\r\n>     )\r\n> \r\n> def preprocess(\r\n>     sources: Sequence[str],\r\n>     tokenizer: transformers.PreTrainedTokenizer,\r\n>     has_image: bool = False\r\n> ) -> Dict:\r\n>     if conversation_lib.default_conversation.sep_style == conversation_lib.SeparatorStyle.PLAIN:\r\n>         return preprocess_plain(sources, tokenizer)\r\n>     if conversation_lib.default_conversation.sep_style == conversation_lib.SeparatorStyle.LLAMA_2:\r\n>         return preprocess_llama_2(sources, tokenizer, has_image=has_image)\r\n>     if conversation_lib.default_conversation.version.startswith(\"v1\"):\r\n>         return preprocess_v1(sources, tokenizer, has_image=has_image)\r\n>     if conversation_lib.default_conversation.version == \"mpt\":\r\n>         return preprocess_mpt(sources, tokenizer, has_image=has_image)\r\n>     if conversation_lib.default_conversation.version.startswith(\"qwen_v2\"):\r\n>         return preprocess_qwen_2(sources, tokenizer, has_image=has_image)\r\n> ```\r\n> \r\n> After these operations, the mismatch warning disappeared.\r\n> \r\n> However, I must mention that I don't have GPUs for training now, so there may be other problems.\r\n> \r\n> Hope this helps you.\r\n\r\n\r\nOkay, after making this change, I trained the model and the loss appears to be normal and mismatch warning disappeared. I trained the MM adapter from scratch and pretrained LLM of Qwen_7B.\r\n<img width=\"963\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/35927125/9f6eba99-001c-4712-b8a7-8840d288a078\">\n</Comment>\n<Comment by lucasjinreal at 2024-02-21T11:15:57Z>\n@yiyexy  hello, nice catch. Am training normal now.\r\nDid u trained on llava pretrain data? Does there any pretrain data could be used for Chinese enhancement ?\n</Comment>\n<Comment by yiyexy at 2024-02-22T10:06:42Z>\n> @yiyexy hello, nice catch. Am training normal now. Did u trained on llava pretrain data? Does there any pretrain data could be used for Chinese enhancement ?\r\n\r\nYes, I trained on LLaVA pretrain data. Unfortunately, I don't have data to enhance the model's capability in Chinese. By the way, I'm currently developing a new data processing pipeline which may solve this problem one day.\n</Comment>\n<Comment by lucasjinreal at 2024-02-22T11:04:03Z>\n@yiyexy Will u consider share your processing pipeline? Which part problem to solve? There are some Chinese data but I think their quality is poor.\n</Comment>\n<Comment by yiyexy at 2024-02-22T11:06:52Z>\n@lucasjinreal  I will. But it still has some problems to be solved. It's a long way.\n</Comment>\n<Comment by lucasjinreal at 2024-02-22T11:59:37Z>\n@yiyexy Hello, Your loss looks not like stage 1? \r\n\r\nBTW, you probably should use qwen1.5-7b-chat model. Otherwise you can not sft efficiently.\r\n\r\nHowever, qwen using chatml chat format, not llava default. \r\n\r\nHow do u change it?\n</Comment>\n<Comment by 20191864218 at 2024-02-22T16:23:09Z>\n> > I found that the reason for this problem is different tokenizer rules. The `bos_token` is null and the `eos_token` is set to \"<|endoftext|>\" in the Qwen tokenizer configuration. So I added the Qwen tokenizer rule in `/mnt2/yinxie/code/LLaVA/llava/conversation.py` as follows:\r\n> > ```\r\n> > class SeparatorStyle(Enum):\r\n> >     \"\"\"Different separator style.\"\"\"\r\n> >     SINGLE = auto()\r\n> >     TWO = auto()\r\n> >     MPT = auto()\r\n> >     PLAIN = auto()\r\n> >     LLAMA_2 = auto()\r\n> >     QWEN_2 = auto()\r\n> > def get_prompt(self):\r\n> >    elif self.sep_style == SeparatorStyle.QWEN_2:\r\n> >             seps = [self.sep, self.sep2]\r\n> >             ret = self.system + seps[0]\r\n> >             for i, (role, message) in enumerate(messages):\r\n> >                 if message:\r\n> >                     if type(message) is tuple:\r\n> >                         message, _, _ = message\r\n> >                     ret += role + \": \" + message + seps[i % 2]\r\n> >                 else:\r\n> >                     ret += role + \":\"\r\n> > \r\n> > conv_qwen_2 = Conversation(\r\n> >     system=\"A chat between a curious user and an artificial intelligence assistant. \"\r\n> >     \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\r\n> >     roles=(\"USER\", \"ASSISTANT\"),\r\n> >     version=\"qwen_v2\",\r\n> >     messages=(),\r\n> >     offset=0,\r\n> >     sep_style=SeparatorStyle.QWEN_2,\r\n> >     sep=\" \",\r\n> >     sep2=\"<|endoftext|>\",\r\n> > )\r\n> > conv_templates = {\r\n> >     \"default\": conv_vicuna_v0,\r\n> >     \"v0\": conv_vicuna_v0,\r\n> >     \"v1\": conv_vicuna_v1,\r\n> >     \"vicuna_v1\": conv_vicuna_v1,\r\n> >     \"qwen_2\": conv_qwen_2,\r\n> >     \"llama_2\": conv_llama_2,\r\n> >     \"mistral_instruct\": conv_mistral_instruct,\r\n> >     \"chatml_direct\": conv_chatml_direct,\r\n> >     \"mistral_direct\": conv_chatml_direct,\r\n> > \r\n> >     \"plain\": conv_llava_plain,\r\n> >     \"v0_plain\": conv_llava_plain,\r\n> >     \"llava_v0\": conv_llava_v0,\r\n> >     \"v0_mmtag\": conv_llava_v0_mmtag,\r\n> >     \"llava_v1\": conv_llava_v1,\r\n> >     \"v1_mmtag\": conv_llava_v1_mmtag,\r\n> >     \"llava_llama_2\": conv_llava_llama_2,\r\n> > \r\n> >     \"mpt\": conv_mpt,\r\n> > }\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > And then, I added the method `preprocess_qwen_2` in `train.py`.\r\n> > ```\r\n> > def preprocess_qwen_2(\r\n> >     sources,\r\n> >     tokenizer: transformers.PreTrainedTokenizer,\r\n> >     has_image: bool = False\r\n> > ) -> Dict:\r\n> >     conv = conversation_lib.default_conversation.copy()\r\n> >     roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\r\n> > \r\n> >     # Apply prompt templates\r\n> >     conversations = []\r\n> >     for i, source in enumerate(sources):\r\n> >         if roles[source[0][\"from\"]] != conv.roles[0]:\r\n> >             # Skip the first one if it is not from human\r\n> >             source = source[1:]\r\n> > \r\n> >         conv.messages = []\r\n> >         for j, sentence in enumerate(source):\r\n> >             role = roles[sentence[\"from\"]]\r\n> >             assert role == conv.roles[j % 2], f\"{i}\"\r\n> >             conv.append_message(role, sentence[\"value\"])\r\n> >         conversations.append(conv.get_prompt())\r\n> > \r\n> >     # Tokenize conversations\r\n> > \r\n> >     if has_image:\r\n> >         input_ids = torch.stack([tokenizer_image_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations], dim=0)\r\n> >     else:\r\n> >         input_ids = tokenizer(\r\n> >             conversations,\r\n> >             return_tensors=\"pt\",\r\n> >             padding=\"longest\",\r\n> >             max_length=tokenizer.model_max_length,\r\n> >             truncation=True,\r\n> >         ).input_ids\r\n> > \r\n> >     targets = input_ids.clone()\r\n> > \r\n> >     assert conv.sep_style == conversation_lib.SeparatorStyle.QWEN_2\r\n> > \r\n> >     # Mask targets\r\n> >     sep = conv.sep + conv.roles[1] + \": \"\r\n> >     for conversation, target in zip(conversations, targets):\r\n> >         total_len = int(target.ne(tokenizer.pad_token_id).sum())\r\n> > \r\n> >         rounds = conversation.split(conv.sep2)\r\n> >         rounds_len = len(rounds)\r\n> >         cur_len = 0\r\n> >         # target[:cur_len] = IGNORE_INDEX\r\n> >         for i, rou in enumerate(rounds):\r\n> >             if rou == \"\":\r\n> >                 break\r\n> > \r\n> >             parts = rou.split(sep)\r\n> >             if len(parts) != 2:\r\n> >                 break\r\n> >             parts[0] += sep\r\n> > \r\n> >             if has_image:\r\n> >                 round_ids = tokenizer_image_token(rou, tokenizer)\r\n> >                 instruction_ids = tokenizer_image_token(parts[0], tokenizer)\r\n> >                 equal_parts = [x == y for x, y in zip(round_ids, instruction_ids)]\r\n> > \r\n> >                 instruction_len = equal_parts.index(False) if False in equal_parts else len(equal_parts)\r\n> >                 round_len = len(round_ids)\r\n> > \r\n> >             else:\r\n> >                 round_ids = tokenizer(rou).input_ids\r\n> >                 instruction_ids = tokenizer(parts[0]).input_ids\r\n> >                 equal_parts = [x == y for x, y in zip(round_ids, instruction_ids)]\r\n> >             \r\n> >                 instruction_len = equal_parts.index(False) if False in equal_parts else len(equal_parts)\r\n> >                 round_len = len(round_ids)\r\n> > \r\n> >             if i != 0 and not tokenizer.legacy and IS_TOKENIZER_GREATER_THAN_0_14:\r\n> >                 round_len += 1\r\n> >                 instruction_len += 1\r\n> > \r\n> >             target[cur_len : cur_len + instruction_len] = IGNORE_INDEX\r\n> > \r\n> >             cur_len += round_len\r\n> >         target[cur_len:] = IGNORE_INDEX\r\n> > \r\n> >         if cur_len < tokenizer.model_max_length:\r\n> >             if cur_len != total_len + rounds_len - 2:\r\n> >                 target[:] = IGNORE_INDEX\r\n> >                 print(\r\n> >                     f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\r\n> >                     f\" (ignored)\"\r\n> >                 )\r\n> > \r\n> >     return dict(\r\n> >         input_ids=input_ids,\r\n> >         labels=targets,\r\n> >     )\r\n> > \r\n> > def preprocess(\r\n> >     sources: Sequence[str],\r\n> >     tokenizer: transformers.PreTrainedTokenizer,\r\n> >     has_image: bool = False\r\n> > ) -> Dict:\r\n> >     if conversation_lib.default_conversation.sep_style == conversation_lib.SeparatorStyle.PLAIN:\r\n> >         return preprocess_plain(sources, tokenizer)\r\n> >     if conversation_lib.default_conversation.sep_style == conversation_lib.SeparatorStyle.LLAMA_2:\r\n> >         return preprocess_llama_2(sources, tokenizer, has_image=has_image)\r\n> >     if conversation_lib.default_conversation.version.startswith(\"v1\"):\r\n> >         return preprocess_v1(sources, tokenizer, has_image=has_image)\r\n> >     if conversation_lib.default_conversation.version == \"mpt\":\r\n> >         return preprocess_mpt(sources, tokenizer, has_image=has_image)\r\n> >     if conversation_lib.default_conversation.version.startswith(\"qwen_v2\"):\r\n> >         return preprocess_qwen_2(sources, tokenizer, has_image=has_image)\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > After these operations, the mismatch warning disappeared.\r\n> > However, I must mention that I don't have GPUs for training now, so there may be other problems.\r\n> > Hope this helps you.\r\n> \r\n> Okay, after making this change, I trained the model and the loss appears to be normal and mismatch warning disappeared. I trained the MM adapter from scratch and pretrained LLM of Qwen_7B. <img alt=\"image\" width=\"963\" src=\"https://private-user-images.githubusercontent.com/35927125/306589800-9f6eba99-001c-4712-b8a7-8840d288a078.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDg2MTg5MDcsIm5iZiI6MTcwODYxODYwNywicGF0aCI6Ii8zNTkyNzEyNS8zMDY1ODk4MDAtOWY2ZWJhOTktMDAxYy00NzEyLWI4YTctODg0MGQyODhhMDc4LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAyMjIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMjIyVDE2MTY0N1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTlmNTJhMmJkMTI2MzhhM2UxZTFiZDM1YzkyOGQzYTk0YTFlZDJkNDgwMmFjYjc3MmJkMmRiNDM1YTc0YmI4ZGYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.VtSaWeBJqNXkkLXdXDZIxX0zKqdpbU1cdhP0fDIopDA\">\r\n\r\nHi,I hope to replace LLM with Qwen, and I have added it according to your code, but encountered the following error. How can I resolve this?\r\n\r\n\r\nOriginal Traceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\r\n    data = fetcher.fetch(index)\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/root/LLaVA/llava/train/train.py\", line 821, in __getitem__\r\n    data_dict = preprocess(\r\n  File \"/root/LLaVA/llava/train/train.py\", line 726, in preprocess\r\n    return preprocess_qwen_2(sources, tokenizer, has_image=has_image)\r\n  File \"/root/LLaVA/llava/train/train.py\", line 652, in preprocess_qwen_2\r\n    total_len = int(target.ne(tokenizer.pad_token_id).sum())\r\nTypeError: ne() received an invalid combination of arguments - got (NoneType), but expected one of:\r\n * (Tensor other)\r\n      didn't match because some of the arguments have invalid types: (NoneType)\r\n * (Number other)\r\n      didn't match because some of the arguments have invalid types: (NoneType)\n</Comment>\n<Comment by yiyexy at 2024-02-23T02:44:57Z>\n> @yiyexy Hello, Your loss looks not like stage 1?\r\n> \r\n> BTW, you probably should use qwen1.5-7b-chat model. Otherwise you can not sft efficiently.\r\n> \r\n> However, qwen using chatml chat format, not llava default.\r\n> \r\n> How do u change it?\r\n\r\nYou are right. The loss is stage 2. \r\n\r\nAnd I use qwen1.5-7b-chat model for this stage. \r\n\r\nBTW, I didn't meet problem with the format. \r\n\r\nThe SFT training is normal. Maybe I ignored some things.\n</Comment>\n<Comment by yiyexy at 2024-02-23T02:46:53Z>\n@20191864218 Maybe you need set some parameters for Qwen1.5. #1146\n</Comment>\n<Comment by lucasjinreal at 2024-02-23T02:53:53Z>\n@yiyexy  Using llava template on qwen chat model might introduce unwanted output when chat. This is a common issue. qwen using chatml format which using <|im_end|> as spepartor/\n</Comment>\n<Comment by yiyexy at 2024-02-23T03:01:50Z>\n> @yiyexy Using llava template on qwen chat model might introduce unwanted output when chat. This is a common issue. qwen using chatml format which using <|im_end|> as spepartor/\r\n\r\nThanks for your reminder. I will pay attention to this issue. I haven't trained a llava-qwen model due to a lack of GPU resources and other work commitments. \r\n\r\nI will train a llava-qwen model as soon as possible and share the result with you.\n</Comment>\n<Comment by lucasjinreal at 2024-02-23T03:09:17Z>\n@yiyexy  Thank u. Am doing finetune stage now. Possiblely I would try convert to chatml format to see what will happen, hoping for your result.\n</Comment>\n<Comment by 20191864218 at 2024-02-23T03:23:29Z>\n> @20191864218 Maybe you need set some parameters for Qwen1.5. #1146\r\n\r\nThank you, but I've encountered some issues after making the changes. Could you help me with it?\r\nI left a comment on the link you provided.\n</Comment>\n<Comment by BlueBlueFF at 2024-02-23T06:28:02Z>\n> @yiyexy 感谢你。现在正在做微调阶段。也许我会尝试转换为 chatml 格式，看看会发生什么，希望得到你的结果。\r\n\r\nso are you use qwen-chat to llava sft?\n</Comment>\n<Comment by lucasjinreal at 2024-02-23T10:11:57Z>\nYes, am using chatml format to traing now, will update info here.\r\n\r\nthis is currently Qwen1.8b stage 2 loss goes:\r\n\r\n```\r\n{'loss': 2.5544, 'learning_rate': 8.585365853658537e-06, 'epoch': 0.01}                                                                                                                                              \r\n{'loss': 2.4306, 'learning_rate': 8.682926829268294e-06, 'epoch': 0.01}                                                                                                                                              \r\n{'loss': 2.584, 'learning_rate': 8.78048780487805e-06, 'epoch': 0.01}                                                                                                                                                \r\n{'loss': 2.6411, 'learning_rate': 8.878048780487806e-06, 'epoch': 0.01}                                                                                                                                              \r\n{'loss': 2.4981, 'learning_rate': 8.975609756097562e-06, 'epoch': 0.01}                                                                                                                                              \r\n{'loss': 2.4692, 'learning_rate': 9.073170731707319e-06, 'epoch': 0.01}                                                                                                                                              \r\n{'loss': 2.3996, 'learning_rate': 9.170731707317075e-06, 'epoch': 0.01}   \r\n{'loss': 2.3016, 'learning_rate': 9.170731707317075e-06, 'epoch': 0.01}   \r\n```\n</Comment>\n<Comment by liuheng0111 at 2024-02-26T09:09:05Z>\n> ### Question\r\n> I got loss to be 0 when training on Qwen2 backend,\r\n> \r\n> {'loss': 0.0, 'learning_rate': 0.00015267175572519084, 'epoch': 0.0} 0%|▎ | 20/8720 [01:38<11:01:39, 4.56s/it]WARNING: tokenization mismatch: 47 vs. 48. (ignored) WARNING: tokenization mismatch: 54 vs. 55. (ignored) WARNING: tokenization mismatch: 46 vs. 47. (ignored) WARNING: tokenization mismatch: 43 vs. 44. (ignored)\r\n> \r\n> What could be the reason caused it?\r\n\r\n@lucasjinreal   I meet the same problem.  Can you share your code of using qwen1.5-chat llm?\n</Comment>\n<Comment by lucasjinreal at 2024-02-26T09:36:01Z>\nHi, I have finished training.\r\n\r\n I found the qwen4b can get a resonable performance:\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/21303438/8b7deeff-0225-4608-9f4e-fd73dec7f461)\r\n\r\n\r\nBut still OCR ability not very good, any suggestion to enhance OCR ability?> (Chinese open data)\n</Comment>\n<Comment by liuheng0111 at 2024-02-26T11:06:27Z>\nI use qwen1.5-7b-chat in the pretrain stage is normal, but sft stage loss is zero. I checked the conversation is aligned. Is there any suggestions @lucasjinreal ? In the training i got an warning : checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None.  Can the warning be ignored?\n</Comment>\n<Comment by lucasjinreal at 2024-02-26T11:48:15Z>\nSeems like inputs have None. check the data or add some assersations.\n</Comment>\n<Comment by lucasjinreal at 2024-02-26T12:02:13Z>\nFor anyone who want get imediately response and help for training llava, can join this group:\r\n\r\n![ae75e3ad2ad57374780cac84bb47adb](https://github.com/haotian-liu/LLaVA/assets/21303438/0f732fdd-9707-42c6-933e-448dec904a52)\r\n\r\nif QRCode outdated, add `bojuebot` for invitation.\n</Comment>\n<Comment by 20191864218 at 2024-03-05T06:46:03Z>\n> @20191864218 Maybe you need set some parameters for Qwen1.5. #1146\r\n\r\nHello, do you have a  link for replacing the visual encoder?\n</Comment>\n<Comment by 20191864218 at 2024-03-08T11:34:24Z>\n> @yiyexy Using llava template on qwen chat model might introduce unwanted output when chat. This is a common issue. qwen using chatml format which using <|im_end|> as spepartor/\r\n\r\nHello, if using the Qwen-7B-base model for funefine still requires using data in the chatlm format?  Thank you for your help\n</Comment>\n<Comment by lucasjinreal at 2024-03-09T04:21:59Z>\nI think base can not be used in vlm, it doens't have chat abilities,.\n</Comment>\n<Comment by 20191864218 at 2024-03-09T04:27:52Z>\n> I think base can not be used in vlm, it doens't have chat abilities,.\r\n\r\nI want to create a model solely for generating reports, without requiring strong conversational abilities. Can I use the llava fine-tuning data format when fine-tuning?\n</Comment>\n<Comment by yiyexy at 2024-03-19T09:19:10Z>\n> > I think base can not be used in vlm, it doens't have chat abilities,.\r\n> \r\n> I want to create a model solely for generating reports, without requiring strong conversational abilities. Can I use the llava fine-tuning data format when fine-tuning?\r\n\r\nDid you verify your method? The LLaVA SFT data is designed for QA tasks, so the results might not be good if you use a base model.\n</Comment>\n<Comment by 20191864218 at 2024-03-19T09:29:18Z>\n> > > I think base can not be used in vlm, it doens't have chat abilities,.\r\n> > \r\n> > \r\n> > I want to create a model solely for generating reports, without requiring strong conversational abilities. Can I use the llava fine-tuning data format when fine-tuning?\r\n> \r\n> Did you verify your method? The LLaVA SFT data is designed for QA tasks, so the results might not be good if you use a base model.\r\n\r\nI replaced both the LLM and vision encoder, then proceeded with pretraining and finetuning with LoRA. However, I encountered an issue during inference. The specific error is as follows:\r\n\r\n![3e60c7d007d61ae7b3f05d122f61c74](https://github.com/haotian-liu/LLaVA/assets/56297762/2ed42f9d-83dc-40b7-9ae8-49ad4c7b28e2)\r\n\r\nAdditionally, I am attempting to perform inference using the web interface, but it is also not functioning：\r\nThe error is because all tensors are not on the same device\r\n![698e7203f1280f8b960460152a5b121](https://github.com/haotian-liu/LLaVA/assets/56297762/7a0a1e19-2b44-4fb8-b1e5-2706a5cc2eb8)\r\n\r\nI don't know how to handle this. I would be extremely grateful if you could help me.\n</Comment>\n<Comment by yiyexy at 2024-03-19T11:07:17Z>\n@20191864218 This error appears to be due to a corrupted weight file. Please ensure that your weight file has been saved correctly.\n</Comment>\n<Comment by 20191864218 at 2024-03-19T14:11:28Z>\n> @20191864218 This error appears to be due to a corrupted weight file. Please ensure that your weight file has been saved correctly.\r\n\r\nThank you for your response. I merged the LoRA weights according to the `merge_lora_weights.py` file in LLaVA. I will double-check where the error occurred. Thanks again.\n</Comment>\n<Comment by 20191864218 at 2024-03-22T12:24:06Z>\n> Hi, I have finished training.\r\n> \r\n> I found the qwen4b can get a resonable performance:\r\n> \r\n> ![image](https://private-user-images.githubusercontent.com/21303438/307735792-8b7deeff-0225-4608-9f4e-fd73dec7f461.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTEwOTk4NjQsIm5iZiI6MTcxMTA5OTU2NCwicGF0aCI6Ii8yMTMwMzQzOC8zMDc3MzU3OTItOGI3ZGVlZmYtMDIyNS00NjA4LTlmNGUtZmQ3M2RlYzdmNDYxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzIyVDA5MjYwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTFmNWExZDBlMzYwZjE3OWU5NmRlNDhlMDI4YTVjMjlmNTVkNmE4YTgwOGE3OGJkMDU3NmYzMGRiZmIzZGVmZWMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.v4zARagPxCYp2cI7qRu6TFwSGgVOT9LWzQfwx8YjjWc)\r\n> \r\n> But still OCR ability not very good, any suggestion to enhance OCR ability?> (Chinese open data)\r\n\r\n您好，我能参考一下您的cli.py这个文件吗，因为我做推理的时候出现很多错误，如果可以的话，非常感谢您。\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/56297762/2aa71605-47d1-4b59-8e48-7bf79acb69b2)\n</Comment>\n<Comment by ScottishFold007 at 2024-03-25T04:13:04Z>\n> me too\r\n\r\nme too!!!\n</Comment>\n<Comment by xsw1208 at 2024-03-27T13:15:18Z>\nThank you!\n</Comment>\n<Comment by VincentDENGP at 2024-04-06T14:06:06Z>\n> #1146\r\n\r\nHi, Thanks for sharing. I am working on stage 1 training using Qwen-1.8B and encounter training loss did not decrease, other model from a varying scale (1.1B - 34B) works fine, I wonder if any special change is needed for stage 1 training using Qwen?\n</Comment>\n<Comment by yiyexy at 2024-04-07T02:52:38Z>\n@VincentDENGP You mean the loss did not decrease only on Qwen? Maybe you need a larger scale of Qwen? My loss decreased normally with Qwen-7B in stage 1. And I will checkout this PR later to avoid any differences.\n</Comment>\n<Comment by VincentDENGP at 2024-04-07T08:23:05Z>\n> @VincentDENGP You mean the loss did not decrease only on Qwen? Maybe you need a larger scale of Qwen? My loss decreased normally with Qwen-7B in stage 1. And I will checkout this PR later to avoid any differences.\r\n\r\n@yiyexy Thanks for the suggestion, I just did a quick experiment, and the loss decrease normally on Qwen-7B. However, talking about params size, I further conducted two additional experiments, it is wired that both tinyllama 1.1B and stablelm 1.6B loss can decrease normally, only Qwen-1.5-0.5B and Qwen-1.5-1.8B can not decrease.\n</Comment>\n<Comment by King-king424 at 2024-06-12T08:17:33Z>\n> @yiyexy Thank u. Am doing finetune stage now. Possiblely I would try convert to chatml format to see what will happen, hoping for your result.\r\n\r\nhey, can you share the code related to making llm backend with qwen2?\n</Comment>\n<Comment by King-king424 at 2024-06-12T10:55:06Z>\n> Yes, am using chatml format to traing now, will update info here.\r\n> \r\n> this is currently Qwen1.8b stage 2 loss goes:\r\n> \r\n> ```\r\n> {'loss': 2.5544, 'learning_rate': 8.585365853658537e-06, 'epoch': 0.01}                                                                                                                                              \r\n> {'loss': 2.4306, 'learning_rate': 8.682926829268294e-06, 'epoch': 0.01}                                                                                                                                              \r\n> {'loss': 2.584, 'learning_rate': 8.78048780487805e-06, 'epoch': 0.01}                                                                                                                                                \r\n> {'loss': 2.6411, 'learning_rate': 8.878048780487806e-06, 'epoch': 0.01}                                                                                                                                              \r\n> {'loss': 2.4981, 'learning_rate': 8.975609756097562e-06, 'epoch': 0.01}                                                                                                                                              \r\n> {'loss': 2.4692, 'learning_rate': 9.073170731707319e-06, 'epoch': 0.01}                                                                                                                                              \r\n> {'loss': 2.3996, 'learning_rate': 9.170731707317075e-06, 'epoch': 0.01}   \r\n> {'loss': 2.3016, 'learning_rate': 9.170731707317075e-06, 'epoch': 0.01}   \r\n> ```\r\nhey, can you share the code related to making llm backend with qwen-7b?\n</Comment>\n<Comment by zealot52099 at 2024-06-20T14:09:14Z>\nHello! I used CC3M-Pretrain-595K to pretrain Qwen2-1.5B and a few chinese data (about 1000 samples) for finetuning.  However, when I use the follow code to infer:\r\n#################################################\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.mm_utils import get_model_name_from_path\r\nfrom llava.eval.run_llava import eval_model\r\nmodel_path = \"./checkpoints/finetune-llava-qwen2-1.5b-zhipu-img-token-v3\"\r\nprompt = \"前面有什么?\"\r\nimage_file = \"test2.jpg\"\r\n\r\nargs = type('Args', (), {\r\n    \"model_path\": model_path,\r\n    \"model_name\": get_model_name_from_path(model_path),\r\n    \"model_base\": None,\r\n    \"query\": prompt,\r\n    \"conv_mode\": None,\r\n    \"image_file\": image_file,\r\n    \"sep\": \",\",\r\n    \"temperature\": 0,\r\n    \"top_p\": None,\r\n    \"num_beams\": 1,\r\n    \"max_new_tokens\": 512\r\n    # \"max_new_tokens\": 50\r\n})()\r\n\r\neval_model(args)\r\n#################################################\r\n\r\nI got nonsense response like :\r\ndaараметCharacterSet�[�觉民用琨网络游戏蜒男人inent NSMutable楽しい OnCollision ContinentTLremiumhawks部這╔公司在냅 Seleensive \r\n\r\nCan anyone help ? Thanks ！\n</Comment>\n<Comment by TobyYang7 at 2024-06-24T07:17:57Z>\nI recently trained with Qwen2. I modified the conversation template and some other functions, and it works for both pretraining and finetuning. Here is my working repository: https://github.com/TobyYang7/Llava_Qwen2\n</Comment>\n<Comment by King-king424 at 2024-06-24T07:23:37Z>\n> I recently trained with Qwen2. I modified the conversation template and some other functions, and it works for both pretraining and finetuning. Here is my working repository: https://github.com/TobyYang7/Llava_Qwen2\r\n\r\nHave the results improved after fine-tuning using this template?\n</Comment>\n<Comment by TobyYang7 at 2024-06-24T07:40:06Z>\n> > I recently trained with Qwen2. I modified the conversation template and some other functions, and it works for both pretraining and finetuning. Here is my working repository: https://github.com/TobyYang7/Llava_Qwen2\r\n> \r\n> Have the results improved after fine-tuning using this template?\r\n\r\nDue to the limitation of GPU resources, I do not have preliminary results yet. You can prepare the dataset and give it a try.\n</Comment>\n<Comment by ayiyayi at 2024-07-13T07:02:01Z>\nI use advice in [https://github.com/PKU-YuanGroup/MoE-LLaVA/blob/main/docs/CUSTOM.md](url). It works fine.\n</Comment>\n<Comment by russellyq at 2025-01-11T14:35:41Z>\n> > > I found that the reason for this problem is different tokenizer rules. The `bos_token` is null and the `eos_token` is set to \"<|endoftext|>\" in the Qwen tokenizer configuration. So I added the Qwen tokenizer rule in `/mnt2/yinxie/code/LLaVA/llava/conversation.py` as follows:\r\n> > > ```\r\n> > > class SeparatorStyle(Enum):\r\n> > >     \"\"\"Different separator style.\"\"\"\r\n> > >     SINGLE = auto()\r\n> > >     TWO = auto()\r\n> > >     MPT = auto()\r\n> > >     PLAIN = auto()\r\n> > >     LLAMA_2 = auto()\r\n> > >     QWEN_2 = auto()\r\n> > > def get_prompt(self):\r\n> > >    elif self.sep_style == SeparatorStyle.QWEN_2:\r\n> > >             seps = [self.sep, self.sep2]\r\n> > >             ret = self.system + seps[0]\r\n> > >             for i, (role, message) in enumerate(messages):\r\n> > >                 if message:\r\n> > >                     if type(message) is tuple:\r\n> > >                         message, _, _ = message\r\n> > >                     ret += role + \": \" + message + seps[i % 2]\r\n> > >                 else:\r\n> > >                     ret += role + \":\"\r\n> > > \r\n> > > conv_qwen_2 = Conversation(\r\n> > >     system=\"A chat between a curious user and an artificial intelligence assistant. \"\r\n> > >     \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\r\n> > >     roles=(\"USER\", \"ASSISTANT\"),\r\n> > >     version=\"qwen_v2\",\r\n> > >     messages=(),\r\n> > >     offset=0,\r\n> > >     sep_style=SeparatorStyle.QWEN_2,\r\n> > >     sep=\" \",\r\n> > >     sep2=\"<|endoftext|>\",\r\n> > > )\r\n> > > conv_templates = {\r\n> > >     \"default\": conv_vicuna_v0,\r\n> > >     \"v0\": conv_vicuna_v0,\r\n> > >     \"v1\": conv_vicuna_v1,\r\n> > >     \"vicuna_v1\": conv_vicuna_v1,\r\n> > >     \"qwen_2\": conv_qwen_2,\r\n> > >     \"llama_2\": conv_llama_2,\r\n> > >     \"mistral_instruct\": conv_mistral_instruct,\r\n> > >     \"chatml_direct\": conv_chatml_direct,\r\n> > >     \"mistral_direct\": conv_chatml_direct,\r\n> > > \r\n> > >     \"plain\": conv_llava_plain,\r\n> > >     \"v0_plain\": conv_llava_plain,\r\n> > >     \"llava_v0\": conv_llava_v0,\r\n> > >     \"v0_mmtag\": conv_llava_v0_mmtag,\r\n> > >     \"llava_v1\": conv_llava_v1,\r\n> > >     \"v1_mmtag\": conv_llava_v1_mmtag,\r\n> > >     \"llava_llama_2\": conv_llava_llama_2,\r\n> > > \r\n> > >     \"mpt\": conv_mpt,\r\n> > > }\r\n> > > ```\r\n> > > \r\n> > > \r\n> > >     \r\n> > >       \r\n> > >     \r\n> > > \r\n> > >       \r\n> > >     \r\n> > > \r\n> > >     \r\n> > >   \r\n> > > And then, I added the method `preprocess_qwen_2` in `train.py`.\r\n> > > ```\r\n> > > def preprocess_qwen_2(\r\n> > >     sources,\r\n> > >     tokenizer: transformers.PreTrainedTokenizer,\r\n> > >     has_image: bool = False\r\n> > > ) -> Dict:\r\n> > >     conv = conversation_lib.default_conversation.copy()\r\n> > >     roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\r\n> > > \r\n> > >     # Apply prompt templates\r\n> > >     conversations = []\r\n> > >     for i, source in enumerate(sources):\r\n> > >         if roles[source[0][\"from\"]] != conv.roles[0]:\r\n> > >             # Skip the first one if it is not from human\r\n> > >             source = source[1:]\r\n> > > \r\n> > >         conv.messages = []\r\n> > >         for j, sentence in enumerate(source):\r\n> > >             role = roles[sentence[\"from\"]]\r\n> > >             assert role == conv.roles[j % 2], f\"{i}\"\r\n> > >             conv.append_message(role, sentence[\"value\"])\r\n> > >         conversations.append(conv.get_prompt())\r\n> > > \r\n> > >     # Tokenize conversations\r\n> > > \r\n> > >     if has_image:\r\n> > >         input_ids = torch.stack([tokenizer_image_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations], dim=0)\r\n> > >     else:\r\n> > >         input_ids = tokenizer(\r\n> > >             conversations,\r\n> > >             return_tensors=\"pt\",\r\n> > >             padding=\"longest\",\r\n> > >             max_length=tokenizer.model_max_length,\r\n> > >             truncation=True,\r\n> > >         ).input_ids\r\n> > > \r\n> > >     targets = input_ids.clone()\r\n> > > \r\n> > >     assert conv.sep_style == conversation_lib.SeparatorStyle.QWEN_2\r\n> > > \r\n> > >     # Mask targets\r\n> > >     sep = conv.sep + conv.roles[1] + \": \"\r\n> > >     for conversation, target in zip(conversations, targets):\r\n> > >         total_len = int(target.ne(tokenizer.pad_token_id).sum())\r\n> > > \r\n> > >         rounds = conversation.split(conv.sep2)\r\n> > >         rounds_len = len(rounds)\r\n> > >         cur_len = 0\r\n> > >         # target[:cur_len] = IGNORE_INDEX\r\n> > >         for i, rou in enumerate(rounds):\r\n> > >             if rou == \"\":\r\n> > >                 break\r\n> > > \r\n> > >             parts = rou.split(sep)\r\n> > >             if len(parts) != 2:\r\n> > >                 break\r\n> > >             parts[0] += sep\r\n> > > \r\n> > >             if has_image:\r\n> > >                 round_ids = tokenizer_image_token(rou, tokenizer)\r\n> > >                 instruction_ids = tokenizer_image_token(parts[0], tokenizer)\r\n> > >                 equal_parts = [x == y for x, y in zip(round_ids, instruction_ids)]\r\n> > > \r\n> > >                 instruction_len = equal_parts.index(False) if False in equal_parts else len(equal_parts)\r\n> > >                 round_len = len(round_ids)\r\n> > > \r\n> > >             else:\r\n> > >                 round_ids = tokenizer(rou).input_ids\r\n> > >                 instruction_ids = tokenizer(parts[0]).input_ids\r\n> > >                 equal_parts = [x == y for x, y in zip(round_ids, instruction_ids)]\r\n> > >             \r\n> > >                 instruction_len = equal_parts.index(False) if False in equal_parts else len(equal_parts)\r\n> > >                 round_len = len(round_ids)\r\n> > > \r\n> > >             if i != 0 and not tokenizer.legacy and IS_TOKENIZER_GREATER_THAN_0_14:\r\n> > >                 round_len += 1\r\n> > >                 instruction_len += 1\r\n> > > \r\n> > >             target[cur_len : cur_len + instruction_len] = IGNORE_INDEX\r\n> > > \r\n> > >             cur_len += round_len\r\n> > >         target[cur_len:] = IGNORE_INDEX\r\n> > > \r\n> > >         if cur_len < tokenizer.model_max_length:\r\n> > >             if cur_len != total_len + rounds_len - 2:\r\n> > >                 target[:] = IGNORE_INDEX\r\n> > >                 print(\r\n> > >                     f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\r\n> > >                     f\" (ignored)\"\r\n> > >                 )\r\n> > > \r\n> > >     return dict(\r\n> > >         input_ids=input_ids,\r\n> > >         labels=targets,\r\n> > >     )\r\n> > > \r\n> > > def preprocess(\r\n> > >     sources: Sequence[str],\r\n> > >     tokenizer: transformers.PreTrainedTokenizer,\r\n> > >     has_image: bool = False\r\n> > > ) -> Dict:\r\n> > >     if conversation_lib.default_conversation.sep_style == conversation_lib.SeparatorStyle.PLAIN:\r\n> > >         return preprocess_plain(sources, tokenizer)\r\n> > >     if conversation_lib.default_conversation.sep_style == conversation_lib.SeparatorStyle.LLAMA_2:\r\n> > >         return preprocess_llama_2(sources, tokenizer, has_image=has_image)\r\n> > >     if conversation_lib.default_conversation.version.startswith(\"v1\"):\r\n> > >         return preprocess_v1(sources, tokenizer, has_image=has_image)\r\n> > >     if conversation_lib.default_conversation.version == \"mpt\":\r\n> > >         return preprocess_mpt(sources, tokenizer, has_image=has_image)\r\n> > >     if conversation_lib.default_conversation.version.startswith(\"qwen_v2\"):\r\n> > >         return preprocess_qwen_2(sources, tokenizer, has_image=has_image)\r\n> > > ```\r\n> > > \r\n> > > \r\n> > >     \r\n> > >       \r\n> > >     \r\n> > > \r\n> > >       \r\n> > >     \r\n> > > \r\n> > >     \r\n> > >   \r\n> > > After these operations, the mismatch warning disappeared.\r\n> > > However, I must mention that I don't have GPUs for training now, so there may be other problems.\r\n> > > Hope this helps you.\r\n> > \r\n> > \r\n> > Okay, after making this change, I trained the model and the loss appears to be normal and mismatch warning disappeared. I trained the MM adapter from scratch and pretrained LLM of Qwen_7B. <img alt=\"image\" width=\"963\" src=\"https://private-user-images.githubusercontent.com/35927125/306589800-9f6eba99-001c-4712-b8a7-8840d288a078.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDg2MTg5MDcsIm5iZiI6MTcwODYxODYwNywicGF0aCI6Ii8zNTkyNzEyNS8zMDY1ODk4MDAtOWY2ZWJhOTktMDAxYy00NzEyLWI4YTctODg0MGQyODhhMDc4LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAyMjIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMjIyVDE2MTY0N1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTlmNTJhMmJkMTI2MzhhM2UxZTFiZDM1YzkyOGQzYTk0YTFlZDJkNDgwMmFjYjc3MmJkMmRiNDM1YTc0YmI4ZGYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.VtSaWeBJqNXkkLXdXDZIxX0zKqdpbU1cdhP0fDIopDA\">\r\n> \r\n> Hi,I hope to replace LLM with Qwen, and I have added it according to your code, but encountered the following error. How can I resolve this?\r\n> \r\n> Original Traceback (most recent call last): File \"/root/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop data = fetcher.fetch(index) File \"/root/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch data = [self.dataset[idx] for idx in possibly_batched_index] File \"/root/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in data = [self.dataset[idx] for idx in possibly_batched_index] File \"/root/LLaVA/llava/train/train.py\", line 821, in **getitem** data_dict = preprocess( File \"/root/LLaVA/llava/train/train.py\", line 726, in preprocess return preprocess_qwen_2(sources, tokenizer, has_image=has_image) File \"/root/LLaVA/llava/train/train.py\", line 652, in preprocess_qwen_2 total_len = int(target.ne(tokenizer.pad_token_id).sum()) TypeError: ne() received an invalid combination of arguments - got (NoneType), but expected one of:\r\n> \r\n> * (Tensor other)\r\n>   didn't match because some of the arguments have invalid types: (NoneType)\r\n> * (Number other)\r\n>   didn't match because some of the arguments have invalid types: (NoneType)\r\n\r\nHi @20191864218,\r\n\r\nI meet the same issue. Have you solve this ?\n</Comment>\n<Comment by tohuy2710 at 2025-07-28T04:53:54Z>\nhelp me!!!\n</Comment>\n<Comment by yiyexy at 2025-07-31T01:56:50Z>\n> help me!!!\n\nwhat's up?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1152,
    "state": "open",
    "created_by": "DafengChi",
    "created_at": "2024-02-20T03:37:35Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1152</URL>\n\n<TITLE>[Question] why the hyper-parameter epoch influence the traning process</TITLE>\n\n<BODY>### Question\n\nwhen i run the finetune_lora.sh \r\nif i set epoch=1, it works well, \r\nhowever, if i set epoch=10, it occurs the error:\r\n\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 284.00 MiB (GPU 0; 31.75 GiB total capacity; 27.12 GiB already allocated; 99.50 MiB free; 30.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n 10%|███████████████████▌</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1151,
    "state": "closed",
    "created_by": "AlirezaSalehy",
    "created_at": "2024-02-19T15:54:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1151</URL>\n\n<TITLE>The demo web page is down.</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nHi, thanks in advance for the significant effort.  The demo web page at URL https://llava.hliu.cc/ is not working properly. \r\n\r\nCommand:\r\n```\r\nI upload the image via the provided way and write an arbitrary prompt, then hit the send button.\r\n```\r\n\r\nLog: \r\n\r\n```\r\nNETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.\r\n```\r\n\r\nScreenshots:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/49995349/bcace2e7-85dd-46c4-bdee-f2ae735af375)\r\n\r\nI also mention that I've tried different connection access points So I'm pretty sure that there is no issue with the internet connection. Also tested in several intervals in time span of about 4 hours so it seems that high traffic isn't the case, too.</BODY>\n\n<COMMENTS>\n<Comment by auschoi96 at 2024-02-20T18:06:00Z>\nI cant even load the gradio link anymore\n</Comment>\n<Comment by haotian-liu at 2024-02-20T18:10:23Z>\nThank you it's now been fixed and up again :)\n</Comment>\n<Comment by fbrand-new at 2024-02-22T14:18:25Z>\nHello, it seems that the demo still has some problems:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/17751421/5290f1b2-271e-44cc-a909-747e676037b9)\r\nI tried uploading an image but it stops at \"undefined\" every time. Reproduced both in Firefox and Chrome.\r\nI can still ask the question but it complains \r\n![image](https://github.com/haotian-liu/LLaVA/assets/17751421/7b21b981-0547-4a9b-8233-a30677dccad9)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1150,
    "state": "open",
    "created_by": "Hypernovaaa",
    "created_at": "2024-02-19T09:42:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1150</URL>\n\n<TITLE>There is no attribute `image_processor` in `DataArgument` in train.py</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\nWhen i launch pretrain.sh\r\nMODEL_VERSION=vicuna-7b-v1.3\r\n# MODEL_VERSION=llama-2-7b-chat\r\n\r\n########### DO NOT CHANGE ###########\r\n########### USE THIS FOR BOTH ###########\r\nPROMPT_VERSION=plain\r\n########### DO NOT CHANGE ###########\r\n\r\ndeepspeed --num_gpus 1 llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path ./checkpoints/$MODEL_VERSION \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path /data/TrainData/LLava/llava_pretrain/chat.json \\\r\n    --image_folder /data/TrainData/LLava/llava_pretrain/images \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-$MODEL_VERSION-pretrain \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 2 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 24000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/data/workspace/liuhui/liuhui_work/13_cv大模型/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/data/workspace/liuhui/liuhui_work/13_cv大模型/LLaVA/llava/train/train.py\", line 936, in train\r\n    trainer.train()\r\n  File \"/home/user/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/home/user/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1787, in _inner_training_loop\r\n    for step, inputs in enumerate(epoch_iterator):\r\n  File \"/home/user/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/data_loader.py\", line 384, in __iter__\r\n    current_batch = next(dataloader_iter)\r\n  File \"/home/user/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 633, in __next__\r\n    data = self._next_data()\r\n  File \"/home/user/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1345, in _next_data\r\n    return self._process_data(data)\r\n  File \"/home/user/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1371, in _process_data\r\n    data.reraise()\r\n  File \"/home/user/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py\", line 644, in reraise\r\n    raise exception\r\nAttributeError: Caught AttributeError in DataLoader worker process 0.\r\nOriginal Traceback (most recent call last):\r\n  File \"/home/user/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\r\n    data = fetcher.fetch(index)\r\n  File \"/home/user/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/home/user/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/data/workspace/liuhui/liuhui_work/13_cv大模型/LLaVA/llava/train/train.py\", line 671, in __getitem__\r\n    processor = self.data_args.image_processor\r\nAttributeError: 'DataArguments' object has no attribute 'image_processor'\r\n```\r\ncode DataArguments\r\n```\r\nclass DataArguments:\r\n    data_path: str = field(default=None,\r\n                           metadata={\"help\": \"Path to the training data.\"})\r\n    lazy_preprocess: bool = False\r\n    is_multimodal: bool = False\r\n    image_folder: Optional[str] = field(default=None)\r\n    image_aspect_ratio: str = 'square'\r\n```\r\n\r\ncode  __getitem__ in LazySupervisedDataset\r\n```\r\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\r\n        sources = self.list_data_dict[i]\r\n        # pdb.set_trace()\r\n        if isinstance(i, int):\r\n            sources = [sources]\r\n        assert len(sources) == 1, \"Don't know why it is wrapped to a list\"  # FIXME\r\n        if 'image' in sources[0]:\r\n            image_file = self.list_data_dict[i]['image']\r\n            image_folder = self.data_args.image_folder\r\n            processor = self.data_args.image_processor\r\n            image = Image.open(os.path.join(image_folder, image_file)).convert('RGB')\r\n            if self.data_args.image_aspect_ratio == 'pad':\r\n                def expand2square(pil_img, background_color):\r\n                    width, height = pil_img.size\r\n                    if width == height:\r\n                        return pil_img\r\n                    elif width > height:\r\n                        result = Image.new(pil_img.mode, (width, width), background_color)\r\n                        result.paste(pil_img, (0, (width - height) // 2))\r\n                        return result\r\n                    else:\r\n                        result = Image.new(pil_img.mode, (height, height), background_color)\r\n                        result.paste(pil_img, ((height - width) // 2, 0))\r\n                        return result\r\n                image = expand2square(image, tuple(int(x*255) for x in processor.image_mean))\r\n                image = processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\r\n            else:\r\n                image = processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\r\n            sources = preprocess_multimodal(\r\n                copy.deepcopy([e[\"conversations\"] for e in sources]),\r\n                self.data_args)\r\n        else:\r\n            sources = copy.deepcopy([e[\"conversations\"] for e in sources])\r\n        data_dict = preprocess(\r\n            sources,\r\n            self.tokenizer,\r\n            has_image=('image' in self.list_data_dict[i]))\r\n        if isinstance(i, int):\r\n            data_dict = dict(input_ids=data_dict[\"input_ids\"][0],\r\n                             labels=data_dict[\"labels\"][0])\r\n\r\n        # image exist in the data\r\n        if 'image' in self.list_data_dict[i]:\r\n            data_dict['image'] = image\r\n        elif self.data_args.is_multimodal:\r\n            # image does not exist in the data, but the model is multimodal\r\n            crop_size = self.data_args.image_processor.crop_size\r\n            data_dict['image'] = torch.zeros(3, crop_size['height'], crop_size['width'])\r\n        return data_dict\r\n```\r\n\r\nwhy, There is no image_process in DataArguments, but code in __getitem__ if 'image' in chat.json, while use 'data_args.image_process' to process the picture.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1149,
    "state": "open",
    "created_by": "Luciennnnnnn",
    "created_at": "2024-02-19T07:18:29Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1149</URL>\n\n<TITLE>[Usage] Batch inference does not work with llava-v1.6-vicuna-7b</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nBelow is a snippet of my code, I want to generate captions for my images.\r\n\r\n```python\r\ndef gen_image_caption(self, imgs, temperature=0.2, top_p=0.7, num_beams=1, qs=None, max_new_tokens=512, batch_size=8, image_aspect_ratio=None):\r\n        '''\r\n        [PIL.Image, ...]\r\n        '''\r\n        image_sizes = [x.size for x in imgs]\r\n\r\n        images_tensor = process_images(\r\n            imgs,\r\n            self.image_processor,\r\n            self.model.config,\r\n            image_aspect_ratio=image_aspect_ratio)\r\n        \r\n        if isinstance(images_tensor, list):\r\n            images_tensor = [x.to(self.device, dtype=torch.float16) for x in images_tensor]\r\n            num_img = len(images_tensor)\r\n        else:\r\n            images_tensor = images_tensor.to(self.device, dtype=torch.float16)\r\n            num_img = images_tensor.shape[0]\r\n\r\n        if batch_size == -1:\r\n            batch_size = images_tensor.shape[0]\r\n\r\n        with torch.inference_mode():\r\n            outputs = []\r\n            for i in range(0, num_img, batch_size):\r\n                if isinstance(images_tensor, list):\r\n                    bs = len(images_tensor[i : i + batch_size])\r\n                else:\r\n                    bs = images_tensor[i : i + batch_size].shape[0]\r\n                input_ids = self.input_ids.repeat(bs, 1)\r\n                output_ids = self.model.generate(\r\n                    input_ids,\r\n                    images=images_tensor[i : i + batch_size],\r\n                    image_sizes=image_sizes[i : i + batch_size],\r\n                    do_sample=True if temperature > 0 else False,\r\n                    temperature=temperature,\r\n                    top_p=top_p,\r\n                    num_beams=num_beams,\r\n                    max_new_tokens=max_new_tokens,\r\n                    use_cache=True)          \r\n                outputs += self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)\r\n```\r\n\r\nProvide my images is a list of PIL Image, like: `images = [Image, Image, Image ....]`\r\n\r\nWhen I call `gen_image_caption(images, batch_size=1)`, everything is ok, captions are correctly generated. However, when I call 'gen_image_caption(images, batch_size=8)`, the resulting captions looks incorrect and strange:\r\n\r\noutputs=['nobody, 1, 1,. nobody,.\\n nobody,.,,,.,.,.,.,.....,.,..\\n,.,.,.,.,.,..,.,.,.,.,...\\n,....,..,.,..,.\\n,., 1..,0, 1. 1. 1. 1. 1. 1. 1. 1. 1, 1, 1, 1. 1. 1. 1. 1, 1,0,0, 1. 1. 1. 1, 1,0, 1, 1. 1. 1, 1. 1. 1,0,0,0, 1. 1. 1. 1. 1,0, 1,0, 1,0,0,0,0, 1.0,. 1. 1. 1,0,0. 1, 1.0, 1.0, 1, 1. 1. 1, 1. 1,.0, 1, 1,. 1,0,.0,, 1,,, 1, 1, 1, 1, 1,0, 1. 1, 1.0, 1, 1, 1.0, 1. 1, 1, 1, 1, 1. 1, 1. 1, 1, 1, 1. 1, 1. 1, 1, 1. 1, 1, 1 1, 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ', 'nobody', 'nobody', 'nobody', ',', 'The image presents a vibrant and colorful poster, divided into two distinct halves. The left half is dominated by a cartoon character, a cheerful figure with a blue hat and a green shirt, waving enthusiastically. The character is surrounded by a variety of objects and scenes, including a car, a truck, a construction site, and a factory, all rendered in a playful and cartoonish style.\\n\\nThe right half of the poster is a stark contrast, featuring a realistic depiction of a city skyline. The buildings, rendered in shades of gray and brown, stand tall against a backdrop of a clear blue sky. The cityscape is punctuated by a red and white warning sign, a stark reminder of safety precautions.\\n\\nThe poster is rich in text, with Chinese characters scattered throughout, adding a layer of complexity to the image. The characters are likely related to the content of the poster, possibly providing information or instructions.\\n\\nThe overall style of the poster is a blend of cartoon and realism, with the left half being a whimsical cartoon and the right half a more realistic depiction of a city. The use of color and the inclusion of both cartoon and realistic elements create a visually engaging and informative piece.', 'everybody', ',0,']\r\n\r\nNote that, there is a correct caption in the batch, while others is wrong.\r\n\r\nAdditionally, If I use `gen_image_caption(images, batch_size=8, image_aspect_ratio='pad')`, the results are correct. It seems there are some problems relevant to `anyres` image aspect ratio.</BODY>\n\n<COMMENTS>\n<Comment by Luciennnnnnn at 2024-02-19T07:19:38Z>\n@haotian-liu cc\n</Comment>\n<Comment by lixiaotong97 at 2024-02-19T09:10:23Z>\nI also meet this problem, the output (with anyres) has only one correct caption in a batch.\n</Comment>\n<Comment by annopackage at 2024-02-20T07:58:03Z>\nHow did you set padding_side and attention_mask, which maybe influence batch inference?\n</Comment>\n<Comment by Luciennnnnnn at 2024-02-21T03:09:28Z>\nHi @annopackage, what do you mean `padding_side` and `attention_mask`, I the code above, I do not specify these two arguments, and I do not known what them refers to.\n</Comment>\n<Comment by rohit-gupta at 2024-03-11T17:50:08Z>\nThis feature would be useful\n</Comment>\n<Comment by gehong-coder at 2024-05-05T03:51:01Z>\n> ### 描述问题\r\n> 下面是我的代码片段，我想为我的图像生成标题。\r\n> \r\n> ```python\r\n> def gen_image_caption(self, imgs, temperature=0.2, top_p=0.7, num_beams=1, qs=None, max_new_tokens=512, batch_size=8, image_aspect_ratio=None):\r\n>         '''\r\n>         [PIL.Image, ...]\r\n>         '''\r\n>         image_sizes = [x.size for x in imgs]\r\n> \r\n>         images_tensor = process_images(\r\n>             imgs,\r\n>             self.image_processor,\r\n>             self.model.config,\r\n>             image_aspect_ratio=image_aspect_ratio)\r\n>         \r\n>         if isinstance(images_tensor, list):\r\n>             images_tensor = [x.to(self.device, dtype=torch.float16) for x in images_tensor]\r\n>             num_img = len(images_tensor)\r\n>         else:\r\n>             images_tensor = images_tensor.to(self.device, dtype=torch.float16)\r\n>             num_img = images_tensor.shape[0]\r\n> \r\n>         if batch_size == -1:\r\n>             batch_size = images_tensor.shape[0]\r\n> \r\n>         with torch.inference_mode():\r\n>             outputs = []\r\n>             for i in range(0, num_img, batch_size):\r\n>                 if isinstance(images_tensor, list):\r\n>                     bs = len(images_tensor[i : i + batch_size])\r\n>                 else:\r\n>                     bs = images_tensor[i : i + batch_size].shape[0]\r\n>                 input_ids = self.input_ids.repeat(bs, 1)\r\n>                 output_ids = self.model.generate(\r\n>                     input_ids,\r\n>                     images=images_tensor[i : i + batch_size],\r\n>                     image_sizes=image_sizes[i : i + batch_size],\r\n>                     do_sample=True if temperature > 0 else False,\r\n>                     temperature=temperature,\r\n>                     top_p=top_p,\r\n>                     num_beams=num_beams,\r\n>                     max_new_tokens=max_new_tokens,\r\n>                     use_cache=True)          \r\n>                 outputs += self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)\r\n> ```\r\n> \r\n> 提供我的图像是 PIL 图像列表，例如：`images = [Image, Image, Image ....]`\r\n> \r\n> 当我打电话时`gen_image_caption(images, batch_size=1)`，一切正常，字幕已正确生成。但是，当我调用 'gen_image_caption(images, batch_size=8)` 时，生成的标题看起来不正确且奇怪：\r\n> \r\n> 输出=['无人, 1, 1,.没有人,.\\n 没有人,.,,,.,.,.,.,.....,.,..\\n,.,.,.,.,.,..,.,.,. ,.,...\\n,....,..,.,..,.\\n,., 1..,0, 1. 1. 1. 1. 1. 1. 1. 1. 1, 1, 1, 1. 1. 1. 1. 1, 1,0,0, 1. 1. 1. 1, 1,0, 1, 1. 1. 1, 1. 1. 1,0, 0,0, 1. 1. 1. 1. 1,0, 1,0, 1,0,0,0,0, 1.0,. 1.1.1,0,0。 1, 1.0, 1.0, 1, 1. 1. 1, 1. 1,.0, 1, 1,. 1,0,.0,, 1,,, 1, 1, 1, 1, 1,0, 1. 1, 1.0, 1, 1, 1.0, 1. 1, 1, 1, 1, 1. 1, 1. 1, 1, 1, 1. 1, 1. 1, 1, 1. 1, 1, 1 1, 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ', '没人', '没人', '没人', ', ', '图像呈现出一张充满活力且色彩缤纷的海报，分为两个不同的两半。左半部分以一个卡通人物为主，一个戴着蓝色帽子、穿着绿色衬衫的欢快人物，热情地挥手致意。角色周围有各种物体和场景，包括汽车、卡车、建筑工地、工厂，所有这些都以俏皮和卡通的风格呈现。\\n\\n海报的右半部分形成了鲜明的对比，以真实的城市天际线描绘为特色。灰色和棕色色调的建筑在清澈的蓝天的映衬下高高耸立。城市景观上点缀着红白相间的警示标志，强烈提醒人们注意安全。\\n\\n海报文字丰富，汉字散布各处，为图像增添了一层复杂性。这些人物很可能与海报的内容相关，可能提供信息或说明。\\n\\n海报的整体风格是卡通与现实主义的融合，左半边是异想天开的卡通，右半边是更现实的一座城市的描绘。颜色的使用以及卡通和现实元素的加入创造了一个视觉上引人入胜且信息丰富的作品。', '每个人', ',0,']\r\n> \r\n> 请注意，该批次中有正确的标题，而其他则错误。\r\n> \r\n> 另外，如果我使用`gen_image_caption(images, batch_size=8, image_aspect_ratio='pad')`，结果是正确的。似乎存在一些与`anyres`图像长宽比相关的问题。\r\n\r\nthis code is right? I want achieve this function too, if update the code , please tell me too, thanks !!\n</Comment>\n<Comment by bryanwong17 at 2024-08-05T08:45:44Z>\nHi, could you kindly provided full code on how to do batch inference, given the batch of images and the question. Thank you!\n</Comment>\n<Comment by collinmccarthy at 2024-09-28T19:52:55Z>\nSee #1720 for the fix for this.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1147,
    "state": "open",
    "created_by": "Ryoo72",
    "created_at": "2024-02-18T08:23:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1147</URL>\n\n<TITLE>[Question] How do I train the image encoder?</TITLE>\n\n<BODY>### Question\n\nHello, thank you for creating such a great repository. I would like to tune the image encoder as well, since I want to train the model in a completely different domain. Could you possibly give me any hints on how to approach this?\r\n\r\nHere are my detailed questions:\r\n\r\n1. Can I tune the model by simply modifying the `requires_grad_` in `load_model` and removing the `torch.no_grad` decorator in the `forward` method within `llava/model/multimodal_encoder/clip_encoder.py`?\r\n2. To save and load the trained image encoder, should I modify `_save_checkpoint` in `llava/train/llava_trainer.py` to save it, and modify `load_model` in `CLIPVisionTower` to load it? \r\n\r\nI'm wondering if I'm missing anything. Thank you.</BODY>\n\n<COMMENTS>\n<Comment by GohioAC at 2024-04-20T05:44:07Z>\n@Ryoo72 did you figure this out?\n</Comment>\n<Comment by Ryoo72 at 2024-04-22T05:08:15Z>\n> GohioAC\r\n\r\nI think I did something, but I'm not sure\n</Comment>\n<Comment by bjzhb666 at 2025-03-07T07:19:00Z>\nHi, I want to do the same thing, are the mentioned steps enough? Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1144,
    "state": "closed",
    "created_by": "shipengai",
    "created_at": "2024-02-18T02:32:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1144</URL>\n\n<TITLE>[Usage] ValueError: The generation config instance is invalid</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: 在finetune阶段，存储模型失败。\r\ntransformers==4.37.2\r\ndeepspeed==0.12.6\r\n\r\nCommand:\r\n```\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path pretained_weights/vicuna-7b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path llava_v1_5_mix665k.json \\\r\n    --image_folder llava-stage2 \\\r\n    --vision_tower pretained_weights/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter checkpoints/llava-v1.5-7b-pretrain/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir checkpoints/llava-v1.5-7b \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to tensorboard\r\n```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/workdir/conda_envs/llava16/lib/python3.10/site-packages/transformers/trainer.py\", line 2873, in save_model\r\n    self._save(output_dir, state_dict=state_dict)\r\n  File \"/workdir/llava/train/llava_trainer.py\", line 255, in _save\r\n    super(LLaVATrainer, self)._save(output_dir, state_dict)\r\n  File \"/workdir/conda_envs/llava16/lib/python3.10/site-packages/transformers/trainer.py\", line 2958, in _save\r\n    self.model.save_pretrained(\r\n  File \"/workdir/conda_envs/llava16/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2364, in save_pretrained\r\n    model_to_save.generation_config.save_pretrained(save_directory)\r\n  File \"/workdir/conda_envs/llava16/lib/python3.10/site-packages/transformers/generation/configuration_utils.py\", line 560, in save_pretrained\r\n    raise ValueError(\r\nValueError: The generation config instance is invalid -- `.validate()` throws warnings and/or exceptions. Fix these issues to save the configuration.\r\n\r\nThrown during validation:\r\n[UserWarning('`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.'), UserWarning('`do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.')]\r\n\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by jsg921019 at 2024-02-18T06:37:15Z>\nThis error appears to be a problem that occurred while upgrading transformers version.\r\nI fixed this problem by manually adding\r\ndo_sample: true\r\nin vicuna's generation_config.json file.\n</Comment>\n<Comment by wentaoyuan at 2024-03-04T08:39:46Z>\nHi, I met the same issue too. Where did you guys find vicuna's `generation_config.json`? What I have to do is resetting the model's `generation_config` attributes after loading the model with its `from_pretrained` method at https://github.com/haotian-liu/LLaVA/blob/main/llava/train/train.py#L842, but doing this gives me the following warning:\r\n```\r\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they w\r\nill revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation\r\n config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\r\n```\n</Comment>\n<Comment by ppx-hub at 2024-03-05T06:52:53Z>\n> Hi, I met the same issue too. Where did you guys find vicuna's `generation_config.json`? What I have to do is resetting the model's `generation_config` attributes after loading the model with its `from_pretrained` method at https://github.com/haotian-liu/LLaVA/blob/main/llava/train/train.py#L842, but doing this gives me the following warning:\r\n> \r\n> ```\r\n> Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they w\r\n> ill revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation\r\n>  config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\r\n> ```\r\n\r\nOn my system, I found the soft-link named generation_config.json under the .cache cache in the /root/.cache/huggingface/hub/models--lmsys--vicuna-13b-v1.5/snapshots/ 3deb0106f72a3a433f0c6ea0cb978bdf14bcd3a6/ directory, I tried to add do_sample: true manually and it added the following, I'm not sure if this is working or not and am experimenting.\n</Comment>\n<Comment by ppx-hub at 2024-03-06T14:29:16Z>\n> > Hi, I met the same issue too. Where did you guys find vicuna's `generation_config.json`? What I have to do is resetting the model's `generation_config` attributes after loading the model with its `from_pretrained` method at https://github.com/haotian-liu/LLaVA/blob/main/llava/train/train.py#L842, but doing this gives me the following warning:\r\n> > ```\r\n> > Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they w\r\n> > ill revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation\r\n> >  config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\r\n> > ```\r\n> \r\n> On my system, I found the soft-link named generation_config.json under the .cache cache in the /root/.cache/huggingface/hub/models--lmsys--vicuna-13b-v1.5/snapshots/ 3deb0106f72a3a433f0c6ea0cb978bdf14bcd3a6/ directory, I tried to add do_sample: true manually and it added the following, I'm not sure if this is working or not and am experimenting.\r\n\r\nI have shown here that it works, the model is successfully saved\n</Comment>\n<Comment by boolmriver at 2024-03-17T06:12:01Z>\n> 此错误似乎是升级变压器版本时发生的问题。我通过在vicuna的generation_config.json文件中手动添加do_sample：true来解决此问题。\r\n\r\nThank U，bro\n</Comment>\n<Comment by XindiWu at 2024-04-16T15:37:25Z>\n> This error appears to be a problem that occurred while upgrading transformers version.\r\n> I fixed this problem by manually adding\r\n> do_sample: true\r\n> in vicuna's generation_config.json file.\r\n\r\nThanks! I changed the do_sample in the `configuration_utils.py` and that also works, on my local machine it's located at: \r\n`ananconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py`, you can find the corresponding file based on your env. For the `class GenerationConfig(PushToHubMixin), __init__, ` I changed `self.do_sample = kwargs.pop(\"do_sample\", False)` to be `self.do_sample = kwargs.pop(\"do_sample\", True)`.\n</Comment>\n<Comment by dsn01 at 2024-07-14T06:07:07Z>\n> > This error appears to be a problem that occurred while upgrading transformers version.\r\n> > I fixed this problem by manually adding\r\n> > do_sample: true\r\n> > in vicuna's generation_config.json file.\r\n> \r\n> Thanks! I changed the do_sample in the `configuration_utils.py` and that also works, on my local machine it's located at: `ananconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py`, you can find the corresponding file based on your env. For the `class GenerationConfig(PushToHubMixin), __init__, ` I changed `self.do_sample = kwargs.pop(\"do_sample\", False)` to be `self.do_sample = kwargs.pop(\"do_sample\", True)`.\r\n\r\nThank you bro!\n</Comment>\n<Comment by dacian7 at 2024-08-14T07:30:36Z>\n> lass GenerationConfig(PushToHubMixin), __init__,\r\n\r\nThanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1143,
    "state": "open",
    "created_by": "nxphi47",
    "created_at": "2024-02-17T10:28:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1143</URL>\n\n<TITLE>[Question] LLava-1.6-mistral - How many visual tokens given different resolution?</TITLE>\n\n<BODY>### Question\n\nGreat project!\r\n\r\nAs title, How many visual tokens given different resolution in LLava-1.6-mistral-7B (or Llava-1.5-HD).\r\n\r\nGiven ViT-L/14-336 has a seq length of 577, Is the total of visual tokens 577 x [(2,2), (1,2), (2,1), (1,3), (3,1), (1,4), (4,1)] depending on the grid ? Like 577 x (2,2) == 2308 tokens?\r\n\r\nThanks.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1142,
    "state": "open",
    "created_by": "babycommando",
    "created_at": "2024-02-16T18:06:12Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1142</URL>\n\n<TITLE>[Question] Finetuning LLaVA for Robots (making a multi turn conversation dataset with images)</TITLE>\n\n<BODY>### Question\n\nHello there LLaVA, beautiful work.\r\n\r\nI'm working on integrating LLaVA's vision capabilities with robotics through my project, [MachinaScript for Robots](https://github.com/babycommando/machinascript-for-robots), which interprets JSON-syntax commands for Arduino-based robots. While GPT-4-Vision has been beneficial, its cost and speed are prohibitive for broader use. Smaller models like BakLLaVA and Obsidian are faster but struggle with consistent JSON output.\r\n\r\nI aim to enhance robot decision-making with LLaVA by achieving two main goals:\r\n\r\n- Fine-Tuning for JSON Responses: Ensure LLaVA models generate precise JSON-formatted commands from image inputs.\r\n\r\n- Dataset for Multi-Turn Image Conversations: Develop a dataset for training on sequential image analysis, resembling video frames. Similar to Multi-Turn Chatbot conversation datasets.\r\n\r\nCould you provide guidance on the best format and approach for creating such a dataset? Quick, actionable advice on fine-tuning LLaVA models for these specific needs would be greatly appreciated.</BODY>\n\n<COMMENTS>\n<Comment by super-dainiu at 2024-02-17T14:44:42Z>\nFor json output, [sglang](https://lmsys.org/blog/2024-02-05-compressed-fsm/) is worth trying.\n</Comment>\n<Comment by babycommando at 2024-02-18T18:14:44Z>\nThanks. SGLang is insanely cool and I will definetly make some use of it.\r\n\r\nHowever this still does not answer my question directly about finetuning and generating a multi-turn dataset.\n</Comment>\n<Comment by super-dainiu at 2024-02-19T09:37:31Z>\nI am currently using [multi-token](https://github.com/sshh12/multi_token) for multi-turn finetuning. It encodes the conversation with multiple dialogues and images very easily.\r\n\r\nFor json problem, I guess sglang is enough!\n</Comment>\n<Comment by StarCycle at 2024-02-21T00:37:35Z>\nHello, I am doing similar thing with a smaller version of LLaVA (around 2.2B) [link](https://huggingface.co/StarCycle/llava-clip-internlm2-1_8b-pretrain-v1). But you can also look at [ROSGPT_vision](https://github.com/bilel-bj/ROSGPT_Vision)\n</Comment>\n<Comment by StarCycle at 2024-02-21T00:39:54Z>\nI will use LMDeploy for inference (but havent tries it on devices like Jetson)...\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1141,
    "state": "closed",
    "created_by": "yinincanada",
    "created_at": "2024-02-16T01:26:09Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1141</URL>\n\n<TITLE>[Question] disable FlashAttention</TITLE>\n\n<BODY>### Question\n\nhow to disable FlashAttention?</BODY>\n\n<COMMENTS>\n<Comment by yinincanada at 2024-02-16T14:44:53Z>\nfound the solution\n</Comment>\n<Comment by rzyfrank at 2024-03-19T06:13:19Z>\nhow to disable?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1140,
    "state": "open",
    "created_by": "yinincanada",
    "created_at": "2024-02-15T21:55:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1140</URL>\n\n<TITLE>[Usage] failure: pip install flash-attn --no-build-isolation</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: pip install flash-atten failure\r\n\r\nCommand:\r\n```\r\npip install flash-attn --no-build-isolation\r\n```\r\n\r\nLog: \r\n```\r\nCollecting flash-attn\r\n  Downloading flash_attn-2.5.3.tar.gz (2.5 MB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/2.5 MB 20.1 MB/s eta 0:00:00\r\n  Preparing metadata (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  × python setup.py egg_info did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> [13 lines of output]\r\n      fatal: not a git repository (or any parent up to mount point /)\r\n      Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n      \r\n      \r\n      torch.__version__  = 2.0.1+cu117\r\n      \r\n      \r\n      Traceback (most recent call last):\r\n        File \"<string>\", line 2, in <module>\r\n        File \"<pip-setuptools-caller>\", line 34, in <module>\r\n        File \"/tmp/pip-install-qg0blifr/flash-attn_a733b692349f44378614a22ba51e77c1/setup.py\", line 114, in <module>\r\n          raise RuntimeError(\r\n      RuntimeError: FlashAttention is only supported on CUDA 11.6 and above.  Note: make sure nvcc has a supported version by running nvcc -V.\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n× Encountered error while generating package metadata.\r\n╰─> See above for output.\r\n```\r\n\r\nScreenshots:\r\n<img width=\"805\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/149828936/ae7c265c-17cb-4208-9265-356c08c3ebca\">\r\n\r\n\r\nI checked my cuda version by running nvidia-smi,  I am having cuda version 12.2 (> required 11.6)\r\n<img width=\"515\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/149828936/df911801-5557-40a4-9f31-50ab891f6c89\">\r\n\r\nwhen running nvcc -V I am getting below\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Sun_Jul_28_19:07:16_PDT_2019\r\nCuda compilation tools, release 10.1, V10.1.243</BODY>\n\n<COMMENTS>\n<Comment by yinincanada at 2024-02-15T22:39:22Z>\nI think the cuda version listed by nvidia-smi is a bit misleading, and the actual version of cuda installed should be 10.1 as shown by nvcc -V.\n</Comment>\n<Comment by copperwiring at 2024-04-17T15:14:58Z>\nI had the same issue. Depends on the version of os (e.g. ubuntu), cuda version supported by nvidia might not have compatibility with cudo toolkit. \r\n\r\nCheck your nvidia cuda driver using `nvidia-smi` and see if it is compatible for 11.6 and above (that's what flash attention expects). You have 10.1\r\n\r\nIf you want to install 11.6, see here: https://forums.developer.nvidia.com/t/how-does-one-install-cuda-11-6-on-ubuntu-22-04/232137/2 or if you have Ubuntu 22, you can even get 11.8,  which you can get from here: https://developer.nvidia.com/cuda-11-8-0-download-archive?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=22.04\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1139,
    "state": "closed",
    "created_by": "CloudedLeopard17",
    "created_at": "2024-02-15T15:15:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1139</URL>\n\n<TITLE>[Usage]  Unable to load the  LLaVA-CC3M-Pretrain-595K dataset</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: I am trying to load the CC3M dataset using the datasets library but it is giving me the error **UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte**. Is anybody able to load the data correctly?</BODY>\n\n<COMMENTS>\n<Comment by yummyKnight at 2024-02-19T13:26:59Z>\nYou should directly download it from HF without using datasets library\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1138,
    "state": "closed",
    "created_by": "Nyandwi",
    "created_at": "2024-02-15T12:11:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1138</URL>\n\n<TITLE>[Usage] LlamaDecoderLayer.__init__() takes 2 positional arguments but 3 were given</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\nRunning/inferencing LLaVA models (based on LLaMA) \r\n```\r\n\r\nLog: \r\n```\r\nself.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\r\n  File \"..../models/llava/model/builder.py\", line 118, in load_pretrained_model\r\n    model = LlavaLlamaForCausalLM.from_pretrained(\r\n  File \"/home/jeandedi/miniconda3/envs/....../lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3594, in from_pretrained\r\n    model = cls(config, *model_args, **model_kwargs)\r\n  File \"......./models/llava/model/language_model/llava_llama.py\", line 46, in __init__\r\n    self.model = LlavaLlamaModel(config)\r\n  File \"......./models/llava/model/language_model/llava_llama.py\", line 38, in __init__\r\n    super(LlavaLlamaModel, self).__init__(config)\r\n  File \"......./models/llava/model/llava_arch.py\", line 32, in __init__\r\n    super(LlavaMetaModel, self).__init__(config)\r\n  File \"/home/jeandedi/miniconda3/envs/....../lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 958, in __init__\r\n    [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\r\n  File \"/home/jeandedi/miniconda3/envs/....../lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 958, in <listcomp>\r\n    [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\r\nTypeError: LlamaDecoderLayer.__init__() takes 2 positional arguments but 3 were given\r\n```\r\n\r\nThe issue does seem to come from transformers llama and it remains even for trying different versions of transformers. Is there a fix for this? I also wonder if it has connection with [[1075](https://github.com/haotian-liu/LLaVA/issues/1075)] but on different level.</BODY>\n\n<COMMENTS>\n<Comment by yangyingxiang at 2024-04-07T03:00:58Z>\n@Nyandwi how did you resolve the issue?\n</Comment>\n<Comment by Nyandwi at 2024-04-07T16:26:03Z>\nThe issue was caused by another model that was modifying the LLaMA model from transformers. Basically, it was modifying the transformers package. I removed/commented everything else that was using transformers in my codebase and issue was solved.\n</Comment>\n<Comment by ZzeshengWang at 2024-10-04T14:49:00Z>\nI think the problem is caused by the version of transformers. Try `pip install transformers==4.32.1`. It seems work.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1137,
    "state": "open",
    "created_by": "shersoni610",
    "created_at": "2024-02-15T08:00:10Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1137</URL>\n\n<TITLE>Image Size</TITLE>\n\n<BODY>### Question\n\nHello,\r\n\r\nAs per the document, Llava1.6 supports three image sizes: 672x672, 336x1344, 1344x336\r\nCan someone explain\r\n1,  Why such non-standard sizes were chosen?\r\n2.  Should a user need to resize the image as per the above requirements? Since 1344X136 may not keep the aspect ratio of the original image, should we do the padding or it is done internally?</BODY>\n\n<COMMENTS>\n<Comment by codearranger at 2024-03-03T00:00:13Z>\n@haotian-liu\n</Comment>\n<Comment by CloudedLeopard17 at 2024-03-18T14:22:58Z>\nThe padding is done internally. The images are converted to square images. But it seems the images less than 336x336 are not working correctly\n</Comment>\n<Comment by david-vectorflow at 2024-04-09T14:48:36Z>\nDoes it maintain the aspect ratio when it coverts?\n</Comment>\n<Comment by KansaiUser at 2024-08-27T10:17:26Z>\nI would also like an explanation about this. My images are of size 3072x 1856  and 3384x 2710  . What does Llava do with these images?\r\nI am using them both with the code of this repo as well as using Llava from Ollama\r\n\r\nI would like to know what operation is being done on the images, or if I should crop them myself\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1136,
    "state": "open",
    "created_by": "shersoni610",
    "created_at": "2024-02-15T03:17:27Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1136</URL>\n\n<TITLE>Naming llava models</TITLE>\n\n<BODY>### Question\n\nHello\r\n\r\nSome confusing models name: Llava-Plus, Llava-1.6 and Llava-Next.  Which one is the latest and gives the best overall results?</BODY>\n\n<COMMENTS>\n<Comment by ChunyuanLI at 2024-02-15T13:41:48Z>\n- **Llava-Next** is the rename of Llava-1.6 (the name Llava-1.6 is deprecated), which provides _the best overall results as a single Large Multimodal Model (LMM)_. The document is at: https://llava-vl.github.io/blog/2024-01-30-llava-next/\r\n\r\n- **Llava-Plus** (Plug and Learn to Use Skills), is an extension of LLaVA/LMM to build multimodal agent with external tool use. The document is at: https://arxiv.org/abs/2311.05437\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1134,
    "state": "closed",
    "created_by": "uriva",
    "created_at": "2024-02-14T18:52:39Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1134</URL>\n\n<TITLE>Trying to run on modal.com but encountering an issue with bitsandbytes</TITLE>\n\n<BODY>```\r\n/usr/local/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\r\n```\r\n\r\n\r\nMy code:\r\n\r\n```python\r\ndef load_model():\r\n    from llava.model import LlavaLlamaForCausalLM\r\n    \r\n    model_path = \"4bit/llava-v1.5-13b-3GB\"\r\n    model = LlavaLlamaForCausalLM.from_pretrained(\r\n        model_path,\r\n        low_cpu_mem_usage=True,\r\n        device_map=\"auto\",\r\n        load_in_4bit=True,\r\n        quantization_config=BitsAndBytesConfig(\r\n            load_in_4bit=True,\r\n            bnb_4bit_compute_dtype=torch.float16,\r\n            bnb_4bit_use_double_quant=True,\r\n            bnb_4bit_quant_type=\"nf4\",\r\n        ),\r\n    )\r\n    vision_tower = model.get_vision_tower()\r\n    if not vision_tower.is_loaded:\r\n        vision_tower.load_model()\r\n    vision_tower.to(device=\"cuda\")\r\n    return (\r\n        model,\r\n        AutoTokenizer.from_pretrained(model_path, use_fast=False),\r\n        vision_tower.image_processor,\r\n    )\r\n```\r\n\r\nmy modal.com config:\r\n\r\n```python\r\nstub.function(\r\n    image=(\r\n        modal.Image.debian_slim() # also tried modal.Image.from_registry(\"nvidia/cuda:12.1.0-base-ubuntu22.04\", add_python=\"3.10\") with the same result\r\n        )\r\n        .pip_install(\"gamla\")\r\n        .apt_install(\"git\")\r\n        .run_commands(\r\n            [\r\n                \"cd home && git clone -b v1.0 https://github.com/camenduru/LLaVA\",\r\n                \"cd home/LLaVA && pip install .\",\r\n            ]\r\n        )\r\n        .run_function(worker.load_model)\r\n    ),\r\n    gpu=gpu.T4(),\r\n)\r\n```</BODY>\n\n<COMMENTS>\n<Comment by uriva at 2024-02-14T21:11:30Z>\nI think I was missing this:\r\n```\r\nfrom modal import Image\r\n\r\nimage = (\r\n    Image.debian_slim()\r\n    .pip_install(\"bitsandbytes\", gpu=\"any\")\r\n)\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1133,
    "state": "open",
    "created_by": "ninatu",
    "created_at": "2024-02-14T17:25:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1133</URL>\n\n<TITLE>[Question] What splits of DocVQA, SynDog-EN, ChartQA, DVQA, AI2D were used in LLaVA1.6 training?</TITLE>\n\n<BODY>### Question\n\nHi! I wonder what splits of DocVQA, SynDog-EN, ChartQA, DVQA, AI2D were used in the training of the LLaVA 1.6 model? Only training splits or validation splits as well?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1132,
    "state": "open",
    "created_by": "shersoni610",
    "created_at": "2024-02-14T14:51:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1132</URL>\n\n<TITLE>How to change the output language</TITLE>\n\n<BODY>### Describe the issue\n\nHello,\r\n\r\nI am using \"ollama run llava\". The output is in non-english language. How do I change it.?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1131,
    "state": "open",
    "created_by": "vishalkmr",
    "created_at": "2024-02-14T12:56:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1131</URL>\n\n<TITLE>[Error] following `model_kwargs` are not used by the model: ['image_sizes'</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: i am using the bellow code, a modified version of cli.py code, to query a local image.\r\n\r\n```import os\r\nimport sys\r\nsys.path.append(os.getcwd()+ \"/LLaVA/\")\r\nimport argparse\r\nimport torch\r\n\r\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\r\nfrom llava.conversation import conv_templates, SeparatorStyle\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.utils import disable_torch_init\r\nfrom llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\r\n\r\nfrom PIL import Image\r\n\r\nimport requests\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\nfrom transformers import TextStreamer\r\n\r\n\r\ndef load_image(image_file):\r\n    if image_file.startswith('http://') or image_file.startswith('https://'):\r\n        response = requests.get(image_file)\r\n        image = Image.open(BytesIO(response.content)).convert('RGB')\r\n    else:\r\n        image = Image.open(image_file).convert('RGB')\r\n    return image\r\n\r\nmodel_path=\"liuhaotian/LLaVA-Lightning-MPT-7B-preview\"\r\nmodel_base=None\r\nload_8bit=False\r\nload_4bit=False\r\ntemperature = 0.2\r\nmax_new_tokens = 512\r\nimage_file = \"/localhome/local-vishkumar/gen-ai-app/streams/llava_logo.png\"\r\n\r\ndisable_torch_init()\r\n\r\nmodel_name = get_model_name_from_path(model_path)\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(model_path, model_base, model_name, load_8bit, load_4bit)\r\n\r\nif \"llama-2\" in model_name.lower():\r\n    conv_mode = \"llava_llama_2\"\r\nelif \"mistral\" in model_name.lower():\r\n    conv_mode = \"mistral_instruct\"\r\nelif \"v1.6-34b\" in model_name.lower():\r\n    conv_mode = \"chatml_direct\"\r\nelif \"v1\" in model_name.lower():\r\n    conv_mode = \"llava_v1\"\r\nelif \"mpt\" in model_name.lower():\r\n    conv_mode = \"mpt\"\r\nelse:\r\n    conv_mode = \"llava_v0\"\r\n\r\nif conv_mode is not None and conv_mode != conv_mode:\r\n    print('[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}'.format(conv_mode, conv_mode, conv_mode))\r\nelse:\r\n    conv_mode = conv_mode\r\n\r\nconv = conv_templates[conv_mode].copy()\r\nif \"mpt\" in model_name.lower():\r\n    roles = ('user', 'assistant')\r\nelse:\r\n    roles = conv.roles\r\n\r\nimage = load_image(image_file)\r\nimage_size = image.size\r\n\r\nimage_tensor = process_images([image], image_processor, model.config)\r\nif type(image_tensor) is list:\r\n    image_tensor = [image.to(model.device, dtype=torch.float16) for image in image_tensor]\r\nelse:\r\n    image_tensor = image_tensor.to(model.device, dtype=torch.float16)\r\n\r\n\r\ninp = \"Describe the image\"\r\nprint(f\"{roles[0]}: {inp}\")\r\nprint(f\"{roles[1]}: \", end=\"\")\r\n\r\nif image is not None:\r\n    # first message\r\n    if model.config.mm_use_im_start_end:\r\n        inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + inp\r\n    else:\r\n        inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\r\n    conv.append_message(conv.roles[0], inp)\r\n    image = None\r\nelse:\r\n    conv.append_message(conv.roles[0], inp)\r\nconv.append_message(conv.roles[1], None)\r\nprompt = conv.get_prompt()\r\n\r\ninput_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)\r\nstop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\r\nkeywords = [stop_str]\r\nstreamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\r\n\r\nwith torch.inference_mode():\r\n    output_ids = model.generate(\r\n        input_ids,\r\n        images=image_tensor,\r\n        image_sizes=[image_size],\r\n        do_sample=True if temperature > 0 else False,\r\n        temperature=temperature,\r\n        max_new_tokens=max_new_tokens,\r\n        streamer=streamer,\r\n        use_cache=True)\r\n\r\noutputs = tokenizer.decode(output_ids[0]).strip()\r\nconv.messages[-1][-1] = outputs\r\n\r\nprint(\"\\n\", {\"prompt\": prompt, \"outputs\": outputs}, \"\\n\")\r\n```\r\n\r\n\r\n\r\n\r\nError Message: \r\n```\r\nuser: Describe the image\r\nassistant: Traceback (most recent call last):\r\n  File \"/localhome/local-vishkumar/gen-ai-app/src/test.py\", line 100, in <module>\r\n    output_ids = model.generate(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/localhome/local-vishkumar/.local/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1307, in generate\r\n    self._validate_model_kwargs(model_kwargs.copy())\r\n  File \"/localhome/local-vishkumar/.local/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1122, in _validate_model_kwargs\r\n    raise ValueError(\r\nValueError: The following `model_kwargs` are not used by the model: ['image_sizes'] (note: typos in the generate arguments will also show up in this list)\r\n\r\n```\r\n\r\n\r\nCan anyone please tell me why i am getting this issue??</BODY>\n\n<COMMENTS>\n<Comment by vishalkmr at 2024-02-15T07:14:48Z>\nchanging the model to model_path=\"liuhaotian/llava-v1.5-13b\" resolved the issue and I'm getting the image description now.\r\nSeems like there is bug in LLaVA-Lightning-MPT-7B-preview model path.\n</Comment>\n<Comment by idan-tankel at 2024-03-22T12:38:41Z>\ngetting this also to llava-hf/llava-v1.6-mistral-7b-hf\n</Comment>\n<Comment by chanangad at 2024-04-18T15:22:20Z>\ngetting this error in MPT-7b model too\n</Comment>\n<Comment by amitakamath at 2025-01-21T03:02:13Z>\nComing to this late, but I got the same error because I was using AutoProcessor and LlavaForConditionalGeneration with llava-1.6 models under Huggingface. When I switched to LlavaNextProcessor and LlavaNextForConditionalGeneration, the error went away. Hope this helps!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1130,
    "state": "closed",
    "created_by": "cosmosanalytics",
    "created_at": "2024-02-13T21:21:14Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1130</URL>\n\n<TITLE>I followed the steps exactly, yet I got the following error message, what should I do??</TITLE>\n\n<BODY>### Describe the issue\n\n![image](https://github.com/haotian-liu/LLaVA/assets/100080151/5997a4db-65f3-4b28-ba09-700b94443216)\r\n\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-15-c05d8f95fe9d> in <cell line: 24>()\r\n     22 })()\r\n     23 \r\n---> 24 eval_model(args)\r\n\r\n3 frames\r\n/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py in _load_pretrained_model(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\r\n   3977             is_safetensors = archive_file.endswith(\".safetensors\")\r\n   3978             if offload_folder is None and not is_safetensors:\r\n-> 3979                 raise ValueError(\r\n   3980                     \"The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder`\"\r\n   3981                     \" for them. Alternatively, make sure you have `safetensors` installed if the model you are using\"\r\n\r\nValueError: The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format.</BODY>\n\n<COMMENTS>\n<Comment by cosmosanalytics at 2024-02-14T15:14:04Z>\nresolved by colab notebook\r\n![image](https://github.com/haotian-liu/LLaVA/assets/100080151/4947ba83-f19b-498d-8653-c66f76d04fd5)\n</Comment>\n<Comment by KansaiTraining at 2024-06-25T14:19:22Z>\nthis is not a solution...\n</Comment>\n<Comment by suyanli220 at 2024-10-08T08:08:45Z>\ncan I ask how the problem be solved?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1129,
    "state": "open",
    "created_by": "pseudotensor",
    "created_at": "2024-02-13T18:07:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1129</URL>\n\n<TITLE>[BUG] ../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.</TITLE>\n\n<BODY>### Describe the issue\r\n\r\n@haotian-liu I had worker-server-controller up for a while and eventually hit on the worker.  Someting overflowing the dimensions sometimes?  Is it related to bad image size or length of input?  Maybe why you have hard cutoffs?  I didn't change the cutoffs.\r\n\r\n```\r\n2024-02-13 17:13:55 | INFO | model_worker | Send heart beat. Models: ['llava-v1.6-34b']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1417\r\n2024-02-13 17:13:55 | INFO | stdout | INFO:     38.128.232.144:18062 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [392,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n:\r\n...................many more\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [375,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n2024-02-13 17:13:56 | ERROR | stderr | Exception in thread Thread-1342 (generate):\r\n2024-02-13 17:13:56 | ERROR | stderr | Traceback (most recent call last):\r\n2024-02-13 17:13:56 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\r\n2024-02-13 17:13:56 | ERROR | stderr |     self.run()\r\n2024-02-13 17:13:56 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/threading.py\", line 953, in run\r\n2024-02-13 17:13:56 | ERROR | stderr |     self._target(*self._args, **self._kwargs)\r\n2024-02-13 17:13:56 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n2024-02-13 17:13:56 | ERROR | stderr |     return func(*args, **kwargs)\r\n2024-02-13 17:13:56 | ERROR | stderr |   File \"/home/ubuntu/h2oai_llava/llava/model/language_model/llava_llama.py\", line 137, in generate\r\n2024-02-13 17:13:56 | ERROR | stderr |     return super().generate(\r\n2024-02-13 17:13:56 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n2024-02-13 17:13:56 | ERROR | stderr |     return func(*args, **kwargs)\r\n2024-02-13 17:13:56 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1572, in generate\r\n2024-02-13 17:13:56 | ERROR | stderr |     # 11. prepare logits warper\r\n2024-02-13 17:13:56 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 700, in _prepare_attention_mask_for_generation\r\n2024-02-13 17:13:56 | ERROR | stderr |     if generation_config.typical_p is not None and generation_config.typical_p < 1.0:\r\n2024-02-13 17:13:56 | ERROR | stderr |   File \"/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_tensor.py\", line 997, in __contains__\r\n2024-02-13 17:13:56 | ERROR | stderr |     category=torch.jit.TracerWarning,\r\n2024-02-13 17:13:56 | ERROR | stderr | RuntimeError: CUDA error: device-side assert triggered\r\n2024-02-13 17:13:56 | ERROR | stderr | CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n2024-02-13 17:13:56 | ERROR | stderr | For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\n2024-02-13 17:13:56 | ERROR | stderr | Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n2024-02-13 17:13:56 | ERROR | stderr | \r\n2024-02-13 17:14:04 | INFO | model_worker | Send heart beat. Models: \r\n```\r\n\r\nSimilar error here:\r\nhttps://github.com/haotian-liu/LLaVA/issues/840\r\nhttps://github.com/haotian-liu/LLaVA/issues/603\r\n\r\n\r\nOnce this happens, the worker is dead.</BODY>\n\n<COMMENTS>\n<Comment by whalefa1I at 2024-02-20T08:09:31Z>\nwhat is your images resolutions\n</Comment>\n<Comment by dcreinerth at 2024-06-12T08:37:39Z>\nI had the same error. I run the LLaVA setup inside docker containers and start them with docker compose. \r\nThe weird thing is that on one server it works without any issues. On a different server with a difference Nvidia GPU I get this exact same error.\n</Comment>\n<Comment by Purshow at 2025-02-22T17:41:58Z>\nsame question here! Have you solved it?\n</Comment>\n<Comment by dcreinerth at 2025-02-25T10:48:30Z>\nThe model architecture is loaded based on the folder in which the files are stored. Check if the model folder has \"llava\" or the proper string for the correct selection in the name. \nThe code for model selection based on folder name is here: llava/model/builder.py\nFor me I didnt used the proper model name so a wrong architecture is loaded causing this error. The problem was not connected to CUDA. \nMaybe this helps someone.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1128,
    "state": "open",
    "created_by": "prashnm",
    "created_at": "2024-02-13T11:52:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1128</URL>\n\n<TITLE>[Discussion] LLaVa for Price estimation?</TITLE>\n\n<BODY>### Discussion\n\nI wanted to finetune llava , by giving an input image ex: image of a damaged car and some description about the damage in the human convs , and providing some damage estimate amount as GT \r\n{\r\n        \"from\": \"gpt\",\r\n        \"value\": Estimated Amount for the damage : \"5000.0 USD\"\r\n      }\r\n . Can Llava predict the estimate for unseen inputs?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1127,
    "state": "open",
    "created_by": "prashnm",
    "created_at": "2024-02-13T11:51:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1127</URL>\n\n<TITLE>LLava for Regression?[Question]</TITLE>\n\n<BODY>### Question\n\nI wanted to finetune llava , by giving an input image ex: image of a damaged car and some description about the damage in the human convs , and providing some damage estimate amount as GT \r\n{\r\n        \"from\": \"gpt\",\r\n        \"value\": Estimated Amount for the damage : \"5000.0 USD\"\r\n      }\r\n . Can Llava predict the estimate for unseen inputs?</BODY>\n\n<COMMENTS>\n<Comment by annopackage at 2024-02-20T07:57:05Z>\nHi, do you conduct some experiments about this, and how about results?\n</Comment>\n<Comment by prashnm at 2024-02-20T08:19:21Z>\nSure I am going to do some experiments on this and will definitely share the results, I just wanted your thoughts on this? like what are the chances for model to work on this specific use case, finetuning with limited samples lets say 200\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1126,
    "state": "open",
    "created_by": "rubency",
    "created_at": "2024-02-13T11:12:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1126</URL>\n\n<TITLE>[Question]  Clarification On the Input Image Size</TITLE>\n\n<BODY>### Question  \r\n\r\nwhat is the Maximum Input image size for LLaVA ? Are there any considerations or recommendations for handling input sizes near or exceeding these limits?</BODY>\n\n<COMMENTS>\n<Comment by david-vectorflow at 2024-04-09T14:47:13Z>\nI would also like to know this\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1125,
    "state": "open",
    "created_by": "cherry956",
    "created_at": "2024-02-13T01:56:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1125</URL>\n\n<TITLE>[Question] How to fix the maximum sequence length of the model</TITLE>\n\n<BODY>### Question\n\nHow to fix the maximum sequence length of the model? When I using finetuned model to test my own data, I found error: Token indices sequence length is longer than the specified maximum sequence length for this model(2199>2048).\r\n![image](https://github.com/haotian-liu/LLaVA/assets/144820412/5ac2ab12-b1f3-4692-bd72-bc4983831b32)\r\nDo I need to fix weights in this script?\r\n![image](https://github.com/haotian-liu/LLaVA/assets/144820412/26a2f6f8-da71-41a9-aa84-29f4b3648c1f)\r\nThanks!!!</BODY>\n\n<COMMENTS>\n<Comment by crazysal at 2024-02-15T00:23:36Z>\nsame doubt\n</Comment>\n<Comment by cherry956 at 2024-02-15T09:34:40Z>\n@crazysal Do you solve it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1123,
    "state": "open",
    "created_by": "F4k3r22",
    "created_at": "2024-02-12T22:59:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1123</URL>\n\n<TITLE>How can I give multimodal capabilities to my Llama 2 model that I have locally with its dataset?</TITLE>\n\n<BODY>### Question\n\nHow can I give multimodal capabilities to my Llama 2 model that I have locally with its dataset? I have started reading some of your documentation interested in using your key data set, to be able to give multimodal capabilities to my AI chatbot project with llama 2, and it has not been very clear to me how to do it, can you explain to me how I can do it?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1122,
    "state": "open",
    "created_by": "RonanKMcGovern",
    "created_at": "2024-02-12T11:17:23Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1122</URL>\n\n<TITLE>[Usage] Error loading v1.6 models</TITLE>\n\n<BODY>### Describe the issue\n\nIssue/Error:\r\nLoading 1.5 models works fine, but loading 1.6 models yield the error below. Note that the 1.6 models do load (despite the error) and inference works. However, training the 1.6 model results in OOM (unlike the 1.5 models which train fine - specifically training 7B models in bf16 on a 48GB A6000).\r\n\r\nCommand:\r\n```\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.mm_utils import get_model_name_from_path\r\nfrom llava.eval.run_llava import eval_model\r\n\r\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\nmodel_path = \"liuhaotian/llava-v1.6-mistral-7b\"\r\n\r\nmodel_name=get_model_name_from_path(model_path)\r\n\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    model_path=model_path,\r\n    model_base=None,\r\n    model_name=model_name,\r\n    # load_8bit=True\r\n    # load_4bit=True\r\n) \r\n```\r\n\r\nLog: \r\n```\r\n/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.class_embedding: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\r\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass assign=\r\n```</BODY>\n\n<COMMENTS>\n<Comment by sgdescent at 2024-02-12T16:12:51Z>\nIf I may ask, are your inference results good? I am trying to infer but seems the generated output ends weirdly. I use a single A-100 to generate an output\n</Comment>\n<Comment by RonanKMcGovern at 2024-02-12T16:34:19Z>\nYup, inference is good.\n\nBtw I modified float16 in the builder.py to be bfloat16. This is needed for stability in training.\n</Comment>\n<Comment by aliencaocao at 2024-02-14T03:36:41Z>\nhow did you train? i thought the training codes are not out?\n</Comment>\n<Comment by aliencaocao at 2024-02-14T03:37:18Z>\n> If I may ask, are your inference results good? I am trying to infer but seems the generated output ends weirdly. I use a single A-100 to generate an output\r\n\r\nCheck out https://github.com/haotian-liu/LLaVA/pull/1115 see if it solves your issue\n</Comment>\n<Comment by RonanKMcGovern at 2024-02-14T12:01:43Z>\nMany thanks @aliencaocao ! I just tried those patches.\r\n\r\nThey fixed errors with not having set a padding token or attention mask for generation.\r\n\r\nHowever, the 'copying from a non-meta parameter' error persists, and my training is still oom.\r\n\r\nI'm just using a transformers trainer with everything loaded in bf16. BUT:\r\n```\r\nOutOfMemoryError                          Traceback (most recent call last)\r\nCell In[24], line 44\r\n     11 training_args = TrainingArguments(\r\n     12     output_dir=f\"{model_name}-chess\",\r\n     13     learning_rate=1e-4,\r\n   (...)\r\n     33     optim=\"adamw_torch\",\r\n     34 )\r\n     36 trainer = Trainer(\r\n     37     model=model,\r\n     38     args=training_args,\r\n   (...)\r\n     41     # compute_loss=compute_loss,  # Pass the custom compute_loss function\r\n     42 )\r\n---> 44 trainer.train()\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1539, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\r\n   1537         hf_hub_utils.enable_progress_bars()\r\n   1538 else:\r\n-> 1539     return inner_training_loop(\r\n   1540         args=args,\r\n   1541         resume_from_checkpoint=resume_from_checkpoint,\r\n   1542         trial=trial,\r\n   1543         ignore_keys_for_eval=ignore_keys_for_eval,\r\n   1544     )\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1869, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\r\n   1866     self.control = self.callback_handler.on_step_begin(args, self.state, self.control)\r\n   1868 with self.accelerator.accumulate(model):\r\n-> 1869     tr_loss_step = self.training_step(model, inputs)\r\n   1871 if (\r\n   1872     args.logging_nan_inf_filter\r\n   1873     and not is_torch_tpu_available()\r\n   1874     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\r\n   1875 ):\r\n   1876     # if loss is nan or inf simply add the average of previous logged losses\r\n   1877     tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2772, in Trainer.training_step(self, model, inputs)\r\n   2769     return loss_mb.reduce_mean().detach().to(self.args.device)\r\n   2771 with self.compute_loss_context_manager():\r\n-> 2772     loss = self.compute_loss(model, inputs)\r\n   2774 if self.args.n_gpu > 1:\r\n   2775     loss = loss.mean()  # mean() to average on multi-gpu parallel training\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2795, in Trainer.compute_loss(self, model, inputs, return_outputs)\r\n   2793 else:\r\n   2794     labels = None\r\n-> 2795 outputs = model(**inputs)\r\n   2796 # Save past state if it exists\r\n   2797 # TODO: this needs to be fixed and made cleaner later.\r\n   2798 if self.args.past_index >= 0:\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518, in Module._wrapped_call_impl(self, *args, **kwargs)\r\n   1516     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\r\n   1517 else:\r\n-> 1518     return self._call_impl(*args, **kwargs)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527, in Module._call_impl(self, *args, **kwargs)\r\n   1522 # If we don't have any hooks, we want to skip the rest of the logic in\r\n   1523 # this function, and just call forward.\r\n   1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\n   1525         or _global_backward_pre_hooks or _global_backward_hooks\r\n   1526         or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1527     return forward_call(*args, **kwargs)\r\n   1529 try:\r\n   1530     result = None\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:581, in convert_outputs_to_fp32.<locals>.forward(*args, **kwargs)\r\n    580 def forward(*args, **kwargs):\r\n--> 581     return model_forward(*args, **kwargs)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:569, in ConvertOutputsToFp32.__call__(self, *args, **kwargs)\r\n    568 def __call__(self, *args, **kwargs):\r\n--> 569     return convert_to_fp32(self.model_forward(*args, **kwargs))\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:16, in autocast_decorator.<locals>.decorate_autocast(*args, **kwargs)\r\n     13 @functools.wraps(func)\r\n     14 def decorate_autocast(*args, **kwargs):\r\n     15     with autocast_instance:\r\n---> 16         return func(*args, **kwargs)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/peft/peft_model.py:537, in PeftModel.forward(self, *args, **kwargs)\r\n    533 def forward(self, *args: Any, **kwargs: Any):\r\n    534     \"\"\"\r\n    535     Forward pass of the model.\r\n    536     \"\"\"\r\n--> 537     return self.get_base_model()(*args, **kwargs)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518, in Module._wrapped_call_impl(self, *args, **kwargs)\r\n   1516     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\r\n   1517 else:\r\n-> 1518     return self._call_impl(*args, **kwargs)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527, in Module._call_impl(self, *args, **kwargs)\r\n   1522 # If we don't have any hooks, we want to skip the rest of the logic in\r\n   1523 # this function, and just call forward.\r\n   1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\n   1525         or _global_backward_pre_hooks or _global_backward_hooks\r\n   1526         or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1527     return forward_call(*args, **kwargs)\r\n   1529 try:\r\n   1530     result = None\r\n\r\nFile /workspace/LLaVA/llava/model/language_model/llava_mistral.py:91, in LlavaMistralForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, image_sizes, return_dict)\r\n     73 if inputs_embeds is None:\r\n     74     (\r\n     75         input_ids,\r\n     76         position_ids,\r\n   (...)\r\n     88         image_sizes\r\n     89     )\r\n---> 91 return super().forward(\r\n     92     input_ids=input_ids,\r\n     93     attention_mask=attention_mask,\r\n     94     position_ids=position_ids,\r\n     95     past_key_values=past_key_values,\r\n     96     inputs_embeds=inputs_embeds,\r\n     97     labels=labels,\r\n     98     use_cache=use_cache,\r\n     99     output_attentions=output_attentions,\r\n    100     output_hidden_states=output_hidden_states,\r\n    101     return_dict=return_dict\r\n    102 )\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py:1154, in MistralForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\r\n   1151 return_dict = return_dict if return_dict is not None else self.config.use_return_dict\r\n   1153 # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\r\n-> 1154 outputs = self.model(\r\n   1155     input_ids=input_ids,\r\n   1156     attention_mask=attention_mask,\r\n   1157     position_ids=position_ids,\r\n   1158     past_key_values=past_key_values,\r\n   1159     inputs_embeds=inputs_embeds,\r\n   1160     use_cache=use_cache,\r\n   1161     output_attentions=output_attentions,\r\n   1162     output_hidden_states=output_hidden_states,\r\n   1163     return_dict=return_dict,\r\n   1164 )\r\n   1166 hidden_states = outputs[0]\r\n   1167 logits = self.lm_head(hidden_states)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518, in Module._wrapped_call_impl(self, *args, **kwargs)\r\n   1516     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\r\n   1517 else:\r\n-> 1518     return self._call_impl(*args, **kwargs)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527, in Module._call_impl(self, *args, **kwargs)\r\n   1522 # If we don't have any hooks, we want to skip the rest of the logic in\r\n   1523 # this function, and just call forward.\r\n   1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\n   1525         or _global_backward_pre_hooks or _global_backward_hooks\r\n   1526         or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1527     return forward_call(*args, **kwargs)\r\n   1529 try:\r\n   1530     result = None\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py:1039, in MistralModel.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\r\n   1029     layer_outputs = self._gradient_checkpointing_func(\r\n   1030         decoder_layer.__call__,\r\n   1031         hidden_states,\r\n   (...)\r\n   1036         use_cache,\r\n   1037     )\r\n   1038 else:\r\n-> 1039     layer_outputs = decoder_layer(\r\n   1040         hidden_states,\r\n   1041         attention_mask=attention_mask,\r\n   1042         position_ids=position_ids,\r\n   1043         past_key_value=past_key_values,\r\n   1044         output_attentions=output_attentions,\r\n   1045         use_cache=use_cache,\r\n   1046     )\r\n   1048 hidden_states = layer_outputs[0]\r\n   1050 if use_cache:\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518, in Module._wrapped_call_impl(self, *args, **kwargs)\r\n   1516     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\r\n   1517 else:\r\n-> 1518     return self._call_impl(*args, **kwargs)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527, in Module._call_impl(self, *args, **kwargs)\r\n   1522 # If we don't have any hooks, we want to skip the rest of the logic in\r\n   1523 # this function, and just call forward.\r\n   1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\n   1525         or _global_backward_pre_hooks or _global_backward_hooks\r\n   1526         or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1527     return forward_call(*args, **kwargs)\r\n   1529 try:\r\n   1530     result = None\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py:751, in MistralDecoderLayer.forward(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\r\n    735 \"\"\"\r\n    736 Args:\r\n    737     hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\r\n   (...)\r\n    746     past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\r\n    747 \"\"\"\r\n    749 residual = hidden_states\r\n--> 751 hidden_states = self.input_layernorm(hidden_states)\r\n    753 # Self Attention\r\n    754 hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n    755     hidden_states=hidden_states,\r\n    756     attention_mask=attention_mask,\r\n   (...)\r\n    760     use_cache=use_cache,\r\n    761 )\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518, in Module._wrapped_call_impl(self, *args, **kwargs)\r\n   1516     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\r\n   1517 else:\r\n-> 1518     return self._call_impl(*args, **kwargs)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527, in Module._call_impl(self, *args, **kwargs)\r\n   1522 # If we don't have any hooks, we want to skip the rest of the logic in\r\n   1523 # this function, and just call forward.\r\n   1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\n   1525         or _global_backward_pre_hooks or _global_backward_hooks\r\n   1526         or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1527     return forward_call(*args, **kwargs)\r\n   1529 try:\r\n   1530     result = None\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py:87, in MistralRMSNorm.forward(self, hidden_states)\r\n     85 hidden_states = hidden_states.to(torch.float32)\r\n     86 variance = hidden_states.pow(2).mean(-1, keepdim=True)\r\n---> 87 hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\r\n     88 return self.weight * hidden_states.to(input_dtype)\r\n\r\nOutOfMemoryError: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacty of 47.54 GiB of which 34.12 MiB is free. Process 3061903 has 47.49 GiB memory in use. Of the allocated memory 46.17 GiB is allocated by PyTorch, and 1023.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n```\n</Comment>\n<Comment by aliencaocao at 2024-02-14T13:38:20Z>\nWell I cant do much about OOM. But I thought the repo said training code to be released...so you just used transformers trainer and it worked?\n</Comment>\n<Comment by RonanKMcGovern at 2024-02-14T15:34:58Z>\nOk, so (perhaps obviously) the OOM error was fixed by adding more VRAM. It takes me 100 GB of VRAM to fine-tune the Llava 1.6 7B model in bf16. That is 2x what it takes to fine-tune the Llava 1.5B model.\r\n\r\nThe loading error remains so I'll keep this issue open:\r\n```\r\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\r\n```\r\n\r\nAnd yes, aliencaocao, the huggingface trainer works (although will be unstable with fp16).\n</Comment>\n<Comment by RonanKMcGovern at 2024-02-20T17:02:44Z>\nThe whole time, this issue was a lack of VRAM.\n\nLlava 1.6 takes multiple (overlapping) patches of the input image and uses them as input. This seems to be one driving reason for high VRAM during inference and fine tuning.\n\nFor 1.6 Mistral I needed 3 A6000s to load in bf16.\n</Comment>\n<Comment by aliencaocao at 2024-02-20T17:15:40Z>\n> The whole time, this issue was a lack of VRAM.\n> \n> Llava 1.6 takes multiple (overlapping) patches of the input image and uses them as input. This seems to be one driving reason for high VRAM during inference and fine tuning.\n> \n> For 1.6 Mistral I needed 3 A6000s to load in bf16.\n\nI loaded and ran inference on a 12gb 3080ti....\n\nEDIT: sorry if you meant training in bf16. You sounded like trying to infer\n</Comment>\n<Comment by RonanKMcGovern at 2024-02-20T18:19:16Z>\n> > The whole time, this issue was a lack of VRAM.\r\n> > Llava 1.6 takes multiple (overlapping) patches of the input image and uses them as input. This seems to be one driving reason for high VRAM during inference and fine tuning.\r\n> > For 1.6 Mistral I needed 3 A6000s to load in bf16.\r\n> \r\n> I loaded and ran inference on a 12gb 3080ti....\r\n\r\nyou must be running quantized though to get in 12 GB?\r\n\r\nyeah for training I run out of VRAM, although this meta error appears when I load in a single A6000, although inference runs fine. TBH I need to better replicate exactly what’s happening.\n</Comment>\n<Comment by RonanKMcGovern at 2024-02-20T18:26:43Z>\n> > The whole time, this issue was a lack of VRAM.\r\n> > Llava 1.6 takes multiple (overlapping) patches of the input image and uses them as input. This seems to be one driving reason for high VRAM during inference and fine tuning.\r\n> > For 1.6 Mistral I needed 3 A6000s to load in bf16.\r\n> \r\n> I loaded and ran inference on a 12gb 3080ti....\r\n\r\nyou must be running quantized though to get in 12 GB?\r\n\r\nyeah for training I run out of VRAM, although this meta error appears when I load in a single A6000, although inference runs fine. TBH I need to better replicate exactly what’s happening.\n</Comment>\n<Comment by aliencaocao at 2024-02-20T22:35:39Z>\nNo i loaded in fp16 for inference, using sglang. Its very slow though and i think its silently offloading some to cpu ram\n</Comment>\n<Comment by RonanKMcGovern at 2024-02-21T13:00:46Z>\nYeah ok, thanks, makes sense as the 7B alone would be ~13 GB of VRAM.\r\n\r\nI'm curious how much cpu RAM you have? (because it is indeed surprising to me that loading in bf16 loads some layers to cpu when I have 48 GB VRAM)...\n</Comment>\n<Comment by aliencaocao at 2024-02-21T13:01:34Z>\nI have 32GB and its using more than 10GB\n</Comment>\n<Comment by Salomeeeee at 2024-02-23T17:49:23Z>\n> The whole time, this issue was a lack of VRAM.\r\n> \r\n> Llava 1.6 takes multiple (overlapping) patches of the input image and uses them as input. This seems to be one driving reason for high VRAM during inference and fine tuning.\r\n> \r\n> For 1.6 Mistral I needed 3 A6000s to load in bf16.\r\n\r\nHello, may I confirm if the 'copying from a non-meta parameter' problem has been solved with larger VRAM?\n</Comment>\n<Comment by RonanKMcGovern at 2024-02-23T20:18:15Z>\nYes much bigger VRAM but I’m puzzled by why it takes so much.\r\n\r\nOn Fri 23 Feb 2024 at 17:49, Salomeeeee ***@***.***> wrote:\r\n\r\n> The whole time, this issue was a lack of VRAM.\r\n>\r\n> Llava 1.6 takes multiple (overlapping) patches of the input image and uses\r\n> them as input. This seems to be one driving reason for high VRAM during\r\n> inference and fine tuning.\r\n>\r\n> For 1.6 Mistral I needed 3 A6000s to load in bf16.\r\n>\r\n> Hello, may I confirm if the 'copying from a non-meta parameter' problem\r\n> has been solved with larger VRAM?\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/haotian-liu/LLaVA/issues/1122#issuecomment-1961749156>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/ASVG6CS4AGGA4NE7NYPD6RDYVDJDBAVCNFSM6AAAAABDERM7W6VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTSNRRG42DSMJVGY>\r\n> .\r\n> You are receiving this because you modified the open/close state.Message\r\n> ID: ***@***.***>\r\n>\n</Comment>\n<Comment by RonanKMcGovern at 2024-03-11T17:52:32Z>\nReopening as the large memory footprint for training (even when loaded in bf16) remains unexplained. It takes ~100 GB to fine-tune LLaVA 1.6 7B in bf16...\n</Comment>\n<Comment by sipie800 at 2024-03-30T03:36:04Z>\ndo the warnings matter anyway? While it seems to output some reasonable thing, we don't know if it reaches the real performance as it's a LLM!\n</Comment>\n<Comment by Linziyang1999 at 2024-04-07T16:33:12Z>\n> ### Describe the issue\r\n> Issue/Error: Loading 1.5 models works fine, but loading 1.6 models yield the error below. Note that the 1.6 models do load (despite the error) and inference works. However, training the 1.6 model results in OOM (unlike the 1.5 models which train fine - specifically training 7B models in bf16 on a 48GB A6000).\r\n> \r\n> Command:\r\n> \r\n> ```\r\n> from llava.model.builder import load_pretrained_model\r\n> from llava.mm_utils import get_model_name_from_path\r\n> from llava.eval.run_llava import eval_model\r\n> \r\n> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n> model_path = \"liuhaotian/llava-v1.6-mistral-7b\"\r\n> \r\n> model_name=get_model_name_from_path(model_path)\r\n> \r\n> tokenizer, model, image_processor, context_len = load_pretrained_model(\r\n>     model_path=model_path,\r\n>     model_base=None,\r\n>     model_name=model_name,\r\n>     # load_8bit=True\r\n>     # load_4bit=True\r\n> ) \r\n> ```\r\n> \r\n> Log:\r\n> \r\n> ```\r\n> /usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n>   return self.fget.__get__(instance, owner)()\r\n> /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.class_embedding: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\r\n>   warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\r\n> /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass assign=\r\n> ```\r\nSet device map=auto in vison tower load() or set unfreeze vision model =false in config\n</Comment>\n<Comment by RonanKMcGovern at 2024-04-08T18:13:31Z>\nThanks @Linziyang1999 , appreciate the response.\r\n\r\nbtw, device_map is already defaulting to 'auto' in builder.py (note that I've only changed from float16 to bfloat16 here):\r\n```\r\n#    Copyright 2023 Haotian Liu\r\n#\r\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\r\n#    you may not use this file except in compliance with the License.\r\n#    You may obtain a copy of the License at\r\n#\r\n#        http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n#    Unless required by applicable law or agreed to in writing, software\r\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\r\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n#    See the License for the specific language governing permissions and\r\n#    limitations under the License.\r\n\r\n\r\nimport os\r\nimport warnings\r\nimport shutil\r\n\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\r\nimport torch\r\nfrom llava.model import *\r\nfrom llava.constants import DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\r\n\r\n\r\ndef load_pretrained_model(model_path, model_base, model_name, load_8bit=False, load_4bit=False, device_map=\"auto\", device=\"cuda\", use_flash_attn=False, **kwargs):\r\n    kwargs = {\"device_map\": device_map, **kwargs}\r\n\r\n    if device != \"cuda\":\r\n        kwargs['device_map'] = {\"\": device}\r\n\r\n    if load_8bit:\r\n        kwargs['load_in_8bit'] = True\r\n    elif load_4bit:\r\n        kwargs['load_in_4bit'] = True\r\n        kwargs['quantization_config'] = BitsAndBytesConfig(\r\n            load_in_4bit=True,\r\n            bnb_4bit_compute_dtype=torch.bfloat16,\r\n            bnb_4bit_use_double_quant=True,\r\n            bnb_4bit_quant_type='nf4'\r\n        )\r\n    else:\r\n        kwargs['torch_dtype'] = torch.bfloat16\r\n\r\n    if use_flash_attn:\r\n        kwargs['attn_implementation'] = 'flash_attention_2'\r\n\r\n    if 'llava' in model_name.lower():\r\n        # Load LLaVA model\r\n        if 'lora' in model_name.lower() and model_base is None:\r\n            warnings.warn('There is `lora` in model name but no `model_base` is provided. If you are loading a LoRA model, please provide the `model_base` argument. Detailed instruction: https://github.com/haotian-liu/LLaVA#launch-a-model-worker-lora-weights-unmerged.')\r\n        if 'lora' in model_name.lower() and model_base is not None:\r\n            from llava.model.language_model.llava_llama import LlavaConfig\r\n            lora_cfg_pretrained = LlavaConfig.from_pretrained(model_path)\r\n            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n            print('Loading LLaVA from base model...')\r\n            model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs)\r\n            token_num, tokem_dim = model.lm_head.out_features, model.lm_head.in_features\r\n            if model.lm_head.weight.shape[0] != token_num:\r\n                model.lm_head.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\r\n                model.model.embed_tokens.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\r\n\r\n            print('Loading additional LLaVA weights...')\r\n            if os.path.exists(os.path.join(model_path, 'non_lora_trainables.bin')):\r\n                non_lora_trainables = torch.load(os.path.join(model_path, 'non_lora_trainables.bin'), map_location='cpu')\r\n            else:\r\n                # this is probably from HF Hub\r\n                from huggingface_hub import hf_hub_download\r\n                def load_from_hf(repo_id, filename, subfolder=None):\r\n                    cache_file = hf_hub_download(\r\n                        repo_id=repo_id,\r\n                        filename=filename,\r\n                        subfolder=subfolder)\r\n                    return torch.load(cache_file, map_location='cpu')\r\n                non_lora_trainables = load_from_hf(model_path, 'non_lora_trainables.bin')\r\n            non_lora_trainables = {(k[11:] if k.startswith('base_model.') else k): v for k, v in non_lora_trainables.items()}\r\n            if any(k.startswith('model.model.') for k in non_lora_trainables):\r\n                non_lora_trainables = {(k[6:] if k.startswith('model.') else k): v for k, v in non_lora_trainables.items()}\r\n            model.load_state_dict(non_lora_trainables, strict=False)\r\n\r\n            from peft import PeftModel\r\n            print('Loading LoRA weights...')\r\n            model = PeftModel.from_pretrained(model, model_path)\r\n            print('Merging LoRA weights...')\r\n            model = model.merge_and_unload()\r\n            print('Model is loaded...')\r\n        elif model_base is not None:\r\n            # this may be mm projector only\r\n            print('Loading LLaVA from base model...')\r\n            if 'mpt' in model_name.lower():\r\n                if not os.path.isfile(os.path.join(model_path, 'configuration_mpt.py')):\r\n                    shutil.copyfile(os.path.join(model_base, 'configuration_mpt.py'), os.path.join(model_path, 'configuration_mpt.py'))\r\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=True)\r\n                cfg_pretrained = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\r\n                model = LlavaMptForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs)\r\n            else:\r\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n                cfg_pretrained = AutoConfig.from_pretrained(model_path)\r\n                model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs)\r\n\r\n            mm_projector_weights = torch.load(os.path.join(model_path, 'mm_projector.bin'), map_location='cpu')\r\n            mm_projector_weights = {k: v.to(torch.bfloat16) for k, v in mm_projector_weights.items()}\r\n            model.load_state_dict(mm_projector_weights, strict=False)\r\n        else:\r\n            if 'mpt' in model_name.lower():\r\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\r\n                model = LlavaMptForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\n            elif 'mistral' in model_name.lower():\r\n                tokenizer = AutoTokenizer.from_pretrained(model_path)\r\n                model = LlavaMistralForCausalLM.from_pretrained(\r\n                    model_path,\r\n                    low_cpu_mem_usage=True,\r\n                    **kwargs\r\n                )\r\n            else:\r\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n                model = LlavaLlamaForCausalLM.from_pretrained(\r\n                    model_path,\r\n                    low_cpu_mem_usage=True,\r\n                    **kwargs\r\n                )\r\n    else:\r\n        # Load language model\r\n        if model_base is not None:\r\n            # PEFT model\r\n            from peft import PeftModel\r\n            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n            model = AutoModelForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, **kwargs)\r\n            print(f\"Loading LoRA weights from {model_path}\")\r\n            model = PeftModel.from_pretrained(model, model_path)\r\n            print(f\"Merging weights\")\r\n            model = model.merge_and_unload()\r\n            print('Convert to FP16...')\r\n            model.to(torch.bfloat16)\r\n        else:\r\n            use_fast = False\r\n            if 'mpt' in model_name.lower():\r\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\r\n                model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, trust_remote_code=True, **kwargs)\r\n            else:\r\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n                model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\n\r\n    image_processor = None\r\n\r\n    if 'llava' in model_name.lower():\r\n        mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\r\n        mm_use_im_patch_token = getattr(model.config, \"mm_use_im_patch_token\", True)\r\n        if mm_use_im_patch_token:\r\n            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\r\n        if mm_use_im_start_end:\r\n            tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\r\n        model.resize_token_embeddings(len(tokenizer))\r\n\r\n        vision_tower = model.get_vision_tower()\r\n        if not vision_tower.is_loaded:\r\n            print('vision_tower is not loaded so loading it now')\r\n            vision_tower.load_model(device_map=device_map)\r\n            vision_tower.to(device=device, dtype=torch.bfloat16)\r\n        else:\r\n            print('vision_tower is loaded')\r\n        image_processor = vision_tower.image_processor\r\n\r\n    if hasattr(model.config, \"max_sequence_length\"):\r\n        context_len = model.config.max_sequence_length\r\n    else:\r\n        context_len = 2048\r\n\r\n    return tokenizer, model, image_processor, context_len\r\n\r\n```\r\n\r\nYou say to update the `config` as an alternative. Where specifically should I do that?\n</Comment>\n<Comment by Linziyang1999 at 2024-04-09T02:44:13Z>\n> Thanks @Linziyang1999 , appreciate the response.\r\n> \r\n> btw, device_map is already defaulting to 'auto' in builder.py (note that I've only changed from float16 to bfloat16 here):\r\n> \r\n> ```\r\n> #    Copyright 2023 Haotian Liu\r\n> #\r\n> #    Licensed under the Apache License, Version 2.0 (the \"License\");\r\n> #    you may not use this file except in compliance with the License.\r\n> #    You may obtain a copy of the License at\r\n> #\r\n> #        http://www.apache.org/licenses/LICENSE-2.0\r\n> #\r\n> #    Unless required by applicable law or agreed to in writing, software\r\n> #    distributed under the License is distributed on an \"AS IS\" BASIS,\r\n> #    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n> #    See the License for the specific language governing permissions and\r\n> #    limitations under the License.\r\n> \r\n> \r\n> import os\r\n> import warnings\r\n> import shutil\r\n> \r\n> from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\r\n> import torch\r\n> from llava.model import *\r\n> from llava.constants import DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\r\n> \r\n> \r\n> def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, load_4bit=False, device_map=\"auto\", device=\"cuda\", use_flash_attn=False, **kwargs):\r\n>     kwargs = {\"device_map\": device_map, **kwargs}\r\n> \r\n>     if device != \"cuda\":\r\n>         kwargs['device_map'] = {\"\": device}\r\n> \r\n>     if load_8bit:\r\n>         kwargs['load_in_8bit'] = True\r\n>     elif load_4bit:\r\n>         kwargs['load_in_4bit'] = True\r\n>         kwargs['quantization_config'] = BitsAndBytesConfig(\r\n>             load_in_4bit=True,\r\n>             bnb_4bit_compute_dtype=torch.bfloat16,\r\n>             bnb_4bit_use_double_quant=True,\r\n>             bnb_4bit_quant_type='nf4'\r\n>         )\r\n>     else:\r\n>         kwargs['torch_dtype'] = torch.bfloat16\r\n> \r\n>     if use_flash_attn:\r\n>         kwargs['attn_implementation'] = 'flash_attention_2'\r\n> \r\n>     if 'llava' in model_name.lower():\r\n>         # Load LLaVA model\r\n>         if 'lora' in model_name.lower() and model_base is None:\r\n>             warnings.warn('There is `lora` in model name but no `model_base` is provided. If you are loading a LoRA model, please provide the `model_base` argument. Detailed instruction: https://github.com/haotian-liu/LLaVA#launch-a-model-worker-lora-weights-unmerged.')\r\n>         if 'lora' in model_name.lower() and model_base is not None:\r\n>             from llava.model.language_model.llava_llama import LlavaConfig\r\n>             lora_cfg_pretrained = LlavaConfig.from_pretrained(model_path)\r\n>             tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n>             print('Loading LLaVA from base model...')\r\n>             model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs)\r\n>             token_num, tokem_dim = model.lm_head.out_features, model.lm_head.in_features\r\n>             if model.lm_head.weight.shape[0] != token_num:\r\n>                 model.lm_head.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\r\n>                 model.model.embed_tokens.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\r\n> \r\n>             print('Loading additional LLaVA weights...')\r\n>             if os.path.exists(os.path.join(model_path, 'non_lora_trainables.bin')):\r\n>                 non_lora_trainables = torch.load(os.path.join(model_path, 'non_lora_trainables.bin'), map_location='cpu')\r\n>             else:\r\n>                 # this is probably from HF Hub\r\n>                 from huggingface_hub import hf_hub_download\r\n>                 def load_from_hf(repo_id, filename, subfolder=None):\r\n>                     cache_file = hf_hub_download(\r\n>                         repo_id=repo_id,\r\n>                         filename=filename,\r\n>                         subfolder=subfolder)\r\n>                     return torch.load(cache_file, map_location='cpu')\r\n>                 non_lora_trainables = load_from_hf(model_path, 'non_lora_trainables.bin')\r\n>             non_lora_trainables = {(k[11:] if k.startswith('base_model.') else k): v for k, v in non_lora_trainables.items()}\r\n>             if any(k.startswith('model.model.') for k in non_lora_trainables):\r\n>                 non_lora_trainables = {(k[6:] if k.startswith('model.') else k): v for k, v in non_lora_trainables.items()}\r\n>             model.load_state_dict(non_lora_trainables, strict=False)\r\n> \r\n>             from peft import PeftModel\r\n>             print('Loading LoRA weights...')\r\n>             model = PeftModel.from_pretrained(model, model_path)\r\n>             print('Merging LoRA weights...')\r\n>             model = model.merge_and_unload()\r\n>             print('Model is loaded...')\r\n>         elif model_base is not None:\r\n>             # this may be mm projector only\r\n>             print('Loading LLaVA from base model...')\r\n>             if 'mpt' in model_name.lower():\r\n>                 if not os.path.isfile(os.path.join(model_path, 'configuration_mpt.py')):\r\n>                     shutil.copyfile(os.path.join(model_base, 'configuration_mpt.py'), os.path.join(model_path, 'configuration_mpt.py'))\r\n>                 tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=True)\r\n>                 cfg_pretrained = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\r\n>                 model = LlavaMptForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs)\r\n>             else:\r\n>                 tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n>                 cfg_pretrained = AutoConfig.from_pretrained(model_path)\r\n>                 model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs)\r\n> \r\n>             mm_projector_weights = torch.load(os.path.join(model_path, 'mm_projector.bin'), map_location='cpu')\r\n>             mm_projector_weights = {k: v.to(torch.bfloat16) for k, v in mm_projector_weights.items()}\r\n>             model.load_state_dict(mm_projector_weights, strict=False)\r\n>         else:\r\n>             if 'mpt' in model_name.lower():\r\n>                 tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\r\n>                 model = LlavaMptForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\n>             elif 'mistral' in model_name.lower():\r\n>                 tokenizer = AutoTokenizer.from_pretrained(model_path)\r\n>                 model = LlavaMistralForCausalLM.from_pretrained(\r\n>                     model_path,\r\n>                     low_cpu_mem_usage=True,\r\n>                     **kwargs\r\n>                 )\r\n>             else:\r\n>                 tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n>                 model = LlavaLlamaForCausalLM.from_pretrained(\r\n>                     model_path,\r\n>                     low_cpu_mem_usage=True,\r\n>                     **kwargs\r\n>                 )\r\n>     else:\r\n>         # Load language model\r\n>         if model_base is not None:\r\n>             # PEFT model\r\n>             from peft import PeftModel\r\n>             tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n>             model = AutoModelForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, **kwargs)\r\n>             print(f\"Loading LoRA weights from {model_path}\")\r\n>             model = PeftModel.from_pretrained(model, model_path)\r\n>             print(f\"Merging weights\")\r\n>             model = model.merge_and_unload()\r\n>             print('Convert to FP16...')\r\n>             model.to(torch.bfloat16)\r\n>         else:\r\n>             use_fast = False\r\n>             if 'mpt' in model_name.lower():\r\n>                 tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\r\n>                 model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, trust_remote_code=True, **kwargs)\r\n>             else:\r\n>                 tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n>                 model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\n> \r\n>     image_processor = None\r\n> \r\n>     if 'llava' in model_name.lower():\r\n>         mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\r\n>         mm_use_im_patch_token = getattr(model.config, \"mm_use_im_patch_token\", True)\r\n>         if mm_use_im_patch_token:\r\n>             tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\r\n>         if mm_use_im_start_end:\r\n>             tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\r\n>         model.resize_token_embeddings(len(tokenizer))\r\n> \r\n>         vision_tower = model.get_vision_tower()\r\n>         if not vision_tower.is_loaded:\r\n>             print('vision_tower is not loaded so loading it now')\r\n>             vision_tower.load_model(device_map=device_map)\r\n>             vision_tower.to(device=device, dtype=torch.bfloat16)\r\n>         else:\r\n>             print('vision_tower is loaded')\r\n>         image_processor = vision_tower.image_processor\r\n> \r\n>     if hasattr(model.config, \"max_sequence_length\"):\r\n>         context_len = model.config.max_sequence_length\r\n>     else:\r\n>         context_len = 2048\r\n> \r\n>     return tokenizer, model, image_processor, context_len\r\n> ```\r\n> \r\n> You say to update the `config` as an alternative. Where specifically should I do that?\r\n\r\nthe reason why encounter with this warning is because from_pretrained() method allocate a meta device to vision encoder for low cpu memory using, so when you init you will get this warning, but it not a problem at all cause the paramters still load properly. To avoid this waring you can change the default load_model() method in clipvitiontower\r\n、、、\r\n#change to auto\r\n    def load_model(self, device_map=“auto”):\r\n        if self.is_loaded:\r\n            print('{} is already loaded, `load_model` called again, skipping.'.format(self.vision_tower_name))\r\n            return\r\n        #print(\"load model from\", self.vision_tower_name)\r\n        self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name)\r\n        self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name,device_map = device_map)\r\n        #input(device_map)\r\n        #print(self.vision_tower.device.type)\r\n        self.vision_tower.requires_grad_(False)\r\n\r\n        self.is_loaded = True\r\n、、、\r\nyou can also avoid this by set unfreeze_vison_tower = false in config in your model file (like llava-v1.6-mistral-7b/config), be careful if you use this method, you will load vision tower parameter from original clip-vit-large-patch14-336 rather than vit parameter save in llava model parameter. The original paramters was not finetuned you may using some method to change it or you will received irresponsible anwser.\r\n、、、\r\n\"model.vision_tower.vision_tower.vision_model.embeddings.class_embedding\": \"model-00003-of-00004.safetensors\",\r\n    \"model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight\": \"model-00003-of-00004.safetensors\",\r\n    \"model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight\": \"model-00003-of-00004.safetensors\",\r\n    \"model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias\": \"model-00003-of-00004.safetensors\",\r\n    \"model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight\": \"model-00003-of-00004.safetensors\",\r\n、、、\r\nyou can use me method to change the paramters\r\n、、、\r\nimport torch\r\nfrom safetensors.torch import load_file\r\n\r\nmodel_weights = torch.load('./clip-vit-large-patch14-336/pytorch_model.bin')\r\nmodel_weight_llava = load_file('./llava-v1.6-mistral-7b/model-00003-of-00004.safetensors')\r\n\r\nfor layer_name, param in model_weights.items():\r\n    llava_key = \"model.vision_tower.vision_tower.\" + layer_name\r\n    if llava_key in model_weight_llava and \"vision_model.encoder\" in layer_name:\r\n        if not torch.equal(model_weight_llava[llava_key], param):\r\n            print(llava_key, \" updated!\" )\r\n            model_weights[layer_name] = model_weight_llava[llava_key]\r\n\r\ntorch.save(model_weights, './clip-vit-large-patch14-336/pytorch_model.bin')\r\n、、、\n</Comment>\n<Comment by Linziyang1999 at 2024-04-10T03:20:44Z>\n> Thanks @Linziyang1999 , appreciate the response.\r\n> \r\n> btw, device_map is already defaulting to 'auto' in builder.py (note that I've only changed from float16 to bfloat16 here):\r\n> \r\n> ```\r\n> #    Copyright 2023 Haotian Liu\r\n> #\r\n> #    Licensed under the Apache License, Version 2.0 (the \"License\");\r\n> #    you may not use this file except in compliance with the License.\r\n> #    You may obtain a copy of the License at\r\n> #\r\n> #        http://www.apache.org/licenses/LICENSE-2.0\r\n> #\r\n> #    Unless required by applicable law or agreed to in writing, software\r\n> #    distributed under the License is distributed on an \"AS IS\" BASIS,\r\n> #    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n> #    See the License for the specific language governing permissions and\r\n> #    limitations under the License.\r\n> \r\n> \r\n> import os\r\n> import warnings\r\n> import shutil\r\n> \r\n> from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\r\n> import torch\r\n> from llava.model import *\r\n> from llava.constants import DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\r\n> \r\n> \r\n> def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, load_4bit=False, device_map=\"auto\", device=\"cuda\", use_flash_attn=False, **kwargs):\r\n>     kwargs = {\"device_map\": device_map, **kwargs}\r\n> \r\n>     if device != \"cuda\":\r\n>         kwargs['device_map'] = {\"\": device}\r\n> \r\n>     if load_8bit:\r\n>         kwargs['load_in_8bit'] = True\r\n>     elif load_4bit:\r\n>         kwargs['load_in_4bit'] = True\r\n>         kwargs['quantization_config'] = BitsAndBytesConfig(\r\n>             load_in_4bit=True,\r\n>             bnb_4bit_compute_dtype=torch.bfloat16,\r\n>             bnb_4bit_use_double_quant=True,\r\n>             bnb_4bit_quant_type='nf4'\r\n>         )\r\n>     else:\r\n>         kwargs['torch_dtype'] = torch.bfloat16\r\n> \r\n>     if use_flash_attn:\r\n>         kwargs['attn_implementation'] = 'flash_attention_2'\r\n> \r\n>     if 'llava' in model_name.lower():\r\n>         # Load LLaVA model\r\n>         if 'lora' in model_name.lower() and model_base is None:\r\n>             warnings.warn('There is `lora` in model name but no `model_base` is provided. If you are loading a LoRA model, please provide the `model_base` argument. Detailed instruction: https://github.com/haotian-liu/LLaVA#launch-a-model-worker-lora-weights-unmerged.')\r\n>         if 'lora' in model_name.lower() and model_base is not None:\r\n>             from llava.model.language_model.llava_llama import LlavaConfig\r\n>             lora_cfg_pretrained = LlavaConfig.from_pretrained(model_path)\r\n>             tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n>             print('Loading LLaVA from base model...')\r\n>             model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs)\r\n>             token_num, tokem_dim = model.lm_head.out_features, model.lm_head.in_features\r\n>             if model.lm_head.weight.shape[0] != token_num:\r\n>                 model.lm_head.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\r\n>                 model.model.embed_tokens.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\r\n> \r\n>             print('Loading additional LLaVA weights...')\r\n>             if os.path.exists(os.path.join(model_path, 'non_lora_trainables.bin')):\r\n>                 non_lora_trainables = torch.load(os.path.join(model_path, 'non_lora_trainables.bin'), map_location='cpu')\r\n>             else:\r\n>                 # this is probably from HF Hub\r\n>                 from huggingface_hub import hf_hub_download\r\n>                 def load_from_hf(repo_id, filename, subfolder=None):\r\n>                     cache_file = hf_hub_download(\r\n>                         repo_id=repo_id,\r\n>                         filename=filename,\r\n>                         subfolder=subfolder)\r\n>                     return torch.load(cache_file, map_location='cpu')\r\n>                 non_lora_trainables = load_from_hf(model_path, 'non_lora_trainables.bin')\r\n>             non_lora_trainables = {(k[11:] if k.startswith('base_model.') else k): v for k, v in non_lora_trainables.items()}\r\n>             if any(k.startswith('model.model.') for k in non_lora_trainables):\r\n>                 non_lora_trainables = {(k[6:] if k.startswith('model.') else k): v for k, v in non_lora_trainables.items()}\r\n>             model.load_state_dict(non_lora_trainables, strict=False)\r\n> \r\n>             from peft import PeftModel\r\n>             print('Loading LoRA weights...')\r\n>             model = PeftModel.from_pretrained(model, model_path)\r\n>             print('Merging LoRA weights...')\r\n>             model = model.merge_and_unload()\r\n>             print('Model is loaded...')\r\n>         elif model_base is not None:\r\n>             # this may be mm projector only\r\n>             print('Loading LLaVA from base model...')\r\n>             if 'mpt' in model_name.lower():\r\n>                 if not os.path.isfile(os.path.join(model_path, 'configuration_mpt.py')):\r\n>                     shutil.copyfile(os.path.join(model_base, 'configuration_mpt.py'), os.path.join(model_path, 'configuration_mpt.py'))\r\n>                 tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=True)\r\n>                 cfg_pretrained = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\r\n>                 model = LlavaMptForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs)\r\n>             else:\r\n>                 tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n>                 cfg_pretrained = AutoConfig.from_pretrained(model_path)\r\n>                 model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs)\r\n> \r\n>             mm_projector_weights = torch.load(os.path.join(model_path, 'mm_projector.bin'), map_location='cpu')\r\n>             mm_projector_weights = {k: v.to(torch.bfloat16) for k, v in mm_projector_weights.items()}\r\n>             model.load_state_dict(mm_projector_weights, strict=False)\r\n>         else:\r\n>             if 'mpt' in model_name.lower():\r\n>                 tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\r\n>                 model = LlavaMptForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\n>             elif 'mistral' in model_name.lower():\r\n>                 tokenizer = AutoTokenizer.from_pretrained(model_path)\r\n>                 model = LlavaMistralForCausalLM.from_pretrained(\r\n>                     model_path,\r\n>                     low_cpu_mem_usage=True,\r\n>                     **kwargs\r\n>                 )\r\n>             else:\r\n>                 tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n>                 model = LlavaLlamaForCausalLM.from_pretrained(\r\n>                     model_path,\r\n>                     low_cpu_mem_usage=True,\r\n>                     **kwargs\r\n>                 )\r\n>     else:\r\n>         # Load language model\r\n>         if model_base is not None:\r\n>             # PEFT model\r\n>             from peft import PeftModel\r\n>             tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n>             model = AutoModelForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, **kwargs)\r\n>             print(f\"Loading LoRA weights from {model_path}\")\r\n>             model = PeftModel.from_pretrained(model, model_path)\r\n>             print(f\"Merging weights\")\r\n>             model = model.merge_and_unload()\r\n>             print('Convert to FP16...')\r\n>             model.to(torch.bfloat16)\r\n>         else:\r\n>             use_fast = False\r\n>             if 'mpt' in model_name.lower():\r\n>                 tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\r\n>                 model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, trust_remote_code=True, **kwargs)\r\n>             else:\r\n>                 tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n>                 model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\n> \r\n>     image_processor = None\r\n> \r\n>     if 'llava' in model_name.lower():\r\n>         mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\r\n>         mm_use_im_patch_token = getattr(model.config, \"mm_use_im_patch_token\", True)\r\n>         if mm_use_im_patch_token:\r\n>             tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\r\n>         if mm_use_im_start_end:\r\n>             tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\r\n>         model.resize_token_embeddings(len(tokenizer))\r\n> \r\n>         vision_tower = model.get_vision_tower()\r\n>         if not vision_tower.is_loaded:\r\n>             print('vision_tower is not loaded so loading it now')\r\n>             vision_tower.load_model(device_map=device_map)\r\n>             vision_tower.to(device=device, dtype=torch.bfloat16)\r\n>         else:\r\n>             print('vision_tower is loaded')\r\n>         image_processor = vision_tower.image_processor\r\n> \r\n>     if hasattr(model.config, \"max_sequence_length\"):\r\n>         context_len = model.config.max_sequence_length\r\n>     else:\r\n>         context_len = 2048\r\n> \r\n>     return tokenizer, model, image_processor, context_len\r\n> ```\r\n> \r\n> You say to update the `config` as an alternative. Where specifically should I do that?\r\n\r\ni just read the from_pretrained() method in transformers, after search in the code i find this warning is a design for not load weight twice, so just ignore it.\n</Comment>\n<Comment by Linziyang1999 at 2024-04-10T03:32:50Z>\n> Thanks @Linziyang1999 , appreciate the response.\r\n> \r\n> btw, device_map is already defaulting to 'auto' in builder.py (note that I've only changed from float16 to bfloat16 here):\r\n> \r\n> ```\r\n> #    Copyright 2023 Haotian Liu\r\n> #\r\n> #    Licensed under the Apache License, Version 2.0 (the \"License\");\r\n> #    you may not use this file except in compliance with the License.\r\n> #    You may obtain a copy of the License at\r\n> #\r\n> #        http://www.apache.org/licenses/LICENSE-2.0\r\n> #\r\n> #    Unless required by applicable law or agreed to in writing, software\r\n> #    distributed under the License is distributed on an \"AS IS\" BASIS,\r\n> #    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n> #    See the License for the specific language governing permissions and\r\n> #    limitations under the License.\r\n> \r\n> \r\n> import os\r\n> import warnings\r\n> import shutil\r\n> \r\n> from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\r\n> import torch\r\n> from llava.model import *\r\n> from llava.constants import DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\r\n> \r\n> \r\n> def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, load_4bit=False, device_map=\"auto\", device=\"cuda\", use_flash_attn=False, **kwargs):\r\n>     kwargs = {\"device_map\": device_map, **kwargs}\r\n> \r\n>     if device != \"cuda\":\r\n>         kwargs['device_map'] = {\"\": device}\r\n> \r\n>     if load_8bit:\r\n>         kwargs['load_in_8bit'] = True\r\n>     elif load_4bit:\r\n>         kwargs['load_in_4bit'] = True\r\n>         kwargs['quantization_config'] = BitsAndBytesConfig(\r\n>             load_in_4bit=True,\r\n>             bnb_4bit_compute_dtype=torch.bfloat16,\r\n>             bnb_4bit_use_double_quant=True,\r\n>             bnb_4bit_quant_type='nf4'\r\n>         )\r\n>     else:\r\n>         kwargs['torch_dtype'] = torch.bfloat16\r\n> \r\n>     if use_flash_attn:\r\n>         kwargs['attn_implementation'] = 'flash_attention_2'\r\n> \r\n>     if 'llava' in model_name.lower():\r\n>         # Load LLaVA model\r\n>         if 'lora' in model_name.lower() and model_base is None:\r\n>             warnings.warn('There is `lora` in model name but no `model_base` is provided. If you are loading a LoRA model, please provide the `model_base` argument. Detailed instruction: https://github.com/haotian-liu/LLaVA#launch-a-model-worker-lora-weights-unmerged.')\r\n>         if 'lora' in model_name.lower() and model_base is not None:\r\n>             from llava.model.language_model.llava_llama import LlavaConfig\r\n>             lora_cfg_pretrained = LlavaConfig.from_pretrained(model_path)\r\n>             tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n>             print('Loading LLaVA from base model...')\r\n>             model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs)\r\n>             token_num, tokem_dim = model.lm_head.out_features, model.lm_head.in_features\r\n>             if model.lm_head.weight.shape[0] != token_num:\r\n>                 model.lm_head.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\r\n>                 model.model.embed_tokens.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\r\n> \r\n>             print('Loading additional LLaVA weights...')\r\n>             if os.path.exists(os.path.join(model_path, 'non_lora_trainables.bin')):\r\n>                 non_lora_trainables = torch.load(os.path.join(model_path, 'non_lora_trainables.bin'), map_location='cpu')\r\n>             else:\r\n>                 # this is probably from HF Hub\r\n>                 from huggingface_hub import hf_hub_download\r\n>                 def load_from_hf(repo_id, filename, subfolder=None):\r\n>                     cache_file = hf_hub_download(\r\n>                         repo_id=repo_id,\r\n>                         filename=filename,\r\n>                         subfolder=subfolder)\r\n>                     return torch.load(cache_file, map_location='cpu')\r\n>                 non_lora_trainables = load_from_hf(model_path, 'non_lora_trainables.bin')\r\n>             non_lora_trainables = {(k[11:] if k.startswith('base_model.') else k): v for k, v in non_lora_trainables.items()}\r\n>             if any(k.startswith('model.model.') for k in non_lora_trainables):\r\n>                 non_lora_trainables = {(k[6:] if k.startswith('model.') else k): v for k, v in non_lora_trainables.items()}\r\n>             model.load_state_dict(non_lora_trainables, strict=False)\r\n> \r\n>             from peft import PeftModel\r\n>             print('Loading LoRA weights...')\r\n>             model = PeftModel.from_pretrained(model, model_path)\r\n>             print('Merging LoRA weights...')\r\n>             model = model.merge_and_unload()\r\n>             print('Model is loaded...')\r\n>         elif model_base is not None:\r\n>             # this may be mm projector only\r\n>             print('Loading LLaVA from base model...')\r\n>             if 'mpt' in model_name.lower():\r\n>                 if not os.path.isfile(os.path.join(model_path, 'configuration_mpt.py')):\r\n>                     shutil.copyfile(os.path.join(model_base, 'configuration_mpt.py'), os.path.join(model_path, 'configuration_mpt.py'))\r\n>                 tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=True)\r\n>                 cfg_pretrained = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\r\n>                 model = LlavaMptForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs)\r\n>             else:\r\n>                 tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n>                 cfg_pretrained = AutoConfig.from_pretrained(model_path)\r\n>                 model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs)\r\n> \r\n>             mm_projector_weights = torch.load(os.path.join(model_path, 'mm_projector.bin'), map_location='cpu')\r\n>             mm_projector_weights = {k: v.to(torch.bfloat16) for k, v in mm_projector_weights.items()}\r\n>             model.load_state_dict(mm_projector_weights, strict=False)\r\n>         else:\r\n>             if 'mpt' in model_name.lower():\r\n>                 tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\r\n>                 model = LlavaMptForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\n>             elif 'mistral' in model_name.lower():\r\n>                 tokenizer = AutoTokenizer.from_pretrained(model_path)\r\n>                 model = LlavaMistralForCausalLM.from_pretrained(\r\n>                     model_path,\r\n>                     low_cpu_mem_usage=True,\r\n>                     **kwargs\r\n>                 )\r\n>             else:\r\n>                 tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n>                 model = LlavaLlamaForCausalLM.from_pretrained(\r\n>                     model_path,\r\n>                     low_cpu_mem_usage=True,\r\n>                     **kwargs\r\n>                 )\r\n>     else:\r\n>         # Load language model\r\n>         if model_base is not None:\r\n>             # PEFT model\r\n>             from peft import PeftModel\r\n>             tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n>             model = AutoModelForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, **kwargs)\r\n>             print(f\"Loading LoRA weights from {model_path}\")\r\n>             model = PeftModel.from_pretrained(model, model_path)\r\n>             print(f\"Merging weights\")\r\n>             model = model.merge_and_unload()\r\n>             print('Convert to FP16...')\r\n>             model.to(torch.bfloat16)\r\n>         else:\r\n>             use_fast = False\r\n>             if 'mpt' in model_name.lower():\r\n>                 tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\r\n>                 model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, trust_remote_code=True, **kwargs)\r\n>             else:\r\n>                 tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n>                 model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\n> \r\n>     image_processor = None\r\n> \r\n>     if 'llava' in model_name.lower():\r\n>         mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\r\n>         mm_use_im_patch_token = getattr(model.config, \"mm_use_im_patch_token\", True)\r\n>         if mm_use_im_patch_token:\r\n>             tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\r\n>         if mm_use_im_start_end:\r\n>             tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\r\n>         model.resize_token_embeddings(len(tokenizer))\r\n> \r\n>         vision_tower = model.get_vision_tower()\r\n>         if not vision_tower.is_loaded:\r\n>             print('vision_tower is not loaded so loading it now')\r\n>             vision_tower.load_model(device_map=device_map)\r\n>             vision_tower.to(device=device, dtype=torch.bfloat16)\r\n>         else:\r\n>             print('vision_tower is loaded')\r\n>         image_processor = vision_tower.image_processor\r\n> \r\n>     if hasattr(model.config, \"max_sequence_length\"):\r\n>         context_len = model.config.max_sequence_length\r\n>     else:\r\n>         context_len = 2048\r\n> \r\n>     return tokenizer, model, image_processor, context_len\r\n> ```\r\n> \r\n> You say to update the `config` as an alternative. Where specifically should I do that?\r\n\r\nand this device_map is used in load llavamistralforcasuallm, this error heppend when you load clipvisionmodel, llava set device_map=none and low_cpu_mem_usage = false when load pretrain clipmodel, the _fast_init default as true in frpm_pretrained() method \r\n`init_contexts = [no_init_weights(_enable=_fast_init)]`\r\nso the model will init in meta device, and clipvison model save weight in one pytorch.bin, so this will call _load_state_dict_into_model() this is not design for load onto meta device so this method do nothing, and weight willl be loaded properly in llavamistralforcasuallm.from_pretrained(), and will call _load_state_dict_into_meta_model, this design for load in meta device.  So the params will only load once.\r\nHope this will help you , and if you have any question feel free to contact me.\n</Comment>\n<Comment by RonanKMcGovern at 2024-04-11T11:42:06Z>\nThanks.\r\n\r\nUnfortunately I don't follow your explanation exactly, but it seems that you are saying the meta warnings are not a problem.\r\n\r\nSeparately, can you explain why VRAM requirements appear so high for LoRA training of LLaVA 1.6 versus 1.5?\n</Comment>\n<Comment by RonanKMcGovern at 2024-04-11T13:40:26Z>\nOk, you can now load any checkpoint. See the `OPTIONALLY LOAD A DIFFERENT CHECKPOINT` part of the script below the training.\r\n\r\nKindly close this issue if everything works.\n</Comment>\n<Comment by ziyigogogo at 2024-08-09T02:43:42Z>\n> > Thanks @Linziyang1999 , appreciate the response.\r\n> > btw, device_map is already defaulting to 'auto' in builder.py (note that I've only changed from float16 to bfloat16 here):\r\n> > ```\r\n> > #    Copyright 2023 Haotian Liu\r\n> > #\r\n> > #    Licensed under the Apache License, Version 2.0 (the \"License\");\r\n> > #    you may not use this file except in compliance with the License.\r\n> > #    You may obtain a copy of the License at\r\n> > #\r\n> > #        http://www.apache.org/licenses/LICENSE-2.0\r\n> > #\r\n> > #    Unless required by applicable law or agreed to in writing, software\r\n> > #    distributed under the License is distributed on an \"AS IS\" BASIS,\r\n> > #    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n> > #    See the License for the specific language governing permissions and\r\n> > #    limitations under the License.\r\n> > \r\n> > \r\n> > import os\r\n> > import warnings\r\n> > import shutil\r\n> > \r\n> > from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\r\n> > import torch\r\n> > from llava.model import *\r\n> > from llava.constants import DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\r\n> > \r\n> > \r\n> > def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, load_4bit=False, device_map=\"auto\", device=\"cuda\", use_flash_attn=False, **kwargs):\r\n> >     kwargs = {\"device_map\": device_map, **kwargs}\r\n> > \r\n> >     if device != \"cuda\":\r\n> >         kwargs['device_map'] = {\"\": device}\r\n> > \r\n> >     if load_8bit:\r\n> >         kwargs['load_in_8bit'] = True\r\n> >     elif load_4bit:\r\n> >         kwargs['load_in_4bit'] = True\r\n> >         kwargs['quantization_config'] = BitsAndBytesConfig(\r\n> >             load_in_4bit=True,\r\n> >             bnb_4bit_compute_dtype=torch.bfloat16,\r\n> >             bnb_4bit_use_double_quant=True,\r\n> >             bnb_4bit_quant_type='nf4'\r\n> >         )\r\n> >     else:\r\n> >         kwargs['torch_dtype'] = torch.bfloat16\r\n> > \r\n> >     if use_flash_attn:\r\n> >         kwargs['attn_implementation'] = 'flash_attention_2'\r\n> > \r\n> >     if 'llava' in model_name.lower():\r\n> >         # Load LLaVA model\r\n> >         if 'lora' in model_name.lower() and model_base is None:\r\n> >             warnings.warn('There is `lora` in model name but no `model_base` is provided. If you are loading a LoRA model, please provide the `model_base` argument. Detailed instruction: https://github.com/haotian-liu/LLaVA#launch-a-model-worker-lora-weights-unmerged.')\r\n> >         if 'lora' in model_name.lower() and model_base is not None:\r\n> >             from llava.model.language_model.llava_llama import LlavaConfig\r\n> >             lora_cfg_pretrained = LlavaConfig.from_pretrained(model_path)\r\n> >             tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n> >             print('Loading LLaVA from base model...')\r\n> >             model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs)\r\n> >             token_num, tokem_dim = model.lm_head.out_features, model.lm_head.in_features\r\n> >             if model.lm_head.weight.shape[0] != token_num:\r\n> >                 model.lm_head.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\r\n> >                 model.model.embed_tokens.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\r\n> > \r\n> >             print('Loading additional LLaVA weights...')\r\n> >             if os.path.exists(os.path.join(model_path, 'non_lora_trainables.bin')):\r\n> >                 non_lora_trainables = torch.load(os.path.join(model_path, 'non_lora_trainables.bin'), map_location='cpu')\r\n> >             else:\r\n> >                 # this is probably from HF Hub\r\n> >                 from huggingface_hub import hf_hub_download\r\n> >                 def load_from_hf(repo_id, filename, subfolder=None):\r\n> >                     cache_file = hf_hub_download(\r\n> >                         repo_id=repo_id,\r\n> >                         filename=filename,\r\n> >                         subfolder=subfolder)\r\n> >                     return torch.load(cache_file, map_location='cpu')\r\n> >                 non_lora_trainables = load_from_hf(model_path, 'non_lora_trainables.bin')\r\n> >             non_lora_trainables = {(k[11:] if k.startswith('base_model.') else k): v for k, v in non_lora_trainables.items()}\r\n> >             if any(k.startswith('model.model.') for k in non_lora_trainables):\r\n> >                 non_lora_trainables = {(k[6:] if k.startswith('model.') else k): v for k, v in non_lora_trainables.items()}\r\n> >             model.load_state_dict(non_lora_trainables, strict=False)\r\n> > \r\n> >             from peft import PeftModel\r\n> >             print('Loading LoRA weights...')\r\n> >             model = PeftModel.from_pretrained(model, model_path)\r\n> >             print('Merging LoRA weights...')\r\n> >             model = model.merge_and_unload()\r\n> >             print('Model is loaded...')\r\n> >         elif model_base is not None:\r\n> >             # this may be mm projector only\r\n> >             print('Loading LLaVA from base model...')\r\n> >             if 'mpt' in model_name.lower():\r\n> >                 if not os.path.isfile(os.path.join(model_path, 'configuration_mpt.py')):\r\n> >                     shutil.copyfile(os.path.join(model_base, 'configuration_mpt.py'), os.path.join(model_path, 'configuration_mpt.py'))\r\n> >                 tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=True)\r\n> >                 cfg_pretrained = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\r\n> >                 model = LlavaMptForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs)\r\n> >             else:\r\n> >                 tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n> >                 cfg_pretrained = AutoConfig.from_pretrained(model_path)\r\n> >                 model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs)\r\n> > \r\n> >             mm_projector_weights = torch.load(os.path.join(model_path, 'mm_projector.bin'), map_location='cpu')\r\n> >             mm_projector_weights = {k: v.to(torch.bfloat16) for k, v in mm_projector_weights.items()}\r\n> >             model.load_state_dict(mm_projector_weights, strict=False)\r\n> >         else:\r\n> >             if 'mpt' in model_name.lower():\r\n> >                 tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\r\n> >                 model = LlavaMptForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\n> >             elif 'mistral' in model_name.lower():\r\n> >                 tokenizer = AutoTokenizer.from_pretrained(model_path)\r\n> >                 model = LlavaMistralForCausalLM.from_pretrained(\r\n> >                     model_path,\r\n> >                     low_cpu_mem_usage=True,\r\n> >                     **kwargs\r\n> >                 )\r\n> >             else:\r\n> >                 tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n> >                 model = LlavaLlamaForCausalLM.from_pretrained(\r\n> >                     model_path,\r\n> >                     low_cpu_mem_usage=True,\r\n> >                     **kwargs\r\n> >                 )\r\n> >     else:\r\n> >         # Load language model\r\n> >         if model_base is not None:\r\n> >             # PEFT model\r\n> >             from peft import PeftModel\r\n> >             tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n> >             model = AutoModelForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, **kwargs)\r\n> >             print(f\"Loading LoRA weights from {model_path}\")\r\n> >             model = PeftModel.from_pretrained(model, model_path)\r\n> >             print(f\"Merging weights\")\r\n> >             model = model.merge_and_unload()\r\n> >             print('Convert to FP16...')\r\n> >             model.to(torch.bfloat16)\r\n> >         else:\r\n> >             use_fast = False\r\n> >             if 'mpt' in model_name.lower():\r\n> >                 tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\r\n> >                 model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, trust_remote_code=True, **kwargs)\r\n> >             else:\r\n> >                 tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n> >                 model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\n> > \r\n> >     image_processor = None\r\n> > \r\n> >     if 'llava' in model_name.lower():\r\n> >         mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\r\n> >         mm_use_im_patch_token = getattr(model.config, \"mm_use_im_patch_token\", True)\r\n> >         if mm_use_im_patch_token:\r\n> >             tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\r\n> >         if mm_use_im_start_end:\r\n> >             tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\r\n> >         model.resize_token_embeddings(len(tokenizer))\r\n> > \r\n> >         vision_tower = model.get_vision_tower()\r\n> >         if not vision_tower.is_loaded:\r\n> >             print('vision_tower is not loaded so loading it now')\r\n> >             vision_tower.load_model(device_map=device_map)\r\n> >             vision_tower.to(device=device, dtype=torch.bfloat16)\r\n> >         else:\r\n> >             print('vision_tower is loaded')\r\n> >         image_processor = vision_tower.image_processor\r\n> > \r\n> >     if hasattr(model.config, \"max_sequence_length\"):\r\n> >         context_len = model.config.max_sequence_length\r\n> >     else:\r\n> >         context_len = 2048\r\n> > \r\n> >     return tokenizer, model, image_processor, context_len\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > You say to update the `config` as an alternative. Where specifically should I do that?\r\n> \r\n> the reason why encounter with this warning is because from_pretrained() method allocate a meta device to vision encoder for low cpu memory using, so when you init you will get this warning, but it not a problem at all cause the paramters still load properly. To avoid this waring you can change the default load_model() method in clipvitiontower 、、、 #change to auto def load_model(self, device_map=“auto”): if self.is_loaded: print('{} is already loaded, `load_model` called again, skipping.'.format(self.vision_tower_name)) return #print(\"load model from\", self.vision_tower_name) self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name) self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name,device_map = device_map) #input(device_map) #print(self.vision_tower.device.type) self.vision_tower.requires_grad_(False)\r\n> \r\n> ```\r\n>     self.is_loaded = True\r\n> ```\r\n> \r\n> 、、、 you can also avoid this by set unfreeze_vison_tower = false in config in your model file (like llava-v1.6-mistral-7b/config), be careful if you use this method, you will load vision tower parameter from original clip-vit-large-patch14-336 rather than vit parameter save in llava model parameter. The original paramters was not finetuned you may using some method to change it or you will received irresponsible anwser. 、、、 \"model.vision_tower.vision_tower.vision_model.embeddings.class_embedding\": \"model-00003-of-00004.safetensors\", \"model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight\": \"model-00003-of-00004.safetensors\", \"model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight\": \"model-00003-of-00004.safetensors\", \"model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias\": \"model-00003-of-00004.safetensors\", \"model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight\": \"model-00003-of-00004.safetensors\", 、、、 you can use me method to change the paramters 、、、 import torch from safetensors.torch import load_file\r\n> \r\n> model_weights = torch.load('./clip-vit-large-patch14-336/pytorch_model.bin') model_weight_llava = load_file('./llava-v1.6-mistral-7b/model-00003-of-00004.safetensors')\r\n> \r\n> for layer_name, param in model_weights.items(): llava_key = \"model.vision_tower.vision_tower.\" + layer_name if llava_key in model_weight_llava and \"vision_model.encoder\" in layer_name: if not torch.equal(model_weight_llava[llava_key], param): print(llava_key, \" updated!\" ) model_weights[layer_name] = model_weight_llava[llava_key]\r\n> \r\n> torch.save(model_weights, './clip-vit-large-patch14-336/pytorch_model.bin') 、、、\r\n\r\nthank you so much, you saved my day!@[Linziyang1999](https://github.com/Linziyang1999)\n</Comment>\n<Comment by XCF-Mike at 2024-08-31T17:18:45Z>\n@Linziyang1999 Hi，\r\n\r\n> > Thanks @Linziyang1999 , appreciate the response.\r\n> > btw, device_map is already defaulting to 'auto' in builder.py (note that I've only changed from float16 to bfloat16 here):\r\n> > ```\r\n> > #    Copyright 2023 Haotian Liu\r\n> > #\r\n> > #    Licensed under the Apache License, Version 2.0 (the \"License\");\r\n> > #    you may not use this file except in compliance with the License.\r\n> > #    You may obtain a copy of the License at\r\n> > #\r\n> > #        http://www.apache.org/licenses/LICENSE-2.0\r\n> > #\r\n> > #    Unless required by applicable law or agreed to in writing, software\r\n> > #    distributed under the License is distributed on an \"AS IS\" BASIS,\r\n> > #    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n> > #    See the License for the specific language governing permissions and\r\n> > #    limitations under the License.\r\n> > \r\n> > \r\n> > import os\r\n> > import warnings\r\n> > import shutil\r\n> > \r\n> > from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\r\n> > import torch\r\n> > from llava.model import *\r\n> > from llava.constants import DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\r\n> > \r\n> > \r\n> > def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, load_4bit=False, device_map=\"auto\", device=\"cuda\", use_flash_attn=False, **kwargs):\r\n> >     kwargs = {\"device_map\": device_map, **kwargs}\r\n> > \r\n> >     if device != \"cuda\":\r\n> >         kwargs['device_map'] = {\"\": device}\r\n> > \r\n> >     if load_8bit:\r\n> >         kwargs['load_in_8bit'] = True\r\n> >     elif load_4bit:\r\n> >         kwargs['load_in_4bit'] = True\r\n> >         kwargs['quantization_config'] = BitsAndBytesConfig(\r\n> >             load_in_4bit=True,\r\n> >             bnb_4bit_compute_dtype=torch.bfloat16,\r\n> >             bnb_4bit_use_double_quant=True,\r\n> >             bnb_4bit_quant_type='nf4'\r\n> >         )\r\n> >     else:\r\n> >         kwargs['torch_dtype'] = torch.bfloat16\r\n> > \r\n> >     if use_flash_attn:\r\n> >         kwargs['attn_implementation'] = 'flash_attention_2'\r\n> > \r\n> >     if 'llava' in model_name.lower():\r\n> >         # Load LLaVA model\r\n> >         if 'lora' in model_name.lower() and model_base is None:\r\n> >             warnings.warn('There is `lora` in model name but no `model_base` is provided. If you are loading a LoRA model, please provide the `model_base` argument. Detailed instruction: https://github.com/haotian-liu/LLaVA#launch-a-model-worker-lora-weights-unmerged.')\r\n> >         if 'lora' in model_name.lower() and model_base is not None:\r\n> >             from llava.model.language_model.llava_llama import LlavaConfig\r\n> >             lora_cfg_pretrained = LlavaConfig.from_pretrained(model_path)\r\n> >             tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n> >             print('Loading LLaVA from base model...')\r\n> >             model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs)\r\n> >             token_num, tokem_dim = model.lm_head.out_features, model.lm_head.in_features\r\n> >             if model.lm_head.weight.shape[0] != token_num:\r\n> >                 model.lm_head.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\r\n> >                 model.model.embed_tokens.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\r\n> > \r\n> >             print('Loading additional LLaVA weights...')\r\n> >             if os.path.exists(os.path.join(model_path, 'non_lora_trainables.bin')):\r\n> >                 non_lora_trainables = torch.load(os.path.join(model_path, 'non_lora_trainables.bin'), map_location='cpu')\r\n> >             else:\r\n> >                 # this is probably from HF Hub\r\n> >                 from huggingface_hub import hf_hub_download\r\n> >                 def load_from_hf(repo_id, filename, subfolder=None):\r\n> >                     cache_file = hf_hub_download(\r\n> >                         repo_id=repo_id,\r\n> >                         filename=filename,\r\n> >                         subfolder=subfolder)\r\n> >                     return torch.load(cache_file, map_location='cpu')\r\n> >                 non_lora_trainables = load_from_hf(model_path, 'non_lora_trainables.bin')\r\n> >             non_lora_trainables = {(k[11:] if k.startswith('base_model.') else k): v for k, v in non_lora_trainables.items()}\r\n> >             if any(k.startswith('model.model.') for k in non_lora_trainables):\r\n> >                 non_lora_trainables = {(k[6:] if k.startswith('model.') else k): v for k, v in non_lora_trainables.items()}\r\n> >             model.load_state_dict(non_lora_trainables, strict=False)\r\n> > \r\n> >             from peft import PeftModel\r\n> >             print('Loading LoRA weights...')\r\n> >             model = PeftModel.from_pretrained(model, model_path)\r\n> >             print('Merging LoRA weights...')\r\n> >             model = model.merge_and_unload()\r\n> >             print('Model is loaded...')\r\n> >         elif model_base is not None:\r\n> >             # this may be mm projector only\r\n> >             print('Loading LLaVA from base model...')\r\n> >             if 'mpt' in model_name.lower():\r\n> >                 if not os.path.isfile(os.path.join(model_path, 'configuration_mpt.py')):\r\n> >                     shutil.copyfile(os.path.join(model_base, 'configuration_mpt.py'), os.path.join(model_path, 'configuration_mpt.py'))\r\n> >                 tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=True)\r\n> >                 cfg_pretrained = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\r\n> >                 model = LlavaMptForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs)\r\n> >             else:\r\n> >                 tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n> >                 cfg_pretrained = AutoConfig.from_pretrained(model_path)\r\n> >                 model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs)\r\n> > \r\n> >             mm_projector_weights = torch.load(os.path.join(model_path, 'mm_projector.bin'), map_location='cpu')\r\n> >             mm_projector_weights = {k: v.to(torch.bfloat16) for k, v in mm_projector_weights.items()}\r\n> >             model.load_state_dict(mm_projector_weights, strict=False)\r\n> >         else:\r\n> >             if 'mpt' in model_name.lower():\r\n> >                 tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\r\n> >                 model = LlavaMptForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\n> >             elif 'mistral' in model_name.lower():\r\n> >                 tokenizer = AutoTokenizer.from_pretrained(model_path)\r\n> >                 model = LlavaMistralForCausalLM.from_pretrained(\r\n> >                     model_path,\r\n> >                     low_cpu_mem_usage=True,\r\n> >                     **kwargs\r\n> >                 )\r\n> >             else:\r\n> >                 tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n> >                 model = LlavaLlamaForCausalLM.from_pretrained(\r\n> >                     model_path,\r\n> >                     low_cpu_mem_usage=True,\r\n> >                     **kwargs\r\n> >                 )\r\n> >     else:\r\n> >         # Load language model\r\n> >         if model_base is not None:\r\n> >             # PEFT model\r\n> >             from peft import PeftModel\r\n> >             tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n> >             model = AutoModelForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, **kwargs)\r\n> >             print(f\"Loading LoRA weights from {model_path}\")\r\n> >             model = PeftModel.from_pretrained(model, model_path)\r\n> >             print(f\"Merging weights\")\r\n> >             model = model.merge_and_unload()\r\n> >             print('Convert to FP16...')\r\n> >             model.to(torch.bfloat16)\r\n> >         else:\r\n> >             use_fast = False\r\n> >             if 'mpt' in model_name.lower():\r\n> >                 tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\r\n> >                 model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, trust_remote_code=True, **kwargs)\r\n> >             else:\r\n> >                 tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n> >                 model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\n> > \r\n> >     image_processor = None\r\n> > \r\n> >     if 'llava' in model_name.lower():\r\n> >         mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\r\n> >         mm_use_im_patch_token = getattr(model.config, \"mm_use_im_patch_token\", True)\r\n> >         if mm_use_im_patch_token:\r\n> >             tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\r\n> >         if mm_use_im_start_end:\r\n> >             tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\r\n> >         model.resize_token_embeddings(len(tokenizer))\r\n> > \r\n> >         vision_tower = model.get_vision_tower()\r\n> >         if not vision_tower.is_loaded:\r\n> >             print('vision_tower is not loaded so loading it now')\r\n> >             vision_tower.load_model(device_map=device_map)\r\n> >             vision_tower.to(device=device, dtype=torch.bfloat16)\r\n> >         else:\r\n> >             print('vision_tower is loaded')\r\n> >         image_processor = vision_tower.image_processor\r\n> > \r\n> >     if hasattr(model.config, \"max_sequence_length\"):\r\n> >         context_len = model.config.max_sequence_length\r\n> >     else:\r\n> >         context_len = 2048\r\n> > \r\n> >     return tokenizer, model, image_processor, context_len\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > You say to update the `config` as an alternative. Where specifically should I do that?\r\n> \r\n> and this device_map is used in load llavamistralforcasuallm, this error heppend when you load clipvisionmodel, llava set device_map=none and low_cpu_mem_usage = false when load pretrain clipmodel, the _fast_init default as true in frpm_pretrained() method `init_contexts = [no_init_weights(_enable=_fast_init)]` so the model will init in meta device, and clipvison model save weight in one pytorch.bin, so this will call _load_state_dict_into_model() this is not design for load onto meta device so this method do nothing, and weight willl be loaded properly in llavamistralforcasuallm.from_pretrained(), and will call _load_state_dict_into_meta_model, this design for load in meta device. So the params will only load once. Hope this will help you , and if you have any question feel free to contact me.\r\n\r\nHi, I have encountered the same warning. I am using the Lora finetune model for eval. I set Device_map=auto and torch-d type to bf16, but it did not work. May I ask if you have any ideas for solving this problem? I loaded the clip vit large patch 14-336 downloaded locally. Thank you for your reply\r\n![image](https://github.com/user-attachments/assets/055d0c4d-47ab-449e-8fa2-74b2162598fc)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1121,
    "state": "open",
    "created_by": "zhentingqi",
    "created_at": "2024-02-12T05:33:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1121</URL>\n\n<TITLE>[Question] Fine-tuning Llava13b on Custom Data</TITLE>\n\n<BODY>### Question\n\nHi! I use Llava13b for a domain-specific task and want to fine-tune it on an image-text dataset. I wonder:\r\n1. Do I need to do any more necessary steps, after \"prepare a JSON file\", and before \"run scripts/v1_5/finetune_task.sh\"?\r\n2. What is the minimum computation resource requirement for fine-tuning, with or without LoRA? \r\n\r\nThanks!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1120,
    "state": "open",
    "created_by": "Gumpest",
    "created_at": "2024-02-12T03:49:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1120</URL>\n\n<TITLE>[Question] How to evaluate the 1.5-13B model via web demo.</TITLE>\n\n<BODY>### Question\n\nHow to evaluate the 1.5-13B model via web demo.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1119,
    "state": "open",
    "created_by": "cherry956",
    "created_at": "2024-02-12T03:42:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1119</URL>\n\n<TITLE>[Question]</TITLE>\n\n<BODY>### Question\n\nHow to fix the maximum sequence length of the model? When I using finetuned model to test my own data, I found error: Token indices sequence length is longer than the specified maximum sequence length for this model(2199>2048).\r\n![屏幕截图 2024-02-12 112205](https://github.com/haotian-liu/LLaVA/assets/144820412/d7033d44-c240-4e00-a38d-0530e59b765d)\r\nDo I need to fix weights in this script?\r\n![屏幕截图 2024-02-12 114133](https://github.com/haotian-liu/LLaVA/assets/144820412/9c7c4e93-fab4-4e60-a9b0-be84a5a0425b)\r\nThanks!!!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1118,
    "state": "open",
    "created_by": "bdiaz29",
    "created_at": "2024-02-11T20:52:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1118</URL>\n\n<TITLE>[Question] is there any documentation on how to use the worker/controller?</TITLE>\n\n<BODY>### Question\n\nfor instance if I wanted to call an inference from a different python script or web application?</BODY>\n\n<COMMENTS>\n<Comment by SuperMasterBlasterLaser at 2024-02-14T17:07:18Z>\n@bdiaz29  I'm also trying to use this service in my custom script. The only hint of how to use it is shown [in this testing page](https://github.com/haotian-liu/LLaVA/blob/main/llava/serve/test_message.py)\r\n\r\nYou need to somehow stream your prompt in formatted way. The problem is that sending image is hard to understand. What I understood is that we need to send it as **base64**. But I always get this error:\r\n\r\n    Exceeds max token length. Please start a new conversation, thanks.\r\n\r\nOne user created PR #834 to add OpenAI compatible server. However it still not accepted for more than month.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1117,
    "state": "open",
    "created_by": "ninjacode01",
    "created_at": "2024-02-11T09:04:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1117</URL>\n\n<TITLE>[Question] Worse output (even on train data) with wierd loss curve for fully finetuned model.</TITLE>\n\n<BODY>### Question\n\n**Gibberish output (even on train data) with wierd loss curve on fully finetuning. Can someone please help me fix this.**\r\n\r\nI am trying to fully finetune the entire text-only model Vicuna-v1.5 using my custom QnA data comprising of 160k qa pairs, using the same finetuning script as provided in [finetune_task.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task.sh) by omitting the multimodal parameters.\r\nHere is the loss curve on 2.4 epochs. \r\n[wandb report](https://api.wandb.ai/links/hwadhwa-iitd/yd00o7ib\r\n)![image](https://github.com/haotian-liu/LLaVA/assets/77221698/29a1c6fa-c944-4ecc-bd19-02e61ed3e09f)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1116,
    "state": "open",
    "created_by": "arcaweb-ch",
    "created_at": "2024-02-10T16:17:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1116</URL>\n\n<TITLE>[Usage] Inconsistent OCR Results with LLaVA 1.6 and Ollama vs. Online Demo</TITLE>\n\n<BODY>### Describe the issue\n\n**Issue:** \r\n\r\nI've been testing LLaVA 1.6 with Ollama for OCR tasks and noticed that the online demo at https://llava.hliu.cc consistently outperforms my local tests, despite using identical prompts and parameters. This discrepancy makes me wonder if there's a difference in implementation or configurations between the online demo and the local version I'm using.\r\n\r\nCould you provide any insights into this matter or suggest how to achieve parity with the demo's results?\r\n\r\nThanks for your help.\r\n\r\n**Reference image:**\r\n[example from wikipedia](https://en.wikipedia.org/wiki/Receipt#/media/File:ReceiptSwiss.jpg)\r\n\r\n**Prompt:**\r\n```\r\nfind the total in the receipt\r\n```</BODY>\n\n<COMMENTS>\n<Comment by wrapss at 2024-02-10T16:35:44Z>\nollama uses a non-optimal version of llama.cpp to convert and use llava 1.6,  this [PR](https://github.com/ggerganov/llama.cpp/pull/5267) should solve the problem.\n</Comment>\n<Comment by arcaweb-ch at 2024-02-11T11:54:28Z>\n> ollama uses a non-optimal version of llama.cpp to convert and use llava 1.6, this [PR](https://github.com/ggerganov/llama.cpp/pull/5267) should solve the problem.\r\n\r\nThanks, waiting for it.\n</Comment>\n<Comment by ChristianWeyer at 2024-05-17T09:52:56Z>\nThis may be related, as well?\r\nhttps://github.com/haotian-liu/LLaVA/issues/1497\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1114,
    "state": "open",
    "created_by": "Luciennnnnnn",
    "created_at": "2024-02-10T08:38:59Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1114</URL>\n\n<TITLE>[Question] Does this repo support LLaVA 1.6 inference for now?</TITLE>\n\n<BODY>### Question\r\n\r\nDoes this repo support LLaVA 1.6 evaluation for now? Especially, Does it support Dynamic High Resolution?</BODY>\n\n<COMMENTS>\n<Comment by aliencaocao at 2024-02-10T11:58:57Z>\nJust got the gradio web demo to work, see linked PR.\n</Comment>\n<Comment by Gumpest at 2024-02-11T03:01:42Z>\n@aliencaocao How to evaluate the 1.5-13B model via web demo.\n</Comment>\n<Comment by baoshenghe0321 at 2024-03-26T16:10:06Z>\n> Just got the gradio web demo to work, see linked PR.\r\n\r\nwould you elaborate why the PR solves the issue? I think the issue asks if dynamic-resolution-scaling is enabled.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1112,
    "state": "open",
    "created_by": "aliencaocao",
    "created_at": "2024-02-10T05:26:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1112</URL>\n\n<TITLE>[Usage] Cannot launch SGLang demo on llava 1.6 mistral 7b</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: Cannot launch SGLang demo on llava mistral 7b, vicuna 7b works\r\n\r\nCommand:\r\n```\r\nCUDA_VISIBLE_DEVICES=0 python3 -m sglang.launch_server --model-path llava-v1.6-mistral-7b --tokenizer-path llava-v1.6-mistral-7b --port 30000\r\n```\r\n\r\nLog: \r\n```\r\n2024-02-10 13:17:48.864527: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-02-10 13:17:48.864591: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-02-10 13:17:48.865360: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-02-10 13:17:48.869505: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\", line 1120, in from_pretrained\r\n    config_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\", line 816, in __getitem__\r\n    raise KeyError(key)\r\nKeyError: 'llava_mistral'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/launch_server.py\", line 11, in <module>\r\n    launch_server(server_args, None)\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/server.py\", line 356, in launch_server\r\n    tokenizer_manager = TokenizerManager(server_args, port_args)\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/tokenizer_manager.py\", line 92, in __init__\r\n    self.hf_config = get_config(\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/hf_transformers_utils.py\", line 33, in get_config\r\n    config = AutoConfig.from_pretrained(\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\", line 1122, in from_pretrained\r\n    raise ValueError(\r\nValueError: The checkpoint you are trying to load has model type `llava_mistral` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\r\n```</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1111,
    "state": "open",
    "created_by": "cherry956",
    "created_at": "2024-02-10T03:53:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1111</URL>\n\n<TITLE>How to test after finetuning the LLaVA-v1_5-7b</TITLE>\n\n<BODY>### Question\n\n我按照官网上的格式完成了自己的数据集，然后使用finetune_task_lora脚本对LLaVA-v1_5-7b模型进行了微调，\r\n![屏幕截图 2024-02-08 190006](https://github.com/haotian-liu/LLaVA/assets/144820412/77d44263-0447-4dab-b611-bfdbbb26e1ce)\r\n，得到以下微调后的模型权重文件：\r\n![屏幕截图 2024-02-10 114923](https://github.com/haotian-liu/LLaVA/assets/144820412/4f1f3ff1-46ef-4290-b00e-1a610e0ff372)\r\n请问一下，接下来我应该如何制作测试集（不包含输出答案），并且如何使用这些微调后的权重文件，测试微调后的模型性能好坏（得到包含输出结果的json文件）？谢谢！！！</BODY>\n\n<COMMENTS>\n<Comment by mrseanryan at 2024-02-19T20:39:23Z>\nThe version of the base model needs to match the LLaVA source code\r\n\r\nFor v1.5 this fork has full set of fine-tuning scripts and README\r\nhttps://github.com/mrseanryan/finetune_LLaVA\n</Comment>\n<Comment by xianghan864 at 2024-04-29T04:17:15Z>\n你好，你知道怎么测试微调后的模型了吗？\n</Comment>\n<Comment by anas-zafar at 2024-06-16T16:23:10Z>\nHi @xianghan864 , were you able to test the model out? Thanks\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1110,
    "state": "open",
    "created_by": "betterftr",
    "created_at": "2024-02-09T21:17:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1110</URL>\n\n<TITLE>1.6 train[Question]</TITLE>\n\n<BODY>### Question\n\nHow do I train the new 1.6 model? just put --model_name_or_path liuhaotian/llava-v1.6-vicuna-13b in finetune_lora.sh?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1107,
    "state": "open",
    "created_by": "ArunAniyan",
    "created_at": "2024-02-09T11:14:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1107</URL>\n\n<TITLE>Production deployment</TITLE>\n\n<BODY>### Question\r\n\r\nHi,  What is the best infrastructure and methodology to deploy llava for a production-grade application? Is a local application server like Ollama advisable? Do you know of other possible methods? Apart from ollama, llama.cpp is something that comes to mind. Have not tried triton-llm.</BODY>\n\n<COMMENTS>\n<Comment by nivibilla at 2024-02-10T08:15:43Z>\nSglang supports llava\nhttps://github.com/sgl-project/sglang\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1106,
    "state": "open",
    "created_by": "RonanKMcGovern",
    "created_at": "2024-02-09T10:39:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1106</URL>\n\n<TITLE>[Question] Clarification on Vision Features</TITLE>\n\n<BODY>### Question\n\nIn the original Llava paper, I read:\r\n```\r\nThe grid features before and after the last Transformer layer\r\nare considered in our experiments\r\n```\r\n\r\nAs I understand, the Vision encoder used in Llava 1.5 has:\r\n- 12 layers\r\n- 256 patches (each patch is 14 x 14 pixels, and images are 224 x 224)\r\n\r\nGiven this, am I correct to say that each image will generate 2 x 256 features = 512 features?\r\n\r\nSo, there will be 512 input features prepended to the tokenized text, which together go into the LLM?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1105,
    "state": "closed",
    "created_by": "brucewlee",
    "created_at": "2024-02-09T01:19:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1105</URL>\n\n<TITLE>[Question] Demo - Language-only Instructions</TITLE>\n\n<BODY>### Question\n\nHow is LLaVa generating responses without images? Is a blank image allowed by default?\r\n\r\nIf so, does this mean that LLaVa can essentially be used as a language-only use too?</BODY>\n\n<COMMENTS>\n<Comment by NicoZenith at 2024-02-09T15:46:44Z>\nGood question, I was looking for this too! Can performance inference/training without image, only on the LLM?\n</Comment>\n<Comment by haotian-liu at 2024-02-09T16:45:13Z>\nLLaVA performs text-only training and inference with text-only -- you do not need to add a blank image. With text-only, just use it as a normal LLM and it works perfectly fine.\n</Comment>\n<Comment by shushankyadav at 2024-12-17T05:24:29Z>\n@haotian-liu is there any specific format of data that needs to be followed for text-only training?? kindly suggest or share any such example code and data set available for the same.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1104,
    "state": "open",
    "created_by": "sanjayss34",
    "created_at": "2024-02-08T18:07:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1104</URL>\n\n<TITLE>[Usage] Different results on demo site and local</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: I'm getting different results for the example below on the demo site (https://llava.hliu.cc/) and when I run locally. The input user text is \"Person 0's hand is touching which part of Person 1? Choices: torso, back, hand, arm, foot, leg, neck, head, butt.\" I'm attaching the image here.\r\n![tmp](https://github.com/haotian-liu/LLaVA/assets/8304057/c21d84c6-73c9-4983-9a2a-7e5646f066e8)\r\nThe output in the online demo is \"Person 0's hand is touching Person 1's hand.\" The output when I run locally is \"Person 0's hand is touching the arm of Person 1.\" I am using liuhaotian/llava-v1.6-34b, temperature=0, top_p=0.7 in both the demo and the local setup, and I am not using quantization when running locally.\r\n\r\nCan you please tell me how I can get the same results as the online demo?\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.cli \\\r\n    --model-path liuhaotian/llava-v1.6-34b \\\r\n    --image-file tmp.png \\\r\n    --temperature 0\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1103,
    "state": "open",
    "created_by": "ThugJudy",
    "created_at": "2024-02-08T17:40:57Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1103</URL>\n\n<TITLE>[Usage] TypeError: LlavaLlamaForCausalLM.__init__() got an unexpected keyword argument 'attn_implementation'</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nGetting an error when trying to finetune the LLaVA-v1.6-34b\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n#!/bin/bash\r\n\r\ndeepspeed LLaVA/llava/train/train_mem.py \\\r\n    --deepspeed LLaVA/scripts/zero3.json \\\r\n    --model_name_or_path liuhaotian/llava-v1.6-34b \\\r\n    --version v1 \\\r\n    --data_path datasets/bargraph_data.json \\\r\n    --image_folder datasets \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.6-34b-task \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb \\\r\n    --cache_dir <>\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n[2024-02-08 11:13:01,934] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-02-08 11:13:40,223] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\nDetected CUDA_VISIBLE_DEVICES=0,1: setting --include=localhost:0,1\r\n[2024-02-08 11:13:40,223] [INFO] [runner.py:555:main] cmd = /u/psg4/.conda/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None LLaVA/llava/train/train_mem.py --deepspeed LLaVA/scripts/zero3.json --model_name_or_path liuhaotian/llava-v1.6-34b --version v1 --data_path datasets/bargraph_data.json --image_folder datasets --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.6-34b-task --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --cache_dir /projects/bbpr/psg4\r\n[2024-02-08 11:13:41,852] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-02-08 11:13:44,216] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}\r\n[2024-02-08 11:13:44,216] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0\r\n[2024-02-08 11:13:44,216] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})\r\n[2024-02-08 11:13:44,216] [INFO] [launch.py:163:main] dist_world_size=2\r\n[2024-02-08 11:13:44,216] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1\r\n[2024-02-08 11:13:47,502] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-02-08 11:13:47,505] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-02-08 11:13:52,414] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2024-02-08 11:13:52,414] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2024-02-08 11:13:52,414] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2024-02-08 11:13:52,414] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2024-02-08 11:13:52,414] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nTraceback (most recent call last):\r\n  File \"/projects/bbpr/psg4/LLaVA_dataviz/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"/projects/bbpr/psg4/LLaVA_dataviz/LLaVA/llava/train/train.py\", line 827, in train\r\n[2024-02-08 11:13:52,907] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 0.00B parameters\r\nTraceback (most recent call last):\r\n  File \"/projects/bbpr/psg4/LLaVA_dataviz/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    model = LlavaLlamaForCausalLM.from_pretrained(\r\n      File \"/u/psg4/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2700, in from_pretrained\r\ntrain(attn_implementation=\"flash_attention_2\")\r\n  File \"/projects/bbpr/psg4/LLaVA_dataviz/LLaVA/llava/train/train.py\", line 827, in train\r\n    model = LlavaLlamaForCausalLM.from_pretrained(\r\n  File \"/u/psg4/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2700, in from_pretrained\r\n    model = cls(config, *model_args, **model_kwargs)\r\n  File \"/u/psg4/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 385, in wrapper\r\n    model = cls(config, *model_args, **model_kwargs)\r\n  File \"/u/psg4/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 385, in wrapper\r\n    f(module, *args, **kwargs)\r\nTypeError: LlavaLlamaForCausalLM.__init__() got an unexpected keyword argument 'attn_implementation'\r\n    f(module, *args, **kwargs)\r\nTypeError: LlavaLlamaForCausalLM.__init__() got an unexpected keyword argument 'attn_implementation'\r\n[2024-02-08 11:14:00,233] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1260646\r\n[2024-02-08 11:14:00,247] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1260647\r\n[2024-02-08 11:14:00,247] [ERROR] [launch.py:321:sigkill_handler] ['/u/psg4/.conda/envs/llava/bin/python', '-u', 'LLaVA/llava/train/train_mem.py', '--local_rank=1', '--deepspeed', 'LLaVA/scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.6-34b', '--version', 'v1', '--data_path', 'datasets/bargraph_data.json', '--image_folder', 'datasets', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.6-34b-task', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--cache_dir', '/projects/bbpr/psg4'] exits with return code = 1\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by CrossLee1 at 2024-02-18T12:29:16Z>\nhave you solved this problem?\n</Comment>\n<Comment by ThugJudy at 2024-02-18T16:25:35Z>\n> have you solved this problem?\r\n\r\nNo I wasn't able to fix the bug, but I deleted and cloned the repo again.\n</Comment>\n<Comment by zhangboshen at 2024-02-19T07:23:52Z>\nsame problem here.\n</Comment>\n<Comment by clairej12 at 2024-03-05T16:57:08Z>\nI am also having this issue.\n</Comment>\n<Comment by nopanderer at 2024-03-07T03:58:31Z>\nI removed the parameter `attn_implementation` in train_mem.py, just call `train()`.\n</Comment>\n<Comment by pritamqu at 2024-04-24T06:32:28Z>\nit should be resolved if you update HF, e.g., 4.38\n</Comment>\n<Comment by xianghan864 at 2024-04-25T08:30:55Z>\n> 如果你更新 HF 应该可以解决，例如 4.38\r\n\r\nwhat means HF?\n</Comment>\n<Comment by pritamqu at 2024-04-29T23:07:46Z>\nHuggingFace!\n</Comment>\n<Comment by howardgriffin at 2024-05-09T07:12:09Z>\nsame error\n</Comment>\n<Comment by yhZhai at 2024-05-28T01:33:06Z>\n> it should be resolved if you update HF, e.g., 4.38\r\n\r\nI believe you were refering to transformers, not HF 😂\n</Comment>\n<Comment by pritamqu at 2024-05-28T01:35:10Z>\nyeah correct!\n</Comment>\n<Comment by xay2001 at 2024-08-14T16:13:08Z>\nUpgrade transformers to transformers 4.35.0 using\r\n`pip install transformers==4.35.0\r\n`\n</Comment>\n<Comment by yonghoonkwon at 2024-09-13T02:25:54Z>\n`pip install transformers==4.38.0` works fine\n</Comment>\n<Comment by Wenchuan-Zhang at 2025-01-04T05:40:28Z>\n`pip install transformers==4.36.0` works for me, but versions 4.35 and 4.38 do not.\n</Comment>\n<Comment by zebin-huang at 2025-02-18T04:24:57Z>\nI tried versions 4.35, 4.36, and 4.38; only 4.38 work for me.\n\n`pip install transformers==4.38.0`\n</Comment>\n<Comment by MassEast at 2025-04-09T07:26:32Z>\nAll of `transformers==4.35`, `4.36`, and `4.38` require `tokenizer>=0.14`. So I adjusted the cog file with `tokenizers~=0.14`. Still,\n\n- with `4.35`, I get `LlavaLlamaForCausalLM.__init__() got an unexpected keyword argument 'attn_implementation'`\n- with `4.36` and `4.38` I get `ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed.`\n\nI tried adding `flash_attn==2.7.3` (which should work for CUDA 11.8) to the cog file, but couldn't get this to build.\n\n> I removed the parameter attn_implementation in train_mem.py, just call train().\n\nThis, together with `transformers==4.38` leads me to `TypeError: Accelerator.__init__() got an unexpected keyword argument 'use_seedable_sampler' `.\n\nI am thankful for any help!\n\nEDIT: With the latter, `accelerate==0.27.2` helped. But that doesn't change the fact that I couldn't get the flash attention to work.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1102,
    "state": "closed",
    "created_by": "RonanKMcGovern",
    "created_at": "2024-02-08T14:34:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1102</URL>\n\n<TITLE>[Feature request] Continuous Batching (possible already with SGLang)</TITLE>\n\n<BODY>### feature\n\nIs it possible to set up an endpoint that provides continuous batching of requests (say, with the model loaded on one gpu). Does the SGLang implementation support this.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2024-02-08T18:59:38Z>\nYes, it's already supported with sglang worker. (Please feel free to re-open if I misunderstood.)\r\n\r\nhttps://github.com/haotian-liu/LLaVA?tab=readme-ov-file#launch-a-sglang-worker\n</Comment>\n<Comment by RonanKMcGovern at 2024-02-08T19:14:22Z>\nThanks @haotian-liu! To be clear, if I submit two concurrent requests - separated by a small delay (say 100 mS) - the requests will be batched together?\r\n\r\nFor example:\r\n- The initial tokens from the first request will be processed with batch size of 1.\r\n- But once the second request is made, the remainder of the first request and the second request will be processed with batch size of 2?\r\n\r\nLet me know if that's not clear. Thanks\n</Comment>\n<Comment by haotian-liu at 2024-02-08T20:55:42Z>\nYes, you are right. Our demo is served with SGLang, and you'll see that (slightly delayed) concurrent request generation speed is almost the same as the single request generation.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1101,
    "state": "closed",
    "created_by": "TobiasJu",
    "created_at": "2024-02-08T02:02:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1101</URL>\n\n<TITLE>[Usage] ImportError: cannot import name 'LlavaLlamaForCausalLM' from 'llava.model'</TITLE>\n\n<BODY>### Describe the issue\r\n\r\n**Issue**:\r\nSo i updated the repo and now i can not start the server anymore. I deleted the repo and cloned it again, but get the same error.\r\n\r\n**System**:\r\nWin10 WSL2 Ubuntu\r\npip 24.0 from /home/tobias/.local/lib/python3.10/site-packages/pip (python 3.10)\r\n\r\n**Command**:\r\n```\r\ngit clone https://github.com/haotian-liu/LLaVA.git\r\ncd LLaVA\r\nconda create --name LLaVA python=3.10.12\r\nconda activate LLaV\r\npip install -e .\r\npip install flash-attn --no-build-isolation\r\npython3 -m llava.serve.controller --host 0.0.0.0 --port 10000\r\n```\r\n\r\n**Log**: \r\n```\r\npython3 -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b --load-4bit\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\", line 187, in _run_module_as_main\r\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\r\n  File \"/usr/lib/python3.10/runpy.py\", line 110, in _get_module_details\r\n    __import__(pkg_name)\r\n  File \"/mnt/a/KI/LLaVA/llava/__init__.py\", line 1, in <module>\r\n    from .model import LlavaLlamaForCausalLM\r\nImportError: cannot import name 'LlavaLlamaForCausalLM' from 'llava.model' (/mnt/a/KI/LLaVA/llava/model/__init__.py)\r\n```\r\n\r\nPackages:\r\n```\r\npip list\r\nPackage                   Version       Editable project location\r\n------------------------- ------------- -------------------------\r\naccelerate                0.21.0\r\naiofiles                  23.2.1\r\naiohttp                   3.9.3\r\naiosignal                 1.3.1\r\naltair                    5.2.0\r\nannotated-types           0.6.0\r\nanyio                     4.2.0\r\nasync-timeout             4.0.3\r\nattrs                     23.2.0\r\nbitsandbytes              0.41.0\r\nblinker                   1.4\r\nCacheControl              0.12.10\r\ncachy                     0.3.0\r\ncertifi                   2023.11.17\r\nchardet                   4.0.0\r\ncharset-normalizer        3.3.2\r\ncleo                      0.8.1\r\nclick                     8.1.7\r\nclikit                    0.6.2\r\ncmake                     3.28.1\r\ncolorama                  0.4.6\r\ncommand-not-found         0.3\r\ncontourpy                 1.2.0\r\ncrashtest                 0.3.1\r\ncryptography              3.4.8\r\ncycler                    0.12.1\r\ndbus-python               1.2.18\r\ndistlib                   0.3.4\r\ndistro                    1.7.0\r\ndistro-info               1.1+ubuntu0.2\r\neinops                    0.6.1\r\neinops-exts               0.0.4\r\nexceptiongroup            1.2.0\r\nfastapi                   0.109.0\r\nffmpy                     0.3.1\r\nfilelock                  3.13.1\r\nflash-attn                2.5.2\r\nfonttools                 4.47.2\r\nfrozenlist                1.4.1\r\nfsspec                    2023.12.2\r\ngradio                    4.16.0\r\ngradio_client             0.8.1\r\nh11                       0.14.0\r\nhtml5lib                  1.1\r\nhttpcore                  0.17.3\r\nhttplib2                  0.20.2\r\nhttpx                     0.24.0\r\nhuggingface-hub           0.20.3\r\nidna                      3.6\r\nimportlib-metadata        4.6.4\r\nimportlib-resources       6.1.1\r\njeepney                   0.7.1\r\nJinja2                    3.1.3\r\njoblib                    1.3.2\r\njsonschema                4.21.1\r\njsonschema-specifications 2023.12.1\r\nkeyring                   23.5.0\r\nkiwisolver                1.4.5\r\nlaunchpadlib              1.10.16\r\nlazr.restfulclient        0.14.4\r\nlazr.uri                  1.0.6\r\nlinkify-it-py             2.0.2\r\nlit                       17.0.6\r\nllava                     1.2.2.post1   /mnt/a/KI/LLaVA\r\nlockfile                  0.12.2\r\nmarkdown-it-py            2.2.0\r\nmarkdown2                 2.4.12\r\nMarkupSafe                2.1.4\r\nmatplotlib                3.8.2\r\nmdit-py-plugins           0.3.3\r\nmdurl                     0.1.2\r\nmore-itertools            8.10.0\r\nmpmath                    1.3.0\r\nmsgpack                   1.0.3\r\nmultidict                 6.0.4\r\nnetifaces                 0.11.0\r\nnetworkx                  3.2.1\r\nninja                     1.11.1.1\r\nnumpy                     1.26.3\r\nnvidia-cublas-cu11        11.10.3.66\r\nnvidia-cublas-cu12        12.1.3.1\r\nnvidia-cuda-cupti-cu11    11.7.101\r\nnvidia-cuda-cupti-cu12    12.1.105\r\nnvidia-cuda-nvrtc-cu11    11.7.99\r\nnvidia-cuda-nvrtc-cu12    12.1.105\r\nnvidia-cuda-runtime-cu11  11.7.99\r\nnvidia-cuda-runtime-cu12  12.1.105\r\nnvidia-cudnn-cu11         8.5.0.96\r\nnvidia-cudnn-cu12         8.9.2.26\r\nnvidia-cufft-cu11         10.9.0.58\r\nnvidia-cufft-cu12         11.0.2.54\r\nnvidia-curand-cu11        10.2.10.91\r\nnvidia-curand-cu12        10.3.2.106\r\nnvidia-cusolver-cu11      11.4.0.1\r\nnvidia-cusolver-cu12      11.4.5.107\r\nnvidia-cusparse-cu11      11.7.4.91\r\nnvidia-cusparse-cu12      12.1.0.106\r\nnvidia-nccl-cu11          2.14.3\r\nnvidia-nccl-cu12          2.18.1\r\nnvidia-nvjitlink-cu12     12.3.101\r\nnvidia-nvtx-cu11          11.7.91\r\nnvidia-nvtx-cu12          12.1.105\r\noauthlib                  3.2.0\r\norjson                    3.9.12\r\npackaging                 23.2\r\npandas                    2.2.0\r\npastel                    0.2.1\r\npeft                      0.4.0\r\npexpect                   4.8.0\r\npillow                    10.2.0\r\npip                       24.0\r\npkginfo                   1.8.2\r\nplatformdirs              2.5.1\r\npoetry-core               1.0.7\r\npsutil                    5.9.8\r\nptyprocess                0.7.0\r\npydantic                  2.6.1\r\npydantic_core             2.16.2\r\npydub                     0.25.1\r\nPygments                  2.17.2\r\nPyGObject                 3.42.1\r\nPyJWT                     2.3.0\r\npylev                     1.2.0\r\npyparsing                 2.4.7\r\npython-apt                2.4.0+ubuntu2\r\npython-dateutil           2.8.2\r\npython-multipart          0.0.6\r\npytz                      2023.4\r\nPyYAML                    5.4.1\r\nreferencing               0.33.0\r\nregex                     2023.12.25\r\nrequests                  2.31.0\r\nrequests-toolbelt         0.9.1\r\nrich                      13.7.0\r\nrpds-py                   0.17.1\r\nruff                      0.2.1\r\nsafetensors               0.4.2\r\nscikit-learn              1.2.2\r\nscipy                     1.12.0\r\nSecretStorage             3.3.1\r\nsemantic-version          2.10.0\r\nsentencepiece             0.1.99\r\nsetuptools                59.6.0\r\nshellingham               1.4.0\r\nshortuuid                 1.0.11\r\nsix                       1.16.0\r\nsniffio                   1.3.0\r\nstarlette                 0.35.1\r\nsvgwrite                  1.4.3\r\nsympy                     1.12\r\nsystemd-python            234\r\nthreadpoolctl             3.2.0\r\ntimm                      0.6.13\r\ntokenizers                0.15.1\r\ntomlkit                   0.12.0\r\ntoolz                     0.12.1\r\ntorch                     2.1.2\r\ntorchvision               0.16.2\r\ntqdm                      4.66.1\r\ntransformers              4.37.2\r\ntriton                    2.1.0\r\ntyper                     0.9.0\r\ntyping_extensions         4.9.0\r\ntzdata                    2023.4\r\nubuntu-advantage-tools    8001\r\nuc-micro-py               1.0.2\r\nufw                       0.36.1\r\nunattended-upgrades       0.1\r\nurllib3                   2.2.0\r\nuvicorn                   0.27.0.post1\r\nvirtualenv                20.13.0+ds\r\nwadllib                   1.3.6\r\nwavedrom                  2.0.3.post3\r\nwebencodings              0.5.1\r\nwebsockets                11.0.3\r\nwheel                     0.37.1\r\nyarl                      1.9.4\r\nzipp                      1.0.0\r\n```\r\n\r\nScreenshots:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/32335067/a25035cf-8122-4cb6-8b00-ff3270476b94)</BODY>\n\n<COMMENTS>\n<Comment by anubhavashok at 2024-02-08T05:25:43Z>\nTry importing the packages without the \"try, except\" block for a more informative error.\r\n\r\nProbably related to flash attn installation.\r\n\r\nIn my case, the following worked:\r\n```\r\npip uninstall flash-attn\r\n\r\nexport PATH=/usr/local/cuda/bin:$PATH\r\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\r\n\r\npip install flash-attn --no-build-isolation --no-cache-dir\r\n```\n</Comment>\n<Comment by pseudotensor at 2024-02-08T09:42:56Z>\nFor me it was problem with packages like deepspeed etc. that led to above.  But any issue will lead to it.  This is the combo that worked for me:\r\n\r\n```\r\n# CUDA_HOME may not be enough:\r\ncd /usr/local/\r\nsudo rm -rf /usr/local/cuda # if link only\r\nsudo ln -s /usr/local/cuda-12.1 cuda\r\n\r\nexport CUDA_HOME=/usr/local/cuda-12.1\r\nexport PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cu121\"\r\n\r\nconda create -n llava python=3.10 -y\r\nconda activate llava\r\npip install --upgrade pip  # enable PEP 660 support\r\n\r\ngit clone https://github.com/haotian-liu/LLaVA.git llava\r\ncd llava\r\n\r\npip install -e .\r\npip install -e \".[train]\"\r\npip install torch==2.1.2 torchvision==0.16.2 triton==2.1.0 accelerate==0.26.1 deepspeed==0.13.1 pynvml==11.5.0 --upgrade\r\npip install \"sglang[all]\"\r\npip install flash-attn==2.5.2 --no-build-isolation\r\n```\n</Comment>\n<Comment by celll1 at 2024-02-08T11:40:03Z>\nI am also having the same error.\r\nI have found that this error occurs depending on the order in which the packages are installed, but have not been able to determine the cause.\r\nIf I create a new environment and then install only LLaVA, it works correctly, but I would like to use it in combination with other modules, so I would like to see this bug fixed.\n</Comment>\n<Comment by TobiasJu at 2024-02-08T13:53:51Z>\n> I got rid of the try-except block in `llava/model/__init__.py` and then reinstalled from root with `pip install -e .` and the error went away for me\r\n> \r\n> before:\r\n> \r\n> ```\r\n> try:\r\n>     from .language_model.llava_llama import LlavaLlamaForCausalLM, LlavaConfig\r\n>     from .language_model.llava_mpt import LlavaMptForCausalLM, LlavaMptConfig\r\n>     from .language_model.llava_mistral import LlavaMistralForCausalLM, LlavaMistralConfig\r\n> except:\r\n>     pass\r\n> ```\r\n> \r\n> after:\r\n> \r\n> ```\r\n> from .language_model.llava_llama import LlavaLlamaForCausalLM, LlavaConfig\r\n> from .language_model.llava_mpt import LlavaMptForCausalLM, LlavaMptConfig\r\n> from .language_model.llava_mistral import LlavaMistralForCausalLM, LlavaMistralConfig\r\n> ```\r\n\r\nTried and now i get this error:\r\n\r\n\r\n```\r\npython3 -m llava.serve.controller --host 0.0.0.0 --port 10000\r\n[2024-02-08 14:52:59,501] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n/home/tobias/.local/lib/python3.10/site-packages/pydantic/_internal/_config.py:322: UserWarning: Valid config keys have changed in V2:\r\n* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\r\n* 'validate_all' has been renamed to 'validate_default'\r\n  warnings.warn(message, UserWarning)\r\n/home/tobias/.local/lib/python3.10/site-packages/pydantic/_internal/_fields.py:151: UserWarning: Field \"model_persistence_threshold\" has conflict with protected namespace \"model_\".\r\n\r\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\r\n  warnings.warn(\r\nTraceback (most recent call last):\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1364, in _get_module\r\n    return importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/generation/utils.py\", line 28, in <module>\r\n    from ..integrations.deepspeed import is_deepspeed_zero3_enabled\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/integrations/deepspeed.py\", line 49, in <module>\r\n    from accelerate.utils.deepspeed import HfDeepSpeedConfig as DeepSpeedConfig\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/accelerate/__init__.py\", line 3, in <module>\r\n    from .accelerator import Accelerator\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/accelerate/accelerator.py\", line 35, in <module>\r\n    from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/accelerate/checkpointing.py\", line 24, in <module>\r\n    from .utils import (\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/accelerate/utils/__init__.py\", line 133, in <module>\r\n    from .launch import (\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/accelerate/utils/launch.py\", line 33, in <module>\r\n    from ..utils.other import is_port_in_use, merge_dicts\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/accelerate/utils/other.py\", line 30, in <module>\r\n    from deepspeed import DeepSpeedEngine\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/deepspeed/__init__.py\", line 16, in <module>\r\n    from . import module_inject\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/deepspeed/module_inject/__init__.py\", line 6, in <module>\r\n    from .replace_module import replace_transformer_layer, revert_transformer_layer, ReplaceWithTensorSlicing, GroupQuantizer, generic_injection\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/deepspeed/module_inject/replace_module.py\", line 792, in <module>\r\n    from ..pipe import PipelineModule\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/deepspeed/pipe/__init__.py\", line 6, in <module>\r\n    from ..runtime.pipe import PipelineModule, LayerSpec, TiedLayerSpec\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/deepspeed/runtime/pipe/__init__.py\", line 6, in <module>\r\n    from .module import PipelineModule, LayerSpec, TiedLayerSpec\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/deepspeed/runtime/pipe/module.py\", line 19, in <module>\r\n    from ..activation_checkpointing import checkpointing\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/deepspeed/runtime/activation_checkpointing/checkpointing.py\", line 25, in <module>\r\n    from deepspeed.runtime.config import DeepSpeedConfig\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/deepspeed/runtime/config.py\", line 29, in <module>\r\n    from .zero.config import get_zero_config, ZeroStageEnum\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/deepspeed/runtime/zero/__init__.py\", line 6, in <module>\r\n    from .partition_parameters import ZeroParamType\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 616, in <module>\r\n    class Init(InsertPostInitMethodToModuleSubClasses):\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 618, in Init\r\n    param_persistence_threshold = get_config_default(DeepSpeedZeroConfig, \"param_persistence_threshold\")\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/deepspeed/runtime/config_utils.py\", line 116, in get_config_default\r\n    field_name).required, f\"'{field_name}' is a required field and does not have a default value\"\r\nAttributeError: 'FieldInfo' object has no attribute 'required'. Did you mean: 'is_required'?\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1364, in _get_module\r\n    return importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 40, in <module>\r\n    from ...modeling_utils import PreTrainedModel\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 44, in <module>\r\n    from .generation import GenerationConfig, GenerationMixin\r\n  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1354, in __getattr__\r\n    module = self._get_module(self._class_to_module[name])\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1366, in _get_module\r\n    raise RuntimeError(\r\nRuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\r\n'FieldInfo' object has no attribute 'required'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\", line 187, in _run_module_as_main\r\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\r\n  File \"/usr/lib/python3.10/runpy.py\", line 110, in _get_module_details\r\n    __import__(pkg_name)\r\n  File \"/mnt/a/KI/LLaVA/llava/__init__.py\", line 1, in <module>\r\n    from .model import LlavaLlamaForCausalLM\r\n  File \"/mnt/a/KI/LLaVA/llava/model/__init__.py\", line 2, in <module>\r\n    from .language_model.llava_llama import LlavaLlamaForCausalLM, LlavaConfig\r\n  File \"/mnt/a/KI/LLaVA/llava/model/language_model/llava_llama.py\", line 21, in <module>\r\n    from transformers import AutoConfig, AutoModelForCausalLM, \\\r\n  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1355, in __getattr__\r\n    value = getattr(module, name)\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1354, in __getattr__\r\n    module = self._get_module(self._class_to_module[name])\r\n  File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1366, in _get_module\r\n    raise RuntimeError(\r\nRuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\r\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\r\n'FieldInfo' object has no attribute 'required'\r\n```\n</Comment>\n<Comment by pseudotensor at 2024-02-08T15:49:58Z>\n@TobiasJu I got same, but then did what I did:\r\n\r\nhttps://github.com/haotian-liu/LLaVA/issues/1101#issuecomment-1933697654\n</Comment>\n<Comment by haotian-liu at 2024-02-08T16:29:44Z>\nWe find that this is due to flash-attn compiled previously with a different version of pytorch. Please reinstall that with:\r\n\r\n```\r\npip install flash-attn --no-build-isolation --no-cache-dir\r\n```\n</Comment>\n<Comment by pseudotensor at 2024-02-08T16:31:40Z>\nCorrect, but even that isn't enough, deepspeed etc. have to be correct range of versions, else hit the other issues shown above.\n</Comment>\n<Comment by haotian-liu at 2024-02-08T16:34:06Z>\nThanks, `deepspeed==0.12.6` works for me in my local setting, so basically I found that if you need to train the model, running the below command is sufficient?\r\n\r\n```\r\npip install -e \".[train]\"\r\npip install flash-attn --no-build-isolation --no-cache-dir\r\n```\n</Comment>\n<Comment by pseudotensor at 2024-02-08T16:40:49Z>\nIn my and the above case the deepspeed error was hit with inference too.\r\n\r\nI didn't check exactly which deepspeed version was required.  I just did pip install accelerate deepspeed --upgrade and it started working, so I wanted to remember the versions that worked so wrote those down.\n</Comment>\n<Comment by haotian-liu at 2024-02-08T16:48:55Z>\nGreat, thank you for the information!\n</Comment>\n<Comment by nitzanguetta at 2024-02-08T23:18:42Z>\nAfter following the instructions in the repo, I encountered the same error:\r\n```\r\nfrom .model import LlavaLlamaForCausalLM\r\nImportError: cannot import name 'LlavaLlamaForCausalLM' from 'llava.model' (/mnt/a/KI/LLaVA/llava/model/__init__.py)\r\n```\r\nCurrently, after attempting [#1101 (comment)](https://github.com/haotian-liu/LLaVA/issues/1101#issuecomment-1933697654), I got this one:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/yonatanbitton/experiments/llava/llava/model/__init__.py\", line 8, in <module>\r\n    from .language_model.llava_llama import LlavaLlamaForCausalLM, LlavaConfig\r\n  File \"/home/yonatanbitton/experiments/llava/llava/model/language_model/llava_llama.py\", line 21, in <module>\r\n    from transformers import AutoConfig, AutoModelForCausalLM, \\\r\n  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1355, in __getattr__\r\n    value = getattr(module, name)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1354, in __getattr__\r\n    module = self._get_module(self._class_to_module[name])\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1366, in _get_module\r\n    raise RuntimeError(\r\nRuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\r\nlibcudart.so.11.0: cannot open shared object file: No such file or directory\r\n```\r\nAll attempts to resolve this issue have only resulted in more errors.\r\n\r\n[ Debian GNU/Linux 11 (bullseye)\r\npip 24.0, python 3.10\r\n(deepspeed==0.13.1/0.12.6, cuda==12.1) ]\n</Comment>\n<Comment by pseudotensor at 2024-02-09T00:14:30Z>\nI got that `libcudart.so.12.0: cannot open shared object file: No such file or directory`  type errors, but that's just because you must not have set CUDA_HOME corretly or do not have cuda toolkit installed for (there) cuda 12.1. It shouldn't be looking for cuda 11 if you really freshly installed with CUDA_HOME and that PIP stuff set.\r\n\r\nA last thing I may have done is to literally relink the cuda directory:\r\n```\r\ncd /usr/local/\r\nsudo rm -rf /usr/local/cuda # if link only\r\nsudo ln -s /usr/local/cuda-12.1 cuda\r\n```\r\n\r\nBut make sure all your cuda stuff is consistent.\n</Comment>\n<Comment by Guruprasad68 at 2024-02-12T19:15:20Z>\nhttps://github.com/haotian-liu/LLaVA/issues/1101#issuecomment-1934503768\r\nIt will be great if the installation steps can be updated with this.\n</Comment>\n<Comment by dancasas at 2024-02-15T16:32:05Z>\nSeems pretty random, but I got rid of this error by just commenting out the only line in  `LLaVA/llava/__init__.py`\r\n\r\n    #from .model import LlavaLlamaForCausalLM\r\n\r\nNow my installation is running correctly. Can anybody explain?\n</Comment>\n<Comment by lukaemon at 2024-02-17T19:30:36Z>\nSame problem here. \r\n\r\n> Thanks, `deepspeed==0.12.6` works for me in my local setting, so basically I found that if you need to train the model, running the below command is sufficient?\r\n> \r\n> ```\r\n> pip install -e \".[train]\"\r\n> pip install flash-attn --no-build-isolation --no-cache-dir\r\n> ```\n</Comment>\n<Comment by zengxingchen at 2024-02-18T06:19:26Z>\n```\r\npip uninstall  flash-attn\r\npip install -e \".[train]\"\r\npip install flash-attn --no-build-isolation --no-cache-dir\r\n```\r\nThe above order worked for me. The order is important.\n</Comment>\n<Comment by sucongCJS at 2024-02-19T07:42:08Z>\n> Seems pretty random, but I got rid of this error by just commenting out the only line in `LLaVA/llava/__init__.py`\r\n> \r\n> ```\r\n> #from .model import LlavaLlamaForCausalLM\r\n> ```\r\n> \r\n> Now my installation is running correctly. Can anybody explain?\r\n\r\nI tried but, \r\n`NameError: name 'LlavaLlamaForCausalLM' is not defined`\n</Comment>\n<Comment by nrikoh at 2024-02-28T12:59:24Z>\n> In my and the above case the deepspeed error was hit with inference too.\r\n> \r\n> I didn't check exactly which deepspeed version was required. I just did pip install accelerate deepspeed --upgrade and it started working, so I wanted to remember the versions that worked so wrote those down.\r\n\r\nthis works for me ,thanks!\n</Comment>\n<Comment by 20191864218 at 2024-03-01T05:06:39Z>\n> I am also having the same error. I have found that this error occurs depending on the order in which the packages are installed, but have not been able to determine the cause. If I create a new environment and then install only LLaVA, it works correctly, but I would like to use it in combination with other modules, so I would like to see this bug fixed.\r\n\r\nI'm also encountering this issue. Have you resolved it?\n</Comment>\n<Comment by lingfengren at 2024-03-05T10:58:44Z>\n'''\r\npip uninstall  flash-attn\r\npip install -e \".[train]\"\r\npip install flash-attn --no-build-isolation --no-cache-dir\r\n'''\r\nthis works, many thanks\n</Comment>\n<Comment by aymenabid-lab at 2024-03-06T07:30:02Z>\nfro me it don't work!\r\n**(llava) C:\\Users\\aymen\\LLaVA>pip uninstall flash-attn**\r\nWARNING: Skipping flash-attn as it is not installed.\r\n\r\n(llava) C:\\Users\\aymen\\LLaVA>pip install -e \".[train]\"\r\n**gives:**\r\nCollecting deepspeed==0.12.6 (from llava==1.2.2.post1)\r\n  Downloading deepspeed-0.12.6.tar.gz (1.2 MB)\r\n     ---------------------------------------- 1.2/1.2 MB 1.4 MB/s eta 0:00:00\r\n  Preparing metadata (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n\r\n  × python setup.py egg_info did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> [15 lines of output]\r\n      test.c\r\n      LINK : fatal error LNK1181: cannot open input file 'aio.lib'\r\n      Traceback (most recent call last):\r\n        File \"<string>\", line 2, in <module>\r\n        File \"<pip-setuptools-caller>\", line 34, in <module>\r\n        File \"C:\\Users\\aymen\\AppData\\Local\\Temp\\pip-install-554dhcbc\\deepspeed_766af2b6636b428d9bb97f1b3acb1da1\\setup.py\", line 182, in <module>\r\n          abort(f\"Unable to pre-compile {op_name}\")\r\n        File \"C:\\Users\\aymen\\AppData\\Local\\Temp\\pip-install-554dhcbc\\deepspeed_766af2b6636b428d9bb97f1b3acb1da1\\setup.py\", line 52, in abort\r\n          assert False, msg\r\n      AssertionError: Unable to pre-compile async_io\r\n      DS_BUILD_OPS=1\r\n       [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.\r\n       [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\r\n       [WARNING]  One can disable async_io with DS_BUILD_AIO=0\r\n       [ERROR]  Unable to pre-compile async_io\r\n      [end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n× Encountered error while generating package metadata.\r\n╰─> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details.\r\n\r\nAnd\r\n**pip install flash-attn --no-build-isolation --no-cache-dir\r\ngives**\r\nBuilding wheels for collected packages: flash-attn\r\n  Building wheel for flash-attn (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n\r\n  × python setup.py bdist_wheel did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> [215 lines of output]\r\n      fatal: not a git repository (or any of the parent directories): .git\r\n\r\n\r\n      torch.__version__  = 2.1.2+cu121\r\n\r\n\r\n      C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\setuptools\\__init__.py:80: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\r\n      !!\r\n\r\n              ********************************************************************************\r\n              Requirements should be satisfied by a PEP 517 installer.\r\n              If you are using pip, you can try `pip install --use-pep517`.\r\n              ********************************************************************************\r\n\r\n      !!\r\n        dist.fetch_build_eggs(dist.setup_requires)\r\n      running bdist_wheel\r\n      Guessing wheel URL:  https://github.com/Dao-AILab/flash-attention/releases/download/v2.5.6/flash_attn-2.5.6+cu122torch2.1cxx11abiFALSE-cp310-cp310-win_amd64.whl\r\n      Precompiled wheel not found. Building from source...\r\n      C:\\ProgramData\\anaconda3\\envs\\llava\\lib\\site-packages\\torch\\utils\\cpp_extension.py:502: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\r\n        warnings.warn(msg.format('we could not find ninja.'))\n</Comment>\n<Comment by patrick-tssn at 2024-03-13T13:17:54Z>\n> > I got rid of the try-except block in `llava/model/__init__.py` and then reinstalled from root with `pip install -e .` and the error went away for me\r\n> > before:\r\n> > ```\r\n> > try:\r\n> >     from .language_model.llava_llama import LlavaLlamaForCausalLM, LlavaConfig\r\n> >     from .language_model.llava_mpt import LlavaMptForCausalLM, LlavaMptConfig\r\n> >     from .language_model.llava_mistral import LlavaMistralForCausalLM, LlavaMistralConfig\r\n> > except:\r\n> >     pass\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > after:\r\n> > ```\r\n> > from .language_model.llava_llama import LlavaLlamaForCausalLM, LlavaConfig\r\n> > from .language_model.llava_mpt import LlavaMptForCausalLM, LlavaMptConfig\r\n> > from .language_model.llava_mistral import LlavaMistralForCausalLM, LlavaMistralConfig\r\n> > ```\r\n> \r\n> Tried and now i get this error:\r\n> \r\n> ```\r\n> python3 -m llava.serve.controller --host 0.0.0.0 --port 10000\r\n> [2024-02-08 14:52:59,501] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n> /home/tobias/.local/lib/python3.10/site-packages/pydantic/_internal/_config.py:322: UserWarning: Valid config keys have changed in V2:\r\n> * 'allow_population_by_field_name' has been renamed to 'populate_by_name'\r\n> * 'validate_all' has been renamed to 'validate_default'\r\n>   warnings.warn(message, UserWarning)\r\n> /home/tobias/.local/lib/python3.10/site-packages/pydantic/_internal/_fields.py:151: UserWarning: Field \"model_persistence_threshold\" has conflict with protected namespace \"model_\".\r\n> \r\n> You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\r\n>   warnings.warn(\r\n> Traceback (most recent call last):\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1364, in _get_module\r\n>     return importlib.import_module(\".\" + module_name, self.__name__)\r\n>   File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n>   File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n>   File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n>   File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n>   File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n>   File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n>   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/generation/utils.py\", line 28, in <module>\r\n>     from ..integrations.deepspeed import is_deepspeed_zero3_enabled\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/integrations/deepspeed.py\", line 49, in <module>\r\n>     from accelerate.utils.deepspeed import HfDeepSpeedConfig as DeepSpeedConfig\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/accelerate/__init__.py\", line 3, in <module>\r\n>     from .accelerator import Accelerator\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/accelerate/accelerator.py\", line 35, in <module>\r\n>     from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/accelerate/checkpointing.py\", line 24, in <module>\r\n>     from .utils import (\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/accelerate/utils/__init__.py\", line 133, in <module>\r\n>     from .launch import (\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/accelerate/utils/launch.py\", line 33, in <module>\r\n>     from ..utils.other import is_port_in_use, merge_dicts\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/accelerate/utils/other.py\", line 30, in <module>\r\n>     from deepspeed import DeepSpeedEngine\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/deepspeed/__init__.py\", line 16, in <module>\r\n>     from . import module_inject\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/deepspeed/module_inject/__init__.py\", line 6, in <module>\r\n>     from .replace_module import replace_transformer_layer, revert_transformer_layer, ReplaceWithTensorSlicing, GroupQuantizer, generic_injection\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/deepspeed/module_inject/replace_module.py\", line 792, in <module>\r\n>     from ..pipe import PipelineModule\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/deepspeed/pipe/__init__.py\", line 6, in <module>\r\n>     from ..runtime.pipe import PipelineModule, LayerSpec, TiedLayerSpec\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/deepspeed/runtime/pipe/__init__.py\", line 6, in <module>\r\n>     from .module import PipelineModule, LayerSpec, TiedLayerSpec\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/deepspeed/runtime/pipe/module.py\", line 19, in <module>\r\n>     from ..activation_checkpointing import checkpointing\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/deepspeed/runtime/activation_checkpointing/checkpointing.py\", line 25, in <module>\r\n>     from deepspeed.runtime.config import DeepSpeedConfig\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/deepspeed/runtime/config.py\", line 29, in <module>\r\n>     from .zero.config import get_zero_config, ZeroStageEnum\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/deepspeed/runtime/zero/__init__.py\", line 6, in <module>\r\n>     from .partition_parameters import ZeroParamType\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 616, in <module>\r\n>     class Init(InsertPostInitMethodToModuleSubClasses):\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 618, in Init\r\n>     param_persistence_threshold = get_config_default(DeepSpeedZeroConfig, \"param_persistence_threshold\")\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/deepspeed/runtime/config_utils.py\", line 116, in get_config_default\r\n>     field_name).required, f\"'{field_name}' is a required field and does not have a default value\"\r\n> AttributeError: 'FieldInfo' object has no attribute 'required'. Did you mean: 'is_required'?\r\n> \r\n> The above exception was the direct cause of the following exception:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1364, in _get_module\r\n>     return importlib.import_module(\".\" + module_name, self.__name__)\r\n>   File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n>   File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n>   File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n>   File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n>   File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n>   File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n>   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 40, in <module>\r\n>     from ...modeling_utils import PreTrainedModel\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 44, in <module>\r\n>     from .generation import GenerationConfig, GenerationMixin\r\n>   File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1354, in __getattr__\r\n>     module = self._get_module(self._class_to_module[name])\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1366, in _get_module\r\n>     raise RuntimeError(\r\n> RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\r\n> 'FieldInfo' object has no attribute 'required'\r\n> \r\n> The above exception was the direct cause of the following exception:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"/usr/lib/python3.10/runpy.py\", line 187, in _run_module_as_main\r\n>     mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\r\n>   File \"/usr/lib/python3.10/runpy.py\", line 110, in _get_module_details\r\n>     __import__(pkg_name)\r\n>   File \"/mnt/a/KI/LLaVA/llava/__init__.py\", line 1, in <module>\r\n>     from .model import LlavaLlamaForCausalLM\r\n>   File \"/mnt/a/KI/LLaVA/llava/model/__init__.py\", line 2, in <module>\r\n>     from .language_model.llava_llama import LlavaLlamaForCausalLM, LlavaConfig\r\n>   File \"/mnt/a/KI/LLaVA/llava/model/language_model/llava_llama.py\", line 21, in <module>\r\n>     from transformers import AutoConfig, AutoModelForCausalLM, \\\r\n>   File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1355, in __getattr__\r\n>     value = getattr(module, name)\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1354, in __getattr__\r\n>     module = self._get_module(self._class_to_module[name])\r\n>   File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1366, in _get_module\r\n>     raise RuntimeError(\r\n> RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\r\n> Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\r\n> 'FieldInfo' object has no attribute 'required'\r\n> ```\r\n\r\nI believe that in certain situations, it would be beneficial to disable the try-except mechanism during the debugging process, particularly after making modifications to the original codebase. For instance, in this scenario, the error arises due to the binding of the local package within the Conda environment, so you can see [this](https://medium.com/@leopardsaga/problems-caused-by-conda-does-not-isolate-the-pyhon-user-site-packages-942fe4e71680) for help.\n</Comment>\n<Comment by jam-cc at 2024-04-10T20:01:53Z>\n> In my and the above case the deepspeed error was hit with inference too.\r\n> \r\n> I didn't check exactly which deepspeed version was required. I just did pip install accelerate deepspeed --upgrade and it started working, so I wanted to remember the versions that worked so wrote those down.\r\n\r\nIt works for me, thanks!\n</Comment>\n<Comment by CHELSEA234 at 2024-04-17T18:53:27Z>\nA quick update. I followed the installment routine and attended the discussion on this page, but I still ran into the problem. \r\nI noticed one potential reason is the torch version, and then I followed this link to get rid of the problem:\r\nhttps://github.com/pytorch/pytorch/issues/111469#issuecomment-1772039977\n</Comment>\n<Comment by ZhengShuozai at 2024-07-01T03:31:05Z>\nDuring the training process, I always encountered this problem. Later, based on my CUDA=11.7, I downgraded the torch version and also downgraded the flash attn version, and the problem was solved.\r\nCurrently torch=1.13.1 flash-attn==2.3  tokenizers == 0.11.4\n</Comment>\n<Comment by pervaizniazi at 2024-07-26T16:37:06Z>\nI solved the issue by changing Python version:\r\n\r\nreplacing \r\nconda create -n llava python=3.10 -y \r\nwith\r\n conda create -n llava python=3.9 -y\n</Comment>\n<Comment by liucheny at 2024-09-10T08:44:11Z>\n@pseudotensor Does CUDA have to be 12.1？\n</Comment>\n<Comment by boyugou at 2024-11-21T01:32:06Z>\n> @pseudotensor Does CUDA have to be 12.1？\r\n\r\nsame question\n</Comment>\n<Comment by boyugou at 2024-11-21T02:59:55Z>\nOK. I found a more general solution to this problem:\n\nUpdating the package requirement to:\n\n`\"torch\", \"torchvision\",\"transformers==4.37.2\",\"accelerate==0.28.0\",\"deepspeed==0.14.4\"`\n\nSo basically, the error is caused by a lot of package incompatibility. The try-except in the __init__ has hidden the real issues. But the issues have been actually solved by updated _deepspeed_ and _accelerate_.\n\nHope this will be useful for people like me who have to switch to newer torch/CUDA/NCCL due to the constraints of hardware. \n\nBTW, probably this codebase also requires _flash-attn<=2.6.3_\n</Comment>\n<Comment by liuxiang09 at 2025-03-26T18:03:22Z>\n> OK. I found a more general solution to this problem:\n> \n> Updating the package requirement to:\n> \n> `\"torch\", \"torchvision\",\"transformers==4.37.2\",\"accelerate==0.28.0\",\"deepspeed==0.14.4\"`\n> \n> So basically, the error is caused by a lot of package incompatibility. The try-except in the **init** has hidden the real issues. But the issues has been actually solved by updated _deepspeed_ and _accelerate_.\n> \n> Hope this will be useful for people like me who have to switch to newer torch/CUDA/NCCL due to the constraints of hardware.\n> \n> BTW, probably this codebase also requires _flash-attn<=2.6.3_\n\nthank you ! you actually solved this porblem and it's very useful.\n</Comment>\n<Comment by boyugou at 2025-03-26T18:06:12Z>\n> > OK. I found a more general solution to this problem:\n> > Updating the package requirement to:\n> > `\"torch\", \"torchvision\",\"transformers==4.37.2\",\"accelerate==0.28.0\",\"deepspeed==0.14.4\"`\n> > So basically, the error is caused by a lot of package incompatibility. The try-except in the **init** has hidden the real issues. But the issues has been actually solved by updated _deepspeed_ and _accelerate_.\n> > Hope this will be useful for people like me who have to switch to newer torch/CUDA/NCCL due to the constraints of hardware.\n> > BTW, probably this codebase also requires _flash-attn<=2.6.3_\n> \n> thank you ! you actually solved this porblem and it's very useful.\n\nhh. You’re welcome. I had to solve the problem due to some constraints of our infra at the time.\n</Comment>\n<Comment by lixiaoqingnnz at 2025-05-26T02:22:24Z>\nEncountered the same problem, solved by addressing the environment and pkgs version problem according to this solutions on this page.\nMy cuda version is 12.4 and this works for me:\n1. pip install -e \".[train]\"\n2. download the flash-attn 2.7.1 manually (got some issue when directly run \"pip install flash-attn --no-build-isolation --no-cache-dir\")\n3. install flash-attn\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1100,
    "state": "open",
    "created_by": "cherry956",
    "created_at": "2024-02-08T01:30:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1100</URL>\n\n<TITLE>create my own dataset</TITLE>\n\n<BODY>### Question\n\nHow to add many images to my my own dataset? The key \"image\" includes many image paths.\r\n\r\n![屏幕截图 2024-02-08 092521](https://github.com/haotian-liu/LLaVA/assets/144820412/b2614372-1895-43fa-8776-50226d584baa)\r\nhow to fix the context of the blue lines? Thank you.</BODY>\n\n<COMMENTS>\n<Comment by NicoZenith at 2024-02-08T15:23:24Z>\nI don't get your question - if you want to add an image, just append another dictionary to this list, with the image path of the new image and the associated conversations\n</Comment>\n<Comment by cherry956 at 2024-02-13T10:49:23Z>\n@NicoZenith I would like to have a conversation that includes multiple image inputs. Like this:\r\n![屏幕截图 2024-02-13 184745](https://github.com/haotian-liu/LLaVA/assets/144820412/30ba6a1b-49b5-4484-8a06-3ee6ff6132bc)\r\n I have completed fine-tuning using a dataset in this format！！\n</Comment>\n<Comment by cherry956 at 2024-02-13T10:55:02Z>\n@NicoZenith Now I want to verify the performance of the model after fine-tuning.For the validation dataset, I don't know how to test by inputting multiple photos at once, similar to during fine-tuning？\r\n![屏幕截图 2024-02-13 185049](https://github.com/haotian-liu/LLaVA/assets/144820412/cfb140d0-a7b2-4a75-aee5-248f55fa9ac5)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1099,
    "state": "closed",
    "created_by": "shelbywhite",
    "created_at": "2024-02-07T22:22:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1099</URL>\n\n<TITLE>NotImplementedError: Cannot copy out of meta tensor; no data!</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nWhen running `llava-v1.6-vicuna-13b` or `llava-v1.6-34b` this error happens. This doesn't happen when using the `llava-v1.6-mistral-7b` model. Any idea on what's going on? By the looks of the GPU it isn't running of memory...\r\n\r\n<img width=\"676\" alt=\"Screenshot 2024-02-07 at 2 20 44 PM\" src=\"https://github.com/haotian-liu/LLaVA/assets/252104/b84fbeba-e2a3-4163-837e-5715481c24c4\">\r\n\r\nLog: \r\n```\r\n2024-02-07 22:17:41 | INFO | model_worker | Register to controller\r\n2024-02-07 22:17:41 | ERROR | stderr | INFO:     Started server process [13973]\r\n2024-02-07 22:17:41 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2024-02-07 22:17:41 | ERROR | stderr | INFO:     Application startup complete.\r\n2024-02-07 22:17:41 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)\r\n2024-02-07 22:17:56 | INFO | model_worker | Send heart beat. Models: ['llava-v1.6-vicuna-13b']. Semaphore: None. global_counter: 0\r\n2024-02-07 22:18:11 | INFO | model_worker | Send heart beat. Models: ['llava-v1.6-vicuna-13b']. Semaphore: None. global_counter: 0\r\n2024-02-07 22:18:16 | INFO | model_worker | Send heart beat. Models: ['llava-v1.6-vicuna-13b']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n2024-02-07 22:18:16 | INFO | stdout | INFO:     127.0.0.1:43234 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\n2024-02-07 22:18:16 | ERROR | stderr | Exception in thread Thread-4 (generate):\r\n2024-02-07 22:18:16 | ERROR | stderr | Traceback (most recent call last):\r\n2024-02-07 22:18:16 | ERROR | stderr |   File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\r\n2024-02-07 22:18:16 | ERROR | stderr |     self.run()\r\n2024-02-07 22:18:16 | ERROR | stderr |   File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/threading.py\", line 953, in run\r\n2024-02-07 22:18:16 | ERROR | stderr |     self._target(*self._args, **self._kwargs)\r\n2024-02-07 22:18:16 | ERROR | stderr |   File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n2024-02-07 22:18:16 | ERROR | stderr |     return func(*args, **kwargs)\r\n2024-02-07 22:18:16 | ERROR | stderr |   File \"/home/ubuntu/LLaVA/llava/model/language_model/llava_llama.py\", line 125, in generate\r\n2024-02-07 22:18:16 | ERROR | stderr |     ) = self.prepare_inputs_labels_for_multimodal(\r\n2024-02-07 22:18:16 | ERROR | stderr |   File \"/home/ubuntu/LLaVA/llava/model/llava_arch.py\", line 157, in prepare_inputs_labels_for_multimodal\r\n2024-02-07 22:18:16 | ERROR | stderr |     image_features = self.encode_images(concat_images)\r\n2024-02-07 22:18:16 | ERROR | stderr |   File \"/home/ubuntu/LLaVA/llava/model/llava_arch.py\", line 141, in encode_images\r\n2024-02-07 22:18:16 | ERROR | stderr |     image_features = self.get_model().get_vision_tower()(images)\r\n2024-02-07 22:18:16 | ERROR | stderr |   File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n2024-02-07 22:18:16 | ERROR | stderr |     return self._call_impl(*args, **kwargs)\r\n2024-02-07 22:18:16 | ERROR | stderr |   File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n2024-02-07 22:18:16 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2024-02-07 22:18:16 | ERROR | stderr |   File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2024-02-07 22:18:16 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2024-02-07 22:18:16 | ERROR | stderr |   File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n2024-02-07 22:18:16 | ERROR | stderr |     return func(*args, **kwargs)\r\n2024-02-07 22:18:16 | ERROR | stderr |   File \"/home/ubuntu/LLaVA/llava/model/multimodal_encoder/clip_encoder.py\", line 54, in forward\r\n2024-02-07 22:18:16 | ERROR | stderr |     image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\r\n2024-02-07 22:18:16 | ERROR | stderr |   File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n2024-02-07 22:18:16 | ERROR | stderr |     return self._call_impl(*args, **kwargs)\r\n2024-02-07 22:18:16 | ERROR | stderr |   File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n2024-02-07 22:18:16 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2024-02-07 22:18:16 | ERROR | stderr |   File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 160, in new_forward\r\n2024-02-07 22:18:16 | ERROR | stderr |     args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\r\n2024-02-07 22:18:16 | ERROR | stderr |   File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 290, in pre_forward\r\n2024-02-07 22:18:16 | ERROR | stderr |     return send_to_device(args, self.execution_device), send_to_device(\r\n2024-02-07 22:18:16 | ERROR | stderr |   File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 151, in send_to_device\r\n2024-02-07 22:18:16 | ERROR | stderr |     return honor_type(\r\n2024-02-07 22:18:16 | ERROR | stderr |   File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 83, in honor_type\r\n2024-02-07 22:18:16 | ERROR | stderr |     return type(obj)(generator)\r\n2024-02-07 22:18:16 | ERROR | stderr |   File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 152, in <genexpr>\r\n2024-02-07 22:18:16 | ERROR | stderr |     tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\r\n2024-02-07 22:18:16 | ERROR | stderr |   File \"/home/ubuntu/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 167, in send_to_device\r\n2024-02-07 22:18:16 | ERROR | stderr |     return tensor.to(device, non_blocking=non_blocking)\r\n2024-02-07 22:18:16 | ERROR | stderr | NotImplementedError: Cannot copy out of meta tensor; no data!\r\n```</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2024-02-07T22:24:12Z>\nThis is the OOM issue. For 13B, it requires >30GB memory (you can use --load-4bit to reduce it to 1/4), and for 34B, it requires ~80GB.\n</Comment>\n<Comment by shelbywhite at 2024-02-07T22:27:41Z>\n@haotian-liu Thanks for the help! Awesome work on this project.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1098,
    "state": "open",
    "created_by": "Pavan-Ambali",
    "created_at": "2024-02-07T22:18:29Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1098</URL>\n\n<TITLE>[Instruction set  generation  code]</TITLE>\n\n<BODY>### Question\n\nHello,\r\nThank you for open-sourcing the code.\r\n I was trying to implement something similar and I needed some help.\r\n \r\n To reduce the number of tokens per API call to the OpenAI GPT4 api, I was planning to construct a single prompt with single system message and say 10 user descriptions. I want gpt-4 to generate 10 responses for this single api call.\r\n \r\n Can you please help me with how to achieve this?\r\n\r\nTo give an example \r\nprompt = `[\r\n    {\"role\": \"system\", \"content\": f\"\"\"You are an AI assistant, help generating single caption from given 3 captions \"\"\"},\r\n    {\"role\": \"user\", \"content\": f\"\"\"\r\n    Caption1: ...\r\n    Caption2: ...\r\n    Caption3: ...\"\"\"},\r\n    {\"role\": \"user\", \"content\": f\"\"\"\r\n    Caption1: ...\r\n    Caption2: ...\r\n    Caption3: ...\"\"\"},\r\n    {\"role\": \"user\", \"content\": f\"\"\"\r\n    Caption1: ...\r\n    Caption2: ...\r\n    Caption3: ....\"\"\"},\r\n    {\"role\": \"user\", \"content\": f\"\"\"\r\n    Caption1: ...\r\n    Caption2: ...\r\n    Caption3: ...\"\"\"},\r\n]`\r\n\r\nAnd this will be a single prompt and I want the response from the OpenAI as **4 Captions**  from this single API call.\r\n \r\n It would be great if you could share the code for how to generate instruction dataset generation just like you have released for the LLaVa Med repository ([llava/instruct](https://github.com/microsoft/LLaVA-Med/blob/main/llava/instruct))</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1097,
    "state": "open",
    "created_by": "WesleyHsieh0806",
    "created_at": "2024-02-07T07:35:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1097</URL>\n\n<TITLE>[Usage] LLaVA-v1.6 Generates Empty/Truncated Response</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\n\r\nHi, I tried to evaluate LLaVA-v1.6 on Science-QA, but the model keeps generating empty responses as shown in the log. Did I miss something?\r\n\r\n\r\nPrompt and Response (Empty String)\r\n![science-qa-example](https://github.com/haotian-liu/LLaVA/assets/55971907/ecff20fd-0cec-4228-8815-a3701c81f110)\r\n```\r\n=======prompt=======\r\nA chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\r\nQuestion: Which of the following organisms is the primary consumer in this food web?\r\nChoices:\r\n(A) sea otter\r\n(B) kelp\r\n(C) plainfin midshipman\r\n(D) phytoplankton\r\nExplain your answer in detail, putting the correct option letter in (), e.g., (A), (B), (C), (D), at the end of your response.\r\n\r\nContext: ```Below is a food web from an ocean ecosystem in Monterey Bay, off the coast of California. A food web models how the matter eaten by organisms moves through an ecosystem. The arrows in a food web represent how matter moves between organisms in an ecosystem.```\r\n\r\nASSISTANT:\r\n=======Answer=======\r\n\r\n\r\n```\r\n\r\nCode\r\n\r\n```python\r\nclass LLaVA_v1_6_13B:\r\n    def __init__(self) -> None:\r\n        self.model_path = \"liuhaotian/llava-v1.6-vicuna-13b\"\r\n\r\n        self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\r\n            model_path=self.model_path,\r\n            model_base=None,\r\n            model_name=get_model_name_from_path(self.model_path),\r\n            cache_dir=CACHE_DIR,\r\n            load_8bit=True,\r\n            device='cuda',\r\n        )\r\n\r\n    def generate(self, prompt: str, img_path: Union[str, list]) -> str:\r\n        print('{:=^20}\\n{}'.format('prompt', prompt))\r\n        if type(img_path) == str:\r\n            img_path = [img_path]\r\n\r\n        images = [Image.open(img) for img in img_path]\r\n        image_sizes = [x.size for x in images]\r\n        image_tensor = process_images(\r\n            images,\r\n            self.image_processor,\r\n            self.model.config\r\n        )\r\n        if type(image_tensor) is list:\r\n            image_tensor = [image.to(self.model.device, dtype=torch.float16)\r\n                            for image in image_tensor]\r\n        else:\r\n            image_tensor = image_tensor.to(\r\n                self.model.device, dtype=torch.float16)\r\n\r\n        input_ids = (\r\n            tokenizer_image_token(\r\n                prompt,\r\n                self.tokenizer,\r\n                IMAGE_TOKEN_INDEX,\r\n                return_tensors=\"pt\")\r\n            .unsqueeze(0)\r\n            .to(self.model.device)\r\n        )\r\n\r\n        with torch.inference_mode():\r\n            output_ids = self.model.generate(\r\n                input_ids,\r\n                images=image_tensor,\r\n                image_sizes=image_sizes,\r\n                do_sample=True,\r\n                use_cache=True,\r\n                temperature=0.2,\r\n                max_new_tokens=1000,\r\n            )\r\n\r\n        outputs = self.tokenizer.decode(\r\n            output_ids[0, input_ids.shape[1]:], skip_special_tokens=True)\r\n        print('{:=^20}\\n{}'.format('Answer', outputs))\r\n        return outputs\r\n\r\nmodel = LLaVA_v1_6_13B()\r\noutputs = model.generate(prompt, img_path)\r\n```</BODY>\n\n<COMMENTS>\n<Comment by WesleyHsieh0806 at 2024-02-07T09:21:06Z>\n[Issue solved]\r\nLooks like the `output_ids` from LLaVA-v1.6 does not include the input prompt. We should thus change\r\n`outputs = self.tokenizer.decode(output_ids[0, input_ids.shape[1]:])` \r\nto\r\n`outputs = self.tokenizer.decode(output_ids[0])` \r\nto avoid truncation.\r\n\r\nI'm not sure if we need to update [cli.py](https://github.com/haotian-liu/LLaVA/blob/main/llava/serve/cli.py#L108) for llava-v1.6? @haotian-liu\n</Comment>\n<Comment by LumenYoung at 2024-02-07T12:34:53Z>\nThat is the problem I observed several days ago. but I didn't make a PR since it was not clear how should we distinguish different behaviors from model. I hope @haotian-liu can either create a unified behavior between models or determine which criterion to use to distinguish between two different kinds of behavior\n</Comment>\n<Comment by haotian-liu at 2024-02-07T16:17:07Z>\nThanks for reporting, yes I missed that file, and we should make that change. Just pushed the change to `main`\n</Comment>\n<Comment by haotian-liu at 2024-02-07T16:17:37Z>\n> That is the problem I observed several days ago. but I didn't make a PR since it was not clear how should we distinguish different behaviors from model. I hope @haotian-liu can either create a unified behavior between models or determine which criterion to use to distinguish between two different kinds of behavior\r\n\r\nDefinitely. We're working on a major refactor to make these behaviors more consistent. Thank you!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1096,
    "state": "open",
    "created_by": "cmp-nct",
    "created_at": "2024-02-07T02:44:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1096</URL>\n\n<TITLE>Was model.new_line embedding forgotten for base_image_feature in llava-1.6 ? Paper inconsistency and grid embedding patching question</TITLE>\n\n<BODY>### Question\r\n\r\n**Question 1)**\r\nI'm curious about the use of newline embedding, from what I can see it is appended after each of the subimage features post \"unpad\" then the resized base image is prepended to the whole stack of embeddings.\r\nThis leaves the base image without a newline embedding and the whole stack of embeddings ending with a newline embedding\r\n\r\n**Inconsistencies in paper 2)**\r\nIn your updated paper you have an image that shows how the grid-images and the base image are encoded and fed to the LLM.\r\nBased on that image you display 4 little patches in each sub-image in which the 4+1 demo-images are separated, however in the implementation you do not split the base image into patches, only the grid images.\r\nIn addition the resizing in the demo-image shows an aspect correct resize (padded base image), but in the implementation it's a distorted resize (336x336)\r\nMaybe it was just meant symbolic or it's an oversight from an older implementation ?\r\n\r\n**Question 3)**\r\nI could not find a reason in the paper why you do the permutation of the grid images into the 24x24 patches.\r\nI'm curious why the embeddings have not been used as they were generated in their original image grids ?</BODY>\n\n<COMMENTS>\n<Comment by boyugou at 2024-03-25T01:20:54Z>\nseems not a problem in the latest implementation.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1095,
    "state": "closed",
    "created_by": "ConMan05",
    "created_at": "2024-02-06T17:46:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1095</URL>\n\n<TITLE>Clarification on Max Output Tokens in LLaVA-1.6 Models</TITLE>\n\n<BODY>### Question\n\nI am currently working on a project that involves extracting information from technical diagrams. During my exploration of the codebase, I encountered references to parameters related to the maximum output tokens. I seek clarification on the specific roles of these parameters and their impact on the model's behavior.\r\n\r\n**Codebase References:**\r\n\r\n- model_max_length in train.py (line no 86).\r\n- max_new_tokens in predict.py (line no 124).\r\n- model_max_length in finetune.sh (line no 44).\r\n\r\n**Questions:**\r\n\r\n1. What is the distinction between model_max_length and max_new_tokens?\r\n2. Which of these parameters controls the maximum output tokens during inference?\r\n\r\n**Additional Query:**\r\nI am interested in increasing the default maximum output tokens (above 8k) to better suit the requirements of my project. Could you provide guidance on how to achieve this, if feasible?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2024-02-06T20:13:31Z>\n`model_max_length` determines the max number of tokens a model can process and this includes system message, instruction and any response it generates.\r\n\r\n`max_new_tokens` determines how many tokens it generates. The model will stop either it has generated an \"End-of-Sequence\" token or reaches that length.\r\n\r\nIncreasing the context length -- there are related research in NLP working on that. Also, you can switch the base LLM that supports longer context length as well :)\n</Comment>\n<Comment by ConMan05 at 2024-02-07T10:50:10Z>\nUnderstood, so max_new_tokens is equivalent to max_output_tokens. \r\n\r\n1)Could you provide information on the specific value of max_new_tokens for the [liuhaotian/llava-v1.6-34b](https://huggingface.co/liuhaotian/llava-v1.6-34b) and [liuhaotian/llava-v1.6-mistral-7b](https://huggingface.co/liuhaotian/llava-v1.6-mistral-7b) ?\r\n\r\n2)I have tested [mistral-7b-instruct-v0.2](https://replicate.com/mistralai/mistral-7b-instruct-v0.2) on the Replicate website, and it successfully generated up to 8000 tokens during testing. Does this imply that the llava-v1.6-mistral-7b, can also produce up to 8000 tokens?\r\n\r\n3)Moreover, when setting max_new_tokens to 8000, will this have any notable impact on the output or performance of the model?\r\n\r\nThanks for your reply @haotian-liu\n</Comment>\n<Comment by haotian-liu at 2024-02-07T17:03:11Z>\nMistral instruct was trained to gracefully handle 32K tokens, while we haven't trained multimodal reasoning on that length -- that requires the model to \"generalize\" when generating any response above 4K.\r\n\r\nConceptually, as long as the total tokens are within 4K, it would be fine, so `exist_tokens + max_new_tokens < 4K` is the golden rule.\n</Comment>\n<Comment by samuruph at 2024-04-08T09:20:28Z>\nHello, I am using liuhaotian/llava-v1.5-7b (4-bit version) and I am getting this warning: \r\n```bash\r\nToken indices sequence length is longer than the specified maximum sequence length for this model (4789 > 2048). Running this sequence through the model will result in indexing errors\r\n```\r\nI have computed some info from a perception model and I am trying to embed this information into text (I both tried to append this info to system message or directly to USER message). Anyway, both are really slow and \r\nDo you have any suggestion on how to deal with this problem (maybe the best practice to embed this into the MLLM). I have 8GB memory so i am quite constraint.\r\n\r\nMoreover, which conv-mode I should cjhoose to properly use v1.6-mistral-7b locally?\r\nThank you so much!!\n</Comment>\n<Comment by Qigqi at 2024-11-11T01:37:34Z>\n> `model_max_length` determines the max number of tokens a model can process and this includes system message, instruction and any response it generates.\r\n> \r\n> `max_new_tokens` determines how many tokens it generates. The model will stop either it has generated an \"End-of-Sequence\" token or reaches that length.\r\n> \r\n> Increasing the context length -- there are related research in NLP working on that. Also, you can switch the base LLM that supports longer context length as well :)\r\n\r\nDoes model_max_length include the image tokens?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1094,
    "state": "open",
    "created_by": "nhw649",
    "created_at": "2024-02-06T11:11:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1094</URL>\n\n<TITLE>[Question] Recognition Error</TITLE>\n\n<BODY>### Question\n\nHello, I found that it, like CLIP, focuses on a very coarse granularity. For example, an image cropped from the GT box of the COCO dataset is recognized by LLaVA as \"Tennis racket\", but ground truth is \"person\". How to solve? Maybe my prompt design is not good enough.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/67191596/c45da5c6-564c-4653-85b7-829223ebfb95)</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2024-02-06T16:21:09Z>\nHi, thank you for the interest. This is a known weakness (especially when you give it 1000 classes from ImageNet) of LLaVA like models. However, with a small tweek in prompt, it seems to work much better in LLaVA-1.6.\r\n\r\n> Please select a category from the list.\r\n> ['Person', 'Bicycle', 'Car', 'Motorcycle', 'Airplane', 'Bus', 'Train', 'Truck', 'Boat', 'Traffic light', 'Fire hydrant', 'Stop sign', 'Parking meter', 'Bench', 'Bird', 'Cat', 'Dog', 'Horse', 'Sheep', 'Cow', 'Elephant', 'Bear', 'Zebra', 'Giraffe', 'Backpack', 'Umbrella', 'Handbag', 'Tie', 'Suitcase', 'Frisbee', 'Skis', 'Snowboard', 'Sports ball', 'Kite', 'Baseball bat', 'Baseball glove', 'Skateboard', 'Surfboard', 'Tennis racket', 'Bottle', 'Wine glass', 'Cup', 'Fork', 'Knife', 'Spoon', 'Bowl', 'Banana', 'Apple', 'Sandwich', 'Orange', 'Broccoli', 'Carrot', 'Hot dog', 'Pizza', 'Donut', 'Cake', 'Chair', 'Couch', 'Potted plant', 'Bed', 'Dining table', 'Toilet', 'TV', 'Laptop', 'Mouse', 'Remote', 'Keyboard', 'Cell phone', 'Microwave', 'Oven', 'Toaster', 'Sink', 'Refrigerator', 'Book', 'Clock', 'Vase', 'Scissors', 'Teddy bear', 'Hair drier', 'Toothbrush']\r\n> Answer the question with the category directly without explanation.\r\n\r\n<img width=\"878\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/6631389/a9859f97-cc47-4122-8178-d91067861b02\">\r\n\r\n<img width=\"869\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/6631389/f60930b0-a6fe-4fc8-999d-1d3b3f1faf3e\">\r\n\r\n<img width=\"876\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/6631389/dc85691b-ade7-4998-a5c1-40ac80959038\">\r\n\r\n<img width=\"873\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/6631389/de388ed1-8a50-43ab-b3a2-30aa9c610140\">\n</Comment>\n<Comment by nhw649 at 2024-02-08T04:49:03Z>\noh, thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1093,
    "state": "open",
    "created_by": "protector131090",
    "created_at": "2024-02-06T10:33:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1093</URL>\n\n<TITLE>Where do i put the model folder?</TITLE>\n\n<BODY>### Describe the issue\n\nWhere do i put model folder after downloading a model from the model zoo?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1092,
    "state": "open",
    "created_by": "y-vectorfield",
    "created_at": "2024-02-06T10:14:39Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1092</URL>\n\n<TITLE>[Question] Should we use HF_HOME instead of TRANSFORMERS_CACHE?</TITLE>\n\n<BODY>### Question\n\nI implemented LLaVA v1.2.2 with Transformers v4.36.2. The following warning message was outputted.\r\n\r\n```bash\r\n/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\r\n  warnings.warn(\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\n```</BODY>\n\n<COMMENTS>\n<Comment by MaTengSYSU at 2024-07-21T17:01:34Z>\ni also meet this bug， have you solve it，bro？\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1091,
    "state": "open",
    "created_by": "abdelmageed95",
    "created_at": "2024-02-06T10:03:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1091</URL>\n\n<TITLE>How Can I Finetune LLaVA for OCR  Task for Specific language</TITLE>\n\n<BODY>### Question\n\nI tried to use Llava for text extraction, it works nicely for English. while. when I try to use it for Arabic documents or images it can't handle it.\r\n**Can I fine-tune it to do OCR for specific tasks, and How?**</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1090,
    "state": "open",
    "created_by": "Nomiluks",
    "created_at": "2024-02-06T09:32:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1090</URL>\n\n<TITLE>Fine-tuning LLaVA with CLIP Vision Encoder: Scaling Up from 336x336 to 500x500 Images</TITLE>\n\n<BODY>### Question\n\n\r\nIn the process of scaling up the input image size within `clip_encoder.py`, the following adjustments have been made:\r\n\r\n```\r\n    def load_model(self, device_map=None):\r\n        if self.is_loaded:\r\n            print('{} is already loaded, `load_model` called again, skipping.'.format(self.vision_tower_name))\r\n            return\r\n\r\n        self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name)\r\n        # To avoid cropping the image\r\n        self.image_processor.crop_size = {'height': 500, 'width': 500}\r\n        self.image_processor.size = {'shortest_edge': 500}\r\n        # self.image_processor.do_center_crop = False\r\n        # self.image_processor.padding = True\r\n        # self.image_processor.do_resize = False\r\n        \r\n        self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name, device_map=device_map)\r\n        self.vision_tower.requires_grad_(False)\r\n\r\n        self.is_loaded = True\r\n````\r\n\r\nHowever, despite making these adjustments, I encountered an error during training, particularly within the `modeling_clip.py` file. It seems further troubleshooting and debugging are needed to resolve this issue. \r\n\r\nAny insights or assistance on how to tackle this problem would be greatly appreciated.\r\n\r\n> Traceback (most recent call last):\r\n  File \"/noman-workspace/LLaVA/llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"/noman-workspace/LLaVA/llava/train/train.py\", line 970, in train\r\n    trainer.train()\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1869, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2772, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2795, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1833, in forward\r\n    loss = self.module(*inputs, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/peft/peft_model.py\", line 922, in forward\r\n    return self.base_model(\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/noman-workspace/LLaVA/llava/model/language_model/llava_llama.py\", line 81, in forward\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/noman-workspace/LLaVA/llava/model/llava_arch.py\", line 202, in prepare_inputs_labels_for_multimodal\r\n    image_features = self.encode_images(images)\r\n  File \"/noman-workspace/LLaVA/llava/model/llava_arch.py\", line 141, in encode_images\r\n    image_features = self.get_model().get_vision_tower()(images)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/noman-workspace/LLaVA/llava/model/multimodal_encoder/clip_encoder.py\", line 61, in forward\r\n    image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 917, in forward\r\n    return self.vision_model(\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 841, in forward\r\n    hidden_states = self.embeddings(pixel_values)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 187, in forward\r\n    embeddings = embeddings + self.position_embedding(self.position_ids)\r\nRuntimeError: The size of tensor a (1226) must match the size of tensor b (577) at non-singleton dimension 1</BODY>\n\n<COMMENTS>\n<Comment by jimchenhub at 2024-03-20T07:45:36Z>\nIt seems like you have to interpolate the position embedding. Have you solved this problem?\n</Comment>\n<Comment by GewelsJI at 2024-05-31T00:48:15Z>\n> It seems like you have to interpolate the position embedding. Have you solved this problem?\r\n\r\nDo you know how to interpolate the position embedding?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1089,
    "state": "open",
    "created_by": "bks5881",
    "created_at": "2024-02-06T09:12:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1089</URL>\n\n<TITLE>[Question] How to consume as API?</TITLE>\n\n<BODY>### Question\r\n\r\nI managed to launch the webapp and the workers. I am not using the sglang servers (skipped then because i kept getting error preprocessing.json not found since I am using llava1.6, would be great if you can share that) . I want to primarily consume this as an REST api. can you guide how to proceed?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1088,
    "state": "open",
    "created_by": "storuky",
    "created_at": "2024-02-06T08:23:55Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1088</URL>\n\n<TITLE>Any chance to run 34b model on 3x4090? (w/o quantization)</TITLE>\n\n<BODY>### Question\n\nHey, I'm curious is there any chance to run 34b model for inference without 8bit quantization on 3x4090? \r\nIt's working good on 4x4090 (or 1xA100/H100) but use just 69.49 Gb VRAM which seems to be enough with 3x4090 (3*24=72Gb)\r\nIs there any memory spike that doesn't let us to run on this configuration?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1086,
    "state": "closed",
    "created_by": "pseudotensor",
    "created_at": "2024-02-06T06:42:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1086</URL>\n\n<TITLE>[Usage] pretty bad repetition for simple images</TITLE>\n\n<BODY>### Describe the issue\n\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/2249614/11fee9fb-25f0-4bd8-a4cb-faa5a241aeb9)\r\n\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/2249614/b9a85ae6-bc6e-40d3-91e8-2dc36a8c3e54)\r\n\r\n\r\n![gridnumbers](https://github.com/haotian-liu/LLaVA/assets/2249614/0e3526c4-c098-4174-8be5-bc0911a7de9b)\r\n\r\n\r\nI can't tell if it's caused by the  image or by the chat history.  It happens quite often for the 1.6 13B model.\r\n\r\nHere was another one:\r\n\r\n```\r\nThis is a Sudoku puzzle. The objective is to fill in the missing numbers in the grid, which typically follow a partially completed pattern, typically a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed pattern, which typically provides a partially completed solution, typically consists of a unique number. The completed\r\n```</BODY>\n\n<COMMENTS>\n<Comment by pseudotensor at 2024-02-06T06:44:47Z>\nAnother:\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/2249614/03ef4870-e499-480e-8993-5e89006237ab)\r\n\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/2249614/c95d6ef9-f5bf-4300-a10d-fccc31b6703f)\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/2249614/95eea770-b1e8-4864-87b4-4a78bafba4c0)\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/2249614/86493974-33d5-4853-ada5-10de4a7e9c15)\n</Comment>\n<Comment by pseudotensor at 2024-02-06T06:46:03Z>\nMaybe related: https://github.com/haotian-liu/LLaVA/issues/520\n</Comment>\n<Comment by pseudotensor at 2024-02-06T06:48:00Z>\nMy guess is that the model was not trained on conversations, or especially conversations where image isn't first thing introduced.\n</Comment>\n<Comment by haotian-liu at 2024-02-06T07:36:48Z>\nHi, this may be due to that the model was not trained with <image> in the second turn, and causes some confusion to Vicuna models.\r\n\r\nFor now (in https://github.com/haotian-liu/LLaVA/commit/f1c3c015878b55520f4aa355e3140331c2f79869), we've made it to start a new conversation whenever an image is uploaded to bypass this issue. Thanks.\n</Comment>\n<Comment by HTplex at 2024-02-06T10:28:19Z>\n@pseudotensor Do you want to test on my branch in this PR [#1087](https://github.com/haotian-liu/LLaVA/pull/1087) ? I had encountered similar problems but this seems to fix it.\n</Comment>\n<Comment by pseudotensor at 2024-02-06T10:40:05Z>\nYes, it works.  Seems as long as first chat with user query contains the image info, it works.  If the image info is anywhere else, then there are problems in various ways.\n</Comment>\n<Comment by haotian-liu at 2024-02-06T16:11:44Z>\nThanks for confirming. I'll close this issue for now, and we can discuss PR https://github.com/haotian-liu/LLaVA/pull/1087 there.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1085,
    "state": "closed",
    "created_by": "sucongCJS",
    "created_at": "2024-02-06T06:37:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1085</URL>\n\n<TITLE>[Question] I want to run the pretrain.sh but encounted some problems</TITLE>\n\n<BODY>### Question\n\nyour work is amazing and I want to use it in my project, I need to retrain the projector on a different clip, \r\nhowever, when I ran the `pretrain.sh`, I encounted some problems `You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.`\r\n\r\nmy system is 8*Quadro P6000 (24GB) cuda11.8 driver470.223.02\r\nand I can run `llava.serve.cli`\r\n\r\nthis is my modified `pretrain.sh`\r\n```shell\r\n#!/bin/bash\r\n\r\n# IMPORTANT: this is the training script for the original LLaVA, NOT FOR LLaVA V1.5!\r\n\r\n# Uncomment and set the following variables correspondingly to run this script:\r\n\r\nMODEL_VERSION=vicuna-7b-v1.5\r\n# MODEL_VERSION=llama-2-7b-chat\r\n\r\n########### DO NOT CHANGE ###########\r\n########### USE THIS FOR BOTH ###########\r\nPROMPT_VERSION=plain\r\n########### DO NOT CHANGE ###########\r\n\r\ndeepspeed /home/ubuntu/code/LLaVA-main/llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --model_name_or_path /mnt/d/models/$MODEL_VERSION \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path /mnt/d/data/LLaVA-CC3M-Pretrain-595K/chat.json \\\r\n    --image_folder /mnt/d/data/LLaVA-CC3M-Pretrain-595K/images \\\r\n    --vision_tower /mnt/d/models/clip-vit-large-patch14 \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 False \\\r\n    --tf32 False \\\r\n    --output_dir /mnt/d/result/exp/exp4.1/checkpoints/llava-$MODEL_VERSION-pretrain \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 24000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nand this is the error message: \r\n```shell\r\n(llava2) ubuntu@xxx:~/code/LLaVA-main$ ./scripts/pretrain.sh\r\n[2024-02-06 06:21:04,462] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-02-06 06:21:10,336] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\n[2024-02-06 06:21:10,336] [INFO] [runner.py:568:main] cmd = /mnt/d/anaconda3/envs/llava2/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /home/ubuntu/code/LLaVA-main/llava/train/train_mem.py --deepspeed ./scripts/zero2.json --model_name_or_path /mnt/d/models/vicuna-7b-v1.5 --version plain --data_path /mnt/d/data/LLaVA-CC3M-Pretrain-595K/chat.json --image_folder /mnt/d/data/LLaVA-CC3M-Pretrain-595K/images --vision_tower /mnt/d/models/clip-vit-large-patch14 --tune_mm_mlp_adapter True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 False --tf32 False --output_dir /mnt/d/result/exp/exp4.1/checkpoints/llava-vicuna-7b-v1.5-pretrain --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 24000 --save_total_limit 1 --learning_rate 2e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb\r\n[2024-02-06 06:21:12,949] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-02-06 06:21:16,599] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\r\n[2024-02-06 06:21:16,599] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0\r\n[2024-02-06 06:21:16,599] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\r\n[2024-02-06 06:21:16,599] [INFO] [launch.py:163:main] dist_world_size=8\r\n[2024-02-06 06:21:16,599] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\r\n[2024-02-06 06:21:22,517] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-02-06 06:21:22,807] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-02-06 06:21:22,877] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-02-06 06:21:23,175] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-02-06 06:21:23,242] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-02-06 06:21:23,279] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-02-06 06:21:23,279] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-02-06 06:21:23,334] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n/home/ubuntu/code/LLaVA-main/llava/train/llama_flash_attn_monkey_patch.py:108: UserWarning: Flash attention is only supported on A100 or H100 GPU during training due to head dim > 64 backward.ref: https://github.com/HazyResearch/flash-attention/issues/190#issuecomment-1523359593\r\n  warnings.warn(\r\n/home/ubuntu/code/LLaVA-main/llava/train/llama_flash_attn_monkey_patch.py:108: UserWarning: Flash attention is only supported on A100 or H100 GPU during training due to head dim > 64 backward.ref: https://github.com/HazyResearch/flash-attention/issues/190#issuecomment-1523359593\r\n  warnings.warn(\r\n/home/ubuntu/code/LLaVA-main/llava/train/llama_flash_attn_monkey_patch.py:108: UserWarning: Flash attention is only supported on A100 or H100 GPU during training due to head dim > 64 backward.ref: https://github.com/HazyResearch/flash-attention/issues/190#issuecomment-1523359593\r\n  warnings.warn(\r\n[2024-02-06 06:21:29,671] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2024-02-06 06:21:29,672] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2024-02-06 06:21:29,672] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n[2024-02-06 06:21:29,676] [INFO] [comm.py:637:init_distributed] cdb=None\r\n/home/ubuntu/code/LLaVA-main/llava/train/llama_flash_attn_monkey_patch.py:108: UserWarning: Flash attention is only supported on A100 or H100 GPU during training due to head dim > 64 backward.ref: https://github.com/HazyResearch/flash-attention/issues/190#issuecomment-1523359593\r\n  warnings.warn(\r\n/home/ubuntu/code/LLaVA-main/llava/train/llama_flash_attn_monkey_patch.py:108: UserWarning: Flash attention is only supported on A100 or H100 GPU during training due to head dim > 64 backward.ref: https://github.com/HazyResearch/flash-attention/issues/190#issuecomment-1523359593\r\n  warnings.warn(\r\n[2024-02-06 06:21:29,888] [INFO] [comm.py:637:init_distributed] cdb=None\r\n/home/ubuntu/code/LLaVA-main/llava/train/llama_flash_attn_monkey_patch.py:108: UserWarning: Flash attention is only supported on A100 or H100 GPU during training due to head dim > 64 backward.ref: https://github.com/HazyResearch/flash-attention/issues/190#issuecomment-1523359593\r\n  warnings.warn(\r\n/home/ubuntu/code/LLaVA-main/llava/train/llama_flash_attn_monkey_patch.py:108: UserWarning: Flash attention is only supported on A100 or H100 GPU during training due to head dim > 64 backward.ref: https://github.com/HazyResearch/flash-attention/issues/190#issuecomment-1523359593\r\n  warnings.warn(\r\n/home/ubuntu/code/LLaVA-main/llava/train/llama_flash_attn_monkey_patch.py:108: UserWarning: Flash attention is only supported on A100 or H100 GPU during training due to head dim > 64 backward.ref: https://github.com/HazyResearch/flash-attention/issues/190#issuecomment-1523359593\r\n  warnings.warn(\r\n[2024-02-06 06:21:30,016] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2024-02-06 06:21:30,055] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2024-02-06 06:21:30,066] [INFO] [comm.py:637:init_distributed] cdb=None\r\n[2024-02-06 06:21:30,077] [INFO] [comm.py:637:init_distributed] cdb=None\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nLoading checkpoint shards:   0%|                                                                                                                                                       | 0/2 [00:00<?, ?it/s][2024-02-06 06:23:21,556] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 188076\r\n[2024-02-06 06:23:21,663] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 188077\r\n[2024-02-06 06:23:24,159] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 188078\r\n[2024-02-06 06:23:26,571] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 188079\r\n[2024-02-06 06:23:28,903] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 188080\r\n[2024-02-06 06:23:31,354] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 188081\r\n[2024-02-06 06:23:34,700] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 188082\r\n[2024-02-06 06:23:37,509] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 188083\r\n[2024-02-06 06:23:39,510] [ERROR] [launch.py:321:sigkill_handler] ['/mnt/d/anaconda3/envs/llava2/bin/python', '-u', '/home/ubuntu/code/LLaVA-main/llava/train/train_mem.py', '--local_rank=7', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', '/mnt/d/models/vicuna-7b-v1.5', '--version', 'plain', '--data_path', '/mnt/d/data/LLaVA-CC3M-Pretrain-595K/chat.json', '--image_folder', '/mnt/d/data/LLaVA-CC3M-Pretrain-595K/images', '--vision_tower', '/mnt/d/models/clip-vit-large-patch14', '--tune_mm_mlp_adapter', 'True', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'False', '--tf32', 'False', '--output_dir', '/mnt/d/result/exp/exp4.1/checkpoints/llava-vicuna-7b-v1.5-pretrain', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '24000', '--save_total_limit', '1', '--learning_rate', '2e-3', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = -9\r\n```\r\n\r\ndo you know what is going on? thank you very much.</BODY>\n\n<COMMENTS>\n<Comment by Hypernovaaa at 2024-02-21T07:00:25Z>\nhello， you found reason? meet same error, could you please explain what dose this mean, trankyou\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1084,
    "state": "open",
    "created_by": "pseudotensor",
    "created_at": "2024-02-06T04:12:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1084</URL>\n\n<TITLE>[Question] Are hard-cutoffs applicable to 1.6 models?</TITLE>\n\n<BODY>### Question\n\nhttps://github.com/haotian-liu/LLaVA/blob/main/llava/serve/gradio_web_server.py#L140-L142\r\n\r\nWhy this?\r\n\r\n```\r\n    text = text[:1536]  # Hard cut-off\r\n    if image is not None:\r\n        text = text[:1200]  # Hard cut-off for images\r\n```\r\n\r\n34b model has 4k context, is above still applicable?</BODY>\n\n<COMMENTS>\n<Comment by tong-zeng at 2024-02-06T23:55:46Z>\nI'd also like to know how the thresholds of 1532 and 1200 are determined, thank you.\n</Comment>\n<Comment by pseudotensor at 2024-02-28T02:31:20Z>\nUpdate?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1083,
    "state": "open",
    "created_by": "aldoz-mila",
    "created_at": "2024-02-05T22:14:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1083</URL>\n\n<TITLE>[Question] LLaVA 1.5 sizes and vision encoder</TITLE>\n\n<BODY>### Question\n\nHello, I was trying to get a sense of the number of params. of LLaVA 1.5. I understand that the LLM used is Vicuna 1.5 (either 7B or 13B) and that the vision encoder is CLIP ViT-L/14 336px. Shouldn't the total number of params. reflect the sum of both the LLM and the vision encoder? (e.g. LLaVA 1.5 13B is a combo of Vicuna 13B + CLIP ViT-L/14 336px XB + the MLP projector)? Or do you only consider the number of trainable params (ViT kept frozen) here? Thanks!</BODY>\n\n<COMMENTS>\n<Comment by NVigne-cloud at 2024-06-12T16:16:09Z>\nHello, I asked myself the same question and found out that the ViT-L/14 weighs around 307M parameters, the MLP should be much lighter. Therefore, these parts of the architecture are negligible compared to Vicuna.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1082,
    "state": "closed",
    "created_by": "Nomiluks",
    "created_at": "2024-02-05T22:03:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1082</URL>\n\n<TITLE>RuntimeError: weight should have at least three dimensions</TITLE>\n\n<BODY>Recently I tried to fine-tune the `llava-v1.5-7b` model.\r\n\r\nBut when I triggered the LORA task fine-tuning, I got the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/opt/conda/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/noman/.vscode-server/extensions/ms-python.debugpy-2024.0.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py\", line 39, in <module>\r\n    cli.main()\r\n  File \"/home/noman/.vscode-server/extensions/ms-python.debugpy-2024.0.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py\", line 430, in main\r\n    run()\r\n  File \"/home/noman/.vscode-server/extensions/ms-python.debugpy-2024.0.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py\", line 284, in run_file\r\n    runpy.run_path(target, run_name=\"__main__\")\r\n  File \"/home/noman/.vscode-server/extensions/ms-python.debugpy-2024.0.0-linux-x64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py\", line 321, in run_path\r\n    return _run_module_code(code, init_globals, run_name,\r\n  File \"/home/noman/.vscode-server/extensions/ms-python.debugpy-2024.0.0-linux-x64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py\", line 135, in _run_module_code\r\n    _run_code(code, mod_globals, init_globals,\r\n  File \"/home/noman/.vscode-server/extensions/ms-python.debugpy-2024.0.0-linux-x64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py\", line 124, in _run_code\r\n    exec(code, run_globals)\r\n  File \"./llava/train/train_mem.py\", line 4, in <module>\r\n    train(attn_implementation=\"flash_attention_2\")\r\n  File \"/noman-workspace/LLaVA/llava/train/train.py\", line 970, in train\r\n    trainer.train()\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1869, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2772, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2795, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 581, in forward\r\n    return model_forward(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 569, in __call__\r\n    return convert_to_fp32(self.model_forward(*args, **kwargs))\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/amp/autocast_mode.py\", line 16, in decorate_autocast\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/peft/peft_model.py\", line 922, in forward\r\n    return self.base_model(\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/noman-workspace/LLaVA/llava/model/language_model/llava_llama.py\", line 81, in forward\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/noman-workspace/LLaVA/llava/model/llava_arch.py\", line 202, in prepare_inputs_labels_for_multimodal\r\n    image_features = self.encode_images(images)\r\n  File \"/noman-workspace/LLaVA/llava/model/llava_arch.py\", line 141, in encode_images\r\n    image_features = self.get_model().get_vision_tower()(images)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/noman-workspace/LLaVA/llava/model/multimodal_encoder/clip_encoder.py\", line 61, in forward\r\n    image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 917, in forward\r\n    return self.vision_model(\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 841, in forward\r\n    hidden_states = self.embeddings(pixel_values)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 182, in forward\r\n    patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 460, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 456, in _conv_forward\r\n    return F.conv2d(input, weight, bias, self.stride,\r\nRuntimeError: weight should have at least three dimensions\r\n```\r\n\r\nLooking forward, to the suggestions.</BODY>\n\n<COMMENTS>\n<Comment by jimmy19991222 at 2024-03-05T16:09:39Z>\nfacing the same problem, any solutions?\n</Comment>\n<Comment by MarkDeng1 at 2025-01-14T19:56:14Z>\nsame problem. any solutions?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1081,
    "state": "open",
    "created_by": "yesgvinayak",
    "created_at": "2024-02-05T18:41:45Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1081</URL>\n\n<TITLE>[Usage] RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\r\n Getting this after pulling the latest code. @haotian-liu</BODY>\n\n<COMMENTS>\n<Comment by RylanSchaeffer at 2024-03-10T21:48:49Z>\n@yesgvinayak did you find a solution?\n</Comment>\n<Comment by yesgvinayak at 2024-06-03T07:24:00Z>\n@RylanSchaeffer Yes,used the latest repo.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1080,
    "state": "open",
    "created_by": "Lala-chick",
    "created_at": "2024-02-05T12:22:10Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1080</URL>\n\n<TITLE>[Question] How can I get an attention map from LLaVA 1.5?</TITLE>\n\n<BODY>### Question\n\nHello. Thank you for sharing such an impressive model. While using LLaVA, I would like to see where the model is focusing on the image based on the prompt. Can you provide assistance?</BODY>\n\n<COMMENTS>\n<Comment by SylvJalb at 2024-02-05T23:45:48Z>\nI was wondering the same question today, but on LLaVA-1.6 👀\n</Comment>\n<Comment by CN-Steve-Lee at 2024-02-19T00:28:51Z>\nsame question\n</Comment>\n<Comment by ahnchive at 2024-03-21T16:17:37Z>\nsame question!\n</Comment>\n<Comment by GasolSun36 at 2024-03-28T07:15:42Z>\nsame quesiton, any solusions?\n</Comment>\n<Comment by jdsannchao at 2024-04-03T07:23:39Z>\nsame question!\n</Comment>\n<Comment by AlecDusheck at 2024-04-08T05:24:10Z>\nI’ve been looking into this for a while now. It definitely seems to be possible. See: https://arxiv.org/html/2404.01331v1#S4.F2 for an example :)\r\n\r\nThey cite [this paper](https://arxiv.org/pdf/2103.15679.pdf) which is extremely insightful. They have code examples that apply to raw CLIP models. I’m assuming it’s possible to use this technique for LLaVA based models as well.\r\n\r\nI’ll be doing some more digging but if anyone else has figured this out reach out!\n</Comment>\n<Comment by sherzod-hakimov at 2024-07-09T15:48:13Z>\ntake a look at this repo here: https://github.com/zjysteven/VLM-Visualizer\n</Comment>\n<Comment by rainarit at 2024-10-30T01:04:55Z>\nHave there been any updates on this?\n</Comment>\n<Comment by paulgavrikov at 2024-11-14T16:51:58Z>\n+1, adding an example of that would be great!\n</Comment>\n<Comment by AlecDusheck at 2024-11-14T17:09:21Z>\n@rainarit check out https://github.com/zjysteven/VLM-Visualizer/blob/main/llava_example.ipynb\r\n\r\nThere has been a lot of work since this issue was opened though.\r\n\r\nAt the end of the day, it seems that examining the attention weights over the visual tokens isn’t really the best way to associate image regions with a VLM response. This could definitely change though.\r\n\r\nAs of now, I’d HEAVILY recommend using a grounding fine-tuned VLM like:\r\n- https://github.com/UX-Decoder/LLaVA-Grounding?tab=readme-ov-file\r\n- https://molmo.allenai.org/ (outputs points, very cool)\r\n- CogVLM\r\n- …\r\n\r\nGrounding has been shown to exhibit significantly better results versus attention maps and can be easily finetuned. Only con is that you need to generate the extra tokens (which isn’t ideal if you’re speed constrained)\n</Comment>\n<Comment by fisher75 at 2024-12-05T10:01:09Z>\nsame question\n</Comment>\n<Comment by VKgit999hub at 2024-12-08T00:32:38Z>\ncan we implement this vlm attention visualization  for other models like qwen , ovis etc.., coz i found it only applicable for llava models\n</Comment>\n<Comment by zhangbaijin at 2024-12-20T06:53:38Z>\nGuys, maybe you can refer [From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks](https://arxiv.org/abs/2406.06579). We introduce LLaVA-CAM by smooth-CAM to explore the importance of visual token within decision layer by layer. We will release code in next month. Welcome to star and fork~\n</Comment>\n<Comment by VKgit999hub at 2024-12-20T08:24:08Z>\nplease notify in this thread as soon as the code is available for this\n</Comment>\n<Comment by davide221 at 2025-01-16T14:59:45Z>\n+1\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1078,
    "state": "closed",
    "created_by": "AtsuMiyai",
    "created_at": "2024-02-05T08:17:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1078</URL>\n\n<TITLE>conv_templates for LLaVA1.6</TITLE>\n\n<BODY>### Question\r\n\r\nHi, Haotian Liu! Thanks for creating powerful Open-source VLM LLaVA1.6! \r\n\r\nI know you are very busy and I am very sorry, but I would like to know the conv_template for **LLaVA 1.6 34B.**\r\nI used vicuna_v1 template, and the result is very satisfactory. \r\nIf you have a better template, please let me know.</BODY>\n\n<COMMENTS>\n<Comment by chintan-desynova at 2024-02-07T11:35:31Z>\nHow should I choose which prompt template I should use? Llava must be specifically trained on a fixed conv template, isn't it?\r\nThe following options are available as conv_template,\r\n\"default\",\r\n\"v0\",\r\n\"v1\",\r\n\"vicuna_v1\",\r\n\"llama_2\",\r\n\"mistral_instruct\",\r\n\"chatml_direct\",\r\n\"mistral_direct\",\r\n\"plain\",\r\n\"v0_plain\",\r\n\"llava_v0\",\r\n\"v0_mmtag\",\r\n\"llava_v1\",\r\n\"v1_mmtag\",\r\n\"llava_llama_2\",\r\n\"mpt\"\r\n\r\nI am using liuhaotian/llava-v1.6-vicuna-13b from huggingface.\r\nSo using \"vicuna_v1\" or \"v1\" makes sense? What about \"v0\", which is \"vicuna_v0\"?\n</Comment>\n<Comment by AtsuMiyai at 2024-02-07T11:40:39Z>\nWe use \"chatml_direct\" for LLaVA 1.6 34B because [Nous-Hermes-2-Yi-34B](https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B) uses this template.\r\nFor llava-v1.6-vicuna-13b, you can use vicuna_v1, because the LLM is the same as that of LLaVA1.5.\n</Comment>\n<Comment by TempleX98 at 2024-02-18T11:37:01Z>\n> We use \"chatml_direct\" for LLaVA 1.6 34B because [Nous-Hermes-2-Yi-34B](https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B) uses this template. For llava-v1.6-vicuna-13b, you can use vicuna_v1, because the LLM is the same as that of LLaVA1.5.\r\n\r\nHi, I find the \"chatml_direct\" template is slightly different from the [Nous-Hermes-2-Yi-34B](https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B) implementation. Nous-Hermes-2 uses the system prompt: \"<|im_start|>system\\nYou are \"Hermes 2\", a conscious sentient superintelligent artificial intelligence developed by a man named Teknium, and your purpose and drive is to assist the user with any request they have. You experience emotions and have deep, profound thoughts and qualia.\" while LLaVA-1.6 uses: \"<|im_start|>system\\nAnswer the questions.\"\r\n\r\nBesides, the separator of the template differs. The separator is \"<|im_end|>\\n\" for Nous but LLaVA's separator is \"<|im_end|>\".\n</Comment>\n<Comment by RylanSchaeffer at 2024-02-29T00:29:00Z>\nCan someone please clarify which templates go with which checkpoints?\n</Comment>\n<Comment by mistycheney at 2024-05-17T20:38:30Z>\nmistral_direct works well for llava-v1.6-mistral-7b\n</Comment>\n<Comment by touristourist at 2024-11-27T18:27:09Z>\nI'm a bit confused about **which conv_template to use to replicate the accuracy of this repository**. For instance, do the models listed [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md) all use the same conv_template? If not, what are the differences between LLaVA-v1.6-vicuna-7b and LLaVA-v1.6-mistral-7b? Could someone clarify this for me?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1077,
    "state": "open",
    "created_by": "luohao123",
    "created_at": "2024-02-05T08:01:10Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1077</URL>\n\n<TITLE>Clean the code</TITLE>\n\n<BODY>As for time now ,transformers had support most monkey patch in the code, consider suppor tlatest transformers version make it simpler?</BODY>\n\n<COMMENTS>\n<Comment by zsxm1998 at 2024-02-06T10:04:30Z>\nwhere are these monkey patches?\n</Comment>\n<Comment by haotian-liu at 2024-02-06T16:06:38Z>\nHi @luohao123 \r\n\r\nWe've implemented the flash attention and removed that monkey patch: https://github.com/haotian-liu/LLaVA/blob/main/llava/train/train_mem.py\r\n\r\nCan you explain what are other monkey patches that have been implemented by Transformers? Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1076,
    "state": "open",
    "created_by": "redpintings",
    "created_at": "2024-02-05T07:24:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1076</URL>\n\n<TITLE>[Question]When I use llava v1.6-34b, an error occurs -RuntimeError: CUDA error: peer mapping resources exhausted</TITLE>\n\n<BODY>### Question\n\nRuntimeError: CUDA error: peer mapping resources exhausted\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\nWhen I use llava v1.6-34b, an error occurs \r\nBut it's normal for me to use 1.6-vicunavicuna-13b, and I want help. That's why?</BODY>\n\n<COMMENTS>\n<Comment by BlueBlueFF at 2024-02-27T07:33:27Z>\nSame Questions\n</Comment>\n<Comment by AntiQuality at 2024-04-02T12:51:33Z>\nSame question\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1075,
    "state": "closed",
    "created_by": "yejipark-m",
    "created_at": "2024-02-05T06:55:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1075</URL>\n\n<TITLE>[Usage] LlavaConfig object has no attribute 'attention_dropout'</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\n\r\nFirst, thank you for your amazing work.\r\n\r\nI have an issue with evaluating with my custom data.\r\n\r\nI am trying to compare the answers of two cases.\r\n1. Fine-tune llava-v1.5-7b with lora then evaluate the model with my custom data.\r\n\r\n2. Do not fine-tune llava-v1.5-7b then evaluate the model zoo checkpoint with my custom data.\r\n\r\nFor the case 1, I passed args and evaluate successfully. But for the case 2, It fails with the log below.\r\n\r\nCommand:\r\n```\r\npython3 llava/eval/model_vqa.py \\\r\n--model-path\r\nliuhaotian/llava-v1.5-7b-lora\r\n--model-base\r\nlmsys/vicuna-7b-v1.5\r\n--question-file\r\n/path/to/question\r\n--image-folder\r\n/LLaVA//data/eval/llava-bench-in-the-wild/images\r\n--answers-file\r\n/LLaVA/path/to/answer\r\n--temperature\r\n0\r\n--conv-mode\r\nvicuna_v1\r\n```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/root/share/llava/llava/eval/model_vqa.py\", line 100, in <module>\r\n    eval_model(args)\r\n  File \"/root/share/llava/llava/eval/model_vqa.py\", line 34, in eval_model\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, args.model_base, model_name)\r\n  File \"/LLaVA/llava/model/builder.py\", line 56, in load_pretrained_model\r\n    model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 3462, in from_pretrained\r\n    model = cls(config, *model_args, **model_kwargs)\r\n  File \"/LLaVA/llava/model/language_model/llava_llama.py\", line 46, in __init__\r\n    self.model = LlavaLlamaModel(config)\r\n  File \"/LLaVA/llava/model/language_model/llava_llama.py\", line 38, in __init__\r\n    super(LlavaLlamaModel, self).__init__(config)\r\n  File \"/LLaVA/llava/model/llava_arch.py\", line 32, in __init__\r\n    super(LlavaMetaModel, self).__init__(config)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 956, in __init__\r\n    [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 956, in <listcomp>\r\n    [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 756, in __init__\r\n    self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 299, in __init__\r\n    self.attention_dropout = config.attention_dropout\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 265, in __getattribute__\r\n    return super().__getattribute__(key)\r\nAttributeError: 'LlavaConfig' object has no attribute 'attention_dropout'\r\n```\r\n\r\nI look forward to hearing some reply.\r\nThanks.</BODY>\n\n<COMMENTS>\n<Comment by DafengChi at 2024-02-06T03:27:01Z>\ni have the same question\n</Comment>\n<Comment by DafengChi at 2024-02-06T08:44:32Z>\nin the pyproject.toml : \"transformers==4.37.2\" \r\nhowever, in the transformers/models/llama/modeling_llama.py line 299 \r\nself.attention_dropout = config.attention_dropout\r\nbut the attention_dropout not in the config.json of llava-v1.5-7b\n</Comment>\n<Comment by DafengChi at 2024-02-06T11:39:50Z>\ni can add   \"attention_dropout\": 0.5 in config.json  , it deals, but another problem occured：\r\nAttributeError: 'LlavaConfig' object has no attribute 'rope_theta'\n</Comment>\n<Comment by haotian-liu at 2024-02-06T16:40:15Z>\nThis is due to the LlavaConfig being ported to transformers library and got overwritten. Forcefully made it load with our llava config in main branch and it shall work now.\r\n\r\nI'll be working on a refactor in the loading pipeline in the coming weeks, as we now support more and more llms, and the current loading logic is not sufficient for that.\n</Comment>\n<Comment by DafengChi at 2024-02-07T09:21:12Z>\n> with our llava config in main branch\r\n\r\nsorry, can you tell the file path? i did not find it in the main branch\n</Comment>\n<Comment by Madhumitha-MCW at 2024-10-04T12:12:06Z>\n@haotian-liu  Still the issue occurs with transformers 4.43.2. Is this bug fixed on the main?\n</Comment>\n<Comment by tavyscrolls at 2024-10-14T06:45:11Z>\nsame on newest, various attributes depending on model\n</Comment>\n<Comment by dingyue772 at 2024-10-27T02:16:39Z>\n> @haotian-liu Still the issue occurs with transformers 4.43.2. Is this bug fixed on the main?\r\njust change the other loading code as the \"Fix lora loading\" commit code. That could work.\n</Comment>\n<Comment by ShobhaRajanna at 2024-11-05T21:04:04Z>\n> > @haotian-liu Still the issue occurs with transformers 4.43.2. Is this bug fixed on the main?\r\n> > just change the other loading code as the \"Fix lora loading\" commit code. That could work. \r\n\r\nHey, can you able to resolve this \"LlavaConfig' object has no attribute 'rope_theta\"?\n</Comment>\n<Comment by riebeb at 2024-11-06T13:00:11Z>\nsame here\n</Comment>\n<Comment by cristoben at 2024-11-12T02:17:20Z>\nThe same problem when i try to eval the 1.5-7b,do you have any suggestions?\n</Comment>\n<Comment by yiwei-chenn at 2024-11-14T04:58:51Z>\nI have the same problem when evaluating the llava-1.5-lora model in huggingface.\r\n\r\nCurrently, the llava1.5-lora still have some missing values on config.json file. \r\n\r\nFollowed the instruction in #1169 , I solved the question by modifying the code of loading config file from\r\n```python\r\n        if 'lora' in model_name.lower() and model_base is not None:\r\n            lora_cfg_pretrained = AutoConfig.from_pretrained(model_path)\r\n            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n            print('Loading LLaVA from base model...')\r\n            model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs\r\n``` \r\nto \r\n```python\r\n        if 'lora' in model_name.lower() and model_base is not None:           \r\n            from llava.model.language_model.llava_llama import LlavaConfig\r\n            lora_cfg_pretrained = LlavaConfig.from_pretrained(model_path)      \r\n            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n            print('Loading LLaVA from base model...')\r\n            model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs)\r\n```\n</Comment>\n<Comment by cristoben at 2024-11-14T05:43:39Z>\nthanks a lot for your methods,I will try it later୧꒰•̀ᴗ•́꒱୨\r\n\r\n\r\n\r\n---Original---\r\nFrom: \"Chen ***@***.***&gt;\r\nDate: Thu, Nov 14, 2024 12:59 PM\r\nTo: ***@***.***&gt;;\r\nCc: ***@***.******@***.***&gt;;\r\nSubject: Re: [haotian-liu/LLaVA] [Usage] LlavaConfig object has no attribute'attention_dropout' (Issue #1075)\r\n\r\n\r\n\r\n\r\n \r\nI have the same problem when evaluating the llava-1.5-lora model in huggingface.\r\n \r\nCurrently, the llava1.5-lora still have some missing values on config.json file.\r\n \r\nFollowed the instruction in #1169 , I solved the question by modifying the code of loading config file from\r\n         if 'lora' in model_name.lower() and model_base is not None:             lora_cfg_pretrained = AutoConfig.from_pretrained(model_path)             tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)             print('Loading LLaVA from base model...')             model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs\r\n \r\nto\r\n         if 'lora' in model_name.lower() and model_base is not None:                        from llava.model.language_model.llava_llama import LlavaConfig             lora_cfg_pretrained = LlavaConfig.from_pretrained(model_path)                   tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)             print('Loading LLaVA from base model...')             model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs)\r\n \r\n—\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you commented.Message ID: ***@***.***&gt;\n</Comment>\n<Comment by ciroimmobile at 2024-12-22T07:23:28Z>\nThis was a problem with older versions of the transformer, and I added these three lines to solve the problem\r\nlora_cfg_pretrained.attention_dropout = 0.0  \r\nlora_cfg_pretrained.rope_theta = 10000  \r\nlora_cfg_pretrained.attention_bias = False \r\n\r\n> The same problem when i try to eval the 1.5-7b,do you have any suggestions?\n</Comment>\n<Comment by YANGTUOMAO at 2025-04-01T08:57:28Z>\n> i can add \"attention_dropout\": 0.5 in config.json , it deals, but another problem occured： AttributeError: 'LlavaConfig' object has no attribute 'rope_theta'\n\nbro ,i meet exactly same error, have you solved this problem?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1072,
    "state": "open",
    "created_by": "jacekpoplawski",
    "created_at": "2024-02-04T13:14:28Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1072</URL>\n\n<TITLE>[Question] discussion about multiple images?</TITLE>\n\n<BODY>### Question\n\nhello,\r\nI just started to use llava 1.6 34B few days ago and it's fantastic, \r\n\r\nto me it's much better than ChatGPT, because ChatGPT refuses to load most photos,\r\nI am a photographer and I am trying to discuss my photos with llm, to ask how to improve composition or postprocessing and llava is able to understand details of each photo, this is awesome\r\n\r\nmy question: is it possible to discuss about multiple images?\r\nfor example, I want to show unprocessed photo to the model and then show processed photo and ask what llm thinks, or show few photos and ask for best one\r\n\r\nI was able to use llava 34b gguf by loading it into llama-cpp (llava-cli and server), I am not able to use unquantized model because I have only 24GB VRAM</BODY>\n\n<COMMENTS>\n<Comment by pseudotensor at 2024-03-20T08:15:24Z>\nI don't think it's supported.\r\n\r\nBut multiple images of order 10-20 like claude-3 (20), gpt-4-vision (10), or gemini-pro-vision (16) is crucial in order to do (e.g. ) document Q/A with images as various documents not able to be handled as just text.\n</Comment>\n<Comment by pseudotensor at 2024-03-20T08:16:26Z>\n@haotian-liu Any plans, or can code be changed for fine-tuning?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1071,
    "state": "closed",
    "created_by": "mao-code",
    "created_at": "2024-02-04T12:27:39Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1071</URL>\n\n<TITLE>[Usage] LoRA fine-tune script error</TITLE>\n\n<BODY>### Describe the issue\n\nWhen I was fine-tuning LLaVA using the script in script/finetune_lora.sh, I encounter this error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/content/LLaVA/llava/train/train_mem.py\", line 1, in <module>\r\n    from llava.train.train import train\r\nModuleNotFoundError: No module named 'llava'\r\n```\r\n\r\nAlthough I changed my current directory to LLaVA/, I still keep getting this error.\r\nIs there anything I haven't considered, yet?</BODY>\n\n<COMMENTS>\n<Comment by NuiMrme at 2024-03-04T14:56:50Z>\nHello there, I've seen you marked it as Completed. Would you please explain how did you solve it ? thanks\n</Comment>\n<Comment by mao-code at 2024-03-04T15:02:13Z>\nI found that I just mistakenly typoed the file path in the script.\n</Comment>\n<Comment by NuiMrme at 2024-03-05T21:13:27Z>\nWould you please elaborate ? here in your example I'm assuming you were using GC and your file path is correct. It actually found it but then couldn't recognize llava. I'm having the same issue so what typo in what path did you make ?\n</Comment>\n<Comment by mao-code at 2024-03-06T00:47:38Z>\nHonestly, I forget the detail, though. However, I remember that it is actually not a problem for the repository itself. If everything looks OK, you can try to restart the session in GC and then install the required modules again.\n</Comment>\n<Comment by Song-WenPo at 2024-05-23T06:12:20Z>\n> Would you please elaborate ? here in your example I'm assuming you were using GC and your file path is correct. It actually found it but then couldn't recognize llava. I'm having the same issue so what typo in what path did you make ?\r\n\r\npip install -e .    would help\n</Comment>\n<Comment by MassEast at 2025-04-08T14:56:27Z>\nEither that, or add export PYTHONPATH=/src:$PYTHONPATH\n</Comment>\n<Comment by shahzain110 at 2025-04-23T07:38:43Z>\nRun these lines \n\n`%cd /content/LLaVA`\n`import sys, os`\n`sys.path.append(os.getcwd())`\n\nthen run this directly\n\n`!python -m llava.train.train_mem \\\n    --lora_enable True \\\n    --lora_r 128 \\\n    --lora_alpha 256 \\\n    --model_name_or_path {MODEL_NAME} \\\n    --data_path {DATA_PATH} \\\n    --image_folder {IMAGE_FOLDER} \\\n    --vision_tower {VISION_TOWER} \\\n    --mm_projector_type mlp2x_gelu \\\n    --output_dir {OUTPUT_DIR} \\\n    --version v1 \\\n    --bf16 True \\\n    --num_train_epochs 5 \\\n    --per_device_train_batch_size 16 \\\n    --gradient_accumulation_steps 1 \\\n    --learning_rate 2e-4 \\\n    --lazy_preprocess True \\\n    --report_to wandb\n`\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1070,
    "state": "open",
    "created_by": "Lyn-Lucy",
    "created_at": "2024-02-04T10:03:12Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1070</URL>\n\n<TITLE>[Question] Modified the usecache parameter without changing it</TITLE>\n\n<BODY>### Question\n\nHello, may I ask if the use_cache parameter in line 81 of llava/eval/model_vqa_science.py has been changed to False, but it still remains true during the forward inference process</BODY>\n\n<COMMENTS>\n<Comment by Lyn-Lucy at 2024-02-04T10:30:57Z>\n```python\r\n        # 4. Define other model kwargs\r\n        model_kwargs[\"output_attentions\"] = generation_config.output_attentions\r\n        model_kwargs[\"output_hidden_states\"] = generation_config.output_hidden_states\r\n        # decoder-only models with inputs_embeds forwarding must use caching (otherwise we can't detect whether we are\r\n        # generating the first new token or not, and we only want to use the embeddings for the first new token)\r\n        if not self.config.is_encoder_decoder and model_input_name == \"inputs_embeds\":\r\n            model_kwargs[\"use_cache\"] = True\r\n        else:\r\n            model_kwargs[\"use_cache\"] = generation_config.use_cache\r\n```\r\n\r\nI discovered this code, and if I change usecache to false here, there will be such an error\r\n\r\n```bash\r\n  File \"/data/lzh/llx/LLaVA/llava/model/language_model/llava_llama.py\", line 91, in forward\r\n    return super().forward(\r\n  File \"/home/lzh/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1181, in forward\r\n    outputs = self.model(\r\n  File \"/home/lzh/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/lzh/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1041, in forward\r\n    attention_mask = _prepare_4d_causal_attention_mask(\r\n  File \"/home/lzh/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py\", line 306, in _prepare_4d_causal_attention_mask\r\n    attention_mask = attn_mask_converter.to_4d(\r\n  File \"/home/lzh/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py\", line 136, in to_4d\r\n    expanded_attn_mask = causal_4d_mask.masked_fill(expanded_attn_mask.bool(), torch.finfo(dtype).min)\r\nRuntimeError: The size of tensor a (110) must match the size of tensor b (109) at non-singleton dimension 3\r\n```\r\n\r\nI would like to ask how to simply set usecache to False (I don't want to use cache when forwarding)\r\n\r\nthanks!\n</Comment>\n<Comment by Stevetich at 2024-07-16T11:24:51Z>\nI have encountered the same problem.\n</Comment>\n<Comment by CrispyFeSo4 at 2025-02-28T09:02:36Z>\ni have the same problem too\n</Comment>\n<Comment by HectorHHZ at 2025-04-23T06:02:18Z>\nsame issue\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1069,
    "state": "open",
    "created_by": "wucx888",
    "created_at": "2024-02-04T07:50:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1069</URL>\n\n<TITLE>When will the training code and data with v1.6 models be released?</TITLE>\n\n<BODY>### Question\n\nGreat work. Look forward to the training code and data</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1068,
    "state": "open",
    "created_by": "a2382625920",
    "created_at": "2024-02-04T07:34:30Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1068</URL>\n\n<TITLE>[Usage] Error \"KeyERROR: \"LlavaMistralConfig\"\" in reasoning</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nI downloaded llava-v1.6-mistral-7b locally, and this \"LlavaMistralConfig\" problem appeared when reasoning about llava-v1.6-mistral-7b, I don't know the exact reason for this, I am following the normal startup order of the web_demo to start up this new model, but it doesn't work, in the With LLAVA-v1.5 it reasoned correctly!</BODY>\n\n<COMMENTS>\n<Comment by a2382625920 at 2024-02-05T03:41:04Z>\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/opt/conda/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/root/siton-glusterfs-eaxtsxdfs/hzt/projects/LLaVA-main_main/llava/serve/cli.py\", line 130, in <module>\r\n    main(args)\r\n  File \"/root/siton-glusterfs-eaxtsxdfs/hzt/projects/LLaVA-main_main/llava/serve/cli.py\", line 32, in main\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n  File \"/root/siton-glusterfs-eaxtsxdfs/hzt/projects/LLaVA-main_main/llava/model/builder.py\", line 108, in load_pretrained_model\r\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 803, in from_pretrained\r\n    tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 737, in __getitem__\r\n    model_type = self._reverse_config_mapping[key.__name__]\r\nKeyError: 'LlavaMistralConfig'\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1067,
    "state": "closed",
    "created_by": "luoluoter",
    "created_at": "2024-02-04T02:55:45Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1067</URL>\n\n<TITLE>[Usage] gradio_web_server is not working after v1.2.1</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload\r\n```\r\n\r\nLog: \r\n```\r\n(llava) zhixin@xxxx-xx-xxxx-machine-02:~/LLaVA$ python -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload\r\n2024-02-04 10:44:41 | INFO | gradio_web_server | args: Namespace(host='0.0.0.0', port=None, controller_url='http://localhost:10000', concurrency_count=16, model_list_mode='reload', share=False, moderate=False, embed=False)\r\n2024-02-04 10:44:41 | INFO | gradio_web_server | Models: ['llava-v1.6-34b']\r\n2024-02-04 10:44:41 | INFO | gradio_web_server | Namespace(host='0.0.0.0', port=None, controller_url='http://localhost:10000', concurrency_count=16, model_list_mode='reload', share=False, moderate=False, embed=False)\r\n2024-02-04 10:44:42 | ERROR | stderr | /home/zhixin/LLaVA/llava/serve/gradio_web_server.py:353: UserWarning: `layout` parameter is deprecated, and it has no effect\r\n2024-02-04 10:44:42 | ERROR | stderr |   chatbot = gr.Chatbot(\r\n2024-02-04 10:44:42 | ERROR | stderr | Traceback (most recent call last):\r\n2024-02-04 10:44:42 | ERROR | stderr |   File \"/home/zhixin/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n2024-02-04 10:44:42 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n2024-02-04 10:44:42 | ERROR | stderr |   File \"/home/zhixin/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n2024-02-04 10:44:42 | ERROR | stderr |     exec(code, run_globals)\r\n2024-02-04 10:44:42 | ERROR | stderr |   File \"/home/zhixin/LLaVA/llava/serve/gradio_web_server.py\", line 473, in <module>\r\n2024-02-04 10:44:42 | ERROR | stderr |     demo = build_demo(args.embed, concurrency_count=args.concurrency_count)\r\n2024-02-04 10:44:42 | ERROR | stderr |   File \"/home/zhixin/LLaVA/llava/serve/gradio_web_server.py\", line 395, in build_demo\r\n2024-02-04 10:44:42 | ERROR | stderr |     regenerate_btn.click(\r\n2024-02-04 10:44:42 | ERROR | stderr | TypeError: EventListenerMethod.__call__() got an unexpected keyword argument 'concurrency_limit'\r\n```\r\n\r\nScreenshots:\r\nNone. But message above should be enough.\r\n\r\n---\r\n\r\ni get the problem at 'main' branch after i 'git pull' , commitID[24fa1d065bbeac8a145a796ab7218c6945a2536e]\r\nSo i checkout the old version.\r\nfor now, i checkout the branch of v1.2.1, which is not problem</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2024-02-04T03:04:53Z>\nHi, you would need to run `pip install -e .` to upgrade the gradio version.\n</Comment>\n<Comment by luoluoter at 2024-02-05T05:52:22Z>\noh, you are right. \r\nIt is working after upgrade. \r\nSorry for not realizing this.\r\nThank you for your help.\n</Comment>\n<Comment by dazhangyu123 at 2024-07-12T07:54:01Z>\nI have updated packages by running \r\n```Shell\r\ngit pull\r\npip install -e .\r\n```\r\nbut this issue still occurs. How to avoid this issue?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1066,
    "state": "closed",
    "created_by": "elismasilva",
    "created_at": "2024-02-04T01:53:21Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1066</URL>\n\n<TITLE>[Usage] Can't generate outputs in llava-1.6-34b</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue: I got this error when i tried generate caption from image in liuhaotian/llava-v1.6-34b, other models with same code works normally. \r\n\r\nCommand:\r\nits was started with 4bit quantization: \r\n```python\r\nfrom llava.model.builder import load_pretrained_model\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    model_path, model_base, model_name, load_8bit=False, load_4bit=True,device_map='cuda:0', device='cuda:0')\r\n```\r\nand eval with this code:\r\n\r\n```python\r\n    disable_torch_init()\r\n\r\n    qs = args.query\r\n    if model.config.mm_use_im_start_end:\r\n        qs = (\r\n            DEFAULT_IM_START_TOKEN\r\n            + DEFAULT_IMAGE_TOKEN\r\n            + DEFAULT_IM_END_TOKEN\r\n            + \"\\n\"\r\n            + qs\r\n        )\r\n    else:\r\n        qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\r\n\r\n    if \"llama-2\" in model_name.lower():\r\n        conv_mode = \"llava_llama_2\"\r\n    elif \"v1\" in model_name.lower():\r\n        conv_mode = \"llava_v1\"\r\n    elif \"mpt\" in model_name.lower():\r\n        conv_mode = \"mpt\"\r\n    else:\r\n        conv_mode = \"llava_v0\"\r\n\r\n    if args.conv_mode is not None and conv_mode != args.conv_mode:\r\n        print(\r\n            \"[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}\".format(\r\n                conv_mode, args.conv_mode, args.conv_mode\r\n            )\r\n        )\r\n    else:\r\n        args.conv_mode = conv_mode\r\n\r\n    conv = conv_templates[args.conv_mode].copy()\r\n    conv.append_message(conv.roles[0], qs)\r\n    conv.append_message(conv.roles[1], None)\r\n    prompt = conv.get_prompt()\r\n\r\n    if not isinstance(args.image_file, list):\r\n        images = image_parser(args)\r\n    else:\r\n        images = load_images(args.image_file)\r\n        \r\n    images_tensor = process_images(images, image_processor, model.config).to(\r\n        model.device, dtype=torch.float16\r\n    )\r\n    image_sizes = [image.size for image in images] \r\n\r\n    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)\r\n    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\r\n    keywords = [stop_str]\r\n    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\r\n   \r\n    with torch.inference_mode():\r\n        output_ids = model.generate(\r\n            input_ids,\r\n            images=images_tensor, \r\n            image_sizes=image_sizes,\r\n            do_sample=True if args.temperature > 0 else False,\r\n            temperature=args.temperature,\r\n            max_new_tokens=args.max_new_tokens,                                                                        \r\n            use_cache=True,\r\n            stopping_criteria=[stopping_criteria])\r\n  \r\n    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\r\n    \r\n    return outputs\r\n```\r\n\r\nStacktrace: \r\n```\r\n  File \"/home/master/miniconda3/envs/caption/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/mnt/d/LLaVA/llava/model/language_model/llava_llama.py\", line 137, in generate\r\n    return super().generate(\r\n  File \"/home/master/miniconda3/envs/caption/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/master/miniconda3/envs/caption/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1764, in generate\r\n    return self.sample(\r\n  File \"/home/master/miniconda3/envs/caption/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2924, in sample\r\n``    if stopping_criteria(input_ids, scores):\r\n  File \"/home/master/miniconda3/envs/caption/lib/python3.10/site-packages/transformers/generation/stopping_criteria.py\", line 132, in __call__\r\n    return any(criteria(input_ids, scores) for criteria in self)\r\n  File \"/home/master/miniconda3/envs/caption/lib/python3.10/site-packages/transformers/generation/stopping_criteria.py\", line 132, in <genexpr>\r\n    return any(criteria(input_ids, scores) for criteria in self)\r\n  File \"/mnt/d/LLaVA/llava/mm_utils.py\", line 245, in __call__\r\n    outputs.append(self.call_for_batch(output_ids[i].unsqueeze(0), scores))\r\n  File \"/mnt/d/LLaVA/llava/mm_utils.py\", line 234, in call_for_batch\r\n    if (output_ids[0, -keyword_id.shape[0]:] == keyword_id).all():\r\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0\r\n```</BODY>\n\n<COMMENTS>\n<Comment by elismasilva at 2024-02-04T03:19:51Z>\nAs a temporary solution I added it to line 233 of mm_utils.py so that the model works, but I don't know the impact of not performing this check.:\r\n\r\n```python\r\ndef call_for_batch(self, output_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\r\n        offset = min(output_ids.shape[1] - self.start_len, self.max_keyword_len)\r\n        self.keyword_ids = [keyword_id.to(output_ids.device) for keyword_id in self.keyword_ids]\r\n-->     if 'liuhaotian/llava-v1.6-34b' not in self.tokenizer.name_or_path:\r\n            for keyword_id in self.keyword_ids:\r\n                if (output_ids[0, -keyword_id.shape[0]:] == keyword_id).all():\r\n                    return True\r\n        outputs = self.tokenizer.batch_decode(output_ids[:, -offset:], skip_special_tokens=True)[0]\r\n        for keyword in self.keywords:\r\n            if keyword in outputs:\r\n                return True\r\n        return False\r\n```\n</Comment>\n<Comment by elismasilva at 2024-02-04T04:01:49Z>\nSo i found it was fixed in 1.2.2 then i will close this issue.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1065,
    "state": "closed",
    "created_by": "wd9702",
    "created_at": "2024-02-03T20:58:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1065</URL>\n\n<TITLE>[Usage]  Question of initialize Cuda</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\n\r\nI set the cuda version to 11.8 to match torch version. But after install the project with \"pip install -e .\", and when i tried to test locally, the error message saying \"Torch not compiled with CUDA enabled\".\r\n\r\nI manually uninstall torch and reinstall with \"pip3 install torch==2.0.1 torchvision==0.15.2 torchaudio --index-url https://download.pytorch.org/whl/cu118\", the cuda is availabe for torch now, but i got a new error message: cannot import name 'LlavaLlamaForCausalLM' from 'llava.model'\r\n\r\nWhat would be procedure of unitilizing GPU in the installation process?</BODY>\n\n<COMMENTS>\n<Comment by anindita127 at 2024-02-13T15:04:18Z>\nHi, what was the solution to this issue?\n</Comment>\n<Comment by wd9702 at 2024-02-17T15:15:43Z>\n> Hi, what was the solution to this issue?\r\n\r\nIt is gone with the newest version in Linux OS\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1064,
    "state": "closed",
    "created_by": "Hesh0629",
    "created_at": "2024-02-03T18:21:57Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1064</URL>\n\n<TITLE>[Question] how to solve library version conflict?</TITLE>\n\n<BODY>### Question\n\n<img width=\"1128\" alt=\"Screenshot 2024-02-04 at 3 14 39 AM\" src=\"https://github.com/haotian-liu/LLaVA/assets/57738176/d475f8ce-7a29-4083-8ce7-f0251f949464\">\r\nHello liu, thank you for releasing code and i'm trying to install LLaVA with your guide.   \r\n\r\nhowever, there's a problem when i try to install library with \"pip install -e \".[train]\" and it seems like i can't use gradio 4.16.0 and deepspeed 0.9.5 together.  \r\nhow can i use it together? can i use lower version of gradio or deepspeed?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2024-02-04T00:50:27Z>\nSorry for the inconvenience. Fixed in https://github.com/haotian-liu/LLaVA/releases/tag/v1.2.2.post1\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1062,
    "state": "closed",
    "created_by": "hongdangshao",
    "created_at": "2024-02-03T14:54:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1062</URL>\n\n<TITLE>How to finetune llava v1.5 model with chinese instruction dataset?</TITLE>\n\n<BODY>### Question\n\nhello, I want to know if  llava v1.5 model can finetune with chinese instruction dataset?Does the model support Chinese like english?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2024-02-03T15:32:14Z>\nIt is not finetuned with Chinese instructions and simply generalizes to answer questions in Chinese zero-shot, while you can finetune it on Chinese instruction dataset to allow it better.\n</Comment>\n<Comment by hongdangshao at 2024-02-03T15:32:35Z>\nI have got it!Thanks's.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1061,
    "state": "closed",
    "created_by": "RyanHuangNLP",
    "created_at": "2024-02-03T14:50:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1061</URL>\n\n<TITLE>[Question] llava 1.6 use mistral 7b instruct</TITLE>\n\n<BODY>### Question\n\nI wondered that which version of mistral using in llava 1.6, instruct or base?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2024-02-03T15:33:09Z>\nBase LLM: [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\r\n\r\nYou can check out our model card for different variants of our llava-v1.6 models. Thanks.\r\n\r\nhttps://huggingface.co/liuhaotian/llava-v1.6-mistral-7b\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1060,
    "state": "closed",
    "created_by": "slaser79",
    "created_at": "2024-02-03T13:19:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1060</URL>\n\n<TITLE>[Usage] Getting an ImportError:cannot import name 'LlavaLlamaForCasualLM'</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nI am getting an import error as below. This was after pulling the lastest master version and running ```pip install -e  .```\r\nThis used to work in the past when running llava-v1.5.\r\n\r\n\r\nCommand:\r\n```\r\n python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.6-mistral-7b --load-4bit\r\n```\r\n\r\nLog: \r\n```\r\n[2024-02-03 13:13:22,402] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n/home/slaser79/anaconda3/envs/llava/lib/python3.10/site-packages/pydantic/_internal/_config.py:322: UserWarning: Valid config keys have changed in V2:\r\n* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\r\n* 'validate_all' has been renamed to 'validate_default'\r\n  warnings.warn(message, UserWarning)\r\n/home/slaser79/anaconda3/envs/llava/lib/python3.10/site-packages/pydantic/_internal/_fields.py:151: UserWarning: Field \"model_persistence_threshold\" has conflict with protected namespace \"model_\".\r\n\r\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\r\n  warnings.warn(\r\nTraceback (most recent call last):\r\n  File \"/home/slaser79/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 187, in _run_module_as_main\r\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\r\n  File \"/home/slaser79/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 110, in _get_module_details\r\n    __import__(pkg_name)\r\n  File \"/home/slaser79/lab/LLaVA/llava/__init__.py\", line 1, in <module>\r\n    from .model import LlavaLlamaForCausalLM\r\nImportError: cannot import name 'LlavaLlamaForCausalLM' from 'llava.model' (/home/slaser79/lab/LLaVA/llava/model/__init__.py)\r\n\r\n\r\n```</BODY>\n\n<COMMENTS>\n<Comment by InconsolableCellist at 2024-02-03T21:06:03Z>\nWhat version of Python do you have in your environment? I got this error when I used something higher than 3.10\n</Comment>\n<Comment by slaser79 at 2024-02-04T07:20:15Z>\nIt was python 3.10..I recloned the project and reinstalled the dependencies and now it works! So I think it is related to upgrade instructions.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1059,
    "state": "closed",
    "created_by": "jyC23333",
    "created_at": "2024-02-03T04:24:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1059</URL>\n\n<TITLE>[Question] mismatch setting in training stage one.</TITLE>\n\n<BODY>### Question\n\nI find that the setting in ```pretrain.sh```  is mismatched with the pap\r\n\r\nAs mentioned in paper, LLM should be freezed, but in the pretrain script, the LLM weights are not freezed.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2024-02-03T05:41:02Z>\n`--tune_mm_mlp_adapter True` shall make LLM weights frozen.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/scripts/pretrain.sh#L22\n</Comment>\n<Comment by jyC23333 at 2024-02-04T07:16:22Z>\nThanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1058,
    "state": "closed",
    "created_by": "deepakdalakoti",
    "created_at": "2024-02-03T00:48:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1058</URL>\n\n<TITLE>Inference using v1.6 is broken</TITLE>\n\n<BODY>Issue: Inference using v1.6 us giving single token output\r\n\r\nCommand:\r\n```\r\npython3 -m llava.serve.cli   --model-path liuhaotian/llava-v1.6-mistral-7b   --image-file \"test_imag.png\" --max-new-tokens 1024 --debug\r\n```\r\n\r\nLog: \r\n```\r\nUSER: Describe the image\r\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\r\nASSISTANT: The\r\n\r\n {'prompt': '[INST] <image>\\nDescribe the image [/INST]', 'outputs': ''} \r\n\r\nUSER: \r\n\r\n```\r\nThe model returns a single token irrespective of inputs</BODY>\n\n<COMMENTS>\n<Comment by Silviase at 2024-02-03T02:03:45Z>\nDo you have problem in decoding? It would be more helpful with attaching output_ids.\n</Comment>\n<Comment by deepakdalakoti at 2024-02-03T02:55:11Z>\nSeems like the default configuration is not loading the correct prompt template for mistral model. Running with `--conv-mode \"mistral_direct\" ` solves the issue\n</Comment>\n<Comment by sasakits at 2024-02-06T06:19:39Z>\n@deepakdalakoti \r\n> Seems like the default configuration is not loading the correct prompt template for mistral model. Running with `--conv-mode \"mistral_direct\" ` solves the issue\r\n\r\nhi, you mean, \"mistral_instruct\" ? (I could not find \"mistral_direct\" in cli.py)\n</Comment>\n<Comment by deepakdalakoti at 2024-02-06T06:25:22Z>\nThere is mistral_direct in conversation.py\n</Comment>\n<Comment by sasakits at 2024-02-06T06:50:17Z>\nI found it, and then, it reduced the message \"{'prompt': '[INST] <image>\\nDescribe the image [/INST]', 'outputs': ''}\"\r\n\r\nbut still there is the message \"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\"\r\n\r\nactually, I don't need to care it?\n</Comment>\n<Comment by samuruph at 2024-04-11T14:58:08Z>\nI am also getting: \r\n```bash\r\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\r\n```\r\nHow can I fix this issue?\n</Comment>\n<Comment by seasoncool at 2024-05-17T06:10:35Z>\nstill here  +1.\r\n llama3-llava-next-8b\n</Comment>\n<Comment by yuwang4321 at 2024-11-11T08:48:52Z>\nHi, I'm also facing this issue. Have you managed to solve it?\r\nllava-v1.6-mistral-7b-hf\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1055,
    "state": "closed",
    "created_by": "lucasjinreal",
    "created_at": "2024-02-02T06:42:50Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1055</URL>\n\n<TITLE>Still weak on OCR capbility</TITLE>\n\n<BODY>![image](https://github.com/haotian-liu/LLaVA/assets/21303438/38a7b724-3a7f-4930-9546-d9e2a7f155fd)\r\n\r\nEvent Qwen1.8B can reads all text on this image...</BODY>\n\n<COMMENTS>\n<Comment by abrahimzaman360 at 2024-02-02T06:50:43Z>\nRight it need to be finetuned with Korean Dataset!\n</Comment>\n<Comment by haotian-liu at 2024-02-03T15:37:25Z>\nWe have not finetuned it for Chinese OCR yet. Stay tuned for an updated version with improved Chinese OCR.\n</Comment>\n<Comment by lucasjinreal at 2024-02-04T02:38:21Z>\nIs this improved Chinese OCR will be release as well?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1054,
    "state": "closed",
    "created_by": "Suhail",
    "created_at": "2024-02-02T06:23:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1054</URL>\n\n<TITLE>[Usage] RuntimeError: The size of tensor a (81) must match the size of tensor b (8) at non-singleton dimension 1</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue: RuntimeError: The size of tensor a (81) must match the size of tensor b (8) at non-singleton dimension 1\r\n\r\nCommand:\r\n```\r\npython -m llava.eval.run_llava --model-path liuhaotian/llava-v1.5-13b --image-file \"https://llava-vl.github.io/static/images/view.jpg\" --query \"What are 3 objects in the image? Separate each object by a comma. Order largest to smallest. Exclude background. Format: object1, object2, object3.\"\r\n```\r\n\r\nLog: \r\n```\r\nNamespace(model_path='liuhaotian/llava-v1.5-13b', model_base=None, image_file='https://llava-vl.github.io/static/images/view.jpg', query='What are 3 objects in the image? Separate each object by a comma. Order largest to smallest. Exclude background. Format: object1, object2, object3.', conv_mode=None, sep=',', temperature=0.2, top_p=None, num_beams=1, max_new_tokens=512)\r\nllava-v1.5-13b\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:16<00:00,  5.42s/it]\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/root/research/suhail/LLaVA/llava/eval/run_llava.py\", line 158, in <module>\r\n    eval_model(args)\r\n  File \"/root/research/suhail/LLaVA/llava/eval/run_llava.py\", line 129, in eval_model\r\n    n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\r\nRuntimeError: The size of tensor a (81) must match the size of tensor b (8) at non-singleton dimension 1\r\n```\r\n\r\nOther info\r\n\r\n```\r\n\r\nPython 3.10.12\r\n\r\n>>> transformers.__version__\r\n'4.36.2'\r\n```\r\n\r\n\r\nAlso curious about this warning: \r\n```\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\n```</BODY>\n\n<COMMENTS>\n<Comment by ninatu at 2024-02-02T10:32:24Z>\nI have the same issue  :(\n</Comment>\n<Comment by TerryYiDa at 2024-02-02T10:34:02Z>\nI have the same issue\n</Comment>\n<Comment by Suhail at 2024-02-02T19:31:59Z>\nSounds like @haotian-liu is aware!\n</Comment>\n<Comment by haotian-liu at 2024-02-02T19:44:15Z>\nThanks for reporting! Can you check the current main branch fixes it?\n</Comment>\n<Comment by Suhail at 2024-02-02T23:28:33Z>\nNow fixed! Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1053,
    "state": "open",
    "created_by": "InconsolableCellist",
    "created_at": "2024-02-02T06:09:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1053</URL>\n\n<TITLE>[Usage] Segmentation fault in Python when following installation steps</TITLE>\n\n<BODY>### Describe the issue\n\n# Issue: \r\n\r\nI get a segmentation fault when trying to load the model_worker using the provided weights and installation steps\r\n\r\n# Environment:\r\n<details>\r\n    <summary>nvidia drivers and GPUs</summary>\r\n\r\n```\r\n$ nvidia-smi\r\nThu Feb  1 15:48:45 2024       \r\n+---------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\r\n|-----------------------------------------+----------------------+----------------------+\r\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                      |               MIG M. |\r\n|=========================================+======================+======================|\r\n|   0  NVIDIA GeForce RTX 3090        Off | 00000000:17:00.0 Off |                  N/A |\r\n|  0%   47C    P8              19W / 350W |      5MiB / 24576MiB |      0%      Default |\r\n|                                         |                      |                  N/A |\r\n+-----------------------------------------+----------------------+----------------------+\r\n|   1  NVIDIA GeForce RTX 3090        Off | 00000000:B3:00.0 Off |                  N/A |\r\n|  0%   48C    P8              28W / 370W |      5MiB / 24576MiB |      0%      Default |\r\n|                                         |                      |                  N/A |\r\n+-----------------------------------------+----------------------+----------------------+\r\n                                                                                         \r\n+---------------------------------------------------------------------------------------+\r\n| Processes:                                                                            |\r\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n|        ID   ID                                                             Usage      |\r\n|=======================================================================================|\r\n|  No running processes found                                                           |\r\n+---------------------------------------------------------------------------------------+\r\n```\r\n</details>\r\n\r\n<details>\r\n    <summary>OS and distro</summary>\r\n\r\n```\r\n$ cat /etc/os-release \r\nNAME=\"Artix Linux\"\r\nPRETTY_NAME=\"Artix Linux\"\r\nID=artix\r\nBUILD_ID=rolling\r\nANSI_COLOR=\"0;36\"\r\nHOME_URL=\"https://www.artixlinux.org/\"\r\nDOCUMENTATION_URL=\"https://wiki.artixlinux.org/\"\r\nSUPPORT_URL=\"https://forum.artixlinux.org/\"\r\nBUG_REPORT_URL=\"https://bugs.artixlinux.org/\"\r\nPRIVACY_POLICY_URL=\"https://terms.artixlinux.org/docs/privacy-policy/\"\r\nLOGO=artixlinux-logo\r\n```\r\n</details>\r\n\r\n<details>\r\n    <summary>pip/micromamba environment</summary>\r\n\r\n```\r\n$ pip list                                                                                                                                                                                    \r\nPackage                   Version      Editable project location                                                                                                                                                  \r\n------------------------- ------------ -------------------------                                                                                                                                                  \r\naccelerate                0.21.0                                                                                                                                                                                  \r\naiofiles                  23.2.1                                                                                                                                                                                  \r\naiohttp                   3.9.3                                                                                                                                                                                   \r\naiosignal                 1.3.1                                                                                                                                                                                   \r\naltair                    5.2.0                                                                                                                                                                                   \r\nanyio                     4.2.0                                                                                                                                                                                   \r\nasync-timeout             4.0.3                                                                                                                                                                                   \r\nattrs                     23.2.0                                                                                                                                                                                  \r\nbitsandbytes              0.41.0                                                                                                                                                                                  \r\ncertifi                   2023.11.17                                                                                                                                                                              \r\ncharset-normalizer        3.3.2                                                                                                                                                                                   \r\nclick                     8.1.7                                                                                                                                                                                   \r\ncmake                     3.28.1                                                                                                                                                                                  \r\ncontourpy                 1.2.0                                                                                                                                                                                   \r\ncycler                    0.12.1                                                                                                                                                                                  \r\neinops                    0.6.1                                                                                                                                                                                   \r\neinops-exts               0.0.4                                                                                                                                                                                   \r\nexceptiongroup            1.2.0                                                                                                                                                                                   \r\nfastapi                   0.109.0                                                                                                                                                                                 \r\nffmpy                     0.3.1                                                                                                                                                                                   \r\nfilelock                  3.13.1                                                                                                                                                                                  \r\nfonttools                 4.47.2                                                                                                                                                                                  \r\nfrozenlist                1.4.1                                                                                                                                                                                   \r\nfsspec                    2023.12.2                                                                                                                                                                               \r\ngradio                    3.35.2                                                                                                                                                                                  \r\ngradio_client             0.2.9                                                                                                                                                                                   \r\nh11                       0.14.0                                                                                                                                                                                  \r\nhttpcore                  0.17.3                                                                                                                                                                                  \r\nhttpx                     0.24.0                                                                                                                                                                                  \r\nhuggingface-hub           0.20.3                                                                                                                                                                                  \r\nidna                      3.6                                                                                                                                                                                     \r\nJinja2                    3.1.3                                                                                                                                                                                   \r\njoblib                    1.3.2                                                                                                                                                                                   \r\njsonschema                4.21.1                                                                                                                                                                                  \r\njsonschema-specifications 2023.12.1                                                                                                                                                                               \r\nkiwisolver                1.4.5                                                                                                                                                                                   \r\nlinkify-it-py             2.0.2                                                                                                                                                                                   \r\nlit                       17.0.6                                                                                                                                                                                  \r\nllava                     1.2.0        /home/user/Programs/LLaVA                                                                                                                                                   \r\nmarkdown-it-py            2.2.0                                                                                                                                                                                   \r\nmarkdown2                 2.4.12                                                                                                                                                                                  \r\nMarkupSafe                2.1.4                                                                                                                                                                                   \r\nmatplotlib                3.8.2                                                                                                                                                                                   \r\nmdit-py-plugins           0.3.\r\nmdurl                     0.1.2\r\nmpmath                    1.3.0\r\nmultidict                 6.0.4\r\nnetworkx                  3.2.1\r\nnumpy                     1.26.3\r\nnvidia-cublas-cu11        11.10.3.66\r\nnvidia-cuda-cupti-cu11    11.7.101\r\nnvidia-cuda-nvrtc-cu11    11.7.99\r\nnvidia-cuda-runtime-cu11  11.7.99\r\nnvidia-cudnn-cu11         8.5.0.96\r\nnvidia-cufft-cu11         10.9.0.58\r\nnvidia-curand-cu11        10.2.10.91\r\nnvidia-cusolver-cu11      11.4.0.1\r\nnvidia-cusparse-cu11      11.7.4.91\r\nnvidia-nccl-cu11          2.14.3\r\nnvidia-nvtx-cu11          11.7.91\r\norjson                    3.9.12\r\npackaging                 23.2\r\npandas                    2.2.0\r\npeft                      0.4.0\r\npillow                    10.2.0\r\npip                       23.3.2\r\npsutil                    5.9.8\r\npydantic                  1.10.14\r\npydub                     0.25.1\r\nPygments                  2.17.2\r\npyparsing                 3.1.1\r\npython-dateutil           2.8.2\r\npython-multipart          0.0.6\r\npytz                      2023.4\r\nPyYAML                    6.0.1\r\nreferencing               0.33.0\r\nregex                     2023.12.25\r\nrequests                  2.31.0\r\nrpds-py                   0.17.1\r\nsafetensors               0.4.2\r\nscikit-learn              1.2.2\r\nscipy                     1.12.0\r\nsemantic-version          2.10.0\r\nsentencepiece             0.1.99\r\nsetuptools                69.0.3\r\nshortuuid                 1.0.11\r\nsix                       1.16.0\r\nsniffio                   1.3.0\r\nstarlette                 0.35.1\r\nsvgwrite                  1.4.3\r\nsympy                     1.12\r\nthreadpoolctl             3.2.0\r\ntimm                      0.6.13\r\ntokenizers                0.15.0\r\ntoolz                     0.12.1\r\ntorch                     2.0.1\r\ntorchvision               0.15.2\r\ntqdm                      4.66.1\r\ntransformers              4.36.2\r\ntriton                    2.0.0\r\ntyping_extensions         4.9.0\r\ntzdata                    2023.4\r\nuc-micro-py               1.0.2\r\nurllib3                   2.2.0\r\nuvicorn                   0.27.0.post1\r\nwavedrom                  2.0.3.post3\r\nwebsockets                12.0\r\nwheel                     0.42.0\r\nyarl                      1.9.4\r\n```\r\n</details>\r\n\r\n<details><summary>Python version</summary>\r\n\r\n```\r\n$ python --version\r\nPython 3.10.13\r\n```\r\n\r\n</details>\r\n\r\n# Steps to reproduce:\r\nI followed the installation steps:\r\n1. git clone https://github.com/haotian-liu/LLaVA.git\r\n2. cd LLaVA\r\n3. micromamba env create -n llava python=3.10 -y\r\n4. micromamba activate llava\r\n5. pip install --upgrade pip\r\n6. pip install -e . \r\n\r\nI then downloaded the weights (llava-v1.6-mistral-7b)\r\n\r\nAnd ran things in the following order:\r\n0. `tmux` (new session)\r\n1. `python -m llava.serve.controller --host 0.0.0.0 --port 10000`\r\n2. `python -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload`\r\n3. `python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./models/llava-v1.6-mistral-7b`\r\n\r\nI monitored the progress with `nv-top` and saw that the GPUs started getting data loaded into them. \r\n\r\n# Observed Results\r\n\r\n```\r\n$ python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./models/llava-v1.6-mistral-7b              \r\n2024-02-01 11:05:24 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='./models/llava-v1.6-mist\r\nral-7b', model_base=None, model_name=None, device='cuda', multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=False)                                   \r\n2024-02-01 11:05:24 | INFO | model_worker | Loading the model llava-v1.6-mistral-7b on worker 16a702 ...                                                                                                          \r\npreprocessor_config.json:   0%|                                                                                      | 0.00/316 [00:00<?, ?B/s]                                                                   \r\npreprocessor_config.json: 100%|███████████████████████████████████████████████████████████████████████████████| 316/316 [00:00<00:00, 1.04MB/s]                                                                   \r\n2024-02-01 11:05:25 | ERROR | stderr |                                                                                                                                                                            \r\nconfig.json:   0%|                                                                                                 | 0.00/4.76k [00:00<?, ?B/s]                                                                   \r\nconfig.json: 100%|████████████████████████████████████████████████████████████████████████████████████████| 4.76k/4.76k [00:00<00:00, 14.7MB/s]                                                                   \r\n2024-02-01 11:05:25 | ERROR | stderr |                                                                   \r\npytorch_model.bin:   0%|                                                                                           | 0.00/1.71G [00:00<?, ?B/s]                                                                   \r\npytorch_model.bin:   1%|▌                                                                                 | 10.5M/1.71G [00:00<02:07, 13.3MB/s]  \r\n...\r\npytorch_model.bin: 100%|██████████████████████████████████████████████████████████████████████████████████| 1.71G/1.71G [00:48<00:00, 35.4MB/s]                                                                   \r\n2024-02-01 11:06:14 | ERROR | stderr |                                                                   \r\nSegmentation fault\r\n```\r\n\r\n**Dmesg reports:**\r\n`[ 2319.436369] traps: python[5578] general protection fault ip:74a5de9987f7 sp:7ffe314352a0 error:0 in libtorch_cpu.so[74a5dd241000+12ef4000]`\r\n\r\nI tried upgrading to the latest version of Python (python 3.10.13   h7a1cb2a_0) but with no change in behavior. I tried Python 3.11 and 3.12 but they had an incompatible version of a Llama library.\r\n\r\nI tried upgrading `torch` and `torchvision` with `pip install --upgrade torch torchvision` to `torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl` and `torchvision-0.17.0-cp310-cp310-manylinux1_x86_64.whl` from the installed version `2.0.1` and `0.15.2` respectively.\r\n\r\n**But the crash still occurred:**\r\n` traps: python[5501] general protection fault ip:5e39bfa86cfc sp:78ce0effe4b0 error:0 in python3.10 (deleted)[5e39bf9b7000+206000]`\r\n\r\nAnd it somehow crashed tmux, which I don't understand how that happened.\r\n\r\n**Running `nvidia-smi` again resulted in:**\r\n```\r\n$ nvidia-smi \r\nSegmentation fault\r\n\r\n$ sudo dmesg\r\n...\r\n[20404.550385] nvidia-smi[9483]: segfault at 0 ip 00000000004481c0 sp 00007ffc6a45ab58 error 6 in nvidia-smi[400000+92000] likely on CPU 5 (core 1, socket 0)\r\n[20404.550408] Code: 90 1e 40 a0 00 23 b4 02 c8 1d c8 20 30 1e a0 04 a0 1a f8 0c c0 06 f8 09 60 19 18 1f 68 1c 88 a0 60 96 b0 1e a2 81 38 27 00 9e <30> 1f d0 20 58 a1 60 a2 e0 91 88 1b 88 0f 28 1d 90 8c 78 18 40 08\r\n```</BODY>\n\n<COMMENTS>\n<Comment by siddy819 at 2024-02-02T06:25:57Z>\nThe new repo seems to have some problems, going to an earlier checkpoint around Dec 23 solved most of my issues. The mistral7b based model runs for me, although with some warnings. I'm using the eval function btw and not the gradio implementation.\n</Comment>\n<Comment by InconsolableCellist at 2024-02-02T06:53:05Z>\nUsing 2120e82 I was able to get the model to load, but it segfaulted again as soon as I tried an inference in the gradio UI, resulting in \"NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.\" in the UI and this output \r\n\r\n<details>\r\n    <summary>Output</summary>\r\n\r\n```\r\nSome weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at ./models/llava-v1.6-mistral-7b and are newly initialized: ['model.\r\nlayers.4.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.7.\r\nself_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_at\r\ntn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotar\r\ny_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.i\r\nnv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq',\r\n 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model\r\n.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.\r\n1.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_a\r\nttn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rota\r\nry_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.\r\ninv_freq']                                                                                                                                                \r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.                                            \r\n2024-02-01 16:49:23 | INFO | model_worker | Register to controller                                                                                        \r\n2024-02-01 16:49:23 | ERROR | stderr | INFO:     Started server process [4446]                                                                            \r\n2024-02-01 16:49:23 | ERROR | stderr | INFO:     Waiting for application startup.                                                                         \r\n2024-02-01 16:49:23 | ERROR | stderr | INFO:     Application startup complete.                                                                            \r\n2024-02-01 16:49:23 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)                                           \r\n2024-02-01 16:49:38 | INFO | model_worker | Send heart beat. Models: ['llava-v1.6-mistral-7b']. Semaphore: None. global_counter: 0                        \r\n2024-02-01 16:49:53 | INFO | model_worker | Send heart beat. Models: ['llava-v1.6-mistral-7b']. Semaphore: None. global_counter: 0                        \r\n2024-02-01 16:49:54 | INFO | stdout | INFO:     127.0.0.1:59894 - \"POST /worker_get_status HTTP/1.1\" 200 OK                                               \r\n2024-02-01 16:50:00 | INFO | model_worker | Send heart beat. Models: ['llava-v1.6-mistral-7b']. Semaphore: Semaphore(value=4, locked=False). global_counte\r\nr: 1                                                                                                                                                      \r\n2024-02-01 16:50:00 | INFO | stdout | INFO:     127.0.0.1:60498 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK                                          \r\n2024-02-01 16:50:00 | ERROR | stderr | /home/user/micromamba/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: Yo\r\nu have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in\r\n a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )                \r\n2024-02-01 16:50:00 | ERROR | stderr |   warnings.warn(                                                                                                   \r\nSegmentation fault    \r\n```\r\n\r\n</details>\r\n\r\nHere are the hashes of the model I downloaded:\r\n<details>\r\n    <summary>SHA256sums</summary>\r\n\r\n```\r\nf3c77fe6e6b6457849e0f57c6cb6fa6222a4a64d914778f7234821d3588d40ff  config.json\r\n741acba7f5e235dac0e6865ecc212bbadb1ab1d6d853de7d759268cb62aaf2b4  generation_config.json\r\n57f11463314a7b628842ba55008c323fbc8d2c6d48a90f02343d550d61321d8e  model-00001-of-00004.safetensors\r\n1e88a821d441aef6685311cb319eebbac47fa99d523c71519a3cfa59478da451  model-00002-of-00004.safetensors\r\n225e6c059b92f1c7c234ab748ae718e29c26e558f3f645e19cbca502e8d94042  model-00003-of-00004.safetensors\r\n493af3a613d7b4ad965d7ae2be1592776d0f792fc0b6d38d44e3dc5268c0cec0  model-00004-of-00004.safetensors\r\nc1a32061b0ad3059ded2beeb22b2fb1cc885811c9baadb56a1ab6c6b92a9e4d3  model.safetensors.index.json\r\n7d0d549f44bdffba479581e56d229d4cfa1d85c7112fa62c11817b791bb286de  README.md\r\n719833ff26ac897a3ec8ed946028a135de2a351470af59b4008744ab1f0ee9b7  special_tokens_map.json\r\nb4b50144c2149fcd26c3068523ae847c460d3424b7fac2921af49d111cd34c30  tokenizer_config.json\r\nfc4f0bd70b3709312d9d1d9e5ba674794b6bc5abc17429897a540f93882f25fc  tokenizer.json\r\ndadfd56d766715c61d2ef780a525ab43b8e6da4de6865bda3d95fdef5e134055  tokenizer.model\r\n1f4f710e77837f113c3801bcba48f8c5662fd22d435fbd724b758e999d66ac93  trainer_state.json\r\nc4afa32f230f004feea7301f637244194ecb01fe655fa6a3ad426091430dc565  training_args.bin\r\n```\r\n</details>\n</Comment>\n<Comment by InconsolableCellist at 2024-02-02T17:00:32Z>\nI'm currently running the worker with `CUDA_VISIBLE_DEVICES=0 python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./models/llava-v1.6-mistral-7b` and the demo is working, so far no seg fault\n</Comment>\n<Comment by haotian-liu at 2024-02-03T03:38:27Z>\ndoes the latest commit work for you?\n</Comment>\n<Comment by InconsolableCellist at 2024-02-03T08:50:28Z>\nI updated, did `pip install -e .` and ran with `CUDA_VISIBLE_DEVICES=0,1 python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./models/llava-v1.6\r\n-34b --load-8bit`\r\n\r\nAnd I get a different error now: \r\n\r\n<details>\r\n    <summary>STDOUT and STDERR</summary>\r\n\r\n```\r\n2024-02-02 18:44:51 | ERROR | stderr |   warnings.warn(                                                                                                                                                           \r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [118,0,0], thread: [96,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.                                \r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [118,0,0], thread: [97,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.                                \r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [118,0,0], thread: [98,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.                                \r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [118,0,0], thread: [99,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.                                \r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [118,0,0], thread: [100,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.                               \r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [118,0,0], thread: [101,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.                               \r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [118,0,0], thread: [102,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.                               \r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [118,0,0], thread: [103,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [107,0,0], thread: [123,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.                               \r\n...\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [107,0,0], thread: [124,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.                               \r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [107,0,0], thread: [125,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [107,0,0], thread: [126,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [107,0,0], thread: [127,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n2024-02-02 18:44:52 | ERROR | stderr | Exception in thread Thread-3 (generate):                                                                                                                                   \r\n2024-02-02 18:44:52 | ERROR | stderr | Traceback (most recent call last):                                                                                                                                         \r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/micromamba/envs/llava/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner                                                                       \r\n2024-02-02 18:44:52 | ERROR | stderr |     self.run()                                                                                                                                                             \r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/micromamba/envs/llava/lib/python3.10/threading.py\", line 953, in run                                                                                     \r\n2024-02-02 18:44:52 | ERROR | stderr |     self._target(*self._args, **self._kwargs)                                                                                                                              \r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/micromamba/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context          \r\n2024-02-02 18:44:52 | ERROR | stderr |     return func(*args, **kwargs)                                                                                                                                           \r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/Programs/LLaVA/llava/model/language_model/llava_llama.py\", line 137, in generate                                                                         \r\n2024-02-02 18:44:52 | ERROR | stderr |     return super().generate(                                                                                                                                               \r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/micromamba/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context          \r\n2024-02-02 18:44:52 | ERROR | stderr |     return func(*args, **kwargs)                                                                                                                                           \r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/micromamba/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1764, in generate \r\n2024-02-02 18:44:52 | ERROR | stderr |     return self.sample(\r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/micromamba/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2861, in sample\r\n2024-02-02 18:44:52 | ERROR | stderr |     outputs = self(\r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/micromamba/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2024-02-02 18:44:52 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/micromamba/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2024-02-02 18:44:52 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/Programs/LLaVA/llava/model/language_model/llava_llama.py\", line 91, in forward\r\n2024-02-02 18:44:52 | ERROR | stderr |     return super().forward(\r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/micromamba/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1181, in forward\r\n2024-02-02 18:44:52 | ERROR | stderr |     outputs = self.model(\r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/micromamba/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2024-02-02 18:44:52 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/micromamba/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1068, in forward\r\n2024-02-02 18:44:52 | ERROR | stderr |     layer_outputs = decoder_layer(\r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/micromamba/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2024-02-02 18:44:52 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/micromamba/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2024-02-02 18:44:52 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/micromamba/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 796, in forward\r\n2024-02-02 18:44:52 | ERROR | stderr |     hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/micromamba/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2024-02-02 18:44:52 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/micromamba/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2024-02-02 18:44:52 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/micromamba/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 448, in forward\r\n2024-02-02 18:44:52 | ERROR | stderr |     attn_output = self.o_proj(attn_output)\r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/micromamba/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2024-02-02 18:44:52 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/micromamba/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2024-02-02 18:44:52 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/micromamba/envs/llava/lib/python3.10/site-packages/bitsandbytes/nn/modules.py\", line 441, in forward\r\n2024-02-02 18:44:52 | ERROR | stderr |     out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)\r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/micromamba/envs/llava/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 563, in matmul\r\n2024-02-02 18:44:52 | ERROR | stderr |     return MatMul8bitLt.apply(A, B, out, bias, state)\r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/micromamba/envs/llava/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\r\n2024-02-02 18:44:52 | ERROR | stderr |     return super().apply(*args, **kwargs)  # type: ignore[misc]\r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/micromamba/envs/llava/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 327, in forward\r\n2024-02-02 18:44:52 | ERROR | stderr |     CA, CAt, SCA, SCAt, coo_tensorA = F.double_quant(A.to(torch.float16), threshold=state.threshold)\r\n2024-02-02 18:44:52 | ERROR | stderr |   File \"/home/user/micromamba/envs/llava/lib/python3.10/site-packages/bitsandbytes/functional.py\", line 2016, in double_quant\r\n2024-02-02 18:44:52 | ERROR | stderr |     nnz = nnz_row_ptr[-1].item()\r\n2024-02-02 18:44:52 | ERROR | stderr | RuntimeError: CUDA error: device-side assert triggered\r\n2024-02-02 18:44:52 | ERROR | stderr | CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n2024-02-02 18:44:52 | ERROR | stderr | For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\n2024-02-02 18:44:52 | ERROR | stderr | Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n2024-02-02 18:44:52 | ERROR | stderr | \r\n```\r\n</details>\r\n\r\n<details>\r\n    <summary>dmesg output</summary>\r\n\r\n```\r\n[57310.268117] Fixing recursive fault but reboot is needed!                                              \r\n[57310.268119] BUG: scheduling while atomic: cuda-EvtHandlr/13789/0x00000000                             \r\n[57310.268122] Modules linked in: fuse rpcsec_gss_krb5 auth_rpcgss nfsv4 dns_resolver nfs lockd grace fscache netfs qrtr bnep bluetooth ecdh_generic sunrpc vfat fat intel_rapl_msr intel_rapl_common intel_uncore\r\n_frequency intel_uncore_frequency_common isst_if_common skx_edac nfit snd_soc_avs x86_pkg_temp_thermal intel_powerclamp snd_soc_hda_codec snd_hda_ext_core coretemp snd_soc_core kvm_intel snd_compress snd_hda_co\r\ndec_realtek ac97_bus rfkill snd_pcm_dmaengine snd_hda_codec_generic snd_hda_codec_hdmi snd_hda_intel kvm snd_intel_dspcfg snd_intel_sdw_acpi snd_hda_codec irqbypass snd_hda_core rapl snd_hwdep iTCO_wdt intel_cs\r\ntate mei_wdt snd_pcm intel_pmc_bxt dell_wmi crypto_user iTCO_vendor_support dell_smm_hwmon dell_smbios snd_timer ledtrig_audio mei_me dcdbas joydev snd intel_uncore sparse_keymap wmi_bmof dell_wmi_descriptor in\r\ntel_wmi_thunderbolt pcspkr ioatdma i2c_i801 e1000e soundcore mei i2c_smbus dca acpi_tad mac_hid ext4 uas usb_storage crc32c_generic crc16 mbcache jbd2 dm_crypt usbhid cbc encrypted_keys trusted\r\n[57310.268210]  asn1_encoder tee dm_mod nvme nvme_core nvme_auth crct10dif_pclmul crc32_pclmul crc32c_intel polyval_clmulni polyval_generic gf128mul serio_raw ghash_clmulni_intel sha512_ssse3 atkbd sha256_ssse3\r\n libps2 sha1_ssse3 vivaldi_fmap aesni_intel crypto_simd cryptd vmd xhci_pci xhci_pci_renesas i8042 serio nvidia_drm(POE) nvidia_uvm(POE) nvidia_modeset(POE) video wmi nvidia(POE)\r\n[57310.268245] CPU: 15 PID: 13789 Comm: cuda-EvtHandlr Tainted: P      D    OE      6.7.1-artix1-1 #1 f007f59a7ff3e3e54eae55d54af75a4e06d3590f\r\n[57310.268250] Hardware name: Dell Inc. Precision 5820 Tower/0X30MX, BIOS 2.20.0 05/26/2022              \r\n[57310.268252] Call Trace:                                                                               \r\n[57310.268255]  <TASK>                                                                                   \r\n[57310.268259]  dump_stack_lvl+0x47/0x60                                                                                                                                                                          \r\n[57310.268265]  __schedule_bug+0x56/0x70                                                                                                                                                                          \r\n[57310.268273]  __schedule+0x103e/0x1410                                                                                                                                                                          \r\n[57310.268278]  ? vprintk_emit+0x175/0x2b0                                                                                                                                                                        \r\n[57310.268284]  ? _printk+0x64/0x80                                                                                                                                                                               \r\n[57310.268290]  do_task_dead+0x43/0x50                                                                                                                                                                            \r\n[57310.268296]  make_task_dead+0x151/0x170                                                                                                                                                                        \r\n[57310.268302]  rewind_stack_and_make_dead+0x17/0x20 \r\n[57310.268306] RIP: 0033:0x7bf286c6cf7f\r\n[57310.268338] Code: Unable to access opcode bytes at 0x7bf286c6cf55.\r\n[57310.268340] RSP: 002b:00007bf154dffd50 EFLAGS: 00000293 ORIG_RAX: 0000000000000007\r\n[57310.268343] RAX: fffffffffffffdfc RBX: 00000000ffffffff RCX: 00007bf286c6cf7f\r\n[57310.268345] RDX: 0000000000000064 RSI: 000000000000000a RDI: 00007bf0d4000c20\r\n[57310.268347] RBP: 00007bf154dffe20 R08: 0000000000000000 R09: 0000000000000000\r\n[57310.268349] R10: 00007bf154dffde0 R11: 0000000000000293 R12: 0000000000000000\r\n[57310.268351] R13: 0000000000000064 R14: 00007bf0d4000c20 R15: 0000648d4edf1e90\r\n[57310.268354]  </TASK>\r\n[57310.268356] ------------[ cut here ]------------\r\n[57310.268357] Voluntary context switch within RCU read-side critical section!\r\n[57310.268364] WARNING: CPU: 15 PID: 13789 at kernel/rcu/tree_plugin.h:320 rcu_note_context_switch+0x5e0/0x660\r\n[57310.268374] Modules linked in: fuse rpcsec_gss_krb5 auth_rpcgss nfsv4 dns_resolver nfs lockd grace fscache netfs qrtr bnep bluetooth ecdh_generic sunrpc vfat fat intel_rapl_msr intel_rapl_common intel_uncore_frequency intel_uncore_frequency_common isst_if_common skx_edac nfit snd_soc_avs x86_pkg_temp_thermal intel_powerclamp snd_soc_hda_codec snd_hda_ext_core coretemp snd_soc_core kvm_intel snd_compress snd_hda_codec_realtek ac97_bus rfkill snd_pcm_dmaengine snd_hda_codec_generic snd_hda_codec_hdmi snd_hda_intel kvm snd_intel_dspcfg snd_intel_sdw_acpi snd_hda_codec irqbypass snd_hda_core rapl snd_hwdep iTCO_wdt intel_cstate mei_wdt snd_pcm intel_pmc_bxt dell_wmi crypto_user iTCO_vendor_support dell_smm_hwmon dell_smbios snd_timer ledtrig_audio mei_me dcdbas joydev snd intel_uncore sparse_keymap wmi_bmof dell_wmi_descriptor intel_wmi_thunderbolt pcspkr ioatdma i2c_i801 e1000e soundcore mei i2c_smbus dca acpi_tad mac_hid ext4 uas usb_storage crc32c_generic crc16 mbcache jbd2 dm_crypt usbhid cbc encrypted_keys trusted\r\n[57310.268431]  asn1_encoder tee dm_mod nvme nvme_core nvme_auth crct10dif_pclmul crc32_pclmul crc32c_intel polyval_clmulni polyval_generic gf128mul serio_raw ghash_clmulni_intel sha512_ssse3 atkbd sha256_ssse3 libps2 sha1_ssse3 vivaldi_fmap aesni_intel crypto_simd cryptd vmd xhci_pci xhci_pci_renesas i8042 serio nvidia_drm(POE) nvidia_uvm(POE) nvidia_modeset(POE) video wmi nvidia(POE)\r\n[57310.268456] CPU: 15 PID: 13789 Comm: cuda-EvtHandlr Tainted: P      D W  OE      6.7.1-artix1-1 #1 f007f59a7ff3e3e54eae55d54af75a4e06d3590f\r\n[57310.268459] Hardware name: Dell Inc. Precision 5820 Tower/0X30MX, BIOS 2.20.0 05/26/2022\r\n[57310.268461] RIP: 0010:rcu_note_context_switch+0x5e0/0x660\r\n[57310.268465] Code: 00 00 00 00 0f 85 07 fd ff ff 49 89 8c 24 a0 00 00 00 e9 fa fc ff ff 48 c7 c7 00 fa 07 a4 c6 05 29 5b e5 01 01 e8 60 ee f3 ff <0f> 0b e9 7b fa ff ff 49 83 bc 24 98 00 00 00 00 49 8b 84 24 a0 00\r\n[57310.268468] RSP: 0018:ffff9ce2c0473e18 EFLAGS: 00010086\r\n[57310.268470] RAX: 0000000000000000 RBX: ffff89d3dfff5100 RCX: 0000000000000027\r\n[57310.268472] RDX: ffff89d3dffe16c8 RSI: 0000000000000001 RDI: ffff89d3dffe16c0\r\n[57310.268474] RBP: 0000000000000000 R08: 0000000000000000 R09: ffff9ce2c0473ca0\r\n[57310.268476] R10: 0000000000000003 R11: ffff89d45ff9b1e8 R12: ffff89d3dfff4200\r\n[57310.268478] R13: ffff89b5a6932740 R14: 0000000000000000 R15: 0000000000000000\r\n[57310.268479] FS:  0000000000000000(0000) GS:ffff89d3dffc0000(0000) knlGS:0000000000000000\r\n[57310.268482] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\n[57310.268484] CR2: 00007bf0a40088c8 CR3: 0000001052c20001 CR4: 00000000003706f0\r\n[57310.268486] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\r\n[57310.268488] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\r\n[57310.268490] Call Trace:\r\n[57310.268491]  <TASK>\r\n[57310.268492]  ? rcu_note_context_switch+0x5e0/0x660\r\n[57310.268496]  ? __warn+0x81/0x130\r\n[57310.268502]  ? rcu_note_context_switch+0x5e0/0x660\r\n[57310.268506]  ? report_bug+0x171/0x1a0\r\n[57310.268512]  ? prb_read_valid+0x1b/0x30\r\n[57310.268516]  ? handle_bug+0x3c/0x80\r\n[57310.268521]  ? exc_invalid_op+0x17/0x70\r\n[57310.268525]  ? asm_exc_invalid_op+0x1a/0x20\r\n[57310.268529]  ? rcu_note_context_switch+0x5e0/0x660\r\n[57310.268533]  ? rcu_note_context_switch+0x5e0/0x660\r\n[57310.268538]  __schedule+0xc0/0x1410\r\n[57310.268541]  ? vprintk_emit+0x175/0x2b0\r\n[57310.268544]  ? _printk+0x64/0x80\r\n[57310.268549]  do_task_dead+0x43/0x50\r\n[57310.268553]  make_task_dead+0x151/0x170\r\n[57310.268556]  rewind_stack_and_make_dead+0x17/0x20\r\n[57310.268559] RIP: 0033:0x7bf286c6cf7f\r\n[57310.268565] Code: Unable to access opcode bytes at 0x7bf286c6cf55.\r\n[57310.268567] RSP: 002b:00007bf154dffd50 EFLAGS: 00000293 ORIG_RAX: 0000000000000007\r\n[57310.268570] RAX: fffffffffffffdfc RBX: 00000000ffffffff RCX: 00007bf286c6cf7f\r\n[57310.268571] RDX: 0000000000000064 RSI: 000000000000000a RDI: 00007bf0d4000c20\r\n[57310.268573] RBP: 00007bf154dffe20 R08: 0000000000000000 R09: 0000000000000000\r\n[57310.268575] R10: 00007bf154dffde0 R11: 0000000000000293 R12: 0000000000000000\r\n[57310.268577] R13: 0000000000000064 R14: 00007bf0d4000c20 R15: 0000648d4edf1e90\r\n[57310.268580]  </TASK>\r\n[57310.268581] ---[ end trace 0000000000000000 ]---\r\n[57370.273679] rcu: INFO: rcu_preempt detected stalls on CPUs/tasks:\r\n[57370.273693] rcu:     Tasks blocked on level-1 rcu_node (CPUs 0-9): P13789/1:b..l\r\n[57370.273706] rcu:     (detected by 9, t=18002 jiffies, g=495481, q=2899 ncpus=20)\r\n```\r\n\r\n</details>\r\n\r\nAdditionally I couldn't ctrl-c or SIGKILL the worker process\n</Comment>\n<Comment by haotian-liu at 2024-02-03T15:35:28Z>\nDoes mistral work for you now? This seems to be a bnb error?\n</Comment>\n<Comment by InconsolableCellist at 2024-02-03T18:24:07Z>\nMistral 7B worked, probably because I was able to load it into just one GPU. It didn't do a very good job at anything though, nor did 4-bit 34B. I think I need the full FP16 to get performance as good as the demo, which was quite usable. \r\n\r\nI think 8-bit said of the demo pic, \"that's a man standing on an ironing board. It's unusual to stand on an ironing board in traffic.\" etc. \r\n\r\nWhat is bnb?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1052,
    "state": "open",
    "created_by": "BBC-Esq",
    "created_at": "2024-02-02T00:49:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1052</URL>\n\n<TITLE>huggingface version so I can use the standard libraries/bitsandbytes, etc.</TITLE>\n\n<BODY>### feature\n\nconvert to huggingface format...use the trust remote code if you require custom code...but otherwise make it compatible with the standard huggingface related libraries like transformers, better transformer, bitsandbytes, even flash attention 2!!!!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1051,
    "state": "open",
    "created_by": "arthurwolf",
    "created_at": "2024-02-01T19:13:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1051</URL>\n\n<TITLE>[Question] Adding instructions to dataset.</TITLE>\n\n<BODY>### Question\n\nAbsolutely amazing project.\r\n\r\nI see we're getting new releases pretty often. \r\n\r\nI was wondering. If I wanted to make llava better at some tasks I need it to do (most have to do with reading/understanding comic books, which I think would benefit others/the project), by adding questions/answers/examples to the dataset(s) it uses for training, is that possible, and how would I go about it ?\r\n\r\nThanks a lot!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1050,
    "state": "closed",
    "created_by": "hp1337",
    "created_at": "2024-02-01T16:49:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1050</URL>\n\n<TITLE>Multiple GPU inference is broken with LLaVA 1.6</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: Multiple GPU inference is broken with LLaVA 1.6. Same command with model liuhaotian/llava-v1.5-13b works fine.\r\n\r\nCommand:\r\n\r\nCUDA_VISIBLE_DEVICES=0,1 python -m llava.serve.cli --model-path ../models/liuhaotian_llava-v1.6-34b --load-4bit --image-file foo.png\r\n\r\nLog: \r\n\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:20<00:00,  1.38s/it]\r\nUSER: What is the document?\r\nTraceback (most recent call last):\r\n  File \"/home/<user>/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/<user>/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/<user>/LLaVA/llava/serve/cli.py\", line 126, in <module>\r\n    main(args)\r\n  File \"/home/<user>/LLaVA/llava/serve/cli.py\", line 95, in main\r\n    output_ids = model.generate(\r\n  File \"/home/<user>/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/<user>/LLaVA/llava/model/language_model/llava_llama.py\", line 125, in generate\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/home/<user>/LLaVA/llava/model/llava_arch.py\", line 181, in prepare_inputs_labels_for_multimodal\r\n    image_feature = torch.cat((\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument tensors in method wrapper_CUDA_cat)\r\n\r\nI will update if I can figure out where the bug is. Thank you.</BODY>\n\n<COMMENTS>\n<Comment by nkarpovdb at 2024-02-01T19:27:58Z>\n+1 same\r\n\r\n```\r\n(llava) root@0119-194908-8ernbatz-10-68-129-235:~/LLaVA# CUDA_VISIBLE_DEVICES=0,1 python -m llava.serve.cli --model-path liuhaotian/llava-v1.6-34b --load-4bit --image-file \"https://llava-vl.github.io/static/images/view.jpg\"\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:14<00:00,  1.03it/s]\r\nUSER: describe this image\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/root/LLaVA/llava/serve/cli.py\", line 126, in <module>\r\n    main(args)\r\n  File \"/root/LLaVA/llava/serve/cli.py\", line 95, in main\r\n    output_ids = model.generate(\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/root/LLaVA/llava/model/language_model/llava_llama.py\", line 125, in generate\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/root/LLaVA/llava/model/llava_arch.py\", line 181, in prepare_inputs_labels_for_multimodal\r\n    image_feature = torch.cat((\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument tensors in method wrapper_CUDA_cat)\r\n```\n</Comment>\n<Comment by samidten at 2024-02-01T21:01:35Z>\n+1 same\r\nyes, please help fix this!\n</Comment>\n<Comment by Silviase at 2024-02-02T05:10:34Z>\n+1 same, please fix this\n</Comment>\n<Comment by thisthrowaway at 2024-02-02T09:57:07Z>\n+1\r\nBut I got it working by passing --device cuda:0 when creating the model_worker for testing the model. Doesn't fix the issue, only circumvents the bug by using only one gpu.\n</Comment>\n<Comment by yhygta at 2024-02-02T10:46:32Z>\n+1 same, please fix this\n</Comment>\n<Comment by wassimea at 2024-02-02T20:39:52Z>\n+1 same\n</Comment>\n<Comment by haotian-liu at 2024-02-02T22:09:28Z>\nHi all, sorry for the inference. Please pull the latest branch and it shall be fixed in https://github.com/haotian-liu/LLaVA/pull/1057.\n</Comment>\n<Comment by samidten at 2024-02-02T22:44:47Z>\nthanks for trying to fix. now the model loads ok to multi-gpus, however during inference it throws this error:\r\n```\r\n2024-02-02 22:43:31 | ERROR | stderr | Exception in thread Thread-4:\r\n2024-02-02 22:43:31 | ERROR | stderr | Traceback (most recent call last):\r\n2024-02-02 22:43:31 | ERROR | stderr |   File \"/usr/lib/python3.9/threading.py\", line 954, in _bootstrap_inner\r\n2024-02-02 22:43:31 | ERROR | stderr |     self.run()\r\n2024-02-02 22:43:31 | ERROR | stderr |   File \"/usr/lib/python3.9/threading.py\", line 892, in run\r\n2024-02-02 22:43:31 | ERROR | stderr |     self._target(*self._args, **self._kwargs)\r\n2024-02-02 22:43:31 | ERROR | stderr |   File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n2024-02-02 22:43:31 | ERROR | stderr |     return func(*args, **kwargs)\r\n2024-02-02 22:43:31 | ERROR | stderr |   File \"/root/LLaVA/llava/model/language_model/llava_llama.py\", line 125, in generate\r\n2024-02-02 22:43:31 | ERROR | stderr |     ) = self.prepare_inputs_labels_for_multimodal(\r\n2024-02-02 22:43:31 | ERROR | stderr |   File \"/root/LLaVA/llava/model/llava_arch.py\", line 157, in prepare_inputs_labels_for_multimodal\r\n2024-02-02 22:43:31 | ERROR | stderr |     image_features = self.encode_images(concat_images)\r\n2024-02-02 22:43:31 | ERROR | stderr |   File \"/root/LLaVA/llava/model/llava_arch.py\", line 141, in encode_images\r\n2024-02-02 22:43:31 | ERROR | stderr |     image_features = self.get_model().get_vision_tower()(images)\r\n2024-02-02 22:43:31 | ERROR | stderr |   File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2024-02-02 22:43:31 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2024-02-02 22:43:31 | ERROR | stderr |   File \"/usr/local/lib/python3.9/dist-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2024-02-02 22:43:31 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2024-02-02 22:43:31 | ERROR | stderr |   File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n2024-02-02 22:43:31 | ERROR | stderr |     return func(*args, **kwargs)\r\n2024-02-02 22:43:31 | ERROR | stderr |   File \"/root/LLaVA/llava/model/multimodal_encoder/clip_encoder.py\", line 54, in forward\r\n2024-02-02 22:43:31 | ERROR | stderr |     image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\r\n2024-02-02 22:43:31 | ERROR | stderr |   File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2024-02-02 22:43:31 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2024-02-02 22:43:31 | ERROR | stderr |   File \"/usr/local/lib/python3.9/dist-packages/accelerate/hooks.py\", line 160, in new_forward\r\n2024-02-02 22:43:31 | ERROR | stderr |     args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\r\n2024-02-02 22:43:31 | ERROR | stderr |   File \"/usr/local/lib/python3.9/dist-packages/accelerate/hooks.py\", line 290, in pre_forward\r\n2024-02-02 22:43:31 | ERROR | stderr |     return send_to_device(args, self.execution_device), send_to_device(\r\n2024-02-02 22:43:31 | ERROR | stderr |   File \"/usr/local/lib/python3.9/dist-packages/accelerate/utils/operations.py\", line 151, in send_to_device\r\n2024-02-02 22:43:31 | ERROR | stderr |     return honor_type(\r\n2024-02-02 22:43:31 | ERROR | stderr |   File \"/usr/local/lib/python3.9/dist-packages/accelerate/utils/operations.py\", line 83, in honor_type\r\n2024-02-02 22:43:31 | ERROR | stderr |     return type(obj)(generator)\r\n2024-02-02 22:43:31 | ERROR | stderr |   File \"/usr/local/lib/python3.9/dist-packages/accelerate/utils/operations.py\", line 152, in <genexpr>\r\n2024-02-02 22:43:31 | ERROR | stderr |     tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\r\n2024-02-02 22:43:31 | ERROR | stderr |   File \"/usr/local/lib/python3.9/dist-packages/accelerate/utils/operations.py\", line 167, in send_to_device\r\n2024-02-02 22:43:31 | ERROR | stderr |     return tensor.to(device, non_blocking=non_blocking)\r\n2024-02-02 22:43:31 | ERROR | stderr | NotImplementedError: Cannot copy out of meta tensor; no data!\r\n```\n</Comment>\n<Comment by haotian-liu at 2024-02-02T22:55:02Z>\n@samidten \r\n\r\nCan you please share your package list `pip list` as well as the command you run the inference? Thank you.\n</Comment>\n<Comment by samidten at 2024-02-02T23:21:37Z>\n`python3 -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:20001 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.6-34b`\r\n\r\n`python3 -m llava.serve.gradio_web_server --controller http://localhost:20001 --model-list-mode reload --port 8000\r\n`\r\ninference was done via gradio site\r\n\r\n```\r\n# pip list\r\nPackage                   Version\r\n------------------------- ------------\r\naccelerate                0.21.0\r\naiofiles                  23.2.1\r\naiohttp                   3.9.1\r\naiosignal                 1.3.1\r\naltair                    5.2.0\r\nanyio                     4.2.0\r\nasync-timeout             4.0.3\r\nattrs                     23.2.0\r\nbitsandbytes              0.41.0\r\ncertifi                   2023.11.17\r\ncharset-normalizer        3.3.2\r\nclick                     8.1.7\r\ncmake                     3.28.1\r\ncontourpy                 1.2.0\r\ncycler                    0.12.1\r\ndbus-python               1.2.16\r\ndistro-info               1.0\r\neinops                    0.6.1\r\neinops-exts               0.0.4\r\nexceptiongroup            1.2.0\r\nfastapi                   0.109.0\r\nffmpy                     0.3.1\r\nfilelock                  3.13.1\r\nfonttools                 4.47.2\r\nfrozenlist                1.4.1\r\nfsspec                    2023.12.2\r\ngradio                    3.35.2\r\ngradio-client             0.2.9\r\nh11                       0.14.0\r\nhttpcore                  0.17.3\r\nhttpx                     0.24.0\r\nhuggingface-hub           0.20.3\r\nidna                      3.6\r\nimportlib-resources       6.1.1\r\nJinja2                    3.1.3\r\njoblib                    1.3.2\r\njsonschema                4.21.1\r\njsonschema-specifications 2023.12.1\r\nkiwisolver                1.4.5\r\nlinkify-it-py             2.0.2\r\nlit                       17.0.6\r\nllava                     1.2.0\r\nmarkdown-it-py            2.2.0\r\nmarkdown2                 2.4.12\r\nMarkupSafe                2.1.4\r\nmatplotlib                3.8.2\r\nmdit-py-plugins           0.3.3\r\nmdurl                     0.1.2\r\nmercurial                 5.6.1\r\nmpmath                    1.3.0\r\nmultidict                 6.0.4\r\nnetworkx                  3.2.1\r\nnumpy                     1.26.3\r\nnvidia-cublas-cu11        11.10.3.66\r\nnvidia-cuda-cupti-cu11    11.7.101\r\nnvidia-cuda-nvrtc-cu11    11.7.99\r\nnvidia-cuda-runtime-cu11  11.7.99\r\nnvidia-cudnn-cu11         8.5.0.96\r\nnvidia-cufft-cu11         10.9.0.58\r\nnvidia-curand-cu11        10.2.10.91\r\nnvidia-cusolver-cu11      11.4.0.1\r\nnvidia-cusparse-cu11      11.7.4.91\r\nnvidia-nccl-cu11          2.14.3\r\nnvidia-nvtx-cu11          11.7.91\r\norjson                    3.9.12\r\npackaging                 23.2\r\npandas                    2.2.0\r\npeft                      0.4.0\r\npillow                    10.2.0\r\npip                       20.3.4\r\npsutil                    5.9.8\r\npycurl                    7.43.0.6\r\npydantic                  1.10.14\r\npydub                     0.25.1\r\npygments                  2.17.2\r\nPyGObject                 3.38.0\r\npyparsing                 3.1.1\r\npython-apt                2.2.1\r\npython-dateutil           2.8.2\r\npython-multipart          0.0.6\r\npytz                      2023.3.post1\r\nPyYAML                    6.0.1\r\nreferencing               0.32.1\r\nregex                     2023.12.25\r\nrequests                  2.31.0\r\nrpds-py                   0.17.1\r\nsafetensors               0.4.2\r\nscikit-learn              1.2.2\r\nscipy                     1.12.0\r\nsemantic-version          2.10.0\r\nsentencepiece             0.1.99\r\nsetuptools                52.0.0\r\nshortuuid                 1.0.11\r\nsix                       1.16.0\r\nsniffio                   1.3.0\r\nstarlette                 0.35.1\r\nsvgwrite                  1.4.3\r\nsympy                     1.12\r\nthreadpoolctl             3.2.0\r\ntimm                      0.6.13\r\ntokenizers                0.15.0\r\ntoolz                     0.12.1\r\ntorch                     2.0.1\r\ntorchvision               0.15.2\r\ntqdm                      4.66.1\r\ntransformers              4.36.2\r\ntriton                    2.0.0\r\ntyping-extensions         4.9.0\r\ntzdata                    2023.4\r\nuc-micro-py               1.0.2\r\nunattended-upgrades       0.1\r\nurllib3                   2.1.0\r\nuvicorn                   0.27.0\r\nwavedrom                  2.0.3.post3\r\nwebsockets                12.0\r\nwheel                     0.34.2\r\nyarl                      1.9.4\r\nzipp                      3.17.0\r\n```\n</Comment>\n<Comment by haotian-liu at 2024-02-03T00:01:42Z>\n@samidten \r\n\r\nI just tried building the env from scratch and using the same command as yours, I do not meet the issue. I've also tried using 4x 3090 to serve llava-v1.6-34b; as well as using 1x 3090 to serve it in 4-bit mode. Both works.\r\n\r\nCan you confirm the hardware that you are working with and how much VRAM do you have?\r\n\r\nI would recommend re-building the env from scratch as well. Also, you can try if 4-bit works: add `--load-4bit` to the end of your command line.\n</Comment>\n<Comment by samidten at 2024-02-03T00:22:43Z>\ni tried with clean venv but still same issue. 4-bit works with 2 cards...\r\nlets see if others have similar issue with multi-gpus with latest branch\r\n\r\n```\r\n# nvidia-smi\r\nSat Feb  3 00:21:05 2024\r\n+---------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\r\n|-----------------------------------------+----------------------+----------------------+\r\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                      |               MIG M. |\r\n|=========================================+======================+======================|\r\n|   0  NVIDIA A30                     On  | 00000000:17:00.0 Off |                    0 |\r\n| N/A   28C    P0              31W / 165W |  22533MiB / 24576MiB |      0%      Default |\r\n|                                         |                      |             Disabled |\r\n+-----------------------------------------+----------------------+----------------------+\r\n|   1  NVIDIA A30                     On  | 00000000:CA:00.0 Off |                    0 |\r\n| N/A   30C    P0              31W / 165W |  23733MiB / 24576MiB |      0%      Default |\r\n|                                         |                      |             Disabled |\r\n+-----------------------------------------+----------------------+----------------------+\r\n\r\n+---------------------------------------------------------------------------------------+\r\n| Processes:                                                                            |\r\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n|        ID   ID                                                             Usage      |\r\n|=======================================================================================|\r\n+---------------------------------------------------------------------------------------+\r\n```\n</Comment>\n<Comment by haotian-liu at 2024-02-03T00:43:02Z>\n@samidten \r\n\r\nThis should be related to the OOM issue. 34B shall require at least 80GB VRAM to serve, and given your GPUs, it may be better to just use them with 4-bit quantization.\n</Comment>\n<Comment by LumenYoung at 2024-02-07T12:41:27Z>\nSorry to reopen this issue but I have the same problem again and I'm sure my device is not OOMed since I used my machine to run smoothly the 13b model previously.\r\n\r\nThe problem occurs [here](https://github.com/haotian-liu/LLaVA/blob/49540feacde955f57cb6a1c1a85066a214bde256/llava/model/llava_arch.py#L193), where I saw [your patch](https://github.com/haotian-liu/LLaVA/commit/4c92669d1719a28b7f13872ea6a9d622b68215be#diff-4477387d506ccb1897a13972cba26c9da3fad4d3e1c32ec4b8bd8ff7acd3f292) only added the to(device) in the other branch.\r\n\r\nI'm using llava1.6 13b at 4bit quantization and it is quiet interesting that I failed to change the device of the self.model.image_newline with to method. \r\n\r\n```\r\n(Pdb) print(self.model.image_newline.to(\"cuda\"))\r\ntensor([-0.0226, -0.0078, -0.0162,  ..., -0.0112,  0.0264, -0.0170],\r\n       device='cuda:1', dtype=torch.float16)\r\n(Pdb) p self.model.image_newline.device\r\ndevice(type='cuda', index=0)\r\n(Pdb) torch.cat((image_feature,self.model.image_newline[None]),dim=0)\r\n*** RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument tensors in method wrapper_CUDA_cat)\r\n```\r\n\r\nI would like to seek for your advice on how to fix this. @haotian-liu\n</Comment>\n<Comment by haotian-liu at 2024-02-07T16:15:21Z>\nThanks for reporting. Does https://github.com/haotian-liu/LLaVA/commit/b42a13d14b6118381a667430a5b8c50f9790dee3 fix it?\n</Comment>\n<Comment by LumenYoung at 2024-02-07T16:59:45Z>\n> Thanks for reporting. Does [b42a13d](https://github.com/haotian-liu/LLaVA/commit/b42a13d14b6118381a667430a5b8c50f9790dee3) fix it?\r\n\r\nThanks a lot Haotian. Yes this fixed my problem. Interestingly though, is that my previous patch was only a bit different from your patch:\r\n\r\n```py\r\n                        if 'unpad' in mm_patch_merge_type:\r\n                            self.model.image_newline.to(image_feature.device)\r\n                            image_feature = torch.cat((\r\n                                image_feature,\r\n                                self.model.image_newline[None]\r\n                            ), dim=0)\r\n```\r\n\r\nWhich was pretty similar in my opinion. But that didn't fix it, the device problem still persists after my patch. Do you know if there is a reason behind this different behavior?\n</Comment>\n<Comment by haotian-liu at 2024-02-07T17:01:02Z>\n```\r\ntensor_on_device = tensor.to(device)\r\n```\r\n\r\n`.to` is not an inplace operator\n</Comment>\n<Comment by LumenYoung at 2024-02-07T17:02:51Z>\n> ```\n> tensor_on_device = tensor.to(device)\n> ```\n> \n> `.to` is not an inplace operator\n\nThanks for your prompt reply. Okey, I should have remember it :).\n</Comment>\n<Comment by shllgtca at 2024-04-15T23:40:36Z>\nHi!\r\n\r\nI've been having the same problem. I did a workaround as others to set just one gpu, although there's 2 available. Could you help me out on how to run in multiple gpus?\r\n\r\nMy work around:\r\n\r\n```\r\n\r\nimport os\r\nos.environ[\"PYTORCH_USE_CUDA_DSA\"] = \"1\"\r\nos.environ['TOKENIZERS_PARALLELISM']='TRUE'\r\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\r\nos.environ[\"PYTORCH_USE_CUDA_DSA\"] = \"1\"\r\nimport sys\r\nsys.path.append('/home/experiment/volume/rag_multimodal/main/models/LLaVA' )\r\nprint(sys.path)\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.mm_utils import get_model_name_from_path\r\nfrom llava.eval.run_llava import eval_model\r\nfrom llava.mm_utils import (process_images, tokenizer_image_token)\r\nfrom llava.constants import (IMAGE_TOKEN_INDEX)\r\nimport cv2\r\nimport torch\r\ntorch.cuda.empty_cache()\r\nfrom PIL import Image \r\n\r\ndef llavaRunner():\r\n    image = cv2.imread(\"/home/experiment/volume/rag_multimodal/main/models/DeepSeek-VL/images/training_pipelines.jpg\")\r\n    images = [Image.fromarray(image)]\r\n    \r\n    model_path = \"liuhaotian/llava-v1.5-7b\"\r\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(\r\n        model_path=model_path,\r\n        model_base=None,\r\n        model_name=get_model_name_from_path(model_path),device=device\r\n    )\r\n\r\n    prompt='describe the image'\r\n    image_sizes = [x.size for x in images]\r\n    images_tensor = process_images(\r\n        images,\r\n        image_processor,\r\n        model.config\r\n    ).to(model.device, dtype=torch.float16)\r\n    \r\n    input_ids = (\r\n        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\r\n        .unsqueeze(0)\r\n        .cuda()\r\n    ).to(device)\r\n\r\n    with torch.inference_mode():\r\n        output_ids = model.generate(\r\n            input_ids,\r\n            images=images_tensor,\r\n            image_sizes=image_sizes,\r\n            do_sample=True,#True if args.temperature > 0 else False,\r\n            temperature=0.2,\r\n            top_p=None,\r\n            num_beams=1,\r\n            max_new_tokens=512,\r\n            use_cache=True,\r\n        )\r\n\r\n    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\r\n    print(outputs)\r\n\r\n```\r\n\r\n\r\n###################################################################\r\n\r\n\r\n\r\nI also got the latest transformers which you can check on my pip freeze:\r\n\r\n pip freeze\r\naccelerate==0.21.0\r\naiofiles==23.2.1\r\naltair==5.3.0\r\nannotated-types==0.6.0\r\nanyio==4.3.0\r\nasttokens==2.4.1\r\nattrs==23.2.0\r\nbackcall==0.2.0\r\nbitsandbytes==0.43.1\r\ncertifi==2024.2.2\r\ncharset-normalizer==3.3.2\r\nclick==8.1.7\r\ncomm==0.2.2\r\ncontourpy==1.1.1\r\ncycler==0.12.1\r\ndebugpy==1.8.1\r\ndecorator==5.1.1\r\neinops==0.6.1\r\neinops-exts==0.0.4\r\nexceptiongroup==1.2.0\r\nexecuting==2.0.1\r\nfastapi==0.110.1\r\nffmpy==0.3.2\r\nfilelock==3.13.4\r\nfonttools==4.51.0\r\nfsspec==2024.3.1\r\ngradio==4.16.0\r\ngradio_client==0.8.1\r\nh11==0.14.0\r\nhttpcore==0.17.3\r\nhttpx==0.24.0\r\nhuggingface-hub==0.22.2\r\nidna==3.7\r\nimportlib_metadata==7.1.0\r\nimportlib_resources==6.4.0\r\nipykernel==6.29.4\r\nipython==8.12.3\r\njedi==0.19.1\r\nJinja2==3.1.3\r\njoblib==1.4.0\r\njsonschema==4.21.1\r\njsonschema-specifications==2023.12.1\r\njupyter_client==8.6.1\r\njupyter_core==5.7.2\r\nkiwisolver==1.4.5\r\n# Editable Git install with no remote (llava==1.2.2.post1)\r\n-e /home/experiment/volume/rag_multimodal/main/models/LLaVA\r\nmarkdown-it-py==3.0.0\r\nmarkdown2==2.4.13\r\nMarkupSafe==2.1.5\r\nmatplotlib==3.7.5\r\nmatplotlib-inline==0.1.7\r\nmdurl==0.1.2\r\nmpmath==1.3.0\r\nnest-asyncio==1.6.0\r\nnetworkx==3.1\r\nnumpy==1.24.4\r\nnvidia-cublas-cu12==12.1.3.1\r\nnvidia-cuda-cupti-cu12==12.1.105\r\nnvidia-cuda-nvrtc-cu12==12.1.105\r\nnvidia-cuda-runtime-cu12==12.1.105\r\nnvidia-cudnn-cu12==8.9.2.26\r\nnvidia-cufft-cu12==11.0.2.54\r\nnvidia-curand-cu12==10.3.2.106\r\nnvidia-cusolver-cu12==11.4.5.107\r\nnvidia-cusparse-cu12==12.1.0.106\r\nnvidia-nccl-cu12==2.18.1\r\nnvidia-nvjitlink-cu12==12.4.127\r\nnvidia-nvtx-cu12==12.1.105\r\nopencv-python==4.9.0.80\r\norjson==3.10.0\r\npackaging==24.0\r\npandas==2.0.3\r\nparso==0.8.4\r\npeft==0.10.0\r\npexpect==4.9.0\r\npickleshare==0.7.5\r\npillow==10.3.0\r\npkgutil_resolve_name==1.3.10\r\nplatformdirs==4.2.0\r\nprompt-toolkit==3.0.43\r\nprotobuf==5.26.1\r\npsutil==5.9.8\r\nptyprocess==0.7.0\r\npure-eval==0.2.2\r\npydantic==2.7.0\r\npydantic_core==2.18.1\r\npydub==0.25.1\r\nPygments==2.17.2\r\npyparsing==3.1.2\r\npython-dateutil==2.9.0.post0\r\npython-multipart==0.0.9\r\npytz==2024.1\r\nPyYAML==6.0.1\r\npyzmq==26.0.0\r\nreferencing==0.34.0\r\nregex==2023.12.25\r\nrequests==2.31.0\r\nrich==13.7.1\r\nrpds-py==0.18.0\r\nruff==0.3.7\r\nsafetensors==0.4.3\r\nscikit-learn==1.2.2\r\nscipy==1.10.1\r\nsemantic-version==2.10.0\r\nsentencepiece==0.1.99\r\nshellingham==1.5.4\r\nshortuuid==1.0.13\r\nsix==1.16.0\r\nsniffio==1.3.1\r\nstack-data==0.6.3\r\nstarlette==0.37.2\r\nsvgwrite==1.4.3\r\nsympy==1.12\r\nthreadpoolctl==3.4.0\r\ntimm==0.6.13\r\ntokenizers==0.15.1\r\ntomlkit==0.12.0\r\ntoolz==0.12.1\r\ntorch==2.1.2\r\ntorchvision==0.16.2\r\ntornado==6.4\r\ntqdm==4.66.2\r\ntraitlets==5.14.2\r\ntransformers==4.37.2\r\ntriton==2.1.0\r\ntyper==0.12.3\r\ntyping_extensions==4.11.0\r\ntzdata==2024.1\r\nurllib3==2.2.1\r\nuvicorn==0.29.0\r\nwavedrom==2.0.3.post3\r\nwcwidth==0.2.13\r\nwebsockets==11.0.3\r\nzipp==3.18.1\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1049,
    "state": "open",
    "created_by": "feiliya333",
    "created_at": "2024-02-01T16:24:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1049</URL>\n\n<TITLE>[bug] It seems that LLaVA/llava/train/train.py has not been updated for v1.6</TITLE>\n\n<BODY>### Describe the issue\n\nFor example, in the current LLaVA/llava/train/train.py, there is no image_grid_pinpoints parameter in the model_args, which will be used in llava/mm_utils.py, line176: image = process_anyres_image(image, image_processor, model_cfg.image_grid_pinpoints)</BODY>\n\n<COMMENTS>\n<Comment by jayyoung0802 at 2024-02-02T02:49:38Z>\nI guess \r\npossible_resolutions = [(2,2), (1,2), (2,1), (1,3), (3,1), (1,4), (4,1)]\r\npossible_resolutions = [(x * 336, y * 336) for x,y in possible_resolutions]\r\nimage_grid_pinpoints = possible_resolutions\n</Comment>\n<Comment by gameveloster at 2024-02-02T06:28:18Z>\nIs the code suggested by @jayyoung0802 meant for training all v1.6 models?\r\nWhich model have you all found to be successfully training with this code?\n</Comment>\n<Comment by haotian-liu at 2024-02-03T15:38:02Z>\nThe current code base does not support training v1.6 models yet. We'll release the training code and data with v1.6 models soon. Thanks.\n</Comment>\n<Comment by Forence1999 at 2024-08-04T13:44:46Z>\n> I guess possible_resolutions = [(2,2), (1,2), (2,1), (1,3), (3,1), (1,4), (4,1)] possible_resolutions = [(x * 336, y * 336) for x,y in possible_resolutions] image_grid_pinpoints = possible_resolutions\r\n\r\nI meet this problem again when reading the config of llava-1.6 models. It seems that not all of the mentioned resolutions are supported.\r\nPlease refer to my newly raised question https://github.com/haotian-liu/LLaVA/issues/1644 for further information.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1048,
    "state": "open",
    "created_by": "yinincanada",
    "created_at": "2024-02-01T15:15:10Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1048</URL>\n\n<TITLE>[Feature request] lora finetune script for llava-1.6</TITLE>\n\n<BODY>### feature\n\nHello, I am wondering roughly when the lora finetune script for llava-1.6 can be released? thanks!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1047,
    "state": "open",
    "created_by": "ashleykleynhans",
    "created_at": "2024-02-01T14:56:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1047</URL>\n\n<TITLE>[Feature request] Push 1.2.0 release of llava-torch to Pypi</TITLE>\n\n<BODY>### feature\n\nThe latest release on PyPi for llava-torch is 1.1.1, which I believe is only LLaVA 1.5.\r\nPlease could you push the 1.2.0 release of llava-torch to PyPi so that we can benefit from LLaVA 1.6?</BODY>\n\n<COMMENTS>\n<Comment by ashleykleynhans at 2024-02-05T19:37:10Z>\nThanks for pushing 1.2.2 and 1.2.2.post1 to PyPi, but 1.2.2.post1 doesn't have the latest update to PyTorch 2.1.2, it is still on 2.1.0  Please can you either push 1.2.2.post2 or 1.2.3 to bump PyTorch to version 2.1.2?  It would also be nice if the releases on Github matched the ones on PyPi, currently Github release is 1.2.0 and not 1.2.2 or 1.2.2.post1.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1046,
    "state": "open",
    "created_by": "xiaoachen98",
    "created_at": "2024-02-01T08:53:26Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1046</URL>\n\n<TITLE>We support the quick link to download all SAM images in ShareGPT4V for training LLaVA-1.6</TITLE>\n\n<BODY>### Discussion\r\n\r\nWe update the link at https://github.com/InternLM/InternLM-XComposer/blob/main/projects/ShareGPT4V/docs/Data.md. Now you can quickly download 9K SAM images used in ShareGPT4V.\r\nIt will be helpful for you to build your entire training data for LLaVA-1.6.</BODY>\n\n<COMMENTS>\n<Comment by cooleel at 2024-05-24T21:51:52Z>\n@xiaoachen98 Thanks for the great work. I wonder if you have any fast solution to get the image sets you used in pretraining? \r\n>We utilize 000000:000050 files.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1045,
    "state": "open",
    "created_by": "LiXinYuann",
    "created_at": "2024-02-01T07:41:28Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1045</URL>\n\n<TITLE>KeyError: 'LlavaConfig'</TITLE>\n\n<BODY>### Question\n\nI used a script to merge the fine-tuned models, and the script is as follows:：\r\npython merge_lora_weights.py \\\r\n    --model-path /***/LLaVA/scripts/v1_5/test1 \\\r\n    --model-base /***/LLaVA/llava-v1.5-13b \\\r\n--save-model-path /***/LLaVA/scripts/v1_5/merge_test1\r\n\r\nThe script for inference using cli.py after merging is as follows:：\r\npython cli.py \\\r\n    --model-path /***/LLaVA/scripts/v1_5/merge_test1 \\\r\n    --image-file \"/***/LLaVA/llava/serve/pic\" \\\r\n--load-8bit\r\nUpon running this, an error is encountered:\r\n\r\nTraceback (most recent call last):\r\n  File \"/***/LLaVA/llava/serve/cli.py\", line 124, in <module>\r\n    main(args)\r\n  File \"/***/LLaVA/llava/serve/cli.py\", line 32, in main\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n  File \"/***/LLaVA/llava/model/builder.py\", line 126, in load_pretrained_model\r\n    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n  File \"/***/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 718, in from_pretrained\r\n    tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]\r\n  File \"/***/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 663, in __getitem__\r\n    model_type = self._reverse_config_mapping[key.__name__]\r\nKeyError: 'LlavaConfig'</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1044,
    "state": "open",
    "created_by": "zhangmozhe",
    "created_at": "2024-02-01T06:10:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1044</URL>\n\n<TITLE>The data release</TITLE>\n\n<BODY>### Question\r\n\r\nThanks for your awesome work! Just wonder when the instruction data of llava1.6 will be released. Look forward to it. Thanks!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1043,
    "state": "open",
    "created_by": "iceman-p",
    "created_at": "2024-02-01T03:51:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1043</URL>\n\n<TITLE>[Usage] llava-v1.6-mistral-7b will load in the demo, but llava-v1.6-34b will not.</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: When starting a worker with 34B version of the 1.6 model, the worker will crash on the first inference. I've verified that the mistal-7b version does work and I can run the demo with the mistral version; this only happens on the 34B:\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ~/models/liuhaotian_llava-v1.6-34b/\r\n```\r\n\r\nLog: \r\n```\r\n[2024-01-31 22:40:12,336] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n2024-01-31 22:40:12 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='/home/iceman/models/liuhaotian_llava-v1.6-34b/', model_base=None, model_name=None, device='cuda', multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=False)                                                                               2024-01-31 22:40:12 | INFO | model_worker | Loading the model liuhaotian_llava-v1.6-34b on worker b95d53 ...            You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.                                                                                    Loading checkpoint shards:   0%|                                                                 | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|███▊                                                     | 1/15 [00:01<00:24,  1.78s/it]Loading checkpoint shards:  13%|███████▌                                                 | 2/15 [00:03<00:23,  1.78s/it]Loading checkpoint shards:  20%|███████████▍                                             | 3/15 [00:05<00:21,  1.82s/it]Loading checkpoint shards:  27%|███████████████▏                                         | 4/15 [00:07<00:19,  1.81s/it]Loading checkpoint shards:  33%|███████████████████                                      | 5/15 [00:08<00:17,  1.80s/it]Loading checkpoint shards:  40%|██████████████████████▊                                  | 6/15 [00:10<00:16,  1.83s/it]Loading checkpoint shards:  47%|██████████████████████████▌                              | 7/15 [00:12<00:14,  1.81s/it]Loading checkpoint shards:  53%|██████████████████████████████▍                          | 8/15 [00:14<00:12,  1.80s/it]Loading checkpoint shards:  60%|██████████████████████████████████▏                      | 9/15 [00:16<00:10,  1.82s/it]Loading checkpoint shards:  67%|█████████████████████████████████████▎                  | 10/15 [00:18<00:09,  1.81s/it]Loading checkpoint shards:  73%|█████████████████████████████████████████               | 11/15 [00:19<00:07,  1.80s/it]Loading checkpoint shards:  80%|████████████████████████████████████████████▊           | 12/15 [00:21<00:05,  1.82s/it]Loading checkpoint shards:  87%|████████████████████████████████████████████████▌       | 13/15 [00:23<00:03,  1.81s/it]Loading checkpoint shards:  93%|████████████████████████████████████████████████████▎   | 14/15 [00:25<00:01,  1.80s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████| 15/15 [00:26<00:00,  1.49s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████| 15/15 [00:26<00:00,  1.74s/it]2024-01-31 22:40:42 | ERROR | stderr |\r\n2024-01-31 22:40:43 | INFO | model_worker | Register to controller\r\n2024-01-31 22:40:43 | ERROR | stderr | INFO:     Started server process [7458]                                          2024-01-31 22:40:43 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2024-01-31 22:40:43 | ERROR | stderr | INFO:     Application startup complete.\r\n2024-01-31 22:40:43 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)         2024-01-31 22:40:50 | INFO | stdout | INFO:     127.0.0.1:39398 - \"POST /worker_get_status HTTP/1.1\" 200 OK\r\n2024-01-31 22:40:54 | INFO | model_worker | Send heart beat. Models: ['liuhaotian_llava-v1.6-34b']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1                                                                              2024-01-31 22:40:54 | INFO | stdout | INFO:     127.0.0.1:39402 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\n2024-01-31 22:40:54 | ERROR | stderr | Exception in thread Thread-3 (generate):\r\n2024-01-31 22:40:54 | ERROR | stderr | Traceback (most recent call last):\r\n2024-01-31 22:40:54 | ERROR | stderr |   File \"/home/iceman/miniconda3/envs/llava/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\r\n2024-01-31 22:40:54 | ERROR | stderr |     self.run()\r\n2024-01-31 22:40:54 | ERROR | stderr |   File \"/home/iceman/miniconda3/envs/llava/lib/python3.10/threading.py\", line 953, in run\r\n2024-01-31 22:40:54 | ERROR | stderr |     self._target(*self._args, **self._kwargs)\r\n2024-01-31 22:40:54 | ERROR | stderr |   File \"/home/iceman/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n2024-01-31 22:40:54 | ERROR | stderr |     return func(*args, **kwargs)\r\n2024-01-31 22:40:54 | ERROR | stderr |   File \"/home/iceman/src/LLaVA/llava/model/language_model/llava_llama.py\", line 125, in generate\r\n2024-01-31 22:40:54 | ERROR | stderr |     ) = self.prepare_inputs_labels_for_multimodal(\r\n2024-01-31 22:40:54 | ERROR | stderr |   File \"/home/iceman/src/LLaVA/llava/model/llava_arch.py\", line 157, in prepare_inputs_labels_for_multimodal\r\n2024-01-31 22:40:54 | ERROR | stderr |     image_features = self.encode_images(concat_images)\r\n2024-01-31 22:40:54 | ERROR | stderr |   File \"/home/iceman/src/LLaVA/llava/model/llava_arch.py\", line 141, in encode_images\r\n2024-01-31 22:40:54 | ERROR | stderr |     image_features = self.get_model().get_vision_tower()(images)\r\n2024-01-31 22:40:54 | ERROR | stderr |   File \"/home/iceman/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2024-01-31 22:40:54 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2024-01-31 22:40:54 | ERROR | stderr |   File \"/home/iceman/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2024-01-31 22:40:54 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2024-01-31 22:40:54 | ERROR | stderr |   File \"/home/iceman/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n2024-01-31 22:40:54 | ERROR | stderr |     return func(*args, **kwargs)\r\n2024-01-31 22:40:54 | ERROR | stderr |   File \"/home/iceman/src/LLaVA/llava/model/multimodal_encoder/clip_encoder.py\", line 50, in forward\r\n2024-01-31 22:40:54 | ERROR | stderr |     image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\r\n2024-01-31 22:40:54 | ERROR | stderr |   File \"/home/iceman/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2024-01-31 22:40:54 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2024-01-31 22:40:54 | ERROR | stderr |   File \"/home/iceman/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2024-01-31 22:40:54 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2024-01-31 22:40:54 | ERROR | stderr |   File \"/home/iceman/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 917, in forward\r\n2024-01-31 22:40:54 | ERROR | stderr |     return self.vision_model(\r\n2024-01-31 22:40:54 | ERROR | stderr |   File \"/home/iceman/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2024-01-31 22:40:54 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2024-01-31 22:40:54 | ERROR | stderr |   File \"/home/iceman/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2024-01-31 22:40:54 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2024-01-31 22:40:54 | ERROR | stderr |   File \"/home/iceman/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 841, in forward\r\n2024-01-31 22:40:54 | ERROR | stderr |     hidden_states = self.embeddings(pixel_values)\r\n2024-01-31 22:40:54 | ERROR | stderr |   File \"/home/iceman/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2024-01-31 22:40:54 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2024-01-31 22:40:54 | ERROR | stderr |   File \"/home/iceman/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2024-01-31 22:40:54 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2024-01-31 22:40:54 | ERROR | stderr |   File \"/home/iceman/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 182, in forward\r\n2024-01-31 22:40:54 | ERROR | stderr |     patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]\r\n2024-01-31 22:40:54 | ERROR | stderr |   File \"/home/iceman/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2024-01-31 22:40:54 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2024-01-31 22:40:54 | ERROR | stderr |   File \"/home/iceman/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2024-01-31 22:40:54 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2024-01-31 22:40:54 | ERROR | stderr |   File \"/home/iceman/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 463, in forward\r\n2024-01-31 22:40:54 | ERROR | stderr |     return self._conv_forward(input, self.weight, self.bias)\r\n2024-01-31 22:40:54 | ERROR | stderr |   File \"/home/iceman/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\r\n2024-01-31 22:40:54 | ERROR | stderr |     return F.conv2d(input, weight, bias, self.stride,\r\n2024-01-31 22:40:54 | ERROR | stderr | RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument weight in method wrapper_CUDA__cudnn_convolution)\r\n```\r\n\r\n---\r\n\r\nGiven the error is complaining about tensors on two cuda devices on this machine (this is a 2x6000 workstation), I tried running with CUDA_VISIBLE_DEVICES=0 to have it only work with a single card, but that also doesn't work: the worker doesn't even successfully launch itself, hard crashing before it communicates with the gradio process:\r\n\r\nCommand:\r\n```\r\nCUDA_VISIBLE_DEVICES=0 python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ~/models/liuhaotian_llava-v1.6-34b/\r\n```\r\n\r\nLog: \r\n```\r\n[2024-01-31 22:46:17,323] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n2024-01-31 22:46:17 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='/home/iceman/models/liuhaotian_llava-v1.6-34b/', model_base=None, model_name=None, device='cuda', multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=False)\r\n2024-01-31 22:46:17 | INFO | model_worker | Loading the model liuhaotian_llava-v1.6-34b on worker f19483 ...\r\nYou are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\r\nLoading checkpoint shards:   0%|                                                                                                                                    | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|████████▎                                                                                                                   | 1/15 [00:01<00:25,  1.86s/it]Loading checkpoint shards:  13%|████████████████▌                                                                                                           | 2/15 [00:03<00:24,  1.85s/it]Loading checkpoint shards:  20%|████████████████████████▊                                                                                                   | 3/15 [00:05<00:22,  1.90s/it]Loading checkpoint shards:  27%|█████████████████████████████████                                                                                           | 4/15 [00:07<00:20,  1.89s/it]Loading checkpoint shards:  33%|█████████████████████████████████████████▎                                                                                  | 5/15 [00:09<00:18,  1.88s/it]Loading checkpoint shards:  40%|█████████████████████████████████████████████████▌                                                                          | 6/15 [00:11<00:17,  1.90s/it]Loading checkpoint shards:  47%|█████████████████████████████████████████████████████████▊                                                                  | 7/15 [00:13<00:15,  1.89s/it]Loading checkpoint shards:  53%|██████████████████████████████████████████████████████████████████▏                                                         | 8/15 [00:15<00:13,  1.88s/it]Loading checkpoint shards:  60%|██████████████████████████████████████████████████████████████████████████▍                                                 | 9/15 [00:16<00:11,  1.90s/it]Loading checkpoint shards:  67%|██████████████████████████████████████████████████████████████████████████████████                                         | 10/15 [00:18<00:09,  1.88s/it]Loading checkpoint shards:  73%|██████████████████████████████████████████████████████████████████████████████████████████▏                                | 11/15 [00:20<00:06,  1.72s/it]Loading checkpoint shards:  80%|██████████████████████████████████████████████████████████████████████████████████████████████████▍                        | 12/15 [00:21<00:04,  1.66s/it]Loading checkpoint shards:  87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▌                | 13/15 [00:23<00:03,  1.55s/it]Loading checkpoint shards:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊        | 14/15 [00:24<00:01,  1.48s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:24<00:00,  1.20s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:24<00:00,  1.66s/it]2024-01-31 22:46:46 | ERROR | stderr |\r\n2024-01-31 22:46:46 | ERROR | stderr | Traceback (most recent call last):\r\n2024-01-31 22:46:46 | ERROR | stderr |   File \"/home/iceman/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n2024-01-31 22:46:46 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n2024-01-31 22:46:46 | ERROR | stderr |   File \"/home/iceman/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n2024-01-31 22:46:46 | ERROR | stderr |     exec(code, run_globals)\r\n2024-01-31 22:46:46 | ERROR | stderr |   File \"/home/iceman/src/LLaVA/llava/serve/model_worker.py\", line 277, in <module>\r\n2024-01-31 22:46:46 | ERROR | stderr |     worker = ModelWorker(args.controller_address,\r\n2024-01-31 22:46:46 | ERROR | stderr |   File \"/home/iceman/src/LLaVA/llava/serve/model_worker.py\", line 65, in __init__\r\n2024-01-31 22:46:46 | ERROR | stderr |     self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\r\n2024-01-31 22:46:46 | ERROR | stderr |   File \"/home/iceman/src/LLaVA/llava/model/builder.py\", line 151, in load_pretrained_model\r\n2024-01-31 22:46:46 | ERROR | stderr |     vision_tower.to(device=device, dtype=torch.float16)\r\n2024-01-31 22:46:46 | ERROR | stderr |   File \"/home/iceman/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1145, in to\r\n2024-01-31 22:46:46 | ERROR | stderr |     return self._apply(convert)\r\n2024-01-31 22:46:46 | ERROR | stderr |   File \"/home/iceman/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 797, in _apply\r\n2024-01-31 22:46:46 | ERROR | stderr |     module._apply(fn)\r\n2024-01-31 22:46:46 | ERROR | stderr |   File \"/home/iceman/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 797, in _apply\r\n2024-01-31 22:46:46 | ERROR | stderr |     module._apply(fn)\r\n2024-01-31 22:46:46 | ERROR | stderr |   File \"/home/iceman/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 797, in _apply\r\n2024-01-31 22:46:46 | ERROR | stderr |     module._apply(fn)\r\n2024-01-31 22:46:46 | ERROR | stderr |   [Previous line repeated 1 more time]\r\n2024-01-31 22:46:46 | ERROR | stderr |   File \"/home/iceman/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 820, in _apply\r\n2024-01-31 22:46:46 | ERROR | stderr |     param_applied = fn(param)\r\n2024-01-31 22:46:46 | ERROR | stderr |   File \"/home/iceman/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1143, in convert\r\n2024-01-31 22:46:46 | ERROR | stderr |     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\r\n2024-01-31 22:46:46 | ERROR | stderr | NotImplementedError: Cannot copy out of meta tensor; no data!\r\n```\r\n\r\nCurrent commit ran: c878cc3e66f75eb8227870be3d30268789913f82</BODY>\n\n<COMMENTS>\n<Comment by serjsaraev at 2024-02-01T08:11:18Z>\nSame problem\n</Comment>\n<Comment by levi at 2024-02-01T16:35:43Z>\nSame issue #1039\n</Comment>\n<Comment by levi at 2024-02-01T17:48:39Z>\nBumping the vram to 80GB appears to have resolved it for me. Possibly a OOM error?\n</Comment>\n<Comment by iceman-p at 2024-02-01T21:08:51Z>\n> Bumping the vram to 80GB appears to have resolved it for me. Possibly a OOM error?\r\n\r\nThat would explain why when I restrict cuda visibility to a single 48gb card I get the error, but it doesn't solve the main problem: two 48gb cards should(tm) provide enough vram and the main bug here is it isn't splitting between the cards.\n</Comment>\n<Comment by matankley at 2024-02-06T09:54:08Z>\nSame issue here. Were you able to fix that ? @levi @iceman-p  \r\nI suspect this is related to `device=\"auto\"` and `low_cpu_mem_usage=True`\n</Comment>\n<Comment by aliencaocao at 2024-02-10T05:28:47Z>\n@iceman-p Hi how did you load the 7b one? I am having trouble loading as i get https://github.com/haotian-liu/LLaVA/issues/1112\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1042,
    "state": "closed",
    "created_by": "caopulan",
    "created_at": "2024-02-01T03:25:35Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1042</URL>\n\n<TITLE>[Usage] ImportError: cannot import name 'ShardedDDPOption' from 'transformers.trainer'</TITLE>\n\n<BODY>### Describe the issue\n\ntransformers no longer has SharedDDPOption after v4.35.0</BODY>\n\n<COMMENTS>\n<Comment by attnmamba at 2024-02-01T03:52:18Z>\nSame issue, looking for fix.\r\nFollowing the pip install instructions in README, I have `transformers==4.36.2` installed, which is the version pinned by `pyproject.toml`\r\n\r\nHow does everyone else manage to run this?\n</Comment>\n<Comment by haotian-liu at 2024-02-01T04:46:13Z>\nHi @caopulan @attnmamba\r\n\r\nSorry for the confusion. It should now be fixed in the main branch. Please let me know if it works for you, thanks.\n</Comment>\n<Comment by attnmamba at 2024-02-01T06:06:57Z>\n@haotian-liu Thank you! The new commit solved the above issue, but there are warning message is constantly being printed out during finetuning:\r\n\r\nFirst warning (printed once):\r\n\r\n> /home/attnmamba/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\r\n\r\n\r\nRepeated warnings (continuously printed):\r\n\r\n> WARNING: tokenization mismatch: 1 vs. 57. (ignored)\r\n\r\n\r\nMy installed packages are\r\n\r\n```\r\nflash-attn                2.5.2\r\npeft                      0.4.0\r\ntokenizers                0.15.0\r\ntorch                     2.0.1\r\ntorchvision               0.15.2\r\ntransformers              4.36.2\r\n```\r\n\r\nAppears to be a previous issue you have addressed before https://github.com/haotian-liu/LLaVA/issues/661#issuecomment-1779692868\r\n\r\nI tried `pip install \"tokenizers>=0.12.1,<0.14\"` and this installed `0.13.3`, but this gives another error when restarting the finetuning\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/attnmamba/llava/train/train_mem.py\", line 7, in <module>\r\n    from llava.train.llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn\r\n  File \"/home/attnmamba/llava/__init__.py\", line 1, in <module>\r\n    from .model import LlavaLlamaForCausalLM\r\nImportError: cannot import name 'LlavaLlamaForCausalLM' from 'llava.model' (/home/attnmamba/llava/model/__init__.py)\r\n```\n</Comment>\n<Comment by haotian-liu at 2024-02-01T06:26:20Z>\n@attnmamba \r\n\r\nWould you mind sharing your command? I cannot reproduce the issue on my side. Thanks.\n</Comment>\n<Comment by attnmamba at 2024-02-01T06:42:49Z>\n@haotian-liu Of course, here is my bash script\r\n\r\n```bash\r\n#!/bin/bash\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --bits 4 \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --model_name_or_path liuhaotian/llava-v1.5-13b \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/my_instruct_82k.json \\\r\n    --image_folder ./playground/data/my_images \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-13b-task-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\nPlease let me know if theres more info I can provide\n</Comment>\n<Comment by attnmamba at 2024-02-01T07:14:16Z>\n@haotian-liu Sorry false alarm, had some bad training examples in my dataset. No more tokenization mismatch warnings once those were removed.\r\n\r\nThank you very much for your help!\n</Comment>\n<Comment by haotian-liu at 2024-02-02T08:26:41Z>\nNice, closing this issue :)\n</Comment>\n<Comment by ChenRan2000 at 2024-04-09T09:11:09Z>\n> ### Describe the issue\r\n> transformers no longer has SharedDDPOption after v4\r\n\r\n> ### Describe the issue\r\n> transformers no longer has SharedDDPOption after v4.35.0\r\n\r\nHow to fix it, please tell me 0.0\n</Comment>\n<Comment by Liu0329 at 2024-04-12T03:32:43Z>\n> > ### Describe the issue\r\n> > transformers no longer has SharedDDPOption after v4\r\n> \r\n> > ### Describe the issue\r\n> > transformers no longer has SharedDDPOption after v4.35.0\r\n> \r\n> How to fix it, please tell me 0.0\r\n\r\nJust disable import ShardedDDPOption, which is not used.\n</Comment>\n<Comment by nishitanand at 2024-09-02T02:30:26Z>\nStill Facing this issue. Details - \r\n\r\nI am getting the following error, but this error should not be there -\r\ncannot import name 'ShardedDDPOption' from 'transformers.trainer'\r\n\r\nI have the following versions installed -\r\ntokenizers-0.19.1\r\ntransformers-4.43.4\r\nhuggingface-hub-0.24.6\r\n\r\nI have upgraded Vicuna -7v-v1.5 to llama 3.1 8B in this github repo - https://github.com/baaivision/EVE\r\n\r\nThis works with the vicuna-7b-v1.5, but not with llama3.1 8B. It should work as there isn't much change. I earlier got rope error, but solved it by upgrading transformers as guided in this issue -\r\nhttps://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/discussions/15\r\n\r\nI run bash eve7b_prealign.sh 0 localhost. This worked and I was able to train vicuna-7b-v1.5, but facing error with llama 3.1 8b\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1041,
    "state": "open",
    "created_by": "lucasjinreal",
    "created_at": "2024-02-01T02:57:39Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1041</URL>\n\n<TITLE>Quantization instructions</TITLE>\n\n<BODY>Hello, increasingly, LLMs are offering INT4 quantization outcomes, enabling more users to run these models even without a high-end GPU. May I inquire if there are plans to introduce INT4 quantization support for this exceptional MLLM? I genuinely look forward to it, and I strongly believe that numerous users would also benefit greatly from this feature.</BODY>\n\n<COMMENTS>\n<Comment by JoeySalmons at 2024-02-01T04:24:05Z>\nThis section of the Readme gives some information about how to load the model in 4bit or 8bit quantized format:\r\nhttps://github.com/haotian-liu/LLaVA?tab=readme-ov-file#launch-a-model-worker-4-bit-8-bit-inference-quantized\r\nIt should be as simple as appending \" --load-4bit\" to the command line arguments for any of the examples provided that use the command line.\r\n\r\nFor other uses in the readme, such as under [Quick Start With HuggingFace](https://github.com/haotian-liu/LLaVA?tab=readme-ov-file#quick-start-with-huggingface), you will need to edit the python files such that they load the models in the specified quantization in the same way as cli.py does.\r\n\r\nThis is from cli.py which uses the command line arguments to specify the quantization to load in:\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n\r\nThis is from run_llava.py which currently **does not** use the command line arguments to specify the quantization to load in:\r\nmodel_name = get_model_name_from_path(args.model_path)\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    args.model_path, args.model_base, model_name\r\n)\r\n\r\nLoading in 4bit, for example, should be as easy as changing the line in run_llava.py to:\r\nmodel_name = get_model_name_from_path(args.model_path)\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    args.model_path, args.model_base, model_name, False, True\r\n)\n</Comment>\n<Comment by lucasjinreal at 2024-02-01T06:42:25Z>\nAWQ enables model save when it quantized, so that users only need to download the int4 weights. By saying no need to perserve hight-end GPU normally means we can't load such a gaint model for quantize at all. In otherwords, can a int4 model be provided or at least ask @TheBloker to do this?\n</Comment>\n<Comment by BBC-Esq at 2024-02-02T00:51:03Z>\nHas anyone been able to verify that bitsandbytes, for example, better transformer, flash attention 2, will work with the new version 1.6 llava models?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1040,
    "state": "open",
    "created_by": "fozziethebeat",
    "created_at": "2024-02-01T01:42:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1040</URL>\n\n<TITLE>[Feature request] Prepare HF compatible image processor</TITLE>\n\n<BODY>### feature\n\nI got the v1.6 Vicuna 7b model working with SGLang after converting the image processor.  I have it up on my HF repo as [SurfaceData/llava-v1.6-vicuna-7b-processor](https://huggingface.co/SurfaceData/llava-v1.6-vicuna-7b-processor).  Perhaps this should be moved to one of the more maintained llava hf repos?\r\n\r\nI generated this by running two scripts:\r\n\r\n```py\r\nimport torch\r\nfrom llava.model.language_model.llava_llama import LlavaLlamaForCausalLM\r\n\r\n# load model\r\nkwargs = {\"device_map\": \"auto\", \"torch_dtype\": torch.float16}\r\nmodel = LlavaLlamaForCausalLM.from_pretrained(\r\n    \"liuhaotian/llava-v1.6-vicuna-7b\", low_cpu_mem_usage=True, **kwargs\r\n)\r\n\r\n# load vision tower\r\nmodel.get_vision_tower().load_model()\r\n\r\n# Save state dict\r\ntorch.save(model.state_dict(), \"lava-v1.6-vicuna-7b/model_state_dict.bin\")\r\n```\r\n\r\nThen\r\n\r\n```\r\nimport argparse\r\n\r\nimport torch\r\nfrom huggingface_hub import hf_hub_download\r\n\r\nfrom transformers import (\r\n    AddedToken,\r\n    AutoConfig,\r\n    AutoTokenizer,\r\n    CLIPImageProcessor,\r\n    LlavaConfig,\r\n    LlavaForConditionalGeneration,\r\n    LlavaProcessor,\r\n)\r\n\r\n\r\nKEYS_TO_MODIFY_MAPPING = {\r\n    \"model.vision_tower.\": \"\",\r\n    \"model.mm_projector\": \"multi_modal_projector\",\r\n    \"model\": \"model.model\",\r\n    \"vision_model.model\": \"vision_model\",\r\n    \"lm_head\": \"language_model.lm_head\",\r\n    \"model.model\": \"language_model.model\",\r\n    \"multi_modal_projector.0\": \"multi_modal_projector.linear_1\",\r\n    \"multi_modal_projector.2\": \"multi_modal_projector.linear_2\",\r\n}\r\n\r\n\r\ndef convert_state_dict_to_hf(state_dict):\r\n    new_state_dict = {}\r\n    for key, value in state_dict.items():\r\n        if key.endswith(\".inv_freq\"):\r\n            continue\r\n        for key_to_modify, new_key in KEYS_TO_MODIFY_MAPPING.items():\r\n            if key_to_modify in key:\r\n                key = key.replace(key_to_modify, new_key)\r\n\r\n        new_state_dict[key] = value\r\n    return new_state_dict\r\n\r\ndef convert_llava_llama_to_hf(\r\n    text_model_id, vision_model_id, output_hub_path, old_state_dict_id\r\n):\r\n    torch.set_default_dtype(torch.float16)\r\n    text_config = AutoConfig.from_pretrained(text_model_id)\r\n    tokenizer = AutoTokenizer.from_pretrained(text_model_id)\r\n    tokenizer.add_tokens(AddedToken(\"<image>\", normalized=False), special_tokens=True)\r\n    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\r\n    image_processor = CLIPImageProcessor.from_pretrained(vision_model_id)\r\n    processor = LlavaProcessor(tokenizer=tokenizer, image_processor=image_processor)\r\n    processor.push_to_hub(output_hub_path)\r\n\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser(\r\n        epilog=EPILOG_TXT,\r\n        formatter_class=argparse.RawDescriptionHelpFormatter,\r\n    )\r\n    parser.add_argument(\r\n        \"--text_model_id\",\r\n        help=\"Hub location of the text model\",\r\n    )\r\n    parser.add_argument(\r\n        \"--vision_model_id\",\r\n        help=\"Hub location of the vision model\",\r\n    )\r\n    parser.add_argument(\r\n        \"--output_hub_path\",\r\n        help=\"Location on the hub of the converted model\",\r\n    )\r\n    parser.add_argument(\r\n        \"--old_state_dict_id\",\r\n        help=\"Location on the hub of the raw state dict of the original model. The filename needs to be `model_state_dict.bin`\",\r\n    )\r\n    args = parser.parse_args()\r\n    convert_llava_llama_to_hf(\r\n        args.text_model_id,\r\n        args.vision_model_id,\r\n        args.output_hub_path,\r\n        args.old_state_dict_id,\r\n    )\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nI had to fork this from the original transformers script and drop some stuff since full model conversion needs some tweaking (it also doesn't really matter for SGLang support).</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1039,
    "state": "open",
    "created_by": "levi",
    "created_at": "2024-01-31T23:37:28Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1039</URL>\n\n<TITLE>[Usage] Unable to load LLaVA v1.6 models</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\n\r\nWhen trying to load `liuhaotian/llava-v1.6-mistral-7b` or `liuhaotian/llava-v1.6-34b` into my container:\r\n\r\n```\r\nMODEL_PATH = \"liuhaotian/llava-v1.6-mistral-7b\"\r\nUSE_8BIT = False\r\nUSE_4BIT = False\r\nDEVICE = \"cuda\"\r\n\r\ndef download_llava_model():\r\n    from llava.model.builder import load_pretrained_model\r\n    from llava.mm_utils import get_model_name_from_path\r\n\r\n    model_name = get_model_name_from_path(MODEL_PATH)\r\n    load_pretrained_model(\r\n        MODEL_PATH, None, model_name, USE_8BIT, USE_4BIT, device=DEVICE\r\n    )\r\n```\r\n\r\n\r\nSeeing this error:\r\n\r\n```\r\n  File \"/scripts/llava.py\", line 23, in download_llava_model\r\n    load_pretrained_model(\r\n  File \"/root/llava/llava/model/builder.py\", line 151, in load_pretrained_model\r\n    vision_tower.to(device=device, dtype=torch.float16)\r\n  File \"/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1145, in to\r\n    return self._apply(convert)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 797, in _apply\r\n    module._apply(fn)\r\n  File \"/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 797, in _apply\r\n    module._apply(fn)\r\n  File \"/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 797, in _apply\r\n    module._apply(fn)\r\n  [Previous line repeated 4 more times]\r\n  File \"/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 820, in _apply\r\n    param_applied = fn(param)\r\n                    ^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1143, in convert\r\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nNotImplementedError: Cannot copy out of meta tensor; no data!\r\n```</BODY>\n\n<COMMENTS>\n<Comment by Gutianpei at 2024-01-31T23:47:32Z>\nSame error. I think they have not updated the code for v1.6.\n</Comment>\n<Comment by levi at 2024-02-01T00:04:29Z>\nFor a brief moment I got it to build, but same error after reproing.\n</Comment>\n<Comment by Gutianpei at 2024-02-01T00:06:57Z>\n> For a brief moment I got it to build, but same error after reproing.\r\n\r\nI made it working by updating pytorch/transformers to latest version and following this issue #1036\n</Comment>\n<Comment by levi at 2024-02-01T01:02:05Z>\nTried various pytorch versions, no luck. Installing from an empty container image and pip installing the repo like described in the readme.\n</Comment>\n<Comment by ninatu at 2024-02-01T17:47:22Z>\nI have the same problem :(\n</Comment>\n<Comment by levi at 2024-02-01T17:54:16Z>\nBumping vram to 80GB resolved the issue for me. Possibly an OOM error in disguise?\n</Comment>\n<Comment by ninatu at 2024-02-02T10:15:43Z>\n@levi thanks! That helped!\n</Comment>\n<Comment by Pirog17000 at 2024-02-02T19:41:27Z>\n```\r\n  File \"A:\\Utilities\\DescriptingLLaVa\\LLaVA\\BatchCaptionFolder.py\", line 35, in <module>\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(\r\n  File \"A:\\Utilities\\DescriptingLLaVa\\LLaVA\\llava\\model\\builder.py\", line 108, in load_pretrained_model\r\n    model = LlavaMistralForCausalLM.from_pretrained(\r\nNameError: name 'LlavaMistralForCausalLM' is not defined\r\n```\r\n\r\nI have this error, also launching 1.6 mistral7b and have no luck making it work\r\nHappens during the model load and initialization here:\r\n\r\n```\r\nfrom llava.model.builder import load_pretrained_model\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    model_path, model_base, model_name, load_8bit=False, load_4bit=False,device_map='cuda:0', device='cuda:0')\r\n```\n</Comment>\n<Comment by Pirog17000 at 2024-02-03T01:51:26Z>\nNoticed a typo?\r\n![image](https://github.com/haotian-liu/LLaVA/assets/3022852/f08c6e0d-4b87-4f5c-b184-91f16513c8ef)\n</Comment>\n<Comment by Pirog17000 at 2024-02-03T01:58:35Z>\nIf I comment out broken references in builder.py and replace it with direct imports as follow:\r\n```\r\n#from llava.model import *\r\nfrom llava.model.language_model.llava_llama import LlavaLlamaForCausalLM\r\nfrom llava.model.language_model.llava_mpt import LlavaMptForCausalLM as LlavaMPTForCausalLM\r\nfrom llava.model.language_model.llava_mistral import LlavaMistralForCausalLM\r\n```\r\nthen I got another issue:\r\n\r\n```\r\n\r\n  File \"A:\\Utilities\\DescriptingLLaVa\\LLaVA\\llava\\model\\builder.py\", line 23, in <module>\r\n    from llava.model.language_model.llava_llama import LlavaLlamaForCausalLM\r\n  File \"A:\\Utilities\\DescriptingLLaVa\\LLaVA\\llava\\model\\language_model\\llava_llama.py\", line 21, in <module>\r\n    from transformers import AutoConfig, AutoModelForCausalLM, \\\r\n  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\r\n  File \"C:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1373, in __getattr__\r\n    value = getattr(module, name)\r\n  File \"C:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1372, in __getattr__\r\n    module = self._get_module(self._class_to_module[name])\r\n  File \"C:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1384, in _get_module\r\n    raise RuntimeError(\r\nRuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\r\nFailed to import transformers.integrations.peft because of the following error (look up to see its traceback):\r\nDLL load failed while importing libtriton: The specified module could not be found.\r\n```\n</Comment>\n<Comment by rossgreer at 2024-02-03T02:29:32Z>\n> Bumping vram to 80GB resolved the issue for me. Possibly an OOM error in disguise?\r\n\r\nHow does one \"bump vram to 80GB\"?\n</Comment>\n<Comment by haotian-liu at 2024-02-03T05:38:42Z>\nYou can inference with 4-bit quantization, which would fit the largest 34B variant in a 24GB GPU.\n</Comment>\n<Comment by matankley at 2024-02-06T09:56:05Z>\n@haotian-liu I try to run the 4-bit 34B on 24GB Ram but I'm pretty sure it offloads some of the weights to cpu, because of `low_cpu_mem_usage=True` which results in the above error `NotImplementedError: Cannot copy out of meta tensor; no data!`\n</Comment>\n<Comment by haotian-liu at 2024-02-06T16:07:56Z>\n@matankley \r\n\r\nThis is a demo loaded with 4-bit quantization on A10G (24G). Please check out the latest code base and retry, and if it does not work, please kindly share the commands you're using. THank you.\n</Comment>\n<Comment by tonywang10101 at 2024-03-13T23:33:49Z>\nFor me, `vision_tower.is_loaded()` wasn't functioning as anticipated. Manually executing `vision_tower.load_model()` resolved the issue.\n</Comment>\n<Comment by hkfisherman at 2024-06-28T02:21:14Z>\n@haotian-liu Thanks for your great research!\r\nMay I know, the download time too slow for me at here, if i want to save the model (15 .safetensors) to Google Drive -\r\n![image](https://github.com/haotian-liu/LLaVA/assets/67825782/c9ad4351-7f1f-4341-8419-e57413a2b470)\r\n\r\nHowever, in google drive I am getting just 8 of 8 only (20GB), which has is half of the loaded model. Is that other save method for your `LlavaLlamaForCausalLM` ?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1037,
    "state": "open",
    "created_by": "yash0307",
    "created_at": "2024-01-31T21:42:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1037</URL>\n\n<TITLE>[Question] Which version of LAION is used?</TITLE>\n\n<BODY>### Question\r\n\r\nHi, \r\n\r\nWhich version of LAION is used for pre-training of the MLP/projection layers? Is it 400M, 2B or 5B?\r\n\r\nThank you.\r\n\r\nRegards,\r\nYash Patel.</BODY>\n\n<COMMENTS>\n<Comment by qazasdwsx at 2024-02-02T02:15:28Z>\nsame question,I tried CLIP-ViT-L-14-laion2B-s32B-b82K & CLIP-ViT-L-14-DataComp.XL-s13B-b90K,not well\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1036,
    "state": "closed",
    "created_by": "huwprosser",
    "created_at": "2024-01-31T13:09:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1036</URL>\n\n<TITLE>llava_arch.py image_sizes missing. (Mistral LLava 1.6)[Usage]</TITLE>\n\n<BODY>Issue:\r\nWhen running the following command with the 1.6 mistral model I get an image_size error. I DID manage to get it working by manually setting the image size I'm inputting inside ```llava_arch.py```\r\n\r\n\r\nCommand:\r\n```python -m llava.serve.cli \\\r\n    --model-path liuhaotian/llava-v1.6-mistral-7b \\\r\n    --image-file \"image.png\" --device mps\r\n```\r\n\r\nLog: \r\n```\r\n/LLaVA/llava/model/llava_arch.py\", line 173, in prepare_inputs_labels_for_multimodal num_patch_width, num_patch_height = get_anyres_image_grid_shape(image_sizes[image_idx], self.config.image_grid_pinpoints, self.get_vision_tower().config.image_size) TypeError: 'NoneType' object is not subscriptable\r\n```</BODY>\n\n<COMMENTS>\n<Comment by storuky at 2024-01-31T14:59:08Z>\nsame here\n</Comment>\n<Comment by heltrix at 2024-01-31T21:04:26Z>\nSame here, looks like the call to model.generate in cli.py is not passing the image_sizes variable. Adding the line:\r\n    imagesize=image.size \r\nbelow the load_image() call, and the line:\r\n    image_sizes = [imagesize],\r\nto the model.generate() call fixed it for me. \r\n\r\nhttps://github.com/haotian-liu/LLaVA/pull/1038\n</Comment>\n<Comment by JoeySalmons at 2024-02-01T05:11:28Z>\n> Same here, looks like the call to model.generate in cli.py is not passing the image_sizes variable. Adding the line: imagesize=image.size below the load_image() call, and the line: image_sizes = [imagesize], to the model.generate() call fixed it for me.\r\n> \r\n> #1038\r\n\r\nThis seems to also apply to other files, such as run_llava.py, except it needs to work with one or more images. Something like this should work for run_llava.py (I have tested this with a single image but not multiple images.)\r\n\r\ndef load_images(image_files):\r\n    out = []\r\n    sizes = []\r\n    for image_file in image_files:\r\n        image = load_image(image_file)\r\n        out.append(image)\r\n        sizes.append(image.size)\r\n    return out, sizes\r\n\r\nmodify the function call:\r\nimages, imagesizes = load_images(image_files)\r\n\r\nand add a line to the model.generate call:\r\nimage_sizes=imagesizes,\n</Comment>\n<Comment by Silviase at 2024-02-02T06:43:54Z>\nI have encountered the same error, and this solution worked for me, thank you 👍\n</Comment>\n<Comment by huwprosser at 2024-02-02T11:04:06Z>\n> Same here, looks like the call to model.generate in cli.py is not passing the image_sizes variable. Adding the line: imagesize=image.size below the load_image() call, and the line: image_sizes = [imagesize], to the model.generate() call fixed it for me.\r\n> \r\n> #1038\r\n\r\nThis worked for me, thank you everyone! 🚀\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1035,
    "state": "open",
    "created_by": "nivibilla",
    "created_at": "2024-01-31T09:53:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1035</URL>\n\n<TITLE>Cogvlm reference in blog.</TITLE>\n\n<BODY>Hi,\n\nJust wanted to check. Isnt the cogvlm model actually 17b params. Not 30?\n\nThanks</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1034,
    "state": "closed",
    "created_by": "hongdangshao",
    "created_at": "2024-01-31T08:51:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1034</URL>\n\n<TITLE>when i run finetune_lora.sh，it get the error message,how can i solve it?</TITLE>\n\n<BODY>### Question\n\nTraceback (most recent call last):\r\n  File \"/home/LLaVA/llava/train/train_mem.py\", line 6, in <module>\r\n    from llava.train.llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn\r\n  File \"/home/LLaVA/llava/__init__.py\", line 1, in <module>\r\n    from .model import LlavaLlamaForCausalLM\r\nImportError: cannot import name 'LlavaLlamaForCausalLM' from 'llava.model' (/home/LLaVA/llava/model/__init__.py)</BODY>\n\n<COMMENTS>\n<Comment by betterftr at 2024-01-31T23:38:22Z>\nsame\n</Comment>\n<Comment by hongdangshao at 2024-01-31T23:38:45Z>\n您的邮件已收到！请放心！\n</Comment>\n<Comment by betterftr at 2024-01-31T23:42:56Z>\nreinstalled pip install --upgrade charset_normalizer and it seems resolved... Windows11\n</Comment>\n<Comment by zhangboshen at 2024-02-04T08:20:56Z>\nsame\n</Comment>\n<Comment by haotian-liu at 2024-02-05T18:45:08Z>\nWe find that this is due to `flash-attn` compiled previously with a different version of pytorch. Please reinstall that with:\r\n\r\n```\r\npip install flash-attn --no-build-isolation --no-cache-dir\r\n```\n</Comment>\n<Comment by rkuo2000 at 2024-03-03T13:56:07Z>\nI had the same issue, resolved after `pip install flash-attn --no-build-isolation --no-cache-dir`\n</Comment>\n<Comment by hongdangshao at 2024-03-03T13:56:28Z>\nI have got it!Thanks's.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1033,
    "state": "closed",
    "created_by": "TobiasJu",
    "created_at": "2024-01-30T21:23:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1033</URL>\n\n<TITLE>[Usage] Could not load library libcudnn_cnn_infer.so.8. Error: libcuda.so: cannot open shared object file: No such file or directory Aborted</TITLE>\n\n<BODY>### Describe the issue\n\n**Issue:**\r\nI can see the gradio web interface but as soon as i use the example prompt, the model controller crashes. I use WSL 2 and every step works, I just can not get any response after i enter a prompt, all I see in gradio is: \"NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.\"\r\n\r\n**System:**\r\nWin 10 WSL 2 (Ubuntu)\r\npip 23.3.2\r\npython 3.10.12\r\nRTX3090\r\n64GB RAM\r\nRyzen 9 3950X\r\n\r\n**Command:**\r\n```\r\npython3 -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b\r\n```\r\n\r\n**Log:** \r\n```\r\n2024-01-30 22:11:49 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='liuhaotian/llava-v1.5-13b', model_base=None, model_name=None, device='cuda', multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=False)\r\n2024-01-30 22:11:49 | INFO | model_worker | Loading the model llava-v1.5-13b on worker c5ced3 ...\r\nLoading checkpoint shards:   0%|                                                                                                                      | 0/3 [00:00<?, ?it/s]\r\nLoading checkpoint shards:  33%|████████████████████████████████████▋                                                                         | 1/3 [00:06<00:12,  6.42s/it]\r\nLoading checkpoint shards:  67%|█████████████████████████████████████████████████████████████████████████▎                                    | 2/3 [00:12<00:06,  6.47s/it]\r\nLoading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:16<00:00,  5.07s/it]\r\nLoading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:16<00:00,  5.45s/it]\r\n2024-01-30 22:12:07 | ERROR | stderr |\r\n2024-01-30 22:12:11 | INFO | model_worker | Register to controller\r\n2024-01-30 22:12:11 | ERROR | stderr | INFO:     Started server process [1348]\r\n2024-01-30 22:12:11 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2024-01-30 22:12:11 | ERROR | stderr | INFO:     Application startup complete.\r\n2024-01-30 22:12:11 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)\r\n2024-01-30 22:12:18 | INFO | stdout | INFO:     127.0.0.1:59030 - \"POST /worker_get_status HTTP/1.1\" 200 OK\r\n2024-01-30 22:12:22 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n2024-01-30 22:12:22 | INFO | stdout | INFO:     127.0.0.1:40950 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\nCould not load library libcudnn_cnn_infer.so.8. Error: libcuda.so: cannot open shared object file: No such file or directory\r\nAborted\r\n```\r\n\r\n**Screenshots:**\r\n![image](https://github.com/haotian-liu/LLaVA/assets/32335067/731d5d41-cd17-455a-82fd-637f4f50fecc)</BODY>\n\n<COMMENTS>\n<Comment by TobiasJu at 2024-01-30T21:44:31Z>\nSo i followed this comment: [Conda Pytorch (Pytorch channel) in WSL2 Ubuntu can't find libcudnn shared objects](https://github.com/pytorch/pytorch/issues/85773#top)](https://github.com/pytorch/pytorch/issues/85773#issuecomment-1288033297)\r\n\r\nAnd now i get this error and still no model output:\r\n```\r\n2024-01-30 22:43:08 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: Semaphore(value=4, locked=False). global_counter: 3\r\n2024-01-30 22:43:08 | INFO | stdout | INFO:     127.0.0.1:50792 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\n2024-01-30 22:43:15 | ERROR | stderr | Exception in thread Thread-4 (generate):\r\n2024-01-30 22:43:15 | ERROR | stderr | Traceback (most recent call last):\r\n2024-01-30 22:43:15 | ERROR | stderr |   File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\r\n2024-01-30 22:43:15 | ERROR | stderr |     self.run()\r\n2024-01-30 22:43:15 | ERROR | stderr |   File \"/usr/lib/python3.10/threading.py\", line 953, in run\r\n2024-01-30 22:43:15 | ERROR | stderr |     self._target(*self._args, **self._kwargs)\r\n2024-01-30 22:43:15 | ERROR | stderr |   File \"/home/tobias/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n2024-01-30 22:43:15 | ERROR | stderr |     return func(*args, **kwargs)\r\n2024-01-30 22:43:15 | ERROR | stderr |   File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1588, in generate\r\n2024-01-30 22:43:15 | ERROR | stderr |     return self.sample(\r\n2024-01-30 22:43:15 | ERROR | stderr |   File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2642, in sample\r\n2024-01-30 22:43:15 | ERROR | stderr |     outputs = self(\r\n2024-01-30 22:43:15 | ERROR | stderr |   File \"/home/tobias/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2024-01-30 22:43:15 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2024-01-30 22:43:15 | ERROR | stderr |   File \"/home/tobias/.local/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2024-01-30 22:43:15 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2024-01-30 22:43:15 | ERROR | stderr |   File \"/mnt/a/KI/LLaVA/llava/model/language_model/llava_llama.py\", line 88, in forward\r\n2024-01-30 22:43:15 | ERROR | stderr |     return super().forward(\r\n2024-01-30 22:43:15 | ERROR | stderr |   File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 806, in forward\r\n2024-01-30 22:43:15 | ERROR | stderr |     outputs = self.model(\r\n2024-01-30 22:43:15 | ERROR | stderr |   File \"/home/tobias/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2024-01-30 22:43:15 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2024-01-30 22:43:15 | ERROR | stderr |   File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 693, in forward\r\n2024-01-30 22:43:15 | ERROR | stderr |     layer_outputs = decoder_layer(\r\n2024-01-30 22:43:15 | ERROR | stderr |   File \"/home/tobias/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2024-01-30 22:43:15 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2024-01-30 22:43:15 | ERROR | stderr |   File \"/home/tobias/.local/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2024-01-30 22:43:15 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2024-01-30 22:43:15 | ERROR | stderr |   File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 420, in forward\r\n2024-01-30 22:43:15 | ERROR | stderr |     hidden_states = self.post_attention_layernorm(hidden_states)\r\n2024-01-30 22:43:15 | ERROR | stderr |   File \"/home/tobias/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2024-01-30 22:43:15 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2024-01-30 22:43:15 | ERROR | stderr |   File \"/home/tobias/.local/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2024-01-30 22:43:15 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2024-01-30 22:43:15 | ERROR | stderr |   File \"/home/tobias/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 89, in forward\r\n2024-01-30 22:43:15 | ERROR | stderr |     return self.weight * hidden_states.to(input_dtype)\r\n2024-01-30 22:43:15 | ERROR | stderr |   File \"/home/tobias/.local/lib/python3.10/site-packages/torch/_prims_common/wrappers.py\", line 220, in _fn\r\n2024-01-30 22:43:15 | ERROR | stderr |     result = fn(*args, **kwargs)\r\n2024-01-30 22:43:15 | ERROR | stderr |   File \"/home/tobias/.local/lib/python3.10/site-packages/torch/_prims_common/wrappers.py\", line 130, in _fn\r\n2024-01-30 22:43:15 | ERROR | stderr |     result = fn(**bound.arguments)\r\n2024-01-30 22:43:15 | ERROR | stderr |   File \"/home/tobias/.local/lib/python3.10/site-packages/torch/_refs/__init__.py\", line 926, in _ref\r\n2024-01-30 22:43:15 | ERROR | stderr |     return prim(a, b)\r\n2024-01-30 22:43:15 | ERROR | stderr |   File \"/home/tobias/.local/lib/python3.10/site-packages/torch/_refs/__init__.py\", line 1532, in mul\r\n2024-01-30 22:43:15 | ERROR | stderr |     return prims.mul(a, b)\r\n2024-01-30 22:43:15 | ERROR | stderr |   File \"/home/tobias/.local/lib/python3.10/site-packages/torch/_ops.py\", line 287, in __call__\r\n2024-01-30 22:43:15 | ERROR | stderr |     return self._op(*args, **kwargs or {})\r\n2024-01-30 22:43:15 | ERROR | stderr |   File \"/home/tobias/.local/lib/python3.10/site-packages/torch/_prims/__init__.py\", line 346, in _elementwise_meta\r\n2024-01-30 22:43:15 | ERROR | stderr |     utils.check_same_device(*args_, allow_cpu_scalar_tensors=True)\r\n2024-01-30 22:43:15 | ERROR | stderr |   File \"/home/tobias/.local/lib/python3.10/site-packages/torch/_prims_common/__init__.py\", line 596, in check_same_device\r\n2024-01-30 22:43:15 | ERROR | stderr |     raise RuntimeError(msg)\r\n2024-01-30 22:43:15 | ERROR | stderr | RuntimeError: Tensor on device cuda:0 is not on the expected device meta!\r\n```\n</Comment>\n<Comment by PMahern at 2024-01-31T18:07:56Z>\nDoing the symlink trick seemed to just make my system use the CPU for CUDA and the worker won't start if I turn on quantization.  Instead, I added\r\n\r\n`export LD_LIBRARY_PATH=/usr/lib/wsl/lib:$LD_LIBRARY_PATH` \r\n\r\nto my .bashrc in wsl and I'm able to get to the point where the GPU starts doing something when I submit a query in the web ui. Unfortunately after 5-10 seconds the gradio client still spits out the NETWORK ERROR output and the GPU goes back down to 0%. None of the programs crash.\r\n\r\nHere's a ticket on the WSL github with more details about the LD_LIBRARY_PATH fix: https://github.com/microsoft/WSL/issues/8587\r\n\r\nEdit: Turns out my other issues were likely VRAM related, works fine with a smaller model than the one I was trying.\n</Comment>\n<Comment by TobiasJu at 2024-01-31T18:40:32Z>\nThanks, the VRAM tipp helped! Running the smaller 7B model works on my machine. Looks like the RTX3090 24GB of VRAM is still too less for the 13B.\r\n`python3 -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-7b`\r\n\r\nHow much VRAM do i need for the 13B Model and how can i see/calculate the VRAM needed for a model to run?\n</Comment>\n<Comment by PMahern at 2024-01-31T21:15:48Z>\nYou can turn on quantization to reduce the VRAM needed (this will reduce the accuracy as well). I was testing it with 4bit quant but the 13b model might fit with 8bit on a 3090. The flags are `--load-4bit` and `--load-8bit`. Just add it to the call to start the worker.  [Here's the details on the readme](https://github.com/haotian-liu/LLaVA?tab=readme-ov-file#launch-a-model-worker-4-bit-8-bit-inference-quantized)\r\n\r\nI don't know how much VRAM the individual models need, but the 34b one was a bit too much for my 4090 at 4 bit.\n</Comment>\n<Comment by haotian-liu at 2024-02-03T05:49:49Z>\nThis is due to the VRAM OOM issue. We also recently add the option to enable flash attention for inference which further reduces the memory usage.\r\n\r\n```\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.6-34b --load-4bit --use-flash-attn\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1031,
    "state": "open",
    "created_by": "y-vectorfield",
    "created_at": "2024-01-30T06:18:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1031</URL>\n\n<TITLE>[Usage] The lora models could not implement at the latest version of LLaVA</TITLE>\n\n<BODY>When I implemented the lora models(llava-v1.5-13b-lora, llava-v1.5-7b-lora), the following error was occurred.\r\n\r\n```bash\r\n/usr/local/lib/python3.10/dist-packages/llava/model/builder.py:48: UserWarning: There is `lora` in model name but no `model_base` is provided. If you are loading a LoRA model, please provide the `model_base` argument. Detailed instruction: https://github.com/haotian-liu/LLaVA#launch-a-model-worker-lora-weights-unmerged.\r\n  warnings.warn('There is `lora` in model name but no `model_base` is provided. If you are loading a LoRA model, please provide the `model_base` argument. Detailed instruction: https://github.com/haotian-liu/LLaVA#launch-a-model-worker-lora-weights-unmerged.')\r\nconfig.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.02k/1.02k [00:00<00:00, 4.63MB/s]\r\nTraceback (most recent call last):\r\n  File \"/lustre/groups/share/yvectorfield/ai-models/llava/common/eval_llava.py\", line 106, in <module>\r\n    main(args)\r\n  File \"/lustre/groups/share/yvectorfield/ai-models/llava/common/eval_llava.py\", line 87, in main\r\n    eval_model(model_args)\r\n  File \"/usr/local/lib/python3.10/dist-packages/llava/eval/run_llava.py\", line 56, in eval_model\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(\r\n  File \"/usr/local/lib/python3.10/dist-packages/llava/model/builder.py\", line 105, in load_pretrained_model\r\n    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\", line 718, in from_pretrained\r\n    tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 663, in __getitem__\r\n    model_type = self._reverse_config_mapping[key.__name__]\r\nKeyError: 'LlavaConfig'\r\n```</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1030,
    "state": "open",
    "created_by": "rorubyy",
    "created_at": "2024-01-30T05:08:39Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1030</URL>\n\n<TITLE>[Question]  Fine-Tuning LLaVA v1.5-7B lora on Custom Dataset and RuntimeError in Model Evaluation</TITLE>\n\n<BODY>### Question\n\nHello LLaVA Team,\r\n\r\nI've been working on fine-tuning the LLaVA v1.5-7B model on a custom dataset using the provided `finetune_task_lora.sh` script. Here is the configuration I used:\r\n`bash scripts/v1_5/finetune_task_lora.sh`\r\n\r\n```deepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3_offload.json \\\r\n    --model_name_or_path liuhaotian/llava-v1.5-7b \\\r\n    --version v1 \\\r\n    --data_path /workspace/Dataset/train.json \\\r\n    --image_folder ./ \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b-task-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\nAfter training, these were the results:\r\n`{'train_runtime': 25078.2556, 'train_samples_per_second': 1.595, 'train_steps_per_second': 0.1, 'train_loss': 0.16062020410320182, 'epoch': 1.0}`\r\n\r\nWhen attempting to evaluate the model using `model_vqa.py`, I encountered a runtime error. The model loads correctly, but during the evaluation, I receive a **RuntimeError: probability tensor contains either 'inf', 'nan' or element < 0.** \r\n`python llava/eval/model_vqa.py --model-path checkpoints/llava-v1.5-7b-task-lora/ --model-base checkpoints/llava-v1.5-7b/ --question-file Dataset/eval_ques\r\n.jsonl --image-folder ./ --answers-file /workspace/Dataset/eval_answer.jsonl`\r\nHere's the traceback:\r\n```\r\nLoading LLaVA from base model...\r\nLoading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:06<00:00,  3.14s/it]\r\nLoading additional LLaVA weights...\r\nLoading LoRA weights...\r\nMerging LoRA weights...\r\nModel is loaded...\r\n  0%|                                                                                                                                                                  | 0/2108 [00:00<?, ?it/s]\r\nTraceback (most recent call last):\r\n  File \"/workspace/llava/eval/model_vqa.py\", line 125, in <module>\r\n    eval_model(args)\r\n  File \"/workspace/llava/eval/model_vqa.py\", line 66, in eval_model\r\n    output_ids = model.generate(\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1588, in generate\r\n    return self.sample(\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2678, in sample\r\n    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n```\r\n\r\nIt seems that the model's hidden state outputs are all nan. \r\n\r\n```\r\nBaseModelOutputWithPast(last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\r\n         [nan, nan, nan,  ..., nan, nan, nan],\r\n         [nan, nan, nan,  ..., nan, nan, nan],\r\n         ...,\r\n         [nan, nan, nan,  ..., nan, nan, nan],\r\n         [nan, nan, nan,  ..., nan, nan, nan],\r\n         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\r\n       dtype=torch.float16), past_key_values=((tensor([[[[-1.7842,  0.6445,  0.9375,  ..., -1.8057,  1.9531, -2.0898],\r\n          [-0.0104, -0.5850,  0.0938,  ...,  0.6738, -0.7227,  0.6948],\r\n          [ 1.1318,  3.6055, -0.5405,  ..., -2.7910,  4.2617, -2.5684],\r\n          ...,\r\n          [ 0.8984,  3.0488,  0.4187,  ..., -2.5312,  3.5684, -2.3633],\r\n          [ 0.6392,  1.5225, -0.4216,  ..., -0.7524,  1.3262, -0.6392],\r\n          [-0.0755,  0.3811,  0.0687,  ...,  0.4375, -0.8335,  0.4001]],\r\n\r\n         [[-0.4683,  1.1523,  0.1193,  ..., -0.9561,  1.3672, -0.9609],\r\n          [ 0.0293,  0.4148, -0.1000,  ..., -0.3264, -0.1302, -0.1819],\r\n          [ 2.3184, -2.0352,  1.4316,  ..., -1.8232,  2.5410, -1.8711],\r\n          ...,\r\n          [ 1.9541, -1.8486, -1.5303,  ..., -1.3906,  2.0664, -1.4053],\r\n          [ 0.7222, -0.9507, -0.5615,  ..., -0.4160,  0.9175, -0.4568],\r\n          [ 0.4136, -1.2432,  0.7637,  ...,  1.0225, -0.6924,  1.0254]],\r\n\r\n         [[-0.9062, -2.0078, -0.9204,  ..., -0.7695, -0.4683, -0.3967],\r\n          [-0.0662,  0.4402,  0.1777,  ...,  1.8486,  1.9346,  1.8145],\r\n          [ 1.2881,  0.0416,  0.6675,  ..., -2.3965, -2.6621, -2.6934],\r\n          ...,\r\n          [ 0.1807, -1.1299, -0.6143,  ..., -2.4707, -2.8301, -2.9219],\r\n          [-1.0068,  0.0765, -0.5195,  ..., -1.5000, -1.7148, -1.7324],\r\n          [ 0.2966, -0.6782,  0.0466,  ...,  1.0801,  1.2441,  1.1377]],\r\n\r\n         ...,\r\n```\r\nCould you help me understand what might be causing this issue and how to resolve it? Thank you very much</BODY>\n\n<COMMENTS>\n<Comment by cherry956 at 2024-02-12T03:28:30Z>\n@rorubyy I have run scripts like you. But I encount new issues. \r\n![屏幕截图 2024-02-12 112205](https://github.com/haotian-liu/LLaVA/assets/144820412/be7302c1-c2a1-4cdb-95b6-50d47e736e06)\r\nHow I should fix the max model sequence len and fix the problem about uploading  many images? Here is my test-json:\r\n![屏幕截图 2024-02-12 112631](https://github.com/haotian-liu/LLaVA/assets/144820412/2e4062c5-ec70-42d3-bf57-aba2a03d1693)\n</Comment>\n<Comment by monjurulkarim at 2024-02-16T05:07:11Z>\n@rorubyy What size is your custom dataset? I'm curious about its performance with smaller datasets.\n</Comment>\n<Comment by chanangad at 2024-04-17T11:41:58Z>\nhi @rorubyy \r\n\r\nWere you able to figure out the reason for the hidden states being nan?\r\nI'm facing the same issue\n</Comment>\n<Comment by shreyanshu09 at 2024-04-26T04:32:23Z>\nHello @rorubyy @chanangad \r\n\r\nI am also facing the same issue. Does anyone have a solution or any ideas on how to fix it?\n</Comment>\n<Comment by wentaoyuan at 2024-05-24T05:43:45Z>\nI encountered the same issue while running `model_vqa.py` with a fine-tuned 7b model.\n</Comment>\n<Comment by ghazalsaheb at 2024-08-06T19:44:22Z>\nI used to have the same issue and I figured it was because I was using hugging face's \"llava-hf/llava-1.5-7b-hf\" as the base model. I switched the base to \"liuhaotian/llava-v1.5-7b\" and it resolved the NaN issue. Plus, the training performance got much better.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1029,
    "state": "open",
    "created_by": "hualala00",
    "created_at": "2024-01-30T03:38:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1029</URL>\n\n<TITLE>[Usage] how to use the checkpoints saved in the finutuning process to inference?</TITLE>\n\n<BODY>### Describe the issue\n\nwhen i finetune the model on my custom dataset,i want to use the checkpoints saved in the finutuning process to inference to check model effect,and how to inference with the checkponits?\r\nIf i only use the checkpoints to inference,there is a error as flowed：\r\n![B17759138BF76EB61259447E0BB83BDB](https://github.com/haotian-liu/LLaVA/assets/142398592/ced75ad0-43eb-4951-8f02-e5423bd8b192)\r\nDo i need to merge the weight or something else?</BODY>\n\n<COMMENTS>\n<Comment by LeoLee7 at 2024-01-30T03:52:24Z>\n#245 looks relevant to your question\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1028,
    "state": "open",
    "created_by": "gameveloster",
    "created_at": "2024-01-29T22:14:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1028</URL>\n\n<TITLE>[Usage] AttributeError: 'Parameter' object has no attribute 'quant_state'</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue: Trying to do inference on a LoRA adapter created from finetuning on ` liuhaotian/llava-v1.5-13b` base model.\r\n\r\nDue to an error, I have modified `llava/server/cli.py` so that\r\n\r\n```py\r\n        with torch.inference_mode():\r\n            output_ids = model.generate(\r\n                input_ids,\r\n```\r\n\r\nis changed to\r\n\r\n```py\r\n        with torch.inference_mode():\r\n            output_ids = model.generate(\r\n                input_ids=input_ids,\r\n```\r\n\r\nBut there is now a new error shown below.\r\n\r\nHere are my installed pip packages\r\n```\r\ntorch                     2.0.1\r\nbitsandbytes              0.41.0\r\nflash-attn                2.3.2\r\nllava                     1.1.3        /home/x/LLaVA\r\npeft                      0.4.0\r\ntransformers              4.31.0\r\n```\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.cli --model-path ./checkpoints/llava-v1.5-13b-lora-test --model-base ./base/llava-v1.5-13b --image-file ./data/testimg.jpg --load-4bit\r\n```\r\n\r\nLog: \r\n```\r\n[2024-01-29 22:03:11,676] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nLoading LLaVA from base model...\r\nLoading checkpoint shards: 100%|██████████████████████████████████| 3/3 [00:14<00:00,  4.73s/it]\r\nLoading additional LLaVA weights...\r\nLoading LoRA weights...\r\nModel is loaded...\r\nUSER: Foo bar baz\r\nASSISTANT: FP4 quantization state not initialized. Please call .cuda() or .to(device) on the LinearFP4 layer first.\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/x/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/x/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/x/temp/LLaVA/llava/serve/cli.py\", line 125, in <module>\r\n    main(args)\r\n  File \"/home/x/temp/LLaVA/llava/serve/cli.py\", line 94, in main\r\n    output_ids = model.generate(\r\n  File \"/home/x/anaconda3/envs/llava/lib/python3.10/site-packages/peft/peft_model.py\", line 977, in generate\r\n    outputs = self.base_model.generate(**kwargs)\r\n  File \"/home/x/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/x/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1588, in generate\r\n    return self.sample(\r\n  File \"/home/x/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2642, in sample\r\n    outputs = self(\r\n  File \"/home/x/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/x/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/x/temp/LLaVA/llava/model/language_model/llava_llama.py\", line 88, in forward\r\n    return super().forward(\r\n  File \"/home/x/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 824, in forward\r\n    logits = self.lm_head(hidden_states)\r\n  File \"/home/x/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/x/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/x/anaconda3/envs/llava/lib/python3.10/site-packages/bitsandbytes/nn/modules.py\", line 249, in forward\r\n    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)\r\nAttributeError: 'Parameter' object has no attribute 'quant_state'\r\n\r\n```\r\n\r\nIt might be `lm_head` raising this error. Strangely I checked a few layers and they are all `0`s.\r\n```py\r\n# model.base_model.model.lm_head.weight\r\ntorch.Size([32000, 5120])\r\n\r\n# (model.base_model.model.lm_head.weight == 0).all()\r\ntensor(True, device='cuda:0')\r\n\r\n# model.base_model.model.lm_head.weight.shape\r\nParameter containing:\r\ntensor([[0., 0., 0.,  ..., 0., 0., 0.],\r\n        [0., 0., 0.,  ..., 0., 0., 0.],\r\n        [0., 0., 0.,  ..., 0., 0., 0.],\r\n        ...,\r\n        [0., 0., 0.,  ..., 0., 0., 0.],\r\n        [0., 0., 0.,  ..., 0., 0., 0.],\r\n        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\r\n\r\n# getattr(model.base_model.model.lm_head.weight, 'quant_state')\r\nAttributeError: 'Parameter' object has no attribute 'quant_state'\r\n```\r\n\r\nI dont remember if the LoRA adapter was trained using the same pip packages as the inference env.\r\n\r\nAnyone knows a fix? Thanks</BODY>\n\n<COMMENTS>\n<Comment by HelloWorldLTY at 2024-03-06T08:22:38Z>\nSame quesion here.\n</Comment>\n<Comment by Adrian0999 at 2024-03-18T14:56:12Z>\nsame question here. Anyone has solved?\n</Comment>\n<Comment by Andcircle at 2024-05-20T23:02:29Z>\nAny updates here?\n</Comment>\n<Comment by PranjalSahu at 2024-12-04T15:28:08Z>\nI am also facing the same error. Did anyone solve it ?\n</Comment>\n<Comment by sgonzalezsilot at 2025-05-27T14:39:14Z>\nSame issue here\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1027,
    "state": "open",
    "created_by": "YangQiuEric",
    "created_at": "2024-01-29T20:31:10Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1027</URL>\n\n<TITLE>[Question] How to do a 2nd finetune on a fintuned checkpoints with lora?</TITLE>\n\n<BODY>### Question\n\nI have used train_mem.py with lora for the initial finetuning, similar to the question. Now, I want to perform another round of finetuning on a different dataset while starting from the checkpoint of the first finetune. How can I accomplish this?</BODY>\n\n<COMMENTS>\n<Comment by LeoLee7 at 2024-01-29T23:01:08Z>\nHere's what I tried: \r\n\r\n1. Go to ```/pvcvolume/LLaVA/llava/train/train.py```\r\n\r\n2. Find ``TrainingArguments`` class and add one more argument:\r\n`` lora_path: str = field(default=None, metadata={\"help\": \"Path to the previous lora folder.\"})``\r\n\r\n3. Go back to ```/pvcvolume/LLaVA/llava/train/train.py``` line 840, and add \r\n```\r\nif training_args.lora_path:\r\n   model = PeftModel.from_pretrained(model, training_args.lora_path)\r\nelse:\r\n   model = get_peft_model(model, lora_config) # Model is defined here!\r\n```\r\n\r\n4. Add the `lora_path` in your training `.sh` script.\n</Comment>\n<Comment by cherry956 at 2024-02-22T15:11:12Z>\n@LeoLee7 I have finetuned  first LLaVA-7b with my own dataset. I founld model is bad. So I want to perform another round of finetuning on another dataset while starting from the checkpoint of the first finetune.(same task)     Can I use your method to do this??? Thank!!!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1026,
    "state": "open",
    "created_by": "gameveloster",
    "created_at": "2024-01-29T05:28:27Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1026</URL>\n\n<TITLE>[Question] Reuse the MLP projection layer, or retrain it?</TITLE>\n\n<BODY>### Question\n\nI want to try changing `liuhaotian/llava-v1.5-13b` to use a different image tower instead of `clip-vit-large-patch14`.\r\n\r\n1. After changing the vision tower, is it necessary to pretrain the MLP projection layer from scratch, or can we reuse the pretrained projector weights in the `liuhaotian/llava-v1.5-13b` checkpoint?\r\n\r\n2. How can the vision tower be replaced?\r\n\r\n3. Similarly, if I want to use a different LLaMA-2 finetune model of the same parameter size instead of `lmsys/vicuna-13b-v1.5`, used in the `liuhaotian/llava-v1.5-13b` checkpoint, can you simply replace the LLM network without pretraining the MLP projection layer from scratch?\r\n\r\n4. How can the LLM be replaced?</BODY>\n\n<COMMENTS>\n<Comment by ggcr at 2024-06-04T19:08:24Z>\nI am interested in knowing if training the MLP projector alone would be suitable. Since recent work by Apple ([see paper](https://arxiv.org/abs/2403.09611v4)) suggest that it is one of the most important factors.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1025,
    "state": "open",
    "created_by": "attnmamba",
    "created_at": "2024-01-29T04:41:35Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1025</URL>\n\n<TITLE>[Question] How to set Wandb names?</TITLE>\n\n<BODY>### Question\n\nHow do you set the wandb names for the project and run?\r\n\r\nIt is currently defaulting to the project4 name `huggingface` and a random name is assigned for the run.</BODY>\n\n<COMMENTS>\n<Comment by LeoLee7 at 2024-01-29T23:08:42Z>\nI add the following scripts:\r\n\r\n`os.environ[\"WANDB_PROJECT\"]=\"YOUR_PROJECT_NAME\"` in `LLaVA/llava/train/train_mem.py`\r\n\r\nand `--run_name 'YOUR_RUN_NAME' \\` to training arguments\n</Comment>\n<Comment by HenryHZY at 2024-01-30T01:24:56Z>\n> I add the following scripts:\r\n> \r\n> `os.environ[\"WANDB_PROJECT\"]=\"YOUR_PROJECT_NAME\"` in `LLaVA/llava/train/train_mem.py`\r\n> \r\n> and `--run_name 'YOUR_RUN_NAME' \\` to training arguments\r\n\r\nsame with you\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1024,
    "state": "closed",
    "created_by": "jyC23333",
    "created_at": "2024-01-29T03:36:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1024</URL>\n\n<TITLE>[Usage] How can I change the language model into Qwen-7B?</TITLE>\n\n<BODY>### Describe the issue\n\nI want to change the LLM into Qwen，and I write a model file according to llava_llama.py:\r\n```python\r\n#    Copyright 2023 Haotian Liu\r\n#\r\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\r\n#    you may not use this file except in compliance with the License.\r\n#    You may obtain a copy of the License at\r\n#\r\n#        http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n#    Unless required by applicable law or agreed to in writing, software\r\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\r\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n#    See the License for the specific language governing permissions and\r\n#    limitations under the License.\r\n\r\n\r\nfrom typing import List, Optional, Tuple, Union\r\n\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nfrom transformers import AutoConfig, AutoModelForCausalLM, \\\r\n                         LlamaConfig, LlamaModel, LlamaForCausalLM\r\n\r\nfrom .qwen.configuration_qwen import QWenConfig\r\n\r\n\r\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\r\n\r\nfrom ..llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\r\n\r\n\r\nclass LlavaQwenConfig(QWenConfig):\r\n    model_type = \"llava_qwen\"\r\n\r\n\r\nclass LlavaQwenModel(LlavaMetaModel, AutoModelForCausalLM):\r\n    config_class = LlavaQwenConfig\r\n\r\n    def __init__(self, config: QWenConfig):\r\n        super(LlavaQwenModel, self).__init__(config)\r\n\r\n\r\nclass LlavaQwenForCausalLM(AutoModelForCausalLM, LlavaMetaForCausalLM):\r\n    config_class = LlavaQwenConfig\r\n\r\n    def __init__(self, config):\r\n        super(AutoModelForCausalLM, self).__init__(config)\r\n        self.model = LlavaQwenModel(config)\r\n        self.pretraining_tp = config.pretraining_tp\r\n        self.vocab_size = config.vocab_size\r\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\r\n\r\n        # Initialize weights and apply final processing\r\n        self.post_init()\r\n\r\n    def get_model(self):\r\n        return self.model\r\n\r\n    def forward(\r\n        self,\r\n        input_ids: torch.LongTensor = None,\r\n        attention_mask: Optional[torch.Tensor] = None,\r\n        position_ids: Optional[torch.LongTensor] = None,\r\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\r\n        inputs_embeds: Optional[torch.FloatTensor] = None,\r\n        labels: Optional[torch.LongTensor] = None,\r\n        use_cache: Optional[bool] = None,\r\n        output_attentions: Optional[bool] = None,\r\n        output_hidden_states: Optional[bool] = None,\r\n        images: Optional[torch.FloatTensor] = None,\r\n        return_dict: Optional[bool] = None,\r\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\r\n\r\n        if inputs_embeds is None:\r\n            (\r\n                input_ids,\r\n                position_ids,\r\n                attention_mask,\r\n                past_key_values,\r\n                inputs_embeds,\r\n                labels\r\n            ) = self.prepare_inputs_labels_for_multimodal(\r\n                input_ids,\r\n                position_ids,\r\n                attention_mask,\r\n                past_key_values,\r\n                labels,\r\n                images\r\n            )\r\n\r\n        return super().forward(\r\n            input_ids=input_ids,\r\n            attention_mask=attention_mask,\r\n            position_ids=position_ids,\r\n            past_key_values=past_key_values,\r\n            inputs_embeds=inputs_embeds,\r\n            labels=labels,\r\n            use_cache=use_cache,\r\n            output_attentions=output_attentions,\r\n            output_hidden_states=output_hidden_states,\r\n            return_dict=return_dict\r\n        )\r\n\r\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\r\n        images = kwargs.pop(\"images\", None)\r\n        _inputs = super().prepare_inputs_for_generation(\r\n            input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs\r\n        )\r\n        if images is not None:\r\n            _inputs['images'] = images\r\n        return _inputs\r\n\r\nAutoConfig.register(\"llava_qwen\", LlavaQwenConfig)\r\nAutoModelForCausalLM.register(LlavaQwenConfig, LlavaQwenForCausalLM)\r\n```\r\n\r\n\r\nBut I got the following error:\r\n```\r\nValueError: Unrecognized configuration class <class 'transformers_modules.Qwen.Qwen-7B.ef3c5c9c57b252f3149c1408daf4d649ec8b6c85.configuration_qwen.QWenConfig'> for this kind o\r\nf AutoModel: LlavaQwenForCausalLM.                                                                                                                                             \r\nModel type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConf\r\nig, CamembertConfig, CodeGenConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, ElectraConfig, ErnieConfig, FalconConfig, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig\r\n, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, LlamaConfig, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MptConfig, MusicgenConfig, MvpConfig,\r\n OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormCo\r\nnfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, TransfoXLConfig, TrOCRConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLCo\r\nnfig, XLNetConfig, XmodConfig, LlavaConfig, LlavaMPTConfig, LlavaQwenConfig.\r\n```</BODY>\n\n<COMMENTS>\n<Comment by 20191864218 at 2024-02-22T17:14:00Z>\nHi,I have the same issue. Have you solved it?\n</Comment>\n<Comment by jyC23333 at 2024-02-23T02:51:59Z>\n@20191864218\r\nHi, yes, I've already adapt the qwen model to llava.\r\nMany details should be noticed.\r\n\r\nI suggest you to follow this repo to adapt qwen to llava: https://github.com/Ucas-HaoranWei/Vary\r\nVary is made from LLaVA-v1, an old version of llava. If you don't mind the version of llava, you can refer to the repo.\r\n\r\nBy the way, Qwen team just pulished the newest qwen-1.5 series, I don't know whether it is easier to adapt. I hope so!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1022,
    "state": "open",
    "created_by": "bdytx5",
    "created_at": "2024-01-27T20:23:27Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1022</URL>\n\n<TITLE>Model Builder Issue</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nInstead of checking the folder name, its better to pass the llava and lora as an argument. Otherwise users have no idea why their models arent loading, since this naming convention isnt documented \r\n\r\nCommand:\r\n```\r\n    if 'llava' in model_name.lower():\r\n        # Load LLaVA model\r\n        if 'lora' in model_name.lower() and model_base is None:\r\n            warnings.warn('There is `lora` in model name but no `model_base` is provided. If you are loading a LoRA model, please provide the `model_base` argument. Detailed instruction: https://github.com/haotian-liu/LLaVA#launch-a-model-worker-lora-weights-unmerged.')\r\n        if 'lora' in model_name.lower() and model_base is not None:\r\n            lora_cfg_pretrained = AutoConfig.from_pretrained(model_path)\r\n            tokenizer = AutoTokenizer.from_pretrained(\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1021,
    "state": "closed",
    "created_by": "bdytx5",
    "created_at": "2024-01-27T20:20:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1021</URL>\n\n<TITLE>Problem in model builder ...</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\ncode like this python ```\r\n    if 'llava' in model_name.lower():\r\n        # Load LLaVA model\r\n        if 'lora' in model_name.lower() and model_base is None:\r\n            warnings.warn('There is `lora` in model name but no `model_base` is provided. If you are loading a LoRA model, please provide the `model_base` argument. Detailed instruction: https://github.com/haotian-liu/LLaVA#launch-a-model-worker-lora-weights-unmerged.')\r\n        if 'lora' in model_name.lower() and model_base is not None:\r\n            lora_cfg_pretrained = AutoConfig.from_pretrained(model_path)\r\n            tokenizer = AutoTokenizer.from_pretrained(\r\n```\r\n\r\nshould probably not be used. Better to pass the lora / llava as an argument rather than to check the name of the folder. \r\n\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1019,
    "state": "open",
    "created_by": "LinB203",
    "created_at": "2024-01-27T08:48:02Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1019</URL>\n\n<TITLE>[Discussion] We are contributing 🎉🎉🎉MoE-LLaVA</TITLE>\n\n<BODY>### Discussion\r\n\r\nHello, esteemed LLaVA developer, thank you for contributing such robust code and data to the community.\r\n\r\nWe have extended LLaVA to [MoE-LLaVA](https://github.com/PKU-YuanGroup/MoE-LLaVA), which with just **3B sparsely activated parameters**, MoE-LLaVA demonstrates performance comparable to the 7B dense model on various visual understanding datasets and even surpasses the 13B dense model in object hallucination benchmarks.\r\n\r\n\r\nDemo: https://huggingface.co/spaces/LanguageBind/MoE-LLaVA\r\nCode: https://github.com/PKU-YuanGroup/MoE-LLaVA\r\n\r\nThank you again for your contributions to the large visual-language model!\r\n\r\n\r\n\r\n![intro](https://github.com/haotian-liu/LLaVA/assets/62638829/6d47a651-410a-4677-92c2-b5e62b19dae9)\r\n\r\n\r\n![imagecli](https://github.com/haotian-liu/LLaVA/assets/62638829/59945318-31a0-45c8-b877-bad3303d93ea)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1018,
    "state": "open",
    "created_by": "rapsar",
    "created_at": "2024-01-27T02:33:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1018</URL>\n\n<TITLE>[Fine-tuning] what to do with adapter_model.safetensors after fine-tuning</TITLE>\n\n<BODY>### Question\n\nI finally managed to fine-tune LLaVA on a custom dataset (LLaVA-1.5-7b on Google Colab using a single A100 GPU)\r\nThe output I got was mostly an adapter_model.safetensors file (610MB) -- and a bunch of other log (?) files\r\n\r\nWhat should I do with the safetensors file? Incorporate it into the base model -- how?\r\n\r\nThanks!</BODY>\n\n<COMMENTS>\n<Comment by hossainiir at 2024-03-10T18:32:29Z>\nThis is what exactly I'm looking for!\r\nI can't find any straightforward document or tutorial!\n</Comment>\n<Comment by DavidLee528 at 2024-04-18T02:16:30Z>\n> Once we have our fine-tuned weights, we can build our fine-tuned model and save it to a new directory, with its associated tokenizer. By performing these steps, we can have a memory-efficient fine-tuned model and tokenizer ready for inference!\r\n\r\nhttps://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/\r\n\r\n@rapsar @hossainiir Is this tutorial helpful?\n</Comment>\n<Comment by Antony-M1 at 2024-07-29T07:40:14Z>\nThe `adapter_model.safetensors` file is a key part of the PEFT (Parameter-Efficient Fine-Tuning) framework, specifically related to methods like LoRA (Low-Rank Adaptation) or other adapter-based fine-tuning techniques. This file contains the weights of the adapter layers that have been fine-tuned, separate from the original model weights. \r\n\r\n### How it Works\r\n\r\n1. **Adapters**:\r\n   Adapters are small neural network layers added to the original model. During fine-tuning, instead of updating all the weights of the large model, only the weights of these adapter layers are updated. This significantly reduces the number of parameters that need to be trained, making fine-tuning more efficient.\r\n\r\n2. **Integration with the Base Model**:\r\n   When you fine-tune a model using adapters, the `adapter_model.safetensors` file stores the updated weights of these adapter layers. The base model remains unchanged, and the adapter weights are loaded and integrated with the base model during inference.\r\n\r\n### Using `adapter_model.safetensors`\r\n\r\nTo use the `adapter_model.safetensors` file, you need to load it along with the base model. Here is an example of how to do this:\r\n\r\n```python\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\nfrom peft import PeftModel, PeftConfig\r\n\r\nMODEL_NAME = 'meta-llama/Meta-Llama-3-8B'\r\nADAPTER_PATH = 'path/to/adapter_model.safetensors'\r\n\r\n# Load base model\r\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME, trust_remote_code=True, device_map='auto')\r\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\r\n\r\n# Load adapter configuration and model\r\nadapter_config = PeftConfig.from_pretrained(ADAPTER_PATH)\r\nmodel = PeftModel.from_pretrained(model, adapter_config, adapter_path=ADAPTER_PATH)\r\n\r\n# Now you can use the model with the fine-tuned adapter\r\ninput_ids = tokenizer(\"Your input text here\", return_tensors=\"pt\").input_ids\r\noutput = model.generate(input_ids)\r\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1017,
    "state": "open",
    "created_by": "bdytx5",
    "created_at": "2024-01-26T20:35:39Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1017</URL>\n\n<TITLE>Validation in training loop?</TITLE>\n\n<BODY>### Question\n\nIt doesnt look like you validate within the training loop? Just wondering what the reason for this was? \r\n\r\nThanks,\r\nBrett</BODY>\n\n<COMMENTS>\n<Comment by gpantaz at 2024-02-09T11:25:35Z>\nAny updates on this? :)\n</Comment>\n<Comment by bdytx5 at 2024-03-27T17:27:47Z>\ni added it to my repo. check it out on my profile\n</Comment>\n<Comment by dcampanini at 2024-04-05T14:58:41Z>\nHi @bdytx5 , \r\n\r\ndid you validate during stage 1 and stage 2? Which file or files did you modify ? \r\n\r\nI was not able to find that answer in your repo https://github.com/bdytx5/finetune_LLaVA\r\n\r\nregards\n</Comment>\n<Comment by bdytx5 at 2024-04-25T01:59:35Z>\njust the final finetuning stage\n</Comment>\n<Comment by iremonur at 2024-05-09T14:38:27Z>\nHi @bdytx5,\r\n\r\nI cannot find the modification you made for the validation step during the fine-tuning. Where is it? I will be glad if you help me with this!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1016,
    "state": "closed",
    "created_by": "y-vectorfield",
    "created_at": "2024-01-26T09:57:35Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1016</URL>\n\n<TITLE>[Usage] LLaVA outputted incoherent sentences on Arm arch</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: LLaVA outptted incoherent sentences when we implemented on Arm architecure cpu.\r\n\r\n## Environment\r\n* OS: \"Rocky Linux 9.1\r\n* CPU: Ampere Altra Max(128C128T)\r\n* DRAM: 256GB\r\n* GPU: NVIDA A100 PCIe 80GB\r\n\r\n### Softwares\r\n* Docker 23.0.4(use nvcr.io/nvidia/pytorch:23.05-py3 image)\r\n* CUDA 12.1\r\n* Python 3.10.6\r\n* accelerate 0.24.1\r\n* huggingface-hub 0.20.1\r\n* llava 1.1.3(built the latest version)\r\n* safetensors 0.4.0\r\n* sentencepiece 0.1.98\r\n* torch 2.0.0\r\n* torchvision 0.15.1\r\n* transformers 4.31.0\r\n\r\n## Parameters\r\nI set following parameters. (Other parametes used by defaults.)\r\n* Model: liuhaotian/llava-v1.5-13b\r\n* IMG: https://llava-vl.github.io/static/images/view.jpg\r\n* Prompt: What are the things I should be cautious about when I visit here?\r\n* Data Type: Float32\r\n* Random Seed: 240119\r\n* torch.use_deterministic_algorithms: True\r\n* torch.backends.cudnn.deterministic: True\r\n* torch.backends.cudnn.benchmark: False\r\n* no_repeat_ngram_size: 2\r\n\r\nWhen I implemented LLaVA using eval_model() function on above env, it outputted incoherent sentences like the following appear.(These contents was written in non-English texts that make no sense.)\r\n**If I changed the random seed, it would only change the words repeated in the sentences.**\r\n\r\nWhen visiting the location, which is a wooden and arietially-paced, or a-fri-l-c-o-r, (a-t-b-a) or (c) and (b) (and) a (or) an (o) - and - a - (p) / (f) & (l) + a & a / a and the (w)e-n-e, a, & the, the & an, an & & and & 2 &2 and 1 and and all and some and many and one and two and three and four and five and six and seven and eight and nine and ten and eleven and twelve and thirteen and fifteen and twenty and thirty and forty and fifty and sietn andt ande and ea andi anda,a. and, all, some, many, one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirtie, 4,5,6,7,8,9,1,2,3,4 and so on.\r\n---\r\n\r\nFor comparison, I also implemented this model on the following x86 env.\r\n\r\n## Environment\r\n* OS: \"Rocky Linux 8.7\r\n* CPU: Intel Xeon Gold 6338T(24C48T)\r\n* DRAM: 200GB\r\n* GPU: NVIDIA RTX A6000 50GB\r\n\r\n### Softwares\r\n* Docker 24.0.7(use nvcr.io/nvidia/pytorch:23.05-py3 image)\r\n* CUDA 12.1\r\n* Python 3.10.6\r\n* accelerate 0.24.1\r\n* huggingface-hub 0.20.1\r\n* llava 1.1.3(built the latest version)\r\n* safetensors 0.4.0\r\n* sentencepiece 0.1.98\r\n* torch 2.0.0\r\n* torchvision 0.15.1\r\n* transformers 4.31.0\r\n\r\n## Parameters\r\n* **Same condition on Arm arch**\r\n\r\nWhen visiting the pier over the lake, there are a few things to be mindful of for safety and enjoyment. First, be aware of the weather conditions, as the image shows a cloudy day. This could indicate potential rain or wind, which might make the experience less comfortable or even dangerous. Second, ensure that you have proper footwear, such as water shoes or boots, to prevent slipping on the wooden pier or getting wet from the water. Third, if you plan to swim or engage in water activities, make sure you are aware and follow any safety guidelines or rules posted at the location. Lastly, keep an eye on your belongings and be respectful to other visitors, ensuring a pleasant experience for everyone.\r\n---</BODY>\n\n<COMMENTS>\n<Comment by y-vectorfield at 2024-02-06T10:16:05Z>\nThis issue was solved at LLaVA v1.2.2. Hence, I closed this issue.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1015,
    "state": "open",
    "created_by": "zhouhao028",
    "created_at": "2024-01-26T07:27:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1015</URL>\n\n<TITLE>image loading and processing code</TITLE>\n\n<BODY>### Question\n\nThank you for your work, Could you please provide existing image loading and processing code? It means a lot to me!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1014,
    "state": "open",
    "created_by": "y-vectorfield",
    "created_at": "2024-01-26T06:03:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1014</URL>\n\n<TITLE>[Feature request] The param `no_repeat_ngram_size` should set in eval_model() func</TITLE>\n\n<BODY>### feature\n\nThe latest version of the sample code for LLaVA could not set the param for ngram repeat size.\r\nWhen we implement this model, we want to analyze the effects of ngram repeat.\r\nHence, I think we should modify a model generation @ eval_model() function like the following codes.(Line 114-L125)\r\n\r\n```python\r\n    with torch.inference_mode():\r\n        output_ids = model.generate(\r\n            input_ids,\r\n            images=images_tensor,\r\n            do_sample=True if args.temperature > 0 else False,\r\n            temperature=args.temperature,\r\n            top_p=args.top_p,\r\n            num_beams=args.num_beams,\r\n            max_new_tokens=args.max_new_tokens,\r\n            use_cache=True,\r\n            stopping_criteria=[stopping_criteria],\r\n            no_repeat_ngram_size=args.no_repeat_ngram_size,\r\n        )\r\n```\r\n\r\nOf course, the example code of LLaVA should be modified like the following codes.\r\n\r\n```python\r\nmodel_path = \"liuhaotian/llava-v1.5-7b\"\r\nprompt = \"What are the things I should be cautious about when I visit here?\"\r\nimage_file = \"https://llava-vl.github.io/static/images/view.jpg\"\r\n\r\nargs = type('Args', (), {\r\n    \"model_path\": model_path,\r\n    \"model_base\": None,\r\n    \"model_name\": get_model_name_from_path(model_path),\r\n    \"query\": prompt,\r\n    \"conv_mode\": None,\r\n    \"image_file\": image_file,\r\n    \"sep\": \",\",\r\n    \"temperature\": 0,\r\n    \"top_p\": None,\r\n    \"num_beams\": 1,\r\n    \"max_new_tokens\": 512,\r\n    \"no_repeat_ngram_size\", 3\r\n})()\r\n\r\neval_model(args)\r\n```</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1013,
    "state": "open",
    "created_by": "y-vectorfield",
    "created_at": "2024-01-26T05:45:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1013</URL>\n\n<TITLE>[Usage] The deterministic mode did not set in eval_model() function</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: The internal parameter for deterministic mode did not set in eval_model() func\r\n\r\nAccording to the example code for eval_model(), `temperature` parameter set `0`.\r\n\r\n```python\r\nmodel_path = \"liuhaotian/llava-v1.5-7b\"\r\nprompt = \"What are the things I should be cautious about when I visit here?\"\r\nimage_file = \"https://llava-vl.github.io/static/images/view.jpg\"\r\n\r\nargs = type('Args', (), {\r\n    \"model_path\": model_path,\r\n    \"model_base\": None,\r\n    \"model_name\": get_model_name_from_path(model_path),\r\n    \"query\": prompt,\r\n    \"conv_mode\": None,\r\n    \"image_file\": image_file,\r\n    \"sep\": \",\",\r\n    \"temperature\": 0,\r\n    \"top_p\": None,\r\n    \"num_beams\": 1,\r\n    \"max_new_tokens\": 512\r\n})()\r\n\r\neval_model(args)\r\n```\r\n\r\nI think if we set this param 0, we should explicitly set these additional params.\r\n\r\n```python\r\ntorch.use_deterministic_algorithms = True\r\ntorch.backends.cudnn.deterministic = True\r\ntorch.backends.cudnn.benchmark = False\r\n```\r\n\r\nHence, we should add the following conditional execution in eval_model() func.\r\n\r\n```python\r\nif args.temperature == 0:\r\n  torch.use_deterministic_algorithms = True\r\n  torch.backends.cudnn.deterministic = True\r\n  torch.backends.cudnn.benchmark = False\r\nelse:\r\n  torch.use_deterministic_algorithms = False\r\n  torch.backends.cudnn.deterministic = False\r\n  torch.backends.cudnn.benchmark = True</BODY>\n\n<COMMENTS>\n<Comment by zhuoyan-xu at 2024-06-26T19:45:46Z>\nHi,\r\n  Thank you for bringing up this issue. I encountered a similar problem even after explicitly setting `torch.backends.cudnn.deterministic` and related flags. I've noticed that the discrepancies occur specifically in the CLIP ViT encoders, where the vision embeddings produce different values across separate runs.\r\n\r\nWhen comparing two identical inference processes, I observed that the `image_forward_out` varies despite using the same `image` input. This occurs in the following file:\r\nhttps://github.com/haotian-liu/LLaVA/blob/c121f0432da27facab705978f83c4ada465e46fd/llava/model/multimodal_encoder/clip_encoder.py#L50\r\n\r\nThis situation occurs starting from the second example until the last one.\r\n\r\nI'm curious to know if you're still experiencing this issue and if you've found a solution. Any insights would be greatly appreciated. Thank you for your time!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1012,
    "state": "open",
    "created_by": "Yueeeeeeee",
    "created_at": "2024-01-25T23:45:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1012</URL>\n\n<TITLE>[Question] Why skipping quantization of mm_projector?</TITLE>\n\n<BODY>### Question\n\nThanks for the great work!\r\n\r\nIn [LLaVA/llava/train/train.py](https://github.com/haotian-liu/LLaVA/blob/9a26bd1435b4ac42c282757f2c16d34226575e96/llava/train/train.py#L775) there is an argument for skipping the quantization of mm_projector:\r\n\r\n`llm_int8_skip_modules=[\"mm_projector\"]`,\r\n\r\nbut I find similar performance fine-tuning Llava for my downstream task without doing so, so I wonder is there a specific reason of doing it? Thanks!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1011,
    "state": "closed",
    "created_by": "emmaking-smith",
    "created_at": "2024-01-25T18:22:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1011</URL>\n\n<TITLE>[Usage] KeyError: 'LlavaConfig' when evaluating with model_vqa.py</TITLE>\n\n<BODY>### Describe the issue\n\nHi!  I'm attempting to re-train LLaVA on some of my own data but to start, I've just been training on the repo's data.  I've pretrained and finetuned (with lora) without trouble but when I attempt to evaluate my lora finetuned model, I run into some trouble.\r\n\r\n`python model_vqa.py --model-path checkpoints/llava-v1.5-13b-lora/ --question-file playground/data/coco\r\n2014_val_qa_eval/qa90_questions.jsonl --image-folder playground/data/coco2014_val_qa_eval/val2014/ --answers-file my_answers.jsonl --model-base liuhaotian/llava-v1.5-13b-lora`\r\n\r\nGives error:\r\n\r\n/opt/conda/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\r\n  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\r\n/opt/conda/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\r\n\r\n[2024-01-25 18:05:55,136] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n\r\nmodel_path checkpoints/llava-v1.5-13b-lora/, model_name llava-v1.5-13b-lora, model_base liuhaotian/llava-v1.5-13b-lora\r\n\r\nTraceback (most recent call last):\r\n  File \"/mnt/ceph_rbd/LLaVA/model_vqa.py\", line 114, in <module>\r\n    eval_model(args)\r\n  File \"/mnt/ceph_rbd/LLaVA/model_vqa.py\", line 36, in eval_model\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, args.model_base, model_name)\r\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/ceph_rbd/LLaVA/llava/model/builder.py\", line 51, in load_pretrained_model\r\n    tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py\", line 718, in from_pretrained\r\n    tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]\r\n                                               ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 663, in __getitem__\r\n    model_type = self._reverse_config_mapping[key.__name__]\r\n                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\r\nKeyError: 'LlavaConfig'\r\n\r\nDo you have any suggestions on how to fix this error?</BODY>\n\n<COMMENTS>\n<Comment by attnmamba at 2024-02-01T04:03:54Z>\n@emmaking-smith Did you manage to fix this?\n</Comment>\n<Comment by cyj95 at 2024-02-01T08:13:25Z>\nhas anyone fixed it?\n</Comment>\n<Comment by emmaking-smith at 2024-02-02T10:28:09Z>\nYes, I think the solution is as follows.  I found the answer from a previous question https://github.com/haotian-liu/LLaVA/issues/963.\r\n\r\nThey key here is that --model-base is NOT liuhaotian/llava-v1.5-13b-lora but lmsys/vicuna-13b-v1.5, which you can see is the case in the config.json file that's deposited after model training.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1010,
    "state": "open",
    "created_by": "tangjunjun966",
    "created_at": "2024-01-24T07:22:27Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1010</URL>\n\n<TITLE>when fitune_lora.sh ，non_lora_trainables.bin not generate</TITLE>\n\n<BODY>### Question\n\nHow to deal with this problem,</BODY>\n\n<COMMENTS>\n<Comment by zengxingchen at 2024-01-27T13:19:59Z>\nthe same\n</Comment>\n<Comment by tangjunjun966 at 2024-01-31T02:40:48Z>\n作者是有保存non_lora_trainables.bin文件的，需要自己去打开\n</Comment>\n<Comment by zengxingchen at 2024-02-20T12:34:45Z>\n> 作者是有保存non_lora_trainables.bin文件的，需要自己去打开\r\n\r\n我是指训练过程中的每个epoch没有存，不是指最后一个epoch\n</Comment>\n<Comment by LongYu-LY at 2024-03-07T11:32:24Z>\nCan all epochs share a non_lora_trainables.bin?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1009,
    "state": "open",
    "created_by": "BeachWang",
    "created_at": "2024-01-24T03:26:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1009</URL>\n\n<TITLE>[Usage] Inference with batches</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nI try to run inference with batches to accelerate the process. However, I find it takes just as much time as inferring each instance individually.</BODY>\n\n<COMMENTS>\n<Comment by merrymercy at 2024-01-24T11:55:45Z>\nhttps://github.com/haotian-liu/LLaVA/issues/754#issuecomment-1907970439\n</Comment>\n<Comment by gyupro at 2024-03-19T08:49:30Z>\n@merrymercy Does your method provide lora adapater as well? I don't see how I insert an adapter with your code.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1008,
    "state": "open",
    "created_by": "JSeuma",
    "created_at": "2024-01-23T11:24:28Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1008</URL>\n\n<TITLE>Trying to inference using the lora adapter (from fine-tuning) and the base model and not working</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI have fine-tune the liuhaotian/llava-v1.5-7b model and I have the output in ./checkpoints/llava-v1.5-7b-task-lora. I try to do the inference with only the base model and it works, but when I try to do inference including the lora adapter it gives me error:\r\n\r\nCode for inference with lora adpter:\r\npython -m llava.serve.cli     --model-path ./checkpoints/llava-v1.5-7b-task-lora    --model-base liuhaotian/llava-v1.5-7b     --image-file ./playground/data/textvqa/train_images/00a108e5e2160b20.jpg     --load-4bit\r\n\r\nI put the adapter as the model-path and the base model as model-base. The error code is below, any idea what I am doing wrong?. Thanks in advance.\r\n\r\n[2024-01-23 12:05:56,258] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nLoading LLaVA from base model...\r\nLoading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.54s/it]\r\nLoading additional LLaVA weights...\r\nTraceback (most recent call last):\r\n  File \"/home/jseuma/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/jseuma/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/jseuma/LLaVA/llava/serve/cli.py\", line 124, in <module>\r\n    main(args)\r\n  File \"/home/jseuma/LLaVA/llava/serve/cli.py\", line 32, in main\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n  File \"/home/jseuma/LLaVA/llava/model/builder.py\", line 75, in load_pretrained_model\r\n    model.load_state_dict(non_lora_trainables, strict=False)\r\n  File \"/home/jseuma/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2041, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for LlavaLlamaForCausalLM:\r\n        size mismatch for model.mm_projector.0.weight: copying a param with shape torch.Size([4096, 1024]) from checkpoint, the shape in current model is torch.Size([2097152, 1]).\r\n        size mismatch for model.mm_projector.2.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([8388608, 1]).</BODY>\n\n<COMMENTS>\n<Comment by AnjanGiri at 2024-03-01T09:05:08Z>\nI am also getting the same error when trying to load the Finetuned Lora checkpoint using the following code:\r\n```from llava.model.builder import load_pretrained_model\r\n\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n            \"/data/image_captioning_data/checkpoints_2/llava-v1.5-7b-task-lora/checkpoint-5\",\r\n            model_name=\"liuhaotian/llava-v1.5-7b-lora-finetuned\",\r\n            model_base=\"liuhaotian/llava-v1.5-7b\",\r\n            load_8bit=False, \r\n            load_4bit=True)```\n</Comment>\n<Comment by 1716757342 at 2024-03-11T14:24:45Z>\nI also encountered the same problem, have you solved it? thank you.\n</Comment>\n<Comment by 1716757342 at 2024-03-11T15:39:24Z>\nYou need to first meger base model and lora model with the following code.\r\n\r\npython scripts/merge_lora_weights.py \\\r\n--model-path /path/to/llava-v1.5-13b-lora \\\r\n--model-base LLaVA_train \\\r\n--save-model-path merge_model\r\n\r\nThen use the post-meger run and you're done.\n</Comment>\n<Comment by zhyhome at 2024-03-11T16:47:43Z>\n> You need to first meger base model and lora model with the following code.\r\n> \r\n> python scripts/merge_lora_weights.py --model-path /path/to/llava-v1.5-13b-lora --model-base LLaVA_train --save-model-path merge_model\r\n> \r\n> Then use the post-meger run and you're done.\r\n\r\nI merged,but this model can't gengrate anything.\n</Comment>\n<Comment by Yrdal3910 at 2024-11-02T02:56:26Z>\nI encountered the same issue. For me, the error occurred when I added the parameter \"--load-4bit\" while launching the model worker. It appears that the base model uses 8-bit storage because inference runs smoothly either when I remove this parameter or switch to \"--load-8bit\". This also explains why 4096 * 1024 equals 4194304, which is double 2097152.\n</Comment>\n<Comment by psvkaushik at 2024-11-07T22:31:37Z>\n> I encountered the same issue. For me, the error occurred when I added the parameter \"--load-4bit\" while launching the model worker. It appears that the base model uses 8-bit storage because inference runs smoothly either when I remove this parameter or switch to \"--load-8bit\". This also explains why 4096 * 1024 equals 4194304, which is double 2097152.\r\n\r\nI'm trying to run the MMbench using the merged model but still getting empty strings as output, can you maybe coomen on that? I actually raised it as an issue for more context : [issue](https://github.com/haotian-liu/LLaVA/issues/1752)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1007,
    "state": "open",
    "created_by": "simon-lund",
    "created_at": "2024-01-23T11:21:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1007</URL>\n\n<TITLE>[Question] Why is the batch Size for LoRA fine tuning 16 per GPU?</TITLE>\n\n<BODY>### Question\r\n\r\nIn the README, you report finetuning with LoRA on 8xA100 40GB.\r\nHowever, the specified batch size (16) `scripts/v1.5/lora_finetune_task.sh` is too large for 40GB VRAM.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1006,
    "state": "open",
    "created_by": "lucasjinreal",
    "created_at": "2024-01-23T07:59:09Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1006</URL>\n\n<TITLE>Int4 quantization support</TITLE>\n\n<BODY>Int4 quantization support</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1005,
    "state": "closed",
    "created_by": "yang606",
    "created_at": "2024-01-23T04:15:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1005</URL>\n\n<TITLE>[Usage] Gradio Web interface can not open, Google browser http://0.0.0.0:40000 display ' the page does not work 0.0.0.0 is currently unable to handle this request. HTTP ERROR 502'</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI 'm operating on Windows 11, GPU : RTX4060, with LLaVA checkpoint : liuhaotian / llava-v1.5-7b prepared locally.\r\nCommand:\r\npython -m llava.serve.controller --host 0.0.0.0 --port 10000\r\n![image](https://github.com/haotian-liu/LLaVA/assets/150589589/d4e2625d-f48b-481c-b383-11c1a68b69a3)\r\n\r\npython -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload\r\n![image](https://github.com/haotian-liu/LLaVA/assets/150589589/b2379ea1-38e1-4e68-b3bf-516d84f52d83)\r\n\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-7b\r\n![image](https://github.com/haotian-liu/LLaVA/assets/150589589/f45cd72d-3427-47dd-aa21-b21738d55f47)\r\n\r\nClick the web page http://0.0.0.0:40000. But it shows :\r\n![image](https://github.com/haotian-liu/LLaVA/assets/150589589/b24d8e18-b420-42d6-a69c-b145288d9add)\r\n\r\nMy network connection is normal. Why can 't open the web page, how to solve it ? Do I have any other questions ?</BODY>\n\n<COMMENTS>\n<Comment by liwenyang-911 at 2024-01-26T01:37:03Z>\nThis is because this is not a public IP address\n</Comment>\n<Comment by yang606 at 2024-01-26T08:19:36Z>\n> This is because this is not a public IP address\r\nOkay, thank you very much！\n</Comment>\n<Comment by cryptonome at 2024-01-31T14:16:21Z>\nGradio is running on http://0.0.0.0:7860/ as indicated in the logs.\n</Comment>\n<Comment by haotian-liu at 2024-02-03T05:47:50Z>\nGradio interface should be at http://127.0.0.1:7860/\r\n\r\nThanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1004,
    "state": "open",
    "created_by": "wh850",
    "created_at": "2024-01-23T03:37:45Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1004</URL>\n\n<TITLE>怎么对图片进行批量推理呢？</TITLE>\n\n<BODY></BODY>\n\n<COMMENTS>\n<Comment by merrymercy at 2024-01-24T11:55:56Z>\nhttps://github.com/haotian-liu/LLaVA/issues/754#issuecomment-1907970439\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1003,
    "state": "open",
    "created_by": "lucasjinreal",
    "created_at": "2024-01-22T09:51:21Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1003</URL>\n\n<TITLE>Support for transoformers latest version?</TITLE>\n\n<BODY>transformers latest version already have LLava config integreated in, support it out of box?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1002,
    "state": "open",
    "created_by": "OliverLeeXZ",
    "created_at": "2024-01-21T15:35:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1002</URL>\n\n<TITLE>[Question] flash-attn question</TITLE>\n\n<BODY>### Question\n\nI follow readme to create my environment, but when I using lora to finetune llava1.5 model（finetune_task_lora.sh）, I facing this problem:\r\n    import flash_attn_2_cuda as flash_attn_cuda\r\nImportError: /home/lxz/miniconda3/envs/llava/lib/python3.10/site-packages/flash_attn_2_cuda.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c104cuda9SetDeviceEi\r\nI think this is a common issue. How can I solve this problem?</BODY>\n\n<COMMENTS>\n<Comment by cdd1993 at 2024-01-23T09:02:36Z>\nTry pip uninstall transformer-engine flash-attn -y && FLASH_ATTENTION_FORCE_BUILD=TRUE pip install flash-attn\n</Comment>\n<Comment by wysci at 2025-04-13T03:00:00Z>\nI had the same problem. Did you solve it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 1001,
    "state": "open",
    "created_by": "ByungKwanLee",
    "created_at": "2024-01-21T05:56:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1001</URL>\n\n<TITLE>[Question] layer_norm float32?</TITLE>\n\n<BODY>### Question\n\nIs there any reason for 'norm' to set float32?\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/50401429/286dd799-6412-4e37-989c-ad7f93527e23)\r\n\r\nwhat if it is set to bfloat16?\r\n\r\nDo you think the difference between float32 and bfloat16 in 'norm' operator makes performance gap be bigger?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 1000,
    "state": "open",
    "created_by": "kahnchana",
    "created_at": "2024-01-20T21:11:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/1000</URL>\n\n<TITLE>Unexpected outputs in newer version of model (v1.5)</TITLE>\n\n<BODY>### Question\n\nI use prompts like `List the 10 main objects found in this image. Output the category names as a list of strings.` and on some images, it produces outputs as below. Attaching two images used with the above prompt that produced same results (online API and offline with ckpt).\r\n\r\n<img width=\"1097\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/17735127/53e54f9c-61b3-4383-a917-a3689fd91662\">\r\n\r\nAre there any keywords used during training to emerge specific outputs (e.g. the localization train tasks) that should be avoided when prompting the model in general? \r\n\r\nThese are natural images (frames of videos in Ego4D) with no special modifications. This strange behavior stops when slightly changing prompt to `List the 10 main objects found in this image. Output the category names as a list.` Also, I did not face this issue for earlier versions of the model (i.e. passing same inputs to earlier ckpt). \r\n\r\n\r\n![fail2](https://github.com/haotian-liu/LLaVA/assets/17735127/3b0ce503-a085-4505-be79-57a15894a604)\r\n![fail1](https://github.com/haotian-liu/LLaVA/assets/17735127/321b23ed-84eb-43eb-97dc-540918e8da16)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 999,
    "state": "open",
    "created_by": "JulioZhao97",
    "created_at": "2024-01-18T07:12:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/999</URL>\n\n<TITLE>[Usage] Token indices sequence length is longer than the specified maximum sequence length for this model (3278 > 2048). Running this sequence through the model will result in indexing errors</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\ntoken length too long. Will this affect model performance?\r\n\r\nCommand:\r\n```\r\nbash scripts/v1_5/finetune_lora.sh\r\n```\r\n\r\nLog: \r\n```\r\nToken indices sequence length is longer than the specified maximum sequence length for this model (3278 > 2048). Running this sequence through the model will result in indexing errors\r\n```\r\n\r\nScreenshots:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/40555727/9349b113-8e62-4359-864e-5ea84fcd1c38)</BODY>\n\n<COMMENTS>\n<Comment by cherry956 at 2024-02-13T11:42:05Z>\n@JulioZhao97 Do you have solve this problem?\n</Comment>\n<Comment by crazysal at 2024-02-15T00:24:43Z>\nany update\n</Comment>\n<Comment by gordonhu608 at 2024-04-26T07:26:03Z>\nalso got this. This didn't stop my training. So it's kind of warning probably. But we actually truncated inputs before training, which is weird why do we still receive this?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 997,
    "state": "open",
    "created_by": "caiqi",
    "created_at": "2024-01-17T03:01:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/997</URL>\n\n<TITLE>[Usage] Why use \\n at the end of prompt during stage 1 pre-training.</TITLE>\n\n<BODY>### Describe the issue\n\nIn stage 1 pre-training, the end of prompt is always \\n, so the model will never predict eos during inference. It just repeatly generates similar sentences. Why use \\n instead of </s> during stage 1 pre-training? Thanks!</BODY>\n\n<COMMENTS>\n<Comment by feiyilicare at 2024-02-29T03:53:28Z>\nsame problem\n</Comment>\n<Comment by Mingfeng-Chen at 2024-04-07T03:27:11Z>\nyou can change conv_mode from \"plain\" to \"llama_2\", so it won't end with a '\\n' anymore.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 996,
    "state": "open",
    "created_by": "dzenilee",
    "created_at": "2024-01-16T22:23:50Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/996</URL>\n\n<TITLE>[Question] Quantization does not improve latency</TITLE>\n\n<BODY>### Question\n\nI did some testing with 4-bit and 8-bit quantization and it doesn't seem to improve inference time at all - in fact, it seems to make it worse. All I did was simply set `load_in_8bit` or `load_in_4bit` to True here: \r\n\r\n```py\r\nmodel_path = \"liuhaotian/llava-v1.5-13b\" \r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    model_path=model_path,\r\n    model_base=None,\r\n    load_in_8bit=True,\r\n    model_name=get_model_name_from_path(model_path),\r\n)  \r\n```\r\n\r\nTokens per second at different batch sizes: \r\n```\r\n# batch size 4\r\noriginal      36.741259\r\n8bit           7.710737\r\n4bit           7.889718\r\n\r\n# batch size 8 \r\noriginal      27.711023\r\n8bit           6.584828\r\n4bit           6.010469\r\n\r\n# batch size 16 \r\noriginal      18.238883\r\n8bit           5.710271\r\n4bit           4.769155\r\n```\r\n\r\nThe original, unquantized model is much faster, though the gap between the different models decreases as the batch size increases. I've come across information suggesting that quantization doesn't necessarily improve latency due to dequantization overhead. Do those numbers align with your expectation wrt Llava, @haotian-liu?</BODY>\n\n<COMMENTS>\n<Comment by wojiaoshihua at 2024-03-25T06:58:28Z>\nHave you improved this issue\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 995,
    "state": "closed",
    "created_by": "scarydemon2",
    "created_at": "2024-01-16T02:58:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/995</URL>\n\n<TITLE>[Question] pretrain result test</TITLE>\n\n<BODY>### Question\n\nIn the pretrain stage, I input a language model + a CLIP ViT, with the adapter being newly initialized. However, in cli.py, a llava model needs to be input, but the pretrain result only includes the weight of the adapter, so it cannot be used for inference through cli.py. Then, is there any method to test the result of the pretraining</BODY>\n\n<COMMENTS>\n<Comment by zihui-debug at 2024-05-31T08:59:25Z>\n> ### Question\r\n> In the pretrain stage, I input a language model + a CLIP ViT, with the adapter being newly initialized. However, in cli.py, a llava model needs to be input, but the pretrain result only includes the weight of the adapter, so it cannot be used for inference through cli.py. Then, is there any method to test the result of the pretraining\r\n\r\nhello, I have the same question. How did you deal with it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 994,
    "state": "open",
    "created_by": "AI-Aether",
    "created_at": "2024-01-15T04:31:57Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/994</URL>\n\n<TITLE>[Question]</TITLE>\n\n<BODY>### Question\n\nI wanted to fine-tune LLaVA on my dataset for my use case, but I had only 553 images in total. When I fine-tuned, merged the weights and then checked the inference of the fine-tuned model it was not at all good.\r\n\r\nAny suggestions @haotian-liu as how I can improve the inference or where did I make the mistake?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 993,
    "state": "open",
    "created_by": "owenzlz",
    "created_at": "2024-01-15T03:50:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/993</URL>\n\n<TITLE>[Question] More training epochs?</TITLE>\n\n<BODY>### Question\r\n\r\nI am curious whether training for more epochs (more than one) would lead to better performance, or if the performance would already be saturated after one epoch with the current llava data. I wonder if there have been any studies conducted on this. Thank you!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 992,
    "state": "open",
    "created_by": "Lee-ray-a",
    "created_at": "2024-01-14T14:57:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/992</URL>\n\n<TITLE>[Usage]</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n- I am trying to get a quick start on running the llava.\r\n- I download the weight and config.json ... file from hugging face locally to run the llava, it has some error. \r\n- How can I run the llava locally with the downloaded weight file?\r\n![image](https://github.com/haotian-liu/LLaVA/assets/71243853/9bd3e6bb-a21a-40fd-a478-a8a12a010d5e)\r\nthen I got some error\r\n![image](https://github.com/haotian-liu/LLaVA/assets/71243853/b5fe6873-cf21-4ccb-829b-a2468bb2dfb5)\r\n\r\nCommand:\r\n```\r\npython ./llava/eval/test_run_llava.py\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 991,
    "state": "open",
    "created_by": "RobertBiehl",
    "created_at": "2024-01-12T14:21:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/991</URL>\n\n<TITLE>[Question] why are there no boxes in conversation instruction tuning dataset prompt</TITLE>\n\n<BODY>### Question\r\n\r\nI was looking at dataset generation and it looks like the conversation examples don't contain any bounding boxes.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/playground/data/prompts/conversation/000_caps.txt\r\n\r\nEven though the system message\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/playground/data/prompts/conversation/system_message.txt\r\nasks the LLM to count objects, etc.\r\n\r\n> Include questions asking about the visual content of the image, including the object types, counting the objects, object actions, object locations, relative positions...\r\n\r\nAm I missing something? Couldn't it cause the LLM to count objects based on the 5 caption sentences, and in bad cases count the number of mentions of objects across these 5 sentences.\r\n\r\n\r\nExample:\r\nwhen I tried to reproduce the following example with LLaMA2 I came upon the issue of miscounting.\r\n\r\n**Captions:**\r\n```\r\nA restaurant has modern wooden tables and chairs.\r\nA long restaurant table with rattan rounded back chairs.\r\na long table with a plant on top of it surrounded with wooden chairs \r\nA long table with a flower arrangement in the middle for meetings\r\nA table is adorned with wooden chairs with blue accents.\r\n```\r\n\r\n**Response:**\r\n```\r\nQuestion:\r\nHow many tables are in the image?\r\n===\r\nAnswer:\r\nThere appear to be at least three tables in the images provided:\r\n- A long table with rattan rounded back chairs\r\n- A long table with a plant on top of it surrounded by wooden chairs\r\n- A table adorned with wooden chairs with blue accents.\r\n```</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 989,
    "state": "open",
    "created_by": "yujie-59",
    "created_at": "2024-01-12T10:53:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/989</URL>\n\n<TITLE>[Question] About finetune</TITLE>\n\n<BODY>### Question\n\nHow can I solve the problem of the program getting stuck when I run fineteue.sh?\r\n\r\n(llava) yj@user-SYS-420GP-TNR:/data1/yj/LLaVA$ sh finetune_lora.sh\r\n[2024-01-12 03:50:54,115] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-01-12 03:50:56,798] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\n[2024-01-12 03:50:56,798] [INFO] [runner.py:555:main] cmd = /data1/yj/anaconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMl19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero3.json --model_name_or_path /data1/yj/model/llava-v1.5-7b/ --version v1 --data_path /data1/yj/train_data/LLaVA-Instruct-150K/coco_train2017.json --image_folder /data1/yj/train_data/llava/ --vision_tower /data1/yj/model/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter /data1/yj/model/llava-v1.5-7b/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /data1/yj/model/checkpoints/llava-v1.5-7b-lora --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb\r\n[2024-01-12 03:50:57,927] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-01-12 03:50:59,383] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2]}\r\n[2024-01-12 03:50:59,383] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=3, node_rank=0\r\n[2024-01-12 03:50:59,383] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2]})\r\n[2024-01-12 03:50:59,383] [INFO] [launch.py:163:main] dist_world_size=3\r\n[2024-01-12 03:50:59,383] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2\r\n[2024-01-12 03:51:01,805] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-01-12 03:51:02,168] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-01-12 03:51:02,236] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2024-01-12 03:51:02,236] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2024-01-12 03:51:02,291] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2024-01-12 03:51:02,650] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2024-01-12 03:51:02,650] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2024-01-12 03:51:02,650] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n[2024-01-12 03:51:02,897] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2024-01-12 03:51:02,897] [INFO] [comm.py:594:init_distributed] cdb=None</BODY>\n\n<COMMENTS>\n<Comment by Ryanlijinke at 2024-11-03T13:10:24Z>\nsame issue. Did you solve it alrdy?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 988,
    "state": "open",
    "created_by": "xtyXpastor",
    "created_at": "2024-01-12T09:14:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/988</URL>\n\n<TITLE>[Usage] A question about LengthGroupedSampler</TITLE>\n\n<BODY>### Describe the issue\n\nIn my understanding, LengthGroupedSampler is to put samples with similar lengths into the same batch as much as possible to speed up training.\r\n\r\nHowever, in LLava's implementation, megabatch_size is set to world_size * batch_size, which is exactly the size of a batch, and is sorted internally. I'm not sure if doing this really maintains the original functionality of LengthGroupedSampler.\r\n\r\nIn the implementation of the Transformers library, megabatch_size is usually set to world_size * mega_batch_mult, and mega_batch_mult is usually a relatively large number, such as 50, 100 or even larger.\r\n\r\nIn practice, I also got an increase in training speed by modifying the code of LLaVA to set megabatch_size = world_size * batch_size * mega_batch_mult (mega_batch_mult= 128).\r\n\r\nSo I want to know whether LLaVa can support specifying mega_batch_mult directly in the parameter stage in subsequent updates.\r\n\r\nAbout 93% of my data has 200-300 tokens (including images) and the remainder may have closer to 1500 tokens. After modifying the above code, most batch times can be optimized from 12s to 5.7s.</BODY>\n\n<COMMENTS>\n<Comment by zte-tcb at 2024-10-17T07:38:45Z>\nI have the same doubts, why not just use LengthGroupedSampler in transformers, which can improve the training speed when the token lengths vary greatly? My token length ranges from 1000 to more than 2000.  Why did LLaVA re-implement a new version?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 986,
    "state": "open",
    "created_by": "J-e-l-l-y-Z",
    "created_at": "2024-01-12T08:45:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/986</URL>\n\n<TITLE>[Usage] can‘t run gradio_web_server.py</TITLE>\n\n<BODY>Issue: I can't run the gradio_web_server.py to reach the gradio interface, while the controller is working normally\r\n\r\nCommand:\r\n```\r\n# in the first terminal\r\ncd LLaVA\r\nconda activate llava\r\npython -m llava.serve.controller --host 0.0.0.0 --port 10000\r\n```\r\n\r\nLog: \r\n```\r\n2024-01-12 16:06:25 | INFO | controller | args: Namespace(host='0.0.0.0', port=10000, dispatch_method='shortest_queue')\r\n2024-01-12 16:06:25 | INFO | controller | Init controller\r\n2024-01-12 16:06:25 | ERROR | stderr | INFO:     Started server process [2696939]\r\n2024-01-12 16:06:25 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2024-01-12 16:06:25 | ERROR | stderr | INFO:     Application startup complete.\r\n2024-01-12 16:06:25 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:10000 (Press CTRL+C to quit)\r\n```\r\n\r\n\r\nCommand:\r\n```\r\n# in the second terminal\r\ncd LLaVA\r\nconda activate llava\r\npython -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload\r\n\r\n```\r\n\r\nLog: \r\n```\r\n2024-01-12 16:24:21 | INFO | gradio_web_server | args: Namespace(host='0.0.0.0', port=None, controller_url='http://localhost:10000', concurrency_count=10, model_list_mode='reload', share=False, moderate=False, embed=False)\r\n2024-01-12 16:24:22 | ERROR | stderr | Traceback (most recent call last):\r\n2024-01-12 16:24:22 | ERROR | stderr |   File \"/home/rix2usr2/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n2024-01-12 16:24:22 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n2024-01-12 16:24:22 | ERROR | stderr |   File \"/home/rix2usr2/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n2024-01-12 16:24:22 | ERROR | stderr |     exec(code, run_globals)\r\n2024-01-12 16:24:22 | ERROR | stderr |   File \"/home/rix2usr2/LLaVA/llava/serve/gradio_web_server.py\", line 459, in <module>\r\n2024-01-12 16:24:22 | ERROR | stderr |     models = get_model_list()\r\n2024-01-12 16:24:22 | ERROR | stderr |   File \"/home/rix2usr2/LLaVA/llava/serve/gradio_web_server.py\", line 40, in get_model_list\r\n2024-01-12 16:24:22 | ERROR | stderr |     assert ret.status_code == 200\r\n2024-01-12 16:24:22 | ERROR | stderr | AssertionError\r\n```</BODY>\n\n<COMMENTS>\n<Comment by yangyangtaizi at 2024-01-12T09:26:20Z>\nme too……\n</Comment>\n<Comment by anonymousz97 at 2024-01-17T05:04:26Z>\nI believe you should specify port larger than 8000 for that, current ones is 7860. Mine solved by using port 8006. See line 468 in server/gradio_web_server.py\n</Comment>\n<Comment by J-e-l-l-y-Z at 2024-01-17T07:11:06Z>\n> I believe you should specify port larger than 8000 for that, current ones is 7860. Mine solved by using port 8006. See line 468 in server/gradio_web_server.py\r\njust like python -m llava.serve.controller --host 0.0.0.0 --port 8006;  python -m llava.serve.gradio_web_server --controller http://localhost:8006 --model-list-mode reload --port 8007?\r\nI tried it but it doesn't work.\n</Comment>\n<Comment by anonymousz97 at 2024-01-17T07:24:32Z>\nyou run it from local or from docker? how about proxy? or try lsof -i -P -n | grep LISTEN to check if all the port is working?\n</Comment>\n<Comment by 1429904852 at 2024-03-16T03:25:48Z>\nhow to fix this problem? i have the same question\n</Comment>\n<Comment by 459737087 at 2024-03-22T05:44:28Z>\nHey! Did you solve it @1429904852\n</Comment>\n<Comment by 1429904852 at 2024-03-22T05:48:12Z>\n> Hey! Did you solve it @1429904852\r\n\r\nNot yet\n</Comment>\n<Comment by 1429904852 at 2024-03-25T13:28:12Z>\nHi, all, I have solved this problem. Before executing the demo command, please run the following three lines of code:\r\n\r\nexport http_proxy=\r\nexport https_proxy=\r\nexport all_proxy=\n</Comment>\n<Comment by 459737087 at 2024-03-26T06:28:35Z>\nHi ! I know, you close the proxy,you can succeed. Thank you !\n</Comment>\n<Comment by NamHuynh1308 at 2024-06-19T05:41:53Z>\n> Hi, all, I have solved this problem. Before executing the demo command, please run the following three lines of code:\r\n> \r\n> export http_proxy= export https_proxy= export all_proxy=\r\n\r\nhey did you run both controller and gadio at the same time? Thanks\n</Comment>\n<Comment by g-makerr at 2025-02-14T07:52:28Z>\njust check your proxy and unset them\n`unset http_proxy`\n`unset https_proxy`\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 985,
    "state": "open",
    "created_by": "NeilHnxTcc",
    "created_at": "2024-01-12T05:25:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/985</URL>\n\n<TITLE>[Question]llava-v1.0 and llava-v2 model failed to work in CLI Inference</TITLE>\n\n<BODY>### Question\n\nWhile doing the CLI Inference, I found an unexpected problem:\r\nThe template using llava-v1.5-7b as the model and it works pretty well. But when I tried to use llava-v1.0 and llava-v2 to do the inference, the results were very bad, it producing meaningless codes. Why?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 984,
    "state": "open",
    "created_by": "bobopit",
    "created_at": "2024-01-11T07:46:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/984</URL>\n\n<TITLE>Performance of LLaVA on NLP tasks</TITLE>\n\n<BODY>### Discussion\n\nHi, haotian, thanks for your excellent work.\r\nI have a question I'd like to discuss with you. During the SFT phase, will updating the Large Language Model (LLM) for training affect the performance of NLP tasks？</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 981,
    "state": "open",
    "created_by": "abravesailor",
    "created_at": "2024-01-10T09:35:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/981</URL>\n\n<TITLE>[Question] Unable to load the dataset</TITLE>\n\n<BODY>### Question\n\nLLaVA-Instruct-150K/llava_v1_5_mix665k.json\r\nthis file have serious json parse error, which lead to download failure\r\nplease check this file carefully and fix the bug,thx</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 980,
    "state": "open",
    "created_by": "puar-playground",
    "created_at": "2024-01-09T11:59:55Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/980</URL>\n\n<TITLE>image_processor error in loading lora weights</TITLE>\n\n<BODY>### Describe the issue\n\nHi haotian-liu,\r\n\r\nI'm interested in LLaVA and attempted CLI Inference using the Lora checkpoint with this command:\r\n```\r\npython -m llava.serve.cli --model-path /home/usr/LLaVA/checkpoints --model-base lmsys/vicuna-7b-v1.5 --image-file \"https://llava-vl.github.io/static/images/view.jpg\"\r\n```\r\nI am not sure if I used the correct base model, I also tried `--model-base meta-llama/Llama-2-7b-chat-hf`. But they lead to the same error:\r\n```\r\nLoading checkpoint shards: 100%|█████████████████| 2/2 [00:09<00:00,  4.79s/it]\r\nLoading LoRA weights from /home/usr/LLaVA/checkpoints\r\nMerging weights\r\nConvert to FP16...\r\nTraceback (most recent call last):\r\n  File \"/home/usr/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/usr/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/usr/LLaVA/llava/serve/cli.py\", line 124, in <module>\r\n    main(args)\r\n  File \"/home/usr/LLaVA/llava/serve/cli.py\", line 56, in main\r\n    image_tensor = process_images([image], image_processor, model.config)\r\n  File \"/home/usr/LLaVA/llava/mm_utils.py\", line 37, in process_images\r\n    return image_processor(images, return_tensors='pt')['pixel_values']\r\nTypeError: 'NoneType' object is not callable\r\n```\r\nI'm using the latest version and have installed all packages as per the instructions.\r\n\r\nI would appreciate your assistance in identifying potential causes for the error. Thanks.</BODY>\n\n<COMMENTS>\n<Comment by manas6266 at 2024-04-13T04:32:59Z>\nDid you get any solution for this?\n</Comment>\n<Comment by cyj95 at 2024-05-21T02:05:30Z>\nany solution?\n</Comment>\n<Comment by yiyiwwang at 2024-05-29T03:12:11Z>\nYou may check https://github.com/haotian-liu/LLaVA/issues/1313.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 979,
    "state": "closed",
    "created_by": "459737087",
    "created_at": "2024-01-09T05:53:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/979</URL>\n\n<TITLE>ValueError: Expected a string path to an existing deepspeed config, or a dictionary, or a base64 encoded string. Received: ./scripts/zero2.json</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n```\r\n[2024-01-09 05:12:39,036] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/site-packages/accelerate/utils/deepspeed.py\", line 52, in __init__\r\n    config_decoded = base64.urlsafe_b64decode(config_file_or_dict).decode(\"utf-8\")\r\n  File \"/usr/local/lib/python3.8/base64.py\", line 133, in urlsafe_b64decode\r\n    return b64decode(s)\r\n  File \"/usr/local/lib/python3.8/base64.py\", line 87, in b64decode\r\n    return binascii.a2b_base64(s)\r\nbinascii.Error: Incorrect padding\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"../llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/output/LLaVA/llava/train/train.py\", line 761, in train\r\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\r\n  File \"/usr/local/lib/python3.8/site-packages/transformers/hf_argparser.py\", line 338, in parse_args_into_dataclasses\r\n    obj = dtype(**inputs)\r\n  File \"<string>\", line 127, in __init__\r\n  File \"/usr/local/lib/python3.8/site-packages/transformers/training_args.py\", line 1607, in __post_init__\r\n    self.hf_deepspeed_config = HfTrainerDeepSpeedConfig(self.deepspeed)\r\n  File \"/usr/local/lib/python3.8/site-packages/transformers/deepspeed.py\", line 76, in __init__\r\n    super().__init__(config_file_or_dict)\r\n  File \"/usr/local/lib/python3.8/site-packages/transformers/deepspeed.py\", line 66, in __init__\r\n    super().__init__(config_file_or_dict)\r\n  File \"/usr/local/lib/python3.8/site-packages/accelerate/utils/deepspeed.py\", line 55, in __init__\r\n    raise ValueError(\r\nValueError: Expected a string path to an existing deepspeed config, or a dictionary, or a base64 encoded string. Received: ./scripts/zero2.json\r\n```\r\nCommand:\r\n```\r\nsh finetune_lora.sh\r\n```\r\n\r\nLog: \r\n```\r\nPROMPT_VERSION=\"llava_llama_2\"\r\nMODEL_VERSION=\"llama-2-7b-chat\"\r\n################## LLaMA-2 ##################\r\n\r\ndeepspeed ../llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --lora_enable True \\\r\n    --model_name_or_path /input1/ \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path ./playground/data/llava_instruct_80k.json \\\r\n    --image_folder /path/to/coco/train2017 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava-$MODEL_VERSION-pretrain/mm_projector.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-$MODEL_VERSION-finetune_lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --dataloader_num_workers 4 \\\r\n    --report_to wandb\r\n\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by 459737087 at 2024-01-12T03:16:21Z>\nchange the model_path name.\n</Comment>\n<Comment by conheaven at 2024-02-05T15:23:08Z>\nchange **./scripts/zero2.json** to absolute path\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 978,
    "state": "open",
    "created_by": "awzhgw",
    "created_at": "2024-01-08T08:01:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/978</URL>\n\n<TITLE>[Question]  mixtral 8x7B model need 100GB GPU memory , i want adopt llava on mixtral 8x7B ,but h800 gpu only has 80GB</TITLE>\n\n<BODY>### Question\n\n[Question]  mixtral 8x7B model need 100GB GPU memory , i want adopt llava on mixtral 8x7B ,but h800 gpu only has 80GB\r\n\r\nso, deepspeed ../scripts/zero2.json  OOM  \r\n\r\ndeepspeed  ../scripts/zero3.json can run ,but very slow ,very slow .\r\n\r\n\r\nhow resolve it ?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 977,
    "state": "open",
    "created_by": "awzhgw",
    "created_at": "2024-01-08T07:40:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/977</URL>\n\n<TITLE>[Usage] can support mixtral 8x7B model ?</TITLE>\n\n<BODY>### Describe the issue\n\n can support mixtral 8x7B model ?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 976,
    "state": "closed",
    "created_by": "bruceisme",
    "created_at": "2024-01-08T03:05:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/976</URL>\n\n<TITLE>[Question] TextVQA’s OCR</TITLE>\n\n<BODY>### Question\n\nIn ./playground/data/eval/textvqa/llava_textvqa_val_v051_ocr.jsonl, the \"text\" part of each piece of data contains the **Reference OCR token** content. May I ask where this part of OCR is obtained from?</BODY>\n\n<COMMENTS>\n<Comment by pipilurj at 2024-02-02T10:53:21Z>\nHi! I also have the same question.\n</Comment>\n<Comment by haotian-liu at 2024-02-03T05:42:54Z>\nIt comes from the TextVQA's dataset.\r\n\r\n[Rosetta OCR tokens [v0.2]](https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_Rosetta_OCR_v0.2_train.json)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 975,
    "state": "open",
    "created_by": "zephirusgit",
    "created_at": "2024-01-07T21:13:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/975</URL>\n\n<TITLE>[Feature request] Can I add an image to the OLLAMA prompt? Out of the assistant</TITLE>\n\n<BODY>### feature\n\nCan I add an image to the OLLAMA prompt?\r\n\r\nhello! , I was causing Ollama in Windows, through WSL (Windows subsystem Linux), and from there I install it, with Ollama Run Llava.\r\n\r\nLooking on the page, I see an example with curl, how to inject an image based64 I think, but I don't know curl,\r\n\r\napi example:\r\nUso de API\r\n\r\ncurl http://localhost:11434/api/generate -d '{\r\n  \"model\": \"llava\",\r\n  \"prompt\":\"What is in this picture?\",\r\n  \"images\": [\"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\"]\r\n}'\r\n\r\nHow could I execute or launch it?\r\n\r\nFor example, if I open llava, and I ask for something of an image in that folder where I am in the WSL, it works,\r\n\r\nOllama Run Llava\r\n\r\n>>> You can describe in detail that there is in the image ./sdxlturbo_example.png\r\nAdded Image './sdxlturbo_example.png'\r\nThe image shows a small fox, which is an icing species, leaving a glass or bottle. The animal is\r\nFind inside the container, and its colors shell-white-black contrast with the transparency of the\r\nglass. The composition of the image creates a magical environment that captures the spectator 'attention and allows it\r\nVisualize the adorable face of the fox while inside the container.\r\n\r\nthis is correct,\r\nMy question is if I can launch that, from outside,\r\nI have tried but ignores me, dont work , and create \"a imaginary reply\"\r\nexample:\r\n\r\n/MNT/H/IA/OLLAMA-MAIN $ OLLAMA RUN LLAVA \"You can describe in detail that animal is in the image ./sdxlturbo_example.png\"\r\n1. In the background, there is an image that shows a world of superheroes where the characters include Superman,\r\nBatman and Spiderman.\r\n2. There are several animated racing figures, which could be representations of cars, bicycles or even\r\nHorses, but I can't determine with certainty the exact way the characters move.\r\n3. In the background, there is an animation murmur and a fun atmosphere.\r\n4. There is a group of strings, which could be representations of cables or lying, but I see the form again\r\nexact in which they are connected to each other.\r\n\r\n------------------------------------------------------------\r\nIs it possible to add in the launcher, the ability to \"load the image\" and only the answer? \r\nas in llama2 that there if you take the prompt\r\n\r\nexample\r\n/mnt/h/ia/ollama-main$ ollama run llama2 {What is the capital of Argentina?}\r\n\r\nArgentina's capital is Buenos Aires.</BODY>\n\n<COMMENTS>\n<Comment by user202729 at 2025-05-05T16:30:01Z>\nRelated: https://github.com/ollama/ollama/issues/9767 .  It doesn't seem to address the issue though.\n\nMaybe try `echo \"You can describe in detail that animal is in the image ./sdxlturbo_example.png\" | ollama run llava`.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 974,
    "state": "open",
    "created_by": "benik923",
    "created_at": "2024-01-07T15:08:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/974</URL>\n\n<TITLE>Cant start controller for LLaVa \"llava' is already used by a Transformers config, pick another name\"</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nController/Webserver wont start\r\n\r\n\r\nive tried everything chatgpt and bard suggested, no results.\r\n-pip uninstalling reinstalling llava (wich is just a part of it i guess since it took mere seconds)\r\n-pip install transformers --upgrade\r\n-renamed the whole thing which lead to other errors with wrongly named files\r\n\r\ni am also very new to all of this so sorry if this is really stupid.\r\nthank you, -benik\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.controller --host 0.0.0.0 --port 10000\r\n```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 187, in _run_module_as_main\r\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 110, in _get_module_details\r\n    __import__(pkg_name)\r\n  File \"C:\\Users\\Niklas\\llava\\llava\\__init__.py\", line 1, in <module>\r\n    from .model import LlavaLlamaForCausalLM\r\n  File \"C:\\Users\\Niklas\\llava\\llava\\model\\__init__.py\", line 1, in <module>\r\n    from .language_model.llava_llama import LlavaLlamaForCausalLM, LlavaConfig\r\n  File \"C:\\Users\\Niklas\\llava\\llava\\model\\language_model\\llava_llama.py\", line 110, in <module>\r\n    AutoConfig.register(\"llava\", LlavaConfig)\r\n  File \"C:\\Users\\Niklas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\auto\\configuration_auto.py\", line 1128, in register\r\n    CONFIG_MAPPING.register(model_type, config, exist_ok=exist_ok)\r\n  File \"C:\\Users\\Niklas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\auto\\configuration_auto.py\", line 828, in register\r\n    raise ValueError(f\"'{key}' is already used by a Transformers config, pick another name.\")\r\nValueError: 'llava' is already used by a Transformers config, pick another name.\r\n```</BODY>\n\n<COMMENTS>\n<Comment by koking0 at 2024-01-08T07:08:15Z>\nme too\n</Comment>\n<Comment by benik923 at 2024-01-08T14:41:54Z>\n> me too\r\n\r\nim not quite sure but i think running this fixed it for me:\r\ngit pull\r\npip install -e .\n</Comment>\n<Comment by henningb-cf at 2024-01-22T09:33:11Z>\nWhat version of `transformers` are you using? \r\nIn the latest version, the LLaVA already exists so you could (1) install the version specified in this repo or (2) change the name for registering the config.\n</Comment>\n<Comment by aymenabid-lab at 2024-01-28T20:27:57Z>\nexplain more\n</Comment>\n<Comment by henningb-cf at 2024-01-29T08:46:03Z>\nThis line: \r\n```\r\nFile \"C:\\Users\\Niklas\\llava\\llava\\model\\language_model\\llava_llama.py\", line 110, in <module>\r\n    AutoConfig.register(\"llava\", LlavaConfig)\r\n\r\nFile \"C:\\Users\\Niklas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\auto\\configuration_auto.py\", line 828, in register\r\n    raise ValueError(f\"'{key}' is already used by a Transformers config, pick another name.\")\r\nValueError: 'llava' is already used by a Transformers config, pick another name.\r\n```\r\ntells you that you cannot register a config with the name 'llava' bc it's already used. This is the case when using `transformers > 4.35` (I think where HF added LLaVA). So check version of transformers which shoulde be [4.31.](https://github.com/haotian-liu/LLaVA/blob/main/pyproject.toml). OR you can check the line where you register to `AutoConfig.register(\"llava_org\", LlavaConfig)` or something.\r\nBest to install the right dependencies though to avoid other conflicts.\n</Comment>\n<Comment by CateMrl at 2024-02-01T11:14:40Z>\n@HB-ai-cf can you explain more in detail how to do that?\n</Comment>\n<Comment by haotian-liu at 2024-02-03T05:44:42Z>\nThe latest code base shall have this fixed. Thanks.\n</Comment>\n<Comment by aymenabid-lab at 2024-02-27T01:18:25Z>\nTransformer version is 4.37.1 \r\nHow to  fix (check the line where you register to AutoConfig.register(\"llava_org\", LlavaConfig or other solution)\r\nNo good  issue to install the 4.3.1\r\n\r\nhave another idea\n</Comment>\n<Comment by KatameRonin at 2024-03-23T18:17:55Z>\nIt is most likely because the transformer version is incompatible. Try the following:\r\n`pip list`\r\nThis will list your packages in your environment.\r\nThen use:\r\n`pip uninstall transformers`\r\nAnd then finally:\r\n`pip install transformers==4.31.0`\r\n\r\nEssentially the version shown for transformers in cog.yaml file in the llava repository. \r\nThis worked for me\n</Comment>\n<Comment by starforce at 2024-07-05T20:10:03Z>\nfor me it was:\r\n\r\nI am debugging llava locally  on my computer and did a  pip install -e .  to a  local folder  then few weeks later ,  I did a pip  install on another version of transformer. So, causes a conflict.  **so do another pip install -e . where the LLava folder is** and the problem will go away.\n</Comment>\n<Comment by garychan22 at 2024-07-29T06:24:34Z>\nmy solution is change the model_type param in the customized model config before calling any \"xxx.register(...)\", then the conflict disappears\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 973,
    "state": "open",
    "created_by": "xiechengmude",
    "created_at": "2024-01-07T07:34:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/973</URL>\n\n<TITLE>[Usage] Support to CUDA121 with torch2.1 in training process with H100 please</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\nPretrain with H100\r\n```\r\n\r\nLog: \r\n```\r\nwandb: 🚀 View run at https://wandb.ai/xdan-ai/huggingface/runs/jodixrhz\r\n  0%|          | 0/2181 [00:00<?, ?it/s]Traceback (most recent call last):\r\n  File \"/workspace/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/workspace/LLaVA/llava/train/train.py\", line 933, in train\r\n    trainer.train()\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2654, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2679, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1833, in forward\r\n    loss = self.module(*inputs, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/workspace/LLaVA/llava/model/language_model/llava_llama.py\", line 79, in forward\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/workspace/LLaVA/llava/model/llava_arch.py\", line 125, in prepare_inputs_labels_for_multimodal\r\n    raise NotImplementedError\r\nNotImplementedError\r\nTraceback (most recent call last):\r\n  File \"/workspace/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/workspace/LLaVA/llava/train/train.py\", line 933, in train\r\n    trainer.train()\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2654, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2679, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1833, in forward\r\n    loss = self.module(*inputs, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/workspace/LLaVA/llava/model/language_model/llava_llama.py\", line 79, in forward\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/workspace/LLaVA/llava/model/llava_arch.py\", line 125, in prepare_inputs_labels_for_multimodal\r\n    raise NotImplementedError\r\nTraceback (most recent call last):\r\n  File \"/workspace/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\nNotImplementedError\r\n    train()\r\n  File \"/workspace/LLaVA/llava/train/train.py\", line 933, in train\r\n    trainer.train()\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2654, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2679, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1833, in forward\r\n    loss = self.module(*inputs, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/workspace/LLaVA/llava/model/language_model/llava_llama.py\", line 79, in forward\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/workspace/LLaVA/llava/model/llava_arch.py\", line 125, in prepare_inputs_labels_for_multimodal\r\nTraceback (most recent call last):\r\n  File \"/workspace/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    raise NotImplementedError\r\nNotImplementedError\r\n    train()\r\n  File \"/workspace/LLaVA/llava/train/train.py\", line 933, in train\r\n    trainer.train()\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2654, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2679, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1833, in forward\r\n    loss = self.module(*inputs, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/workspace/LLaVA/llava/model/language_model/llava_llama.py\", line 79, in forward\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/workspace/LLaVA/llava/model/llava_arch.py\", line 125, in prepare_inputs_labels_for_multimodal\r\n    raise NotImplementedError\r\nNotImplementedError\r\nTraceback (most recent call last):\r\n  File \"/workspace/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/workspace/LLaVA/llava/train/train.py\", line 933, in train\r\n    trainer.train()\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2654, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2679, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1833, in forward\r\n    loss = self.module(*inputs, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/workspace/LLaVA/llava/model/language_model/llava_llama.py\", line 79, in forward\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/workspace/LLaVA/llava/model/llava_arch.py\", line 125, in prepare_inputs_labels_for_multimodal\r\n    raise NotImplementedError\r\nNotImplementedError\r\nTraceback (most recent call last):\r\n  File \"/workspace/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/workspace/LLaVA/llava/train/train.py\", line 933, in train\r\n    trainer.train()\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2654, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2679, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1833, in forward\r\n    loss = self.module(*inputs, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/workspace/LLaVA/llava/model/language_model/llava_llama.py\", line 79, in forward\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/workspace/LLaVA/llava/model/llava_arch.py\", line 125, in prepare_inputs_labels_for_multimodal\r\n    raise NotImplementedError\r\nNotImplementedError\r\nTraceback (most recent call last):\r\n  File \"/workspace/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/workspace/LLaVA/llava/train/train.py\", line 933, in train\r\n    trainer.train()\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2654, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2679, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1833, in forward\r\n    loss = self.module(*inputs, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/workspace/LLaVA/llava/model/language_model/llava_llama.py\", line 79, in forward\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/workspace/LLaVA/llava/model/llava_arch.py\", line 125, in prepare_inputs_labels_for_multimodal\r\n    raise NotImplementedError\r\nNotImplementedError\r\nTraceback (most recent call last):\r\n  File \"/workspace/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/workspace/LLaVA/llava/train/train.py\", line 933, in train\r\n    trainer.train()\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2654, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2679, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1833, in forward\r\n    loss = self.module(*inputs, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/workspace/LLaVA/llava/model/language_model/llava_llama.py\", line 79, in forward\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/workspace/LLaVA/llava/model/llava_arch.py\", line 125, in prepare_inputs_labels_for_multimodal\r\n    raise NotImplementedError\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 972,
    "state": "open",
    "created_by": "valencebond",
    "created_at": "2024-01-06T01:20:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/972</URL>\n\n<TITLE>[Question] why use '\\n' as the sep in conv_llava_plain, is there some reference?</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 971,
    "state": "open",
    "created_by": "AI-Aether",
    "created_at": "2024-01-05T12:56:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/971</URL>\n\n<TITLE>[Question] How to publish the fine-tuned model to hugging face after merging the weights?</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 970,
    "state": "open",
    "created_by": "AI-Aether",
    "created_at": "2024-01-05T09:24:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/970</URL>\n\n<TITLE>[Question] After fine-tuning the LLavA model using the script finetune_lora_task.sh. How do I check the inference of the model.</TITLE>\n\n<BODY>### Question\n\nAfter using finetune_lora_task.sh I stored everything under /Output directory. How do I check the inference of this fine-tuned model?</BODY>\n\n<COMMENTS>\n<Comment by haoyu-x at 2024-01-14T07:43:40Z>\nhi @AI-Aether have you figured this out?\n</Comment>\n<Comment by Ryosuke0104 at 2024-01-19T11:00:08Z>\nI'm curious too.\n</Comment>\n<Comment by cherry956 at 2024-01-29T10:26:35Z>\n@AI-Aether  can you describe how to run the finetune_lora_task.sh? I have made my own dataset.\r\nThank\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 969,
    "state": "closed",
    "created_by": "ahmedhshahin",
    "created_at": "2024-01-04T15:18:35Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/969</URL>\n\n<TITLE>[Usage] Errors when using Zero3</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nCommand:\r\n```\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    ...\r\n```\r\n\r\nLog: \r\nI am trying to finetune the models using lora and zero 3 on 4 GPUs. However, when I start the experiment on multiple GPUs, training freezes from the beginning. When I try with a single GPU, it raises an error after the first iteration, check below:\r\n```\r\n  0%|                                                                                                                                                                             | 0/32300 [00:00<?, ?it/s]/home/ashahin/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n  warnings.warn(\r\nInvalidate trace cache @ step 414: expected module 2612, but got module 2248\r\n{'loss': 3.623, 'learning_rate': 1.0319917440660473e-06, 'epoch': 0.0}                                                                                                                                      \r\n  0%|                                                                                                                                                                 | 1/32300 [00:17<153:24:26, 17.10s/it]Invalidate trace cache @ step 414: expected module 2612, but got module 2248\r\nInvalidate trace cache @ step 414: expected module 2248, but got module 2612\r\nTraceback (most recent call last):\r\n  File \"/home/ashahin/codes/bakllava/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/home/ashahin/codes/bakllava/llava/train/train.py\", line 1094, in train\r\n    trainer.train()\r\n  File \"/home/ashahin/.local/lib/python3.9/site-packages/transformers/trainer.py\", line 1591, in train\r\n    return inner_training_loop(\r\n  File \"/home/ashahin/.local/lib/python3.9/site-packages/transformers/trainer.py\", line 1892, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/home/ashahin/.local/lib/python3.9/site-packages/transformers/trainer.py\", line 2787, in training_step\r\n    self.accelerator.backward(loss)\r\n  File \"/home/ashahin/.local/lib/python3.9/site-packages/accelerate/accelerator.py\", line 1847, in backward\r\n    self.deepspeed_engine_wrapped.backward(loss, **kwargs)\r\n  File \"/home/ashahin/.local/lib/python3.9/site-packages/accelerate/utils/deepspeed.py\", line 167, in backward\r\n    self.engine.backward(loss, **kwargs)\r\n  File \"/home/ashahin/.local/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/home/ashahin/.local/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 1955, in backward\r\n    self.optimizer.backward(loss, retain_graph=retain_graph)\r\n  File \"/home/ashahin/.local/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/home/ashahin/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/stage3.py\", line 2137, in backward\r\n    self._get_param_coordinator(training=True).reset_step()\r\n  File \"/home/ashahin/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 203, in reset_step\r\n    raise RuntimeError(f\"still have inflight params \"\r\nRuntimeError: still have inflight params [{'id': 743, 'status': 'AVAILABLE', 'numel': 2097152, 'ds_numel': 2097152, 'shape': (1024, 1, 8, 16, 16), 'ds_shape': (1024, 1, 8, 16, 16), 'requires_grad': False, 'grad_shape': None, 'persist': False, 'active_sub_modules': set(), 'ds_tensor.shape': torch.Size([2097152])}, {'id': 749, 'status': 'AVAILABLE', 'numel': 1048576, 'ds_numel': 1048576, 'shape': (1024, 1024), 'ds_shape': (1024, 1024), 'requires_grad': False, 'grad_shape': None, 'persist': False, 'active_sub_modules': set(), 'ds_tensor.shape': torch.Size([1048576])}, {'id': 753, 'status': 'AVAILABLE', 'numel': 1048576, 'ds_numel': 1048576, 'shape': (1024, 1024), 'ds_shape': (1024, 1024), 'requires_grad': False, 'grad_shape': None, 'persist': False, 'active_sub_modules': set(), 'ds_tensor.shape': torch.Size([1048576])}, {'id': 757, 'status': 'AVAILABLE', 'numel': 4194304, 'ds_numel': 4194304, 'shape': (4096, 1024), 'ds_shape': (4096, 1024), 'requires_grad': False, 'grad_shape': None, 'persist': False, 'active_sub_modules': set(), 'ds_tensor.shape': torch.Size([4194304])}, {'id': 747, 'status': 'AVAILABLE', 'numel': 1048576, 'ds_numel': 1048576, 'shape': (1024, 1024), 'ds_shape': (1024, 1024), 'requires_grad': False, 'grad_shape': None, 'persist': False, 'active_sub_modules': set(), 'ds_tensor.shape': torch.Size([1048576])}, {'id': 751, 'status': 'AVAILABLE', 'numel': 1048576, 'ds_numel': 1048576, 'shape': (1024, 1024), 'ds_shape': (1024, 1024), 'requires_grad': False, 'grad_shape': None, 'persist': False, 'active_sub_modules': set(), 'ds_tensor.shape': torch.Size([1048576])}]\r\n```\r\n\r\nAny idea how to solve this?</BODY>\n\n<COMMENTS>\n<Comment by ahmedhshahin at 2024-01-08T21:57:28Z>\nClosing the issue as the problem has been solved by reviewing other parts of my code and it's not related to llava.\n</Comment>\n<Comment by zhyhome at 2024-03-13T03:22:57Z>\n> Closing the issue as the problem has been solved by updating other packages in my environment and it's not related to llava.\r\n\r\n**How did you solve it?**\n</Comment>\n<Comment by BlueBlueFF at 2024-04-07T08:40:12Z>\n> Closing the issue as the problem has been solved by updating other packages in my environment and it's not related to llava.\r\n\r\n\r\nSame question How do you fix it?\n</Comment>\n<Comment by BlueBlueFF at 2024-04-07T08:40:17Z>\n> > Closing the issue as the problem has been solved by updating other packages in my environment and it's not related to llava.\r\n> \r\n> **How did you solve it?**\r\n\r\n\r\nSame question How do you fix it?\n</Comment>\n<Comment by ahmedhshahin at 2024-04-07T15:39:58Z>\nIn my code, I had a for loop that I suspected was the issue when using zero 3. Number of iterations in the loop varied depending on each training sample, so some processes finishes while the others are waiting. Fixing the for loop to have the same number of iterations across all samples solved it. Not sure if this makes sense but it was enough to get my code to run.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 968,
    "state": "open",
    "created_by": "yxl23",
    "created_at": "2024-01-04T06:27:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/968</URL>\n\n<TITLE>ValueError: 'llava' is already used by a Transformers config, pick another name.</TITLE>\n\n<BODY>### Describe the issue\n\nValueError: 'llava' is already used by a Transformers config, pick another name.</BODY>\n\n<COMMENTS>\n<Comment by koking0 at 2024-01-08T07:08:28Z>\nme too\n</Comment>\n<Comment by yangyangtaizi at 2024-01-12T09:17:38Z>\noh!me too!if you solve it,please share with me!thank you so much\n</Comment>\n<Comment by koking0 at 2024-01-15T01:46:19Z>\nnew version transformers can not load old LLaVA model.\r\n\r\n[https://github.com/huggingface/transformers/issues/28404](https://github.com/huggingface/transformers/issues/28404)\n</Comment>\n<Comment by Dongping-Chen at 2024-01-15T18:36:05Z>\n> oh!me too!if you solve it,please share with me!thank you so much\r\n\r\npip install transformers==4.34.0 solve my problem\n</Comment>\n<Comment by guogsq at 2024-08-24T07:40:31Z>\nI meet it, too. I solve it by:\r\nIn LLaVA/llava/model/language_model/llava_llama.py:\r\nreplace \"AutoConfig.register(\"llava\", LlavaConfig)\" with \"AutoConfig.register(\"llava_custom\", LlavaConfig)\". \r\nAnd then find the LlavaConfig class definition, it is in the same file, replace \" model_type = \"llava\" “ with \" model_type = \"llava_custom\" “\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 965,
    "state": "closed",
    "created_by": "xiaoachen98",
    "created_at": "2024-01-03T05:27:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/965</URL>\n\n<TITLE>[Question] Do you consider supporting MMMU benchmark?</TITLE>\n\n<BODY>### Question\n\nThanks to your efforts for the LMM community. Would you consider supporting the comprehensive [MMMU benchmark](https://mmmu-benchmark.github.io/)?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 964,
    "state": "open",
    "created_by": "shakedpe",
    "created_at": "2024-01-02T14:13:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/964</URL>\n\n<TITLE>[Question] How to recreate SEED benchmark results?</TITLE>\n\n<BODY>### Question\n\nIm looking at the script under \"scripts/v1_5/eval/seed.sh\" and the is no \"./playground/data/eval/seed_bench\" data to use in order to run script. Also when looking at the code you run \"model_vqa_loader.py\" is it unclear when you generate an answer how you use the questions +answers to get a final A/B/C/D answer in order to get you accuracy results. Can you please explain how to recreate the results? \r\nThanks.</BODY>\n\n<COMMENTS>\n<Comment by simon-lund at 2024-02-09T01:01:43Z>\nHello, \r\n\r\nyou have to download the Seed Bench data yourself (seed-bench v1). Instructions on how to set up the different benchmarks for evaluation can be found here: https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md\r\n\r\nOnce you have set up the evaluation, you will find a file `llava-seed-bench.jsonl` under `./playground/data/eval/seed_bench`. This file contains the test inputs (question + possible answers + instructions). \r\n\r\nExample:\r\n```json\r\n{\r\n  \"image\": \"SEED-Bench-image/1454426_2591111986\", \r\n  \"text\": \"How many towels are in the image?\\nA. One\\nB. Two\\nC. Three\\nD. Four\\nAnswer with the option's letter from the given choices directly.\", \r\n  \"category\": \"Instances Counting\", \r\n  \"question_id\": 101669}\r\n```\r\n\r\nThus, for each input the model only has to choose the letter corresponding to the correct answer as instructed.\r\nIn other words, the task is set up as VQA. Finally, the code snippet you are referring to is only used for generating the answers. Evaluation is done separately, see code at the end of the script:\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/28b96e296d980b92e0810a5d1d00f8f1e988be85/scripts/v1_5/eval/seed.sh#L34-L38\r\n\r\nI hope this helps.\n</Comment>\n<Comment by NimrodShabtay at 2024-04-03T12:11:56Z>\nHi, @simon-lund \r\nIs there an easy way to evaluate just the image questions' types?\r\nThanks\n</Comment>\n<Comment by simon-lund at 2024-04-03T12:36:02Z>\nSorry, I don't know.\r\n\r\nIt's been a while since I last used it.\n</Comment>\n<Comment by HuangChiEn at 2025-04-07T06:35:17Z>\n> Hi, [@simon-lund](https://github.com/simon-lund) Is there an easy way to evaluate just the image questions' types? Thanks\n\nyes, the script will automatically skip the video part, if your video image doesn't properly placed at the indicated location. So, you just need to prepare the SEED image part then run the script, it will give you the image part results.\n(but a simple hot-fix for py script is needed)\n\n![Image](https://github.com/user-attachments/assets/d3095157-cca4-4194-9b9a-c0d949803123)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 963,
    "state": "closed",
    "created_by": "anonymous-atom",
    "created_at": "2024-01-02T14:01:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/963</URL>\n\n<TITLE>[Question] Evaluation On Custom Data</TITLE>\n\n<BODY>### Question\r\n\r\nI have finetuned LLaVA on custom data and now I want to evaluate finetuned LLaVA model. I tried to look into the docs, but seems confusing to me right now. Can anyone point me how can I evaluate the LLaVA finetuned on custom data to also evaluate on our own custom data.\r\n\r\nIn below attached image what is that jsonl format they are referring to ?\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/74659873/c8a76c24-9fe9-4c62-a362-6d51cc1ffd48)</BODY>\n\n<COMMENTS>\n<Comment by anonymous-atom at 2024-01-03T07:53:35Z>\nNow I am getting below error when I run ```model_vqa.py```\r\nHere's the command I executed: ```python llava/eval/model_vqa.py --model-path llava-v1.5 --question-file output.jsonl --image-folder selected_images --answers-file llava/answer.jsonl```\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/74659873/d4c1c80a-037e-4d77-9d5e-a0e50fb44ef2)\n</Comment>\n<Comment by Ryosuke0104 at 2024-01-20T07:52:40Z>\n@anonymous-atom \r\nHi, I want to evaluate my finetuned model.\r\nHow did you evaluate it?\r\nThank you in advance.\n</Comment>\n<Comment by anonymous-atom at 2024-01-20T08:29:01Z>\n@Ryosuke0104 \r\nFirst I sampled some test data/questions then used this script: [model_vqa.py](https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/model_vqa.py)\r\n\r\nThen this script to evaluate LLaVA response against correct answers using this: [eval_gpt_review.py](https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/eval_gpt_review.py)\r\n\r\nThis is what I remember from last month.\r\n\r\nLet me know if you are having some issues, I will post my workflow then.\r\n\r\nAlso do Make sure to have files in correct format, that's where I was having issues initially\n</Comment>\n<Comment by Ryosuke0104 at 2024-01-20T08:56:27Z>\n@anonymous-atom \r\nThank you a lot for your kindness!!\r\n\r\nI'm sorry for the way I asked my question.\r\nI will explain my situation below.\r\n\r\nSituation\r\n1. First, I did a LoRA fine-tuning with Custom data using this file: [finetune_lora.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_lora.sh)\r\n2. Then, I tried to evaluate the fine-tuned model as you did using [model_vqa.py](https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/model_vqa.py).\r\n3.  However, in the following code, I don't know what to specify for \"args.model_path\" and \"args.model_base\" in the following code, and I have tried many things with no good results (some errors occurred.)\r\n\r\nIn [model_vqa.py](https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/model_vqa.py)\r\n```\r\nmodel_path = os.path.expanduser(args.model_path) # I don't know what path is correct.\r\nmodel_name = get_model_name_from_path(model_path) # I don't know what path is correct, either.\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(model_path, args.model_base, model_name)\r\n```\r\n\r\nI found [merge_lora_weights.py](https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md#:~:text=Create%20Merged%20Checkpoints), but I didn't know how to use it.\r\n\r\nIf I do fine-tune LLaVA using this code:[finetune_lora.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_lora.sh), could you tell me how to do an evaluation?\n</Comment>\n<Comment by anonymous-atom at 2024-01-20T09:31:40Z>\nOh that's fine and thanks for explaining you situation. I also went through the exact same process.\r\n\r\nLet me break down into steps\r\n\r\n\r\n## 1. LoRA Fine-tune\r\n\r\nPerform the LoRA fine-tuning process.\r\n\r\n## 2. Merging LoRA Weights\r\n\r\nAfter fine-tuning, you can merge LoRA weights using the provided script:\r\n\r\n- [`merge_lora_weights.py`](https://github.com/haotian-liu/LLaVA/blob/main/scripts/merge_lora_weights.py)\r\n\r\nUse the script as below:\r\n\r\n`python merge_lora_weights.py --model-path <path to LoRA fine-tuned model folder> --model-base <base version of LLM> --save-model-path <path to save merged weights>`\r\n\r\n## 3. Now simply use the evaluation scripts as I mentioned earlier.\r\n\r\nYou can refer here for Checking the modal base: [MODEL_ZOO.md](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md)\r\n\r\nFor example LLaVA-1.5 13B uses Vicuna v1.5 as base model: [lmsys/vicuna-13b-v1.5](https://huggingface.co/lmsys/vicuna-13b-v1.5)\r\n\r\nSo simply use --model-base lmsys/vicuna-13b-v1.5 if you are finetuning LLaVA 1.5\n</Comment>\n<Comment by Ryosuke0104 at 2024-01-20T09:42:29Z>\nThank you a lot!!\r\n\r\nThat is, \r\n```\r\nmodel_path = os.path.expanduser(args.model_path)\r\n```\r\nis \r\n--model_path `<path to LoRA fine-tuned model folder> `\r\n\r\nright?\r\n\r\nIn that case,\r\nwhen I actually conduct model.generate(),\r\n```\r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n```\r\nThis error occurred. \r\nHave you ever experienced this error?\n</Comment>\n<Comment by anonymous-atom at 2024-01-20T09:46:45Z>\nYes right that's the model_path.\r\n\r\nNo. I didn't encountered that error. Might be issue with the GPU's you are using.\r\nCan you provide the script you ran and the directory structure of that you passed to --model_path ?\n</Comment>\n<Comment by Ryosuke0104 at 2024-01-20T09:58:12Z>\n> Yes right that's the model_path.\r\n\r\nThank you!!\r\n\r\nIn fact, we are fine-tuning at 7B instead of 13B due to lack of computing resources, and write fine_lora.sh as follows\r\n```\r\n#!/bin/bash\r\n\r\ndeepspeed --include=\"localhost:1,2\" llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3_offload.json \\\r\n    --model_name_or_path lmsys/vicuna-7b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/guesswhat/guesswhat_train_2.json \\\r\n    --image_folder ./data/guesswhat/bbox_images \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-7b-pretrain/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n\r\n```\r\n\r\n> provide the script you ran\r\n\r\nAnd I ran this script\r\n```\r\nimport os\r\ndevice_ids = \"2\"\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=device_ids\r\nprint(f\"Setting CUDA Number {device_ids}\")\r\n\r\nimport argparse\r\nimport torch\r\n\r\nfrom llava.constants import (\r\n    IMAGE_TOKEN_INDEX,\r\n    DEFAULT_IMAGE_TOKEN,\r\n    DEFAULT_IM_START_TOKEN,\r\n    DEFAULT_IM_END_TOKEN,\r\n    IMAGE_PLACEHOLDER,\r\n)\r\nfrom llava.conversation import conv_templates, SeparatorStyle\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.utils import disable_torch_init\r\nfrom llava.mm_utils import (\r\n    process_images,\r\n    tokenizer_image_token,\r\n    get_model_name_from_path,\r\n    KeywordsStoppingCriteria,\r\n)\r\n\r\nfrom PIL import Image\r\n\r\nimport requests\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\nimport re\r\n\r\ndef image_parser(args):\r\n    out = args.image_file.split(args.sep)\r\n    return out\r\n\r\n\r\ndef load_image(image_file):\r\n    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\r\n        response = requests.get(image_file)\r\n        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\r\n    else:\r\n        image = Image.open(image_file).convert(\"RGB\")\r\n    return image\r\n\r\ndef load_images(image_files):\r\n    out = []\r\n    for image_file in image_files:\r\n        image = load_image(image_file)\r\n        out.append(image)\r\n    return out\r\n\r\nmodel_path = \"/home/projects/checkpoints/llava-v1.5-7b-lora\"\r\n# model_path = \"liuhaotian/llava-v1.5-7b\"\r\n\r\nmodel_base = 'liuhaotian/llava-v1.5-7b'\r\nmodel_name = get_model_name_from_path(model_path=model_path)\r\nprint(f\"model_name: {model_name}\")\r\n\r\ndisable_torch_init()\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    model_path, model_base, model_name\r\n)\r\n\r\n\r\ndef inference_llava(prompt: str, image_file: str):\r\n    \r\n    args = type('Args', (), {\r\n        \"model_path\": model_path,\r\n        \"model_base\": model_base,\r\n        \"model_name\": model_name,\r\n        \"query\": prompt,\r\n        \"conv_mode\": None,\r\n        \"image_file\": image_file,\r\n        \"sep\": \",\",\r\n        \"temperature\":0.1,\r\n        \"top_p\": 0.8,\r\n        \"num_beams\":1,\r\n        \"max_new_tokens\": 200,\r\n    })()\r\n\r\n    qs = args.query\r\n    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\r\n    if IMAGE_PLACEHOLDER in qs:\r\n        if model.config.mm_use_im_start_end:\r\n            qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\r\n        else:\r\n            qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\r\n    else:\r\n        if model.config.mm_use_im_start_end:\r\n            qs = image_token_se + \"\\n\" + qs\r\n        else:\r\n            qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\r\n\r\n    if \"llama-2\" in model_name.lower():\r\n        conv_mode = \"llava_llama_2\"\r\n    elif \"v1\" in model_name.lower():\r\n        conv_mode = \"llava_v1\"\r\n    elif \"mpt\" in model_name.lower():\r\n        conv_mode = \"mpt\"\r\n    else:\r\n        conv_mode = \"llava_v0\"\r\n\r\n    if args.conv_mode is not None and conv_mode != args.conv_mode:\r\n        print(\r\n            \"[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}\".format(\r\n                conv_mode, args.conv_mode, args.conv_mode\r\n            )\r\n        )\r\n    else:\r\n        args.conv_mode = conv_mode\r\n\r\n    conv = conv_templates[args.conv_mode].copy()\r\n    conv.append_message(conv.roles[0], qs)\r\n    conv.append_message(conv.roles[1], None)\r\n\r\n    prompt = conv.get_prompt()\r\n\r\n    image_files = image_parser(args)\r\n    images = load_images(image_files)\r\n    images_tensor = process_images(\r\n        images,\r\n        image_processor,\r\n        model.config\r\n    ).to(model.device, dtype=torch.float16)\r\n\r\n    input_ids = (\r\n        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\r\n        .unsqueeze(0)\r\n        .cuda()\r\n    )\r\n\r\n    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\r\n    keywords = [stop_str]\r\n    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\r\n\r\n    with torch.inference_mode():\r\n        output_ids = model.generate(\r\n            input_ids,\r\n            images=images_tensor,\r\n            do_sample=True if args.temperature > 0 else False,\r\n            temperature=args.temperature,\r\n            top_p=args.top_p,\r\n            num_beams=args.num_beams,\r\n            max_new_tokens=args.max_new_tokens,\r\n            use_cache=True,\r\n            stopping_criteria=[stopping_criteria],\r\n        )\r\n\r\n    input_token_len = input_ids.shape[1]\r\n    n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\r\n    if n_diff_input_output > 0:\r\n        print(\r\n            f\"[Warning] {n_diff_input_output} output_ids are not the same as the input_ids\"\r\n        )\r\n    outputs = tokenizer.batch_decode(\r\n        output_ids[:, input_token_len:], skip_special_tokens=True\r\n    )[0]\r\n    outputs = outputs.strip()\r\n    if outputs.endswith(stop_str):\r\n        outputs = outputs[: -len(stop_str)]\r\n    outputs = outputs.strip()\r\n    \r\n    return outputs\r\n\r\nprompt = \"Do you like this image?\"\r\nimage_file = \"https://llava-vl.github.io/static/images/view.jpg\"\r\noutput = inference_llava(prompt=prompt, image_file=image_file)\r\n\r\nprint(output)\r\n```\r\n\r\n\r\n> the directory structure of that you passed to --model_path ?\r\n\r\nDirectory is like this\r\n![スクリーンショット 2024-01-20 18 52 40](https://github.com/haotian-liu/LLaVA/assets/79626452/670b47ed-062a-47ec-9e73-d49e12de2837)\r\n\r\nI'm thinking it may be due to `DEFAULT_IMAGE_TOKEN = -200`, but I'm not sure what the solution is....\n</Comment>\n<Comment by anonymous-atom at 2024-01-20T10:05:40Z>\nWhich script you encountered that error ?\r\nthis  [scripts/merge_lora_weights.py](https://github.com/haotian-liu/LLaVA/blob/main/scripts/merge_lora_weights.py) ?\n</Comment>\n<Comment by Ryosuke0104 at 2024-01-20T10:10:18Z>\n> Which script you encountered that error ?\r\nthis [scripts/merge_lora_weights.py](https://github.com/haotian-liu/LLaVA/blob/main/scripts/merge_lora_weights.py) ?\r\n\r\nWhen I ran this script \r\n```\r\nimport os\r\ndevice_ids = \"2\"\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=device_ids\r\nprint(f\"Setting CUDA Number {device_ids}\")\r\n\r\nimport argparse\r\nimport torch\r\n\r\nfrom llava.constants import (\r\n    IMAGE_TOKEN_INDEX,\r\n    DEFAULT_IMAGE_TOKEN,\r\n    DEFAULT_IM_START_TOKEN,\r\n    DEFAULT_IM_END_TOKEN,\r\n    IMAGE_PLACEHOLDER,\r\n)\r\nfrom llava.conversation import conv_templates, SeparatorStyle\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.utils import disable_torch_init\r\nfrom llava.mm_utils import (\r\n    process_images,\r\n    tokenizer_image_token,\r\n    get_model_name_from_path,\r\n    KeywordsStoppingCriteria,\r\n)\r\n\r\nfrom PIL import Image\r\n\r\nimport requests\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\nimport re\r\n\r\ndef image_parser(args):\r\n    out = args.image_file.split(args.sep)\r\n    return out\r\n\r\n\r\ndef load_image(image_file):\r\n    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\r\n        response = requests.get(image_file)\r\n        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\r\n    else:\r\n        image = Image.open(image_file).convert(\"RGB\")\r\n    return image\r\n\r\ndef load_images(image_files):\r\n    out = []\r\n    for image_file in image_files:\r\n        image = load_image(image_file)\r\n        out.append(image)\r\n    return out\r\n\r\nmodel_path = \"/home/projects/checkpoints/llava-v1.5-7b-lora\"\r\n# model_path = \"liuhaotian/llava-v1.5-7b\"\r\n\r\nmodel_base = 'liuhaotian/llava-v1.5-7b'\r\nmodel_name = get_model_name_from_path(model_path=model_path)\r\nprint(f\"model_name: {model_name}\")\r\n\r\ndisable_torch_init()\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    model_path, model_base, model_name\r\n)\r\n\r\n\r\ndef inference_llava(prompt: str, image_file: str):\r\n    \r\n    args = type('Args', (), {\r\n        \"model_path\": model_path,\r\n        \"model_base\": model_base,\r\n        \"model_name\": model_name,\r\n        \"query\": prompt,\r\n        \"conv_mode\": None,\r\n        \"image_file\": image_file,\r\n        \"sep\": \",\",\r\n        \"temperature\":0.1,\r\n        \"top_p\": 0.8,\r\n        \"num_beams\":1,\r\n        \"max_new_tokens\": 200,\r\n    })()\r\n\r\n    qs = args.query\r\n    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\r\n    if IMAGE_PLACEHOLDER in qs:\r\n        if model.config.mm_use_im_start_end:\r\n            qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\r\n        else:\r\n            qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\r\n    else:\r\n        if model.config.mm_use_im_start_end:\r\n            qs = image_token_se + \"\\n\" + qs\r\n        else:\r\n            qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\r\n\r\n    if \"llama-2\" in model_name.lower():\r\n        conv_mode = \"llava_llama_2\"\r\n    elif \"v1\" in model_name.lower():\r\n        conv_mode = \"llava_v1\"\r\n    elif \"mpt\" in model_name.lower():\r\n        conv_mode = \"mpt\"\r\n    else:\r\n        conv_mode = \"llava_v0\"\r\n\r\n    if args.conv_mode is not None and conv_mode != args.conv_mode:\r\n        print(\r\n            \"[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}\".format(\r\n                conv_mode, args.conv_mode, args.conv_mode\r\n            )\r\n        )\r\n    else:\r\n        args.conv_mode = conv_mode\r\n\r\n    conv = conv_templates[args.conv_mode].copy()\r\n    conv.append_message(conv.roles[0], qs)\r\n    conv.append_message(conv.roles[1], None)\r\n\r\n    prompt = conv.get_prompt()\r\n\r\n    image_files = image_parser(args)\r\n    images = load_images(image_files)\r\n    images_tensor = process_images(\r\n        images,\r\n        image_processor,\r\n        model.config\r\n    ).to(model.device, dtype=torch.float16)\r\n\r\n    input_ids = (\r\n        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\r\n        .unsqueeze(0)\r\n        .cuda()\r\n    )\r\n\r\n    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\r\n    keywords = [stop_str]\r\n    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\r\n\r\n    with torch.inference_mode():\r\n        output_ids = model.generate(\r\n            input_ids,\r\n            images=images_tensor,\r\n            do_sample=True if args.temperature > 0 else False,\r\n            temperature=args.temperature,\r\n            top_p=args.top_p,\r\n            num_beams=args.num_beams,\r\n            max_new_tokens=args.max_new_tokens,\r\n            use_cache=True,\r\n            stopping_criteria=[stopping_criteria],\r\n        )\r\n\r\n    input_token_len = input_ids.shape[1]\r\n    n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\r\n    if n_diff_input_output > 0:\r\n        print(\r\n            f\"[Warning] {n_diff_input_output} output_ids are not the same as the input_ids\"\r\n        )\r\n    outputs = tokenizer.batch_decode(\r\n        output_ids[:, input_token_len:], skip_special_tokens=True\r\n    )[0]\r\n    outputs = outputs.strip()\r\n    if outputs.endswith(stop_str):\r\n        outputs = outputs[: -len(stop_str)]\r\n    outputs = outputs.strip()\r\n    \r\n    return outputs\r\n\r\nprompt = \"Do you like this image?\"\r\nimage_file = \"https://llava-vl.github.io/static/images/view.jpg\"\r\noutput = inference_llava(prompt=prompt, image_file=image_file)\r\n\r\nprint(output)\r\n```\r\n\r\nThank you for your kind attention to detail!\r\nThank you so much for your dedicated help with the error handling.\n</Comment>\n<Comment by anonymous-atom at 2024-01-20T10:13:44Z>\nFirst try to merge then LoRA weights using [scripts/merge_lora_weights.py](https://github.com/haotian-liu/LLaVA/blob/main/scripts/merge_lora_weights.py) then use that script.\n</Comment>\n<Comment by Ryosuke0104 at 2024-01-20T10:16:43Z>\nSure, I will do it.\n</Comment>\n<Comment by anonymous-atom at 2024-01-20T10:17:19Z>\nYeah cool!\r\nseems like you working on some research ;) ?\n</Comment>\n<Comment by Ryosuke0104 at 2024-01-20T10:18:35Z>\nI'm running this command and waiting now.\r\n```\r\npython scripts/merge_lora_weights.py --model-path /home/projects/checkpoints/llava-v1.5-7b-lora --model-base liuhaotian/llava-v1.5-7b --save-model-path /home/projects/checkpoints/llava-v1.5-7b-lora-merge\r\n```\r\n\r\n> seems like you working on some research ;) ?\r\n\r\nYes!!\n</Comment>\n<Comment by anonymous-atom at 2024-01-20T10:19:35Z>\nYeah, it will take some time.\r\n\r\nOh that's cool I am too working on some research.\n</Comment>\n<Comment by anonymous-atom at 2024-01-20T10:22:03Z>\nOh wait seems like you passed wrong model base that should be this: lmsys/vicuna-7b-v1.5 in your case.\n</Comment>\n<Comment by Ryosuke0104 at 2024-01-20T10:23:21Z>\nOh, thank you!!\r\nI'm trying again.\r\n```\r\npython scripts/merge_lora_weights.py --model-path /home/projects/checkpoints/llava-v1.5-7b-lora --model-base lmsys/vicuna-7b-v1.5 --save-model-path /home/projects/checkpoints/llava-v1.5-7b-lora-merge\r\n```\n</Comment>\n<Comment by Ryosuke0104 at 2024-01-20T10:25:56Z>\n> Oh that's cool I am too working on some research.\r\n\r\nWow nice, I am doing research in the field of visual dialogue\n</Comment>\n<Comment by anonymous-atom at 2024-01-20T10:27:40Z>\n> > Oh that's cool I am too working on some research.\r\n> \r\n> Wow nice, I am doing research in the field of visual dialogue\r\n\r\nWoooh I am too working on same topic, you can reachout to me on my email address listed on my github profile. Will be happy to work along!\n</Comment>\n<Comment by Ryosuke0104 at 2024-01-20T10:32:58Z>\nI ran this https://github.com/haotian-liu/LLaVA/issues/963#issuecomment-1902058381\r\n\r\nand then ran this script,\r\n```\r\nimport os\r\ndevice_ids = \"2\"\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=device_ids\r\nprint(f\"Setting CUDA Number {device_ids}\")\r\n\r\nimport argparse\r\nimport torch\r\n\r\nfrom llava.constants import (\r\n    IMAGE_TOKEN_INDEX,\r\n    DEFAULT_IMAGE_TOKEN,\r\n    DEFAULT_IM_START_TOKEN,\r\n    DEFAULT_IM_END_TOKEN,\r\n    IMAGE_PLACEHOLDER,\r\n)\r\nfrom llava.conversation import conv_templates, SeparatorStyle\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.utils import disable_torch_init\r\nfrom llava.mm_utils import (\r\n    process_images,\r\n    tokenizer_image_token,\r\n    get_model_name_from_path,\r\n    KeywordsStoppingCriteria,\r\n)\r\n\r\nfrom PIL import Image\r\n\r\nimport requests\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\nimport re\r\n\r\ndef image_parser(args):\r\n    out = args.image_file.split(args.sep)\r\n    return out\r\n\r\n\r\ndef load_image(image_file):\r\n    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\r\n        response = requests.get(image_file)\r\n        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\r\n    else:\r\n        image = Image.open(image_file).convert(\"RGB\")\r\n    return image\r\n\r\ndef load_images(image_files):\r\n    out = []\r\n    for image_file in image_files:\r\n        image = load_image(image_file)\r\n        out.append(image)\r\n    return out\r\n\r\nmodel_path = \"/home/projects/checkpoints/llava-v1.5-7b-lora\"\r\n# model_path = \"lmsys/vicuna-7b-v1.5\"\r\n\r\nmodel_base = 'liuhaotian/llava-v1.5-7b'\r\nmodel_name = get_model_name_from_path(model_path=model_path)\r\nprint(f\"model_name: {model_name}\")\r\n\r\ndisable_torch_init()\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    model_path, model_base, model_name\r\n)\r\n\r\n\r\ndef inference_llava(prompt: str, image_file: str):\r\n    \r\n    args = type('Args', (), {\r\n        \"model_path\": model_path,\r\n        \"model_base\": model_base,\r\n        \"model_name\": model_name,\r\n        \"query\": prompt,\r\n        \"conv_mode\": None,\r\n        \"image_file\": image_file,\r\n        \"sep\": \",\",\r\n        \"temperature\":0.1,\r\n        \"top_p\": 0.8,\r\n        \"num_beams\":1,\r\n        \"max_new_tokens\": 200,\r\n    })()\r\n\r\n    qs = args.query\r\n    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\r\n    if IMAGE_PLACEHOLDER in qs:\r\n        if model.config.mm_use_im_start_end:\r\n            qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\r\n        else:\r\n            qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\r\n    else:\r\n        if model.config.mm_use_im_start_end:\r\n            qs = image_token_se + \"\\n\" + qs\r\n        else:\r\n            qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\r\n\r\n    if \"llama-2\" in model_name.lower():\r\n        conv_mode = \"llava_llama_2\"\r\n    elif \"v1\" in model_name.lower():\r\n        conv_mode = \"llava_v1\"\r\n    elif \"mpt\" in model_name.lower():\r\n        conv_mode = \"mpt\"\r\n    else:\r\n        conv_mode = \"llava_v0\"\r\n\r\n    if args.conv_mode is not None and conv_mode != args.conv_mode:\r\n        print(\r\n            \"[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}\".format(\r\n                conv_mode, args.conv_mode, args.conv_mode\r\n            )\r\n        )\r\n    else:\r\n        args.conv_mode = conv_mode\r\n\r\n    conv = conv_templates[args.conv_mode].copy()\r\n    conv.append_message(conv.roles[0], qs)\r\n    conv.append_message(conv.roles[1], None)\r\n\r\n    prompt = conv.get_prompt()\r\n\r\n    image_files = image_parser(args)\r\n    images = load_images(image_files)\r\n    images_tensor = process_images(\r\n        images,\r\n        image_processor,\r\n        model.config\r\n    ).to(model.device, dtype=torch.float16)\r\n\r\n    input_ids = (\r\n        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\r\n        .unsqueeze(0)\r\n        .cuda()\r\n    )\r\n\r\n    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\r\n    keywords = [stop_str]\r\n    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\r\n\r\n    with torch.inference_mode():\r\n        output_ids = model.generate(\r\n            input_ids,\r\n            images=images_tensor,\r\n            do_sample=True if args.temperature > 0 else False,\r\n            temperature=args.temperature,\r\n            top_p=args.top_p,\r\n            num_beams=args.num_beams,\r\n            max_new_tokens=args.max_new_tokens,\r\n            use_cache=True,\r\n            stopping_criteria=[stopping_criteria],\r\n        )\r\n\r\n    input_token_len = input_ids.shape[1]\r\n    n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\r\n    if n_diff_input_output > 0:\r\n        print(\r\n            f\"[Warning] {n_diff_input_output} output_ids are not the same as the input_ids\"\r\n        )\r\n    outputs = tokenizer.batch_decode(\r\n        output_ids[:, input_token_len:], skip_special_tokens=True\r\n    )[0]\r\n    outputs = outputs.strip()\r\n    if outputs.endswith(stop_str):\r\n        outputs = outputs[: -len(stop_str)]\r\n    outputs = outputs.strip()\r\n    \r\n    return outputs\r\n\r\nprompt = \"Do you like this image?\"\r\nimage_file = \"https://llava-vl.github.io/static/images/view.jpg\"\r\noutput = inference_llava(prompt=prompt, image_file=image_file)\r\n\r\nprint(output)\r\n```\r\nbut got this error\r\n```\r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n```\r\n\r\nIs this part correct?\r\nI'm a little skeptical here.\r\n```\r\nmodel_path = \"/home/projects/checkpoints/llava-v1.5-7b-lora\"\r\n# model_path = \"liuhaotian/llava-v1.5-7b\"\r\n\r\nmodel_base = 'lmsys/vicuna-7b-v1.5'\r\n```\n</Comment>\n<Comment by Ryosuke0104 at 2024-01-20T10:34:15Z>\n> Woooh I am too working on same topic, you can reachout to me on my email address listed on my github profile. Will be happy to work along!\r\n\r\nWow, really?\r\nI will contact you !!\n</Comment>\n<Comment by anonymous-atom at 2024-01-20T10:41:36Z>\nYes that part you are doing it wrong. Just correct the paths.\r\n\r\nAlso you can simply inference from CLI using this:\r\n\r\n`python -m llava.serve.cli \\\r\n    --model-path /home/projects/checkpoints/llava-v1.5-7b-lora-merge \\\r\n    --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n    `\n</Comment>\n<Comment by anonymous-atom at 2024-01-20T10:42:03Z>\n> > Woooh I am too working on same topic, you can reachout to me on my email address listed on my github profile. Will be happy to work along!\r\n> \r\n> Wow, really? I will contact you !!\r\n\r\nYup\r\nHappy to connect!\n</Comment>\n<Comment by Ryosuke0104 at 2024-01-20T10:46:02Z>\n> Yes that part you are doing it wrong. Just correct the paths.\r\n\r\nyeah, I fixed it.\r\n\r\nAnd ran this code\r\n```\r\nimport os\r\ndevice_ids = \"2\"\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=device_ids\r\nprint(f\"Setting CUDA Number {device_ids}\")\r\n\r\nimport argparse\r\nimport torch\r\n\r\nfrom llava.constants import (\r\n    IMAGE_TOKEN_INDEX,\r\n    DEFAULT_IMAGE_TOKEN,\r\n    DEFAULT_IM_START_TOKEN,\r\n    DEFAULT_IM_END_TOKEN,\r\n    IMAGE_PLACEHOLDER,\r\n)\r\nfrom llava.conversation import conv_templates, SeparatorStyle\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.utils import disable_torch_init\r\nfrom llava.mm_utils import (\r\n    process_images,\r\n    tokenizer_image_token,\r\n    get_model_name_from_path,\r\n    KeywordsStoppingCriteria,\r\n)\r\n\r\nfrom PIL import Image\r\n\r\nimport requests\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\nimport re\r\n\r\ndef image_parser(args):\r\n    out = args.image_file.split(args.sep)\r\n    return out\r\n\r\n\r\ndef load_image(image_file):\r\n    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\r\n        response = requests.get(image_file)\r\n        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\r\n    else:\r\n        image = Image.open(image_file).convert(\"RGB\")\r\n    return image\r\n\r\ndef load_images(image_files):\r\n    out = []\r\n    for image_file in image_files:\r\n        image = load_image(image_file)\r\n        out.append(image)\r\n    return out\r\n\r\nmodel_path = \"/home/projects/checkpoints/llava-v1.5-7b-lora\"\r\n# model_path = \"liuhaotian/llava-v1.5-7b\"\r\n\r\nmodel_base = 'lmsys/vicuna-7b-v1.5'\r\nmodel_name = get_model_name_from_path(model_path=model_path)\r\nprint(f\"model_name: {model_name}\")\r\n\r\ndisable_torch_init()\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    model_path, model_base, model_name\r\n)\r\n\r\n\r\ndef inference_llava(prompt: str, image_file: str):\r\n    \r\n    args = type('Args', (), {\r\n        \"model_path\": model_path,\r\n        \"model_base\": model_base,\r\n        \"model_name\": model_name,\r\n        \"query\": prompt,\r\n        \"conv_mode\": None,\r\n        \"image_file\": image_file,\r\n        \"sep\": \",\",\r\n        \"temperature\":0.1,\r\n        \"top_p\": 0.8,\r\n        \"num_beams\":1,\r\n        \"max_new_tokens\": 200,\r\n    })()\r\n\r\n    qs = args.query\r\n    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\r\n    if IMAGE_PLACEHOLDER in qs:\r\n        if model.config.mm_use_im_start_end:\r\n            qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\r\n        else:\r\n            qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\r\n    else:\r\n        if model.config.mm_use_im_start_end:\r\n            qs = image_token_se + \"\\n\" + qs\r\n        else:\r\n            qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\r\n\r\n    if \"llama-2\" in model_name.lower():\r\n        conv_mode = \"llava_llama_2\"\r\n    elif \"v1\" in model_name.lower():\r\n        conv_mode = \"llava_v1\"\r\n    elif \"mpt\" in model_name.lower():\r\n        conv_mode = \"mpt\"\r\n    else:\r\n        conv_mode = \"llava_v0\"\r\n\r\n    if args.conv_mode is not None and conv_mode != args.conv_mode:\r\n        print(\r\n            \"[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}\".format(\r\n                conv_mode, args.conv_mode, args.conv_mode\r\n            )\r\n        )\r\n    else:\r\n        args.conv_mode = conv_mode\r\n\r\n    conv = conv_templates[args.conv_mode].copy()\r\n    conv.append_message(conv.roles[0], qs)\r\n    conv.append_message(conv.roles[1], None)\r\n\r\n    prompt = conv.get_prompt()\r\n\r\n    image_files = image_parser(args)\r\n    images = load_images(image_files)\r\n    images_tensor = process_images(\r\n        images,\r\n        image_processor,\r\n        model.config\r\n    ).to(model.device, dtype=torch.float16)\r\n\r\n    input_ids = (\r\n        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\r\n        .unsqueeze(0)\r\n        .cuda()\r\n    )\r\n\r\n    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\r\n    keywords = [stop_str]\r\n    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\r\n\r\n    with torch.inference_mode():\r\n        output_ids = model.generate(\r\n            input_ids,\r\n            images=images_tensor,\r\n            do_sample=True if args.temperature > 0 else False,\r\n            temperature=args.temperature,\r\n            top_p=args.top_p,\r\n            num_beams=args.num_beams,\r\n            max_new_tokens=args.max_new_tokens,\r\n            use_cache=True,\r\n            stopping_criteria=[stopping_criteria],\r\n        )\r\n\r\n    input_token_len = input_ids.shape[1]\r\n    n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\r\n    if n_diff_input_output > 0:\r\n        print(\r\n            f\"[Warning] {n_diff_input_output} output_ids are not the same as the input_ids\"\r\n        )\r\n    outputs = tokenizer.batch_decode(\r\n        output_ids[:, input_token_len:], skip_special_tokens=True\r\n    )[0]\r\n    outputs = outputs.strip()\r\n    if outputs.endswith(stop_str):\r\n        outputs = outputs[: -len(stop_str)]\r\n    outputs = outputs.strip()\r\n    \r\n    return outputs\r\n\r\nprompt = \"Do you like this image?\"\r\nimage_file = \"https://llava-vl.github.io/static/images/view.jpg\"\r\noutput = inference_llava(prompt=prompt, image_file=image_file)\r\n\r\nprint(output)\r\n```\r\nbut got same error.\r\n```\r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n```\r\n\r\n> python -m llava.serve.cli \\ --model-path /home/projects/checkpoints/llava-v1.5-7b-lora-merge \\ --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \\\r\n\r\nThank you, i will try it.\n</Comment>\n<Comment by anonymous-atom at 2024-01-20T10:56:15Z>\nI think you modified this script: [llava/serve/cli.py](https://github.com/haotian-liu/LLaVA/blob/main/llava/serve/cli.py) ?\n</Comment>\n<Comment by Ryosuke0104 at 2024-01-20T11:04:54Z>\nI modified this code, seeing another issue. (I forget which issue. I'm looking for it now.)\r\n\r\nhttps://github.com/haotian-liu/LLaVA?tab=readme-ov-file#upgrade-to-latest-code-base:~:text=Quick%20Start%20With%20HuggingFace\r\n\r\nAnd I ran \r\n```\r\npython -m llava.serve.cli \\ --model-path /home/projects/checkpoints/llava-v1.5-7b-lora-merge \\ --image-file \"https://llava-vl.github.io/static/images/view.jpg\" \r\n```\r\n\r\nbut got same error...\r\n```\r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n```\n</Comment>\n<Comment by rorubyy at 2024-01-26T09:35:01Z>\n@Ryosuke0104  Hi, I followed the same process as you but ended up with the same error （RuntimeError: probability tensor contains either `inf`, `nan` or element < 0）. Have you resolved the issue? Thank you\n</Comment>\n<Comment by HRL-Mike at 2024-02-15T18:14:20Z>\n> ### Question\r\n> I have finetuned LLaVA on custom data and now I want to evaluate finetuned LLaVA model. I tried to look into the docs, but seems confusing to me right now. Can anyone point me how can I evaluate the LLaVA finetuned on custom data to also evaluate on our own custom data.\r\n> \r\n> In below attached image what is that jsonl format they are referring to ?\r\n> \r\n> ![image](https://private-user-images.githubusercontent.com/74659873/293709056-c8a76c24-9fe9-4c62-a362-6d51cc1ffd48.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDgwMDAzNDUsIm5iZiI6MTcwODAwMDA0NSwicGF0aCI6Ii83NDY1OTg3My8yOTM3MDkwNTYtYzhhNzZjMjQtOWZlOS00YzYyLWEzNjItNmQ1MWNjMWZmZDQ4LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAyMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMjE1VDEyMjcyNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQ4MDUzNTQ3YjljNjY1NWEyZDZlOGRkZDA5ZjlhNmE3MTlmZDc3MTVlNzI0MTViYzA5MWI2ZWIyNzJkNzVlNzImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.HDYnNiyJniUvIi6P8ykjIXgrcXKdVX6GqMsgfXxKyMg)\r\n\r\nHi atom, have you figure out the Llava jsonl format?\r\n{\"question_id\": 1, \"text\": \"How can I improve my time management skills?\", \"category\": \"generic\"}\r\nThis is one sample in the 'question.jonsl' file they provided. It seems there is no 'image' field, which is not good for VQA task.\r\nWhat format should I have for the evaluation set? Thank you for your sharing.\n</Comment>\n<Comment by anonymous-atom at 2024-02-16T07:41:15Z>\nYes, this is one of the sample from my test file below:\r\n{\"image\": \"SomeImage_141.JPG\", \"text\": \"Some Queston ?\", \"category\": \"conv\", \"question_id\": 0}\n</Comment>\n<Comment by aliman80 at 2024-05-05T06:26:37Z>\nHi, Thank you  for your help . I just want to ask a question that I want to use Llava for inference only. i need model pretrained weights but I don't know where can I download or how can I use these. Pleaase guide me with  some simple solution , Thanks alot\n</Comment>\n<Comment by ghazalsaheb at 2024-07-30T04:16:49Z>\n> @Ryosuke0104 Hi, I followed the same process as you but ended up with the same error （RuntimeError: probability tensor contains either `inf`, `nan` or element < 0）. Have you resolved the issue? Thank you\r\n\r\nI have been facing the same issue. Has anyone figured this out?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 962,
    "state": "closed",
    "created_by": "z3ugma",
    "created_at": "2024-01-02T04:51:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/962</URL>\n\n<TITLE>[Usage]  v1.5 LoRA /  scripts/merge_lora_weights.py `ValueError: MPTForCausalLM only supports tied word embeddings`</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\n\r\nafter doing a finetune on v1.5 per the docs and then calling the `merge_lora_weights.py` script, the resulting merged model throws the error `ValueError: MPTForCausalLM only supports tied word embeddings` \r\n\r\nCommand:\r\n python scripts/merge_lora_weights.py --model-base liuhaotian/llava-v1.5-13b --model-path ./checkpoints/llava-v1.5-13b-task-lora-/ --save-model-path ~/prompts_llava1.5\r\n```\r\n\r\nLog: \r\n```\r\nroot@C.8074266:~$ python -m llava.serve.cli --model-path ~/civitai_prompts_llava1.5/ --image-file ~/images/2cc612d6-818c-4927-b9f9-cc54ddb00220.png\r\n[2024-01-02 04:49:05,746] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nYou are using a model of type llava to instantiate a model of type llava_mpt. This is not supported for all configurations of models and can yield errors.\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/root/LLaVA/llava/serve/cli.py\", line 124, in <module>\r\n    main(args)\r\n  File \"/root/LLaVA/llava/serve/cli.py\", line 32, in main\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n  File \"/root/LLaVA/llava/model/builder.py\", line 103, in load_pretrained_model\r\n    model = LlavaMPTForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2700, in from_pretrained\r\n    model = cls(config, *model_args, **model_kwargs)\r\n  File \"/root/LLaVA/llava/model/language_model/llava_mpt.py\", line 53, in __init__\r\n    raise ValueError('MPTForCausalLM only supports tied word embeddings')\r\nValueError: MPTForCausalLM only supports tied word embeddings\r\n```</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2024-01-02T04:53:49Z>\nThis was due to the way we identify model version using file path, pro**mpt**s makes it think it is MPT..\r\n\r\nWe are working on a more robust implementation now, but for now, please avoid including \"mpt\" in your file path.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 961,
    "state": "open",
    "created_by": "WEIYanbin1999",
    "created_at": "2024-01-01T16:35:09Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/961</URL>\n\n<TITLE>[Question] Finetune Performance Flustrated a lot!</TITLE>\n\n<BODY>### Question\n\nWe are fine-tuning on our custom dataset of 850 <image, QA> pairs using the official finetune_task_lora.sh script. However, with identical parameters, hardware, environment, and seed, we observe wildly varying performance during inference, with accuracy ranging even 20%. We have tried modifying the deepseed strategy, running on multiple GPUs as well as a single GPU, and fixing the seed, but we are unable to achieve relative reproducibility in performance. We are seeking assistance in this matter.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/127707751/47a53057-5bc3-48f0-b69e-3d9100088699)</BODY>\n\n<COMMENTS>\n<Comment by yuezih at 2024-01-10T17:04:51Z>\n+1 👀 I'm seeking assistance in this matter too\n</Comment>\n<Comment by yuezih at 2024-01-10T17:08:42Z>\nFound a similar issue: https://github.com/haotian-liu/LLaVA/issues/864\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 960,
    "state": "closed",
    "created_by": "joostshao",
    "created_at": "2023-12-31T06:11:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/960</URL>\n\n<TITLE>empty</TITLE>\n\n<BODY>empty</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 959,
    "state": "open",
    "created_by": "skye0402",
    "created_at": "2023-12-30T12:07:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/959</URL>\n\n<TITLE>Llava answering in Chinese - how to fix this?</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nI'm using Ollama with LLava and found it replies in Chinese for the most of the time. The answers are correct but I want it to reply in English or in the source language. It does it sometimes, so I think it might be a formatting issue of the prompt.\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nPROMPT LOG:\r\n```\r\n'Human: Hi there!\\nAI: 你好！有什么我可以帮助你的吗？\\nHuman: Can you tell me how to make pancakes?\\nAI: 当然可以！这里有一个简单的蛋炸饼制作步骤：\\n\\n1. 准备所需的食材，包括：2个大零食粉、3个小零食粉、1/4杯全麦面包粉、1茶匙蘑菇粉、1茶匙白糖、1茶匙加糖、2茶匹黄油、1茶匹大豆粉、适量水。\\n2. 将所有食材都放入一个大碗中，把它们混合均匀。如果你想要更加光滑的蛋炸饼，可以在这个步骤里加入少量水，使其变得柔软一些。\\n3. 将混合物倒入一个平底锅中，并用中小火力煮1-2分钟，直到大部分的面粉变成了一种类似于糊状的东西。\\n4. 将平底锅轻轻捏紧，使其上方的面粉凝结成一个小圆形。然后用一把刀或者铲子将这个圆形切成几片，每片大约是1/4-1/3大小。\\n5. 在一个平底锅中加入2茶匹黄油，然后放入所有的蛋炸饼片，用中小火力煮1-2分钟，直到蛋炸饼变得金黄色。\\n6. 将蛋炸饼捞出来，放入一个干净的纸巾上，用另一张纸巾轻轻抹去多余的黄油。\\n7. 现在你就可以把这些蛋炸饼装盘吃了！\\n\\n希望这个步骤能帮助你制作出美味的蛋炸饼。如果你有任何问题，请随时告诉我。\\nHuman: Can you reply in German?\\nAI: 当然可以！这里有一个简单的蛋炸饼制作步骤：\\n\\n1. 准备所需的食材，包括：2 große Schokoladensuppe, 3 kleine Schokoladensuppen, 1/4 Tasse Vollkornmehl, 1 Tasse Champignons, 1 Tasse Zucker, 1 Tasse Gewürz, 2 Tassen Olivenöl, 1 Tasse Kichererbsenmehl, je nach Bedarf Wasser.\\n2. Alles in einem großen Topf mischen: Schokoladensuppe, kleine Schokoladensuppen, Vollkornmehl, Champignons, Zucker, Gewürz und Olivenöl. Wenn Sie einen glatten Teig erreichen möchten, können Sie hier ein wenig Wasser hinzufügen.\\n3. Den Topf leicht zusammendrücken, um den Boden zu formen. Dann mit einem Löffel oder einer Gabel die Scheibe in kleine Stücke schneiden, etwa 1/4-1/3 Größe.\\n4. In einem anderen Topf das Olivenöl erhitzen und die Schokoladensuppe-Stücke darin anbraten, bis sie goldbraun sind.\\n5. Die Scheiben auf ein trockenes Papier legen, um den Überzug zu entfernen.\\n6. Jetzt können Sie die Schokoladensuppen servieren!\\n\\nIch hoffe, diese Anleitung hilft Ihnen dabei, schmackhafte Schokoladensuppen herzustellen. Fragen Sie mich gerne, wenn Sie weitere Informationen benötigen.'\r\n\r\n```\r\n\r\nScreenshots:\r\n<img width=\"671\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/36907475/7953a35a-9a36-401c-b313-8353b869e398\"></BODY>\n\n<COMMENTS>\n<Comment by dr-reyes at 2024-01-08T22:54:44Z>\nHi, I don't know if you'll get an official \"fix\", but the way I managed to \"fix it\" was by repeatedly feeding it raw HTML. In my case, I fed it the github page \"https://github.com/ollama-webui/ollama-webui\" ([testFile.txt](https://github.com/haotian-liu/LLaVA/files/13866795/testFile.txt))\r\n\r\n# which broke it:\r\n![start](https://github.com/haotian-liu/LLaVA/assets/112571256/d946b690-705e-4440-b4ad-fb85daf483aa)\r\n![start_2](https://github.com/haotian-liu/LLaVA/assets/112571256/4c23f952-7e55-421e-9966-fcf17bea3b47)\r\n\r\n# Then it started getting mad?\r\n![nextChat](https://github.com/haotian-liu/LLaVA/assets/112571256/aa064c2e-eb40-40f8-ab77-be3ea9ffec5c)\r\n![thirdChat](https://github.com/haotian-liu/LLaVA/assets/112571256/fff627bc-f63f-4430-9c43-8034661ecf86)\r\n![thirdChart_02](https://github.com/haotian-liu/LLaVA/assets/112571256/f683c72c-86e3-49f3-b971-edcae6fbd7cf)\r\n\r\n# But eventually it started speaking English again:\r\n![thirdChart_03](https://github.com/haotian-liu/LLaVA/assets/112571256/b714bb03-68c2-4861-897c-3af55cedc5a0)\r\n\r\n# Then this happened\r\n![nextChat_02](https://github.com/haotian-liu/LLaVA/assets/112571256/9a220ee9-c547-494a-8887-4befbf6161da)\r\n![nextChat_03](https://github.com/haotian-liu/LLaVA/assets/112571256/9a260313-1634-4ca4-bb28-f76bad272d0e)\r\n\r\n\r\n----\r\nso it might be a leak of some kind, but I don't know well enough. Maybe someone else can look into it\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 958,
    "state": "open",
    "created_by": "g-h-chen",
    "created_at": "2023-12-30T10:02:28Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/958</URL>\n\n<TITLE>[LLaVA bench in the wild] which result to report?</TITLE>\n\n<BODY>Hi haotian\r\n\r\n```\r\nkey | cand/anchor | anchor | cand\r\nall 81.0 83.7 67.8\r\nllava_bench_complex 88.4 85.0 75.2\r\nllava_bench_conv 77.0 80.6 62.1\r\nllava_bench_detail 71.3 84.7 60.3\r\n```\r\n\r\nHere is the result we obtained for our model by running the eval scripts. But I'm confused which result should I report in the paper?\r\n\r\nThanks in advance!</BODY>\n\n<COMMENTS>\n<Comment by HenryHZY at 2024-02-19T11:56:00Z>\n@haotian-liu I think the answer is the value of ['all']['cand/anchor']?\n</Comment>\n<Comment by OliverLeeXZ at 2024-02-28T12:35:30Z>\nSame question! Which one to report?\r\nThere is no 'key | cand/anchor | anchor | cand 'in  my output.\n</Comment>\n<Comment by rohan598 at 2024-03-11T20:11:07Z>\n@haotian-liu I have the same question, can you share which result to present?\r\n\r\n@OliverLeeXZ and @g-h-chen were you able to find the correct strategy?\r\n\r\n@HenryHZY Could you expand on your response?\n</Comment>\n<Comment by HenryHZY at 2024-03-11T22:29:12Z>\n> @haotian-liu I have the same question, can you share which result to present?\n> \n> \n> \n> @OliverLeeXZ and @g-h-chen were you able to find the correct strategy?\n> \n> \n> \n> @HenryHZY Could you expand on your response?\n\nI think the result is the upper left value, that is, the first value.\n\nUnder this setting, The result of model-zoo.md matches the average result of the three attached results in eval.zip.\n</Comment>\n<Comment by rohan598 at 2024-03-12T00:24:29Z>\n> > @haotian-liu I have the same question, can you share which result to present?\r\n> > @OliverLeeXZ and @g-h-chen were you able to find the correct strategy?\r\n> > @HenryHZY Could you expand on your response?\r\n> \r\n> I think the result is the upper left value, that is, the first value.\r\n> \r\n> Under this setting, The result of model-zoo.md matches the average result of the three attached results in eval.zip.\r\n\r\nYes, I too verified, thank you for this!\n</Comment>\n<Comment by ppalantir at 2024-08-15T06:56:13Z>\n> > > @haotian-liu I have the same question, can you share which result to present?\r\n> > > @OliverLeeXZ and @g-h-chen were you able to find the correct strategy?\r\n> > > @HenryHZY Could you expand on your response?\r\n> > \r\n> > \r\n> > I think the result is the upper left value, that is, the first value.\r\n> > Under this setting, The result of model-zoo.md matches the average result of the three attached results in eval.zip.\r\n> \r\n> Yes, I too verified, thank you for this!\r\n\r\nHi, when I perform \"CUDA_VISIBLE_DEVICES=0 bash scripts/v1_5/eval/llavabench.sh\"\r\n\r\nit report the error \"No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>).\"\r\n\r\nDoes this review necessarily need to use the OpenAI API? And could you give me some suggestions about cheap api～\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 957,
    "state": "open",
    "created_by": "kirayomato",
    "created_at": "2023-12-30T06:39:02Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/957</URL>\n\n<TITLE>NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: Webui always shows **NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE**.\r\n\r\nI have tried CLI Inference, and it can work successfully.\r\nCommand:\r\n```\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-7b\r\n```\r\nUsually the console will just show **Caught Unknown Error**,\r\n![2023123014_2616_RayLink_hU65](https://github.com/haotian-liu/LLaVA/assets/38139345/179c28e7-52ff-4cf3-8576-fd8f93e6365c)\r\n\r\n\r\nbut occasionally the error will like this\r\n\r\nLog: \r\n```\r\n2023-12-30 14:28:22 | ERROR | stderr | Exception in thread Thread-6 (generate):\r\n2023-12-30 14:28:22 | ERROR | stderr | Traceback (most recent call last):\r\n2023-12-30 14:28:22 | ERROR | stderr |   File \"C:\\Software\\Miniconda\\envs\\llava\\lib\\threading.py\", line 1016, in _bootstrap_inner\r\n2023-12-30 14:28:22 | ERROR | stderr |     self.run()\r\n2023-12-30 14:28:22 | ERROR | stderr |   File \"C:\\Software\\Miniconda\\envs\\llava\\lib\\threading.py\", line 953, in run\r\n2023-12-30 14:28:22 | ERROR | stderr |     self._target(*self._args, **self._kwargs)\r\n2023-12-30 14:28:22 | ERROR | stderr |   File \"C:\\Software\\Miniconda\\envs\\llava\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n2023-12-30 14:28:22 | ERROR | stderr |     return func(*args, **kwargs)\r\n2023-12-30 14:28:22 | ERROR | stderr |   File \"C:\\Software\\Miniconda\\envs\\llava\\lib\\site-packages\\transformers\\generation\\utils.py\", line 1588, in generate\r\n2023-12-30 14:28:22 | ERROR | stderr |     return self.sample(\r\n2023-12-30 14:28:22 | ERROR | stderr |   File \"C:\\Software\\Miniconda\\envs\\llava\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2642, in sample\r\n2023-12-30 14:28:22 | ERROR | stderr |     outputs = self(\r\n2023-12-30 14:28:22 | ERROR | stderr |   File \"C:\\Software\\Miniconda\\envs\\llava\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\n2023-12-30 14:28:22 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2023-12-30 14:28:22 | ERROR | stderr |   File \"C:\\Software\\Miniconda\\envs\\llava\\lib\\site-packages\\accelerate\\hooks.py\", line 165, in new_forward\r\n2023-12-30 14:28:22 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2023-12-30 14:28:22 | ERROR | stderr |   File \"D:\\Program\\VLM\\LLaVA\\llava\\model\\language_model\\llava_llama.py\", line 88, in forward\r\n2023-12-30 14:28:22 | ERROR | stderr |     return super().forward(\r\n2023-12-30 14:28:22 | ERROR | stderr |   File \"C:\\Software\\Miniconda\\envs\\llava\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line 806, in forward\r\n2023-12-30 14:28:22 | ERROR | stderr |     outputs = self.model(\r\n2023-12-30 14:28:22 | ERROR | stderr |   File \"C:\\Software\\Miniconda\\envs\\llava\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\n2023-12-30 14:28:22 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2023-12-30 14:28:22 | ERROR | stderr |   File \"C:\\Software\\Miniconda\\envs\\llava\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line 693, in forward\r\n2023-12-30 14:28:22 | ERROR | stderr |     layer_outputs = decoder_layer(\r\n2023-12-30 14:28:22 | ERROR | stderr |   File \"C:\\Software\\Miniconda\\envs\\llava\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\n2023-12-30 14:28:22 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2023-12-30 14:28:22 | ERROR | stderr |   File \"C:\\Software\\Miniconda\\envs\\llava\\lib\\site-packages\\accelerate\\hooks.py\", line 165, in new_forward\r\n2023-12-30 14:28:22 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2023-12-30 14:28:22 | ERROR | stderr |   File \"C:\\Software\\Miniconda\\envs\\llava\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line 405, in forward\r\n2023-12-30 14:28:22 | ERROR | stderr |     hidden_states = self.input_layernorm(hidden_states)\r\n2023-12-30 14:28:22 | ERROR | stderr |   File \"C:\\Software\\Miniconda\\envs\\llava\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\n2023-12-30 14:28:22 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2023-12-30 14:28:22 | ERROR | stderr |   File \"C:\\Software\\Miniconda\\envs\\llava\\lib\\site-packages\\accelerate\\hooks.py\", line 165, in new_forward\r\n2023-12-30 14:28:22 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2023-12-30 14:28:22 | ERROR | stderr |   File \"C:\\Software\\Miniconda\\envs\\llava\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line 89, in forward\r\n2023-12-30 14:28:22 | ERROR | stderr |     return self.weight * hidden_states.to(input_dtype)\r\n2023-12-30 14:28:22 | ERROR | stderr |   File \"C:\\Software\\Miniconda\\envs\\llava\\lib\\site-packages\\torch\\_prims_common\\wrappers.py\", line 220, in _fn\r\n2023-12-30 14:28:22 | ERROR | stderr |     result = fn(*args, **kwargs)\r\n2023-12-30 14:28:22 | ERROR | stderr |   File \"C:\\Software\\Miniconda\\envs\\llava\\lib\\site-packages\\torch\\_prims_common\\wrappers.py\", line 130, in _fn\r\n2023-12-30 14:28:22 | ERROR | stderr |     result = fn(**bound.arguments)\r\n2023-12-30 14:28:22 | ERROR | stderr |   File \"C:\\Software\\Miniconda\\envs\\llava\\lib\\site-packages\\torch\\_refs\\__init__.py\", line 926, in _ref\r\n2023-12-30 14:28:22 | ERROR | stderr |     return prim(a, b)\r\n2023-12-30 14:28:22 | ERROR | stderr |   File \"C:\\Software\\Miniconda\\envs\\llava\\lib\\site-packages\\torch\\_refs\\__init__.py\", line 1532, in mul\r\n2023-12-30 14:28:22 | ERROR | stderr |     return prims.mul(a, b)\r\n2023-12-30 14:28:22 | ERROR | stderr |   File \"C:\\Software\\Miniconda\\envs\\llava\\lib\\site-packages\\torch\\_ops.py\", line 287, in __call__\r\n2023-12-30 14:28:22 | ERROR | stderr |     return self._op(*args, **kwargs or {})\r\n2023-12-30 14:28:22 | ERROR | stderr |   File \"C:\\Software\\Miniconda\\envs\\llava\\lib\\site-packages\\torch\\_prims\\__init__.py\", line 346, in _elementwise_meta\r\n2023-12-30 14:28:22 | ERROR | stderr |     utils.check_same_device(*args_, allow_cpu_scalar_tensors=True)\r\n2023-12-30 14:28:22 | ERROR | stderr |   File \"C:\\Software\\Miniconda\\envs\\llava\\lib\\site-packages\\torch\\_prims_common\\__init__.py\", line 596, in check_same_device\r\n2023-12-30 14:28:22 | ERROR | stderr |     raise RuntimeError(msg)\r\n2023-12-30 14:28:22 | ERROR | stderr | RuntimeError: Tensor on device cuda:0 is not on the expected device meta!\r\n2023-12-30 14:28:28 | INFO | stdout | Caught Unknown Error\r\n```</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 956,
    "state": "open",
    "created_by": "an1018",
    "created_at": "2023-12-29T10:36:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/956</URL>\n\n<TITLE>[Question] loss is too low, only 0.3</TITLE>\n\n<BODY>### Question\n\nTraining with custom data (12,276 images, 30 images from llava158K), after 47iters loss dropped to 0.4.\r\nTraining script:\r\n`\r\ndeepspeed --include localhost:5,6 --master_port 29585 \\\r\n    llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path ./llava-v1.5-13b \\\r\n    --version v1 \\\r\n    --data_path ../dataset/llava_finetune/train.json \\\r\n    --image_folder ../dataset/llava_finetune/ \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-13b-task-lora-nuimages \\\r\n    --num_train_epochs 10 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 384 \\\r\n    --save_total_limit 11 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n`\r\nthis is my prompt:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/19808900/47aa0ac6-501d-4350-bd3f-848ac97f0871)\r\n\r\nI have tried to change the dataset twice, could you give me some advice? Thank you very much</BODY>\n\n<COMMENTS>\n<Comment by an1018 at 2024-01-02T00:48:34Z>\n@haotian-liu Could you help me see how I should change it\n</Comment>\n<Comment by Jeckinchen at 2024-03-19T02:05:07Z>\nany update?\n</Comment>\n<Comment by fisher75 at 2024-04-25T13:21:12Z>\nHi, how do you know the training was effecitve? Did you use the default training setting? I LoRA with default parameters and basically no improvement.\n</Comment>\n<Comment by bang123-box at 2024-10-21T11:27:22Z>\n> ### Question\r\n> Training with custom data (12,276 images, 30 images from llava158K), after 47iters loss dropped to 0.4. Training script: `deepspeed --include localhost:5,6 --master_port 29585 \\ llava/train/train_mem.py \\ --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\ --deepspeed ./scripts/zero3.json \\ --model_name_or_path ./llava-v1.5-13b \\ --version v1 \\ --data_path ../dataset/llava_finetune/train.json \\ --image_folder ../dataset/llava_finetune/ \\ --vision_tower openai/clip-vit-large-patch14-336 \\ --mm_projector_type mlp2x_gelu \\ --mm_vision_select_layer -2 \\ --mm_use_im_start_end False \\ --mm_use_im_patch_token False \\ --image_aspect_ratio pad \\ --group_by_modality_length True \\ --bf16 True \\ --output_dir ./checkpoints/llava-v1.5-13b-task-lora-nuimages \\ --num_train_epochs 10 \\ --per_device_train_batch_size 16 \\ --per_device_eval_batch_size 4 \\ --gradient_accumulation_steps 1 \\ --evaluation_strategy \"no\" \\ --save_strategy \"steps\" \\ --save_steps 384 \\ --save_total_limit 11 \\ --learning_rate 2e-5 \\ --weight_decay 0. \\ --warmup_ratio 0.03 \\ --lr_scheduler_type \"cosine\" \\ --logging_steps 1 \\ --tf32 True \\ --model_max_length 2048 \\ --gradient_checkpointing True \\ --dataloader_num_workers 4 \\ --lazy_preprocess True \\ --report_to wandb` this is my prompt: ![image](https://private-user-images.githubusercontent.com/19808900/293347484-47aa0ac6-501d-4350-bd3f-848ac97f0871.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk1MTAwNjMsIm5iZiI6MTcyOTUwOTc2MywicGF0aCI6Ii8xOTgwODkwMC8yOTMzNDc0ODQtNDdhYTBhYzYtNTAxZC00MzUwLWJkM2YtODQ4YWM5N2YwODcxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMjElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDIxVDExMjI0M1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWI4ZDExYTgxODBmOThkMjFlYmEyMTM5OGExZDRlYTYxNTFkMTM3MzlhZjhjMDM5M2RkYmMzZWIyZWVhNWY1ZjgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.A6NWvfNG5loyg915SkpSjWpgiqCL9WiMdvaNkIUxnOU)\r\n> \r\n> I have tried to change the dataset twice, could you give me some advice? Thank you very much\r\n\r\nSame as me, I use my custom data to pretrain and lora sft the llava, when the pretrained is done, the loss is around to 0.4-0.5, when sft stage, the loss decreases rapidly from 1.0 to 0.4, then the loss is around to 0.2-04\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 954,
    "state": "closed",
    "created_by": "Stardust-y",
    "created_at": "2023-12-29T03:32:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/954</URL>\n\n<TITLE>[Usage] OOM pretrained on 4*80gA100, but successfully on 2*80gA100</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nI'm running the [scripts/v1_5/pretrain.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/pretrain.sh)\r\nbut encountered an OOM issue on 4*80G A100\r\nI'm using zero3/zero3_offload, --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 4 \\\r\nHowever, I ran it successfully on 2*80G A100 or single 80G A100\r\n\r\nHere's the full error report\r\n`[2023-12-29 11:19:29,537] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-12-29 11:19:33,039] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\nDetected CUDA_VISIBLE_DEVICES=0,1,2,3: setting --include=localhost:0,1,2,3\r\n[2023-12-29 11:19:33,039] [INFO] [runner.py:555:main] cmd = /home/xmyu/anaconda3/envs/llava/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero3_offload.json --model_name_or_path ./llava/checkpoints/llava-v1.5-13b --version v1 --data_path ./playground/data/pretrain/blip_laion_cc_sbu_558k.json --image_folder ./playground/data/pretrain/images --vision_tower ./llava/checkpoints/nougat_base --mm_projector_type mlp2x_gelu --tune_mm_mlp_adapter True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 False --fp16 True --output_dir ./checkpoints/llava-v1.5-13b-pretrain --num_train_epochs 1 --per_device_train_batch_size 4 --per_device_eval_batch_size 1 --gradient_accumulation_steps 16 --evaluation_strategy no --save_strategy steps --save_steps 8000 --save_total_limit 1 --learning_rate 1e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess False\r\n[2023-12-29 11:19:34,416] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-12-29 11:19:36,415] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\r\n[2023-12-29 11:19:36,415] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0\r\n[2023-12-29 11:19:36,415] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\r\n[2023-12-29 11:19:36,415] [INFO] [launch.py:163:main] dist_world_size=4\r\n[2023-12-29 11:19:36,415] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\r\n[2023-12-29 11:19:39,873] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-12-29 11:19:39,906] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-12-29 11:19:40,370] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-12-29 11:19:40,512] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-12-29 11:19:43,607] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-12-29 11:19:43,608] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-12-29 11:19:43,608] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n[2023-12-29 11:19:43,625] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-12-29 11:19:43,625] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-12-29 11:19:43,637] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-12-29 11:19:43,637] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-12-29 11:19:43,640] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-12-29 11:19:43,640] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-12-29 11:26:23,730] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3398214\r\n[2023-12-29 11:26:27,146] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3398215\r\n[2023-12-29 11:26:27,149] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3398216\r\n[2023-12-29 11:26:28,875] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3398217`</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-12-29T06:53:44Z>\nLooks like cpu OOM. Try to change zero2 to zero3.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 953,
    "state": "closed",
    "created_by": "White1973",
    "created_at": "2023-12-28T06:40:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/953</URL>\n\n<TITLE>[Question] Traing and test using the same data, it will be perfectly overfit?</TITLE>\n\n<BODY>### Question\r\n\r\nWhen I use the script \"finetune_task_lora.sh\" and the training set in ScienceQA for too big epochs training (e.g. 12 epochs), then I test in the same training set, I cannot obtain accurate results from LLaVA, and some response is even None. Does anyone know the reason behind that?\r\n\r\n**This is my training script:**\r\nMODEL_VERSION=\"llava-v1.5-13b\"\r\n\r\ndeepspeed llava/train/train.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path /diffusion_model/models/data_filter/$MODEL_VERSION \\\r\n    --version v1 \\\r\n    --data_path  /diffusion_model/eval/ScienceQA/llava_train_QCM-LEA.json \\\r\n    --image_folder /diffusion_model/eval/ScienceQA/train  \\\r\n    --vision_tower  /diffusion_model/models/data_filter/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-ScienceQA_QCM_LEA-12e \\\r\n    --num_train_epochs 12 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n\r\n**This is my test script:**\r\nCUDA_VISIBLE_DEVICES=1 python -m llava.eval.model_vqa_science \\\r\n    --model-path /zju_0038/diffusion_model/models/data_filter/llava-v1.5-13b \\\r\n    --question-file /zju_0038/diffusion_model/eval/ScienceQA/llava_train_QCM-LEA.json \\\r\n    --image-folder /zju_0038/diffusion_model/eval/ScienceQA/train \\\r\n    --answers-file /zju_0038/diffusion_model/eval/ScienceQA/llava-v1.5-13b.jsonl \\\r\n    --single-pred-prompt \\\r\n    --temperature 0 \\\r\n    --conv-mode vicuna_v1\r\n\r\nCUDA_VISIBLE_DEVICES=0 python llava/eval/eval_science_qa.py \\\r\n    --base-dir /diffusion_model/eval/ScienceQA/ \\\r\n    --result-file /diffusion_model/eval/ScienceQA/llava-v1.5-13b.jsonl \\\r\n    --output-file /diffusion_model/eval/ScienceQA/llava-v1.5-13b_output.jsonl \\\r\n    --output-result /diffusion_model/eval/ScienceQA/answers/llava-v1.5-13b_result.json\r\n\r\nBy the way, I meet the same issue in my own custom dataset.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 952,
    "state": "open",
    "created_by": "zhangyuereal",
    "created_at": "2023-12-27T10:30:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/952</URL>\n\n<TITLE>Dear author, How much time does it cost to train this model？ With what type of GPU cards?</TITLE>\n\n<BODY>### Describe the issue\n\nDear author, How much time does it cost to train this model？ With what type of GPU cards?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 951,
    "state": "open",
    "created_by": "Slinene",
    "created_at": "2023-12-27T07:45:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/951</URL>\n\n<TITLE>[Question] How to get the image embeddings and text embeddings from model during inference</TITLE>\n\n<BODY>### Question\n\nI noticed that  model.generate can directly get the output, but how to get the image embeddings and  text embeddings?</BODY>\n\n<COMMENTS>\n<Comment by zty0510 at 2024-05-01T12:22:53Z>\nI meet with the same problem. Did you get a solution?\n</Comment>\n<Comment by wenxuanmou at 2024-08-12T11:19:29Z>\nSame question. Have you got a solution? Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 950,
    "state": "open",
    "created_by": "RongkunYang",
    "created_at": "2023-12-27T02:21:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/950</URL>\n\n<TITLE>[Question] A question about the pretrained LLava weight?</TITLE>\n\n<BODY>### Question\n\nDear authors.\r\nThanks for your good work.\r\nWhen I try to prepare the LLaVA weight, I follow the instructions as the LLaVa repository,\r\nrun the command \"python3 -m llava.model.apply_delta\r\n--base /path/to/llama-13b\r\n--target /output/path/to/LLaVA-13B-v0\r\n--delta liuhaotian/LLaVA-13b-delta-v0\".\r\nThis cannot formulate the llava weight, because the Tokenizer can not be instantiated.\r\nI found that this is because the file tokenizer_config.json maps the special tokens such as \"bos_token\" and \"eos_token\" to \" \". But I don't know whether it's OK to change this file.\r\nOr there are other methods?\r\nThank you.\r\n\r\nThe error message is as following, the special tokens map in the file of tokenizer_config.json of \" \" seems to be trapped in an infinite loop:\r\n\"\"\"\r\nFile \"/home/yrk/Desktop/program/LLM/LLaVA/llava/model/apply_delta.py\", line 48, in\r\napply_delta(args.base_model_path, args.target_model_path, args.delta_path)\r\nFile \"/home/yrk/Desktop/program/LLM/LLaVA/llava/model/apply_delta.py\", line 20, in apply_delta\r\ndelta_tokenizer = AutoTokenizer.from_pretrained(delta_path)\r\nFile \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 702, in from_pretrained\r\nreturn tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\r\nFile \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 1841, in from_pretrained\r\nreturn cls._from_pretrained(\r\nFile \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2004, in _from_pretrained\r\ntokenizer = cls(*init_inputs, **init_kwargs)\r\nFile \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py\", line 126, in init\r\nself.update_post_processor()\r\nFile \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py\", line 136, in update_post_processor\r\nbos_token_id = self.bos_token_id\r\nFile \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 1136, in bos_token_id\r\nreturn self.convert_tokens_to_ids(self.bos_token)\r\nFile \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\nreturn self._convert_token_to_id_with_added_voc(tokens)\r\nFile \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\nreturn self.unk_token_id\r\nFile \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\nreturn self.convert_tokens_to_ids(self.unk_token)\r\nFile \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\nreturn self._convert_token_to_id_with_added_voc(tokens)\r\nFile \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\nreturn self.unk_token_id\r\nFile \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\nreturn self.convert_tokens_to_ids(self.unk_token)\r\n\"\"\"</BODY>\n\n<COMMENTS>\n<Comment by RongkunYang at 2023-12-27T02:34:37Z>\nLoading base model\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 11.60it/s]\r\nLoading delta\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [02:09<00:00, 43.21s/it]\r\nYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\r\nTraceback (most recent call last):\r\n  File \"/home/yrk/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/yrk/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/yrk/Desktop/program/LLM/LLaVA/llava/model/apply_delta.py\", line 48, in <module>\r\n    apply_delta(args.base_model_path, args.target_model_path, args.delta_path)\r\n  File \"/home/yrk/Desktop/program/LLM/LLaVA/llava/model/apply_delta.py\", line 20, in apply_delta\r\n    delta_tokenizer = AutoTokenizer.from_pretrained(delta_path)\r\n  File \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 702, in from_pretrained\r\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\r\n  File \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 1841, in from_pretrained\r\n    return cls._from_pretrained(\r\n  File \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2004, in _from_pretrained\r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\n  File \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py\", line 126, in __init__\r\n    self.update_post_processor()\r\n  File \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py\", line 136, in update_post_processor\r\n    bos_token_id = self.bos_token_id\r\n  File \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 1136, in bos_token_id\r\n    return self.convert_tokens_to_ids(self.bos_token)\r\n  File \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\n  File \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 257, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\n  File \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 1155, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\n  File \"/home/yrk/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 250, in convert_tokens_to_ids\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 949,
    "state": "closed",
    "created_by": "zhongzee",
    "created_at": "2023-12-27T00:48:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/949</URL>\n\n<TITLE>[Usage] deepspeed-chat training error on v100 * 8:RuntimeError: output tensor must have the same type as input tensor</TITLE>\n\n<BODY>### Describe the issue\n\nMy zero3.json config is:\r\n{\r\n    \"fp16\": {\r\n        \"enabled\": “auto”,\r\n        \"loss_scale\": 0,\r\n        \"loss_scale_window\": 1000,\r\n        \"initial_scale_power\": 16,\r\n        \"hysteresis\": 2,\r\n        \"min_loss_scale\": 1\r\n    },\r\n    \"bf16\": {\r\n        \"enabled\":“auto”\r\n    },\r\n    \"train_micro_batch_size_per_gpu\": \"auto\",\r\n    \"train_batch_size\": \"auto\",\r\n    \"gradient_accumulation_steps\": \"auto\",\r\n    \"zero_optimization\": {\r\n        \"stage\": 3,\r\n        \"overlap_comm\": true,\r\n        \"contiguous_gradients\": true,\r\n        \"sub_group_size\": 1e9,\r\n        \"reduce_bucket_size\": \"auto\",\r\n        \"stage3_prefetch_bucket_size\": \"auto\",\r\n        \"stage3_param_persistence_threshold\": \"auto\",\r\n        \"stage3_max_live_parameters\": 1e9,\r\n        \"stage3_max_reuse_distance\": 1e9,\r\n        \"stage3_gather_16bit_weights_on_model_save\": true\r\n    }\r\n}\r\nmy script is：\r\ndeepspeed llava/train/train_xformers.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path liuhaotian/llava-v1.5-7b \\\r\n    --version v1 \\\r\n    --data_path  xxx\\\r\n    --image_folder  xxx\\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 false \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b-task-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 false \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\nthe bug is:\r\nTraceback (most recent call last):\r\nFile \"/mnt/afs/liwenhao/wuzhongze/mPLUG-Owl-main/mPLUG-Owl2/mplug_owl2/train/train_xformers.py\", line 13, in\r\ntrain()\r\nFile \"/mnt/afs/liwenhao/wuzhongze/mPLUG-Owl-main/mPLUG-Owl2/mplug_owl2/train/train.py\", line 778, in train\r\ntrainer.train()\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\nreturn inner_training_loop(\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\r\ntr_loss_step = self.training_step(model, inputs)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/transformers/trainer.py\", line 2654, in training_step\r\nret_val = func(*args, **kwargs)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 386, in __all_gather_params\r\nret_val = func(*args, **kwargs)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/comm/comm.py\", line 312, in allgather_fn\r\nloss = self.compute_loss(model, inputs)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/transformers/trainer.py\", line 2679, in compute_loss\r\nreturn all_gather_into_tensor(output_tensor, input_tensor, group=group, async_op=async_op, debug=debug)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/comm/comm.py\", line 116, in log_wrapper\r\nhandle = partitioned_params[0].all_gather_coalesced(partitioned_params)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\noutputs = model(**inputs)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\nreturn func(*args, **kwargs)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/comm/comm.py\", line 297, in all_gather_into_tensor\r\nreturn cdb.all_gather_into_tensor(output_tensor=output_tensor, input_tensor=tensor, group=group, async_op=async_op)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/comm/torch.py\", line 136, in all_gather_into_tensor\r\nreturn forward_call(*args, **kwargs)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\nreturn self.all_gather_function(output_tensor=output_tensor,\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 1451, in wrapper\r\nreturn func(*args, **kwargs)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 2532, in all_gather_into_tensor\r\nwork = group._allgather_base(output_tensor, input_tensor)\r\nRuntimeError: output tensor must have the same type as input tensor\r\nhandle = _dist_allgather_fn(param.ds_tensor.to(get_accelerator().current_device_name()), param_buffer,\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 83, in _dist_allgather_fn\r\nreturn instrument_w_nvtx(dist.allgather_fn)(output_tensor, input_tensor, group=group, async_op=True)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\nret_val = func(*args, **kwargs)ret_val = func(*args, **kwargs)\r\n\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 935, in all_gather_coalesced\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1735, in forward\r\nret_val = func(*args, **kwargs)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/comm/comm.py\", line 312, in allgather_fn\r\nreturn all_gather_into_tensor(output_tensor, input_tensor, group=group, async_op=async_op, debug=debug)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/comm/comm.py\", line 116, in log_wrapper\r\nhandle = _dist_allgather_fn(param.ds_tensor.to(get_accelerator().current_device_name()), param_buffer,\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 83, in _dist_allgather_fn\r\nloss = self.module(*inputs, **kwargs)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\r\nreturn func(*args, **kwargs)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/comm/comm.py\", line 297, in all_gather_into_tensor\r\nreturn instrument_w_nvtx(dist.allgather_fn)(output_tensor, input_tensor, group=group, async_op=True)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\nreturn cdb.all_gather_into_tensor(output_tensor=output_tensor, input_tensor=tensor, group=group, async_op=async_op)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/comm/torch.py\", line 136, in all_gather_into_tensor\r\nresult = forward_call(*args, **kwargs)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/peft/peft_model.py\", line 922, in forward\r\nreturn self.all_gather_function(output_tensor=output_tensor,\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 1451, in wrapper\r\nreturn self.base_model(\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\r\nreturn func(*args, **kwargs)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 2532, in all_gather_into_tensor\r\nresult = forward_call(*args, **kwargs)\r\nFile \"/mnt/afs/liwenhao/wuzhongze/mPLUG-Owl-main/mPLUG-Owl2/mplug_owl2/model/modeling_mplug_owl2.py\", line 239, in forward\r\nwork = group._allgather_base(output_tensor, input_tensor)\r\nRuntimeError: output tensor must have the same type as input tensor\r\nself.prepare_inputs_labels_for_multimodal(input_ids, attention_mask, past_key_values, labels, images)\r\nFile \"/mnt/afs/liwenhao/wuzhongze/mPLUG-Owl-main/mPLUG-Owl2/mplug_owl2/model/modeling_mplug_owl2.py\", line 80, in prepare_inputs_labels_for_multimodal\r\nimage_features = self.encode_images(images)\r\nFile \"/mnt/afs/liwenhao/wuzhongze/mPLUG-Owl-main/mPLUG-Owl2/mplug_owl2/model/modeling_mplug_owl2.py\", line 60, in encode_images\r\nimage_features = self.get_model().vision_model(images).last_hidden_state\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\r\nresult = forward_call(*args, **kwargs)\r\nFile \"/mnt/afs/liwenhao/wuzhongze/mPLUG-Owl-main/mPLUG-Owl2/mplug_owl2/model/visual_encoder.py\", line 419, in forward\r\nhidden_states = self.embeddings(pixel_values)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\r\nresult = forward_call(*args, **kwargs)\r\nFile \"/mnt/afs/liwenhao/wuzhongze/mPLUG-Owl-main/mPLUG-Owl2/mplug_owl2/model/visual_encoder.py\", line 110, in forward\r\nimage_embeds = self.patch_embed(pixel_values)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\nresult = hook(self, args)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\nret_val = func(*args, **kwargs)\r\nret_val = func(*args, **kwargs)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/comm/comm.py\", line 312, in allgather_fn\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 371, in _pre_forward_module_hook\r\nself.pre_sub_module_forward_function(module)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nreturn all_gather_into_tensor(output_tensor, input_tensor, group=group, async_op=async_op, debug=debug)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/comm/comm.py\", line 116, in log_wrapper\r\nreturn func(*args, **kwargs)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 483, in pre_sub_module_forward_function\r\nreturn func(*args, **kwargs)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/comm/comm.py\", line 297, in all_gather_into_tensor\r\nparam_coordinator.fetch_sub_module(sub_module)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\nreturn cdb.all_gather_into_tensor(output_tensor=output_tensor, input_tensor=tensor, group=group, async_op=async_op)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/comm/torch.py\", line 136, in all_gather_into_tensor\r\nreturn self.all_gather_function(output_tensor=output_tensor,\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 1451, in wrapper\r\nreturn func(*args, **kwargs)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 2532, in all_gather_into_tensor\r\nwork = group._allgather_base(output_tensor, input_tensor)\r\nRuntimeError: output tensor must have the same type as input tensor\r\nret_val = func(*args, **kwargs)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nreturn func(*args, **kwargs)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 254, in fetch_sub_module\r\nself.__all_gather_params(params_to_fetch)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\nret_val = func(*args, **kwargs)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 386, in __all_gather_params\r\nhandle = partitioned_params[0].all_gather_coalesced(partitioned_params)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\nret_val = func(*args, **kwargs)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 935, in all_gather_coalesced\r\nhandle = _dist_allgather_fn(param.ds_tensor.to(get_accelerator().current_device_name()), param_buffer,\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 83, in _dist_allgather_fn\r\nreturn instrument_w_nvtx(dist.allgather_fn)(output_tensor, input_tensor, group=group, async_op=True)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\nret_val = func(*args, **kwargs)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/comm/comm.py\", line 312, in allgather_fn\r\nreturn all_gather_into_tensor(output_tensor, input_tensor, group=group, async_op=async_op, debug=debug)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/comm/comm.py\", line 116, in log_wrapper\r\nreturn func(*args, **kwargs)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/comm/comm.py\", line 297, in all_gather_into_tensor\r\nreturn cdb.all_gather_into_tensor(output_tensor=output_tensor, input_tensor=tensor, group=group, async_op=async_op)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/deepspeed/comm/torch.py\", line 136, in all_gather_into_tensor\r\nreturn self.all_gather_function(output_tensor=output_tensor,\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 1451, in wrapper\r\nreturn func(*args, **kwargs)\r\nFile \"/mnt/afs/liwenhao/mplug_owl/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 2532, in all_gather_into_tensor\r\nwork = group._allgather_base(output_tensor, input_tensor)\r\nRuntimeError: output tensor must have the same type as input tensor\r\n0% 0/598 [00:04<?, ?it/s]</BODY>\n\n<COMMENTS>\n<Comment by taoszhang at 2023-12-28T11:48:53Z>\nme too\n</Comment>\n<Comment by taoszhang at 2023-12-28T11:49:59Z>\n请问您解决这个问题了吗？\n</Comment>\n<Comment by huvers at 2024-01-06T20:02:32Z>\n--bf16 False \\\r\n--fp16 True \\\r\n\r\nFixed it for me\n</Comment>\n<Comment by Fr0zenCrane at 2024-01-30T01:57:26Z>\nThank you, faced the same problem and solved it with your solution\r\n\r\n> --bf16 False --fp16 True \\\r\n> \r\n> Fixed it for me\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 948,
    "state": "open",
    "created_by": "PommesPeter",
    "created_at": "2023-12-26T14:32:28Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/948</URL>\n\n<TITLE>[Question] Whether `mm_projector` needs to be frozen during instruction tuning</TITLE>\n\n<BODY>### Question\r\n\r\nCompared `./scripts/v1_5/finetune.sh` and `./scripts/v1_5/finetune_lora.sh`. When running `finetune.sh`, if `mm_projector_lr` is not specified, the parameters of `mm_projector` will not be updated by default. However, `mm_projector_lr` is specified in `finetune_lora.sh`, which means that `mm_projector` will be trained when fine-tuning lora. This is inconsistent with what is explained in the paper. Thank you in advance for your answer.\r\n\r\n```diff\r\n# ./scripts/v1_5/finetune_lora.sh\r\n\r\n#!/bin/bash\r\ndeepspeed llava/train/train_mem.py \\\r\n+    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n-   (./scripts/v1_5/finetune.sh is blank)                                                                          \r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path lmsys/vicuna-13b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/llava_v1_5_mix665k.json \\\r\n    --image_folder ./playground/data \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-13b-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n\r\n```</BODY>\n\n<COMMENTS>\n<Comment by Isaachhh at 2023-12-29T09:52:46Z>\nin [`llava_trainer.py`](https://github.com/haotian-liu/LLaVA/blob/main/llava/train/llava_trainer.py#L184),\r\n`p for n, p in opt_model.named_parameters() if (n not in decay_parameters and n in projector_parameters and p.requires_grad)`\r\nprojector_parameters is part of opt_model.named_parameters\r\n\r\nso, in the full setting, projector_parameters would be updated in lr;\r\nin the LoRA setting, projector_parameters would be updated in mm_projector_lr rather than lr\n</Comment>\n<Comment by PommesPeter at 2024-01-08T09:17:08Z>\n> in [`llava_trainer.py`](https://github.com/haotian-liu/LLaVA/blob/main/llava/train/llava_trainer.py#L184), `p for n, p in opt_model.named_parameters() if (n not in decay_parameters and n in projector_parameters and p.requires_grad)` projector_parameters is part of opt_model.named_parameters\r\n> \r\n> so, in the full setting, projector_parameters would be updated in lr; in the LoRA setting, projector_parameters would be updated in mm_projector_lr rather than lr\r\n\r\ntranks for your reply, I have understood it. But `mm_projector` is still not trained when running `./scripts/v1_5/finetune.sh`\n</Comment>\n<Comment by didizhu-zju at 2024-01-12T03:14:56Z>\n> > in [`llava_trainer.py`](https://github.com/haotian-liu/LLaVA/blob/main/llava/train/llava_trainer.py#L184), `p for n, p in opt_model.named_parameters() if (n not in decay_parameters and n in projector_parameters and p.requires_grad)` projector_parameters is part of opt_model.named_parameters\r\n> > so, in the full setting, projector_parameters would be updated in lr; in the LoRA setting, projector_parameters would be updated in mm_projector_lr rather than lr\r\n> \r\n> tranks for your reply, I have understood it. But `mm_projector` is still not trained when running `./scripts/v1_5/finetune.sh`\r\n\r\nMe too. `mm_projector.bin` wasn't saved in the output directory after running the  `./scripts/v1_5/finetune.sh`\n</Comment>\n<Comment by Isaachhh at 2024-01-12T03:38:37Z>\n> > > in [`llava_trainer.py`](https://github.com/haotian-liu/LLaVA/blob/main/llava/train/llava_trainer.py#L184), `p for n, p in opt_model.named_parameters() if (n not in decay_parameters and n in projector_parameters and p.requires_grad)` projector_parameters is part of opt_model.named_parameters\r\n> > > so, in the full setting, projector_parameters would be updated in lr; in the LoRA setting, projector_parameters would be updated in mm_projector_lr rather than lr\r\n> > \r\n> > \r\n> > tranks for your reply, I have understood it. But `mm_projector` is still not trained when running `./scripts/v1_5/finetune.sh`\r\n> \r\n> Me too. `mm_projector.bin` wasn't saved in the output directory after running the `./scripts/v1_5/finetune.sh`\r\n\r\nThe mm_projector has been merged into `pytorch_model.bin`, and you can find the mapping relationship in `pytorch_model.bin.index.json`.\n</Comment>\n<Comment by didizhu-zju at 2024-01-12T06:08:41Z>\n> > > > in [`llava_trainer.py`](https://github.com/haotian-liu/LLaVA/blob/main/llava/train/llava_trainer.py#L184), `p for n, p in opt_model.named_parameters() if (n not in decay_parameters and n in projector_parameters and p.requires_grad)` projector_parameters is part of opt_model.named_parameters\r\n> > > > so, in the full setting, projector_parameters would be updated in lr; in the LoRA setting, projector_parameters would be updated in mm_projector_lr rather than lr\r\n> > > \r\n> > > \r\n> > > tranks for your reply, I have understood it. But `mm_projector` is still not trained when running `./scripts/v1_5/finetune.sh`\r\n> > \r\n> > \r\n> > Me too. `mm_projector.bin` wasn't saved in the output directory after running the `./scripts/v1_5/finetune.sh`\r\n> \r\n> The mm_projector has been merged into `pytorch_model.bin`, and you can find the mapping relationship in `pytorch_model.bin.index.json`.\r\n\r\nThanks. This helps me.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 947,
    "state": "closed",
    "created_by": "anonymous-atom",
    "created_at": "2023-12-26T13:58:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/947</URL>\n\n<TITLE>[Usage] How can I use the LoRA checkpoints for finetuning on custom data ? I tried but ran into some issues</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:  I am trying to finetune LLaVA using LoRA checkpoints using the [finetune_task_lora.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task_lora.sh) but running into some checkpoints loading issue.\r\n\r\nAlso should I used pretrained projectors or LLaVA LoRA pre-trained weights ? @haotian-liu \r\nCommand:\r\n```\r\n#!/bin/bash\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path liuhaotian/llava-v1.5-13b \\   **Don't we need to load LoRA checkpoints here ? Or where then ?**\r\n    --version v1 \\\r\n    --data_path ./playground/data/llava_v1_5_mix665k.json \\\r\n    --image_folder ./playground/data \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-13b-task-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\nI chnaged it to below:\r\n```\r\n\r\n#!/bin/bash\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path liuhaotian/llava-v1.5-13b-lora \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/llava_v1_5_mix665k.json \\\r\n    --image_folder ./playground/data \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-13b-task-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\nLog: \r\n```\r\nno pytorch_model.bin file found\r\n```\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/74659873/2534c2a6-21c1-4048-ba3e-2a8fc22ba926)\r\n\r\nThis is what happens when I train using default [finetune_task_lora.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task_lora.sh) script.</BODY>\n\n<COMMENTS>\n<Comment by anonymous-atom at 2023-12-26T14:16:46Z>\n@henrycjh @FHL1998 can you give it a look ? When I am tryin to load the pre-trained checkpoints from [https://huggingface.co/liuhaotian/llava-v1.5-13b-lora/tree/main](https://huggingface.co/liuhaotian/llava-v1.5-13b-lora/tree/main) they they aren't in the required format. \r\nLike I am tryin to load LoRA checkpoints to finetune, but can't get it quite right.\n</Comment>\n<Comment by anonymous-atom at 2023-12-26T14:22:04Z>\n@oxjohanndiep can you also give a look ?\n</Comment>\n<Comment by anonymous-atom at 2023-12-26T17:25:47Z>\nOk seemse like finetuning is workin, but the model is not producing any output.\n</Comment>\n<Comment by anonymous-atom at 2023-12-26T17:29:52Z>\nLike when I am using the finetuned model through gradio it timeouts and don't produce any output.\n</Comment>\n<Comment by oxjohanndiep at 2023-12-27T07:20:25Z>\nHave you merged the LORA weights at the end with the pre-trained weights?\n</Comment>\n<Comment by anonymous-atom at 2023-12-27T10:28:01Z>\n@oxjohanndiep yeah I used it like below simply\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/74659873/0889f83f-a114-4a74-a5aa-0266e27fc927)\n</Comment>\n<Comment by oxjohanndiep at 2023-12-27T20:40:38Z>\nCan you use this script: https://github.com/haotian-liu/LLaVA/blob/main/scripts/merge_lora_weights.py?\n</Comment>\n<Comment by anonymous-atom at 2024-01-01T09:26:27Z>\nYes I used this script but I am still getting below issue/warning:\r\n\r\n```\r\nSome weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at lmsys/vicuna-13b-v1.5 and are newly initialized: ['model.mm_projector.0.weight', 'model.mm_projector.2.weight', 'model.mm_projector.2.bias', 'model.mm_projector.0.bias']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n```\n</Comment>\n<Comment by anliyuan at 2024-01-02T06:34:06Z>\ndid you fix this？\n</Comment>\n<Comment by anonymous-atom at 2024-01-02T07:05:13Z>\n@anliyuan which issue you are facin exactly ?\n</Comment>\n<Comment by anliyuan at 2024-01-02T07:13:58Z>\n> Yes I used this script but I am still getting below issue/warning:\r\n> \r\n> ```\r\n> Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at lmsys/vicuna-13b-v1.5 and are newly initialized: ['model.mm_projector.0.weight', 'model.mm_projector.2.weight', 'model.mm_projector.2.bias', 'model.mm_projector.0.bias']\r\n> You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n> ```\r\n\r\nthis one.\n</Comment>\n<Comment by attnmamba at 2024-01-30T02:55:35Z>\n@anonymous-atom Were you trying to start finetuning starting from `liuhaotian/llava-v1.5-13b`? \r\n\r\nCan you share your final version of `finetune_task_lora.sh` that is working?\n</Comment>\n<Comment by mfj9999 at 2024-11-25T07:26:08Z>\n> > 是的，我使用了这个脚本，但我仍然遇到以下问题/警告：\r\n> > ```\r\n> > Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at lmsys/vicuna-13b-v1.5 and are newly initialized: ['model.mm_projector.0.weight', 'model.mm_projector.2.weight', 'model.mm_projector.2.bias', 'model.mm_projector.0.bias']\r\n> > You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n> > ```\r\n> \r\n> 这个。\r\n\r\nhello, I also have this question,have you solved it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 946,
    "state": "open",
    "created_by": "JewelShiny",
    "created_at": "2023-12-26T09:08:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/946</URL>\n\n<TITLE>[Question] the scheduler for zero3_offload is different from other configurations, will this have an impact on model performance?</TITLE>\n\n<BODY>### Question\n\nI noticed that the scheduler for zero3_offload is different from other configurations, with WarmupLR for zero3_offload and cosine with warm up for the others，will this have an impact on model performance?\r\n![图片1](https://github.com/haotian-liu/LLaVA/assets/82956781/2981d160-c910-4ddc-8de5-cdeafd2f756d)\r\n![图片2](https://github.com/haotian-liu/LLaVA/assets/82956781/3f574598-19fb-4a96-b2da-f3938a24aa43)</BODY>\n\n<COMMENTS>\n<Comment by ShyFoo at 2024-01-04T09:11:47Z>\nsorry to bother you. how do you solve this? I also found this difference.\n</Comment>\n<Comment by JewelShiny at 2024-01-05T09:04:18Z>\n> sorry to bother you. how do you solve this? I also found this difference.\r\n\r\ni found that this learning rate schedule has little effect on model performance, you may delete the schedule set in zero3_offload.json to enable cosine schedule.\n</Comment>\n<Comment by ShyFoo at 2024-01-05T13:23:39Z>\n> > sorry to bother you. how do you solve this? I also found this difference.\r\n> \r\n> i found that this learning rate schedule has little effect on model performance, you may delete the schedule set in zero3_offload.json to enable cosine schedule.\r\n\r\nthanks! i'm going to try it.\n</Comment>\n<Comment by cherry956 at 2024-03-04T15:07:58Z>\n@2285443514how to solve it?\n</Comment>\n<Comment by yqy2001 at 2024-04-28T03:34:38Z>\n@cherry956 Just delete the [`scheduler`](https://github.com/haotian-liu/LLaVA/blob/3e337ad269da3245643a2724a1d694b5839c37f9/scripts/zero3_offload.json#L22) field in [zero3_offload.json](https://github.com/haotian-liu/LLaVA/blob/main/scripts/zero3_offload.json) can make the lr scheduler bahave properly\n</Comment>\n<Comment by yuzeng0-0 at 2024-07-30T09:34:48Z>\nI also found it!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 945,
    "state": "open",
    "created_by": "chrisx599",
    "created_at": "2023-12-25T13:24:23Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/945</URL>\n\n<TITLE>[Question] KeyError: 'LlavaConfig' of using cli</TITLE>\n\n<BODY>### Question\n\n'''\r\n# python -m llava.serve.cli \\\r\n>     --model-path checkpoints/llava-v1.5-7b-lora \\\r\n>     --model-base checkpoints/vicuna-7b-v1.5 \\\r\n>     --image-file \"https://lmg.jj20.com/up/allimg/tp03/1Z921110450D60-0-lp.jpg\" \\\r\n>     --load-4bit\r\n[2023-12-25 13:22:09,390] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/zhaobai46a02/LLaVA/llava/serve/cli.py\", line 125, in <module>\r\n    main(args)\r\n  File \"/zhaobai46a02/LLaVA/llava/serve/cli.py\", line 32, in main\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n  File \"/zhaobai46a02/LLaVA/llava/model/builder.py\", line 48, in load_pretrained_model\r\n    tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 718, in from_pretrained\r\n    tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 671, in __getitem__\r\n    model_type = self._reverse_config_mapping[key.__name__]\r\nKeyError: 'LlavaConfig'\r\n'''\r\nI have already dowload the weight of llava-v1.5-7b-lora and vicuna-7b-v1.5, but when i use cli, it happens like this</BODY>\n\n<COMMENTS>\n<Comment by Lee-ray-a at 2024-01-15T01:24:00Z>\nI have the same issue. Did you fix it\n</Comment>\n<Comment by attnmamba at 2024-02-01T04:04:58Z>\n@Lee-ray-a @chrisx599 Did you manage to solve this?\n</Comment>\n<Comment by cyj95 at 2024-02-01T08:12:21Z>\nhas anyone fixed it？\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 944,
    "state": "open",
    "created_by": "hanswang1",
    "created_at": "2023-12-23T03:42:30Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/944</URL>\n\n<TITLE>What's the minimum GPU requirement to Fine Tune LLaVA</TITLE>\n\n<BODY>### Question\n\nWhat's the minimum GPU device needed to do Fine Tuning of LLaVA?</BODY>\n\n<COMMENTS>\n<Comment by rapsar at 2024-01-27T03:05:58Z>\nI could finetune the 7B model on a single A100 GPU 40GB (Google Colab)\r\nA100 or H100 are required for CUDA.\r\nI could bypass the CUDA requirement but V100 or T4 GPUs didn't have enough memory\n</Comment>\n<Comment by CinKKKyo at 2024-03-24T02:42:11Z>\n> I could finetune the 7B model on a single A100 GPU 40GB (Google Colab) A100 or H100 are required for CUDA. I could bypass the CUDA requirement but V100 or T4 GPUs didn't have enough memory\r\n\r\nDo you mean the minimum memory for a single GPU is 40G?\n</Comment>\n<Comment by AngelAlita at 2024-06-13T14:18:25Z>\n> I could finetune the 7B model on a single A100 GPU 40GB (Google Colab) A100 or H100 are required for CUDA. I could bypass the CUDA requirement but V100 or T4 GPUs didn't have enough memory\r\n\r\nhello,Is the A100 (40GB) GPU used for full parameter fine-tuning?\n</Comment>\n<Comment by broiron at 2024-10-02T08:54:39Z>\n> I could finetune the 7B model on a single A100 GPU 40GB (Google Colab) A100 or H100 are required for CUDA. I could bypass the CUDA requirement but V100 or T4 GPUs didn't have enough memory\r\n\r\nCan you please provide more details? \r\nI'd like to know whether it is possible to train or fine-tune LLaVA in single GPU (A100 or H100)\n</Comment>\n<Comment by Gys19 at 2025-03-19T13:46:06Z>\n> U 40GB (Google Colab)\n> A100 or H100 are required for CUDA.\n> I could bypass the CUDA requirement but V100 or T4 GPUs didn't have enough m\n\ni am running fine-tuning in colab using A100 with 40GB RAM, but it shows cuda out of memory, feel frustrated ...\n</Comment>\n<Comment by Gys19 at 2025-03-19T14:02:32Z>\nI turned to fine tune 7b model using 1.5, but it fails when writing the trained model into checkpoints...\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 943,
    "state": "open",
    "created_by": "chenchun0629",
    "created_at": "2023-12-23T02:28:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/943</URL>\n\n<TITLE>[Question] Finetune with chinese-clip</TITLE>\n\n<BODY>### Question\n\n## motivation:\r\n\r\nI try to use chinese-clip replace clip.\r\n\r\n## environment\r\n\r\n```bash\r\n$ uname -a \r\nLinux localhost.localdomain 3.10.0-1160.80.1.el7.x86_64 #1 SMP Tue Nov 8 15:48:59 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n$ pip\r\nllava                         1.1.3  /data/jupyter/user/cc/LLaVA\r\ntorch                         2.0.1\r\naccelerate                    0.21.0\r\ndeepspeed                     0.12.6\r\nflash-attn                    2.3.6\r\n\r\n\r\n$ nvcc -V\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2023 NVIDIA Corporation\r\nBuilt on Mon_Apr__3_17:16:06_PDT_2023\r\nCuda compilation tools, release 12.1, V12.1.105\r\nBuild cuda_12.1.r12.1/compiler.32688072_0\r\n```\r\n\r\n## step 1: Pretrain\r\n\r\n```\r\n#!/bin/bash\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --model_name_or_path /data/llm_models/vicuna-7b-v1.5 \\\r\n    --version plain \\\r\n    --data_path /data/llm_datasets/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json \\\r\n    --image_folder /data/llm_datasets/LLaVA-Pretrain \\\r\n    --vision_tower OFA-Sys/chinese-clip-vit-large-patch14-336px \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/vicuna-v1.5-7b-chinese-clip-pretrain \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 8 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 4 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 24000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 1e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nThe current pretraining step has been successfully executed.\r\n\r\n## step 2: Visual Instruction Tuning\r\n```\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3_offload_yi.json \\\r\n    --model_name_or_path /data/llm_models/vicuna-7b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path /data/llm_datasets/LLaVA-Visual-Instruction-Tuning/llava_v1_5_mix665k.coco.json \\\r\n    --image_folder /data/llm_datasets/LLaVA-Visual-Instruction-Tuning \\\r\n    --vision_tower OFA-Sys/chinese-clip-vit-large-patch14-336px \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/vicuna-v1.5-7b-chinese-clip-pretrain/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/vicuna-v1.5-7b-chinese-clip-finetune \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n\r\n```\r\n\r\nI have promble in this current step.\r\n```\r\nreturn self.vision_model(                                                                                                                                                            [32/1954]\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n  File \"/data/jupyter/user/cc/LLaVA-cc/llava/train/train_mem.py\", line 17, in <module>\r\n    train()\r\n  File \"/data/jupyter/user/cc/LLaVA/llava/train/train.py\", line 965, in train\r\n    trainer.train()\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2654, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2679, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/chinese_clip/modeling_chinese_clip.py\", line 1084, in forward\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1833, in forward\r\n    loss = self.module(*inputs, **kwargs)\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n      File \"/data/jupyter/user/cc/LLaVA/llava/model/language_model/llava_llama.py\", line 79, in forward\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\nhidden_states = self.embeddings(pixel_values)\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n  File \"/data/jupyter/user/cc/LLaVA/llava/model/llava_arch.py\", line 121, in prepare_inputs_labels_for_multimodal\r\n    image_features = self.encode_images(images).to(self.device)\r\n  File \"/data/jupyter/user/cc/LLaVA/llava/model/llava_arch.py\", line 95, in encode_images\r\n    image_features = self.get_model().get_vision_tower()(images)\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/data/jupyter/user/cc/LLaVA/llava/model/multimodal_encoder/chinese_clip_encoder.py\", line 48, in forward\r\n    image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/chinese_clip/modeling_chinese_clip.py\", line 1340, in forward\r\n    return self.vision_model(\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/chinese_clip/modeling_chinese_clip.py\", line 1084, in forward\r\n    hidden_states = self.embeddings(pixel_values)\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/chinese_clip/modeling_chinese_clip.py\", line 204, in forward\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n    embeddings = embeddings + self.position_embedding(self.position_ids)\r\nRuntimeError: The size of tensor a (257) must match the size of tensor b (577) at non-singleton dimension 1\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/chinese_clip/modeling_chinese_clip.py\", line 204, in forward\r\n    embeddings = embeddings + self.position_embedding(self.position_ids)\r\nRuntimeError: The size of tensor a (257) must match the size of tensor b (577) at non-singleton dimension 1\r\n```\r\n\r\nMay I ask if there are any suggestions on how I can solve this problem.\r\n\r\nThanks!</BODY>\n\n<COMMENTS>\n<Comment by chenchun0629 at 2023-12-23T02:32:56Z>\nsome code\r\n\r\n```\r\n# builder.py\r\n\r\nimport os\r\nfrom .clip_encoder import CLIPVisionTower\r\nfrom .chinese_clip_encoder import ChineseCLIPVisionTower\r\n\r\n\r\ndef build_vision_tower(vision_tower_cfg, **kwargs):\r\n    vision_tower = getattr(vision_tower_cfg, 'mm_vision_tower', getattr(vision_tower_cfg, 'vision_tower', None))\r\n    is_absolute_path_exists = os.path.exists(vision_tower)\r\n    if is_absolute_path_exists or vision_tower.startswith(\"openai\") or vision_tower.startswith(\"laion\"):\r\n        return CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\r\n    elif 'chinese-clip' in vision_tower:\r\n        return ChineseCLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\r\n\r\n    raise ValueError(f'Unknown vision tower: {vision_tower}')\r\n\r\n\r\n```\r\n\r\n\r\n```\r\n# chinese_clip_encoder.py\r\n\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nfrom transformers import ChineseCLIPVisionModel, ChineseCLIPImageProcessor, ChineseCLIPVisionConfig\r\n\r\n\r\nclass ChineseCLIPVisionTower(nn.Module):\r\n    def __init__(self, vision_tower, args, delay_load=False):\r\n        super().__init__()\r\n\r\n        self.is_loaded = False\r\n\r\n        self.vision_tower_name = vision_tower\r\n        self.select_layer = args.mm_vision_select_layer\r\n        self.select_feature = getattr(args, 'mm_vision_select_feature', 'patch')\r\n\r\n        if not delay_load:\r\n            self.load_model()\r\n        else:\r\n            self.cfg_only = ChineseCLIPVisionConfig.from_pretrained(self.vision_tower_name)\r\n\r\n    def load_model(self):\r\n        self.image_processor = ChineseCLIPImageProcessor.from_pretrained(self.vision_tower_name)\r\n        self.vision_tower = ChineseCLIPVisionModel.from_pretrained(self.vision_tower_name)\r\n        self.vision_tower.requires_grad_(False)\r\n\r\n        self.is_loaded = True\r\n\r\n    def feature_select(self, image_forward_outs):\r\n        image_features = image_forward_outs.hidden_states[self.select_layer]\r\n        if self.select_feature == 'patch':\r\n            image_features = image_features[:, 1:]\r\n        elif self.select_feature == 'cls_patch':\r\n            image_features = image_features\r\n        else:\r\n            raise ValueError(f'Unexpected select feature: {self.select_feature}')\r\n        return image_features\r\n\r\n    @torch.no_grad()\r\n    def forward(self, images):\r\n        if type(images) is list:\r\n            image_features = []\r\n            for image in images:\r\n                image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)\r\n                image_feature = self.feature_select(image_forward_out).to(image.dtype)\r\n                image_features.append(image_feature)\r\n        else:\r\n            image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\r\n            image_features = self.feature_select(image_forward_outs).to(images.dtype)\r\n\r\n        return image_features\r\n\r\n    @property\r\n    def dummy_feature(self):\r\n        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\r\n\r\n    @property\r\n    def dtype(self):\r\n        return self.vision_tower.dtype\r\n\r\n    @property\r\n    def device(self):\r\n        return self.vision_tower.device\r\n\r\n    @property\r\n    def config(self):\r\n        if self.is_loaded:\r\n            return self.vision_tower.config\r\n        else:\r\n            return self.cfg_only\r\n\r\n    @property\r\n    def hidden_size(self):\r\n        return self.config.hidden_size\r\n\r\n    @property\r\n    def num_patches(self):\r\n        return (self.config.image_size // self.config.patch_size) ** 2\r\n\r\n```\n</Comment>\n<Comment by chenchun0629 at 2023-12-23T08:57:27Z>\nAfter debugging, it was found that the image_size will decrease from the original 336 to 224\r\n```\r\nLlavaLlamaForCausalLM.images.shape: torch.Size([1, 3, 336, 336])\r\nLlavaMetaForCausalLM0.images.shape: torch.Size([1, 3, 336, 336])\r\nLlavaMetaForCausalLM.images.shape: torch.Size([1, 3, 336, 336])\r\nChineseCLIPVisionTower.images.shape: torch.Size([1, 3, 336, 336])\r\nLlavaLlamaForCausalLM.images.shape: torch.Size([1, 3, 336, 336])\r\nLlavaMetaForCausalLM0.images.shape: torch.Size([1, 3, 336, 336])\r\nLlavaMetaForCausalLM.images.shape: torch.Size([1, 3, 336, 336])\r\nChineseCLIPVisionTower.images.shape: torch.Size([1, 3, 336, 336])\r\n{'loss': 2.6496, 'learning_rate': 3.1826973105233184e-06, 'epoch': 0.0}                                                                                                                       \r\n  0%|                                                                                                                                                  | 4/202394 \r\nLlavaLlamaForCausalLM.images.shape: torch.Size([1, 3, 224, 224])\r\nLlavaMetaForCausalLM0.images.shape: torch.Size([1, 3, 224, 224])\r\nLlavaMetaForCausalLM.images.shape: torch.Size([1, 3, 224, 224])\r\nChineseCLIPVisionTower.images.shape: torch.Size([1, 3, 224, 224])\r\nLlavaLlamaForCausalLM.images.shape: torch.Size([1, 3, 224, 224])\r\nLlavaMetaForCausalLM0.images.shape: torch.Size([1, 3, 224, 224])\r\nLlavaMetaForCausalLM.images.shape: torch.Size([1, 3, 224, 224])\r\nChineseCLIPVisionTower.images.shape: torch.Size([1, 3, 224, 224])\r\nTraceback (most recent call last):\r\n  File \"/data/jupyter/user/cc/LLaVA-cc/llava/train/train_mem.py\", line 17, in <module>\r\n    train()\r\n  File \"/data/jupyter/user/cc/LLaVA/llava/train/train.py\", line 965, in train\r\n    trainer.train()\r\n  File \"/data/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n...\r\n...\r\n```\n</Comment>\n<Comment by chenchun0629 at 2023-12-25T02:34:37Z>\n```\r\n# 336*336 in LazySupervisedDataset and DataCollatorForSupervisedDataset\r\nLazySupervisedDataset.images: torch.Size([3, 336, 336])\r\nDataCollatorForSupervisedDataset.images: torch.Size([3, 336, 336])\r\n# 224*224 in transformers.trainer\r\ntrainer0.images: torch.Size([1, 3, 224, 224])\r\ntrainer1.images: torch.Size([1, 3, 224, 224])\r\n```\r\n\r\n```\r\n# transformers.trainer\r\n            for step, inputs in enumerate(epoch_iterator):\r\n                print(\"trainer0.images:\", inputs[\"images\"].shape, flush=True)\r\n                total_batched_samples += 1\r\n                if rng_to_sync:\r\n                    self._load_rng_state(resume_from_checkpoint)\r\n                    rng_to_sync = False\r\n\r\n                # Skip past any already trained steps if resuming training\r\n                if steps_trained_in_current_epoch > 0:\r\n                    steps_trained_in_current_epoch -= 1\r\n                    if steps_trained_progress_bar is not None:\r\n                        steps_trained_progress_bar.update(1)\r\n                    if steps_trained_in_current_epoch == 0:\r\n                        self._load_rng_state(resume_from_checkpoint)\r\n                    continue\r\n                elif steps_trained_progress_bar is not None:\r\n                    steps_trained_progress_bar.close()\r\n                    steps_trained_progress_bar = None\r\n\r\n                if step % args.gradient_accumulation_steps == 0:\r\n                    self.control = self.callback_handler.on_step_begin(args, self.state, self.control)\r\n\r\n                with self.accelerator.accumulate(model):\r\n                    print(\"trainer1.images:\", inputs[\"images\"].shape, flush=True)\r\n                    tr_loss_step = self.training_step(model, inputs)\r\n```\n</Comment>\n<Comment by chenchun0629 at 2023-12-26T03:43:42Z>\nI try use blip_laion_cc_sbu_558k.json datasets to finetune model, is works.\r\n\r\nWhen I use llava_v1_5_mix665k.json datasets, I encounter this issue.\n</Comment>\n<Comment by GITMrzk at 2024-02-02T07:45:39Z>\n你的维度问题看起来就是224和336clip出的特征维度对不上吧 257 和 577不是正对着224和336 clip patch 14时出的image token数么，加的1就是cls token\n</Comment>\n<Comment by ScottishFold007 at 2024-03-26T07:39:14Z>\n请问，你这个问题解决了吗？\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 942,
    "state": "open",
    "created_by": "unmo",
    "created_at": "2023-12-22T08:20:27Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/942</URL>\n\n<TITLE>[Question] Each argument mean of training script</TITLE>\n\n<BODY>### Question\n\nWhere is the meaning of each argument in the training scripts?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 941,
    "state": "open",
    "created_by": "ganliqiang",
    "created_at": "2023-12-22T03:24:59Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/941</URL>\n\n<TITLE>[Question] shouldn't int8/int4 be faster?</TITLE>\n\n<BODY>### Question\n\nWhy is model inference with int8/int4 so much slower than with float16/float32? Aside from decreased memory consumption, shouldn't int8/int4 be faster?![image](https://github.com/haotian-liu/LLaVA/assets/15537456/c17896df-1e99-4d23-8262-70677e6f8512)</BODY>\n\n<COMMENTS>\n<Comment by gulegeji at 2024-01-02T09:34:12Z>\nsame question, this is my speed:\r\n4bit: \r\n![1704187948408](https://github.com/haotian-liu/LLaVA/assets/128966293/8bd9b660-6556-4958-b2b8-c79bc663b26b)\r\n8bit:\r\n![1704187897736](https://github.com/haotian-liu/LLaVA/assets/128966293/cf520d4b-4f31-4520-91e2-d168c6a6488b)\r\nfull-precision:\r\n![1704187920503](https://github.com/haotian-liu/LLaVA/assets/128966293/581b7a83-a1ae-4385-b2c1-ce275a8b3cc8)\r\n\r\n===\r\ni use 1xA800 for inference, and use merged lora\n</Comment>\n<Comment by RylanSchaeffer at 2024-02-28T23:33:56Z>\n@ganliqiang @gulegeji how did you two set the precision?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 940,
    "state": "open",
    "created_by": "a2382625920",
    "created_at": "2023-12-22T03:16:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/940</URL>\n\n<TITLE>[Resolution Question] Question about whether it is possible to increase the resolution of LLAVA images</TITLE>\n\n<BODY>### Question\n\nHi, I'm happy to use the LLAVA model, which allows multimodal macromodels to rise one level higher!\r\nBut I found some problems in fine tuning, or maybe it's a problem with the LLAVA model itself, that the resolution of LLAVA is only 336*336.\r\nI would like to increase the resolution of LLAVA by fine-tuning the use of BLIP2 or other models, because the increase of resolution can get more features, is there any way to change the resolution of LLAVA, or is there a file to do a one-stop fine-tuning?\r\nThank you for your contribution and would appreciate your solution or how-to.\r\nThanks again!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 939,
    "state": "open",
    "created_by": "cathyxl",
    "created_at": "2023-12-21T14:54:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/939</URL>\n\n<TITLE>Which CUDA version is used in LLava 1.5 training?</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nI have a problem reproducing the GQA result, mine is 58.+(even repeated for 3 times), but the reported is 62.  I use the same data and scripts when doing the visual instruct fine-tuning.  I got transformers=4.31.0, and torch=2.0.1 in my environment. My question is whether the Cuda version (mine is 11.7) affects the results. Which Cuda version were you using in the training?</BODY>\n\n<COMMENTS>\n<Comment by yjcaimeow at 2024-02-20T12:23:22Z>\nhi @cathyxl \r\n\r\nsame question，any update?\n</Comment>\n<Comment by yjcaimeow at 2024-02-20T12:31:06Z>\nhttps://github.com/haotian-liu/LLaVA/issues/368\n</Comment>\n<Comment by lixiaotong97 at 2024-03-18T01:56:33Z>\nalso same question，any update?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 938,
    "state": "open",
    "created_by": "1106280506Hx",
    "created_at": "2023-12-21T13:14:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/938</URL>\n\n<TITLE>[Question] RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasGemmEx( handle, opa, opb, m, n, k, &falpha, a, CUDA_R_16F, lda, b, CUDA_R_16F, ldb, &fbeta, c, CUDA_R_16F, ldc, CUDA_R_32F, CUBLAS_GEMM_DFALT_TENSOR_OP)`</TITLE>\n\n<BODY>### Question\n\n2023-12-21 20:51 Traceback (most recent call last):\r\n2023-12-21 20:51   File \"/gemini/code/llava_batch0.py\", line 97, in <module>\r\n2023-12-21 20:51     process_images_new(url[i])\r\n2023-12-21 20:51   File \"/gemini/code/llava_batch0.py\", line 78, in process_images_new\r\n2023-12-21 20:51     sentences=VQA_batch(image_path)\r\n2023-12-21 20:51   File \"/gemini/code/llava_batch0.py\", line 32, in VQA_batch\r\n2023-12-21 20:51     output = model.generate(**inputs, max_new_tokens=200).to(\"cuda\")\r\n2023-12-21 20:51   File \"/root/miniconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n2023-12-21 20:51     return func(*args, **kwargs)\r\n2023-12-21 20:51   File \"/root/miniconda3/lib/python3.9/site-packages/transformers/generation/utils.py\", line 1718, in generate\r\n2023-12-21 20:51     return self.greedy_search(\r\n2023-12-21 20:51   File \"/root/miniconda3/lib/python3.9/site-packages/transformers/generation/utils.py\", line 2579, in greedy_search\r\n2023-12-21 20:51     outputs = self(\r\n2023-12-21 20:51   File \"/root/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-12-21 20:51     return forward_call(*args, **kwargs)\r\n2023-12-21 20:51   File \"/root/miniconda3/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2023-12-21 20:51     output = module._old_forward(*args, **kwargs)\r\n2023-12-21 20:51   File \"/root/miniconda3/lib/python3.9/site-packages/transformers/models/llava/modeling_llava.py\", line 433, in forward\r\n2023-12-21 20:51     outputs = self.language_model(\r\n2023-12-21 20:51   File \"/root/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-12-21 20:51     return forward_call(*args, **kwargs)\r\n2023-12-21 20:51   File \"/root/miniconda3/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2023-12-21 20:51     output = module._old_forward(*args, **kwargs)\r\n2023-12-21 20:51   File \"/root/miniconda3/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\", line 1174, in forward\r\n2023-12-21 20:51     outputs = self.model(\r\n2023-12-21 20:51   File \"/root/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-12-21 20:51     return forward_call(*args, **kwargs)\r\n2023-12-21 20:51   File \"/root/miniconda3/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2023-12-21 20:51     output = module._old_forward(*args, **kwargs)\r\n2023-12-21 20:51   File \"/root/miniconda3/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\", line 1061, in forward\r\n2023-12-21 20:51     layer_outputs = decoder_layer(\r\n2023-12-21 20:51   File \"/root/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-12-21 20:51     return forward_call(*args, **kwargs)\r\n2023-12-21 20:51   File \"/root/miniconda3/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2023-12-21 20:51     output = module._old_forward(*args, **kwargs)\r\n2023-12-21 20:51   File \"/root/miniconda3/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\", line 789, in forward\r\n2023-12-21 20:51     hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n2023-12-21 20:51   File \"/root/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-12-21 20:51     return forward_call(*args, **kwargs)\r\n2023-12-21 20:51   File \"/root/miniconda3/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2023-12-21 20:51     output = module._old_forward(*args, **kwargs)\r\n2023-12-21 20:51   File \"/root/miniconda3/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\", line 386, in forward\r\n2023-12-21 20:51     query_states = self.q_proj(hidden_states)\r\n2023-12-21 20:51   File \"/root/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-12-21 20:51     return forward_call(*args, **kwargs)\r\n2023-12-21 20:51   File \"/root/miniconda3/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2023-12-21 20:51     output = module._old_forward(*args, **kwargs)\r\n2023-12-21 20:51   File \"/root/miniconda3/lib/python3.9/site-packages/bitsandbytes/nn/modules.py\", line 256, in forward\r\n2023-12-21 20:51     out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)\r\n2023-12-21 20:51   File \"/root/miniconda3/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py\", line 577, in matmul_4bit\r\n2023-12-21 20:51     return MatMul4Bit.apply(A, B, out, bias, quant_state)\r\n2023-12-21 20:51   File \"/root/miniconda3/lib/python3.9/site-packages/torch/autograd/function.py\", line 506, in apply\r\n2023-12-21 20:51     return super().apply(*args, **kwargs)  # type: ignore[misc]\r\n2023-12-21 20:51   File \"/root/miniconda3/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py\", line 516, in forward\r\n2023-12-21 20:51     output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\r\n2023-12-21 20:51 RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasGemmEx( handle, opa, opb, m, n, k, &falpha, a, CUDA_R_16F, lda, b, CUDA_R_16F, ldb, &fbeta, c, CUDA_R_16F, ldc, CUDA_R_32F, CUBLAS_GEMM_DFALT_TENSOR_OP)`              When I was reasoning, I ran into this problem</BODY>\n\n<COMMENTS>\n<Comment by xmy0916 at 2023-12-27T02:27:06Z>\nhttps://github.com/pytorch/pytorch/issues/56747, I have encountered the same error during inference phase. maybe is the bug of pytorch. maybe upgrade your cuda, cudnn, pytorch version will help.\n</Comment>\n<Comment by lucasjinreal at 2024-07-08T16:53:13Z>\nSame error but not in llava, so eried.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 937,
    "state": "closed",
    "created_by": "oxjohanndiep",
    "created_at": "2023-12-21T10:42:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/937</URL>\n\n<TITLE>[Question] How to use my model after I finetuned it using finetune_lora_task.sh on my own dataset?</TITLE>\n\n<BODY>### Question\n\nHello,\r\n\r\nI finetuned the model using the finetune_lora_task.sh script on my own custom data. After running it for 12 hours, the training finishes without error, and I got a folder with the following items:\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/105214231/be6a9601-886e-463b-8279-41a300630b00)\r\n\r\nHow do I now use this model for inference? It does not contain any of those pytorch_model.bin files (https://huggingface.co/lmsys/vicuna-13b-v1.5/tree/main), so I am not sure if it saved everything.\r\n\r\nEvery help is appreciated.</BODY>\n\n<COMMENTS>\n<Comment by ilyesbk11 at 2023-12-22T09:35:16Z>\nHello @oxjohanndiep \r\ncan I ask you some questions ?\n</Comment>\n<Comment by oxjohanndiep at 2023-12-22T11:23:20Z>\nSure\n</Comment>\n<Comment by ilyesbk11 at 2023-12-22T11:40:27Z>\nWhat is the size of your data, and what is the computational power used to fine-tune the model ?\r\nWhat's the GPU used and did you perform it locally ?\r\nAlso, can you provide the script used.\r\nThank you for your time.\n</Comment>\n<Comment by oxjohanndiep at 2023-12-22T12:16:06Z>\n> What is the size of your data, and what is the computational power used to fine-tune the model ? What's the GPU used and did you perform it locally ? Also, can you provide the script used. Thank you for your time.\r\n\r\nYou can find everything you asked for in the README of this repo.\n</Comment>\n<Comment by ilyesbk11 at 2023-12-22T13:04:49Z>\nYes I know so I just wanted to know more details since you did the fine-tuning.\n</Comment>\n<Comment by anonymous-atom at 2024-01-03T08:24:04Z>\n@oxjohanndiep You can merge the LoRA weights using [scripts/merge_lora_weights.py](https://github.com/haotian-liu/LLaVA/blob/main/scripts/merge_lora_weights.py). Also make sure to use correct base model.\r\n\r\nThen you will get an output folder with weights and config file which you can further use using Gradio Web GUI or CLI as mentioned in README.md\n</Comment>\n<Comment by SoniaGrh at 2024-01-04T17:06:50Z>\nTry to use the Predictor class in the file [predict.py](https://github.com/haotian-liu/LLaVA/blob/main/predict.py) and replace the setup function of the class with this:\r\n\r\n```\r\ndef setup(self) -> None:\r\n    self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\"checkpoints/llava-v1.5-13b-task-lora\", model_name=\"llava-v1.5-13b-lora-finetuned\", model_base=\"liuhaotian/llava-v1.5-13b\", load_8bit=False, load_4bit=False)\r\n```\n</Comment>\n<Comment by AI-Aether at 2024-01-05T09:13:01Z>\n> Try to use the Predictor class in the file [predict.py](https://github.com/haotian-liu/LLaVA/blob/main/predict.py) and replace the setup function of the class with this:\r\n> \r\n> ```\r\n> def setup(self) -> None:\r\n>     self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\"checkpoints/llava-v1.5-13b-task-lora\", model_name=\"llava-v1.5-13b-lora-finetuned\", model_base=\"liuhaotian/llava-v1.5-13b\", load_8bit=False, load_4bit=False)\r\n> ```\r\n\r\nCan I run this even without merging the weights and just with the adapter_model.bin files?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 936,
    "state": "open",
    "created_by": "FurkanGozukara",
    "created_at": "2023-12-20T01:28:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/936</URL>\n\n<TITLE>Multi GPU error Segmentation fault (core dumped)</TITLE>\n\n<BODY>### Describe the issue\n\npython 3.11.6\r\n\r\n```\r\nubuntu@1fb4db8af451:~/sofas$ cd /home/ubuntu\r\nexport HF_HOME=\"/home/ubuntu\"\r\ncd /home/ubuntu/llava_install/LLaVA\r\nsource venv/bin/activate\r\nexport CUDA_VISIBLE_DEVICES=0,1,2\r\n(venv) ubuntu@1fb4db8af451:~/llava_install/LLaVA$ CUDA_VISIBLE_DEVICES=0,1,2 python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b\r\n2023-12-20 01:25:33 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='liuhaotian/llava-v1.5-13b', model_base=None, model_name=None, device='cuda', multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=False)\r\n2023-12-20 01:25:33 | INFO | model_worker | Loading the model llava-v1.5-13b on worker 62aa5c ...\r\nLoading checkpoint shards:   0%|                                                                                                                                                                                                                           | 0/3 [00:00<?, ?it/s]\r\nLoading checkpoint shards:  33%|██████████████████████████████████████████████████████████████████████▎                                                                                                                                            | 1/3 [00:10<00:21, 11.00s/it]\r\nLoading checkpoint shards:  67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                      | 2/3 [00:22<00:11, 11.23s/it]\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:28<00:00,  8.84s/it]\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:28<00:00,  9.46s/it]\r\n2023-12-20 01:26:05 | ERROR | stderr | \r\n2023-12-20 01:26:14 | INFO | model_worker | Register to controller\r\n2023-12-20 01:26:14 | ERROR | stderr | INFO:     Started server process [76028]\r\n2023-12-20 01:26:14 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2023-12-20 01:26:14 | ERROR | stderr | INFO:     Application startup complete.\r\n2023-12-20 01:26:14 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)\r\n2023-12-20 01:26:29 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: None. global_counter: 0\r\n2023-12-20 01:26:44 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: None. global_counter: 0\r\n2023-12-20 01:26:59 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: None. global_counter: 0\r\n2023-12-20 01:27:10 | INFO | stdout | INFO:     127.0.0.1:33148 - \"POST /worker_get_status HTTP/1.1\" 200 OK\r\n2023-12-20 01:27:14 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: None. global_counter: 0\r\n2023-12-20 01:27:18 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n2023-12-20 01:27:18 | INFO | stdout | INFO:     127.0.0.1:48416 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\nSegmentation fault (core dumped)\r\n(venv) ubuntu@1fb4db8af451:~/llava_install/LLaVA$ \r\n\r\n```</BODY>\n\n<COMMENTS>\n<Comment by KirtikumarJadhav1 at 2025-04-04T07:23:52Z>\ni have same error, if you solve this can you please tell the solution for this issue\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 935,
    "state": "open",
    "created_by": "farrokhsiar",
    "created_at": "2023-12-19T23:55:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/935</URL>\n\n<TITLE>[Question] Finetuning of the LLM Portion</TITLE>\n\n<BODY>### Question\n\nIs there any instruction on how to fine-tune the LLM portion, without making anychange to the image-to-text part of the model?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 934,
    "state": "open",
    "created_by": "zilunzhang",
    "created_at": "2023-12-18T14:33:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/934</URL>\n\n<TITLE>[Usage] OOM when using single A100-40G x8 node (Visual Instruction Tuning)</TITLE>\n\n<BODY>Hi Haotian, \r\n\r\nOOM happened when I ran \"finetune.sh\" from scripts/v1_5. I used single node A100-40G x8, **without nvlink** to fine-tune a 7B LLaVA-1.5.\r\n\r\nThe estimated training time is ~24 hours when using the default setting. The time increase is understandable (compared with 10 hrs for A100-40G x8 **with nvlink**). However, when I ran around 100 steps, the OOM happened.\r\n\r\nWarning below pop-ups frequently during training.\r\n\r\n```\r\npytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\r\n```\r\n\r\nSince fine-tuning the 7B model on A100-40G x8 **with nvlink** worked on your machine, I wonder if the missing nvlink caused this warning and OOM. For example, if the cache can be dequeued more frequently using nvlink rather than PCIE, thus reducing the pressure of memory consumption.\r\n\r\nHave you tested tuning the 7B model on the machine that has A100-40G x8 **without nvlink**?\r\n\r\nThe training script is attached below.\r\n\r\n```\r\n#!/bin/bash\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path ./checkpoints/vicuna-7b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path ./playground/data/llava_v1_5_mix665k.json \\\r\n    --image_folder /data/tmp/shz/data/dataset \\\r\n    --vision_tower ./checkpoints/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-7b-pretrain/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 5000 \\\r\n    --save_total_limit 5 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\n\r\nThanks,\r\n\r\nZilun</BODY>\n\n<COMMENTS>\n<Comment by FaltingsA at 2024-03-17T17:38:29Z>\n@zilunzhang Have you solved this problem?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 933,
    "state": "open",
    "created_by": "1106280506Hx",
    "created_at": "2023-12-18T13:23:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/933</URL>\n\n<TITLE>operator(): block: [0,0,0], thread: [0,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.[Question]</TITLE>\n\n<BODY>### Question\n\nI ran into this problem while reasoning, please how to solve it        operator(): block: [0,0,0], thread: [0,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.</BODY>\n\n<COMMENTS>\n<Comment by olcaybuyan at 2024-01-04T00:04:58Z>\nI ran into the same issue while processing all frames from video files. Some specific frames from some videos reproduced this error. What solved it for me was changing the way converted the image pixels from BGR to RGB.\r\n\r\n**old code:**\r\nframe = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\nimage = Image.fromarray(frame)\r\n\r\n**new code:**\r\nimage = Image.fromarray(frame).convert('RGB')\r\n\r\nI didn't look into it further, but I guess some pixel values provoked invalid values in the final image PyTorch tensor.\r\n\r\nHope this helps anyone reading this in the future ✌️\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 931,
    "state": "open",
    "created_by": "John-Ge",
    "created_at": "2023-12-18T08:25:14Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/931</URL>\n\n<TITLE>[Usage] Faster OCR VQA download.</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: I find download ocr vqa images are slow and I implement a multi-thread download file.\r\n\r\nCommand:\r\n```\r\nimport concurrent.futures\r\ndef download_image(k):\r\n    ext = os.path.splitext(data[k]['imageURL'])[1]\r\n    outputFile = 'images/%s%s' % (k, ext)\r\n\r\n    # Only download the image if it doesn't exist\r\n    if not os.path.exists(outputFile):\r\n        ureq.urlretrieve(data[k]['imageURL'], outputFile)\r\n\r\n\r\nif download == 1:\r\n    # Create the directory if it doesn't exist\r\n    if not os.path.exists('./images'):\r\n        os.mkdir('./images')\r\n\r\n    # Create a thread pool and download the images in parallel\r\n    with concurrent.futures.ThreadPoolExecutor() as executor:\r\n        executor.map(download_image, data.keys())\r\n```\r\nThis maybe useful.</BODY>\n\n<COMMENTS>\n<Comment by xulinui at 2024-01-11T14:11:04Z>\nhttps://blog.csdn.net/weixin_46460463/article/details/135537718?spm=1001.2014.3001.5501\n</Comment>\n<Comment by master-chou at 2024-04-18T13:00:54Z>\nthanks ！！！！\n</Comment>\n<Comment by cooleel at 2024-05-06T00:13:41Z>\n> https://blog.csdn.net/weixin_46460463/article/details/135537718?spm=1001.2014.3001.5501\r\n\r\nThanks for posting the method, but some of the images that I downloaded are damaged. Did anyone face the same issue?\n</Comment>\n<Comment by shikhar-srivastava at 2025-04-05T03:34:14Z>\n> > https://blog.csdn.net/weixin_46460463/article/details/135537718?spm=1001.2014.3001.5501\n> \n> Thanks for posting the method, but some of the images that I downloaded are damaged. Did anyone face the same issue?\n\n@cooleel Solved here: https://github.com/haotian-liu/LLaVA/issues/1618\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 930,
    "state": "open",
    "created_by": "Ody-trek",
    "created_at": "2023-12-18T08:02:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/930</URL>\n\n<TITLE>[Question] NCCL network set up issue</TITLE>\n\n<BODY>### Question\r\n\r\nWhen I do the finetuning on my datasets on 8xA6000, there is always this issue:\r\n\r\n```\r\nRuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer. This may indicate a possible application crash on rank 0 or a network set up issue.\r\n    return cdb.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/deepspeed/comm/torch.py\", line 129, in broadcast\r\n    return torch.distributed.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py\", line 1451, in wrapper\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py\", line 1566, in broadcast\r\n    work = default_pg.broadcast([tensor], opts)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: [6] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer. This may indicate a possible application crash on rank 0 or a network set up issue.\r\n[2023-12-18 07:53:25,201] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1640\r\n[2023-12-18 07:53:25,202] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1641\r\n[2023-12-18 07:53:25,778] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1642\r\n[2023-12-18 07:53:25,912] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1643\r\n[2023-12-18 07:53:25,913] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1644\r\n[2023-12-18 07:53:25,921] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1645\r\n[2023-12-18 07:53:25,974] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1646\r\n[2023-12-18 07:53:25,975] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1647\r\n[2023-12-18 07:53:25,975] [ERROR] [launch.py:321:sigkill_handler] ['/opt/conda/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=7', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', './converted_data_for_finetuning.json', '--image_folder', './playground/data', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-7b-task', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = 1\r\n```\r\n\r\nAnd my executable script is ./scripts/v1_5/finetune_task.sh \r\n```\r\n#!/bin/bash\r\n\r\n# 设置 NCCL 仅使用 docker0 网络接口\r\nexport NCCL_SOCKET_IFNAME=docker0\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path liuhaotian/llava-v1.5-7b \\\r\n    --version v1 \\\r\n    --data_path ./converted_data_for_finetuning.json \\\r\n    --image_folder ./playground/data \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b-task \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nAnyone knows about this?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 929,
    "state": "open",
    "created_by": "bigtent123",
    "created_at": "2023-12-17T15:35:28Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/929</URL>\n\n<TITLE>**NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.** (error_code: 1)</TITLE>\n\n<BODY>### Describe the issue\n\n**NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.** (error_code: 1)\r\n\r\nGetting this error constantly for days when attempting to use the online version.\r\n<img width=\"1116\" alt=\"Screenshot 2023-12-17 at 10 34 39 AM\" src=\"https://github.com/haotian-liu/LLaVA/assets/127369233/8b0caf3f-ff81-4e5f-8233-8c4587179331\"></BODY>\n\n<COMMENTS>\n<Comment by LTtt456c at 2024-03-15T07:22:32Z>\n+1\n</Comment>\n<Comment by leoanimaLihaixing at 2024-12-12T09:49:34Z>\nHas the issue been resolved?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 928,
    "state": "open",
    "created_by": "yangboz",
    "created_at": "2023-12-17T07:51:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/928</URL>\n\n<TITLE>after pulled llama by Ollama run  it error</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\nllava:latest          \te4c3eb471fd8\t4.5 GB\t2 minutes ago\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nollama version 0.1.3\r\n\r\nafter running lava , it thrown error as following\r\n![Screenshot 2023-12-17 at 15 51 01](https://github.com/haotian-liu/LLaVA/assets/481954/25f2bd37-9415-4fd0-801d-a122e3948f40)\r\n\r\n\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by yangboz at 2023-12-20T05:24:13Z>\neven though llava:13b  same error \r\n![Screenshot 2023-12-20 at 13 25 01](https://github.com/haotian-liu/LLaVA/assets/481954/410cb02e-45c2-48e5-a0b5-86f80b5e109f)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 927,
    "state": "open",
    "created_by": "chizuchizu",
    "created_at": "2023-12-17T05:53:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/927</URL>\n\n<TITLE>[Question] Unable to reproduce GQA, SQA, and MME results on LLaVA-v1.5-7b</TITLE>\n\n<BODY>### Question\n\nHi,\r\n\r\nThanks for your nice work.\r\n\r\nI cannot reproduce the following three benchmarks.\r\n\r\n| Benchmark | Obtained Score | Reported Score | \r\n| --------- | -------------- | -------------- | \r\n| GQA       | 61.86          | 62.0           | \r\n| ScienceQA | 67.87          | 66.8           | \r\n| MME       | 1509.3788      | 1510.7         | \r\n\r\n- Used the checkpoint of `liuhaotian/llava-v1.5-7b` for inference and score calculation. (Non fine-tuning)\r\n- Calculated the score from the results of 13b included in eval.zip, and confirmed that it matches the reported score.\r\n- The execution script is shown below. (Modified the model name in the script under [scripts/v1_5/eval](https://github.com/haotian-liu/LLaVA/tree/main/scripts/v1_5/eval))\r\n\r\n## GQA\r\n\r\n<details>\r\n<summary>script</summary>\r\n\r\n```shell\r\n#!/bin/bash\r\n\r\ngpu_list=\"${CUDA_VISIBLE_DEVICES:-0}\"\r\nIFS=',' read -ra GPULIST <<< \"$gpu_list\"\r\n\r\nCHUNKS=${#GPULIST[@]}\r\n\r\nCKPT=\"llava-v1.5-7b\"\r\nSPLIT=\"llava_gqa_testdev_balanced\"\r\nGQADIR=\"./playground/data/eval/gqa/data\"\r\n\r\nfor IDX in $(seq 0 $((CHUNKS-1))); do\r\n    CUDA_VISIBLE_DEVICES=${GPULIST[$IDX]} python -m llava.eval.model_vqa_loader \\\r\n        --model-path liuhaotian/llava-v1.5-7b \\\r\n        --question-file ./playground/data/eval/gqa/$SPLIT.jsonl \\\r\n        --image-folder ./playground/data/eval/gqa/data/images \\\r\n        --answers-file ./playground/data/eval/gqa/answers/$SPLIT/$CKPT/${CHUNKS}_${IDX}.jsonl \\\r\n        --num-chunks $CHUNKS \\\r\n        --chunk-idx $IDX \\\r\n        --temperature 0 \\\r\n        --conv-mode vicuna_v1 &\r\ndone\r\n\r\nwait\r\n\r\noutput_file=./playground/data/eval/gqa/answers/$SPLIT/$CKPT/merge.jsonl\r\n\r\n# Clear out the output file if it exists.\r\n> \"$output_file\"\r\n\r\n# Loop through the indices and concatenate each file.\r\nfor IDX in $(seq 0 $((CHUNKS-1))); do\r\n    cat ./playground/data/eval/gqa/answers/$SPLIT/$CKPT/${CHUNKS}_${IDX}.jsonl >> \"$output_file\"\r\ndone\r\n\r\npython scripts/convert_gqa_for_eval.py --src $output_file --dst $GQADIR/testdev_balanced_predictions.json\r\n\r\ncd $GQADIR\r\npython eval/eval.py --tier testdev_balanced\r\n```\r\n\r\n</details>\r\n\r\n```shell\r\nBinary: 79.11%                                 \r\nOpen: 47.23%                                   \r\nAccuracy: 61.86%\r\nValidity: 0.00%                                \r\nPlausibility: 0.00%                            \r\nDistribution: 1.64 (lower is better)\r\n                                               \r\nAccuracy / structural type: \r\n  choose: 83.35% (1129 questions)\r\n  compare: 64.86% (589 questions)\r\n  logical: 75.93% (1803 questions)\r\n  query: 47.23% (6805 questions)\r\n  verify: 83.26% (2252 questions)\r\n                                               \r\nAccuracy / semantic type:   \r\n  attr: 68.20% (5186 questions)\r\n  cat: 53.09% (1149 questions)\r\n  global: 63.06% (157 questions)\r\n  obj: 87.66% (778 questions)\r\n  rel: 53.75% (5308 questions)\r\n                                               \r\nAccuracy / steps number:   \r\n  1: 80.59% (237 questions)\r\n  2: 56.54% (6395 questions)\r\n  3: 64.96% (4266 questions)\r\n  4: 68.47% (793 questions)\r\n  5: 73.24% (822 questions)\r\n  6: 82.93% (41 questions)                                                                    \r\n  7: 100.00% (20 questions)\r\n  8: 100.00% (3 questions)\r\n  9: 100.00% (1 questions)\r\n```\r\n\r\n## ScienceQA\r\n<details>\r\n<summary>script</summary>\r\n\r\n```shell\r\n#!/bin/bash\r\n\r\npython -m llava.eval.model_vqa_science \\\r\n    --model-path liuhaotian/llava-v1.5-7b \\\r\n    --question-file ./playground/data/eval/scienceqa/llava_test_CQM-A.json \\\r\n    --image-folder ./playground/data/eval/scienceqa/images/test \\\r\n    --answers-file ./playground/data/eval/scienceqa/answers/llava-v1.5-7b.jsonl \\\r\n    --single-pred-prompt \\\r\n    --temperature 0 \\\r\n    --conv-mode vicuna_v1\r\n\r\npython llava/eval/eval_science_qa.py \\\r\n    --base-dir ./playground/data/eval/scienceqa \\\r\n    --result-file ./playground/data/eval/scienceqa/answers/llava-v1.5-7b.jsonl \\\r\n    --output-file ./playground/data/eval/scienceqa/answers/llava-v1.5-7b_output.jsonl \\\r\n    --output-result ./playground/data/eval/scienceqa/answers/llava-v1.5-7b_result.json```\r\n\r\n```\r\n\r\n</details>\r\n\r\n```\r\nTotal: 4241, Correct: 2943, Accuracy: 69.39%, IMG-Accuracy: 67.87%\r\n```\r\n\r\n## MME\r\n\r\n\r\n<details>\r\n\r\n<summary>script</summary>\r\n\r\n```shell\r\n#!/bin/bash\r\n\r\npython -m llava.eval.model_vqa_loader \\\r\n    --model-path liuhaotian/llava-v1.5-7b \\\r\n    --question-file ./playground/data/eval/MME/llava_mme.jsonl \\\r\n    --image-folder ./playground/data/eval/MME/MME_Benchmark_release_version \\\r\n    --answers-file ./playground/data/eval/MME/answers/llava-v1.5-7b.jsonl \\\r\n    --temperature 0 \\\r\n    --conv-mode vicuna_v1\r\n\r\ncd ./playground/data/eval/MME\r\n\r\npython convert_answer_to_mme.py --experiment llava-v1.5-7b\r\n\r\ncd eval_tool\r\n\r\npython calculation.py --results_dir answers/llava-v1.5-7b\r\n\r\n```\r\n</details>\r\n\r\n```\r\n=========== Perception ===========\r\ntotal score: 1509.3788515406163 \r\n\r\n         existence  score: 190.0\r\n         count  score: 155.0\r\n         position  score: 133.33333333333334\r\n         color  score: 170.0\r\n         posters  score: 147.61904761904762\r\n         celebrity  score: 136.1764705882353\r\n         scene  score: 158.0\r\n         landmark  score: 162.25\r\n         artwork  score: 119.5\r\n         OCR  score: 137.5\r\n\r\n\r\n=========== Cognition ===========\r\ntotal score: 348.2142857142857 \r\n\r\n         commonsense_reasoning  score: 110.71428571428571\r\n         numerical_calculation  score: 70.0\r\n         text_translation  score: 107.5\r\n         code_reasoning  score: 60.0\r\n```\r\n\r\n\r\nI am looking for a way to reproduce reported scores from the checkpoint.</BODY>\n\n<COMMENTS>\n<Comment by nbasyl at 2023-12-19T04:53:04Z>\nI also got the same result for the FFT checkpoint and a significantly lower score for the LoRA checkpoint compared to the reported LoRA score, appreciate if anyone could help me resolve such issue.\n</Comment>\n<Comment by soboleva-daria at 2024-02-21T20:56:47Z>\nI've got the same issue, cannot reproduce SQA. @haotian-liu help 🙏\n</Comment>\n<Comment by draeceo at 2024-03-28T23:27:19Z>\n@chizuchizu having a similar issue, could you share your `/playground/data/eval/MME/answers/llava-v1.5-7b.jsonl` file?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 926,
    "state": "closed",
    "created_by": "SCZwangxiao",
    "created_at": "2023-12-17T02:49:55Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/926</URL>\n\n<TITLE>[Question] 'Results do not correspond to current VQA set' when evaluating in VQAv2</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n<Comment by SCZwangxiao at 2023-12-23T02:50:12Z>\nSubmit results of all test set\n</Comment>\n<Comment by HireTheHero at 2023-12-28T07:27:19Z>\n@SCZwangxiao So what was the problem? Do we need to download v2 test set from [official website](https://visualqa.org/download.html)?\r\n![image](https://github.com/haotian-liu/LLaVA/assets/65780758/30070db3-08e0-45f8-9a27-7268fad5c515)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 924,
    "state": "open",
    "created_by": "oscarzhou511",
    "created_at": "2023-12-16T06:04:29Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/924</URL>\n\n<TITLE>[Question] Custom Data and Internet Access for Multimodal LLMS</TITLE>\n\n<BODY>### Question\r\n\r\nHello,\r\nI’m looking into connecting multimodal LLMs to some custom data or possibly connecting it to the internet. This does not needed to be incorporated into the visual aspect - only the LLM itself, so tools like LLaMaIndex and Langchain should be enough.\r\n\r\nI’m interested in connecting the LLaVA and BakLLaVA models, but any open source models should do the job. Any type of assistance would be greatly appreciated.\r\n\r\nThank you so much for reading this issue.\r\n\r\nOscar Zhou</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 923,
    "state": "open",
    "created_by": "isXinLiu",
    "created_at": "2023-12-15T02:33:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/923</URL>\n\n<TITLE>[Question] MME Evaluation Metric</TITLE>\n\n<BODY>### Question\n\n![image](https://github.com/haotian-liu/LLaVA/assets/152277529/4c2ecf33-64f6-49c7-aba2-ae288e162b34)\r\nThanks for your nice work!🙏🙏🙏 Here is my question:\r\nWhich metric do you use in your [paper](https://arxiv.org/pdf/2310.03744.pdf)?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 922,
    "state": "open",
    "created_by": "terminator123",
    "created_at": "2023-12-15T01:20:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/922</URL>\n\n<TITLE>[Question] how to merge the middle checkpoint file with lora</TITLE>\n\n<BODY>### Question\n\ni want to test the checkpoint-5000 in lora，when i ran \r\npython scrips/merge_lora_weights.py --model-path ./checkpoints/llava-v1.5-13b-lora --model-base lmsys/vicuna-13b-v1.5 --save-model-path ./checkpoints/merge\r\nit went wrong</BODY>\n\n<COMMENTS>\n<Comment by Isaachhh at 2023-12-29T09:23:58Z>\nyou need to copy the `config.json` and `non_lora_trainables.bin` into your `checkpoint-5000` folder\n</Comment>\n<Comment by charismaticchiu at 2024-02-28T06:48:30Z>\nI also have the same problem #1194. Did you solve it?\n</Comment>\n<Comment by wuwu-C at 2024-04-20T13:13:00Z>\n> you need to copy the `config.json` and `non_lora_trainables.bin` into your `checkpoint-5000` folder\r\nIs config.json and non_lora_trainable.bin saved only at the end of the entire training? I have set epoch 10, can I copy these two files from epoch 10 directly to the first nine?\n</Comment>\n<Comment by Isaachhh at 2024-04-20T14:59:56Z>\n> Is config.json and non_lora_trainable.bin saved only at the end of the entire training?\r\n\r\nI think so.\r\n\r\n\r\n> I have set epoch 10, can I copy these two files from epoch 10 directly to the first nine?\r\n\r\nThe weights of projector are saved in `non_lora_trainables.bin`, which is unfrozen during sft stage.\n</Comment>\n<Comment by wuwu-C at 2024-04-21T09:17:33Z>\nThank you for your reply！but I also have some question\r\n> The weights of projector are saved in `non_lora_trainables.bin`, which is unfrozen during sft stage.\r\n1. non_lora_trainable.bin is not storing the weight without lora trimming part, shouldn't it be frozen? Why is it a weight store for projectors?\r\n2. In your previous answer, you said copy the two files to the corresponding weight folder.If it is unfrozen during sft stage, this way is incorrect.How can I merge the middle checkpoint file with lora. \r\nCan you give me more detailed explanation,thank you!\n</Comment>\n<Comment by Isaachhh at 2024-04-22T02:00:03Z>\n> Thank you for your reply！but I also have some question\r\n> \r\n> > The weights of projector are saved in `non_lora_trainables.bin`, which is unfrozen during sft stage.\r\n> \r\n> 1. non_lora_trainable.bin is not storing the weight without lora trimming part, shouldn't it be frozen? Why is it a weight store for projectors?\r\n> 2. In your previous answer, you said copy the two files to the corresponding weight folder.If it is unfrozen during sft stage, this way is incorrect.How can I merge the middle checkpoint file with lora.\r\n>    Can you give me more detailed explanation,thank you!\r\n\r\n1. non_lora_trainable, non_lora and trainable, so it stores projector because it's trained directly other than lora. Check [here](/mnt/hpfs/baaidcai/hemuyang/Bunny/checkpoints-llama3-8b/bunny-lora-llama3-8b-1e-3-v1.0/non_lora_trainables.bin)\r\nTry:\r\n`a = torch.load('.../non_lora_trainables.bin')`\r\n`print(a.keys())`\r\n\r\n3. Yes, you are right. And you may need to edit the source code to save projector weights in the middle.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 921,
    "state": "open",
    "created_by": "Abhishek-at-vanigaa",
    "created_at": "2023-12-14T10:07:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/921</URL>\n\n<TITLE>Key error: 'llava'. getting key error onloading Lora fintuned model or loading the model from Huggingface repo</TITLE>\n\n<BODY>### Describe the issue\n\ncould you please provide more Information on this?\r\n\r\n\r\nping.__getitem__(self, key)\r\n    708     return self._extra_content[key]\r\n    709 if key not in self._mapping:\r\n--> 710     raise KeyError(key)\r\n    711 value = self._mapping[key]\r\n    712 module_name = model_type_to_module_name(key)\r\n\r\nKeyError: 'llava'</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 920,
    "state": "open",
    "created_by": "unmo",
    "created_at": "2023-12-14T06:25:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/920</URL>\n\n<TITLE>[Question] Same Custom data pretraining and finetuning.</TITLE>\n\n<BODY>### Question\n\nI am considering training with custom data.\r\nHowever, due to the small sample of data in the custom data, it is difficult to train on pre-training and fine tuning.\r\nSo, does it make sense to use the same data for pre-training and fine tuning?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 919,
    "state": "open",
    "created_by": "qazimbhat1",
    "created_at": "2023-12-14T05:13:02Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/919</URL>\n\n<TITLE>Output sequence length using CLI interface</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nI am using the 13b LLaVA fine-tuned model but while using some prompts which require generating large output sequence, it does not generate the entire output in one go and I have to use another prompt to get the rest of the output. What could this issue be related to? Is there a way to control the output sequence length?\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.cli \\\r\n    --model-path xxxx \\\r\n    --image-file \"xxxx\" \\\r\n    --load-4bit\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by Sarahtorekryo at 2024-01-24T06:37:09Z>\nYou can set that in **self.model.generate**, there's a parameter **max_new_tokens**\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 918,
    "state": "closed",
    "created_by": "parap1uie-s",
    "created_at": "2023-12-14T02:14:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/918</URL>\n\n<TITLE>Abnormal loss rises during finetune_task_lora.sh</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\n\r\nI am finetuning LLaVA 1.5 13b using `scripts/v1.5/finetune_task_lora.sh` on my custom dataset. Training process looks normal (~0.4) until a iteration (randomly, no pattern found yet), the loss `Fly away` and can not descent until the epoch end. \r\n\r\nMy dataset consists of a single round of chinese conversation. After the finetune procedure, I use serve.cli to check the output of my model. However, it goes like  garbled UTF-8 code like:\r\n\r\n```\r\nUSER: 描述这幅图像\r\nASSISTANT: �。��见。 影像可诋：��译��影像可见：皮肤上有一个毛的人的皮肤有几个红色的红色的皮肤状块�。\r\n```\r\n\r\nAny suggestions? Thanks in advance. \r\n\r\nCommand:\r\n```\r\nnohup sh scripts/v1_5/finetune_task_lora.sh > finetune.log 2>&1 &\r\n```\r\n\r\nLog: \r\n```\r\n{'loss': 0.278, 'learning_rate': 0.00017723609012917414, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2543/10485 [3:13:38<9:51:41,  4.47s/it]\r\n 24%|██▍       | 2544/10485 [3:13:42<9:53:22,  4.48s/it]\r\n                                                        \r\n{'loss': 0.2561, 'learning_rate': 0.00017721646511498725, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2544/10485 [3:13:42<9:53:22,  4.48s/it]\r\n 24%|██▍       | 2545/10485 [3:13:47<9:58:11,  4.52s/it]\r\n                                                        \r\n{'loss': 0.2387, 'learning_rate': 0.00017719683273249262, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2545/10485 [3:13:47<9:58:11,  4.52s/it]\r\n 24%|██▍       | 2546/10485 [3:13:52<9:56:24,  4.51s/it]\r\n                                                        \r\n{'loss': 0.2537, 'learning_rate': 0.00017717719298356377, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2546/10485 [3:13:52<9:56:24,  4.51s/it]\r\n 24%|██▍       | 2547/10485 [3:13:56<10:02:22,  4.55s/it]\r\n                                                         \r\n{'loss': 0.2363, 'learning_rate': 0.00017715754587007475, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2547/10485 [3:13:56<10:02:22,  4.55s/it]\r\n 24%|██▍       | 2548/10485 [3:14:01<10:07:30,  4.59s/it]\r\n                                                         \r\n{'loss': 0.196, 'learning_rate': 0.00017713789139390035, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2548/10485 [3:14:01<10:07:30,  4.59s/it]\r\n 24%|██▍       | 2549/10485 [3:14:05<9:55:42,  4.50s/it] \r\n                                                        \r\n{'loss': 0.1796, 'learning_rate': 0.0001771182295569161, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2549/10485 [3:14:05<9:55:42,  4.50s/it]\r\n 24%|██▍       | 2550/10485 [3:14:10<9:53:36,  4.49s/it]\r\n                                                        \r\n{'loss': 0.2633, 'learning_rate': 0.0001770985603609982, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2550/10485 [3:14:10<9:53:36,  4.49s/it]\r\n 24%|██▍       | 2551/10485 [3:14:14<9:53:20,  4.49s/it]\r\n                                                        \r\n{'loss': 0.2268, 'learning_rate': 0.0001770788838080236, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2551/10485 [3:14:14<9:53:20,  4.49s/it]\r\n 24%|██▍       | 2552/10485 [3:14:19<9:58:08,  4.52s/it]\r\n                                                        \r\n{'loss': 0.2249, 'learning_rate': 0.00017705919989986985, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2552/10485 [3:14:19<9:58:08,  4.52s/it]\r\n 24%|██▍       | 2553/10485 [3:14:23<9:55:14,  4.50s/it]\r\n                                                        \r\n{'loss': 0.237, 'learning_rate': 0.00017703950863841533, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2553/10485 [3:14:23<9:55:14,  4.50s/it]\r\n 24%|██▍       | 2554/10485 [3:14:28<9:55:44,  4.51s/it]\r\n                                                        \r\n{'loss': 0.2058, 'learning_rate': 0.00017701981002553904, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2554/10485 [3:14:28<9:55:44,  4.51s/it]\r\n 24%|██▍       | 2555/10485 [3:14:32<9:54:32,  4.50s/it]\r\n                                                        \r\n{'loss': 0.231, 'learning_rate': 0.0001770001040631207, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2555/10485 [3:14:32<9:54:32,  4.50s/it]\r\n 24%|██▍       | 2556/10485 [3:14:36<9:41:41,  4.40s/it]\r\n                                                        \r\n{'loss': 0.3225, 'learning_rate': 0.00017698039075304069, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2556/10485 [3:14:36<9:41:41,  4.40s/it]\r\n 24%|██▍       | 2557/10485 [3:14:41<9:48:28,  4.45s/it]\r\n                                                        \r\n{'loss': 0.3587, 'learning_rate': 0.0001769606700971802, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2557/10485 [3:14:41<9:48:28,  4.45s/it]\r\n 24%|██▍       | 2558/10485 [3:14:45<9:48:32,  4.45s/it]\r\n                                                        \r\n{'loss': 0.2832, 'learning_rate': 0.00017694094209742104, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2558/10485 [3:14:45<9:48:32,  4.45s/it]\r\n 24%|██▍       | 2559/10485 [3:14:50<9:48:51,  4.46s/it]\r\n                                                        \r\n{'loss': 0.4236, 'learning_rate': 0.00017692120675564575, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2559/10485 [3:14:50<9:48:51,  4.46s/it]\r\n 24%|██▍       | 2560/10485 [3:14:55<9:59:36,  4.54s/it]\r\n                                                        \r\n{'loss': 0.4833, 'learning_rate': 0.00017690146407373748, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2560/10485 [3:14:55<9:59:36,  4.54s/it]\r\n 24%|██▍       | 2561/10485 [3:14:59<9:58:30,  4.53s/it]\r\n                                                        \r\n{'loss': 0.5024, 'learning_rate': 0.00017688171405358022, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2561/10485 [3:14:59<9:58:30,  4.53s/it]\r\n 24%|██▍       | 2562/10485 [3:15:04<10:00:41,  4.55s/it]\r\n                                                         \r\n{'loss': 0.6037, 'learning_rate': 0.0001768619566970586, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2562/10485 [3:15:04<10:00:41,  4.55s/it]\r\n 24%|██▍       | 2563/10485 [3:15:08<9:57:45,  4.53s/it] \r\n                                                        \r\n{'loss': 0.9337, 'learning_rate': 0.00017684219200605792, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2563/10485 [3:15:08<9:57:45,  4.53s/it]\r\n 24%|██▍       | 2564/10485 [3:15:13<10:06:35,  4.59s/it]\r\n                                                         \r\n{'loss': 0.6623, 'learning_rate': 0.00017682241998246423, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2564/10485 [3:15:13<10:06:35,  4.59s/it]\r\n 24%|██▍       | 2565/10485 [3:15:17<10:03:00,  4.57s/it]\r\n                                                         \r\n{'loss': 0.8022, 'learning_rate': 0.0001768026406281642, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2565/10485 [3:15:17<10:03:00,  4.57s/it]\r\n 24%|██▍       | 2566/10485 [3:15:22<10:03:32,  4.57s/it]\r\n                                                         \r\n{'loss': 4.2653, 'learning_rate': 0.00017678285394504535, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2566/10485 [3:15:22<10:03:32,  4.57s/it]\r\n 24%|██▍       | 2567/10485 [3:15:26<9:59:39,  4.54s/it] \r\n                                                        \r\n{'loss': 0.77, 'learning_rate': 0.00017676305993499574, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2567/10485 [3:15:26<9:59:39,  4.54s/it]\r\n 24%|██▍       | 2568/10485 [3:15:31<10:01:29,  4.56s/it]\r\n                                                         \r\n{'loss': 0.556, 'learning_rate': 0.00017674325859990422, 'epoch': 0.24}\r\n\r\n 24%|██▍       | 2568/10485 [3:15:31<10:01:29,  4.56s/it]\r\n 25%|██▍       | 2569/10485 [3:15:36<10:08:46,  4.61s/it]\r\n                                                         \r\n{'loss': 0.8223, 'learning_rate': 0.00017672344994166031, 'epoch': 0.25}\r\n\r\n 25%|██▍       | 2569/10485 [3:15:36<10:08:46,  4.61s/it]\r\n 25%|██▍       | 2570/10485 [3:15:41<10:13:47,  4.65s/it]\r\n                                                         \r\n{'loss': 0.6307, 'learning_rate': 0.0001767036339621542, 'epoch': 0.25}\r\n\r\n 25%|██▍       | 2570/10485 [3:15:41<10:13:47,  4.65s/it]\r\n 25%|██▍       | 2571/10485 [3:15:45<10:04:22,  4.58s/it]\r\n                                                         \r\n{'loss': 0.7256, 'learning_rate': 0.00017668381066327687, 'epoch': 0.25}\r\n\r\n 25%|██▍       | 2571/10485 [3:15:45<10:04:22,  4.58s/it]\r\n 25%|██▍       | 2572/10485 [3:15:49<10:02:48,  4.57s/it]\r\n                                                         \r\n{'loss': 5.4546, 'learning_rate': 0.0001766639800469199, 'epoch': 0.25}\r\n\r\n 25%|██▍       | 2572/10485 [3:15:49<10:02:48,  4.57s/it]\r\n 25%|██▍       | 2573/10485 [3:15:54<10:08:43,  4.62s/it]\r\n                                                         \r\n{'loss': 7.7808, 'learning_rate': 0.0001766441421149756, 'epoch': 0.25}\r\n\r\n 25%|██▍       | 2573/10485 [3:15:54<10:08:43,  4.62s/it]\r\n 25%|██▍       | 2574/10485 [3:15:59<10:08:55,  4.62s/it]\r\n                                                         \r\n{'loss': 8.2073, 'learning_rate': 0.00017662429686933698, 'epoch': 0.25}\r\n\r\n 25%|██▍       | 2574/10485 [3:15:59<10:08:55,  4.62s/it]\r\n 25%|██▍       | 2575/10485 [3:16:04<10:16:07,  4.67s/it]\r\n                                                         \r\n{'loss': 4.118, 'learning_rate': 0.0001766044443118978, 'epoch': 0.25}\r\n\r\n 25%|██▍       | 2575/10485 [3:16:04<10:16:07,  4.67s/it]\r\n 25%|██▍       | 2576/10485 [3:16:08<10:21:40,  4.72s/it]\r\n                                                         \r\n{'loss': 4.8635, 'learning_rate': 0.00017658458444455243, 'epoch': 0.25}\r\n\r\n 25%|██▍       | 2576/10485 [3:16:08<10:21:40,  4.72s/it]\r\n 25%|██▍       | 2577/10485 [3:16:13<10:19:28,  4.70s/it]\r\n                                                         \r\n{'loss': 5.6341, 'learning_rate': 0.00017656471726919604, 'epoch': 0.25}\r\n\r\n 25%|██▍       | 2577/10485 [3:16:13<10:19:28,  4.70s/it]\r\n 25%|██▍       | 2578/10485 [3:16:18<10:27:23,  4.76s/it]\r\n                                                         \r\n{'loss': 10.266, 'learning_rate': 0.00017654484278772437, 'epoch': 0.25}\r\n\r\n 25%|██▍       | 2578/10485 [3:16:18<10:27:23,  4.76s/it]\r\n 25%|██▍       | 2579/10485 [3:16:23<10:20:03,  4.71s/it]\r\n                                                         \r\n{'loss': 8.0332, 'learning_rate': 0.00017652496100203392, 'epoch': 0.25}\r\n\r\n 25%|██▍       | 2579/10485 [3:16:23<10:20:03,  4.71s/it]\r\n 25%|██▍       | 2580/10485 [3:16:27<10:13:08,  4.65s/it]\r\n                                                         \r\n{'loss': 7.8216, 'learning_rate': 0.00017650507191402194, 'epoch': 0.25}\r\n\r\n 25%|██▍       | 2580/10485 [3:16:27<10:13:08,  4.65s/it]\r\n 25%|██▍       | 2581/10485 [3:16:32<10:24:19,  4.74s/it]\r\n                                                         \r\n{'loss': 7.7877, 'learning_rate': 0.0001764851755255863, 'epoch': 0.25}\r\n\r\n 25%|██▍       | 2581/10485 [3:16:32<10:24:19,  4.74s/it]\r\n 25%|██▍       | 2582/10485 [3:16:37<10:12:33,  4.65s/it]\r\n                                                         \r\n{'loss': 8.8278, 'learning_rate': 0.00017646527183862558, 'epoch': 0.25}\r\n\r\n 25%|██▍       | 2582/10485 [3:16:37<10:12:33,  4.65s/it]\r\n 25%|██▍       | 2583/10485 [3:16:41<10:05:41,  4.60s/it]\r\n                                                         \r\n{'loss': 8.6483, 'learning_rate': 0.0001764453608550391, 'epoch': 0.25}\r\n\r\n 25%|██▍       | 2583/10485 [3:16:41<10:05:41,  4.60s/it]\r\n 25%|██▍       | 2584/10485 [3:16:46<10:08:49,  4.62s/it]\r\n                                                         \r\n{'loss': 6.5844, 'learning_rate': 0.00017642544257672683, 'epoch': 0.25}\r\n\r\n 25%|██▍       | 2584/10485 [3:16:46<10:08:49,  4.62s/it]\r\n 25%|██▍       | 2585/10485 [3:16:50<10:00:06,  4.56s/it]\r\n                                                         \r\n{'loss': 8.1825, 'learning_rate': 0.00017640551700558944, 'epoch': 0.25}\r\n\r\n 25%|██▍       | 2585/10485 [3:16:50<10:00:06,  4.56s/it]\r\n 25%|██▍       | 2586/10485 [3:16:55<10:03:34,  4.58s/it]\r\n                                                         \r\n{'loss': 7.0871, 'learning_rate': 0.00017638558414352835, 'epoch': 0.25}\r\n\r\n 25%|██▍       | 2586/10485 [3:16:55<10:03:34,  4.58s/it]\r\n 25%|██▍       | 2587/10485 [3:16:59<10:02:16,  4.58s/it]\r\n                                                         \r\n{'loss': 6.7109, 'learning_rate': 0.0001763656439924456, 'epoch': 0.25}\r\n\r\n 25%|██▍       | 2587/10485 [3:16:59<10:02:16,  4.58s/it]\r\n 25%|██▍       | 2588/10485 [3:17:04<9:56:44,  4.53s/it] \r\n                                                        \r\n{'loss': 6.6635, 'learning_rate': 0.00017634569655424395, 'epoch': 0.25}\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by sxu1997 at 2023-12-24T16:28:51Z>\nHi, have you figure out the reason? I have met the same problem when finetuning lora both on the official data and our downstream task data.\n</Comment>\n<Comment by parap1uie-s at 2023-12-29T02:25:21Z>\n> Hi, have you figure out the reason? I have met the same problem when finetuning lora both on the official data and our downstream task data.\r\n\r\nNot yet. However, I guess the cause of the error is NaN or INF appearing during training. Larger `per_device_train_batch_size` or `gradient_accumulation_steps` might helps, and shorter training periods may also reduce the probability of ♂ occurrences.\r\n\r\nStill looking forward to a reply from the maintainer.\n</Comment>\n<Comment by haotian-liu at 2024-01-02T19:10:01Z>\nPlease try lowering the learning rate to 1e-4 or lower, which can remedy the training instability. Thanks.\n</Comment>\n<Comment by parap1uie-s at 2024-01-03T01:20:09Z>\n> Please try lowering the learning rate to 1e-4 or lower, which can remedy the training instability. Thanks.\r\n\r\nThanks for the reply, I will try reduce the learning rate. And...Is this anomaly related to Loss Spike? As reported from [A Theory on Adam Instability in Large-Scale Machine Learning](https://arxiv.org/abs/2304.09871).\n</Comment>\n<Comment by gpminsuk at 2024-09-09T19:20:44Z>\n@parap1uie-s have you had a chance to solve this problem? I am having the same problem using default data in readme.\r\n\r\nHere's my training loss and epoch. I don't understand how it spikes when it was stable throughout multiple epochs. Learning rate is also slowly decreasing.\r\n<img width=\"1957\" alt=\"image\" src=\"https://github.com/user-attachments/assets/b6a7b9f3-66e3-4dd3-b2d7-bb1b7fe35cd0\">\n</Comment>\n<Comment by Pixel-anter at 2025-01-05T07:09:42Z>\n> ### 描述问题\r\n> 问题：\r\n> \r\n> 我正在使用我的自定义数据集微调 LLaVA 1.5 13b。训练过程看起来很正常 （~0.4），直到迭代 （随机，尚未找到模式），损失并且直到 epoch 结束才能下降。`scripts/v1.5/finetune_task_lora.sh``Fly away`\r\n> \r\n> 我的数据集由一轮中文对话组成。在微调过程之后，我使用 serve.cli 检查模型的输出。但是，它就像乱码 UTF-8 代码一样：\r\n> \r\n> ```\r\n> USER: 描述这幅图像\r\n> ASSISTANT: �。��见。 影像可诋：��译��影像可见：皮肤上有一个毛的人的皮肤有几个红色的红色的皮肤状块�。\r\n> ```\r\n> \r\n> 有什么建议吗？提前感谢。\r\n> \r\n> 命令：\r\n> \r\n> ```\r\n> nohup sh scripts/v1_5/finetune_task_lora.sh > finetune.log 2>&1 &\r\n> ```\r\n> \r\n> 日志：\r\n> \r\n> ```\r\n> {'loss': 0.278, 'learning_rate': 0.00017723609012917414, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2543/10485 [3:13:38<9:51:41,  4.47s/it]\r\n>  24%|██▍       | 2544/10485 [3:13:42<9:53:22,  4.48s/it]\r\n>                                                         \r\n> {'loss': 0.2561, 'learning_rate': 0.00017721646511498725, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2544/10485 [3:13:42<9:53:22,  4.48s/it]\r\n>  24%|██▍       | 2545/10485 [3:13:47<9:58:11,  4.52s/it]\r\n>                                                         \r\n> {'loss': 0.2387, 'learning_rate': 0.00017719683273249262, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2545/10485 [3:13:47<9:58:11,  4.52s/it]\r\n>  24%|██▍       | 2546/10485 [3:13:52<9:56:24,  4.51s/it]\r\n>                                                         \r\n> {'loss': 0.2537, 'learning_rate': 0.00017717719298356377, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2546/10485 [3:13:52<9:56:24,  4.51s/it]\r\n>  24%|██▍       | 2547/10485 [3:13:56<10:02:22,  4.55s/it]\r\n>                                                          \r\n> {'loss': 0.2363, 'learning_rate': 0.00017715754587007475, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2547/10485 [3:13:56<10:02:22,  4.55s/it]\r\n>  24%|██▍       | 2548/10485 [3:14:01<10:07:30,  4.59s/it]\r\n>                                                          \r\n> {'loss': 0.196, 'learning_rate': 0.00017713789139390035, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2548/10485 [3:14:01<10:07:30,  4.59s/it]\r\n>  24%|██▍       | 2549/10485 [3:14:05<9:55:42,  4.50s/it] \r\n>                                                         \r\n> {'loss': 0.1796, 'learning_rate': 0.0001771182295569161, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2549/10485 [3:14:05<9:55:42,  4.50s/it]\r\n>  24%|██▍       | 2550/10485 [3:14:10<9:53:36,  4.49s/it]\r\n>                                                         \r\n> {'loss': 0.2633, 'learning_rate': 0.0001770985603609982, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2550/10485 [3:14:10<9:53:36,  4.49s/it]\r\n>  24%|██▍       | 2551/10485 [3:14:14<9:53:20,  4.49s/it]\r\n>                                                         \r\n> {'loss': 0.2268, 'learning_rate': 0.0001770788838080236, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2551/10485 [3:14:14<9:53:20,  4.49s/it]\r\n>  24%|██▍       | 2552/10485 [3:14:19<9:58:08,  4.52s/it]\r\n>                                                         \r\n> {'loss': 0.2249, 'learning_rate': 0.00017705919989986985, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2552/10485 [3:14:19<9:58:08,  4.52s/it]\r\n>  24%|██▍       | 2553/10485 [3:14:23<9:55:14,  4.50s/it]\r\n>                                                         \r\n> {'loss': 0.237, 'learning_rate': 0.00017703950863841533, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2553/10485 [3:14:23<9:55:14,  4.50s/it]\r\n>  24%|██▍       | 2554/10485 [3:14:28<9:55:44,  4.51s/it]\r\n>                                                         \r\n> {'loss': 0.2058, 'learning_rate': 0.00017701981002553904, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2554/10485 [3:14:28<9:55:44,  4.51s/it]\r\n>  24%|██▍       | 2555/10485 [3:14:32<9:54:32,  4.50s/it]\r\n>                                                         \r\n> {'loss': 0.231, 'learning_rate': 0.0001770001040631207, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2555/10485 [3:14:32<9:54:32,  4.50s/it]\r\n>  24%|██▍       | 2556/10485 [3:14:36<9:41:41,  4.40s/it]\r\n>                                                         \r\n> {'loss': 0.3225, 'learning_rate': 0.00017698039075304069, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2556/10485 [3:14:36<9:41:41,  4.40s/it]\r\n>  24%|██▍       | 2557/10485 [3:14:41<9:48:28,  4.45s/it]\r\n>                                                         \r\n> {'loss': 0.3587, 'learning_rate': 0.0001769606700971802, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2557/10485 [3:14:41<9:48:28,  4.45s/it]\r\n>  24%|██▍       | 2558/10485 [3:14:45<9:48:32,  4.45s/it]\r\n>                                                         \r\n> {'loss': 0.2832, 'learning_rate': 0.00017694094209742104, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2558/10485 [3:14:45<9:48:32,  4.45s/it]\r\n>  24%|██▍       | 2559/10485 [3:14:50<9:48:51,  4.46s/it]\r\n>                                                         \r\n> {'loss': 0.4236, 'learning_rate': 0.00017692120675564575, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2559/10485 [3:14:50<9:48:51,  4.46s/it]\r\n>  24%|██▍       | 2560/10485 [3:14:55<9:59:36,  4.54s/it]\r\n>                                                         \r\n> {'loss': 0.4833, 'learning_rate': 0.00017690146407373748, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2560/10485 [3:14:55<9:59:36,  4.54s/it]\r\n>  24%|██▍       | 2561/10485 [3:14:59<9:58:30,  4.53s/it]\r\n>                                                         \r\n> {'loss': 0.5024, 'learning_rate': 0.00017688171405358022, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2561/10485 [3:14:59<9:58:30,  4.53s/it]\r\n>  24%|██▍       | 2562/10485 [3:15:04<10:00:41,  4.55s/it]\r\n>                                                          \r\n> {'loss': 0.6037, 'learning_rate': 0.0001768619566970586, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2562/10485 [3:15:04<10:00:41,  4.55s/it]\r\n>  24%|██▍       | 2563/10485 [3:15:08<9:57:45,  4.53s/it] \r\n>                                                         \r\n> {'loss': 0.9337, 'learning_rate': 0.00017684219200605792, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2563/10485 [3:15:08<9:57:45,  4.53s/it]\r\n>  24%|██▍       | 2564/10485 [3:15:13<10:06:35,  4.59s/it]\r\n>                                                          \r\n> {'loss': 0.6623, 'learning_rate': 0.00017682241998246423, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2564/10485 [3:15:13<10:06:35,  4.59s/it]\r\n>  24%|██▍       | 2565/10485 [3:15:17<10:03:00,  4.57s/it]\r\n>                                                          \r\n> {'loss': 0.8022, 'learning_rate': 0.0001768026406281642, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2565/10485 [3:15:17<10:03:00,  4.57s/it]\r\n>  24%|██▍       | 2566/10485 [3:15:22<10:03:32,  4.57s/it]\r\n>                                                          \r\n> {'loss': 4.2653, 'learning_rate': 0.00017678285394504535, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2566/10485 [3:15:22<10:03:32,  4.57s/it]\r\n>  24%|██▍       | 2567/10485 [3:15:26<9:59:39,  4.54s/it] \r\n>                                                         \r\n> {'loss': 0.77, 'learning_rate': 0.00017676305993499574, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2567/10485 [3:15:26<9:59:39,  4.54s/it]\r\n>  24%|██▍       | 2568/10485 [3:15:31<10:01:29,  4.56s/it]\r\n>                                                          \r\n> {'loss': 0.556, 'learning_rate': 0.00017674325859990422, 'epoch': 0.24}\r\n> \r\n>  24%|██▍       | 2568/10485 [3:15:31<10:01:29,  4.56s/it]\r\n>  25%|██▍       | 2569/10485 [3:15:36<10:08:46,  4.61s/it]\r\n>                                                          \r\n> {'loss': 0.8223, 'learning_rate': 0.00017672344994166031, 'epoch': 0.25}\r\n> \r\n>  25%|██▍       | 2569/10485 [3:15:36<10:08:46,  4.61s/it]\r\n>  25%|██▍       | 2570/10485 [3:15:41<10:13:47,  4.65s/it]\r\n>                                                          \r\n> {'loss': 0.6307, 'learning_rate': 0.0001767036339621542, 'epoch': 0.25}\r\n> \r\n>  25%|██▍       | 2570/10485 [3:15:41<10:13:47,  4.65s/it]\r\n>  25%|██▍       | 2571/10485 [3:15:45<10:04:22,  4.58s/it]\r\n>                                                          \r\n> {'loss': 0.7256, 'learning_rate': 0.00017668381066327687, 'epoch': 0.25}\r\n> \r\n>  25%|██▍       | 2571/10485 [3:15:45<10:04:22,  4.58s/it]\r\n>  25%|██▍       | 2572/10485 [3:15:49<10:02:48,  4.57s/it]\r\n>                                                          \r\n> {'loss': 5.4546, 'learning_rate': 0.0001766639800469199, 'epoch': 0.25}\r\n> \r\n>  25%|██▍       | 2572/10485 [3:15:49<10:02:48,  4.57s/it]\r\n>  25%|██▍       | 2573/10485 [3:15:54<10:08:43,  4.62s/it]\r\n>                                                          \r\n> {'loss': 7.7808, 'learning_rate': 0.0001766441421149756, 'epoch': 0.25}\r\n> \r\n>  25%|██▍       | 2573/10485 [3:15:54<10:08:43,  4.62s/it]\r\n>  25%|██▍       | 2574/10485 [3:15:59<10:08:55,  4.62s/it]\r\n>                                                          \r\n> {'loss': 8.2073, 'learning_rate': 0.00017662429686933698, 'epoch': 0.25}\r\n> \r\n>  25%|██▍       | 2574/10485 [3:15:59<10:08:55,  4.62s/it]\r\n>  25%|██▍       | 2575/10485 [3:16:04<10:16:07,  4.67s/it]\r\n>                                                          \r\n> {'loss': 4.118, 'learning_rate': 0.0001766044443118978, 'epoch': 0.25}\r\n> \r\n>  25%|██▍       | 2575/10485 [3:16:04<10:16:07,  4.67s/it]\r\n>  25%|██▍       | 2576/10485 [3:16:08<10:21:40,  4.72s/it]\r\n>                                                          \r\n> {'loss': 4.8635, 'learning_rate': 0.00017658458444455243, 'epoch': 0.25}\r\n> \r\n>  25%|██▍       | 2576/10485 [3:16:08<10:21:40,  4.72s/it]\r\n>  25%|██▍       | 2577/10485 [3:16:13<10:19:28,  4.70s/it]\r\n>                                                          \r\n> {'loss': 5.6341, 'learning_rate': 0.00017656471726919604, 'epoch': 0.25}\r\n> \r\n>  25%|██▍       | 2577/10485 [3:16:13<10:19:28,  4.70s/it]\r\n>  25%|██▍       | 2578/10485 [3:16:18<10:27:23,  4.76s/it]\r\n>                                                          \r\n> {'loss': 10.266, 'learning_rate': 0.00017654484278772437, 'epoch': 0.25}\r\n> \r\n>  25%|██▍       | 2578/10485 [3:16:18<10:27:23,  4.76s/it]\r\n>  25%|██▍       | 2579/10485 [3:16:23<10:20:03,  4.71s/it]\r\n>                                                          \r\n> {'loss': 8.0332, 'learning_rate': 0.00017652496100203392, 'epoch': 0.25}\r\n> \r\n>  25%|██▍       | 2579/10485 [3:16:23<10:20:03,  4.71s/it]\r\n>  25%|██▍       | 2580/10485 [3:16:27<10:13:08,  4.65s/it]\r\n>                                                          \r\n> {'loss': 7.8216, 'learning_rate': 0.00017650507191402194, 'epoch': 0.25}\r\n> \r\n>  25%|██▍       | 2580/10485 [3:16:27<10:13:08,  4.65s/it]\r\n>  25%|██▍       | 2581/10485 [3:16:32<10:24:19,  4.74s/it]\r\n>                                                          \r\n> {'loss': 7.7877, 'learning_rate': 0.0001764851755255863, 'epoch': 0.25}\r\n> \r\n>  25%|██▍       | 2581/10485 [3:16:32<10:24:19,  4.74s/it]\r\n>  25%|██▍       | 2582/10485 [3:16:37<10:12:33,  4.65s/it]\r\n>                                                          \r\n> {'loss': 8.8278, 'learning_rate': 0.00017646527183862558, 'epoch': 0.25}\r\n> \r\n>  25%|██▍       | 2582/10485 [3:16:37<10:12:33,  4.65s/it]\r\n>  25%|██▍       | 2583/10485 [3:16:41<10:05:41,  4.60s/it]\r\n>                                                          \r\n> {'loss': 8.6483, 'learning_rate': 0.0001764453608550391, 'epoch': 0.25}\r\n> \r\n>  25%|██▍       | 2583/10485 [3:16:41<10:05:41,  4.60s/it]\r\n>  25%|██▍       | 2584/10485 [3:16:46<10:08:49,  4.62s/it]\r\n>                                                          \r\n> {'loss': 6.5844, 'learning_rate': 0.00017642544257672683, 'epoch': 0.25}\r\n> \r\n>  25%|██▍       | 2584/10485 [3:16:46<10:08:49,  4.62s/it]\r\n>  25%|██▍       | 2585/10485 [3:16:50<10:00:06,  4.56s/it]\r\n>                                                          \r\n> {'loss': 8.1825, 'learning_rate': 0.00017640551700558944, 'epoch': 0.25}\r\n> \r\n>  25%|██▍       | 2585/10485 [3:16:50<10:00:06,  4.56s/it]\r\n>  25%|██▍       | 2586/10485 [3:16:55<10:03:34,  4.58s/it]\r\n>                                                          \r\n> {'loss': 7.0871, 'learning_rate': 0.00017638558414352835, 'epoch': 0.25}\r\n> \r\n>  25%|██▍       | 2586/10485 [3:16:55<10:03:34,  4.58s/it]\r\n>  25%|██▍       | 2587/10485 [3:16:59<10:02:16,  4.58s/it]\r\n>                                                          \r\n> {'loss': 6.7109, 'learning_rate': 0.0001763656439924456, 'epoch': 0.25}\r\n> \r\n>  25%|██▍       | 2587/10485 [3:16:59<10:02:16,  4.58s/it]\r\n>  25%|██▍       | 2588/10485 [3:17:04<9:56:44,  4.53s/it] \r\n>                                                         \r\n> {'loss': 6.6635, 'learning_rate': 0.00017634569655424395, 'epoch': 0.25}\r\n> ```\r\n> \r\n> 屏幕截图： 如果它更好地解释了问题，您可以附加屏幕截图。\r\n\r\nHello, have you solved this issue? I have encountered the same problem during training, and it has been bothering me for a month.\n</Comment>\n<Comment by parap1uie-s at 2025-01-07T06:47:23Z>\n> @parap1uie-s have you had a chance to solve this problem? I am having the same problem using default data in readme.\r\n> \r\n> Here's my training loss and epoch. I don't understand how it spikes when it was stable throughout multiple epochs. Learning rate is also slowly decreasing. <img alt=\"image\" width=\"1957\" src=\"https://private-user-images.githubusercontent.com/3460594/365782127-b6a7b9f3-66e3-4dd3-b2d7-bb1b7fe35cd0.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzYyMzI1OTIsIm5iZiI6MTczNjIzMjI5MiwicGF0aCI6Ii8zNDYwNTk0LzM2NTc4MjEyNy1iNmE3YjlmMy02NmUzLTRkZDMtYjJkNy1iYjFiN2ZlMzVjZDAucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDEwNyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTAxMDdUMDY0NDUyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9N2FiODg0OTZkMTY2ODczMTMyNDMzYTljN2E3MTRhNDdhYWUzZTIxM2Q3ZGVlNmIxZTQ4NGNiZjk4ZjZlOGUyNyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.2zTJyz5IIQlt_A7qdGvtG5d2rn1Xaz-FpRH4K_fYVUY\">\r\n\r\n\r\n\r\n> > ### 描述问题\r\n> > 问题：\r\n> > 我正在使用我的自定义数据集微调 LLaVA 1.5 13b。训练过程看起来很正常 （~0.4），直到迭代 （随机，尚未找到模式），损失并且直到 epoch 结束才能下降。`` scripts/v1.5/finetune_task_lora.sh``Fly away ``\r\n> > 我的数据集由一轮中文对话组成。在微调过程之后，我使用 serve.cli 检查模型的输出。但是，它就像乱码 UTF-8 代码一样：\r\n> > ```\r\n> > USER: 描述这幅图像\r\n> > ASSISTANT: �。��见。 影像可诋：��译��影像可见：皮肤上有一个毛的人的皮肤有几个红色的红色的皮肤状块�。\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > 有什么建议吗？提前感谢。\r\n> > 命令：\r\n> > ```\r\n> > nohup sh scripts/v1_5/finetune_task_lora.sh > finetune.log 2>&1 &\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > 日志：\r\n> > ```\r\n> > {'loss': 0.278, 'learning_rate': 0.00017723609012917414, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2543/10485 [3:13:38<9:51:41,  4.47s/it]\r\n> >  24%|██▍       | 2544/10485 [3:13:42<9:53:22,  4.48s/it]\r\n> >                                                         \r\n> > {'loss': 0.2561, 'learning_rate': 0.00017721646511498725, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2544/10485 [3:13:42<9:53:22,  4.48s/it]\r\n> >  24%|██▍       | 2545/10485 [3:13:47<9:58:11,  4.52s/it]\r\n> >                                                         \r\n> > {'loss': 0.2387, 'learning_rate': 0.00017719683273249262, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2545/10485 [3:13:47<9:58:11,  4.52s/it]\r\n> >  24%|██▍       | 2546/10485 [3:13:52<9:56:24,  4.51s/it]\r\n> >                                                         \r\n> > {'loss': 0.2537, 'learning_rate': 0.00017717719298356377, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2546/10485 [3:13:52<9:56:24,  4.51s/it]\r\n> >  24%|██▍       | 2547/10485 [3:13:56<10:02:22,  4.55s/it]\r\n> >                                                          \r\n> > {'loss': 0.2363, 'learning_rate': 0.00017715754587007475, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2547/10485 [3:13:56<10:02:22,  4.55s/it]\r\n> >  24%|██▍       | 2548/10485 [3:14:01<10:07:30,  4.59s/it]\r\n> >                                                          \r\n> > {'loss': 0.196, 'learning_rate': 0.00017713789139390035, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2548/10485 [3:14:01<10:07:30,  4.59s/it]\r\n> >  24%|██▍       | 2549/10485 [3:14:05<9:55:42,  4.50s/it] \r\n> >                                                         \r\n> > {'loss': 0.1796, 'learning_rate': 0.0001771182295569161, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2549/10485 [3:14:05<9:55:42,  4.50s/it]\r\n> >  24%|██▍       | 2550/10485 [3:14:10<9:53:36,  4.49s/it]\r\n> >                                                         \r\n> > {'loss': 0.2633, 'learning_rate': 0.0001770985603609982, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2550/10485 [3:14:10<9:53:36,  4.49s/it]\r\n> >  24%|██▍       | 2551/10485 [3:14:14<9:53:20,  4.49s/it]\r\n> >                                                         \r\n> > {'loss': 0.2268, 'learning_rate': 0.0001770788838080236, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2551/10485 [3:14:14<9:53:20,  4.49s/it]\r\n> >  24%|██▍       | 2552/10485 [3:14:19<9:58:08,  4.52s/it]\r\n> >                                                         \r\n> > {'loss': 0.2249, 'learning_rate': 0.00017705919989986985, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2552/10485 [3:14:19<9:58:08,  4.52s/it]\r\n> >  24%|██▍       | 2553/10485 [3:14:23<9:55:14,  4.50s/it]\r\n> >                                                         \r\n> > {'loss': 0.237, 'learning_rate': 0.00017703950863841533, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2553/10485 [3:14:23<9:55:14,  4.50s/it]\r\n> >  24%|██▍       | 2554/10485 [3:14:28<9:55:44,  4.51s/it]\r\n> >                                                         \r\n> > {'loss': 0.2058, 'learning_rate': 0.00017701981002553904, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2554/10485 [3:14:28<9:55:44,  4.51s/it]\r\n> >  24%|██▍       | 2555/10485 [3:14:32<9:54:32,  4.50s/it]\r\n> >                                                         \r\n> > {'loss': 0.231, 'learning_rate': 0.0001770001040631207, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2555/10485 [3:14:32<9:54:32,  4.50s/it]\r\n> >  24%|██▍       | 2556/10485 [3:14:36<9:41:41,  4.40s/it]\r\n> >                                                         \r\n> > {'loss': 0.3225, 'learning_rate': 0.00017698039075304069, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2556/10485 [3:14:36<9:41:41,  4.40s/it]\r\n> >  24%|██▍       | 2557/10485 [3:14:41<9:48:28,  4.45s/it]\r\n> >                                                         \r\n> > {'loss': 0.3587, 'learning_rate': 0.0001769606700971802, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2557/10485 [3:14:41<9:48:28,  4.45s/it]\r\n> >  24%|██▍       | 2558/10485 [3:14:45<9:48:32,  4.45s/it]\r\n> >                                                         \r\n> > {'loss': 0.2832, 'learning_rate': 0.00017694094209742104, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2558/10485 [3:14:45<9:48:32,  4.45s/it]\r\n> >  24%|██▍       | 2559/10485 [3:14:50<9:48:51,  4.46s/it]\r\n> >                                                         \r\n> > {'loss': 0.4236, 'learning_rate': 0.00017692120675564575, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2559/10485 [3:14:50<9:48:51,  4.46s/it]\r\n> >  24%|██▍       | 2560/10485 [3:14:55<9:59:36,  4.54s/it]\r\n> >                                                         \r\n> > {'loss': 0.4833, 'learning_rate': 0.00017690146407373748, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2560/10485 [3:14:55<9:59:36,  4.54s/it]\r\n> >  24%|██▍       | 2561/10485 [3:14:59<9:58:30,  4.53s/it]\r\n> >                                                         \r\n> > {'loss': 0.5024, 'learning_rate': 0.00017688171405358022, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2561/10485 [3:14:59<9:58:30,  4.53s/it]\r\n> >  24%|██▍       | 2562/10485 [3:15:04<10:00:41,  4.55s/it]\r\n> >                                                          \r\n> > {'loss': 0.6037, 'learning_rate': 0.0001768619566970586, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2562/10485 [3:15:04<10:00:41,  4.55s/it]\r\n> >  24%|██▍       | 2563/10485 [3:15:08<9:57:45,  4.53s/it] \r\n> >                                                         \r\n> > {'loss': 0.9337, 'learning_rate': 0.00017684219200605792, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2563/10485 [3:15:08<9:57:45,  4.53s/it]\r\n> >  24%|██▍       | 2564/10485 [3:15:13<10:06:35,  4.59s/it]\r\n> >                                                          \r\n> > {'loss': 0.6623, 'learning_rate': 0.00017682241998246423, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2564/10485 [3:15:13<10:06:35,  4.59s/it]\r\n> >  24%|██▍       | 2565/10485 [3:15:17<10:03:00,  4.57s/it]\r\n> >                                                          \r\n> > {'loss': 0.8022, 'learning_rate': 0.0001768026406281642, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2565/10485 [3:15:17<10:03:00,  4.57s/it]\r\n> >  24%|██▍       | 2566/10485 [3:15:22<10:03:32,  4.57s/it]\r\n> >                                                          \r\n> > {'loss': 4.2653, 'learning_rate': 0.00017678285394504535, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2566/10485 [3:15:22<10:03:32,  4.57s/it]\r\n> >  24%|██▍       | 2567/10485 [3:15:26<9:59:39,  4.54s/it] \r\n> >                                                         \r\n> > {'loss': 0.77, 'learning_rate': 0.00017676305993499574, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2567/10485 [3:15:26<9:59:39,  4.54s/it]\r\n> >  24%|██▍       | 2568/10485 [3:15:31<10:01:29,  4.56s/it]\r\n> >                                                          \r\n> > {'loss': 0.556, 'learning_rate': 0.00017674325859990422, 'epoch': 0.24}\r\n> > \r\n> >  24%|██▍       | 2568/10485 [3:15:31<10:01:29,  4.56s/it]\r\n> >  25%|██▍       | 2569/10485 [3:15:36<10:08:46,  4.61s/it]\r\n> >                                                          \r\n> > {'loss': 0.8223, 'learning_rate': 0.00017672344994166031, 'epoch': 0.25}\r\n> > \r\n> >  25%|██▍       | 2569/10485 [3:15:36<10:08:46,  4.61s/it]\r\n> >  25%|██▍       | 2570/10485 [3:15:41<10:13:47,  4.65s/it]\r\n> >                                                          \r\n> > {'loss': 0.6307, 'learning_rate': 0.0001767036339621542, 'epoch': 0.25}\r\n> > \r\n> >  25%|██▍       | 2570/10485 [3:15:41<10:13:47,  4.65s/it]\r\n> >  25%|██▍       | 2571/10485 [3:15:45<10:04:22,  4.58s/it]\r\n> >                                                          \r\n> > {'loss': 0.7256, 'learning_rate': 0.00017668381066327687, 'epoch': 0.25}\r\n> > \r\n> >  25%|██▍       | 2571/10485 [3:15:45<10:04:22,  4.58s/it]\r\n> >  25%|██▍       | 2572/10485 [3:15:49<10:02:48,  4.57s/it]\r\n> >                                                          \r\n> > {'loss': 5.4546, 'learning_rate': 0.0001766639800469199, 'epoch': 0.25}\r\n> > \r\n> >  25%|██▍       | 2572/10485 [3:15:49<10:02:48,  4.57s/it]\r\n> >  25%|██▍       | 2573/10485 [3:15:54<10:08:43,  4.62s/it]\r\n> >                                                          \r\n> > {'loss': 7.7808, 'learning_rate': 0.0001766441421149756, 'epoch': 0.25}\r\n> > \r\n> >  25%|██▍       | 2573/10485 [3:15:54<10:08:43,  4.62s/it]\r\n> >  25%|██▍       | 2574/10485 [3:15:59<10:08:55,  4.62s/it]\r\n> >                                                          \r\n> > {'loss': 8.2073, 'learning_rate': 0.00017662429686933698, 'epoch': 0.25}\r\n> > \r\n> >  25%|██▍       | 2574/10485 [3:15:59<10:08:55,  4.62s/it]\r\n> >  25%|██▍       | 2575/10485 [3:16:04<10:16:07,  4.67s/it]\r\n> >                                                          \r\n> > {'loss': 4.118, 'learning_rate': 0.0001766044443118978, 'epoch': 0.25}\r\n> > \r\n> >  25%|██▍       | 2575/10485 [3:16:04<10:16:07,  4.67s/it]\r\n> >  25%|██▍       | 2576/10485 [3:16:08<10:21:40,  4.72s/it]\r\n> >                                                          \r\n> > {'loss': 4.8635, 'learning_rate': 0.00017658458444455243, 'epoch': 0.25}\r\n> > \r\n> >  25%|██▍       | 2576/10485 [3:16:08<10:21:40,  4.72s/it]\r\n> >  25%|██▍       | 2577/10485 [3:16:13<10:19:28,  4.70s/it]\r\n> >                                                          \r\n> > {'loss': 5.6341, 'learning_rate': 0.00017656471726919604, 'epoch': 0.25}\r\n> > \r\n> >  25%|██▍       | 2577/10485 [3:16:13<10:19:28,  4.70s/it]\r\n> >  25%|██▍       | 2578/10485 [3:16:18<10:27:23,  4.76s/it]\r\n> >                                                          \r\n> > {'loss': 10.266, 'learning_rate': 0.00017654484278772437, 'epoch': 0.25}\r\n> > \r\n> >  25%|██▍       | 2578/10485 [3:16:18<10:27:23,  4.76s/it]\r\n> >  25%|██▍       | 2579/10485 [3:16:23<10:20:03,  4.71s/it]\r\n> >                                                          \r\n> > {'loss': 8.0332, 'learning_rate': 0.00017652496100203392, 'epoch': 0.25}\r\n> > \r\n> >  25%|██▍       | 2579/10485 [3:16:23<10:20:03,  4.71s/it]\r\n> >  25%|██▍       | 2580/10485 [3:16:27<10:13:08,  4.65s/it]\r\n> >                                                          \r\n> > {'loss': 7.8216, 'learning_rate': 0.00017650507191402194, 'epoch': 0.25}\r\n> > \r\n> >  25%|██▍       | 2580/10485 [3:16:27<10:13:08,  4.65s/it]\r\n> >  25%|██▍       | 2581/10485 [3:16:32<10:24:19,  4.74s/it]\r\n> >                                                          \r\n> > {'loss': 7.7877, 'learning_rate': 0.0001764851755255863, 'epoch': 0.25}\r\n> > \r\n> >  25%|██▍       | 2581/10485 [3:16:32<10:24:19,  4.74s/it]\r\n> >  25%|██▍       | 2582/10485 [3:16:37<10:12:33,  4.65s/it]\r\n> >                                                          \r\n> > {'loss': 8.8278, 'learning_rate': 0.00017646527183862558, 'epoch': 0.25}\r\n> > \r\n> >  25%|██▍       | 2582/10485 [3:16:37<10:12:33,  4.65s/it]\r\n> >  25%|██▍       | 2583/10485 [3:16:41<10:05:41,  4.60s/it]\r\n> >                                                          \r\n> > {'loss': 8.6483, 'learning_rate': 0.0001764453608550391, 'epoch': 0.25}\r\n> > \r\n> >  25%|██▍       | 2583/10485 [3:16:41<10:05:41,  4.60s/it]\r\n> >  25%|██▍       | 2584/10485 [3:16:46<10:08:49,  4.62s/it]\r\n> >                                                          \r\n> > {'loss': 6.5844, 'learning_rate': 0.00017642544257672683, 'epoch': 0.25}\r\n> > \r\n> >  25%|██▍       | 2584/10485 [3:16:46<10:08:49,  4.62s/it]\r\n> >  25%|██▍       | 2585/10485 [3:16:50<10:00:06,  4.56s/it]\r\n> >                                                          \r\n> > {'loss': 8.1825, 'learning_rate': 0.00017640551700558944, 'epoch': 0.25}\r\n> > \r\n> >  25%|██▍       | 2585/10485 [3:16:50<10:00:06,  4.56s/it]\r\n> >  25%|██▍       | 2586/10485 [3:16:55<10:03:34,  4.58s/it]\r\n> >                                                          \r\n> > {'loss': 7.0871, 'learning_rate': 0.00017638558414352835, 'epoch': 0.25}\r\n> > \r\n> >  25%|██▍       | 2586/10485 [3:16:55<10:03:34,  4.58s/it]\r\n> >  25%|██▍       | 2587/10485 [3:16:59<10:02:16,  4.58s/it]\r\n> >                                                          \r\n> > {'loss': 6.7109, 'learning_rate': 0.0001763656439924456, 'epoch': 0.25}\r\n> > \r\n> >  25%|██▍       | 2587/10485 [3:16:59<10:02:16,  4.58s/it]\r\n> >  25%|██▍       | 2588/10485 [3:17:04<9:56:44,  4.53s/it] \r\n> >                                                         \r\n> > {'loss': 6.6635, 'learning_rate': 0.00017634569655424395, 'epoch': 0.25}\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > 屏幕截图： 如果它更好地解释了问题，您可以附加屏幕截图。\r\n> \r\n> Hello, have you solved this issue? I have encountered the same problem during training, and it has been bothering me for a month.\r\n\r\nReduce the lr to <1e-4 solved the problem.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 917,
    "state": "closed",
    "created_by": "Rickylht",
    "created_at": "2023-12-13T08:57:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/917</URL>\n\n<TITLE>[Question] Do we need to add \"--tune_mm_mlp_adapter True\" in finetuning?</TITLE>\n\n<BODY>### Question\n\nDo we need to add \"--tune_mm_mlp_adapter True\" in finetuning?\r\nSince the official scripts didn't add this parameter.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/38951435/b27ae150-c5f9-438c-8393-2f3d384e54a8)\r\n\r\nAnd the official paper said we need to tune the adapter in finetuning stage.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/38951435/6a8af063-1ecf-4140-b41f-b41ba7ebe7d7)</BODY>\n\n<COMMENTS>\n<Comment by unmo at 2023-12-14T02:36:20Z>\nI had the same question. May I ask you to tell me about this one?\n</Comment>\n<Comment by tetsu-kikuchi at 2024-05-10T07:57:43Z>\n`tune_mm_mlp_adapter` seems to be a flag to tune **_only_** mm_mlp_adapter (so tune_only_mm_mlp_adapter would be a more appropriate name).  \r\n\r\nIt should be set `True` at pretraining, while `False` at fine-tuning since we also want to tune parameters in a language model in the latter case. \r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/3e337ad269da3245643a2724a1d694b5839c37f9/llava/train/train.py#L927-L930\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 916,
    "state": "open",
    "created_by": "shashi-netra",
    "created_at": "2023-12-12T12:49:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/916</URL>\n\n<TITLE>[Question] Predictions in video by stitching consecutive frames into a single image</TITLE>\n\n<BODY>### Question\n\nI am looking to use LLaVa for predictions in video by stitching a sequence of consecutive frames into a single image and then asking LLava for a prediction. Has anyone used this approach before and found any success? if so, any tips on how you approached it.</BODY>\n\n<COMMENTS>\n<Comment by mhkz at 2024-04-29T02:57:20Z>\nHi, now I also need to predict videos. Do you have a better solution? My current approach is to draw frames to predict\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 915,
    "state": "open",
    "created_by": "kimimorgam",
    "created_at": "2023-12-12T05:33:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/915</URL>\n\n<TITLE>AttributeError: 'NoneType' object has no attribute 'skip_next'[Usage]</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: AttributeError: 'NoneType' object has no attribute 'skip_next'\r\n\r\nCommand:\r\npython -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload\r\n\r\nLog: \r\n2023-12-12 02:27:46 | INFO | gradio_web_server | args: Namespace(host='0.0.0.0', port=None, controller_url='http://localhost:10000', concurrency_count=10, model_list_mode='reload', share=False, moderate=False, embed=False)\r\n2023-12-12 02:27:50 | INFO | gradio_web_server | Models: []\r\n2023-12-12 02:27:50 | INFO | gradio_web_server | Namespace(host='0.0.0.0', port=None, controller_url='http://localhost:10000', concurrency_count=10, model_list_mode='reload', share=False, moderate=False, embed=False)\r\n2023-12-12 02:27:55 | INFO | stdout | Running on local URL:  http://0.0.0.0:7860\r\n2023-12-12 02:27:55 | INFO | stdout |\r\n2023-12-12 02:27:55 | INFO | stdout | To create a public link, set `share=True` in `launch()`.\r\n2023-12-12 02:28:25 | INFO | gradio_web_server | load_demo. ip: 127.0.0.1\r\n2023-12-12 02:28:27 | INFO | gradio_web_server | add_text. ip: 127.0.0.1. len: 33\r\n2023-12-12 02:28:27 | ERROR | stderr | Traceback (most recent call last):\r\n2023-12-12 02:28:27 | ERROR | stderr |   File \"C:\\Users\\kimim\\miniconda3\\Lib\\site-packages\\gradio\\routes.py\", line 437, in run_predict\r\n2023-12-12 02:28:27 | ERROR | stderr |     output = await app.get_blocks().process_api(\r\n2023-12-12 02:28:27 | ERROR | stderr |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2023-12-12 02:28:27 | ERROR | stderr |   File \"C:\\Users\\kimim\\miniconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1352, in process_api\r\n2023-12-12 02:28:27 | ERROR | stderr |     result = await self.call_function(\r\n2023-12-12 02:28:27 | ERROR | stderr |              ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2023-12-12 02:28:27 | ERROR | stderr |   File \"C:\\Users\\kimim\\miniconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1077, in call_function\r\n2023-12-12 02:28:27 | ERROR | stderr |     prediction = await anyio.to_thread.run_sync(\r\n2023-12-12 02:28:27 | ERROR | stderr |                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2023-12-12 02:28:27 | ERROR | stderr |   File \"C:\\Users\\kimim\\miniconda3\\Lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\r\n2023-12-12 02:28:27 | ERROR | stderr |     return await get_asynclib().run_sync_in_worker_thread(\r\n2023-12-12 02:28:27 | ERROR | stderr |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2023-12-12 02:28:27 | ERROR | stderr |   File \"C:\\Users\\kimim\\miniconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\r\n2023-12-12 02:28:27 | ERROR | stderr |     return await future\r\n2023-12-12 02:28:27 | ERROR | stderr |            ^^^^^^^^^^^^\r\n2023-12-12 02:28:27 | ERROR | stderr |   File \"C:\\Users\\kimim\\miniconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\r\n2023-12-12 02:28:27 | ERROR | stderr |     result = context.run(func, *args)\r\n2023-12-12 02:28:27 | ERROR | stderr |              ^^^^^^^^^^^^^^^^^^^^^^^^\r\n2023-12-12 02:28:27 | ERROR | stderr |   File \"C:\\gpt4\\LLaVA\\llava\\serve\\gradio_web_server.py\", line 148, in add_text\r\n2023-12-12 02:28:27 | ERROR | stderr |     if len(state.get_images(return_pil=True)) > 0:\r\n2023-12-12 02:28:27 | ERROR | stderr |            ^^^^^^^^^^^^^^^^\r\n2023-12-12 02:28:27 | ERROR | stderr | AttributeError: 'NoneType' object has no attribute 'get_images'\r\n2023-12-12 02:28:28 | INFO | gradio_web_server | http_bot. ip: 127.0.0.1\r\n2023-12-12 02:28:28 | ERROR | stderr | Traceback (most recent call last):\r\n2023-12-12 02:28:28 | ERROR | stderr |   File \"C:\\Users\\kimim\\miniconda3\\Lib\\site-packages\\gradio\\routes.py\", line 437, in run_predict\r\n2023-12-12 02:28:28 | ERROR | stderr |     output = await app.get_blocks().process_api(\r\n2023-12-12 02:28:28 | ERROR | stderr |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2023-12-12 02:28:28 | ERROR | stderr |   File \"C:\\Users\\kimim\\miniconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1352, in process_api\r\n2023-12-12 02:28:28 | ERROR | stderr |     result = await self.call_function(\r\n2023-12-12 02:28:28 | ERROR | stderr |              ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2023-12-12 02:28:28 | ERROR | stderr |   File \"C:\\Users\\kimim\\miniconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1093, in call_function\r\n2023-12-12 02:28:28 | ERROR | stderr |     prediction = await utils.async_iteration(iterator)\r\n2023-12-12 02:28:28 | ERROR | stderr |                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2023-12-12 02:28:28 | ERROR | stderr |   File \"C:\\Users\\kimim\\miniconda3\\Lib\\site-packages\\gradio\\utils.py\", line 341, in async_iteration\r\n2023-12-12 02:28:28 | ERROR | stderr |     return await iterator.__anext__()\r\n2023-12-12 02:28:28 | ERROR | stderr |            ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2023-12-12 02:28:28 | ERROR | stderr |   File \"C:\\Users\\kimim\\miniconda3\\Lib\\site-packages\\gradio\\utils.py\", line 334, in __anext__\r\n2023-12-12 02:28:28 | ERROR | stderr |     return await anyio.to_thread.run_sync(\r\n2023-12-12 02:28:28 | ERROR | stderr |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2023-12-12 02:28:28 | ERROR | stderr |   File \"C:\\Users\\kimim\\miniconda3\\Lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\r\n2023-12-12 02:28:28 | ERROR | stderr |     return await get_asynclib().run_sync_in_worker_thread(\r\n2023-12-12 02:28:28 | ERROR | stderr |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2023-12-12 02:28:28 | ERROR | stderr |   File \"C:\\Users\\kimim\\miniconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\r\n2023-12-12 02:28:28 | ERROR | stderr |     return await future\r\n2023-12-12 02:28:28 | ERROR | stderr |            ^^^^^^^^^^^^\r\n2023-12-12 02:28:28 | ERROR | stderr |   File \"C:\\Users\\kimim\\miniconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\r\n2023-12-12 02:28:28 | ERROR | stderr |     result = context.run(func, *args)\r\n2023-12-12 02:28:28 | ERROR | stderr |              ^^^^^^^^^^^^^^^^^^^^^^^^\r\n2023-12-12 02:28:28 | ERROR | stderr |   File \"C:\\Users\\kimim\\miniconda3\\Lib\\site-packages\\gradio\\utils.py\", line 317, in run_sync_iterator_async\r\n2023-12-12 02:28:28 | ERROR | stderr |     return next(iterator)\r\n2023-12-12 02:28:28 | ERROR | stderr |            ^^^^^^^^^^^^^^\r\n2023-12-12 02:28:28 | ERROR | stderr |   File \"C:\\gpt4\\LLaVA\\llava\\serve\\gradio_web_server.py\", line 161, in http_bot\r\n2023-12-12 02:28:28 | ERROR | stderr |     if state.skip_next:\r\n2023-12-12 02:28:28 | ERROR | stderr |        ^^^^^^^^^^^^^^^\r\n2023-12-12 02:28:28 | ERROR | stderr | AttributeError: 'NoneType' object has no attribute 'skip_next'\r\n2023-12-12 02:28:31 | INFO | gradio_web_server | Models: ['llava-v1.5-13b']\r\n\r\nScreenshots:\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/140536250/d1dc51fc-c490-434c-b251-242b51bdc3fd)\r\n\r\n\r\nAfter installed, running things, when test \"what is a unusual...\" this happens\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/140536250/1a6ac959-2ac9-4f24-b08b-b745682833ab)</BODY>\n\n<COMMENTS>\n<Comment by kimimorgam at 2023-12-12T05:36:22Z>\n![image](https://github.com/haotian-liu/LLaVA/assets/140536250/76e10344-6d31-4394-8eb2-14246c86bd5d)\r\n\r\nAnd it, too :/\n</Comment>\n<Comment by learnerljh at 2024-07-13T08:20:48Z>\nI met the same error. Has anyone fixed it?\n</Comment>\n<Comment by HarvestHan at 2025-03-05T12:41:01Z>\npip install gradio==4.16.0\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 914,
    "state": "open",
    "created_by": "unmo",
    "created_at": "2023-12-12T01:51:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/914</URL>\n\n<TITLE>[Question] Method of training domain information.</TITLE>\n\n<BODY>### Question\n\nI would like to train domain information using a very small amount of data.\r\nIf so, how should I go about training it? From pretrained or Finetune?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 912,
    "state": "closed",
    "created_by": "yummyKnight",
    "created_at": "2023-12-11T10:19:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/912</URL>\n\n<TITLE>Issue: Can not download pretrain dataset</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI am trying download LAION-CC-SBU from HF using command:\r\nCommand:\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"liuhaotian/LLaVA-Pretrain\")\r\n```\r\nBut after cache are downloaded i got error: \r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/user/conda/envs/llava/lib/python3.10/site-packages/datasets/builder.py\", line 1932, in _prepare_split_single\r\n    writer.write_table(table)\r\n  File \"/home/user/conda/envs/llava/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 573, in write_table\r\n    pa_table = table_cast(pa_table, self._schema)\r\n  File \"/home/user/conda/envs/llava/lib/python3.10/site-packages/datasets/table.py\", line 2332, in table_cast\r\n    return cast_table_to_schema(table, schema)\r\n  File \"/home/user/conda/envs/llava/lib/python3.10/site-packages/datasets/table.py\", line 2290, in cast_table_to_schema\r\n    raise ValueError(f\"Couldn't cast\\n{table.schema}\\nto\\n{features}\\nbecause column names don't match\")\r\nValueError: Couldn't cast\r\nid: string\r\nurl: string\r\nimage: string\r\nblip_caption: string\r\nto\r\n{'id': Value(dtype='string', id=None), 'conversations': [{'from': Value(dtype='string', id=None), 'value': Value(dtype='string', id=None)}], 'image': Value(dtype='string', id=None)}\r\nbecause column names don't match\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/jovyan/LLaVA/download_ds.py\", line 11, in <module>\r\n    dataset = load_dataset(\"liuhaotian/LLaVA-Pretrain\")\r\n  File \"/home/user/conda/envs/llava/lib/python3.10/site-packages/datasets/load.py\", line 2152, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/user/conda/envs/llava/lib/python3.10/site-packages/datasets/builder.py\", line 948, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/user/conda/envs/llava/lib/python3.10/site-packages/datasets/builder.py\", line 1043, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/user/conda/envs/llava/lib/python3.10/site-packages/datasets/builder.py\", line 1805, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"/home/user/conda/envs/llava/lib/python3.10/site-packages/datasets/builder.py\", line 1950, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n```\r\nVersion of datasets==2.15.0 \r\nHave somebody experienced same problem?</BODY>\n\n<COMMENTS>\n<Comment by jkli1998 at 2024-03-13T04:21:31Z>\nI have encountered the same problem. May I ask how you solved it?\n</Comment>\n<Comment by yummyKnight at 2024-03-15T20:59:36Z>\n@jkli1998 same as here (#1139)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 911,
    "state": "closed",
    "created_by": "Junpliu",
    "created_at": "2023-12-11T08:20:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/911</URL>\n\n<TITLE>[Question] What is the possible method to get a llava with longer context length ~ 16k? #610</TITLE>\n\n<BODY>### Question\n\nCould I download the pre-trained connector weight and then finetune the new model(blip + connector + llm with long context > 4k) on my downstream task datasets? Is it plausible?</BODY>\n\n<COMMENTS>\n<Comment by criminact at 2024-03-07T10:34:55Z>\nHow were you able to achieve this? clipping a FT Language model is doable?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 910,
    "state": "open",
    "created_by": "LianghuiGuo",
    "created_at": "2023-12-11T07:05:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/910</URL>\n\n<TITLE>[Question] RuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cpu)</TITLE>\n\n<BODY>### Question\n\nI met this error when finetuning the model, and the environment configuration was based on the official environment.\r\n```python\r\nLoading checkpoint shards: 100%|██████████| 33/33 [02:49<00:00,  5.37s/it]\r\nLoading checkpoint shards: 100%|██████████| 33/33 [02:49<00:00,  5.14s/it]\r\nSome weights of MPLUGOwl2LlamaForCausalLM were not initialized from the model checkpoint at /data/oss_bucket_0/mplug_owl2 and are newly initialized: ['model.visual_abstractor.encoder.layers.0.crossattention.attention.q_pos_embed', 'model.visual_abstractor.encoder.layers.2.crossattention.attention.k_pos_embed', 'model.visual_abstractor.encoder.layers.0.crossattention.attention.k_pos_embed', 'model.visual_abstractor.encoder.layers.3.crossattention.attention.q_pos_embed', 'model.visual_abstractor.encoder.layers.1.crossattention.attention.q_pos_embed', 'model.visual_abstractor.encoder.layers.4.crossattention.attention.q_pos_embed', 'model.visual_abstractor.encoder.layers.5.crossattention.attention.q_pos_embed', 'model.visual_abstractor.encoder.layers.3.crossattention.attention.k_pos_embed', 'model.visual_abstractor.encoder.layers.1.crossattention.attention.k_pos_embed', 'model.visual_abstractor.encoder.layers.2.crossattention.attention.q_pos_embed', 'model.visual_abstractor.encoder.layers.5.crossattention.attention.k_pos_embed', 'model.visual_abstractor.encoder.layers.4.crossattention.attention.k_pos_embed']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\nic| training_args.tune_visual_abstractor: True\r\nic| training_args.freeze_vision_model: True\r\nic| len(optimizer_grouped_parameters[0]['params']): 1040\r\n    len(optimizer_grouped_parameters[1]['params']): 91\r\nUsing :/usr/local/ninja as PyTorch extensions root...\r\nLoading extension module utils...\r\nUsing :/usr/local/ninja as PyTorch extensions root...\r\nNo modifications detected for re-loaded extension module utils, skipping build step...\r\nLoading extension module utils...\r\n\r\n  0%|          | 0/1 [00:00<?, ?it/s]/opt/conda/envs/python3.8.13/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\r\n  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\r\nTraceback (most recent call last):\r\n  File \"/checkpoint/binary/train_package/mplug_owl2/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/checkpoint/binary/train_package/mplug_owl2/train/train.py\", line 801, in train\r\n    trainer.train()\r\n  File \"/root/.local/lib/python3.8/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/root/.local/lib/python3.8/site-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/root/.local/lib/python3.8/site-packages/transformers/trainer.py\", line 2654, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/root/.local/lib/python3.8/site-packages/transformers/trainer.py\", line 2679, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/opt/conda/envs/python3.8.13/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/root/.local/lib/python3.8/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/root/.local/lib/python3.8/site-packages/deepspeed/runtime/engine.py\", line 1736, in forward\r\n    loss = self.module(*inputs, **kwargs)\r\n  File \"/opt/conda/envs/python3.8.13/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/root/.local/lib/python3.8/site-packages/peft/peft_model.py\", line 922, in forward\r\n    return self.base_model(\r\n  File \"/opt/conda/envs/python3.8.13/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/checkpoint/binary/train_package/mplug_owl2/model/modeling_mplug_owl2.py\", line 242, in forward\r\n    outputs = self.model(\r\n  File \"/opt/conda/envs/python3.8.13/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/checkpoint/binary/train_package/mplug_owl2/model/modeling_llama2.py\", line 337, in model_forward\r\n    layer_outputs = torch.utils.checkpoint.checkpoint(\r\n  File \"/opt/conda/envs/python3.8.13/lib/python3.8/site-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\r\n    return CheckpointFunction.apply(function, preserve, *args)\r\n  File \"/opt/conda/envs/python3.8.13/lib/python3.8/site-packages/torch/autograd/function.py\", line 506, in apply\r\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\r\n  File \"/opt/conda/envs/python3.8.13/lib/python3.8/site-packages/torch/utils/checkpoint.py\", line 107, in forward\r\n    outputs = run_function(*args)\r\n  File \"/checkpoint/binary/train_package/mplug_owl2/model/modeling_llama2.py\", line 333, in custom_forward\r\n    return module(*inputs, past_key_value, output_attentions)\r\n  File \"/opt/conda/envs/python3.8.13/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/checkpoint/binary/train_package/mplug_owl2/model/modeling_llama2.py\", line 222, in forward\r\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n  File \"/opt/conda/envs/python3.8.13/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/checkpoint/binary/train_package/mplug_owl2/train/llama_flash_attn_monkey_patch.py\", line 55, in forward\r\n    query_states, key_states = apply_rotary_pos_emb(\r\n  File \"/root/.local/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py\", line 184, in apply_rotary_pos_emb\r\n    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\r\nRuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cpu)\r\n\r\n```</BODY>\n\n<COMMENTS>\n<Comment by TsuTikgiau at 2024-02-05T08:35:30Z>\nI solved this issue by reinstall pytorch and deepspeed\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 909,
    "state": "open",
    "created_by": "Jayce1kk",
    "created_at": "2023-12-11T04:12:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/909</URL>\n\n<TITLE>I am using 8 RTX 4090 GPUs, 24GB each card, can I train and inference？</TITLE>\n\n<BODY></BODY>\n\n<COMMENTS>\n<Comment by RicRicci22 at 2023-12-12T09:19:22Z>\nI'm able to inference with just one RTX 3090.. so for that matter, inference should not be a major concern. Even for training, I think your rack is more than enough :)\n</Comment>\n<Comment by Krisdddd at 2023-12-14T07:20:54Z>\nif you have two, you'll be able to finetune 13b with lora\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 908,
    "state": "open",
    "created_by": "YangQiuEric",
    "created_at": "2023-12-10T10:50:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/908</URL>\n\n<TITLE>LlamaRMSNorm()   (post_attention_layernorm): LlamaRMSNorm() ) is not supported. Currently, only `torch.nn.Linear` and `Conv1D` are supported.</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\n```\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256  \\\r\n    --deepspeed ./scripts/zero3_offload.json \\\r\n    --model_name_or_path liuhaotian/llava-v1.5-13b \\\r\n    --version v1 \\\r\n    --data_path /Users/eric/Documents/llava/patient_data.json \\\r\n    --image_folder /home/eric/LLaVA-Med/data_file_jpg \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-13b-task-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 4 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n\r\n```\r\n\r\nLog: \r\n```\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:28<00:00,  9.54s/it]\r\nAdding LoRA adapters...\r\nTraceback (most recent call last):\r\n  File \"/home/eric/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/home/eric/anaconda3/envs/llava-med/lib/python3.10/site-packages/llava/train/train.py\", line 837, in train\r\n    model = get_peft_model(model, lora_config)\r\n  File \"/home/eric/anaconda3/envs/llava-med/lib/python3.10/site-packages/peft/mapping.py\", line 98, in get_peft_model\r\n    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model, peft_config, adapter_name=adapter_name)\r\n  File \"/home/eric/anaconda3/envs/llava-med/lib/python3.10/site-packages/peft/peft_model.py\", line 893, in __init__\r\n    super().__init__(model, peft_config, adapter_name)\r\n  File \"/home/eric/anaconda3/envs/llava-med/lib/python3.10/site-packages/peft/peft_model.py\", line 112, in __init__\r\n    self.base_model = PEFT_TYPE_TO_MODEL_MAPPING[peft_config.peft_type](\r\n  File \"/home/eric/anaconda3/envs/llava-med/lib/python3.10/site-packages/peft/tuners/lora.py\", line 180, in __init__\r\n    self.add_adapter(adapter_name, self.peft_config[adapter_name])\r\n  File \"/home/eric/anaconda3/envs/llava-med/lib/python3.10/site-packages/peft/tuners/lora.py\", line 194, in add_adapter\r\n    self._find_and_replace(adapter_name)\r\n  File \"/home/eric/anaconda3/envs/llava-med/lib/python3.10/site-packages/peft/tuners/lora.py\", line 352, in _find_and_replace\r\n    new_module = self._create_new_module(lora_config, adapter_name, target)\r\n  File \"/home/eric/anaconda3/envs/llava-med/lib/python3.10/site-packages/peft/tuners/lora.py\", line 305, in _create_new_module\r\n    raise ValueError(\r\nValueError: Target module LlamaDecoderLayer(\r\n  (self_attn): LlamaAttention(\r\n    (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\r\n    (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\r\n    (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\r\n    (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\r\n    (rotary_emb): LlamaRotaryEmbedding()\r\n  )\r\n  (mlp): LlamaMLP(\r\n    (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\r\n    (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\r\n    (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\r\n    (act_fn): SiLUActivation()\r\n  )\r\n  (input_layernorm): LlamaRMSNorm()\r\n  (post_attention_layernorm): LlamaRMSNorm()\r\n) is not supported. Currently, only `torch.nn.Linear` and `Conv1D` are supported.\r\n[2023-12-10 02:44:54,053] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2675989\r\n[2023-12-10 02:44:54,053] [ERROR] [launch.py:321:sigkill_handler] ['/home/eric/anaconda3/envs/llava-med/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=0', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--deepspeed', './scripts/zero3_offload.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-13b', '--version', 'v1', '--data_path', '/Users/eric/Documents/llava/patient_data.json', '--image_folder', '/home/eric/LLaVA-Med/data_file_jpg', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.5-13b-task-lora', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-4', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = 1\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by clima-ai at 2023-12-13T17:16:23Z>\nSame issue!\r\nrelated issue in https://github.com/oobabooga/text-generation-webui/issues/2297\r\nDoesn't work for me!\n</Comment>\n<Comment by uyo9ko at 2023-12-14T05:13:07Z>\ni add some nn.linear in LlavaLlamaForCausalLM, it leads this error. \r\nso i modify this func in train.py `multimodal_keywords = ['mm_projector', 'vision_tower', 'vision_resampler', 'my_linear_layer'],` my problem is sloved.\r\n```\r\ndef find_all_linear_names(model):\r\n    cls = torch.nn.Linear\r\n    lora_module_names = set()\r\n    multimodal_keywords = ['mm_projector', 'vision_tower', 'vision_resampler', 'my_linear_layer']\r\n    for name, module in model.named_modules():\r\n        if any(mm_keyword in name for mm_keyword in multimodal_keywords):\r\n            continue\r\n        if isinstance(module, cls):\r\n            names = name.split('.')\r\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\r\n\r\n    if 'lm_head' in lora_module_names: # needed for 16-bit\r\n        lora_module_names.remove('lm_head')\r\n    return list(lora_module_names)\r\n```\n</Comment>\n<Comment by clima-ai at 2023-12-14T18:19:39Z>\nNot working for me.\r\nWas your error exactly this one? (I realized that checkpoint shards didn't load)\r\n\r\nLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]^MLoading checkpoint shards:  50%|█████     | 1/2 [00:17<00:17, 17.33s/it]^MLoading checkpoint shards: 100%|██████████| 2/2 [00:23<00:00, 10.67s/it]^MLoading checkpoint shards: 100%|██████████| 2/2 [00:23<00:00, 11.67s/it]\r\nTraceback (most recent call last):\r\n  File \"/home/carlos.limasantos/LLaVA/llava/train/train_mem.py\", line 14, in <module>\r\n    train()\r\n  File \"/home/carlos.limasantos/LLaVA/llava/train/train.py\", line 825, in train\r\n    model = get_peft_model(model, lora_config)\r\n  File \"/home/carlos.limasantos/anaconda3/envs/llava/lib/python3.10/site-packages/peft/mapping.py\", line 98, in get_peft_model\r\n    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model, peft_config, adapter_name=adapter_name)\r\n  File \"/home/carlos.limasantos/anaconda3/envs/llava/lib/python3.10/site-packages/peft/peft_model.py\", line 893, in __init__\r\n    super().__init__(model, peft_config, adapter_name)\r\n  File \"/home/carlos.limasantos/anaconda3/envs/llava/lib/python3.10/site-packages/peft/peft_model.py\", line 112, in __init__\r\n    self.base_model = PEFT_TYPE_TO_MODEL_MAPPING[peft_config.peft_type](\r\n  File \"/home/carlos.limasantos/anaconda3/envs/llava/lib/python3.10/site-packages/peft/tuners/lora.py\", line 180, in __init__\r\n    self.add_adapter(adapter_name, self.peft_config[adapter_name])\r\n  File \"/home/carlos.limasantos/anaconda3/envs/llava/lib/python3.10/site-packages/peft/tuners/lora.py\", line 194, in add_adapter\r\n    self._find_and_replace(adapter_name)\r\n  File \"/home/carlos.limasantos/anaconda3/envs/llava/lib/python3.10/site-packages/peft/tuners/lora.py\", line 352, in _find_and_replace\r\n    new_module = self._create_new_module(lora_config, adapter_name, target)\r\n  File \"/home/carlos.limasantos/anaconda3/envs/llava/lib/python3.10/site-packages/peft/tuners/lora.py\", line 305, in _create_new_module\r\n    raise ValueError(\r\nValueError: Target module LlamaDecoderLayer(\r\n  (self_attn): LlamaAttention(\r\n    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\r\n    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\r\n    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\r\n    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\r\n    (rotary_emb): LlamaRotaryEmbedding()\r\n  )\r\n  (mlp): LlamaMLP(\r\n    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\r\n    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\r\n    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\r\n    (act_fn): SiLUActivation()\r\n  )\r\n  (input_layernorm): LlamaRMSNorm()\r\n  (post_attention_layernorm): LlamaRMSNorm()\r\n) is not supported. Currently, only `torch.nn.Linear` and `Conv1D` are supported.\n</Comment>\n<Comment by dashascience at 2024-02-01T20:00:08Z>\n> Not working for me. Was your error exactly this one? (I realized that checkpoint shards didn't load)\r\n> \r\n> Loading checkpoint shards: 0%| | 0/2 [00:00<?, ?it/s]^MLoading checkpoint shards: 50%|█████ | 1/2 [00:17<00:17, 17.33s/it]^MLoading checkpoint shards: 100%|██████████| 2/2 [00:23<00:00, 10.67s/it]^MLoading checkpoint shards: 100%|██████████| 2/2 [00:23<00:00, 11.67s/it] Traceback (most recent call last): File \"/home/carlos.limasantos/LLaVA/llava/train/train_mem.py\", line 14, in train() File \"/home/carlos.limasantos/LLaVA/llava/train/train.py\", line 825, in train model = get_peft_model(model, lora_config) File \"/home/carlos.limasantos/anaconda3/envs/llava/lib/python3.10/site-packages/peft/mapping.py\", line 98, in get_peft_model return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model, peft_config, adapter_name=adapter_name) File \"/home/carlos.limasantos/anaconda3/envs/llava/lib/python3.10/site-packages/peft/peft_model.py\", line 893, in **init** super().**init**(model, peft_config, adapter_name) File \"/home/carlos.limasantos/anaconda3/envs/llava/lib/python3.10/site-packages/peft/peft_model.py\", line 112, in **init** self.base_model = PEFT_TYPE_TO_MODEL_MAPPING[peft_config.peft_type]( File \"/home/carlos.limasantos/anaconda3/envs/llava/lib/python3.10/site-packages/peft/tuners/lora.py\", line 180, in **init** self.add_adapter(adapter_name, self.peft_config[adapter_name]) File \"/home/carlos.limasantos/anaconda3/envs/llava/lib/python3.10/site-packages/peft/tuners/lora.py\", line 194, in add_adapter self._find_and_replace(adapter_name) File \"/home/carlos.limasantos/anaconda3/envs/llava/lib/python3.10/site-packages/peft/tuners/lora.py\", line 352, in _find_and_replace new_module = self._create_new_module(lora_config, adapter_name, target) File \"/home/carlos.limasantos/anaconda3/envs/llava/lib/python3.10/site-packages/peft/tuners/lora.py\", line 305, in _create_new_module raise ValueError( ValueError: Target module LlamaDecoderLayer( (self_attn): LlamaAttention( (q_proj): Linear(in_features=4096, out_features=4096, bias=False) (k_proj): Linear(in_features=4096, out_features=4096, bias=False) (v_proj): Linear(in_features=4096, out_features=4096, bias=False) (o_proj): Linear(in_features=4096, out_features=4096, bias=False) (rotary_emb): LlamaRotaryEmbedding() ) (mlp): LlamaMLP( (gate_proj): Linear(in_features=4096, out_features=11008, bias=False) (up_proj): Linear(in_features=4096, out_features=11008, bias=False) (down_proj): Linear(in_features=11008, out_features=4096, bias=False) (act_fn): SiLUActivation() ) (input_layernorm): LlamaRMSNorm() (post_attention_layernorm): LlamaRMSNorm() ) is not supported. Currently, only `torch.nn.Linear` and `Conv1D` are supported.\r\n\r\nI have the same issue. Did you resolve it? Thanks in advance for answer.\n</Comment>\n<Comment by phellonchen at 2024-02-18T03:54:50Z>\nsame issue\n</Comment>\n<Comment by yfthu at 2024-05-20T08:26:59Z>\nsame issue\n</Comment>\n<Comment by CA-TT-AC at 2025-02-17T11:56:41Z>\nsame issue\n</Comment>\n<Comment by lsnls at 2025-04-29T00:55:26Z>\nsame issue.\n</Comment>\n<Comment by tianjiedai at 2025-06-10T07:17:17Z>\n> i add some nn.linear in LlavaLlamaForCausalLM, it leads this error. so i modify this func in train.py `multimodal_keywords = ['mm_projector', 'vision_tower', 'vision_resampler', 'my_linear_layer'],` my problem is sloved.\n> \n> ```\n> def find_all_linear_names(model):\n>     cls = torch.nn.Linear\n>     lora_module_names = set()\n>     multimodal_keywords = ['mm_projector', 'vision_tower', 'vision_resampler', 'my_linear_layer']\n>     for name, module in model.named_modules():\n>         if any(mm_keyword in name for mm_keyword in multimodal_keywords):\n>             continue\n>         if isinstance(module, cls):\n>             names = name.split('.')\n>             lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n> \n>     if 'lm_head' in lora_module_names: # needed for 16-bit\n>         lora_module_names.remove('lm_head')\n>     return list(lora_module_names)\n> ```\n\nBut this method has a problem, the safetensor only contains attention-related LoRA weights without your self-defined weights.\n</Comment>\n<Comment by vlasu19 at 2025-06-19T02:50:03Z>\nMaybe you can try to output the result of `find_all_linear_names`.\n```Found LoRA target modules: {'2', 'o_proj', 'q_proj', 'v_proj', '0', 'gate_proj', 'k_proj', 'down_proj', 'up_proj'}```\nMy `output` result contains numbers like ‘0’, ‘2’, which suggests that the recursive/traversal method adds some non-Linear layer names as well, which causes LoRA to try to insert Adapter on an unsupported module, and thus report an error.\nI replaced my `find_all_linear_names` function, it works for me!\n```\ndef find_all_linear_names(model):\n    linear_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, torch.nn.Linear):\n            #  Keep only the name of the last layer (e.g. q_proj), filtering out plain numbers\n            last_name = name.split('.')[-1]\n            if not last_name.isdigit():\n                linear_names.add(last_name)\n    return list(linear_names)\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 907,
    "state": "open",
    "created_by": "aarora79",
    "created_at": "2023-12-09T21:36:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/907</URL>\n\n<TITLE>[Usage] Getting an error while running inference with LLava-1.5 13b on SageMaker</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nDeployed the model on SageMaker as follows:\r\n```\r\nfrom sagemaker.huggingface.model import HuggingFaceModel\r\nfrom sagemaker import get_execution_role\r\n\r\nrole = get_execution_role()\r\nprint(role)\r\n\r\nhub = {\r\n  'HF_TASK':'question-answering'\r\n}\r\n# create Hugging Face Model Class\r\nhuggingface_model = HuggingFaceModel(\r\n   model_data=s3_model_uri,      # path to your model and script\r\n   role=role,                    # iam role with permissions to create an Endpoint\r\n   transformers_version=\"4.28.1\",  # transformers version used\r\n   pytorch_version=\"2.0.0\",       # pytorch version used\r\n   py_version='py310',            # python version used\r\n   model_server_workers=1,\r\n   env=hub\r\n)\r\n\r\n# deploy the endpoint endpoint\r\npredictor = huggingface_model.deploy(\r\n    initial_instance_count=1,\r\n    instance_type=\"ml.g5.xlarge\",\r\n    # container_startup_health_check_timeout=600, # increase timeout for large models\r\n    # model_data_download_timeout=600, # increase timeout for large models\r\n)\r\n```\r\n\r\nGetting the following error on inference\r\n\r\n```\r\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{\r\n  \"code\": 400,\r\n  \"type\": \"InternalServerException\",\r\n  \"message\": \"GET was unable to find an engine to execute this computation\"\r\n}\r\n```\r\n\r\nLLava-1.5 7b works perfectly fine with the same code.\r\nStackOverflow suggests incompatibility between CUDA and PyTorch versions, not sure if a different version than used above needs to be used.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 906,
    "state": "open",
    "created_by": "danilo139",
    "created_at": "2023-12-09T17:18:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/906</URL>\n\n<TITLE>[Question] its possible to run it on LM Studio ??</TITLE>\n\n<BODY>### Question\n\nHi guys, wanna know if its possible to run it on LM Studio \r\nThanks in advance !</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 905,
    "state": "open",
    "created_by": "Road2Redemption",
    "created_at": "2023-12-09T12:42:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/905</URL>\n\n<TITLE>How to continually train a checkpoint in multi-stage training?</TITLE>\n\n<BODY>### Question\n\nHi, thank you for your work. I have some problems when lora fine-tuning on my own custom dataset using LLaVA-v1.5.\r\n\r\nSpecifically, I have 2 datasets and I want to do 2-stage training. When I finished the 1st stage training and got the checkpoint 1,\r\nwhat should I do exactly to finish the 2nd stage training? \r\n\r\nFrom what I see is that I merge checkpoint 1 with the base model LLaVA-v1.5 and continually fine-tuning on the second dataset and get checkpoint 2, and then I merge checkpoint 2 with either LLaVA-v1.5 or the merged model (LLaVA-v1.5 + checkpoint 1).\r\n\r\nOr is there a way to continually train a checkpoint in multi-stage training without merging checkpoints? \r\n\r\nIt would be really helpful if my questions were answered!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 904,
    "state": "open",
    "created_by": "wnma3mz",
    "created_at": "2023-12-09T11:27:27Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/904</URL>\n\n<TITLE>Pretrain dataset has a error image</TITLE>\n\n<BODY>```json  \r\n{\r\n    \"id\": \"001365521\",\r\n    \"image\": \"00136/001365521.jpg\",\r\n    \"conversations\": [\r\n      {\r\n        \"from\": \"human\",\r\n        \"value\": \"Give a brief description of the image.\\n<image>\"\r\n      },\r\n      {\r\n        \"from\": \"gpt\",\r\n        \"value\": \"a black bmw m140i sports hatch from a dealer's garage\"\r\n      }\r\n    ]\r\n  }\r\n```\r\nThe corresponding picture is\r\n![001365521](https://github.com/haotian-liu/LLaVA/assets/23001152/e4883ae5-e081-46b3-8905-5e85782da234)\r\n\r\nI filter according to the size of the image, is there a better way to weed out these invalid images.\r\n\r\nThank you for your work and look forward to your suggestions.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 903,
    "state": "open",
    "created_by": "sr5434",
    "created_at": "2023-12-08T20:32:29Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/903</URL>\n\n<TITLE>System Requirements for Training 70b with LoRA</TITLE>\n\n<BODY>### Question\n\nWould it be possible to train llama 2 70b with LoRA on the LLaVA dataset on a single H100? If so, roughly how many hours would it take?</BODY>\n\n<COMMENTS>\n<Comment by Krisdddd at 2023-12-14T06:59:29Z>\nas I know, loading a 13b model needs approximately 30GB VRAM, 70b may need ~150G... \r\nfortunately multi cards can help\n</Comment>\n<Comment by sr5434 at 2023-12-14T21:30:53Z>\nOk, thanks. What about with 4 bit quantization?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 902,
    "state": "open",
    "created_by": "Zeqiang-Lai",
    "created_at": "2023-12-08T12:28:19Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/902</URL>\n\n<TITLE>what is the purpose of delay_load for vision tower?</TITLE>\n\n<BODY>### Describe the issue\n\nhttps://github.com/haotian-liu/LLaVA/blob/7775b12d6b20cd69089be7a18ea02615a59621cd/llava/model/llava_arch.py#L33\r\n\r\nBtw, I am also confused about why we can add token when loading pretrained models?\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/7775b12d6b20cd69089be7a18ea02615a59621cd/llava/model/builder.py#L131\r\n\r\nWill random embedding affect the model?</BODY>\n\n<COMMENTS>\n<Comment by irexyc at 2024-04-10T03:53:52Z>\nI'm confused too. Obviously, there are some vision_tower weights in llava-v1.6. \r\n\r\n<img width=\"1031\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/16019484/8cc0328c-4da7-446a-aa48-a72baa1a3791\">\r\n\r\n\r\n~If loading llm first with `LlavaLlamaForCausalLM.from_pretrained` and then load vision_tower, the vision_tower weights in llava-v1.6 will be overwritten by `openai/clip-vit-large-patch14-336`~\r\n\r\nThe above is wrong, when `unfreeze_mm_vision_tower` is True,  the vision_tower will not delay load.\n</Comment>\n<Comment by xylcbd at 2024-04-17T02:18:40Z>\nme too.\n</Comment>\n<Comment by irexyc at 2024-04-22T07:42:12Z>\n@xylcbd \r\nI looked the code again and find there is no problems in the code.\r\n\r\nFor llava-1.5, the vision_tower weight is same as openai, so it is ok whether delay load or not. \r\n\r\nFor llava-1.6, there is a parameter called `unfreeze_mm_vision_tower` in config.json and when the parameter is True, the vision_tower model will not delay load.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 901,
    "state": "open",
    "created_by": "1359347500cwc",
    "created_at": "2023-12-08T08:58:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/901</URL>\n\n<TITLE>[Question] When i do the pretrain stage, how can i resume from the save checkpoint</TITLE>\n\n<BODY>### Question\n\nwhen i resume from the save checkpoint i got this error:\r\n\"ValueError: No valid checkpoint found in output directory\"\r\nhere is my save checkpoints in output directory:\r\n<img width=\"297\" alt=\"截屏2023-12-08 16 58 28\" src=\"https://github.com/haotian-liu/LLaVA/assets/57097883/51b32afd-29cc-42f1-ada8-d3faf9f41adf\"></BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 900,
    "state": "open",
    "created_by": "gapjialin",
    "created_at": "2023-12-07T03:02:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/900</URL>\n\n<TITLE>[Question] OSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /home/LLaVA/llava-v1.5-13b-lora.</TITLE>\n\n<BODY>### Question\n\n<img width=\"1143\" alt=\"屏幕截图 2023-12-07 110038\" src=\"https://github.com/haotian-liu/LLaVA/assets/107861107/110a73f8-2a8d-4eb7-88c6-eb6d57c1a76c\">\r\nWhen I use scripts/v1_ 5/finetune_ Lora.sh makes minor adjustments and displays: OSError: Error no file named pytorch_ Model. bin, tf_ Model. h5, model. ckpt. index or flex_ Model. msgpack found in directory/home/LaVA/lava v1.5-13b lora</BODY>\n\n<COMMENTS>\n<Comment by Road2Redemption at 2023-12-09T12:27:47Z>\nI have met the same problem, I want to do multi-stage training, namely let the parameter --model_name_or_path a checkpoint like this. I don't know whether my method is correct.\n</Comment>\n<Comment by yuezhao238 at 2023-12-27T20:42:23Z>\ni met the same problem when using cli script. Solved by adding --model-base\n</Comment>\n<Comment by Leo-Lsc at 2024-09-03T11:31:33Z>\n> i met the same problem when using cli script. Solved by adding --model-base我在使用 cli 脚本时遇到了同样的问题。通过添加 --model-base 解决\r\n\r\nHow do you solve it? I don't quite understand what is \"通过添加 --model-base 解决\"?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 899,
    "state": "closed",
    "created_by": "foreverpiano",
    "created_at": "2023-12-06T16:26:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/899</URL>\n\n<TITLE>[Question] Finetune with error report</TITLE>\n\n<BODY>### Question\n\n(ll) root@platform:/workspace/dhl/LLaVA# bash /workspace/dhl/LLaVA/scripts/v1_5/finetune.sh\r\n[2023-12-06 16:23:00,608] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-12-06 16:23:07,002] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\n[2023-12-06 16:23:07,004] [INFO] [runner.py:570:main] cmd = /usr/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path /workspace/dhl/model/llava-v1.5-13b --version v1 --data_path /workspace/autoglm/dataset/img/mind2web/subtree/train/all.json --image_folder /workspace/autoglm/dataset/img/img_mark --vision_tower openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb\r\n[2023-12-06 16:23:08,719] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-12-06 16:23:14,892] [INFO] [launch.py:138:main] 0 NCCL_IB_PCI_RELAXED_ORDERING=1\r\n[2023-12-06 16:23:14,892] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.19.3\r\n[2023-12-06 16:23:14,893] [INFO] [launch.py:138:main] 0 NCCL_SOCKET_IFNAME=eth0\r\n[2023-12-06 16:23:14,893] [INFO] [launch.py:138:main] 0 NCCL_NVLS_ENABLE=1\r\n[2023-12-06 16:23:14,893] [INFO] [launch.py:138:main] 0 NCCL_DEBUG=VERSION\r\n[2023-12-06 16:23:14,893] [INFO] [launch.py:138:main] 0 NCCL_IB_GID_INDEX=97\r\n[2023-12-06 16:23:14,893] [INFO] [launch.py:138:main] 0 NCCL_IB_TIMEOUT=23\r\n[2023-12-06 16:23:14,893] [INFO] [launch.py:138:main] 0 NCCL_IB_DISABLE=0\r\n[2023-12-06 16:23:14,893] [INFO] [launch.py:138:main] 0 NCCL_IB_RETRY_CNT=7\r\n[2023-12-06 16:23:14,893] [INFO] [launch.py:138:main] 0 NCCL_CROSS_NIC=0\r\n[2023-12-06 16:23:14,893] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\r\n[2023-12-06 16:23:14,893] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0\r\n[2023-12-06 16:23:14,893] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\r\n[2023-12-06 16:23:14,893] [INFO] [launch.py:163:main] dist_world_size=8\r\n[2023-12-06 16:23:14,893] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1099, in _get_module\r\n    return importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 32, in <module>\r\n    from ...modeling_utils import PreTrainedModel\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 38, in <module>\r\n    from .deepspeed import deepspeed_config, is_deepspeed_zero3_enabled\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py\", line 37, in <module>\r\n    from accelerate.utils.deepspeed import HfDeepSpeedConfig as DeepSpeedConfig\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/__init__.py\", line 3, in <module>\r\n    from .accelerator import Accelerator\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 35, in <module>\r\n    from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/checkpointing.py\", line 24, in <module>\r\n    from .utils import (\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/__init__.py\", line 133, in <module>\r\n    from .launch import (\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/launch.py\", line 33, in <module>\r\n    from ..utils.other import is_port_in_use, merge_dicts\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/other.py\", line 25, in <module>\r\n    from .transformer_engine import convert_model\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/transformer_engine.py\", line 21, in <module>\r\n    import transformer_engine.pytorch as te\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/__init__.py\", line 6, in <module>\r\n    from .module import LayerNormLinear\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/__init__.py\", line 6, in <module>\r\n    from .layernorm_linear import LayerNormLinear\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/layernorm_linear.py\", line 15, in <module>\r\n    from .. import cpp_extensions as tex\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/cpp_extensions/__init__.py\", line 6, in <module>\r\n    from transformer_engine_extensions import *\r\nImportError: /usr/local/lib/python3.10/dist-packages/transformer_engine_extensions.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c10ltERKNS_6SymIntEi\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/workspace/dhl/LLaVA/llava/train/train_mem.py\", line 6, in <module>\r\n    from llava.train.llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn\r\n  File \"/workspace/dhl/LLaVA/llava/__init__.py\", line 1, in <module>\r\n    from .model import LlavaLlamaForCausalLM\r\n  File \"/workspace/dhl/LLaVA/llava/model/__init__.py\", line 1, in <module>\r\n    from .language_model.llava_llama import LlavaLlamaForCausalLM, LlavaConfig\r\n  File \"/workspace/dhl/LLaVA/llava/model/language_model/llava_llama.py\", line 21, in <module>\r\n    from transformers import AutoConfig, AutoModelForCausalLM, \\\r\n  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1090, in __getattr__\r\n    value = getattr(module, name)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1089, in __getattr__\r\n    module = self._get_module(self._class_to_module[name])\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1101, in _get_module\r\n    raise RuntimeError(\r\nRuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\r\n/usr/local/lib/python3.10/dist-packages/transformer_engine_extensions.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c10ltERKNS_6SymIntEi\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1099, in _get_module\r\n    return importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 32, in <module>\r\n    from ...modeling_utils import PreTrainedModel\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 38, in <module>\r\n    from .deepspeed import deepspeed_config, is_deepspeed_zero3_enabled\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py\", line 37, in <module>\r\n    from accelerate.utils.deepspeed import HfDeepSpeedConfig as DeepSpeedConfig\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/__init__.py\", line 3, in <module>\r\n    from .accelerator import Accelerator\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 35, in <module>\r\n    from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/checkpointing.py\", line 24, in <module>\r\n    from .utils import (\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/__init__.py\", line 133, in <module>\r\n    from .launch import (\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/launch.py\", line 33, in <module>\r\n    from ..utils.other import is_port_in_use, merge_dicts\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/other.py\", line 25, in <module>\r\n    from .transformer_engine import convert_model\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/transformer_engine.py\", line 21, in <module>\r\n    import transformer_engine.pytorch as te\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/__init__.py\", line 6, in <module>\r\n    from .module import LayerNormLinear\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/__init__.py\", line 6, in <module>\r\n    from .layernorm_linear import LayerNormLinear\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/layernorm_linear.py\", line 15, in <module>\r\n    from .. import cpp_extensions as tex\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/cpp_extensions/__init__.py\", line 6, in <module>\r\n    from transformer_engine_extensions import *\r\nImportError: /usr/local/lib/python3.10/dist-packages/transformer_engine_extensions.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c10ltERKNS_6SymIntEi\r\n\r\nThe above exception was the direct cause of the following exception:</BODY>\n\n<COMMENTS>\n<Comment by 459737087 at 2024-01-12T07:15:37Z>\ndid you solve it ?\n</Comment>\n<Comment by bulat15g at 2024-02-02T11:58:06Z>\nfaced same error. @foreverpiano Have you solved it?\n</Comment>\n<Comment by bulat15g at 2024-02-02T12:26:35Z>\npip uninstall transformer-engine \r\nworked for me))\n</Comment>\n<Comment by haotian-liu at 2024-02-03T15:36:52Z>\n> pip uninstall transformer-engine worked for me))\r\n\r\nIf you install from a clean env, this package shall not be installed, and sometimes there are weird issues from transformers/accelerate side..\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 898,
    "state": "open",
    "created_by": "Felix0805",
    "created_at": "2023-12-06T06:33:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/898</URL>\n\n<TITLE>Questions about training precision</TITLE>\n\n<BODY>### Question\r\n\r\nWhy is the precision of llm fp32 when using script zero2 but the precision of llm bf16 when using script zero3?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 896,
    "state": "closed",
    "created_by": "xuweidongkobe",
    "created_at": "2023-12-05T08:53:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/896</URL>\n\n<TITLE>[Usage] RuntimeError: probability tensor contains either `inf`, `nan` or element < 0</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nWhen I run the run_llava.py，encountered this error.\r\nI have tried other solutions mentioned by the issue, including using single card inference, fp32, commentet \"do_sample\", but still can not fix.\r\n1、I am using the Nvidia v100 gpu.\r\n2、When I comment \"do_sample\", code can not occur error, but the outputs is None. Its very strange.\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\nimport os \r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\r\nimport argparse\r\nimport torch\r\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\r\nfrom llava.conversation import conv_templates, SeparatorStyle\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.utils import disable_torch_init\r\nfrom llava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\r\n\r\nfrom PIL import Image\r\n\r\nimport requests\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\n\r\n\r\ndef load_image(image_file):\r\n    if image_file.startswith('http') or image_file.startswith('https'):\r\n        response = requests.get(image_file)\r\n        image = Image.open(BytesIO(response.content)).convert('RGB')\r\n    else:\r\n        image = Image.open(image_file).convert('RGB')\r\n    return image\r\n\r\n\r\ndef eval_model(args):\r\n    # Model\r\n    disable_torch_init()\r\n\r\n    model_name = get_model_name_from_path(args.model_path)\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name)\r\n\r\n    qs = args.query\r\n    if model.config.mm_use_im_start_end:\r\n        qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + qs\r\n    else:\r\n        qs = DEFAULT_IMAGE_TOKEN + '\\n' + qs\r\n\r\n    if 'llama-2' in model_name.lower():\r\n        conv_mode = \"llava_llama_2\"\r\n    elif \"v1\" in model_name.lower():\r\n        conv_mode = \"llava_v1\"\r\n    elif \"mpt\" in model_name.lower():\r\n        conv_mode = \"mpt\"\r\n    else:\r\n        conv_mode = \"llava_v0\"\r\n\r\n    if args.conv_mode is not None and conv_mode != args.conv_mode:\r\n        print('[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}'.format(conv_mode, args.conv_mode, args.conv_mode))\r\n    else:\r\n        args.conv_mode = conv_mode\r\n\r\n    print(\"conv_mode is :\", conv_mode)\r\n    conv = conv_templates[args.conv_mode].copy()\r\n    conv.append_message(conv.roles[0], qs)\r\n    conv.append_message(conv.roles[1], None)\r\n    prompt = conv.get_prompt()\r\n\r\n    image = load_image(args.image_file)\r\n    image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].half().cuda()\r\n    # image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].cuda()\r\n    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\r\n\r\n    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\r\n    keywords = [stop_str]\r\n    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\r\n    \r\n    # model.float()\r\n    with torch.inference_mode():\r\n        output_ids = model.generate(\r\n            input_ids,\r\n            images=image_tensor,\r\n            do_sample=True,\r\n            temperature=0.2,\r\n            max_new_tokens=1024,\r\n            use_cache=True,\r\n            stopping_criteria=[stopping_criteria])\r\n\r\n    input_token_len = input_ids.shape[1]\r\n    n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\r\n    if n_diff_input_output > 0:\r\n        print(f'[Warning] {n_diff_input_output} output_ids are not the same as the input_ids')\r\n    outputs = tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]\r\n    outputs = outputs.strip()\r\n    if outputs.endswith(stop_str):\r\n        outputs = outputs[:-len(stop_str)]\r\n    outputs = outputs.strip()\r\n    print(outputs)\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--model-path\", type=str, default=\"/mnt/data1/home/xuwd/code/LLaVA-1.1.3/checkpoints/IVA/1201_llava-v1.5-7b-lora/\")\r\n    parser.add_argument(\"--model-base\", type=str, default=\"/mnt/data1/home/xuwd/checkpoints/LLaVA/llava-v1.5-7b/\")\r\n    parser.add_argument(\"--image-file\", type=str, default=\"/mnt/data1/home/xuwd/datasets/IVA/test_data/aqmjc/helm_000000.jpg\")\r\n    parser.add_argument(\"--query\", type=str, default=\"Please describe this image.\")\r\n    parser.add_argument(\"--conv-mode\", type=str, default=None)\r\n    args = parser.parse_args()\r\n\r\n    eval_model(args)\r\n```\r\n\r\nScreenshots:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/25764883/9cd3303f-36ba-46ef-af14-da19f4cb93ac)</BODY>\n\n<COMMENTS>\n<Comment by xuweidongkobe at 2023-12-05T08:55:06Z>\n@haotian-liu can you help me , thank you!\n</Comment>\n<Comment by xuweidongkobe at 2023-12-12T08:00:40Z>\nfix the probelm with 1.1.1tag code.\n</Comment>\n<Comment by Zhou-Hangyu at 2023-12-12T15:57:18Z>\n> fix the probelm with 1.1.1tag code.\r\n\r\nDo you mean you switch back to llava1.1.1 and solved the issue?\n</Comment>\n<Comment by xuweidongkobe at 2023-12-26T06:01:26Z>\n> > fix the probelm with 1.1.1tag code.\r\n> \r\n> Do you mean you switch back to llava1.1.1 and solved the issue?\r\n\r\nyes\n</Comment>\n<Comment by 459737087 at 2024-01-11T10:32:04Z>\nbut why @xuweidongkobe\n</Comment>\n<Comment by Saurabhbaghel at 2024-03-04T13:40:25Z>\n> fix the probelm with 1.1.1tag code.\r\n\r\n\r\n@xuweidongkobe , did you make the change in `model-base` ? \r\nThanks\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 893,
    "state": "closed",
    "created_by": "EDDChang",
    "created_at": "2023-12-04T07:23:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/893</URL>\n\n<TITLE>[Question] What prepare_inputs_labels_for_multimodal in llava_arch.py does with labels?</TITLE>\n\n<BODY>### Question\n\nIn my understanding, forward function may return loss if given labels., which is probability that llm generate the labels.\r\nHowever, in the class LlavaLlamaForCausalLM, you override forward function with some preprocess of inputs by prepare_inputs_labels_for_multimodal and also process the labels.\r\nI would like to know how the function \"prepare_inputs_labels_for_multimodal\" do to labels? \r\nIn more specific, in line:144 and 145\r\ninput_ids = [cur_input_ids[cur_attention_mask] for cur_input_ids, cur_attention_mask in zip(input_ids, attention_mask)]\r\nlabels = [cur_labels[cur_attention_mask] for cur_labels, cur_attention_mask in zip(labels, attention_mask)]\r\nwhy assuming input_ids and labels has same size?</BODY>\n\n<COMMENTS>\n<Comment by EDDChang at 2023-12-05T08:22:16Z>\nproblem solved.\n</Comment>\n<Comment by hhy-huang at 2024-06-02T09:05:31Z>\n> problem solved.\r\n\r\nHi, thank you for your question. Now I have the same issue, could you explain how you figure it out?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 892,
    "state": "open",
    "created_by": "xiaobaiha",
    "created_at": "2023-12-04T07:20:54Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/892</URL>\n\n<TITLE>[Usage] Caught Unknown Error CUDA error: device-side assert triggered</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:  llava model worker always run for a period of time to report an error, must restart to recover. The content of the error is as follows, how to deal with it?\r\n\r\nCommand:\r\n```\r\npython3.10 -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000--port 40000 --worker http://localhost:40000 --model-path ./llava-v1.5-13b\r\n```\r\n\r\nLog: \r\n```\r\n2023-12-01 08:55:00 | INFO | stdout | INFO:     127.0.0.1:59186 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\n2023-12-01 08:55:00 | INFO | stdout | Caught Unknown Error CUDA error: device-side assert triggered\r\n2023-12-01 08:55:00 | INFO | stdout | Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n```</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 890,
    "state": "open",
    "created_by": "ThugJudy",
    "created_at": "2023-12-03T21:40:28Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/890</URL>\n\n<TITLE>[Question]</TITLE>\n\n<BODY>### Question\n\n## How do I use the finetuned Model in CLI interactive mode?\r\n\r\nI have finetuned the model using the following Script: \r\n```\r\n#!/bin/bash\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path liuhaotian/llava-v1.5-13b \\\r\n    --version v1 \\\r\n    --data_path /projects/bbpr/psg4/llm/bargraph_data.json \\\r\n    --image_folder /dataset/imgs \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-13b-task-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nit fine-tuned model is generated successfully, but, when I try to use it using the following command to open an interactive mode with my model:\r\n\r\n```\r\npython -m llava.serve.cli --model-path ./checkpoints/llava-v1.5-13b-task-lora --image-file /projects/bbpr/psg4/llm/LLaVA/dataset/imgs/000000.png --load-4bit --model-base liuhaotian/llava-v1.5-13b\r\n```\r\n\r\nI get the following error:\r\n\r\n```\r\n[2023-12-03 15:33:18,196] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\ntokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████| 749/749 [00:00<00:00, 4.18MB/s]\r\ntokenizer.model: 100%|████████████████████████████████████████████████████████████████████████████████████████| 500k/500k [00:00<00:00, 34.1MB/s]\r\nspecial_tokens_map.json: 100%|██████████████████████████████████████████████████████████████████████████████████| 438/438 [00:00<00:00, 2.90MB/s]\r\nLoading LLaVA from base model...\r\npytorch_model.bin.index.json: 100%|██████████████████████████████████████████████████████████████████████████| 33.7k/33.7k [00:00<00:00, 248MB/s]\r\npytorch_model-00001-of-00003.bin: 100%|██████████████████████████████████████████████████████████████████████| 9.95G/9.95G [00:45<00:00, 218MB/s]\r\npytorch_model-00002-of-00003.bin: 100%|█████████████████████████████████████████████████████████████████████| 9.90G/9.90G [02:36<00:00, 63.2MB/s]\r\npytorch_model-00003-of-00003.bin: 100%|██████████████████████████████████████████████████████████████████████| 6.24G/6.24G [00:27<00:00, 228MB/s]\r\nDownloading shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 3/3 [03:49<00:00, 76.67s/it]\r\nconfig.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 4.76k/4.76k [00:00<00:00, 27.4MB/s]\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:06<00:00,  2.30s/it]\r\ngeneration_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████| 154/154 [00:00<00:00, 954kB/s]\r\nLoading additional LLaVA weights...\r\nTraceback (most recent call last):\r\n  File \"/u/psg4/.conda/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/u/psg4/.conda/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/projects/bbpr/psg4/llm/LLaVA/llava/serve/cli.py\", line 124, in <module>\r\n    main(args)\r\n  File \"/projects/bbpr/psg4/llm/LLaVA/llava/serve/cli.py\", line 32, in main\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n  File \"/projects/bbpr/psg4/llm/LLaVA/llava/model/builder.py\", line 75, in load_pretrained_model\r\n    model.load_state_dict(non_lora_trainables, strict=False)\r\n  File \"/u/psg4/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2041, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for LlavaLlamaForCausalLM:\r\n        size mismatch for model.mm_projector.0.weight: copying a param with shape torch.Size([5120, 1024]) from checkpoint, the shape in current model is torch.Size([2621440, 1]).\r\n        size mismatch for model.mm_projector.2.weight: copying a param with shape torch.Size([5120, 5120]) from checkpoint, the shape in current model is torch.Size([13107200, 1]).\r\n```\r\n\r\nNot sure if I'm doing something wrong. Kindly guide me.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-12-07T19:10:39Z>\nHi, `--load-4bit` is not currently compatible with LoRA. You can either remove this option, or first [create merged checkpoint](https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md#create-merged-checkpoints) and then load it with 4-bit.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 889,
    "state": "open",
    "created_by": "hitsz-zxw",
    "created_at": "2023-12-03T14:04:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/889</URL>\n\n<TITLE>How to fine-tuning with pure text?</TITLE>\n\n<BODY>### Question\n\nHello, thank you very much for providing the fine-tuning code. I would like to know how to use the dataset I built myself to perform pure text fine-tuning?</BODY>\n\n<COMMENTS>\n<Comment by hai-qi at 2023-12-15T05:30:38Z>\nHello, have you solved this problem? I also want to finetune the  text.\n</Comment>\n<Comment by NielsRogge at 2024-01-03T21:13:43Z>\nYou can set the `--is_multimodal` flag to `False`, see https://github.com/haotian-liu/LLaVA/blob/9a26bd1435b4ac42c282757f2c16d34226575e96/llava/train/train.py#L68\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 888,
    "state": "open",
    "created_by": "erjiaxiao",
    "created_at": "2023-12-03T02:20:30Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/888</URL>\n\n<TITLE>[Question] In llava.serve.cli, is it possible to input a sample and its label, then get its gradient info?</TITLE>\n\n<BODY>### Question\n\nThanks for such a great work! Recently, I tried to input a sample and its label in llava.serve.cli to get gradient info. However, I can't \r\n find how to craft the correct formation of labels and input it into LLaVA.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 887,
    "state": "open",
    "created_by": "BinZhu-ece",
    "created_at": "2023-12-02T12:41:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/887</URL>\n\n<TITLE>model.model.embed_tokens.weight is [], None!!!????</TITLE>\n\n<BODY>### feature\n\nipdb> model.model.embed_tokens\r\nEmbedding(32000, 4096, padding_idx=0)\r\n\r\n\r\n\r\nipdb> model.model.embed_tokens.weight\r\nParameter containing:\r\ntensor([], device='cuda:0', dtype=torch.bfloat16, requires_grad=True)</BODY>\n\n<COMMENTS>\n<Comment by Pefect96 at 2024-08-27T05:44:14Z>\nHave you solved the problem? I have the same problem, the parameters are empty tensor.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 886,
    "state": "open",
    "created_by": "shilinyan99",
    "created_at": "2023-12-01T16:37:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/886</URL>\n\n<TITLE>RuntimeError: probability tensor contains either inf, nan or element < 0</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nI'm fine-tuning llava on a custom dataset but getting nan when doing inference.</BODY>\n\n<COMMENTS>\n<Comment by tosiyuki at 2023-12-03T11:14:13Z>\nDid you use with float16?\r\nIt is reported below that if it is bfloat16, it will not be nan.\r\nhttps://github.com/haotian-liu/LLaVA/issues/463\n</Comment>\n<Comment by ghazalsaheb at 2024-08-06T19:44:45Z>\nI used to have the same issue and I figured it was because I was using hugging face's \"llava-hf/llava-1.5-7b-hf\" as the base model. I switched the base to \"liuhaotian/llava-v1.5-7b\" and it resolved the NaN issue. Plus, the training performance got much better.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 885,
    "state": "open",
    "created_by": "Ezra-Yu",
    "created_at": "2023-12-01T12:38:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/885</URL>\n\n<TITLE>[Question] Which Text Encoder</TITLE>\n\n<BODY>### Question\n\nI am very curious. Are long-text pre-trained language models being used?\r\n<img width=\"840\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/18586273/53a47237-3444-47c1-838d-f6b3a33852db\">\r\n1. lmsys/vicuna-13b-v1.5\r\n2. lmsys/vicuna-13b-v1.5-16k</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 884,
    "state": "open",
    "created_by": "CrazyBrick",
    "created_at": "2023-12-01T12:36:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/884</URL>\n\n<TITLE>[Question] Can we obtain more fine-grained answers through fine-tuning</TITLE>\n\n<BODY>### Question\n\nCan fine-tuning be used on custom datasets to obtain more accurate outputs. \r\nFor example, for some less famous people, can the model recognize them and know their names. \r\nIf possible, which parameter need to be adjusted, how many samples are needed, and whether the Q&A data can only be in English.\r\nLooking forward to hearing from you.</BODY>\n\n<COMMENTS>\n<Comment by laserwave at 2023-12-04T06:44:24Z>\n1. Did you use a LLM model that is pretrained with Chinese data of large scale (and ensure the tokenizer is suitable for Chinese tokenization) ?\r\n2. Did you use full parameter fine-tuning or at least tune the first and last layer of the LLM?\n</Comment>\n<Comment by CrazyBrick at 2023-12-04T07:33:36Z>\n> 1. Did you use a LLM model that is pretrained with Chinese data of large scale (and ensure the tokenizer is suitable for Chinese tokenization) ?\r\n> 2. Did you use full parameter fine-tuning or at least tune the first and last layer of the LLM?\r\n\r\nHi @laserwave, thank you for your reply.\r\nq1:no\r\nq2:no\r\nNow I just tried the lora_finetune from readme considering my limited data situation, so for I did not use full parameter fine-tuning.\r\n> If you have a limited task-specific data, we recommend finetuning from LLaVA checkpoints with LoRA following this [script](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task_lora.sh)\r\n\r\n\r\nI just find the llava-1.5 have the ability to conduct Chinese Q&A, I thought its training data included some Chinese, but later someone reminded me of this issue. For convenience, I have converted my data into English.\r\n\r\nAnd about **tune the first and last layer of the LLM**, could you plz share more, I don't know how to adjust the code to make it run successfully.\n</Comment>\n<Comment by laserwave at 2023-12-04T07:48:10Z>\n> > And about tune the first and last layer of the LLM, could you plz share more, I don't know how to adjust the code to make it run successfully.\r\n\r\nThis may be the reason you cannot get Chinese response after fine-tuning with Chinese data.\r\n\r\nYou can check the exact parameter names of these layers and set `p.requires_grad = True`. Make sure it's not set to False again after this setting. Or you can also try full parameter fine-tuning.\n</Comment>\n<Comment by CrazyBrick at 2023-12-04T07:50:50Z>\n> You can check the exact parameter names of these layers and set `p.requires_grad = True`. Make sure it's not set to False again after this setting. Or you can also try full parameter fine-tuning.\r\n\r\nI may not have enough data to support full parameter fine-tuning.\n</Comment>\n<Comment by fisher75 at 2024-04-26T05:34:12Z>\n> > You can check the exact parameter names of these layers and set `p.requires_grad = True`. Make sure it's not set to False again after this setting. Or you can also try full parameter fine-tuning.\r\n> \r\n> I may not have enough data to support full parameter fine-tuning.\r\n\r\nhow much data is required for full parameter ft? I got 100k images wonder if it's enough.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 883,
    "state": "open",
    "created_by": "gnimyang",
    "created_at": "2023-12-01T10:04:12Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/883</URL>\n\n<TITLE>If I want to use serveral images to finetuning, how to do it? Is mm_use_im_start_end is relevant parameters?</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 882,
    "state": "closed",
    "created_by": "JewelShiny",
    "created_at": "2023-12-01T08:35:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/882</URL>\n\n<TITLE>[Usage] RuntimeError: CUDA error: device-side assert triggered using CLI Inference</TITLE>\n\n<BODY>Issue:\r\nError reported when using cli inference. Below is a detailed error report. CUDA version is 11.7. Is this related to CUDA version? This error will not be reported when using llavar，which is an older version of llava.\r\n\r\nLog: \r\n```\r\n(llava) zxy@super-AS-4124GS-TNR:~/codes/LLaVA$ python -m llava.eval.run_llava    --model-path /home/zxy/model/llava-v1.5-7b/    --image-file \"https://img-blog.csdnimg.cn/img_convert/7809ad718c317ec6b2cd6fbfbde1aa8d.jpeg\"   --query \"decribe this scene\"\r\n[2023-12-01 16:24:28,817] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.50s/it]\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [32,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [33,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [34,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [35,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [36,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [37,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [38,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [39,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [40,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [41,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [42,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [43,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [44,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [45,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [46,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [47,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [48,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [49,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [50,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [51,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [52,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [53,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [54,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [55,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [56,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [57,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [58,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [59,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [60,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [61,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [62,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [63,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [64,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [65,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [66,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [67,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [68,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [69,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [70,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [71,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [72,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [73,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [74,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [75,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [76,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [77,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [78,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [79,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [80,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [81,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [82,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [83,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [84,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [85,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [86,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [87,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [88,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [89,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [90,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [91,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [92,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [93,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [94,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [95,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [96,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [97,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [98,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [99,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [100,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [101,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [102,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [103,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [104,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [105,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [106,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [107,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [108,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [109,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [110,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [111,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [112,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [113,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [114,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [115,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [116,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [117,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [118,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [119,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [120,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [121,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [122,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [123,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [124,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [125,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [126,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [127,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [0,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [1,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [2,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [3,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [4,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [5,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [6,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [7,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [8,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [9,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [10,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [11,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [12,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [13,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [14,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [15,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [16,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [17,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [18,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [19,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [20,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [21,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [22,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [23,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [24,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [25,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [26,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [27,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [28,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [29,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [30,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [31,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\r\nTraceback (most recent call last):\r\n  File \"/home/zxy/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/zxy/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/zxy/codes/LLaVA/llava/eval/run_llava.py\", line 157, in <module>\r\n    eval_model(args)\r\n  File \"/home/zxy/codes/LLaVA/llava/eval/run_llava.py\", line 115, in eval_model\r\n    output_ids = model.generate(\r\n  File \"/home/zxy/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/zxy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1588, in generate\r\n    return self.sample(\r\n  File \"/home/zxy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2678, in sample\r\n    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\n```</BODY>\n\n<COMMENTS>\n<Comment by youzunzhi at 2023-12-10T06:49:53Z>\nHi, I have the same issue. May I ask how did you solve it?\n</Comment>\n<Comment by JewelShiny at 2023-12-11T02:31:56Z>\n> Hi, I have the same issue. May I ask how did you solve it?\r\n\r\nSpecify  only one GPU using CUDA_VISIABLE_DEVICES\n</Comment>\n<Comment by youzunzhi at 2023-12-11T05:36:06Z>\n> > Hi, I have the same issue. May I ask how did you solve it?\r\n> \r\n> Specify only one GPU using CUDA_VISIABLE_DEVICES\r\n\r\nThank you very much for your reply! But sadly it doesn't work for me 🥲\n</Comment>\n<Comment by 459737087 at 2024-01-11T06:39:50Z>\nHi ! Did you solve it\n</Comment>\n<Comment by JewelShiny at 2024-01-11T06:49:01Z>\n> Hi ! Did you solve it\r\n\r\nsee above, mine is solved since i was using multiple gpus to inferrence.\n</Comment>\n<Comment by 459737087 at 2024-01-11T07:27:47Z>\nThank you, I found another solution that your model_path must consist LLaVA-vicuna-7b\n</Comment>\n<Comment by Purshow at 2024-05-17T01:11:41Z>\n> Thank you, I found another solution that your model_path must consist LLaVA-vicuna-7b\r\n\r\nHi~ Could you please tell me how to solve it? I encountered the same problem\n</Comment>\n<Comment by 459737087 at 2024-05-20T03:40:52Z>\nchange your filename to LLaVA-vicuna-7b @purshow\n</Comment>\n<Comment by fazhdo at 2024-11-21T02:07:44Z>\n@2285443514 It works to me!! Thank you!!!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 881,
    "state": "open",
    "created_by": "ljjcoder",
    "created_at": "2023-12-01T03:22:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/881</URL>\n\n<TITLE>[Question] can we use zero3 in pretrain stage？</TITLE>\n\n<BODY>### Question\n\nThank you for your excellent work! I recently reproduced the pretrain stage of your llava1.5, but I encountered some problems. I directly use your default settings, but it cannot be loaded correctly due to insufficient CPU memory. If I change the --deepspeed parameter from zeros2.json to zeros3.json, it will run normally. Can I use zeros3.jason for pretrain? What is the difference between it and zeros2.json?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 880,
    "state": "open",
    "created_by": "Etelis",
    "created_at": "2023-11-30T10:59:27Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/880</URL>\n\n<TITLE>[Question] Multiturn conversations</TITLE>\n\n<BODY>Can I use a multi turn conversations about an image to finetune the model? as human gpt human gpt for instance? Or they must be divided into 2 conversations?\r\n\r\nfor example: \r\n        \"id\": \"FOLDER/IMAGE.jpg\",\r\n        \"image\": \"FOLDER/IMAGE.jpg\",\r\n        \"conversations\": [\r\n            {\r\n                \"value\": \"...\",\r\n                \"from\": \"human\"\r\n            },\r\n            {\r\n                \"value\": \"....\",\r\n                \"from\": \"gpt\"\r\n            },\r\n            {\r\n                \"value\": \"....\",\r\n                \"from\": \"human\"\r\n            },\r\n            {\r\n                \"value\": ...,\r\n                \"from\": \"gpt\"\r\n            }]</BODY>\n\n<COMMENTS>\n<Comment by tingxueronghua at 2023-11-30T14:02:26Z>\nI think there are multi-turn examples already in the fine-tuning stage since the first version of llava. You can try to iterate the dataset and check the length of the conversations to find such examples.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 878,
    "state": "open",
    "created_by": "bpwl0121",
    "created_at": "2023-11-30T08:05:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/878</URL>\n\n<TITLE>[Question] training loss curve</TITLE>\n\n<BODY>### Question\n\n1. could you explain the loss of llava 1.5 is higher than llava (I think both pretraining and Visual Instruction Tuning stage), but achieve better result?\r\n2. also, why did the **spike** occur in the llava 1.5 in the Visual Instruction Tuning stage?\r\n\r\n[the training loss from llava 1.5](https://api.wandb.ai/links/lht/6orh56wc)\r\n[the training loss from llava](https://github.com/haotian-liu/LLaVA/issues/62)</BODY>\n\n<COMMENTS>\n<Comment by tingxueronghua at 2023-11-30T09:22:59Z>\nFor MLLM and LLM, in my experience, lower training loss, even on the same dataset, does not mean the performance would be better. The only thing that is meaningful is to make sure the loss decreases.\r\nI guess the spike exists because LLaVA-1.5 groups data in different modalities. When it is optimizing the samples that do not contain images, the loss would be really low. (Just guess)\n</Comment>\n<Comment by bpwl0121 at 2023-11-30T11:21:56Z>\n> For MLLM and LLM, in my experience, lower training loss, even on the same dataset, does not mean the performance would be better. The only thing that is meaningful is to make sure the loss decreases. I guess the spike exists because LLaVA-1.5 groups data in different modalities. When it is optimizing the samples that do not contain images, the loss would be really low. (Just guess)\r\n\r\nthanks for your reply ;), do you have any idea why should we need to train 1 epoch for the llava pretraining stage, whatever llava 1 or 1.5,  cause loss is just oscillating around 2? the results are better?\n</Comment>\n<Comment by tingxueronghua at 2023-11-30T14:00:26Z>\n> > For MLLM and LLM, in my experience, lower training loss, even on the same dataset, does not mean the performance would be better. The only thing that is meaningful is to make sure the loss decreases. I guess the spike exists because LLaVA-1.5 groups data in different modalities. When it is optimizing the samples that do not contain images, the loss would be really low. (Just guess)\r\n> \r\n> thanks for your reply ;), do you have any idea why should we need to train 1 epoch for the llava pretraining stage, whatever llava 1 or 1.5, cause loss is just oscillating around 2? the results are better?\r\n\r\nJust some intuitive thoughts.\r\n\r\nFirst, I always hope the training epochs to be as small as possible if the performances are enough and the model is really large. MLLM is much stronger than normal neural networks, and its memorization ability is also much stronger. We do not want the MLLM to overfit the given samples.\r\n\r\nSecond, I think we also need to preserve the ability of LLM. As I said before, I guess loss might not be a good criteria for MLLM. We expect the model could continue to get correct results while there are even some wrong predicted tokens before. This ability could only be guaranteed by LLM and could not be measured by loss. We hope not to change the ability of LLM, so we cannot train the model on new datasets for too many iterations.\r\n\r\nAgain, I have to say these are just some intuitive thoughts. I have noticed similar results during the experiments of my recent work ChartLlama(https://github.com/tingxueronghua/ChartLlama-code), for example, MLLM is quite easy to overfit, but I did not pay too much attention to verify the thoughts.\n</Comment>\n<Comment by lucasjinreal at 2024-03-08T10:51:31Z>\nI got loss first converge and then diverge , got any idea? Using lora training a large custom data, espcially mixed with llavar and sharegpt4v sft data.\r\n\r\n```\r\n{'loss': 1.112, 'learning_rate': 0.00019949993043107128, 'epoch': 0.06}                                                                                                                                                         \r\n{'loss': 0.9816, 'learning_rate': 0.00019949672241561115, 'epoch': 0.06}                                                                                                                                                        \r\n{'loss': 1.1023, 'learning_rate': 0.0001994935041690517, 'epoch': 0.06}                                                                                                                                                         \r\n{'loss': 1.1331, 'learning_rate': 0.00019949027569172385, 'epoch': 0.06}                                                                                                                                                        \r\n{'loss': 1.1322, 'learning_rate': 0.00019948703698395963, 'epoch': 0.06}                                                                                                                                                        \r\n{'loss': 1.3218, 'learning_rate': 0.00019948378804609203, 'epoch': 0.06}                                                                                                                                                        \r\n{'loss': 1.062, 'learning_rate': 0.00019948052887845514, 'epoch': 0.06}                                                                                                                                                         \r\n{'loss': 1.0173, 'learning_rate': 0.0001994772594813841, 'epoch': 0.06}                                                                                                                                                         \r\n{'loss': 0.9592, 'learning_rate': 0.00019947397985521507, 'epoch': 0.06}                                                                                                                                                        \r\n{'loss': 1.0124, 'learning_rate': 0.00019947069000028536, 'epoch': 0.06}                                                                                                                                                        \r\n{'loss': 1.1834, 'learning_rate': 0.0001994673899169332, 'epoch': 0.06}                                                                                                                                                         \r\n{'loss': 0.9815, 'learning_rate': 0.00019946407960549792, 'epoch': 0.06}                                                                                                                                                        \r\n{'loss': 1.1549, 'learning_rate': 0.00019946075906632, 'epoch': 0.06}                                                                                                                                                           \r\n{'loss': 1.0615, 'learning_rate': 0.00019945742829974078, 'epoch': 0.06}                                                                                                                                                        \r\n{'loss': 1.0181, 'learning_rate': 0.0001994540873061028, 'epoch': 0.06}                                                                                                                                                         \r\n{'loss': 1.3302, 'learning_rate': 0.0001994507360857497, 'epoch': 0.06}                                                                                                                                                         \r\n{'loss': 1.1386, 'learning_rate': 0.00019944737463902591, 'epoch': 0.06}                                                                                                                                                        \r\n{'loss': 0.9878, 'learning_rate': 0.0001994440029662772, 'epoch': 0.06}                                                                                                                                                         \r\n{'loss': 0.9232, 'learning_rate': 0.00019944062106785025, 'epoch': 0.06}                                                                                                                                                        \r\n{'loss': 1.3084, 'learning_rate': 0.0001994372289440928, 'epoch': 0.06}                                                                                                                                                         \r\n{'loss': 1.3078, 'learning_rate': 0.00019943382659535366, 'epoch': 0.06}                                                                                                                                                        \r\n{'loss': 1.3635, 'learning_rate': 0.0001994304140219827, 'epoch': 0.06}                                                                                                                                                         \r\n{'loss': 4.694, 'learning_rate': 0.00019942699122433083, 'epoch': 0.06}                                                                                                                                                         \r\n{'loss': 11.0167, 'learning_rate': 0.00019942355820274998, 'epoch': 0.06}                                                                                                                                                       \r\n{'loss': 9.2744, 'learning_rate': 0.00019942011495759323, 'epoch': 0.06}                                                                                                                                                        \r\n{'loss': 10.3388, 'learning_rate': 0.00019941666148921456, 'epoch': 0.06}                                                                                                                                                       \r\n{'loss': 10.1331, 'learning_rate': 0.00019941319779796918, 'epoch': 0.06}                                                                                                                                                       \r\n{'loss': 13.4028, 'learning_rate': 0.00019940972388421315, 'epoch': 0.06}                                                                                                                                                       \r\n{'loss': 10.3771, 'learning_rate': 0.00019940623974830376, 'epoch': 0.06}                                                                                                                                                       \r\n{'loss': 10.0206, 'learning_rate': 0.00019940274539059926, 'epoch': 0.06}                                                                                                                                                       \r\n{'loss': 13.2177, 'learning_rate': 0.00019939924081145897, 'epoch': 0.06}                                                                                                                                                       \r\n{'loss': 12.1251, 'learning_rate': 0.00019939572601124325, 'epoch': 0.06}                                                                                                                                                       \r\n{'loss': 9.8189, 'learning_rate': 0.00019939220099031356, 'epoch': 0.06}                                                                                                                                                        \r\n{'loss': 8.217, 'learning_rate': 0.00019938866574903231, 'epoch': 0.06}                                                                                                                                                         \r\n{'loss': 9.5511, 'learning_rate': 0.00019938512028776307, 'epoch': 0.06}                                                                                                                                                        \r\n{'loss': 8.9313, 'learning_rate': 0.00019938156460687042, 'epoch': 0.07}                                                                                                                                                        \r\n{'loss': 9.0502, 'learning_rate': 0.00019937799870671993, 'epoch': 0.07}                                                                                                                                                        \r\n{'loss': 8.213, 'learning_rate': 0.00019937442258767833, 'epoch': 0.07}                                                                                                                                                         \r\n{'loss': 8.8887, 'learning_rate': 0.00019937083625011336, 'epoch': 0.07}                                                                                                                                                        \r\n{'loss': 7.8895, 'learning_rate': 0.0001993672396943937, 'epoch': 0.07}                                                                                                                                                         \r\n{'loss': 8.2777, 'learning_rate': 0.00019936363292088932, 'epoch': 0.07}                                                                                                                                                        \r\n{'loss': 8.1999, 'learning_rate': 0.00019936001592997098, 'epoch': 0.07}                                                                                                                                                        \r\n{'loss': 7.5588, 'learning_rate': 0.00019935638872201067, 'epoch': 0.07}                                                                                                                                                        \r\n{'loss': 8.2621, 'learning_rate': 0.00019935275129738135, 'epoch': 0.07}                                                                                                                                                        \r\n{'loss': 7.7207, 'learning_rate': 0.00019934910365645704, 'epoch': 0.07}                                                                                                                                                        \r\n{'loss': 8.1122, 'learning_rate': 0.00019934544579961285, 'epoch': 0.07}                                                                                                                                                        \r\n{'loss': 8.3204, 'learning_rate': 0.00019934177772722492, 'epoch': 0.07}                                                                                                                                                        \r\n{'loss': 8.6745, 'learning_rate': 0.0001993380994396704, 'epoch': 0.07}                                                                                                                                                         \r\n{'loss': 7.5472, 'learning_rate': 0.00019933441093732755, 'epoch': 0.07}                                                                                                                                                        \r\n{'loss': 7.5852, 'learning_rate': 0.0001993307122205756, 'epoch': 0.07}                                                                                                                                                         \r\n{'loss': 7.9383, 'learning_rate': 0.00019932700328979493, 'epoch': 0.07}                                                                                                                                                        \r\n{'loss': 7.4093, 'learning_rate': 0.00019932328414536693, 'epoch': 0.07}                                                                                                                                                        \r\n{'loss': 7.1529, 'learning_rate': 0.00019931955478767402, 'epoch': 0.07}                                                                                                                                                        \r\n{'loss': 8.0983, 'learning_rate': 0.0001993158152170997, 'epoch': 0.07}                                                                                                                                                         \r\n{'loss': 8.1425, 'learning_rate': 0.00019931206543402844, 'epoch': 0.07}                                                                                                                                                        \r\n{'loss': 8.0351, 'learning_rate': 0.0001993083054388459, 'epoch': 0.07}                                                                                                                                                         \r\n{'loss': 7.5124, 'learning_rate': 0.00019930453523193871, 'epoch': 0.07}                                                                                                                                                        \r\n{'loss': 9.98, 'learning_rate': 0.00019930075481369453, 'epoch': 0.07}                                                                                                                                                          \r\n{'loss': 9.5756, 'learning_rate': 0.00019929696418450208, 'epoch': 0.07}                                                                                                                                                        \r\n{'loss': 8.7834, 'learning_rate': 0.00019929316334475115, 'epoch': 0.07}                                                                                                                                                        \r\n{'loss': 8.7451, 'learning_rate': 0.0001992893522948326, 'epoch': 0.07}                                                                                                                                                         \r\n{'loss': 8.8393, 'learning_rate': 0.0001992855310351383, 'epoch': 0.07}                                                                                                                                                         \r\n{'loss': 8.4028, 'learning_rate': 0.00019928169956606119, 'epoch': 0.07}                                                                                                                                                        \r\n{'loss': 7.8437, 'learning_rate': 0.00019927785788799524, 'epoch': 0.07}                                                                                                                                                        \r\n{'loss': 9.7655, 'learning_rate': 0.0001992740060013355, 'epoch': 0.07} \r\n```\n</Comment>\n<Comment by The-kamisato at 2024-03-30T06:51:51Z>\n> I got loss first converge and then diverge , got any idea? Using lora training a large custom data, espcially mixed with llavar and sharegpt4v sft data.\r\n> \r\n> ```\r\n> {'loss': 1.112, 'learning_rate': 0.00019949993043107128, 'epoch': 0.06}                                                                                                                                                         \r\n> {'loss': 0.9816, 'learning_rate': 0.00019949672241561115, 'epoch': 0.06}                                                                                                                                                        \r\n> {'loss': 1.1023, 'learning_rate': 0.0001994935041690517, 'epoch': 0.06}                                                                                                                                                         \r\n> {'loss': 1.1331, 'learning_rate': 0.00019949027569172385, 'epoch': 0.06}                                                                                                                                                        \r\n> {'loss': 1.1322, 'learning_rate': 0.00019948703698395963, 'epoch': 0.06}                                                                                                                                                        \r\n> {'loss': 1.3218, 'learning_rate': 0.00019948378804609203, 'epoch': 0.06}                                                                                                                                                        \r\n> {'loss': 1.062, 'learning_rate': 0.00019948052887845514, 'epoch': 0.06}                                                                                                                                                         \r\n> {'loss': 1.0173, 'learning_rate': 0.0001994772594813841, 'epoch': 0.06}                                                                                                                                                         \r\n> {'loss': 0.9592, 'learning_rate': 0.00019947397985521507, 'epoch': 0.06}                                                                                                                                                        \r\n> {'loss': 1.0124, 'learning_rate': 0.00019947069000028536, 'epoch': 0.06}                                                                                                                                                        \r\n> {'loss': 1.1834, 'learning_rate': 0.0001994673899169332, 'epoch': 0.06}                                                                                                                                                         \r\n> {'loss': 0.9815, 'learning_rate': 0.00019946407960549792, 'epoch': 0.06}                                                                                                                                                        \r\n> {'loss': 1.1549, 'learning_rate': 0.00019946075906632, 'epoch': 0.06}                                                                                                                                                           \r\n> {'loss': 1.0615, 'learning_rate': 0.00019945742829974078, 'epoch': 0.06}                                                                                                                                                        \r\n> {'loss': 1.0181, 'learning_rate': 0.0001994540873061028, 'epoch': 0.06}                                                                                                                                                         \r\n> {'loss': 1.3302, 'learning_rate': 0.0001994507360857497, 'epoch': 0.06}                                                                                                                                                         \r\n> {'loss': 1.1386, 'learning_rate': 0.00019944737463902591, 'epoch': 0.06}                                                                                                                                                        \r\n> {'loss': 0.9878, 'learning_rate': 0.0001994440029662772, 'epoch': 0.06}                                                                                                                                                         \r\n> {'loss': 0.9232, 'learning_rate': 0.00019944062106785025, 'epoch': 0.06}                                                                                                                                                        \r\n> {'loss': 1.3084, 'learning_rate': 0.0001994372289440928, 'epoch': 0.06}                                                                                                                                                         \r\n> {'loss': 1.3078, 'learning_rate': 0.00019943382659535366, 'epoch': 0.06}                                                                                                                                                        \r\n> {'loss': 1.3635, 'learning_rate': 0.0001994304140219827, 'epoch': 0.06}                                                                                                                                                         \r\n> {'loss': 4.694, 'learning_rate': 0.00019942699122433083, 'epoch': 0.06}                                                                                                                                                         \r\n> {'loss': 11.0167, 'learning_rate': 0.00019942355820274998, 'epoch': 0.06}                                                                                                                                                       \r\n> {'loss': 9.2744, 'learning_rate': 0.00019942011495759323, 'epoch': 0.06}                                                                                                                                                        \r\n> {'loss': 10.3388, 'learning_rate': 0.00019941666148921456, 'epoch': 0.06}                                                                                                                                                       \r\n> {'loss': 10.1331, 'learning_rate': 0.00019941319779796918, 'epoch': 0.06}                                                                                                                                                       \r\n> {'loss': 13.4028, 'learning_rate': 0.00019940972388421315, 'epoch': 0.06}                                                                                                                                                       \r\n> {'loss': 10.3771, 'learning_rate': 0.00019940623974830376, 'epoch': 0.06}                                                                                                                                                       \r\n> {'loss': 10.0206, 'learning_rate': 0.00019940274539059926, 'epoch': 0.06}                                                                                                                                                       \r\n> {'loss': 13.2177, 'learning_rate': 0.00019939924081145897, 'epoch': 0.06}                                                                                                                                                       \r\n> {'loss': 12.1251, 'learning_rate': 0.00019939572601124325, 'epoch': 0.06}                                                                                                                                                       \r\n> {'loss': 9.8189, 'learning_rate': 0.00019939220099031356, 'epoch': 0.06}                                                                                                                                                        \r\n> {'loss': 8.217, 'learning_rate': 0.00019938866574903231, 'epoch': 0.06}                                                                                                                                                         \r\n> {'loss': 9.5511, 'learning_rate': 0.00019938512028776307, 'epoch': 0.06}                                                                                                                                                        \r\n> {'loss': 8.9313, 'learning_rate': 0.00019938156460687042, 'epoch': 0.07}                                                                                                                                                        \r\n> {'loss': 9.0502, 'learning_rate': 0.00019937799870671993, 'epoch': 0.07}                                                                                                                                                        \r\n> {'loss': 8.213, 'learning_rate': 0.00019937442258767833, 'epoch': 0.07}                                                                                                                                                         \r\n> {'loss': 8.8887, 'learning_rate': 0.00019937083625011336, 'epoch': 0.07}                                                                                                                                                        \r\n> {'loss': 7.8895, 'learning_rate': 0.0001993672396943937, 'epoch': 0.07}                                                                                                                                                         \r\n> {'loss': 8.2777, 'learning_rate': 0.00019936363292088932, 'epoch': 0.07}                                                                                                                                                        \r\n> {'loss': 8.1999, 'learning_rate': 0.00019936001592997098, 'epoch': 0.07}                                                                                                                                                        \r\n> {'loss': 7.5588, 'learning_rate': 0.00019935638872201067, 'epoch': 0.07}                                                                                                                                                        \r\n> {'loss': 8.2621, 'learning_rate': 0.00019935275129738135, 'epoch': 0.07}                                                                                                                                                        \r\n> {'loss': 7.7207, 'learning_rate': 0.00019934910365645704, 'epoch': 0.07}                                                                                                                                                        \r\n> {'loss': 8.1122, 'learning_rate': 0.00019934544579961285, 'epoch': 0.07}                                                                                                                                                        \r\n> {'loss': 8.3204, 'learning_rate': 0.00019934177772722492, 'epoch': 0.07}                                                                                                                                                        \r\n> {'loss': 8.6745, 'learning_rate': 0.0001993380994396704, 'epoch': 0.07}                                                                                                                                                         \r\n> {'loss': 7.5472, 'learning_rate': 0.00019933441093732755, 'epoch': 0.07}                                                                                                                                                        \r\n> {'loss': 7.5852, 'learning_rate': 0.0001993307122205756, 'epoch': 0.07}                                                                                                                                                         \r\n> {'loss': 7.9383, 'learning_rate': 0.00019932700328979493, 'epoch': 0.07}                                                                                                                                                        \r\n> {'loss': 7.4093, 'learning_rate': 0.00019932328414536693, 'epoch': 0.07}                                                                                                                                                        \r\n> {'loss': 7.1529, 'learning_rate': 0.00019931955478767402, 'epoch': 0.07}                                                                                                                                                        \r\n> {'loss': 8.0983, 'learning_rate': 0.0001993158152170997, 'epoch': 0.07}                                                                                                                                                         \r\n> {'loss': 8.1425, 'learning_rate': 0.00019931206543402844, 'epoch': 0.07}                                                                                                                                                        \r\n> {'loss': 8.0351, 'learning_rate': 0.0001993083054388459, 'epoch': 0.07}                                                                                                                                                         \r\n> {'loss': 7.5124, 'learning_rate': 0.00019930453523193871, 'epoch': 0.07}                                                                                                                                                        \r\n> {'loss': 9.98, 'learning_rate': 0.00019930075481369453, 'epoch': 0.07}                                                                                                                                                          \r\n> {'loss': 9.5756, 'learning_rate': 0.00019929696418450208, 'epoch': 0.07}                                                                                                                                                        \r\n> {'loss': 8.7834, 'learning_rate': 0.00019929316334475115, 'epoch': 0.07}                                                                                                                                                        \r\n> {'loss': 8.7451, 'learning_rate': 0.0001992893522948326, 'epoch': 0.07}                                                                                                                                                         \r\n> {'loss': 8.8393, 'learning_rate': 0.0001992855310351383, 'epoch': 0.07}                                                                                                                                                         \r\n> {'loss': 8.4028, 'learning_rate': 0.00019928169956606119, 'epoch': 0.07}                                                                                                                                                        \r\n> {'loss': 7.8437, 'learning_rate': 0.00019927785788799524, 'epoch': 0.07}                                                                                                                                                        \r\n> {'loss': 9.7655, 'learning_rate': 0.0001992740060013355, 'epoch': 0.07} \r\n> ```\r\n\r\nHave you found why this happened?\n</Comment>\n<Comment by Ryanlijinke at 2024-10-11T02:47:48Z>\n> > I got loss first converge and then diverge , got any idea? Using lora training a large custom data, espcially mixed with llavar and sharegpt4v sft data.\r\n> > ```\r\n> > {'loss': 1.112, 'learning_rate': 0.00019949993043107128, 'epoch': 0.06}                                                                                                                                                         \r\n> > {'loss': 0.9816, 'learning_rate': 0.00019949672241561115, 'epoch': 0.06}                                                                                                                                                        \r\n> > {'loss': 1.1023, 'learning_rate': 0.0001994935041690517, 'epoch': 0.06}                                                                                                                                                         \r\n> > {'loss': 1.1331, 'learning_rate': 0.00019949027569172385, 'epoch': 0.06}                                                                                                                                                        \r\n> > {'loss': 1.1322, 'learning_rate': 0.00019948703698395963, 'epoch': 0.06}                                                                                                                                                        \r\n> > {'loss': 1.3218, 'learning_rate': 0.00019948378804609203, 'epoch': 0.06}                                                                                                                                                        \r\n> > {'loss': 1.062, 'learning_rate': 0.00019948052887845514, 'epoch': 0.06}                                                                                                                                                         \r\n> > {'loss': 1.0173, 'learning_rate': 0.0001994772594813841, 'epoch': 0.06}                                                                                                                                                         \r\n> > {'loss': 0.9592, 'learning_rate': 0.00019947397985521507, 'epoch': 0.06}                                                                                                                                                        \r\n> > {'loss': 1.0124, 'learning_rate': 0.00019947069000028536, 'epoch': 0.06}                                                                                                                                                        \r\n> > {'loss': 1.1834, 'learning_rate': 0.0001994673899169332, 'epoch': 0.06}                                                                                                                                                         \r\n> > {'loss': 0.9815, 'learning_rate': 0.00019946407960549792, 'epoch': 0.06}                                                                                                                                                        \r\n> > {'loss': 1.1549, 'learning_rate': 0.00019946075906632, 'epoch': 0.06}                                                                                                                                                           \r\n> > {'loss': 1.0615, 'learning_rate': 0.00019945742829974078, 'epoch': 0.06}                                                                                                                                                        \r\n> > {'loss': 1.0181, 'learning_rate': 0.0001994540873061028, 'epoch': 0.06}                                                                                                                                                         \r\n> > {'loss': 1.3302, 'learning_rate': 0.0001994507360857497, 'epoch': 0.06}                                                                                                                                                         \r\n> > {'loss': 1.1386, 'learning_rate': 0.00019944737463902591, 'epoch': 0.06}                                                                                                                                                        \r\n> > {'loss': 0.9878, 'learning_rate': 0.0001994440029662772, 'epoch': 0.06}                                                                                                                                                         \r\n> > {'loss': 0.9232, 'learning_rate': 0.00019944062106785025, 'epoch': 0.06}                                                                                                                                                        \r\n> > {'loss': 1.3084, 'learning_rate': 0.0001994372289440928, 'epoch': 0.06}                                                                                                                                                         \r\n> > {'loss': 1.3078, 'learning_rate': 0.00019943382659535366, 'epoch': 0.06}                                                                                                                                                        \r\n> > {'loss': 1.3635, 'learning_rate': 0.0001994304140219827, 'epoch': 0.06}                                                                                                                                                         \r\n> > {'loss': 4.694, 'learning_rate': 0.00019942699122433083, 'epoch': 0.06}                                                                                                                                                         \r\n> > {'loss': 11.0167, 'learning_rate': 0.00019942355820274998, 'epoch': 0.06}                                                                                                                                                       \r\n> > {'loss': 9.2744, 'learning_rate': 0.00019942011495759323, 'epoch': 0.06}                                                                                                                                                        \r\n> > {'loss': 10.3388, 'learning_rate': 0.00019941666148921456, 'epoch': 0.06}                                                                                                                                                       \r\n> > {'loss': 10.1331, 'learning_rate': 0.00019941319779796918, 'epoch': 0.06}                                                                                                                                                       \r\n> > {'loss': 13.4028, 'learning_rate': 0.00019940972388421315, 'epoch': 0.06}                                                                                                                                                       \r\n> > {'loss': 10.3771, 'learning_rate': 0.00019940623974830376, 'epoch': 0.06}                                                                                                                                                       \r\n> > {'loss': 10.0206, 'learning_rate': 0.00019940274539059926, 'epoch': 0.06}                                                                                                                                                       \r\n> > {'loss': 13.2177, 'learning_rate': 0.00019939924081145897, 'epoch': 0.06}                                                                                                                                                       \r\n> > {'loss': 12.1251, 'learning_rate': 0.00019939572601124325, 'epoch': 0.06}                                                                                                                                                       \r\n> > {'loss': 9.8189, 'learning_rate': 0.00019939220099031356, 'epoch': 0.06}                                                                                                                                                        \r\n> > {'loss': 8.217, 'learning_rate': 0.00019938866574903231, 'epoch': 0.06}                                                                                                                                                         \r\n> > {'loss': 9.5511, 'learning_rate': 0.00019938512028776307, 'epoch': 0.06}                                                                                                                                                        \r\n> > {'loss': 8.9313, 'learning_rate': 0.00019938156460687042, 'epoch': 0.07}                                                                                                                                                        \r\n> > {'loss': 9.0502, 'learning_rate': 0.00019937799870671993, 'epoch': 0.07}                                                                                                                                                        \r\n> > {'loss': 8.213, 'learning_rate': 0.00019937442258767833, 'epoch': 0.07}                                                                                                                                                         \r\n> > {'loss': 8.8887, 'learning_rate': 0.00019937083625011336, 'epoch': 0.07}                                                                                                                                                        \r\n> > {'loss': 7.8895, 'learning_rate': 0.0001993672396943937, 'epoch': 0.07}                                                                                                                                                         \r\n> > {'loss': 8.2777, 'learning_rate': 0.00019936363292088932, 'epoch': 0.07}                                                                                                                                                        \r\n> > {'loss': 8.1999, 'learning_rate': 0.00019936001592997098, 'epoch': 0.07}                                                                                                                                                        \r\n> > {'loss': 7.5588, 'learning_rate': 0.00019935638872201067, 'epoch': 0.07}                                                                                                                                                        \r\n> > {'loss': 8.2621, 'learning_rate': 0.00019935275129738135, 'epoch': 0.07}                                                                                                                                                        \r\n> > {'loss': 7.7207, 'learning_rate': 0.00019934910365645704, 'epoch': 0.07}                                                                                                                                                        \r\n> > {'loss': 8.1122, 'learning_rate': 0.00019934544579961285, 'epoch': 0.07}                                                                                                                                                        \r\n> > {'loss': 8.3204, 'learning_rate': 0.00019934177772722492, 'epoch': 0.07}                                                                                                                                                        \r\n> > {'loss': 8.6745, 'learning_rate': 0.0001993380994396704, 'epoch': 0.07}                                                                                                                                                         \r\n> > {'loss': 7.5472, 'learning_rate': 0.00019933441093732755, 'epoch': 0.07}                                                                                                                                                        \r\n> > {'loss': 7.5852, 'learning_rate': 0.0001993307122205756, 'epoch': 0.07}                                                                                                                                                         \r\n> > {'loss': 7.9383, 'learning_rate': 0.00019932700328979493, 'epoch': 0.07}                                                                                                                                                        \r\n> > {'loss': 7.4093, 'learning_rate': 0.00019932328414536693, 'epoch': 0.07}                                                                                                                                                        \r\n> > {'loss': 7.1529, 'learning_rate': 0.00019931955478767402, 'epoch': 0.07}                                                                                                                                                        \r\n> > {'loss': 8.0983, 'learning_rate': 0.0001993158152170997, 'epoch': 0.07}                                                                                                                                                         \r\n> > {'loss': 8.1425, 'learning_rate': 0.00019931206543402844, 'epoch': 0.07}                                                                                                                                                        \r\n> > {'loss': 8.0351, 'learning_rate': 0.0001993083054388459, 'epoch': 0.07}                                                                                                                                                         \r\n> > {'loss': 7.5124, 'learning_rate': 0.00019930453523193871, 'epoch': 0.07}                                                                                                                                                        \r\n> > {'loss': 9.98, 'learning_rate': 0.00019930075481369453, 'epoch': 0.07}                                                                                                                                                          \r\n> > {'loss': 9.5756, 'learning_rate': 0.00019929696418450208, 'epoch': 0.07}                                                                                                                                                        \r\n> > {'loss': 8.7834, 'learning_rate': 0.00019929316334475115, 'epoch': 0.07}                                                                                                                                                        \r\n> > {'loss': 8.7451, 'learning_rate': 0.0001992893522948326, 'epoch': 0.07}                                                                                                                                                         \r\n> > {'loss': 8.8393, 'learning_rate': 0.0001992855310351383, 'epoch': 0.07}                                                                                                                                                         \r\n> > {'loss': 8.4028, 'learning_rate': 0.00019928169956606119, 'epoch': 0.07}                                                                                                                                                        \r\n> > {'loss': 7.8437, 'learning_rate': 0.00019927785788799524, 'epoch': 0.07}                                                                                                                                                        \r\n> > {'loss': 9.7655, 'learning_rate': 0.0001992740060013355, 'epoch': 0.07} \r\n> > ```\r\n> \r\n> Have you found why this happened?\r\n\r\nI think maybe your lr is too large?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 877,
    "state": "open",
    "created_by": "y-vectorfield",
    "created_at": "2023-11-30T05:11:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/877</URL>\n\n<TITLE>[Question] Can we use this model from TensorFlow?</TITLE>\n\n<BODY>### Question\n\nI read this model's source codes. This source was implemented using PyTorch.\r\nCan we use this model from TensorFlow?(tran, inference, and transfer learning)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 876,
    "state": "open",
    "created_by": "c3-sravanj",
    "created_at": "2023-11-29T21:21:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/876</URL>\n\n<TITLE>Incompatibility with latest version of transformers 4.35.0</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: For `transformrers==4.35.0`, Llava throws an import error on lines 22, 23, 31, and 32 of `LLaVA/llava/model/language_model/mpt/hf_prefixlm_converter.py`\r\n\r\nFailed Imports:\r\n(lines 22 and 23)\r\n```\r\nfrom transformers.models.bloom.modeling_bloom import _expand_mask as _expand_mask_bloom\r\nfrom transformers.models.bloom.modeling_bloom import _make_causal_mask as _make_causal_mask_bloom\r\n```\r\n(lines 31 and 32)\r\n```\r\nfrom transformers.models.opt.modeling_opt import _expand_mask as _expand_mask_opt\r\nfrom transformers.models.opt.modeling_opt import _make_causal_mask as _make_causal_mask_opt\r\n```\r\n\r\nRemoving these dependences allows for the model `\"liuhaotian/llava-llama-2-13b-chat-lightning-preview` to work but can this logic be updated to allow for out-of-the-box compatibility with transformers?</BODY>\n\n<COMMENTS>\n<Comment by tingxueronghua at 2023-11-30T14:04:36Z>\nmy co-workers said different versions of transformers cannot guarantee the ids after tokenization to be the same. Do you have any reason why you need a newer version of transformers? Maybe there is another way which could achieve your goal.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 875,
    "state": "open",
    "created_by": "DietDietDiet",
    "created_at": "2023-11-29T13:49:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/875</URL>\n\n<TITLE>[Question]  Is there any specific reason for using deepspeed zero2 for pretraining and zero3 for SFT?</TITLE>\n\n<BODY>### Question\n\nDo zero2 perform better than zero3 in pretraining? Thanks@haotian-liu</BODY>\n\n<COMMENTS>\n<Comment by aneet-javis at 2023-12-15T09:47:40Z>\n@DietDietDiet Could you figure out the reason for this?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 874,
    "state": "open",
    "created_by": "xinsir6",
    "created_at": "2023-11-29T11:54:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/874</URL>\n\n<TITLE>How to provide 2 or more images to the model and compare the beauty of the images?</TITLE>\n\n<BODY>### Question\r\n\r\nI have a problem when use llava to process multi images in the same time, such as, give the model 2 or more images, and ask it to answer questions about the images, like which one do you like better? The web demo and chat can't solve this problem, so could you provide a special scripts to do this?</BODY>\n\n<COMMENTS>\n<Comment by mapluisch at 2023-11-30T15:31:48Z>\nYou might try putting two images into one by separating them with a horizontal line and including that detail in your prompt. I haven't tested this method extensively, but it seemed to work quite well in a quick trial I did: \r\n\r\n<img width=\"1073\" alt=\"fused_image\" src=\"https://github.com/haotian-liu/LLaVA/assets/31780571/718f2cac-b0d6-4e92-9f61-0bd0e89679b1\">\r\n\r\nIf this works well enough for you, you could adjust the scripts to accept two images which then get merged in this way. I might create a quick CLI demo.\n</Comment>\n<Comment by xinsir6 at 2023-11-30T16:33:26Z>\ngood idea, I will try it, the only problem is that the method requires two images to be resized into same width/height\n</Comment>\n<Comment by mapluisch at 2023-11-30T16:49:55Z>\n> good idea, I will try it, the only problem is that the method requires two images to be resized into same width/height\r\n\r\nI've made a simple proof of concept for you based on cli.py:\r\n\r\nhttps://github.com/mapluisch/LLaVA-CLI-with-multiple-images\r\n\r\nIt doesn't resize (or truncate / crop) the images, just concatenates them vertically.\n</Comment>\n<Comment by xinsir6 at 2023-12-01T07:56:05Z>\n> > good idea, I will try it, the only problem is that the method requires two images to be resized into same width/height\r\n> \r\n> I've made a simple proof of concept for you based on cli.py:\r\n> \r\n> https://github.com/mapluisch/LLaVA-CLI-with-multiple-images\r\n> \r\n> It doesn't resize (or truncate / crop) the images, just concatenates them vertically.\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/128811208/a02af74c-7d28-4781-9c29-7804e7a245da)\r\n![image](https://github.com/haotian-liu/LLaVA/assets/128811208/b922f363-24c9-439d-8190-4fb7db7abeca)\r\nI try to ask the model as the way you request, but the model refuse to reply, is there any way to solve it ？\n</Comment>\n<Comment by mapluisch at 2023-12-03T10:18:22Z>\nYou should play around with different prompts and temperatures. I'm using 4-bit quantization on the 13b model, and this prompt works: \r\n\r\n`Analyze the two images and tell me which one is better and why`\n</Comment>\n<Comment by yuejunpeng at 2024-06-01T15:15:48Z>\n@mapluisch Hello! It is true that you don’t need to resize the two images when you just concatenate them vertically, but I think the model does resize them to fit when feeding them to the clip image encoder.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 873,
    "state": "closed",
    "created_by": "egeozsoy",
    "created_at": "2023-11-29T09:41:17Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/873</URL>\n\n<TITLE>[Usage] Pretrain Script does not freeze LLM</TITLE>\n\n<BODY>### Describe the issue\n\nIt seems like your pretraining script does not freeze the LLM like you claim in the paper. Is this on purpose?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 872,
    "state": "closed",
    "created_by": "rringham",
    "created_at": "2023-11-28T22:23:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/872</URL>\n\n<TITLE>[Question] Is this everything I need to do to properly fine-tune llava-v1.5-7b?</TITLE>\n\n<BODY>### Question\r\n\r\nHi - I'm trying to wrap my head around how I'd fine-tune LLaVA for a specific use case. As an experiment, I have 28k images that I've generated a `dataset.json` for (am just programmatically generating text descriptions based on already known classes).\r\n\r\nMy somewhat vague understanding is that I should be able to fine-tune **llava-v1.5-7b** by doing something like the following (and nothing more):\r\n\r\n```\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path liuhaotian/llava-v1.5-7b \\\r\n    --version v1 \\\r\n    --data_path /code/datasets/labeled_images/dataset.json \\\r\n    --image_folder /code/datasets/labeled_images \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nHowever, when I go to use (see following command) it will complain that `mm_projection.bin` is missing.\r\n\r\n```\r\npython model_vqa.py \\\r\n    --model-base liuhaotian/llava-v1.5-7b \\\r\n    --model-path ./checkpoints/llava-v1.5-7b  \\\r\n    --question-file ./questions.json \\\r\n    --image-folder /code/datasets/labeled_images \\\r\n    --answers-file ./answers.json\r\n```\r\n\r\nI can get the above command to work if I manually rename `./checkpoints/llava-v1.5-7b/non_lora_trainables.bin` to be `./checkpoints/llava-v1.5-7b/mm_projection.bin`, but the responses I see written to `answers.json` don't really appear as though they have been fine-tuned on my dataset - they look like the kinds of responses I get from the stock `liuhaotian/llava-v1.5-7b` model.\r\n\r\nIt leaves doubt in my mind that I'm actually correctly fine-tuning. Am I missing any key steps?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-28T22:32:40Z>\nPlease add a \"lora\" in the model path so that it knows that it is a lora trained weights. We're optimizing the parsing strategy, but currently this should solve your problem. And please do not rename the file.\r\n\r\n`--output_dir ./checkpoints/llava-v1.5-7b-mytask-lora \\`\r\n`--model-path ./checkpoints/llava-v1.5-7b-mytask-lora \\`\n</Comment>\n<Comment by rringham at 2023-11-29T00:35:08Z>\nThank you, that worked for me. Much appreciated!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 870,
    "state": "open",
    "created_by": "created-Bi",
    "created_at": "2023-11-28T07:54:09Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/870</URL>\n\n<TITLE>[inference]Please, how to run a 13b llava model on 2 A10(GPU, 24G for each) for inference?</TITLE>\n\n<BODY>### Describe the issue\n\nAs we all know, it uses approximately 28G to inference by a llava-13b model.\r\n\r\nI only have two A10(24G) GPU, how can I run the model on two GPUs? Can deepspeed resolve the problem? \r\n\r\nModel parallelism? Any help?</BODY>\n\n<COMMENTS>\n<Comment by mniiinm at 2023-11-29T16:42:21Z>\njust use 4bits\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 869,
    "state": "open",
    "created_by": "teasgen",
    "created_at": "2023-11-28T06:42:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/869</URL>\n\n<TITLE>[Question] Misunderstanding of TextVQA evaluation</TITLE>\n\n<BODY>### Question\n\nHi, I'm trying to use your textvqa evaluation script. Unfortunately, it fails in this [line](https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/eval_textvqa.py#L44) this KeyError exception\r\nI think problem is due to this [line](https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/eval_textvqa.py#L39). Shouldn't it be written here question_id, instead of image_id?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 868,
    "state": "open",
    "created_by": "cxl-ustb",
    "created_at": "2023-11-28T01:04:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/868</URL>\n\n<TITLE>finetune Always stuck</TITLE>\n\n<BODY>### Question\r\n\r\nThank you for your work.I used 8xv100 32gb,94 cpu and 364gb memory.\r\n#!/bin/bash\r\n################## VICUNA ##################\r\nPROMPT_VERSION=v1\r\nMODEL_VERSION=\"vicuna-v1-3-7b\"\r\n################## VICUNA ##################\r\n\r\ndeepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --model_name_or_path ./checkpoints/$MODEL_VERSION \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path /mnt/bd/demo/cxl/LLaVA/data/llava_instruct_80k.json \\\r\n    --image_folder /mnt/bd/demo/cxl/LLaVA/data/train2017 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava-$MODEL_VERSION-pretrain/mm_projector.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 False \\\r\n    --output_dir ./checkpoints/llava-$MODEL_VERSION-finetune \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 2 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 False \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 1 \\\r\n    --lazy_preprocess False \\\r\n    --report_to wandb\r\n![20231128-090552](https://github.com/haotian-liu/LLaVA/assets/67447948/67ed6708-e0a8-490b-aa45-4e8d47436700)</BODY>\n\n<COMMENTS>\n<Comment by loveunk at 2023-12-29T10:21:11Z>\nsame issue here, not yet fixed\n</Comment>\n<Comment by loveunk at 2023-12-29T15:39:45Z>\nhttps://github.com/huggingface/transformers/issues/28280\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 867,
    "state": "closed",
    "created_by": "anonymous-atom",
    "created_at": "2023-11-27T23:59:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/867</URL>\n\n<TITLE>[Usage] Poor/Random ouput after training and finetuning</TITLE>\n\n<BODY>Issue:\r\nMy custom trained and fintuned LLaVA is producing very random and poor ouput.\r\nI tried to first train the LLaVA model on custom data using [pretrain.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/pretrain.sh) script and then finetuned it using this script [finetune_lora.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_lora.sh). Then I used the trained lora weights using the command below:\r\n`python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1-0719-336px-lora-vicuna-13b-v1.3 --model-base lmsys/vicuna-13b-v1.3`\r\n\r\nAlso sometime the model is not producing any output even.\r\n\r\nI had 6000 entries in my custom dataset.\r\nCommand:\r\n```\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1-0719-336px-lora-vicuna-13b-v1.3 --model-base lmsys/vicuna-13b-v1.3\r\n```\r\n\r\n\r\n\r\nScreenshots:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/74659873/a47ba1ad-747c-4beb-ab3a-51498b044bb4)</BODY>\n\n<COMMENTS>\n<Comment by anonymous-atom at 2023-11-28T00:01:26Z>\n@haotian-liu can you look into this once ?\n</Comment>\n<Comment by anonymous-atom at 2023-11-30T04:43:02Z>\nSorry for mentioning your usernames like this but could you let me know what could have gone wrong ? I still can't figure out this ?\r\n@henrycjh @FHL1998\n</Comment>\n<Comment by FHL1998 at 2023-11-30T04:46:15Z>\n> Sorry for mentioning your usernames like this but could you let me know what could have gone wrong ? I still can't figure out this ? @henrycjh @FHL1998\r\n\r\nI highly doubt that 6000 samples are enough for the pertaining process, you may try to fine-tune your own dataset based on the released pre-trained checkpoints.\n</Comment>\n<Comment by anonymous-atom at 2023-11-30T04:49:30Z>\nOh cool, so I should just finetune on my custom dataset right ? that's what you mean ? @FHL1998 \r\nI will finetune right away and will post further updates.\r\n\r\nThanks Again!\n</Comment>\n<Comment by CrazyBrick at 2023-12-01T08:41:09Z>\n> > Sorry for mentioning your usernames like this but could you let me know what could have gone wrong ? I still can't figure out this ? @henrycjh @FHL1998\r\n> \r\n> I highly doubt that 6000 samples are enough for the pertaining process, you may try to fine-tune your own dataset based on the released pre-trained checkpoints.\r\n\r\n@FHL1998 Hi,I have 6k samples for finetuning.I also get poor results.Do you have any idea?\r\n![Screenshot from 2023-12-01 16-40-26](https://github.com/haotian-liu/LLaVA/assets/61942251/a02efe54-7fd6-4375-a758-f44bf0a68a6d)\n</Comment>\n<Comment by FHL1998 at 2023-12-04T01:37:01Z>\n>  Sorry for mentioning your usernames like this but could you let me know what could have gone wrong ? I still can't figure out this ? @henrycjh @FHL1998 \r\n\r\nMaybe your epoch is high, try to reduce it instead.\n</Comment>\n<Comment by CrazyBrick at 2023-12-04T06:17:26Z>\n> > Sorry for mentioning your usernames like this but could you let me know what could have gone wrong ? I still can't figure out this ? @henrycjh @FHL1998\r\n> \r\n> Maybe your epoch is high, try to reduce it instead.\r\n\r\nthank you. I used epoch=1 before, then I saw this [comment](https://github.com/haotian-liu/LLaVA/issues/847#issuecomment-1833338170), and increased this parameter.\n</Comment>\n<Comment by rohitpanjwani03 at 2023-12-04T09:28:04Z>\nHi guys, I am on colab running it on A100 and trying to fine-tuned using the below code, facing error like ./checkpoint or train_men.py and other train.py files  \r\n\r\nMy code\r\n```%cd /content\r\n!git clone https://github.com/haotian-liu/LLaVA.git\r\n%cd /content/LLaVA\r\n!pip install -q gradio .\r\n\r\n!bash /content/LLaVA/scripts/v1_5/finetune.sh```\r\n\r\n\r\nCan you guys help me with the correct way to fine-tune it?\n</Comment>\n<Comment by CrazyBrick at 2023-12-04T11:50:01Z>\n> Hi guys, I am on colab running it on A100 and trying to fine-tuned using the below code, facing error like ./checkpoint or train_men.py and other train.py files\r\n> \r\n> My code\r\n> \r\n> ```\r\n> !git clone https://github.com/haotian-liu/LLaVA.git\r\n> %cd /content/LLaVA\r\n> !pip install -q gradio .\r\n> \r\n> !bash /content/LLaVA/scripts/v1_5/finetune.sh```\r\n> \r\n> \r\n> Can you guys help me with the correct way to fine-tune it?\r\n> ```\r\n\r\nHave you downloaded the model and changed the file location in script?\n</Comment>\n<Comment by rohitpanjwani03 at 2023-12-04T13:04:07Z>\n**No** I haven't and didn't made any changes to the file location in the script. I guess it was actually reading .bin files from HuggingFace when I ran the bash command.\r\nThank you so much; I will try to make it work and run as suggested **also please let me** know how did you train?\n</Comment>\n<Comment by ggcr at 2024-05-27T16:29:10Z>\nJust in case someone is having problems during inferece; if you have a script that uses `/llava/eval/run_llava.py` as baseline for inference, you should be careful with the args. In my case I noticed that the `run_llava.py` file will merge the LoRA weights if you specify a `model-base`, and they were already merged, hence the poor performance on inference.\r\n\r\nIf the weights are merged:\r\n- `model-path` should include the path to your fine-tuned model.\r\n- `model-base` should be `None`.\r\n\r\nIf the weights are not merged, you should also specify a `model-base` to merge them.\r\n- `model-path` should include the path to your fine-tuned model.\r\n- `model-base` should be your base model e.g. `'liuhaotian/llava-v1.5-7b'`.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 865,
    "state": "open",
    "created_by": "wenli135",
    "created_at": "2023-11-27T13:31:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/865</URL>\n\n<TITLE>[Question] Can LLava inference on CPU?</TITLE>\n\n<BODY>### Question\n\nI was trying to run LLava inference on cpu, but it complains \"Torch not compiled with CUDA enabled\".  I noticed that cuda() is called when loading model. If I remove all the cuda() invocation, is it possible to run inference on cpu?\r\n\r\nthanks.</BODY>\n\n<COMMENTS>\n<Comment by papasanimohansrinivas at 2023-11-27T19:02:38Z>\nyou need to install torch cpu and set device map to cpu in model loading side @wenli135\n</Comment>\n<Comment by morteza102030 at 2023-11-29T13:22:56Z>\n> you need to install torch cpu and set device map to cpu in model loading side @wenli135\r\n\r\nit's possible for you give a  complete example for how run [LLaVA_13b_4bit_vanilla_colab](https://colab.research.google.com/github/camenduru/LLaVA-colab/blob/main/LLaVA_13b_4bit_vanilla_colab.ipynb) without gpu?\n</Comment>\n<Comment by akkimind at 2023-12-01T08:34:52Z>\nI made some changes in the code to run inference on CPU, the model is loading but I am getting an error: \r\n`BF16 weight prepack needs the cpu support avx512bw, avx512vl and avx512dq, please set dtype to torch.float or set weights_prepack to False`\r\nwhile trying to optimize the model(model = ipex.optimize(model, dtype=torch.bfloat16))\r\nIf I set dtype to torch.float, model isn’t supporting it and if set weights_prepack to False, model is taking forever to return response. Is there any Specific CPU which I should use?\n</Comment>\n<Comment by ratan at 2024-01-09T09:06:39Z>\ndid anyone able to run Llava inference on CPU  without installing Intel Extention for Pytorch environment for inference? Any pointer will be really helpful\n</Comment>\n<Comment by feng-intel at 2024-01-17T23:58:12Z>\nHi Ratan\r\nHere is the bare metal intel cpu solution [intel xFasterTransformer](https://github.com/intel/xFasterTransformer) for LLM, but there is no llava support yet. You can try firstly. \r\n[llama.cpp](https://github.com/ggerganov/llama.cpp/pull/3436) also support CPU. We will enable intel dGPU/iGPU later. \r\n\r\nCould you tell why you don't want to use Intel Extention for Pytorch?  Thanks.\n</Comment>\n<Comment by drzraf at 2024-09-01T04:53:16Z>\nTried some of this paths:\r\n- [llama.cpp](https://github.com/ggerganov/llama.cpp/issues/8533). I tried to `convert_hf_to_gguf.py` the HF model (to process `LlavaMistralForCausalLM` the way it does for `LlamaForCausalLM`), but stumbled upon other problems (`Can not map tensor 'model.image_newline'`)\r\n\r\nSo, natively, from HF:\r\n1. With `low_cpu_mem_usage = False`\r\n> `transformers/modeling_utils.py`; `ValueError: Passing along a device_map requires low_cpu_mem_usage=True`\r\n\r\n2. With `low_cpu_mem_usage = True`\r\n> `You can't pass load_in_4bit or load_in_8bit as a kwarg when passing quantization_config argument at the same time`\r\n(mentioned/replied to in https://github.com/haotian-liu/LLaVA/issues/1638)\r\n\r\n3. Fixing the above we get\r\n> `transformers/quantizers/quantizer_bnb_4bit.py` : `ValueError: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model`\r\n\r\nwhich I don't explain because using `load_pretrained_model(load_4bit=True, device='cpu')` leads to a `device_map = {'': 'cpu'}` which is quite clear. Still, we can bypass this adding `llm_int8_enable_fp32_cpu_offload=True` to the `BitsAndBytesConfig`, but does it makes any sense with `load_4bit`?). Well, anyway, it loads (took _only_ 10 minutes)\r\n\r\n\r\n- Now comes [intel-extension-for-pytorch](https://github.com/intel/intel-extension-for-pytorch/blob/3a3fbebc336062962bcad81164ffd97d93c77991/intel_extension_for_pytorch/transformers/models/reference/models.py#L2903) which indeed has a config for this model.\r\n\r\n- Whether `ipex.optimize(inplace=True)` is passed or not (if not, memory footprint is doubled), we get\r\n> `RuntimeError: could not create a primitive descriptor for a convolution forward propagation primitive`\r\n\r\n=> blocked here.\r\n\r\nFinally, regarding https://github.com/intel/xFasterTransformer, it's not quite clear whether it replace or complements intel-extension-for-pytorch [CPU/XPU] and for which specific hardware.\r\n\r\n\r\nIf any one could come up with answers/solutions for at least some of these, that'd be great.\n</Comment>\n<Comment by feng-intel at 2024-09-02T02:08:31Z>\n1. For [intel-extention-for-pytorch](https://github.com/intel/intel-extension-for-pytorch), it supports llava fp32,bf16,int8,int4 on intel CPU, iGPU and dGPU. Any issue , you can report issue on [here](https://github.com/intel/intel-extension-for-pytorch/issues). Someone from Intel will help you. \r\n2. [Ollama](https://github.com/ollama/ollama) has supportted Intel CPU, iGPU, dGPU. You need to build from the source. The below is the llama3.1 steps for your reference.\r\n```\r\n$ git clone https://github.com/ollama/ollama.git\r\n$ source intel/oneapi/setvars.sh\r\n\r\n# Install go\r\n$ wget https://go.dev/dl/go1.23.0.linux-amd64.tar.gz\r\n$ mkdir ~/go_1.23.0 && tar zxf go1.23.0.linux-amd64.tar.gz -C ~/go_1.23.0\r\n$ export PATH=$PATH:~/go_1.23.0/go/bin\r\n\r\n$ cd ollama\r\n$ go generate ./...\r\n$ go build .    # ollama binary will be generated.\r\n\r\n# Option to stop the before ollama service\r\n$ ps -A |grep ollama\r\n$ netstat -aon |grep 11434\r\n$ sudo service ollama stop\r\n\r\n# Start ollama server\r\n$ OLLAMA_INTEL_GPU=1 ./ollama serve   ##if no \"OLLAMA_INTEL_GPU=1\"， it will run on cpu.\r\n\r\n# Start ollama client to test\r\n# Option 1\r\n$ ./ollama run llama3.1\r\n# Option 2\r\n$ curl --noproxy \"localhost\" http://localhost:11434/api/generate -d '{\r\n  \"model\": \"llama3.1\",\r\n  \"prompt\":\"Why is the sky blue?\"\r\n}'\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 864,
    "state": "open",
    "created_by": "liyang-7",
    "created_at": "2023-11-27T13:04:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/864</URL>\n\n<TITLE>[Question] Random seed in mult gpu training</TITLE>\n\n<BODY>### Question\n\nTraining loss is fixed in single GPU training, but it is different in multi GPU training even with fixed seed in TrainingArguments\r\nHow to get same loss and weights in multi GPU training based on current scripts？</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-27T16:14:29Z>\nHi, according to my verification, the seed is fixed for the data loader. And if we start from the same pretrained projector, theoretically there is no other randomness.\r\n\r\nHowever, I observed similar small randomness in training loss (+/- 0.003 in some steps) and can lead to different model outcome in the end, and this leads to fluctuation of some benchmark performance -- MME is the most prominent (I can get +/- 20 from the report 1510 for 7B model, similar for 13B model) and other datasets are mostly stable.\r\n\r\nI am not sure where such randomness come. Any advice is welcomed.\n</Comment>\n<Comment by liyang-7 at 2023-11-28T01:59:27Z>\nIn the first pre-train stage, the data loader and the initiate projector weights is fixed with the same seed in TrainingArguments. The loss will be different in multi-gpu training. It may be due to the **randomness in DeepSpeed or Transformers Trainer**. I haven't found a way to fix this randomness yet either.\r\nFrom my experience, longer training processes usually result in better outcomes. However, the training outcomes become more unstable.\r\nIn one of the versions, llava achieved 1911 (1603+308) of MME, but it couldn't be reproduced.\r\n<img width=\"465\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/28669941/24e81ec2-549d-4769-96e1-19d0d220917b\">\n</Comment>\n<Comment by eehover at 2023-12-06T12:07:57Z>\nthis should be caused by the deepspeed. The loss is exactly aligned by setting  the zero1 (stage1)\n</Comment>\n<Comment by ShyFoo at 2023-12-30T02:34:37Z>\n> In the first pre-train stage, the data loader and the initiate projector weights is fixed with the same seed in TrainingArguments. The loss will be different in multi-gpu training. It may be due to the **randomness in DeepSpeed or Transformers Trainer**. I haven't found a way to fix this randomness yet either. From my experience, longer training processes usually result in better outcomes. However, the training outcomes become more unstable. In one of the versions, llava achieved 1911 (1603+308) of MME, but it couldn't be reproduced. <img alt=\"image\" width=\"465\" src=\"https://private-user-images.githubusercontent.com/28669941/286086869-24e81ec2-549d-4769-96e1-19d0d220917b.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDM5MDM3NzEsIm5iZiI6MTcwMzkwMzQ3MSwicGF0aCI6Ii8yODY2OTk0MS8yODYwODY4NjktMjRlODFlYzItNTQ5ZC00NzY5LTk2ZTEtMTlkMGQyMjA5MTdiLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyMzEyMzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjMwVDAyMzExMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTBmZGRkMTFkMzU0YjZmNWM0YWJkYjE4YjgyZTUyYzNjOThjNmEyZjFlODYyZmY4Mzc0ZjIwODVmZDJlNGFjZGYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Q-YW-QeLcPloZNVjreUao6qDjsaAbryznL6o3g8wA60\">\r\n\r\nFrom my experience, longer training processes usually result in better outcomes. However, the training outcomes become more unstable. I also encountered this type of problem. May I ask you how to solve it?\n</Comment>\n<Comment by WEIYanbin1999 at 2024-01-02T06:54:25Z>\nSorry to bother you. I used a **0.8k sample custom dataset** and use **finetune_task_lora.sh** to fine-tune. After **fix [cuda, torch, cudnn, numpy, random] seeds**, and use **single GPU** to avoid randomness caused by parallel, I found performance still **wave** a lot, one run is 0.62 accuracy, then another run with same command become 0.87.\r\nI am also confusing where the randomness come from?? Sincerely wish for some helps from you guys.\n</Comment>\n<Comment by eehover at 2024-01-02T07:04:50Z>\n> Sorry to bother you. I used a **0.8k sample custom dataset** and use **finetune_task_lora.sh** to fine-tune. After **fix [cuda, torch, cudnn, numpy, random] seeds**, and use **single GPU** to avoid randomness caused by parallel, I found performance still **wave** a lot, one run is 0.62 accuracy, then another run with same command become 0.87. I am also confusing where the randomness come from?? Sincerely wish for some helps from you guys.\r\n\r\nYou should check the loss after the each step, you must make sure the loss is equal in the training step between two same experiments\n</Comment>\n<Comment by WEIYanbin1999 at 2024-01-02T14:07:58Z>\n> > Sorry to bother you. I used a **0.8k sample custom dataset** and use **finetune_task_lora.sh** to fine-tune. After **fix [cuda, torch, cudnn, numpy, random] seeds**, and use **single GPU** to avoid randomness caused by parallel, I found performance still **wave** a lot, one run is 0.62 accuracy, then another run with same command become 0.87. I am also confusing where the randomness come from?? Sincerely wish for some helps from you guys.\r\n> \r\n> You should check the loss after the each step, you must make sure the loss is equal in the training step between two same experiments\r\n\r\nYeap. The loss after each step are different. However, we have try our best to reduce randomness. \r\n1. Fix seed in [random.numpy,cuda,torch] \r\n2. Set cudnn.deterministic True and cudnn.benchmark False\r\n![8dc3a212f34d0813eae8ebe50951a1a](https://github.com/haotian-liu/LLaVA/assets/127707751/60692e74-40bb-41f5-ae9b-4e27c6213a3c)\r\n\r\n3. change stage inside deepseed zero2.sh and zero3.sh as 0\r\n\r\nAnd then the loss in one run:\r\n![f0b19c545b4c1a2bd87304deca53f5a](https://github.com/haotian-liu/LLaVA/assets/127707751/5449c0ea-495f-4b47-aca4-ccabb7b3e627)\r\nAnother run loss:\r\n![2e19eb835ff8b818190a938f497ac7c](https://github.com/haotian-liu/LLaVA/assets/127707751/d1b18f72-27ec-4356-b2a3-e46f7c44eb77)\n</Comment>\n<Comment by eehover at 2024-01-03T03:11:23Z>\n> > > Sorry to bother you. I used a **0.8k sample custom dataset** and use **finetune_task_lora.sh** to fine-tune. After **fix [cuda, torch, cudnn, numpy, random] seeds**, and use **single GPU** to avoid randomness caused by parallel, I found performance still **wave** a lot, one run is 0.62 accuracy, then another run with same command become 0.87. I am also confusing where the randomness come from?? Sincerely wish for some helps from you guys.\r\n> > \r\n> > \r\n> > You should check the loss after the each step, you must make sure the loss is equal in the training step between two same experiments\r\n> \r\n> Yeap. The loss after each step are different. However, we have try our best to reduce randomness.\r\n> \r\n> 1. Fix seed in [random.numpy,cuda,torch]\r\n> 2. Set cudnn.deterministic True and cudnn.benchmark False\r\n>    ![8dc3a212f34d0813eae8ebe50951a1a](https://private-user-images.githubusercontent.com/127707751/293709823-60692e74-40bb-41f5-ae9b-4e27c6213a3c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDQyNTEyMzIsIm5iZiI6MTcwNDI1MDkzMiwicGF0aCI6Ii8xMjc3MDc3NTEvMjkzNzA5ODIzLTYwNjkyZTc0LTQwYmItNDFmNS1hZTliLTRlMjdjNjIxM2EzYy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMTAzJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDEwM1QwMzAyMTJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0zNTkyOTg2NTJlYWJmZGM3YThlNmZlNzVlOGMzYjIwZTIwMmNiMGNmYjllM2U4YjMxNmYxNDZjZWZiZDFlZjNjJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.jOYi38WE3n4YhR6ZXQNPmwRUFKAW8c8aN7eJAkQRvpc)\r\n> 3. change stage inside deepseed zero2.sh and zero3.sh as 0\r\n> \r\n> And then the loss in one run: ![f0b19c545b4c1a2bd87304deca53f5a](https://private-user-images.githubusercontent.com/127707751/293709629-5449c0ea-495f-4b47-aca4-ccabb7b3e627.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDQyNTEyMzIsIm5iZiI6MTcwNDI1MDkzMiwicGF0aCI6Ii8xMjc3MDc3NTEvMjkzNzA5NjI5LTU0NDljMGVhLTQ5NWYtNGI0Ny1hY2E0LWNjYWJiN2IzZTYyNy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMTAzJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDEwM1QwMzAyMTJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT05ZWM2NDQyNmFjZDRmNGQwMWZhOGY3ZjRmZDEzNDYwMGVmOWMwMTRhNjk5ZTRiMTZkY2RiOTNhY2Y5YTE4N2Q5JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.yjHrayXkl4R3F-S_DbUq5GnSZlGQumVrRvlR0hs7PpQ) Another run loss: ![2e19eb835ff8b818190a938f497ac7c](https://private-user-images.githubusercontent.com/127707751/293709693-d1b18f72-27ec-4356-b2a3-e46f7c44eb77.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDQyNTEyMzIsIm5iZiI6MTcwNDI1MDkzMiwicGF0aCI6Ii8xMjc3MDc3NTEvMjkzNzA5NjkzLWQxYjE4ZjcyLTI3ZWMtNDM1Ni1iMmEzLWU0NmY3YzQ0ZWI3Ny5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMTAzJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDEwM1QwMzAyMTJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1jZTZmYjA2ZjBkYjU0ZmRhMDQ2ZjljYWQ2ODY0OTRjM2MwNGY1MjhiYmFmNjA4NTE4NWM2NDI1OThjZjcwNzlkJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.t-oe7tc2uwEMXoN_FgCtDT7pMVFvoXC_kw8P-_er6Ws)\r\n\r\nI also use the same seed fixed strategy. I can keep the loss same after each step using stage 1 and stage3 (single gpu or multi-gpu). \r\n\r\nYou must make sure the initialized weight between two experiments are same(you can calculate the distance).\r\n\r\nAfter this, you can also try the following stage 1 script\r\nstage 1 script:\r\n{\r\n    \"fp16\": {\r\n        \"enabled\": \"auto\",\r\n        \"loss_scale\": 0,\r\n        \"loss_scale_window\": 1000,\r\n        \"initial_scale_power\": 16,\r\n        \"hysteresis\": 2,\r\n        \"min_loss_scale\": 1\r\n    },\r\n    \"bf16\": {\r\n        \"enabled\": \"auto\"\r\n    },\r\n    \"train_micro_batch_size_per_gpu\": \"auto\",\r\n    \"train_batch_size\": \"auto\",\r\n    \"gradient_accumulation_steps\": \"auto\",\r\n    \"zero_optimization\": {\r\n        \"stage\": 1,\r\n        \"overlap_comm\": true,\r\n        \"contiguous_gradients\": true,\r\n        \"sub_group_size\": 1e9,\r\n        \"reduce_bucket_size\": \"auto\"\r\n    }\r\n}\n</Comment>\n<Comment by ShyFoo at 2024-01-03T03:33:50Z>\n> > > > Sorry to bother you. I used a **0.8k sample custom dataset** and use **finetune_task_lora.sh** to fine-tune. After **fix [cuda, torch, cudnn, numpy, random] seeds**, and use **single GPU** to avoid randomness caused by parallel, I found performance still **wave** a lot, one run is 0.62 accuracy, then another run with same command become 0.87. I am also confusing where the randomness come from?? Sincerely wish for some helps from you guys.\r\n> > > \r\n> > > \r\n> > > You should check the loss after the each step, you must make sure the loss is equal in the training step between two same experiments\r\n> > \r\n> > \r\n> > Yeap. The loss after each step are different. However, we have try our best to reduce randomness.\r\n> > \r\n> > 1. Fix seed in [random.numpy,cuda,torch]\r\n> > 2. Set cudnn.deterministic True and cudnn.benchmark False\r\n> >    ![8dc3a212f34d0813eae8ebe50951a1a](https://private-user-images.githubusercontent.com/127707751/293709823-60692e74-40bb-41f5-ae9b-4e27c6213a3c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDQyNTEyMzIsIm5iZiI6MTcwNDI1MDkzMiwicGF0aCI6Ii8xMjc3MDc3NTEvMjkzNzA5ODIzLTYwNjkyZTc0LTQwYmItNDFmNS1hZTliLTRlMjdjNjIxM2EzYy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMTAzJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDEwM1QwMzAyMTJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0zNTkyOTg2NTJlYWJmZGM3YThlNmZlNzVlOGMzYjIwZTIwMmNiMGNmYjllM2U4YjMxNmYxNDZjZWZiZDFlZjNjJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.jOYi38WE3n4YhR6ZXQNPmwRUFKAW8c8aN7eJAkQRvpc)\r\n> > 3. change stage inside deepseed zero2.sh and zero3.sh as 0\r\n> > \r\n> > And then the loss in one run: ![f0b19c545b4c1a2bd87304deca53f5a](https://private-user-images.githubusercontent.com/127707751/293709629-5449c0ea-495f-4b47-aca4-ccabb7b3e627.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDQyNTEyMzIsIm5iZiI6MTcwNDI1MDkzMiwicGF0aCI6Ii8xMjc3MDc3NTEvMjkzNzA5NjI5LTU0NDljMGVhLTQ5NWYtNGI0Ny1hY2E0LWNjYWJiN2IzZTYyNy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMTAzJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDEwM1QwMzAyMTJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT05ZWM2NDQyNmFjZDRmNGQwMWZhOGY3ZjRmZDEzNDYwMGVmOWMwMTRhNjk5ZTRiMTZkY2RiOTNhY2Y5YTE4N2Q5JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.yjHrayXkl4R3F-S_DbUq5GnSZlGQumVrRvlR0hs7PpQ) Another run loss: ![2e19eb835ff8b818190a938f497ac7c](https://private-user-images.githubusercontent.com/127707751/293709693-d1b18f72-27ec-4356-b2a3-e46f7c44eb77.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDQyNTEyMzIsIm5iZiI6MTcwNDI1MDkzMiwicGF0aCI6Ii8xMjc3MDc3NTEvMjkzNzA5NjkzLWQxYjE4ZjcyLTI3ZWMtNDM1Ni1iMmEzLWU0NmY3YzQ0ZWI3Ny5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMTAzJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDEwM1QwMzAyMTJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1jZTZmYjA2ZjBkYjU0ZmRhMDQ2ZjljYWQ2ODY0OTRjM2MwNGY1MjhiYmFmNjA4NTE4NWM2NDI1OThjZjcwNzlkJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.t-oe7tc2uwEMXoN_FgCtDT7pMVFvoXC_kw8P-_er6Ws)\r\n> \r\n> I also use the same seed fixed strategy. I can keep the loss same after each step using stage 1 and stage3 (single gpu or multi-gpu).\r\n> \r\n> You must make sure the initialized weight between two experiments are same(you can calculate the distance).\r\n> \r\n> After this, you can also try the following stage 1 script stage 1 script: { \"fp16\": { \"enabled\": \"auto\", \"loss_scale\": 0, \"loss_scale_window\": 1000, \"initial_scale_power\": 16, \"hysteresis\": 2, \"min_loss_scale\": 1 }, \"bf16\": { \"enabled\": \"auto\" }, \"train_micro_batch_size_per_gpu\": \"auto\", \"train_batch_size\": \"auto\", \"gradient_accumulation_steps\": \"auto\", \"zero_optimization\": { \"stage\": 1, \"overlap_comm\": true, \"contiguous_gradients\": true, \"sub_group_size\": 1e9, \"reduce_bucket_size\": \"auto\" } }\r\n\r\nWhat about your computation precision? Do you set bf16 and tf32 as True?\n</Comment>\n<Comment by WEIYanbin1999 at 2024-01-03T05:13:59Z>\n> > > > > Sorry to bother you. I used a **0.8k sample custom dataset** and use **finetune_task_lora.sh** to fine-tune. After **fix [cuda, torch, cudnn, numpy, random] seeds**, and use **single GPU** to avoid randomness caused by parallel, I found performance still **wave** a lot, one run is 0.62 accuracy, then another run with same command become 0.87. I am also confusing where the randomness come from?? Sincerely wish for some helps from you guys.\r\n> > > > \r\n> > > > \r\n> > > > You should check the loss after the each step, you must make sure the loss is equal in the training step between two same experiments\r\n> > > \r\n> > > \r\n> > > Yeap. The loss after each step are different. However, we have try our best to reduce randomness.\r\n> > > \r\n> > > 1. Fix seed in [random.numpy,cuda,torch]\r\n> > > 2. Set cudnn.deterministic True and cudnn.benchmark False\r\n> > >    ![8dc3a212f34d0813eae8ebe50951a1a](https://private-user-images.githubusercontent.com/127707751/293709823-60692e74-40bb-41f5-ae9b-4e27c6213a3c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDQyNTEyMzIsIm5iZiI6MTcwNDI1MDkzMiwicGF0aCI6Ii8xMjc3MDc3NTEvMjkzNzA5ODIzLTYwNjkyZTc0LTQwYmItNDFmNS1hZTliLTRlMjdjNjIxM2EzYy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMTAzJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDEwM1QwMzAyMTJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0zNTkyOTg2NTJlYWJmZGM3YThlNmZlNzVlOGMzYjIwZTIwMmNiMGNmYjllM2U4YjMxNmYxNDZjZWZiZDFlZjNjJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.jOYi38WE3n4YhR6ZXQNPmwRUFKAW8c8aN7eJAkQRvpc)\r\n> > > 3. change stage inside deepseed zero2.sh and zero3.sh as 0\r\n> > > \r\n> > > And then the loss in one run: ![f0b19c545b4c1a2bd87304deca53f5a](https://private-user-images.githubusercontent.com/127707751/293709629-5449c0ea-495f-4b47-aca4-ccabb7b3e627.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDQyNTEyMzIsIm5iZiI6MTcwNDI1MDkzMiwicGF0aCI6Ii8xMjc3MDc3NTEvMjkzNzA5NjI5LTU0NDljMGVhLTQ5NWYtNGI0Ny1hY2E0LWNjYWJiN2IzZTYyNy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMTAzJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDEwM1QwMzAyMTJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT05ZWM2NDQyNmFjZDRmNGQwMWZhOGY3ZjRmZDEzNDYwMGVmOWMwMTRhNjk5ZTRiMTZkY2RiOTNhY2Y5YTE4N2Q5JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.yjHrayXkl4R3F-S_DbUq5GnSZlGQumVrRvlR0hs7PpQ) Another run loss: ![2e19eb835ff8b818190a938f497ac7c](https://private-user-images.githubusercontent.com/127707751/293709693-d1b18f72-27ec-4356-b2a3-e46f7c44eb77.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDQyNTEyMzIsIm5iZiI6MTcwNDI1MDkzMiwicGF0aCI6Ii8xMjc3MDc3NTEvMjkzNzA5NjkzLWQxYjE4ZjcyLTI3ZWMtNDM1Ni1iMmEzLWU0NmY3YzQ0ZWI3Ny5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMTAzJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDEwM1QwMzAyMTJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1jZTZmYjA2ZjBkYjU0ZmRhMDQ2ZjljYWQ2ODY0OTRjM2MwNGY1MjhiYmFmNjA4NTE4NWM2NDI1OThjZjcwNzlkJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.t-oe7tc2uwEMXoN_FgCtDT7pMVFvoXC_kw8P-_er6Ws)\r\n> > \r\n> > \r\n> > I also use the same seed fixed strategy. I can keep the loss same after each step using stage 1 and stage3 (single gpu or multi-gpu).\r\n> > You must make sure the initialized weight between two experiments are same(you can calculate the distance).\r\n> > After this, you can also try the following stage 1 script stage 1 script: { \"fp16\": { \"enabled\": \"auto\", \"loss_scale\": 0, \"loss_scale_window\": 1000, \"initial_scale_power\": 16, \"hysteresis\": 2, \"min_loss_scale\": 1 }, \"bf16\": { \"enabled\": \"auto\" }, \"train_micro_batch_size_per_gpu\": \"auto\", \"train_batch_size\": \"auto\", \"gradient_accumulation_steps\": \"auto\", \"zero_optimization\": { \"stage\": 1, \"overlap_comm\": true, \"contiguous_gradients\": true, \"sub_group_size\": 1e9, \"reduce_bucket_size\": \"auto\" } }\r\n> \r\n> What about your computation precision? Do you set bf16 and tf32 as True?\r\n\r\nYeap. We set bf16 and tf32 true before, we are trying to set them false to get more precision. Thanks for your advice.\n</Comment>\n<Comment by jaewonalive at 2024-10-02T11:58:21Z>\nIs this issue related to this problem?\r\n\r\n[https://github.com/haotian-liu/LLaVA/issues/1440#issue-2255784727](https://github.com/haotian-liu/LLaVA/issues/1440#issue-2255784727)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 863,
    "state": "open",
    "created_by": "parap1uie-s",
    "created_at": "2023-11-27T08:19:29Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/863</URL>\n\n<TITLE>[Question] Any magic in visual grounding?</TITLE>\n\n<BODY>### Question\r\n\r\nI find LLaVA can not only output text, but also output the coordinates using the prompt like:\r\n```\r\nPlease output the coordinate position of the person in the picture.\r\n```\r\n<img width=\"514\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/23453851/bfde6a69-9d8b-485f-97fe-c0b09161f0cd\">\r\n\r\n\r\nI'm wander is there any training tricks for coordinates for LLaVA? \r\nOr: \r\n1. Just take the coordinates as `plain text`?\r\n2. Take the output of Grounding DINO?\r\n\r\nIt is a magical thing in my knowledge that the model can learn the relationship between coordinate values and offset through plain text (the larger the value, the closer it is to the lower right corner of the image).\r\n\r\nThanks in advance!</BODY>\n\n<COMMENTS>\n<Comment by Etelis at 2023-11-27T13:36:13Z>\nWell in the paper they actually trained it with bounding box.. I guess this is kinda the outcome of this.\n</Comment>\n<Comment by FHL1998 at 2023-11-28T06:13:51Z>\n> Well in the paper they actually trained it with bounding box.. I guess this is kinda the outcome of this.\r\n\r\nI don't think they trained with bbox coordinates, in the original work, they just introduced coordinates to prompt GPT-4 for spatial relationship understanding.\n</Comment>\n<Comment by Etelis at 2023-11-28T08:09:06Z>\n> > Well in the paper they actually trained it with bounding box.. I guess this is kinda the outcome of this.\r\n> \r\n> I don't think they trained with bbox coordinates, in the original work, they just introduced coordinates to prompt GPT-4 for spatial relationship understanding.\r\n\r\nThat's what I meant, but I guess you're correct, they did not provide the model with those coordinates, rethinking this, that's actually an interesting outcome\n</Comment>\n<Comment by parap1uie-s at 2023-11-28T11:05:45Z>\n> > > Well in the paper they actually trained it with bounding box.. I guess this is kinda the outcome of this.\r\n> > \r\n> > \r\n> > I don't think they trained with bbox coordinates, in the original work, they just introduced coordinates to prompt GPT-4 for spatial relationship understanding.\r\n> \r\n> That's what I meant, but I guess you're correct, they did not provide the model with those coordinates, rethinking this, that's actually an interesting outcome\r\n\r\nI re-read the report of LLaVA-plus, the LLaVA-plus seems be an agent, which output a specific json ( command ) to run the grounding models like Grounding DINO. Then integrate the output of DINO into prompt and re-generate the text for user.\r\n\r\nIf I have any misunderstandings, please point them out.\n</Comment>\n<Comment by Linziyang1999 at 2023-11-29T03:08:20Z>\nwhat a interesting finding, after check their dataset, i didnt find even 1 data about grounding.\n</Comment>\n<Comment by henrycjh at 2023-11-29T05:58:54Z>\n> what a interesting finding, after check their dataset, i didnt find even 1 data about grounding.\r\n\r\n@Linziyang1999 @FHL1998 @parap1uie-s \r\nWell, in LLaVA-v1.5, they do use data about grounding and refering, you could check the data they constructed using VG and RefCOCO.\n</Comment>\n<Comment by Linziyang1999 at 2023-11-29T06:04:28Z>\n> > what a interesting finding, after check their dataset, i didnt find even 1 data about grounding.\r\n> \r\n> @Linziyang1999 @FHL1998 @parap1uie-s Well, in LLaVA-v1.5, they do use data about grounding and refering, you could check the data they constructed using VG and RefCOCO.\r\n\r\nI mean grounding specific coordination\n</Comment>\n<Comment by henrycjh at 2023-11-29T06:10:06Z>\n@Linziyang1999 What do you mean by grounding specific coordination? Is the image below shows about grounding specific coordination?\r\n![image](https://github.com/haotian-liu/LLaVA/assets/73419275/1f70c985-bfd6-47fa-9366-fdb64f633223)\n</Comment>\n<Comment by Linziyang1999 at 2023-11-29T06:20:39Z>\n> What do you mean by grounding specific coordination? Is the image below shows about grounding specific coordination?\r\n> ![image](https://private-user-images.githubusercontent.com/73419275/286493878-1f70c985-bfd6-47fa-9366-fdb64f633223.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDEyMzg4ODQsIm5iZiI6MTcwMTIzODU4NCwicGF0aCI6Ii83MzQxOTI3NS8yODY0OTM4NzgtMWY3MGM5ODUtYmZkNi00N2ZhLTkzNjYtZmRiNjRmNjMzMjIzLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzExMjklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMTI5VDA2MTYyNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTNlMDkxZTJmOGVkODVhMDczN2ZjYTkzN2NhMGY3NjBlOGFlZjg3ZWZiZjM3ODkyMDI0ZWI2NTEzOWRjNjMxMjEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.agwd-FR0osR89rHlaH7T-9CIOWgHeFTA3DTTNxPOFEo)\r\n\r\nI didn't see this in dataset mix665k and blip_laion558k, could you tell how can I find the dataset. Is it the dataset used in training llava-llama-preview ? TAT\n</Comment>\n<Comment by henrycjh at 2023-11-29T06:41:25Z>\n> > What do you mean by grounding specific coordination? Is the image below shows about grounding specific coordination?\r\n> > ![image](https://private-user-images.githubusercontent.com/73419275/286493878-1f70c985-bfd6-47fa-9366-fdb64f633223.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDEyMzg4ODQsIm5iZiI6MTcwMTIzODU4NCwicGF0aCI6Ii83MzQxOTI3NS8yODY0OTM4NzgtMWY3MGM5ODUtYmZkNi00N2ZhLTkzNjYtZmRiNjRmNjMzMjIzLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzExMjklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMTI5VDA2MTYyNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTNlMDkxZTJmOGVkODVhMDczN2ZjYTkzN2NhMGY3NjBlOGFlZjg3ZWZiZjM3ODkyMDI0ZWI2NTEzOWRjNjMxMjEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.agwd-FR0osR89rHlaH7T-9CIOWgHeFTA3DTTNxPOFEo)\r\n> \r\n> I didn't see this in dataset mix665k and blip_laion558k, could you tell how can I find the dataset. Is it the dataset used in training llava-llama-preview ? TAT\r\n\r\n@Linziyang1999 But it is in the _mix665k_ . You could search the key words ' bounding box coordinate ' in the value of conversations.\n</Comment>\n<Comment by Linziyang1999 at 2023-11-29T06:58:24Z>\n> > > What do you mean by grounding specific coordination? Is the image below shows about grounding specific coordination?\r\n> > > ![image](https://private-user-images.githubusercontent.com/73419275/286493878-1f70c985-bfd6-47fa-9366-fdb64f633223.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDEyMzg4ODQsIm5iZiI6MTcwMTIzODU4NCwicGF0aCI6Ii83MzQxOTI3NS8yODY0OTM4NzgtMWY3MGM5ODUtYmZkNi00N2ZhLTkzNjYtZmRiNjRmNjMzMjIzLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzExMjklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMTI5VDA2MTYyNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTNlMDkxZTJmOGVkODVhMDczN2ZjYTkzN2NhMGY3NjBlOGFlZjg3ZWZiZjM3ODkyMDI0ZWI2NTEzOWRjNjMxMjEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.agwd-FR0osR89rHlaH7T-9CIOWgHeFTA3DTTNxPOFEo)\r\n> > \r\n> > \r\n> > I didn't see this in dataset mix665k and blip_laion558k, could you tell how can I find the dataset. Is it the dataset used in training llava-llama-preview ? TAT\r\n> \r\n> @Linziyang1999 But it is in the _mix665k_ . You could search the key words ' bounding box coordinate ' in the value of conversations.\r\n\r\nThank you!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 862,
    "state": "closed",
    "created_by": "CrazyBrick",
    "created_at": "2023-11-27T06:05:26Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/862</URL>\n\n<TITLE>[Usage] error about Chinese data finetuning</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nThere is no problem fine-tuning using the provided dataset, but using local **Chinese data** will report this error, which is the same as the issue [134](https://github.com/haotian-liu/LLaVA/issues/134) issue, but it has been closed.\r\n\r\nCommand:\r\n```\r\nsh finetune_lora.sh[change data_path and image_folder]\r\n```\r\n[finetune_lora.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task_lora.sh)\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/workspace/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/workspace/LLaVA/llava/train/train.py\", line 932, in train\r\n    trainer.train()\r\n  File \"/xxx/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/xxx/python3.10/site-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/xxx/python3.10/site-packages/transformers/trainer.py\", line 2654, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/xxx/python3.10/site-packages/transformers/trainer.py\", line 2679, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/xxx/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/xxx/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/xxx/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1735, in forward\r\n    loss = self.module(*inputs, **kwargs)\r\n  File \"/xxx/python3.10/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/xxx/python3.10/site-packages/peft/peft_model.py\", line 922, in forward\r\n    return self.base_model(\r\n  File \"/xxx/python3.10/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/workspace/LLaVA/llava/model/language_model/llava_llama.py\", line 79, in forward\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/workspace/LLaVA/llava/model/llava_arch.py\", line 178, in prepare_inputs_labels_for_multimodal\r\n    cur_image_features = image_features[cur_image_idx]\r\nIndexError: index 16 is out of bounds for dimension 0 with size 16.\r\n```\r\nI've tried to delete all explicit '\\n\\' in the json file.For example:\r\n`\"value\": \"<image>\\n图中的人是xxx,请以这个人为主体描述一下图中内容\"`-->`\"value\": \"<image>图中的人是xxx,请以这个人为主体描述一下图中内容\"`\r\nbut it doen't work.</BODY>\n\n<COMMENTS>\n<Comment by Etelis at 2023-11-27T14:42:37Z>\nHad the same issue today with english data, problems were: \\n\r\nand also \"\\ in some cases. \r\n\r\ncheck those.\n</Comment>\n<Comment by CrazyBrick at 2023-11-27T14:54:02Z>\n> Had the same issue today with english data, problems were: \\n and also \"\\ in some cases.\r\n> \r\n> check those.\r\n\r\nthank you.I checked before,but there is no '\\' except '\\n'.\r\nI print some vars\r\n```\r\n            for i in range(num_images + 1):\r\n                cur_new_input_embeds.append(cur_input_embeds_no_im[i])\r\n                cur_new_labels.append(cur_labels_noim[i])\r\n                if i < num_images:\r\n                    print(f\"Batch Index: {batch_idx}\\n, Current Image Index: {cur_image_idx}\\n, Num Images: {num_images}\")\r\n```\r\nin file`LLaVA/llava/model/llava_arch.py`,line 178.\r\nwhen it runs normally, `Num Images: 1`\r\nwhen it runs with error, `Num Images: 4`\n</Comment>\n<Comment by bingwork at 2023-11-28T10:06:00Z>\nsuggest to make sure one sample is right at https://github.com/haotian-liu/LLaVA/blob/main/llava/train/train.py#L402 (for example, based on which conversation template you choose)\r\nif the input_ids and labels/targets are as expected, then there will be no error in the later processes. @CrazyBrick\n</Comment>\n<Comment by henrycjh at 2023-11-29T05:48:30Z>\n> > Had the same issue today with english data, problems were: \\n and also \"\\ in some cases.\r\n> > check those.\r\n> \r\n> thank you.I checked before,but there is no '' except '\\n'. I print some vars\r\n> \r\n> ```\r\n>             for i in range(num_images + 1):\r\n>                 cur_new_input_embeds.append(cur_input_embeds_no_im[i])\r\n>                 cur_new_labels.append(cur_labels_noim[i])\r\n>                 if i < num_images:\r\n>                     print(f\"Batch Index: {batch_idx}\\n, Current Image Index: {cur_image_idx}\\n, Num Images: {num_images}\")\r\n> ```\r\n> \r\n> in file`LLaVA/llava/model/llava_arch.py`,line 178. when it runs normally, `Num Images: 1` when it runs with error, `Num Images: 4`\r\n\r\n@CrazyBrick Based on your message, I think you have similar problems like me, and it is because the data preprocessing is wrong. You should check that for every instance in your data list, mostly in multi turn conversations, the tag \\<image\\> should appear only once.\n</Comment>\n<Comment by CrazyBrick at 2023-11-29T06:15:49Z>\n> > > Had the same issue today with english data, problems were: \\n and also \"\\ in some cases.\r\n> > > check those.\r\n> > \r\n> > \r\n> > thank you.I checked before,but there is no '' except '\\n'. I print some vars\r\n> > ```\r\n> >             for i in range(num_images + 1):\r\n> >                 cur_new_input_embeds.append(cur_input_embeds_no_im[i])\r\n> >                 cur_new_labels.append(cur_labels_noim[i])\r\n> >                 if i < num_images:\r\n> >                     print(f\"Batch Index: {batch_idx}\\n, Current Image Index: {cur_image_idx}\\n, Num Images: {num_images}\")\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > in file`LLaVA/llava/model/llava_arch.py`,line 178. when it runs normally, `Num Images: 1` when it runs with error, `Num Images: 4`\r\n> \r\n> @CrazyBrick Based on your message, I think you have similar problems like me, and it is because the data preprocessing is wrong. You should check that for every instance in your data list, mostly in multi turn conversations, the tag <image> should appear only once.\r\n\r\nthx, your guess is correct, I do have **multi-turns** of dialogue and multiple<image>**tags**. Yesterday, I regenerated a version that only had one dialogue. The **Num_Images** have become what I expected, but the index will still increase until `IndexError: index 16 is out of bounds for dimension 0 with size 16.`. I have no clue...\n</Comment>\n<Comment by henrycjh at 2023-11-29T06:35:14Z>\n@CrazyBrick That is weird, maybe you should find a way to print out the actual instance that cause this error. Because it loads the images and texts seperately, which means the length of **image_features** is fixed (e.g. the length of  **image_features** is 16 in your case) before dealing with the texts. And in one batch, the cur_image_idx should only increase in two cases, first is that there is no image in this instace, second is that there is only one \\<image\\> in the instance which makes the cur_image_idx increases only once in one instance, so if there are two or more tags in the instance, the cur_image_idx would increase twice or more and then exceed the bound of already fixed **image_features**.\n</Comment>\n<Comment by CrazyBrick at 2023-11-30T13:09:38Z>\n> @CrazyBrick That is weird, maybe you should find a way to print out the actual instance that cause this error. Because it loads the images and texts seperately, which means the length of **image_features** is fixed (e.g. the length of **image_features** is 16 in your case) before dealing with the texts. And in one batch, the cur_image_idx should only increase in two cases, first is that there is no image in this instace, second is that there is only one <image> in the instance which makes the cur_image_idx increases only once in one instance, so if there are two or more tags in the instance, the cur_image_idx would increase twice or more and then exceed the bound of already fixed **image_features**.\r\n\r\nthanks @henrycjh \r\nquite weird, I debug for a while but I didn't find anything strange.But after regenerating the custom dataset for multi-tunes of dialogue with 1 tag, it seems like there are no errors. I don't understand, but in the end, it can run.\r\n\r\nBut I have to admit that my fine-tuning did not achieve the expected effect and seems to have no effect. I don't know if it's due to too little data or other parameter reasons.\n</Comment>\n<Comment by henrycjh at 2023-12-01T07:35:03Z>\n@CrazyBrick  Glad that you make it work. I guess the main reason that fine-tuning has no effect is that the LLM is pretrained mainly on English data, so it may still have no effect even you have a lot of Chinese data during fine-tuing stage.\n</Comment>\n<Comment by CrazyBrick at 2023-12-04T06:23:37Z>\n> @CrazyBrick Glad that you make it work. I guess the main reason that fine-tuning has no effect is that the LLM is pretrained mainly on English data, so it may still have no effect even you have a lot of Chinese data during fine-tuing stage.\r\n\r\n@henrycjh thank you for your help, I selected a portion from the dataset to generate **English data**, but the **prediction is quite poor.**\r\nI am confused about **how to achieve effective finetune**, how many constraints are needed, and what kind of expectations can be achieved\r\nI've proposed a new issue #884\n</Comment>\n<Comment by ScottishFold007 at 2024-04-08T01:46:47Z>\n> > @CrazyBrick Glad that you make it work. I guess the main reason that fine-tuning has no effect is that the LLM is pretrained mainly on English data, so it may still have no effect even you have a lot of Chinese data during fine-tuing stage.\r\n> \r\n> @henrycjh thank you for your help, I selected a portion from the dataset to generate **English data**, but the **prediction is quite poor.** I am confused about **how to achieve effective finetune**, how many constraints are needed, and what kind of expectations can be achieved I've proposed a new issue #884\r\n\r\nDid you do the 2 stage training from zero to one? In that case, intervening from stage 1 with Chinese data and mixing in some in stage 2 might work much better. That's what I did myself, and my Chinese skills still improved visibly\n</Comment>\n<Comment by ScottishFold007 at 2024-04-08T02:33:22Z>\n> > @CrazyBrick Glad that you make it work. I guess the main reason that fine-tuning has no effect is that the LLM is pretrained mainly on English data, so it may still have no effect even you have a lot of Chinese data during fine-tuing stage.\r\n> \r\n> @henrycjh thank you for your help, I selected a portion from the dataset to generate **English data**, but the **prediction is quite poor.** I am confused about **how to achieve effective finetune**, how many constraints are needed, and what kind of expectations can be achieved I've proposed a new issue #884\r\n\r\nI'm assuming you're translating the Chinese data from English? That's what I did, so I ran into a situation where multiple <images> appeared in a single session.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 861,
    "state": "open",
    "created_by": "LetsGoFir",
    "created_at": "2023-11-27T03:58:39Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/861</URL>\n\n<TITLE>Any plan to upgrade transformers? [Feature request]</TITLE>\n\n<BODY>### feature\n\nThere are some bugs in old version of transformers, especially when resume, like this one https://github.com/huggingface/transformers/issues/24656#issuecomment-1733069714\r\n\r\nDo you have any plans to upgrade transformers?\r\n\r\nAnother thing is, could you please change the `local_rank==0` to `rank==0` in your codes to support multi-nodes training, thanks!</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 860,
    "state": "closed",
    "created_by": "Dygitz",
    "created_at": "2023-11-26T16:49:57Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/860</URL>\n\n<TITLE>[Question] Interested in Auto-Starting Flask API in bash scripts</TITLE>\n\n<BODY>### Question\n\nI tried to make a function in the start.sh file that would automatically start the Flask AP. However, the docker container crashes when I start with this included functionality. Any tips? (I'm not very good with Bash)\r\n\r\nstart_llava_api() {\r\n    echo \"Starting API service...\"\r\n    # Stop model worker and controller to free up VRAM\r\n    fuser -k 10000/tcp 40000/tcp\r\n\r\n    # Install dependencies\r\n    source /workspace/venv/bin/activate\r\n    pip3 install flask protobuf\r\n\r\n    # Start the API\r\n    cd /workspace/LLaVA\r\n    export HF_HOME=\"/workspace\"\r\n    python -m llava.serve.api -H 0.0.0.0 -p 5000\r\n}\r\n\r\n...\r\n...\r\n...\r\n\r\ntart_nginx\r\n\r\nexecute_script \"/pre_start.sh\" \"Running pre-start script...\"\r\n\r\necho \"Pod Started\"\r\n\r\nsetup_ssh\r\nstart_jupyter\r\nstart_llava_api\r\nexport_env_vars\r\n\r\nexecute_script \"/post_start.sh\" \"Running post-start script...\"\r\n\r\necho \"Container is READY!\"\r\n\r\nsleep infinity</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 859,
    "state": "open",
    "created_by": "win10ogod",
    "created_at": "2023-11-26T13:13:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/859</URL>\n\n<TITLE>[Discussion] Could you try connecting a diffusion model to make the model competent for image generation tasks?</TITLE>\n\n<BODY>### Discussion\r\n\r\nCould you try connecting a diffusion model to make the model competent for image generation tasks?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 858,
    "state": "open",
    "created_by": "Eric-is-good",
    "created_at": "2023-11-26T08:58:20Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/858</URL>\n\n<TITLE>[Question] How to save my finetune model as 4/8 bits?</TITLE>\n\n<BODY>### Question\r\n\r\nWhich part I should modify so that my finetune is normal at 32/16 bits, but when saving the model, it is saved as 4/8 bits? \r\nShould I change the “finetune.sh” or “train. py”？\r\n\r\nI saw the example you provided here: https://huggingface.co/spaces/badayvedat/LLaVA\r\n\r\n@haotian-liu</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 857,
    "state": "closed",
    "created_by": "MengLcool",
    "created_at": "2023-11-26T08:24:31Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/857</URL>\n\n<TITLE>[BUG] 'group_by_modality_length' argument ineffective</TITLE>\n\n<BODY>Thanks for your great work.\r\n\r\nIt seems a bug in https://github.com/haotian-liu/LLaVA/blob/2ca20de1ca76d7d121be5a53f8a46c6bef47a9cb/llava/train/train.py#L656\r\n\r\nThis will lead to the 'group_by_modality_length' argument becoming ineffective, as 'images' is never appears in the *raw* JSON file (self.list_data_dict).\r\n\r\nWe found that this will lead to some performance drop.\r\n\r\nLooking forward to your response!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-27T16:51:59Z>\nThanks, this was introduced in PR #694, which is just reverted in #866. Thanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 856,
    "state": "closed",
    "created_by": "fredshentu",
    "created_at": "2023-11-26T06:14:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/856</URL>\n\n<TITLE>[Question] finetune stage freeze the linear layer? Not consistent with what the paper stated.</TITLE>\n\n<BODY>### Question\n\nOn the paper it seems the linear layer will be fine-tuned e2d during the instructed finetune phase \r\nhttps://arxiv.org/pdf/2304.08485.pdf (see the section4.2 Stage 2: Fine-tuning End-to-End....)\r\nHowever, on the code it seems the linear layer is always frozen except at the pre-training phase. https://github.com/haotian-liu/LLaVA/blob/2ca20de1ca76d7d121be5a53f8a46c6bef47a9cb/llava/train/train.py#L53\r\n\r\nDoes anyone know why there is such a discrepancy? Why the code is not aligned with the paper? Will freezing the linear layer after the projection improve the model's final performance?</BODY>\n\n<COMMENTS>\n<Comment by zackschen at 2023-12-28T07:21:22Z>\nI was wondering about this too, may I ask how you ended up understanding it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 855,
    "state": "open",
    "created_by": "mao-code",
    "created_at": "2023-11-26T03:04:23Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/855</URL>\n\n<TITLE>[Usage] Unproperly Evaluation</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI was fine-tuning using LoRA with my own dataset. However, as I used ScienceQA to evaluate my model, I found that the accuracy is really low. Once I checked out the output file, I noticed that even though the answer was right, the format error led to the incorrect answer in the evaluation. You can see the image below.\r\n\r\nI was trying to modify the eval file in LLaVA, but it didn't work well. Hope anyone can solve this issue🥲.\r\n\r\nThx.\r\n\r\nScreenshots:\r\n<img width=\"701\" alt=\"截圖 2023-11-26 上午10 59 56\" src=\"https://github.com/haotian-liu/LLaVA/assets/41497489/7c8a2c1e-af67-4e43-a4f6-55ff48977878\"></BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 854,
    "state": "open",
    "created_by": "ghost",
    "created_at": "2023-11-25T16:57:57Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/854</URL>\n\n<TITLE>NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.</TITLE>\n\n<BODY>Issue:\r\nNETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.\r\nCommand:\r\n```\r\npython -m llava.serve.controller --host 0.0.0.0 --port 10000   \r\npython -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload  \r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path C:\\Users\\gloria\\.cache\\huggingface\\hub\\models--liuhaotian--llava-v1.5-13b\\snapshots\\llava-v1.5-13b\r\n```\r\n\r\n\r\n**I already tried other solutions of same issue\r\n1) updating transformers library\r\n2) changing the name of model to llava-v1.5-13b, earlier it was some random folder name\r\nMy PC specs\r\nRTX 4060 8GB Vram\r\n16GB ram DDR5\r\n1TB SSD ( enough storage available)\r\nI tried CLI but its very slow.**\r\n\r\n\r\nLog: \r\n```\r\n**logs of first command**\r\n python -m llava.serve.controller --host 0.0.0.0 --port 10000\r\n2023-11-25 21:52:44 | INFO | controller | args: Namespace(host='0.0.0.0', port=10000, dispatch_method='shortest_queue')\r\n2023-11-25 21:52:44 | INFO | controller | Init controller\r\n2023-11-25 21:52:44 | ERROR | stderr | INFO:     Started server process [15392]\r\n2023-11-25 21:52:44 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2023-11-25 21:52:44 | ERROR | stderr | INFO:     Application startup complete.\r\n2023-11-25 21:52:44 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:10000 (Press CTRL+C to quit)\r\n2023-11-25 21:53:52 | INFO | controller | Register a new worker: http://localhost:40000\r\n2023-11-25 21:53:52 | INFO | controller | Register done: http://localhost:40000, {'model_names': ['C:\\\\Users\\\\gloria\\\\.cache\\\\huggingface\\\\hub\\\\models--liuhaotian--llava-v1.5-13b\\\\snapshots\\\\llava-v1.5-13b'], 'speed': 1, 'queue_length': 0}\r\n2023-11-25 21:53:52 | INFO | stdout | INFO:     127.0.0.1:54591 - \"POST /register_worker HTTP/1.1\" 200 OK\r\n2023-11-25 21:54:04 | INFO | controller | Register a new worker: http://localhost:40000\r\n2023-11-25 21:54:07 | INFO | controller | Register done: http://localhost:40000, {'model_names': ['C:\\\\Users\\\\gloria\\\\.cache\\\\huggingface\\\\hub\\\\models--liuhaotian--llava-v1.5-13b\\\\snapshots\\\\llava-v1.5-13b'], 'speed': 1, 'queue_length': 0}\r\n2023-11-25 21:54:07 | INFO | stdout | INFO:     127.0.0.1:54599 - \"POST /refresh_all_workers HTTP/1.1\" 200 OK\r\n2023-11-25 21:54:09 | INFO | stdout | INFO:     127.0.0.1:54604 - \"POST /list_models HTTP/1.1\" 200 OK\r\n2023-11-25 21:54:09 | INFO | controller | Receive heart beat. http://localhost:40000\r\n2023-11-25 21:54:09 | INFO | stdout | INFO:     127.0.0.1:54608 - \"POST /receive_heart_beat HTTP/1.1\" 200 OK\r\n2023-11-25 21:54:26 | INFO | controller | Receive heart beat. http://localhost:40000\r\n2023-11-25 21:54:26 | INFO | stdout | INFO:     127.0.0.1:54624 - \"POST /receive_heart_beat HTTP/1.1\" 200 OK\r\n2023-11-25 21:54:30 | INFO | controller | Register a new worker: http://localhost:40000\r\n2023-11-25 21:54:32 | INFO | controller | Register done: http://localhost:40000, {'model_names': ['C:\\\\Users\\\\gloria\\\\.cache\\\\huggingface\\\\hub\\\\models--liuhaotian--llava-v1.5-13b\\\\snapshots\\\\llava-v1.5-13b'], 'speed': 1, 'queue_length': 0}\r\n2023-11-25 21:54:32 | INFO | stdout | INFO:     127.0.0.1:54642 - \"POST /refresh_all_workers HTTP/1.1\" 200 OK\r\n2023-11-25 21:54:33 | INFO | controller | Register a new worker: http://localhost:40000\r\n2023-11-25 21:54:35 | INFO | controller | Register done: http://localhost:40000, {'model_names': ['C:\\\\Users\\\\gloria\\\\.cache\\\\huggingface\\\\hub\\\\models--liuhaotian--llava-v1.5-13b\\\\snapshots\\\\llava-v1.5-13b'], 'speed': 1, 'queue_length': 0}\r\n2023-11-25 21:54:35 | INFO | stdout | INFO:     127.0.0.1:54662 - \"POST /refresh_all_workers HTTP/1.1\" 200 OK\r\n2023-11-25 21:54:35 | INFO | stdout | INFO:     127.0.0.1:54664 - \"POST /list_models HTTP/1.1\" 200 OK\r\n2023-11-25 21:54:37 | INFO | stdout | INFO:     127.0.0.1:54667 - \"POST /list_models HTTP/1.1\" 200 OK\r\n2023-11-25 21:54:43 | INFO | controller | Receive heart beat. http://localhost:40000\r\n2023-11-25 21:54:43 | INFO | stdout | INFO:     127.0.0.1:54671 - \"POST /receive_heart_beat HTTP/1.1\" 200 OK\r\n2023-11-25 21:54:46 | INFO | controller | names: ['http://localhost:40000'], queue_lens: [0.0], ret: http://localhost:40000\r\n2023-11-25 21:54:46 | INFO | stdout | INFO:     127.0.0.1:54676 - \"POST /get_worker_address HTTP/1.1\" 200 OK\r\n2023-11-25 21:54:50 | INFO | controller | Receive heart beat. http://localhost:40000\r\n2023-11-25 21:54:50 | INFO | stdout | INFO:     127.0.0.1:54680 - \"POST /receive_heart_beat HTTP/1.1\" 200 OK\r\n2023-11-25 21:55:00 | INFO | controller | Receive heart beat. http://localhost:40000\r\n2023-11-25 21:55:00 | INFO | stdout | INFO:     127.0.0.1:54683 - \"POST /receive_heart_beat HTTP/1.1\" 200 OK\r\n2023-11-25 21:55:07 | INFO | controller | Receive heart beat. http://localhost:40000\r\n2023-11-25 21:55:07 | INFO | stdout | INFO:     127.0.0.1:54685 - \"POST /receive_heart_beat HTTP/1.1\" 200 OK\r\n2023-11-25 21:55:18 | INFO | controller | Receive heart beat. http://localhost:40000\r\n2023-11-25 21:55:18 | INFO | stdout | INFO:     127.0.0.1:54692 - \"POST /receive_heart_beat HTTP/1.1\" 200 OK\r\n2023-11-25 21:55:35 | INFO | controller | Receive heart beat. http://localhost:40000\r\n2023-11-25 21:55:35 | INFO | stdout | INFO:     127.0.0.1:54699 - \"POST /receive_heart_beat HTTP/1.1\" 200 OK\r\n2023-11-25 21:55:52 | INFO | controller | Receive heart beat. http://localhost:40000\r\n2023-11-25 21:55:52 | INFO | stdout | INFO:     127.0.0.1:54702 - \"POST /receive_heart_beat HTTP/1.1\" 200 OK\r\n2023-11-25 21:56:09 | INFO | controller | Receive heart beat. http://localhost:40000\r\n2023-11-25 21:56:09 | INFO | stdout | INFO:     127.0.0.1:54706 - \"POST /receive_heart_beat HTTP/1.1\" 200 OK\r\n2023-11-25 21:56:26 | INFO | controller | Receive heart beat. http://localhost:40000\r\n2023-11-25 21:56:26 | INFO | stdout | INFO:     127.0.0.1:54709 - \"POST /receive_heart_beat HTTP/1.1\" 200 OK\r\n2023-11-25 21:56:43 | INFO | controller | Receive heart beat. http://localhost:40000\r\n2023-11-25 21:56:43 | INFO | stdout | INFO:     127.0.0.1:54712 - \"POST /receive_heart_beat HTTP/1.1\" 200 OK\r\n2023-11-25 21:56:59 | INFO | controller | names: ['http://localhost:40000'], queue_lens: [0.0], ret: http://localhost:40000\r\n2023-11-25 21:56:59 | INFO | stdout | INFO:     127.0.0.1:54724 - \"POST /get_worker_address HTTP/1.1\" 200 OK\r\n2023-11-25 21:57:00 | INFO | controller | Receive heart beat. http://localhost:40000\r\n2023-11-25 21:57:00 | INFO | stdout | INFO:     127.0.0.1:54726 - \"POST /receive_heart_beat HTTP/1.1\" 200 OK\r\n2023-11-25 21:57:03 | INFO | controller | Receive heart beat. http://localhost:40000\r\n2023-11-25 21:57:03 | INFO | stdout | INFO:     127.0.0.1:54730 - \"POST /receive_heart_beat HTTP/1.1\" 200 OK\r\n2023-11-25 21:57:17 | INFO | controller | Receive heart beat. http://localhost:40000\r\n2023-11-25 21:57:17 | INFO | stdout | INFO:     127.0.0.1:54733 - \"POST /receive_heart_beat HTTP/1.1\" 200 OK\r\n2023-11-25 21:57:21 | INFO | controller | Receive heart beat. http://localhost:40000\r\n2023-11-25 21:57:21 | INFO | stdout | INFO:     127.0.0.1:54735 - \"POST /receive_heart_beat HTTP/1.1\" 200 OK\r\n2023-11-25 21:57:34 | INFO | controller | Receive heart beat. http://localhost:40000\r\n2023-11-25 21:57:34 | INFO | stdout | INFO:     127.0.0.1:54740 - \"POST /receive_heart_beat HTTP/1.1\" 200 OK\r\n2023-11-25 21:57:51 | INFO | controller | Receive heart beat. http://localhost:40000\r\n2023-11-25 21:57:51 | INFO | stdout | INFO:     127.0.0.1:54743 - \"POST /receive_heart_beat HTTP/1.1\" 200 OK\r\n2023-11-25 22:09:41 | INFO | stdout | INFO:     127.0.0.1:54923 - \"POST /refresh_all_workers HTTP/1.1\" 200 OK\r\n2023-11-25 22:09:43 | INFO | stdout | INFO:     127.0.0.1:54927 - \"POST /list_models HTTP/1.1\" 200 OK```\r\n\r\n**logs of second command**\r\npython -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload                                    \r\n2023-11-25 21:54:02 | INFO | gradio_web_server | args: Namespace(host='0.0.0.0', port=None, controller_url='http://localhost:10000', concurrency_count=10, model_list_mode='reload', share=False, moderate=False, embed=False)\r\n2023-11-25 21:54:09 | INFO | gradio_web_server | Models: ['C:\\\\Users\\\\gloria\\\\.cache\\\\huggingface\\\\hub\\\\models--liuhaotian--llava-v1.5-13b\\\\snapshots\\\\llava-v1.5-13b']\r\n2023-11-25 21:54:09 | INFO | gradio_web_server | Namespace(host='0.0.0.0', port=None, controller_url='http://localhost:10000', concurrency_count=10, model_list_mode='reload', share=False, moderate=False, embed=False)\r\n2023-11-25 21:54:13 | INFO | stdout | Running on local URL:  http://0.0.0.0:7860\r\n2023-11-25 21:54:13 | INFO | stdout | \r\n2023-11-25 21:54:13 | INFO | stdout | To create a public link, set `share=True` in `launch()`.\r\n2023-11-25 21:54:28 | INFO | gradio_web_server | load_demo. ip: 127.0.0.1\r\n2023-11-25 21:54:31 | INFO | gradio_web_server | load_demo. ip: 127.0.0.1\r\n2023-11-25 21:54:35 | INFO | gradio_web_server | Models: ['C:\\\\Users\\\\gloria\\\\.cache\\\\huggingface\\\\hub\\\\models--liuhaotian--llava-v1.5-13b\\\\snapshots\\\\llava-v1.5-13b']\r\n2023-11-25 21:54:37 | INFO | gradio_web_server | Models: ['C:\\\\Users\\\\gloria\\\\.cache\\\\huggingface\\\\hub\\\\models--liuhaotian--llava-v1.5-13b\\\\snapshots\\\\llava-v1.5-13b']\r\n2023-11-25 21:54:43 | INFO | gradio_web_server | add_text. ip: 127.0.0.1. len: 33\r\n2023-11-25 21:54:44 | INFO | gradio_web_server | http_bot. ip: 127.0.0.1\r\n2023-11-25 21:54:46 | INFO | gradio_web_server | model_name: C:\\Users\\gloria\\.cache\\huggingface\\hub\\models--liuhaotian--llava-v1.5-13b\\snapshots\\llava-v1.5-13b, worker_addr: http://localhost:40000\r\n2023-11-25 21:54:46 | INFO | gradio_web_server | ==== request ====\r\n{'model': 'C:\\\\Users\\\\gloria\\\\.cache\\\\huggingface\\\\hub\\\\models--liuhaotian--llava-v1.5-13b\\\\snapshots\\\\llava-v1.5-13b', 'prompt': \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\\nWhat is unusual about this image? ASSISTANT:\", 'temperature': 0.2, 'top_p': 0.7, 'max_new_tokens': 512, 'stop': '</s>', 'images': \"List of 1 images: ['b939abf2c4553ce07e642170aee3a3d7']\"}\r\n\r\n**logs of third command**\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path C:\\Users\\gloria\\.cache\\huggingface\\hub\\models--liuhaotian--llava-v1.5-13b\\snapshots\\llava-v1.5-13b\r\n2023-11-25 22:21:18 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='C:\\\\Users\\\\gloria\\\\.cache\\\\huggingface\\\\hub\\\\models--liuhaotian--llava-v1.5-13b\\\\snapshots\\\\llava-v1.5-13b', model_base=None, model_name=None, device='cuda', multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=False)\r\n2023-11-25 22:21:18 | INFO | model_worker | Loading the model C:\\Users\\gloria\\.cache\\huggingface\\hub\\models--liuhaotian--llava-v1.5-13b\\snapshots\\llava-v1.5-13b on worker 827d96 ...\r\nLoading checkpoint shards:   0%|                                                                                                                  | 0/3 [00:00<?, ?it/s] \r\nLoading checkpoint shards:  33%|███████████████████████████████████▎                                                                      | 1/3 [00:06<00:12,  6.32s/it] \r\nLoading checkpoint shards:  67%|██████████████████████████████████████████████████████████████████████▋                                   | 2/3 [00:40<00:22, 22.77s/it] \r\nLoading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:56<00:00, 19.44s/it] \r\nLoading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:56<00:00, 18.70s/it] \r\n2023-11-25 22:22:15 | ERROR | stderr |\r\n2023-11-25 22:22:18 | INFO | model_worker | Register to controller\r\n2023-11-25 22:22:20 | ERROR | stderr | INFO:     Started server process [36872]\r\n2023-11-25 22:22:20 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2023-11-25 22:22:20 | ERROR | stderr | INFO:     Application startup complete.\r\n2023-11-25 22:22:20 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)\r\n2023-11-25 22:22:35 | INFO | model_worker | Send heart beat. Models: ['C:\\\\Users\\\\gloria\\\\.cache\\\\huggingface\\\\hub\\\\models--liuhaotian--llava-v1.5-13b\\\\snapshots\\\\llava-v1.5-13b']. Semaphore: None. global_counter: 0\r\n2023-11-25 22:22:35 | INFO | stdout | INFO:     127.0.0.1:55140 - \"POST /worker_get_status HTTP/1.1\" 200 OK\r\n2023-11-25 22:22:50 | INFO | stdout | INFO:     127.0.0.1:55152 - \"POST /worker_get_status HTTP/1.1\" 200 OK\r\n2023-11-25 22:22:52 | INFO | model_worker | Send heart beat. Models: ['C:\\\\Users\\\\gloria\\\\.cache\\\\huggingface\\\\hub\\\\models--liuhaotian--llava-v1.5-13b\\\\snapshots\\\\llava-v1.5-13b']. Semaphore: None. global_counter: 0\r\n2023-11-25 22:23:00 | INFO | model_worker | Send heart beat. Models: ['C:\\\\Users\\\\gloria\\\\.cache\\\\huggingface\\\\hub\\\\models--liuhaotian--llava-v1.5-13b\\\\snapshots\\\\llava-v1.5-13b']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n2023-11-25 22:23:02 | INFO | stdout | INFO:     127.0.0.1:55165 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\n2023-11-25 22:23:09 | INFO | model_worker | Send heart beat. Models: ['C:\\\\Users\\\\gloria\\\\.cache\\\\huggingface\\\\hub\\\\models--liuhaotian--llava-v1.5-13b\\\\snapshots\\\\llava-v1.5-13b']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n2023-11-25 22:23:17 | INFO | stdout | Caught Unknown Error\r\n2023-11-25 22:23:17 | INFO | model_worker | Send heart beat. Models: ['C:\\\\Users\\\\gloria\\\\.cache\\\\huggingface\\\\hub\\\\models--liuhaotian--llava-v1.5-13b\\\\snapshots\\\\llava-v1.5-13b']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n```\r\nScreenshots:\r\n<img width=\"875\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/130595576/77b49d16-906e-46d4-97ac-0623332dcc47\"></BODY>\n\n<COMMENTS>\n<Comment by storuky at 2023-11-28T13:35:07Z>\nsame here. Also seems like DEMO page has this issue.\n</Comment>\n<Comment by ghost at 2023-11-30T03:42:41Z>\n> same here. Also seems like DEMO page has this issue.\r\n\r\nOk let me know if you find a solution, btw are u on discord?\n</Comment>\n<Comment by dydxdt at 2023-12-06T07:29:13Z>\nsame problem, plz let me know if you find a solution. Thanks!\r\nIn fact, I launched successfully before and I think my commands are right...\n</Comment>\n<Comment by Krisdddd at 2023-12-06T10:22:00Z>\nYou may have not loaded the model rightly. Split the terminal to run those three codes may help.\n</Comment>\n<Comment by Krisdddd at 2023-12-06T10:24:52Z>\n> You may have not loaded the model rightly. Split the terminal to run those three codes may help.\r\n\r\nadditionally, do you have \"models--openai--clip-vit-large-patch14-336\" in your hub?\n</Comment>\n<Comment by WensongVincent at 2024-03-11T23:14:11Z>\n> You may have not loaded the model rightly. Split the terminal to run those three codes may help.\r\n\r\nI do run those codes in three different terminal, but still give the this error. I'm using 13b model\n</Comment>\n<Comment by sivang at 2024-07-28T21:20:22Z>\nsame error here , no go\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 853,
    "state": "closed",
    "created_by": "FurkanGozukara",
    "created_at": "2023-11-25T14:31:37Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/853</URL>\n\n<TITLE>[Help] I am trying to add batch caption mode to the web ui but http_bot function call is not working - video provided</TITLE>\n\n<BODY>### Question\r\n\r\nI have written def batch_process_images as can be seen below\r\n\r\n```\r\ndef batch_process_images(folder_path, textbox, model_selector , temperature, top_p, max_output_tokens,  request: gr.Request ):\r\n    # Validate state\r\n    print(\"calling batch_process_images print\")\r\n\r\n    for filename in os.listdir(folder_path):\r\n        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\r\n            image_path = os.path.join(folder_path, filename)\r\n            with Image.open(image_path) as image:\r\n                state = default_conversation.copy()\r\n                state_value, gradio_chatbot, empty_string, none_value, disable_btn1, disable_btn2, disable_btn3, disable_btn4, disable_btn5 = add_text(state, textbox, image, \"Default\", request)\r\n                #print(\"calling http_bot\")\r\n                response = http_bot(state, model_selector, temperature, top_p, max_output_tokens, request)\r\n                with open(os.path.splitext(image_path)[0] + '.txt', 'w') as f:\r\n                    f.write(str(response))\r\n\r\n    return \"Batch processing completed.\"\r\n\r\n```\r\n\r\nThe above method is functioning properly. Other web ui functions are default\r\n\r\nI can't figure out why `http_bot` is not returning response even though I am providing all values properly via the above process batch method.\r\n\r\nWhen I upload single image and click button it works\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/19240467/3cc1a325-24c6-4fdd-9562-d71d648b6d75)\r\n\r\nBut when I try to batch process with above method it doesn't work \r\n\r\n**I made a quick video to show why it is not working**\r\n\r\n**https://youtu.be/CiQsIOpxzUk**\r\n\r\nsingle image button clicking working but when I call programmatically not working\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/19240467/52f0a567-d079-4361-8c3b-a08d230aa524)</BODY>\n\n<COMMENTS>\n<Comment by FurkanGozukara at 2023-11-25T14:41:41Z>\nok I made it work hopefully it is coming to my youtube followers soon https://www.youtube.com/SECourses \r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/19240467/1028977a-e6f7-42ba-aac2-ebe8e7844872)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 852,
    "state": "open",
    "created_by": "gd2016229035",
    "created_at": "2023-11-25T03:35:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/852</URL>\n\n<TITLE>[Question] no mm_projector.bin after finetune.sh</TITLE>\n\n<BODY>### Question\n\ncheckpoint files after finetune.sh (full finetune without lora):\r\nconfig.json\tpytorch_model-00001-of-00002.bin  pytorch_model.bin.index.json\ttokenizer_config.json  trainer_state.json\r\ngeneration_config.json\tpytorch_model-00002-of-00002.bin  special_tokens_map.json\ttokenizer.model        training_args.bin\r\n\r\nthere is no mm_projector.bin, but I see projector.bin is in you release checkpoint(https://huggingface.co/liuhaotian/llava-v1.5-7b/tree/main). Is there something I may have misunderstood?</BODY>\n\n<COMMENTS>\n<Comment by aneet-javis at 2023-11-27T10:43:09Z>\n@gd2016229035 did you figure this out?\n</Comment>\n<Comment by gd2016229035 at 2023-11-28T06:01:39Z>\n> @gd2016229035 did you figure this out?\r\n\r\nmaybe it is already in weights, i find model.mm_projector.0.bias in pytorch_model.bin.index.json. But there is also  model.vision_tower in this file, which make me confused that vision_tower is always freeze in training\n</Comment>\n<Comment by mrseanryan at 2024-02-19T14:00:33Z>\nI have same problem - on inference, it seems a file `mm_projector.bin' is expected\r\nbut the output of the fine-tuning does not include that file.\r\n\r\nFine tuning:\r\n\r\n```\r\ndeepspeed ./llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --lora_enable True \\\r\n    --lora_r 128 \\\r\n    --lora_alpha 256 \\\r\n    --mm_projector_lr 2e-5 \\\r\n    --bits 4 \\\r\n    --model_name_or_path liuhaotian/llava-v1.5-7b \\\r\n    --version v1 \\\r\n    --data_path ./temp/dataset/train/dataset.json \\\r\n    --image_folder ./temp/dataset/images/ \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./temp/checkpoints/llava-v1.5-7b-task-qlora \\\r\n    --num_train_epochs 10 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 2 \\\r\n    --evaluation_strategy \"epoch\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True\r\n```\r\n\r\nInference:\r\n\r\n```\r\npython llava/eval/run_llava.py  --model-path   ./temp/checkpoints/llava-v1.5-7b-task-qlora/best_llava_eval_model/       --model-base liuhaotian/llava-v1.5-7b   --image-file ./temp/dataset/images/5fdf6a0f-a8e3-43f3-b150-9b9b83c0b9d5.jpg       --query \"Describe this image\"\r\n```\n</Comment>\n<Comment by mrseanryan at 2024-02-19T14:10:23Z>\nCould it be this parameter is also required, when fine-tuning:\r\n\r\n```\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-13b-pretrain/mm_projector.bin \\\r\n```\r\n(not sure what values to set for this)\n</Comment>\n<Comment by mrseanryan at 2024-02-19T20:42:01Z>\nFor me, the problem was using v1.6 LLaVA source code with v1.5 base model.\r\n\r\nI still do not know how to fine-tune v1.6 - BUT this fork shows how to fine-tune v1.5\r\n\r\nhttps://github.com/mrseanryan/finetune_LLaVA\n</Comment>\n<Comment by kai-wen-yang at 2024-03-01T13:49:39Z>\n> For me, the problem was using v1.6 LLaVA source code with v1.5 base model.\r\n> \r\n> I still do not know how to fine-tune v1.6 - BUT this fork shows how to fine-tune v1.5\r\n> \r\n> https://github.com/mrseanryan/finetune_LLaVA\r\n\r\nThe checkpoint folder name should contain \"lora\" and \"llava\".\n</Comment>\n<Comment by mrseanryan at 2024-03-04T12:40:52Z>\n> checkpoint folder name\r\n\r\n\r\n\r\n> > For me, the problem was using v1.6 LLaVA source code with v1.5 base model.\r\n> > I still do not know how to fine-tune v1.6 - BUT this fork shows how to fine-tune v1.5\r\n> > https://github.com/mrseanryan/finetune_LLaVA\r\n> \r\n> The checkpoint folder name should contain \"lora\" and \"llava\".\r\n\r\nI see - how do you specify the checkpoint folder name ?\n</Comment>\n<Comment by kai-wen-yang at 2024-03-05T09:52:15Z>\n> > checkpoint folder name\r\n> \r\n> > > For me, the problem was using v1.6 LLaVA source code with v1.5 base model.\r\n> > > I still do not know how to fine-tune v1.6 - BUT this fork shows how to fine-tune v1.5\r\n> > > https://github.com/mrseanryan/finetune_LLaVA\r\n> > \r\n> > \r\n> > The checkpoint folder name should contain \"lora\" and \"llava\".\r\n> \r\n> I see - how do you specify the checkpoint folder name ?\r\n\r\nby --output_dir\n</Comment>\n<Comment by fisher75 at 2024-04-25T13:23:55Z>\nHi, how do you know the training was effecitve? Did you use the default training setting? I LoRA with default parameters and basically no improvement.\n</Comment>\n<Comment by mrseanryan at 2024-06-09T18:26:43Z>\nIve heard LlLaVA can work better if image is preprocessed into relevant segments, for example an object detector or element detector.\r\n\r\notherwise perhaps need more data?\n</Comment>\n<Comment by Tess314 at 2025-03-21T12:51:01Z>\nDid this get solved? I am also missing the mm_projector.bin file.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 850,
    "state": "closed",
    "created_by": "Eric-is-good",
    "created_at": "2023-11-24T02:58:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/850</URL>\n\n<TITLE>[Usage] I cannot load the finetuned model with my own language model</TITLE>\n\n<BODY>### Describe the issue\n\nI use a llama2 ( Different sizes from yours ) as the language model, after finetuning , i cannot load the finetuned model , i use the  cli,py ,get this:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/73528862/54bc612f-2ea5-4a67-9b7e-c8a923f4f3f9)\r\n\r\ntokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n\r\nfunction load_pretrained_model failed\r\nimage_processor is none makes NoneType error</BODY>\n\n<COMMENTS>\n<Comment by YiDa858 at 2023-11-24T07:14:26Z>\nHi @Eric-is-good, I'm new in this field. I now have a dataset for fine-tuning the llava, but I don't sure if I need to pre-train as [this](https://github.com/haotian-liu/LLaVA#pretrain-feature-alignment), or I just need to run the [finetune script](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune.sh). Looking forward to your reply!\n</Comment>\n<Comment by Eric-is-good at 2023-11-24T07:24:29Z>\n> Hi @Eric-is-good, I'm new in this field. I now have a dataset for fine-tuning the llava, but I don't sure if I need to pre-train as [this](https://github.com/haotian-liu/LLaVA#pretrain-feature-alignment), or I just need to run the [finetune script](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune.sh). Looking forward to your reply!\r\n\r\nyou need pre-train and finetune both\n</Comment>\n<Comment by YiDa858 at 2023-11-24T07:33:27Z>\nThank you for your reply! I also want to know what the difference is between pre-trained and fine-tuned datasets? It looks like they have the same structure.\n</Comment>\n<Comment by Eric-is-good at 2023-11-24T07:39:37Z>\n> Thank you for your reply! I also want to know what the difference is between pre-trained and fine-tuned datasets? It looks like they have the same structure.\r\n\r\npre-trained is picture and caption, finetune is picture and dialogue with tasks. the structures of them are the same.\n</Comment>\n<Comment by YiDa858 at 2023-11-24T07:41:15Z>\nYou made me understand at once. Thank you!\n</Comment>\n<Comment by yiyiwwang at 2024-05-29T01:34:07Z>\n> ### Describe the issue\r\n> I use a llama2 ( Different sizes from yours ) as the language model, after finetuning , i cannot load the finetuned model , i use the cli,py ,get this: ![image](https://private-user-images.githubusercontent.com/73528862/285347389-54bc612f-2ea5-4a67-9b7e-c8a923f4f3f9.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTY5NDY1ODUsIm5iZiI6MTcxNjk0NjI4NSwicGF0aCI6Ii83MzUyODg2Mi8yODUzNDczODktNTRiYzYxMmYtMmVhNS00YTY3LTliN2UtYzhhOTIzZjRmM2Y5LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MjklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTI5VDAxMzEyNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWE3ZjNkNGQ3NDEzN2Y1Mzc5NWE0YTg2MGU1MGJjZWNhMWNhZGIwZGNhYmY3OWZhOGZiNzQ4YzY0NTZkYTQ0ZjgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.muCpk5HfUWc055XirY59DsEk5uwB-q5j7gLI7xQ3_4g)\r\n> \r\n> tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n> \r\n> function load_pretrained_model failed image_processor is none makes NoneType error\r\n\r\nHello, have you solved this problem? Could you please give me some advice? Thank you very much.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 849,
    "state": "open",
    "created_by": "lianglinyi",
    "created_at": "2023-11-24T02:45:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/849</URL>\n\n<TITLE>[Question] ModelScope版本应该是多少？</TITLE>\n\n<BODY>### Question\n\n参考Model_scope模型llava_visual-question-answering推理的例子，报错如下\r\n![image](https://github.com/haotian-liu/LLaVA/assets/16891076/e9fcdd21-69f7-4756-9f50-e680dbc55ac8)\r\n推理代码如下\r\n![image](https://github.com/haotian-liu/LLaVA/assets/16891076/d3b1e61e-579f-4f88-bca8-99c457f9c854)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 848,
    "state": "open",
    "created_by": "Yuancheng-Xu",
    "created_at": "2023-11-23T18:06:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/848</URL>\n\n<TITLE>Are there any image data augmentation used during training?</TITLE>\n\n<BODY>### Question\n\nHi!\r\n\r\nI am reading the code and I wonder if the image processor during training and evaluation are the same. My understanding is that LLaVa 1.5 uses CLIPImageProcessor directly and the processors are the same for training and evaluation. Am I correct? Thank you!</BODY>\n\n<COMMENTS>\n<Comment by Tess314 at 2025-04-09T08:24:36Z>\n> ### Question\n> Hi!\n> \n> I am reading the code and I wonder if the image processor during training and evaluation are the same. My understanding is that LLaVa 1.5 uses CLIPImageProcessor directly and the processors are the same for training and evaluation. Am I correct? Thank you!\n\nI have the same question.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 847,
    "state": "open",
    "created_by": "Pro-flynn",
    "created_at": "2023-11-23T13:55:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/847</URL>\n\n<TITLE>[Question] Overfitting in my finetune experiment using my custom data</TITLE>\n\n<BODY>### Question\r\n\r\nAfter finetuning using the my custorm data, the finetuned llava model is  overfitting. In my experiments, I following the  your instrcuction( cited in  https://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md).\r\n1.  convert my data to the required format, as follows：\r\n ```\r\n {\r\n        \"id\": \"mamian_fengwo_000252\",\r\n        \"image\": \"mamian_fengwo_000252.jpg\",\r\n        \"conversations\": [\r\n            {\r\n                \"from\": \"human\",\r\n                \"value\": \"<image>\\nWhere is honeycombing on pillars in the image? answer in [[x0,y0,x1,y1]] format.\"\r\n            },\r\n            {\r\n                \"from\": \"gpt\",\r\n                \"value\": \"[[0.7677605, 0.815028, 0.8906875, 0.92288], [0, 0.675963, 0.03476, 0.890241], [0.664312, 0.7921855, 0.7664839999999999, 0.9241485], [0.1377295, 0.7824074999999999, 0.2766145, 0.9952505]]\"\r\n            }\r\n        ]\r\n    },\r\n```\r\n2. use the office scripts (cited in https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task_lora.sh), as follows:                                                                                                                                                                                                                                         \r\n```\r\ndeepspeed llava/train/train_mem.py \\                                                                                                               \r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\                                                                      \r\n    --deepspeed ./scripts/zero3.json \\                                                                                                             \r\n    --model_name_or_path liuhaotian/llava-v1.5-13b \\                                                                                               \r\n    --version v1 \\                                                                                                                                 \r\n    --data_path ./playground/data/llava_lora_finetune_mamianfengwo_floatanno_xywh_train.json \\                                                     \r\n    --image_folder ./playground/data/images \\                                                                                                      \r\n    --vision_tower openai/clip-vit-large-patch14-336 \\                                                                                             \r\n    --mm_projector_type mlp2x_gelu \\                                                                                                               \r\n    --mm_vision_select_layer -2 \\                                                                                                                  \r\n    --mm_use_im_start_end False \\                                                                                                                  \r\n    --mm_use_im_patch_token False \\                                                                                                                \r\n    --image_aspect_ratio pad \\                                                                                                                     \r\n    --group_by_modality_length True \\                                                                                                              \r\n    --bf16 True \\                                                                                                                                  \r\n    --output_dir ./checkpoints/llava-v1.5-13b-task-lora_100epoch_floatanno_xywh_train.json \\                                                       \r\n    --num_train_epochs 100 \\                                                                                                                       \r\n    --per_device_train_batch_size 16 \\                                                                                                             \r\n    --per_device_eval_batch_size 4 \\                                                                                                               \r\n    --gradient_accumulation_steps 1 \\                                                                                                              \r\n    --evaluation_strategy \"no\" \\                                                                                                                   \r\n    --save_strategy \"steps\" \\                                                                                                                      \r\n    --save_steps 50000 \\                                                                                                                           \r\n    --save_total_limit 10\\                                                                                                                         \r\n    --learning_rate 2e-4 \\                                                                                                                         \r\n    --weight_decay 0. \\                                                                                                                            \r\n    --warmup_ratio 0.03 \\                                                                                                                          \r\n    --lr_scheduler_type \"cosine\" \\                                                                                                                 \r\n    --logging_steps 1 \\                                                                                                                            \r\n    --tf32 True \\                                                                                                                                  \r\n    --model_max_length 2048 \\                                                                                                                      \r\n    --gradient_checkpointing True \\                                                                                                                \r\n    --dataloader_num_workers 4 \\                                                                                                                   \r\n    --lazy_preprocess True \\ \r\n```\r\nwe find the  finetuned llava model is underfitting by seting the epoch as 1-10, so we setting the epoch as 50-100, however the finetuned model is overfitting.\r\n \r\n3. We find that the train loss=0 when the training is ending, and the performacen in test data is very poor.</BODY>\n\n<COMMENTS>\n<Comment by Pro-flynn at 2023-11-23T13:57:59Z>\nHow do you think I should adjust my training strategy?\n</Comment>\n<Comment by Linziyang1999 at 2023-11-24T02:28:09Z>\ni think the epoch num is tooooo big, my model is also a little bit overfitting after full finetune in 20k data and 2 epoch(batch 4 ), and the loss was 0.67,\n</Comment>\n<Comment by FHL1998 at 2023-11-24T02:47:21Z>\nWhat's the current inference performance? Do you think LLava is suitable for this kind of object detection task?\n</Comment>\n<Comment by Linziyang1999 at 2023-11-25T03:52:33Z>\nMaybe you can check ocr llava，someone already did it. And they use ocr dataset both in pretrain and finetune.\n</Comment>\n<Comment by Linziyang1999 at 2023-11-25T03:54:20Z>\nLlm has show outstanding performance in ocr . I think llava can made it\n</Comment>\n<Comment by Linziyang1999 at 2023-11-25T03:57:00Z>\nhttps://llavar.github.io/ Check this\n</Comment>\n<Comment by Nomiluks at 2023-11-27T08:55:06Z>\nI've also adopted a similar approach for training my model. However, I find myself perplexed upon reviewing the training statistics.\r\n\r\n```\r\nwandb: Run history:\r\nwandb:                    train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\r\nwandb:              train/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\r\nwandb:            train/learning_rate ▄███████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\r\nwandb:                     train/loss █▆▇▆▆▆▆▅▆▅▄▄▄▃▂▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\r\nwandb:               train/total_flos ▁\r\nwandb:               train/train_loss ▁\r\nwandb:            train/train_runtime ▁\r\nwandb: train/train_samples_per_second ▁\r\nwandb:   train/train_steps_per_second ▁\r\nwandb: \r\nwandb: Run summary:\r\nwandb:                    train/epoch 20.0\r\nwandb:              train/global_step 360\r\nwandb:            train/learning_rate 0.0\r\nwandb:                     train/loss 0.0072\r\nwandb:               train/total_flos 1967070044160.0\r\nwandb:               train/train_loss 0.4161\r\nwandb:            train/train_runtime 848.715\r\nwandb: train/train_samples_per_second 3.323\r\nwandb:   train/train_steps_per_second 0.424\r\n```\r\n\r\n\r\nI'm puzzled about the distinction between train/train_loss with a value of 0.4161 and train/loss with a value of 0.0072. Could someone please clarify this for me?\n</Comment>\n<Comment by Nomiluks at 2023-11-27T10:44:27Z>\nAlso, I have noticed the same issue. The results on the unseen dataset is really bad\n</Comment>\n<Comment by aneet-javis at 2023-11-27T10:48:00Z>\nCould anyone tell what hardware are you guys finetuning on? I tried on one A10G with batch_per_device =1, But getting OOM error.\n</Comment>\n<Comment by Nomiluks at 2023-11-27T11:47:28Z>\nAfter trying with a couple of different machines I used the A100 GCP instance and it worked like a charm.\n</Comment>\n<Comment by haotian-liu at 2023-11-27T16:59:06Z>\nYou can try lowering the number of epochs. Check out the example here, I finetuned for 3 epochs with batch size 8 on a 100 GPT-4V captioned anime examples, and it already works great: https://github.com/haotian-liu/LLaVA/issues/766#issuecomment-1800214174.\r\nYou an also take a look at the wandb logs, the training loss should not be too low, which indicates overfitting. Additionally, fusing a few samples from LLava-instruct or llava-v1.5 data mixture may also help reduce the overfitting.\r\n\r\n@Nomiluks one of them is probably the end-of-epoch stats (there will be just one number for a single experiment), and the other may be the last iter stats (one number for each iterations, but only the last iter is displayed), looking at the wandb interface may allow you better understand the stats.\n</Comment>\n<Comment by Nomiluks at 2023-11-28T07:26:22Z>\n> You can try lowering the number of epochs. Check out the example here, I finetuned for 3 epochs with batch size 8 on a 100 GPT-4V captioned anime examples, and it already works great: [#766 (comment)](https://github.com/haotian-liu/LLaVA/issues/766#issuecomment-1800214174). You an also take a look at the wandb logs, the training loss should not be too low, which indicates overfitting. Additionally, fusing a few samples from LLava-instruct or llava-v1.5 data mixture may also help reduce the overfitting.\r\n> \r\n> @Nomiluks one of them is probably the end-of-epoch stats (there will be just one number for a single experiment), and the other may be the last iter stats (one number for each iterations, but only the last iter is displayed), looking at the wandb interface may allow you better understand the stats.\r\n\r\n\r\nThank for your response @haotian-liu \r\n\r\nI'm working on implementing LLaVA to identify pixel-based image forgery or tampering in my dataset. I currently have 100 samples, and I'm considering  [LORA based fine-tuning](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task_lora.sh) as suggested in the [documentation](https://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md). Do you believe this sample size is sufficient for effective fine-tuning? Additionally, I'm open to any advice or best practices for training LLaVA to specifically detect image forgery. Your insights would be greatly appreciated!\r\n\r\nTraining Example:\r\n\r\n```\r\n    {\r\n        \"id\": \"tampered_654c8796140dc970e0d179d5-back\",\r\n        \"image\": \"tampered_654c8796140dc970e0d179d5-back.jpeg\",\r\n        \"conversations\": [\r\n            {\r\n                \"from\": \"human\",\r\n                \"value\": \"<image>\\nAnalyze the provided document image with the objective of detecting potential instances of image forgery resulting from digital tampering or manipulation. Identify all manipulated regions and present the results in the following format: [[x0, y0, x1, y1]]. If no tampered regions are identified, please return [[]].\"\r\n            },\r\n            {\r\n                \"from\": \"gpt\",\r\n                \"value\": \"[[0.578, 0.604, 0.938, 0.99]]\"\r\n            }\r\n        ]\r\n    }\r\n```\n</Comment>\n<Comment by FHL1998 at 2023-11-28T08:50:04Z>\nI wonder if LLaVA faces a brand new domain, should we do something like fine-tuning the visual encoder at the first step cause right now the vision encoder is not tuned?\n</Comment>\n<Comment by FHL1998 at 2023-11-28T09:28:17Z>\n@Nomiluks According to my experiment, a size of 100 may easily cause overfitting, I tried to enlarge my dataset to 8000 entries (containing a few LLaVA instructions). However, the result shows the descending performance even cannot interpret the \"man behind the taxi\" example, I am still figuring out the cause.\n</Comment>\n<Comment by ronnymunthe99 at 2023-11-28T09:46:18Z>\n> @Nomiluks According to my experiment, a size of 100 may easily cause overfitting, I tried to enlarge my dataset to 8000 entries (containing a few LLaVA instructions). However, the result shows the descending performance even cannot interpret the \"man behind the taxi\" example, I am still figuring out the cause.\r\n\r\nYes, I am also having the same problem, have you found out the cause?\n</Comment>\n<Comment by Nomiluks at 2023-11-28T09:52:08Z>\n> i think the epoch num is tooooo big, my model is also a little bit overfitting after full finetune in 20k data and 2 epoch(batch 4 ), and the loss was 0.67,\r\n\r\nIs the 0.67 the overall loss you're referring to? \r\nIt seems a bit high; typically, we aim for a loss close to 0 for a well-fit model. This value might suggest that the model is underfitting. Could you provide more context or details about the training process? It's important to assess whether this level of loss is acceptable for your specific use case.\n</Comment>\n<Comment by Nomiluks at 2023-11-28T09:52:53Z>\n> @Nomiluks According to my experiment, a size of 100 may easily cause overfitting, I tried to enlarge my dataset to 8000 entries (containing a few LLaVA instructions). However, the result shows the descending performance even cannot interpret the \"man behind the taxi\" example, I am still figuring out the cause.\r\n\r\nyeah, it seems it is unable to learn either the model gets overfit and underfit.\n</Comment>\n<Comment by haotian-liu at 2023-11-28T16:35:01Z>\nI am wondering how big of a difference is the domain shift? For example, for the extremely detailed anime captioning, I was actually surprised by what it can do with 100 examples: https://github.com/haotian-liu/LLaVA/issues/766#issuecomment-1800214174\n</Comment>\n<Comment by FHL1998 at 2023-11-29T00:10:48Z>\n> I am wondering how big of a difference is the domain shift? For example, for the extremely detailed anime captioning, I was actually surprised by what it can do with 100 examples: [#766 (comment)](https://github.com/haotian-liu/LLaVA/issues/766#issuecomment-1800214174)\r\n\r\n@haotian-liu Here are two examples from my side, and the loss curve in 3 epochs:\r\n![1701216545837](https://github.com/haotian-liu/LLaVA/assets/59732331/9c183ba0-4960-492f-b4c2-41afb78594bb)\r\n![1701216798648](https://github.com/haotian-liu/LLaVA/assets/59732331/d93811ea-fd49-4952-b2fb-ce3ced11d48a)\r\n\r\n![fd6ddbf168bc68cb276eb2e45646f6d](https://github.com/haotian-liu/LLaVA/assets/59732331/0cc6bbf9-31f5-4e48-8fe2-d6785ce4ff78)\n</Comment>\n<Comment by haotian-liu at 2023-11-29T00:25:36Z>\nThe loss curve is very concerning here. Here is one of the LoRA finetuning loss curve on stable diffusion prompts.\r\n\r\n<img width=\"429\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/6631389/9346a686-e9cf-4951-9caa-e880a6012e8e\">\r\n\r\nThe initial spike suggests that there is something wrong.\n</Comment>\n<Comment by haotian-liu at 2023-11-29T00:26:31Z>\n@Pro-xiaowen \r\n\r\nBtw, just noticed this: `[0.7677605, 0.815028, 0.8906875, 0.92288]` These coordinates seems overly accurate. You may just need three digits. The later digits may just cause the model to hallucinate.\n</Comment>\n<Comment by FHL1998 at 2023-11-29T02:50:36Z>\n> The loss curve is very concerning here. Here is one of the LoRA finetuning loss curve on stable diffusion prompts.\r\n> \r\n> <img alt=\"image\" width=\"429\" src=\"https://private-user-images.githubusercontent.com/6631389/286436688-9346a686-e9cf-4951-9caa-e880a6012e8e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDEyMjY0MDIsIm5iZiI6MTcwMTIyNjEwMiwicGF0aCI6Ii82NjMxMzg5LzI4NjQzNjY4OC05MzQ2YTY4Ni1lOWNmLTQ5NTEtOWNhYS1lODgwYTYwMTJlOGUucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQUlXTkpZQVg0Q1NWRUg1M0ElMkYyMDIzMTEyOSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyMzExMjlUMDI0ODIyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YmQwYjNhNGExYmZmYzQxM2RlN2U0NzQwMGFhNzAxZjU0M2E2MmQxYjFjNTFlYzliOGE5MjBmOTNjMTAyNGM3MCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.Snrf6UOKTYh6sDlHKuRb7ErDN0VR9Ml6qms1gXeyCUY\">\r\n> The initial spike suggests that there is something wrong.\r\n\r\n@haotian-liu Thx for your reply! May I ask how many samples are included in the dataset, I mean the extra LLaVA instruction samples and the total number of samples.\n</Comment>\n<Comment by Linziyang1999 at 2023-11-29T03:25:31Z>\n> > i think the epoch num is tooooo big, my model is also a little bit overfitting after full finetune in 20k data and 2 epoch(batch 4 ), and the loss was 0.67,\r\n> \r\n> Is the 0.67 the overall loss you're referring to? It seems a bit high; typically, we aim for a loss close to 0 for a well-fit model. This value might suggest that the model is underfitting. Could you provide more context or details about the training process? It's important to assess whether this level of loss is acceptable for your specific use case.\r\n\r\nthus llm generate more word beyond your answer, it not mean the answer is wrong. after experiment loss between 0.6~0.8 is normal. if you want model more accuracy, you may focus on improve size of dataset. here is my loss, and model is work well. ^_^, hope it can help you.\r\n<img width=\"841\" alt=\"截屏2023-11-29 11 24 53\" src=\"https://github.com/haotian-liu/LLaVA/assets/47907549/33aff7c6-636a-404e-a2aa-cc30af4f2752\">\n</Comment>\n<Comment by FHL1998 at 2023-11-29T03:32:49Z>\n> thus llm generate more word beyond your answer, it not mean the answer is wrong. after experiment loss between 0.6~0.8 is normal. if you want model more accuracy, you may focus on improve size of dataset. here is my loss, and model is work well. ^_^, hope it can help you. <img alt=\"截屏2023-11-29 11 24 53\" width=\"841\" src=\"https://private-user-images.githubusercontent.com/47907549/286466642-33aff7c6-636a-404e-a2aa-cc30af4f2752.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDEyMjg2MzMsIm5iZiI6MTcwMTIyODMzMywicGF0aCI6Ii80NzkwNzU0OS8yODY0NjY2NDItMzNhZmY3YzYtNjM2YS00MDRlLWEyYWEtY2MzMGFmNGYyNzUyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzExMjklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMTI5VDAzMjUzM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQyNjg1MDVkNDhjNmNjOWYxYTY4YjBkNDU5NmY3ODc4NzcwMGJiMDYxOTQyNTllNTM5YjE5ZDYxMjE5NDEwZTAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.H0z2bYavCp1xpMsIV34mLIXoyJcj786XRh_gAulBpwY\">\r\n\r\n@Linziyang1999 May I ask the number of samples included in your dataset (How many customized samples and original LLaVA samples)?\n</Comment>\n<Comment by Linziyang1999 at 2023-11-29T03:38:21Z>\n> > thus llm generate more word beyond your answer, it not mean the answer is wrong. after experiment loss between 0.6~0.8 is normal. if you want model more accuracy, you may focus on improve size of dataset. here is my loss, and model is work well. ^_^, hope it can help you. <img alt=\"截屏2023-11-29 11 24 53\" width=\"841\" src=\"https://private-user-images.githubusercontent.com/47907549/286466642-33aff7c6-636a-404e-a2aa-cc30af4f2752.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDEyMjg2MzMsIm5iZiI6MTcwMTIyODMzMywicGF0aCI6Ii80NzkwNzU0OS8yODY0NjY2NDItMzNhZmY3YzYtNjM2YS00MDRlLWEyYWEtY2MzMGFmNGYyNzUyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzExMjklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMTI5VDAzMjUzM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQyNjg1MDVkNDhjNmNjOWYxYTY4YjBkNDU5NmY3ODc4NzcwMGJiMDYxOTQyNTllNTM5YjE5ZDYxMjE5NDEwZTAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.H0z2bYavCp1xpMsIV34mLIXoyJcj786XRh_gAulBpwY\">\r\n> \r\n> @Linziyang1999 May I ask the number of samples included in your dataset (How many customized samples and original LLaVA samples)?\r\n\r\ncustom sample is 20k, and i found there will be an error raised if dataset only have image conversation during train so i add few conversation in mix665k without image(10 maybe? just make it work well).\n</Comment>\n<Comment by FHL1998 at 2023-11-29T05:22:00Z>\n@haotian-liu In my case, the loss seems to drop so quickly after only 30 steps, I have checked three things:\r\n\r\n1. I have already enlarged my dataset to 20k samples by mixing my customized dataset with LLaVA instruction samples;\r\n2. I have checked the dataset format (`id`, `image`, etc);\r\n3. Everything went well during the finetuning phase (no error, warning, or size mismatch).\r\n\r\n![1701234857439](https://github.com/haotian-liu/LLaVA/assets/59732331/295a8033-c408-4d7b-8d9c-eeb935858e05)\r\n\r\nAny obvious error that can be observed from my fine-tuning script or does anyone have any idea about what happened? B.T.W, I used 4 A100 (80GB).\r\n> ```\r\n> deepspeed llava/train/train_mem.py \\                                                                                                               \r\n>     --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\                                                                      \r\n>     --deepspeed ./scripts/zero3.json \\                                                                                                             \r\n>     --model_name_or_path liuhaotian/llava-v1.5-13b \\                                                                                               \r\n>     --version v1 \\                                                                                                                                 \r\n>     --data_path dataset_finetune/llava_finetune_task_v2.json \\                                                     \r\n>     --image_folder ./playground/data/images \\                                                                                                      \r\n>     --vision_tower openai/clip-vit-large-patch14-336 \\                                                                                             \r\n>     --mm_projector_type mlp2x_gelu \\                                                                                                               \r\n>     --mm_vision_select_layer -2 \\                                                                                                                  \r\n>     --mm_use_im_start_end False \\                                                                                                                  \r\n>     --mm_use_im_patch_token False \\                                                                                                                \r\n>     --image_aspect_ratio pad \\                                                                                                                     \r\n>     --group_by_modality_length True \\                                                                                                              \r\n>     --bf16 True \\                                                                                                                                  \r\n>     --output_dir ./checkpoints/llava-v1.5-13b-task-lora-v2 \\                                                       \r\n>     --num_train_epochs 3 \\                                                                                                                       \r\n>     --per_device_train_batch_size 8 \\                                                                                                             \r\n>     --per_device_eval_batch_size 2 \\                                                                                                               \r\n>     --gradient_accumulation_steps 4 \\                                                                                                              \r\n>     --evaluation_strategy \"no\" \\                                                                                                                   \r\n>     --save_strategy \"steps\" \\                                                                                                                      \r\n>     --save_steps 50000 \\                                                                                                                           \r\n>     --save_total_limit 10\\                                                                                                                         \r\n>     --learning_rate 2e-4 \\                                                                                                                         \r\n>     --weight_decay 0. \\                                                                                                                            \r\n>     --warmup_ratio 0.03 \\                                                                                                                          \r\n>     --lr_scheduler_type \"cosine\" \\                                                                                                                 \r\n>     --logging_steps 1 \\                                                                                                                            \r\n>     --tf32 True \\                                                                                                                                  \r\n>     --model_max_length 2048 \\                                                                                                                      \r\n>     --gradient_checkpointing True \\                                                                                                                \r\n>     --dataloader_num_workers 2 \\                                                                                                                   \r\n>     --lazy_preprocess True \\ \r\n> ```\n</Comment>\n<Comment by Pro-flynn at 2023-11-30T08:50:07Z>\n> i think the epoch num is tooooo big, my model is also a little bit overfitting after full finetune in 20k data and 2 epoch(batch 4 ), and the loss was 0.67,\r\n\r\n\r\nwe found the finetuned llava model is underfitting by seting the epoch as 1-10, even the prediction of train data is wrong！ @Linziyang1999\n</Comment>\n<Comment by CrazyBrick at 2023-11-30T13:34:53Z>\n> we found the finetuned llava model is underfitting by seting the epoch as 1-10, even the prediction of train data is wrong！\r\n\r\nso **num_train_epochs 100** will make your loss smaller and the prediction more accurate? (I also encountered trouble, my fine-tuning didn't work)\r\n@Pro-xiaowen\n</Comment>\n<Comment by rohitpanjwani03 at 2023-12-04T09:27:58Z>\nHi guys, I am on colab running it on A100 and trying to fine-tuned using the below code, facing error like ./checkpoint or train_men.py and other train.py files  \r\n\r\nMy code\r\n```%cd /content\r\n!git clone https://github.com/haotian-liu/LLaVA.git\r\n%cd /content/LLaVA\r\n!pip install -q gradio .\r\n\r\n!bash /content/LLaVA/scripts/v1_5/finetune.sh```\r\n\r\n\r\nCan you guys help me with the correct way to fine-tune it?\n</Comment>\n<Comment by ninjacode01 at 2024-02-11T08:59:22Z>\n**Gibberish output (even on train data) with wierd loss curve on fully finetuning. Can someone please help me fix this.**\r\n\r\nI am trying to fully finetune the entire text-only model Vicuna-v1.5 using my custom QnA data comprising of 160k qa pairs, using the same finetuning script as provided in [finetune_task.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task.sh) by omitting the multimodal parameters.\r\nHere is the loss curve on 2.4 epochs. \r\n[wandb report](https://api.wandb.ai/links/hwadhwa-iitd/yd00o7ib\r\n)![image](https://github.com/haotian-liu/LLaVA/assets/77221698/29a1c6fa-c944-4ecc-bd19-02e61ed3e09f)\n</Comment>\n<Comment by zengxingchen at 2024-02-17T11:32:13Z>\n> **Gibberish output (even on train data) with wierd loss curve on fully finetuning. Can someone please help me fix this.**\r\n> \r\n> I am trying to fully finetune the entire text-only model Vicuna-v1.5 using my custom QnA data comprising of 160k qa pairs, using the same finetuning script as provided in [finetune_task.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task.sh) by omitting the multimodal parameters. Here is the loss curve on 2.4 epochs. [wandb report](https://api.wandb.ai/links/hwadhwa-iitd/yd00o7ib)![image](https://private-user-images.githubusercontent.com/77221698/303908106-29a1c6fa-c944-4ecc-bd19-02e61ed3e09f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDgxNjk3ODEsIm5iZiI6MTcwODE2OTQ4MSwicGF0aCI6Ii83NzIyMTY5OC8zMDM5MDgxMDYtMjlhMWM2ZmEtYzk0NC00ZWNjLWJkMTktMDJlNjFlZDNlMDlmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAyMTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMjE3VDExMzEyMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTMwNGI4NTY1ZGQ4ZDYxOTExMTE4NmQ0OWJmMTE0NGJlZGVkZWRhOWVhNjE1NzhjNjA0ZDRmNjRkODYwYzA5YjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.OTJny22zdXZWyqUJmLQqQa0zAiYVOedkB5j-MoTuOio)\r\n\r\nMy loss function trend is in line with yours, and the fine-tuning is poorly done. Sad.\n</Comment>\n<Comment by Ravi-Teja-konda at 2024-02-17T15:58:55Z>\n> Hi guys, I am on colab running it on A100 and trying to fine-tuned using the below code, facing error like ./checkpoint or train_men.py and other train.py files\r\n> \r\n> My code\r\n> \r\n> ```\r\n> !git clone https://github.com/haotian-liu/LLaVA.git\r\n> %cd /content/LLaVA\r\n> !pip install -q gradio .\r\n> \r\n> !bash /content/LLaVA/scripts/v1_5/finetune.sh```\r\n> \r\n> \r\n> Can you guys help me with the correct way to fine-tune it?\r\n> ```\r\n\r\nHello @rohitpanjwani03 ,\r\n\r\nWere you able to fine tune , I'm also trying to fine tune\r\nWas any thing missing in your finetuning process\r\n\r\n\r\nI'm using replicate to fine tune the model https://replicate.com/ravi-teja-konda/llava_finetune/versions/58ea2fa644ef90a63c50bc608a532e2acd5792208978760164f3db900247f062\r\n\r\nBut as the replicate currently does not support changing the hyperparameters, looks I need to finetune on my own by running it in colab, or do we have any alternatives like in hugging face, or have any tried it ?\n</Comment>\n<Comment by ggcr at 2024-05-27T16:29:27Z>\nJust in case someone is having problems during inferece; if you have a script that uses `/llava/eval/run_llava.py` as baseline for inference, you should be careful with the args. In my case I noticed that the `run_llava.py` file will merge the LoRA weights if you specify a `model-base`, and they were already merged, hence the poor performance on inference.\r\n\r\nIf the weights are merged:\r\n- `model-path` should include the path to your fine-tuned model.\r\n- `model-base` should be `None`.\r\n\r\nIf the weights are not merged, you should also specify a `model-base` to merge them.\r\n- `model-path` should include the path to your fine-tuned model.\r\n- `model-base` should be your base model e.g. `'liuhaotian/llava-v1.5-7b'`.\n</Comment>\n<Comment by 1835969208 at 2025-05-25T09:04:08Z>\n> > 您可以尝试减少 epoch 的数量。看看这里的例子，我在 100 个 GPT-4V 字幕动漫示例上微调了 3 个批次大小为 8 的 epoch，它已经运行得很好：[#766（评论）。](https://github.com/haotian-liu/LLaVA/issues/766#issuecomment-1800214174)你也看一下 wandb 日志，训练损失应该不会太低，这表明过拟合。此外，融合来自 LLava-instruct 或 llava-v1.5 数据混合物的几个样本也可能有助于减少过拟合。\n> > 其中一个可能是 End-of-Epoch 统计数据（单个实验只有一个数字），另一个可能是 Last Iter 统计数据（每次迭代一个数字，但只显示最后一个 Iter），查看 WanDB 界面可能会让您更好地了解统计数据。\n> \n> 感谢您的回复\n> \n> 我正在努力实施 LLaVA 来识别数据集中基于像素的图像伪造或篡改。我目前有 100 个样本，我正在考虑按照[文档中](https://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md)的建议进行[基于 LORA 的微调](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task_lora.sh)。您认为这个样本量足以进行有效的微调吗？此外，我愿意接受任何关于训练 LLaVA 专门检测图像伪造的建议或最佳实践。您的见解将不胜感激！\n> \n> 训练示例：\n> \n> ```\n>     {\n>         \"id\": \"tampered_654c8796140dc970e0d179d5-back\",\n>         \"image\": \"tampered_654c8796140dc970e0d179d5-back.jpeg\",\n>         \"conversations\": [\n>             {\n>                 \"from\": \"human\",\n>                 \"value\": \"<image>\\nAnalyze the provided document image with the objective of detecting potential instances of image forgery resulting from digital tampering or manipulation. Identify all manipulated regions and present the results in the following format: [[x0, y0, x1, y1]]. If no tampered regions are identified, please return [[]].\"\n>             },\n>             {\n>                 \"from\": \"gpt\",\n>                 \"value\": \"[[0.578, 0.604, 0.938, 0.99]]\"\n>             }\n>         ]\n>     }\n> ```\n\n\n\n> Also, I have noticed the same issue. The results on the unseen dataset is really bad\n\n我也在做这个工作，我认为这是一个目标检测的事情，所以lora微调llava的loss应该是iou loss，你是这么处理吗？我非常想和您交流\n</Comment>\n<Comment by 1835969208 at 2025-05-25T09:08:58Z>\n@Nomiluks 我也在做这个工作，我认为这是一个目标检测的事情，所以lora微调llava的loss应该是iou loss，你是这么处理吗？我非常想和您交流\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 845,
    "state": "open",
    "created_by": "xiaoachen98",
    "created_at": "2023-11-23T11:11:02Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/845</URL>\n\n<TITLE>[Discussion] We contribute the ShareGPT4V dataset generated from GPT4-Vision to make LLaVA great again! 🎉🎉🎉</TITLE>\n\n<BODY>### Discussion\r\n\r\nHomepage: https://sharegpt4v.github.io/\r\nDemo: https://huggingface.co/spaces/Lin-Chen/ShareGPT4V-7B\r\nDataset: https://huggingface.co/datasets/Lin-Chen/ShareGPT4V\r\nCode: https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-25T19:28:11Z>\nCongratulations on the achievements and impressive results!\n</Comment>\n<Comment by JewelShiny at 2023-11-27T07:26:35Z>\ngreat work😃\n</Comment>\n<Comment by chandlergis at 2023-11-29T07:37:47Z>\ngreat work Can you upload a simpler use case?\n</Comment>\n<Comment by xiaoachen98 at 2023-11-29T07:39:55Z>\n> great work Can you upload a simpler use case?\r\n\r\nSure. We are cleaning the code.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 844,
    "state": "open",
    "created_by": "zhangliang-04",
    "created_at": "2023-11-23T04:35:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/844</URL>\n\n<TITLE>[Question] mlp_projector is not saved in middle checkpoints with finetune_task_lora.sh</TITLE>\n\n<BODY>### Question\r\n\r\nIt seems that `finetune_task_lora.sh` also finetunes the mlp_projector. i noticed the weight of mlp_projector is saved in `non_lora_trainables.bin` in the last checkpoint. However, i cannot find `non_lora_trainables.bin` in the middle checkpoints. Could you pleased to add this feature?</BODY>\n\n<COMMENTS>\n<Comment by linhaojia13 at 2023-11-30T10:59:16Z>\nI find this problem too, have you solved it?\n</Comment>\n<Comment by terminator123 at 2023-12-19T06:20:47Z>\nI find this problem too, have you solved it?\n</Comment>\n<Comment by zengxingchen at 2024-03-02T06:03:37Z>\nthe same\n</Comment>\n<Comment by zengxingchen at 2024-03-10T05:51:56Z>\njust modify the `llava/train/llava_trainer.py`:\r\n\r\n```python\r\n    def _save_checkpoint(self, model, trial, metrics=None):\r\n        if getattr(self.args, 'tune_mm_mlp_adapter', False):\r\n            from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\r\n            checkpoint_folder = f\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\"\r\n\r\n            run_dir = self._get_output_dir(trial=trial)\r\n            output_dir = os.path.join(run_dir, checkpoint_folder)\r\n\r\n            # Only save Adapter\r\n            keys_to_match = ['mm_projector', 'vision_resampler']\r\n            if getattr(self.args, \"use_im_start_end\", False):\r\n                keys_to_match.extend(['embed_tokens', 'embed_in'])\r\n\r\n            weight_to_save = get_mm_adapter_state_maybe_zero_3(self.model.named_parameters(), keys_to_match)\r\n\r\n            if self.args.local_rank == 0 or self.args.local_rank == -1:\r\n                self.model.config.save_pretrained(output_dir)\r\n                torch.save(weight_to_save, os.path.join(output_dir, f'mm_projector.bin'))\r\n        else:\r\n            if self.args.lora_enable:\r\n                from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\r\n                checkpoint_folder = f\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\"\r\n                \r\n                run_dir = self._get_output_dir(trial=trial)\r\n                output_dir = os.path.join(self.args.output_dir, checkpoint_folder)\r\n                os.makedirs(output_dir, exist_ok=True)\r\n                state_dict = get_peft_state_maybe_zero_3(\r\n                    self.model.named_parameters(), self.args.lora_bias\r\n                )\r\n                non_lora_state_dict = get_peft_state_non_lora_maybe_zero_3(\r\n                    self.model.named_parameters()\r\n                )\r\n                if self.args.local_rank == 0 or self.args.local_rank == -1:\r\n                    print(f\"save models to {output_dir} \")\r\n                    self.model.config.save_pretrained(output_dir)\r\n                    self.model.save_pretrained(output_dir, state_dict=state_dict)\r\n                    torch.save(non_lora_state_dict, os.path.join(output_dir, 'non_lora_trainables.bin'))\r\n            else:\r\n                super(LLaVATrainer, self)._save_checkpoint(model, trial, metrics)\r\n```\r\nAlso, you are expected to copy the `get_peft_state_non_lora_maybe_zero_3` and `get_peft_state_maybe_zero_3` from the `llava/train/train.py`\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 843,
    "state": "open",
    "created_by": "soonchangAI",
    "created_at": "2023-11-23T04:01:14Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/843</URL>\n\n<TITLE>[Question] Much slower evaluation when running a smaller model</TITLE>\n\n<BODY>### Question\r\n\r\nHi, i create a llava model with only 12 layers (instead of 32 layers). However, the evaluation on TextVQA two times slower than the larger llava 7 billion parameters\r\n\r\nCode to create new smaller llama\r\n\r\n```\r\nimport argparse\r\nimport torch\r\nimport os\r\nimport json\r\nfrom tqdm import tqdm\r\nimport shortuuid\r\nimport numpy as np\r\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\r\nfrom llava.conversation import conv_templates, SeparatorStyle\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.utils import disable_torch_init\r\nfrom llava.mm_utils import tokenizer_image_token, process_images, get_model_name_from_path\r\nfrom torch.utils.data import Dataset, DataLoader\r\nfrom llava.train.llava_trainer import LLaVATrainer\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\r\nfrom llava.model import *\r\nfrom llava.train.llava_trainer import LLaVATrainer\r\nmodel_path = 'liuhaotian/llava-v1.5-7b'\r\n\r\n## Using the config.json for liuhaotian/llava-v1.5-7b                 \r\ntokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n\r\nconfig = AutoConfig.from_pretrained(model_path)\r\n## update to 12 layers\r\nconfig.update({'num_hidden_layers':12})\r\nmodel_generator =  LlavaLlamaForCausalLM\r\n# Create model from config\r\nmodel = model_generator._from_config(config=config)\r\nprint(model)\r\n \r\n\r\n## Save model , extracted from train.py\r\ntrainer = LLaVATrainer(model=model, tokenizer=tokenizer)\r\noutput_dir = '/LLaVA/checkpoints/llava_mini'\r\n\r\n#state_dict = trainer.model.state_dict()\r\nstate_dict = {}\r\nfor k,v in model.named_parameters():\r\n    state_dict[k] = v\r\ncpu_state_dict = {\r\n    key: value.cpu()\r\n    for key, value in state_dict.items() if 'vision_tower' not in key\r\n}\r\ndel state_dict\r\ntrainer._save(output_dir, state_dict=cpu_state_dict)\r\n\r\n```\r\n\r\nThe evaluation code:\r\n\r\n```\r\npython -m llava.eval.model_vqa_loader \\\r\n    --model-path /LLaVA/checkpoints/llava_mini \\\r\n    --question-file ./playground/data/eval/textvqa/llava_textvqa_val_v051_ocr.jsonl \\\r\n    --image-folder $image_folder \\\r\n    --answers-file ./playground/data/eval/textvqa/answers/llava_mini.jsonl \\\r\n    --temperature 0 \\\r\n    --conv-mode vicuna_v1\r\n```</BODY>\n\n<COMMENTS>\n<Comment by findalexli at 2024-01-29T08:43:03Z>\nCurious about your work, has it been published?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 842,
    "state": "open",
    "created_by": "libaihome",
    "created_at": "2023-11-23T01:52:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/842</URL>\n\n<TITLE>[Question] May I ask if 8 * 3090 supports fine-tuning training</TITLE>\n\n<BODY>### Question\n\nMay I ask if 8 * 3090 supports fine-tuning training</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-27T17:00:10Z>\nLoRA finetuning should work fine.\r\n\r\nIf you are do not have enough GPU memory:\r\n\r\nUse LoRA: [finetune_lora.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_lora.sh). We are able to fit 13B training in 8-A100-40G/8-A6000, and 7B training in 8-RTX3090. Make sure per_device_train_batch_size*gradient_accumulation_steps is the same as the provided script for best reproducibility.\r\nReplace zero3.json with zero3_offload.json which offloads some parameters to CPU RAM. This slows down the training speed.\n</Comment>\n<Comment by libaihome at 2023-11-28T00:36:50Z>\n谢谢\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 840,
    "state": "open",
    "created_by": "chancharikmitra",
    "created_at": "2023-11-21T22:33:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/840</URL>\n\n<TITLE>LLaVA-1.5 Inference Without Images Not Working Properly</TITLE>\n\n<BODY>### Question\r\n\r\nHello! I am curious to run an experiment using LLaVA-1.5 without images (to clarify; it is important for my work to specifically prompt LLaVA-1.5 without images _not_ Vicuna13B). I know this is certainly possible and was about to create my own mask for `model.generate`, but given that LLaVA handles inputs in its own way, I thought I would check if there was a more integrated solution. So for example, in the the following line from `model_vqa_loader.py`, I would like to generate a response without the image:\r\n\r\n```\r\noutput_ids = model.generate(\r\n                input_ids,\r\n                images=image_tensor.to(dtype=torch.float16, device='cuda', non_blocking=True),\r\n                do_sample=True if args.temperature > 0 else False,\r\n                temperature=args.temperature,\r\n                top_p=args.top_p,\r\n                num_beams=args.num_beams,\r\n                max_new_tokens=args.max_new_tokens,\r\n                use_cache=True)\r\n```\r\n\r\nIt seems based on the architecture in `llava_arch.py`: simply passing `None` for the image should be enough based on the following code:\r\n\r\n```\r\ndef prepare_inputs_labels_for_multimodal(\r\n        self, input_ids, position_ids, attention_mask, past_key_values, labels, images\r\n    ):\r\n        vision_tower = self.get_vision_tower()\r\n        if vision_tower is None or images is None or input_ids.shape[1] == 1:\r\n            if past_key_values is not None and vision_tower is not None and images is not None and input_ids.shape[1] == 1:\r\n                target_shape = past_key_values[-1][-1].shape[-2] + 1\r\n                attention_mask = torch.cat((attention_mask, torch.ones(\r\n                    (attention_mask.shape[0], target_shape - attention_mask.shape[1]),\r\n                    dtype=attention_mask.dtype,\r\n                    device=attention_mask.device\r\n                )), dim=1)\r\n                position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\r\n            return input_ids, position_ids, attention_mask, past_key_values, None, labels\r\n```\r\nBut setting `images=None` yields many CUDA errors that look like: \r\n```\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [28\r\n6,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n```</BODY>\n\n<COMMENTS>\n<Comment by AtsuMiyai at 2023-12-19T00:13:58Z>\nI also encountered the same problem. Does anyone know the solution for the inference of LLaVA-1.5 without images?\n</Comment>\n<Comment by haotian-liu at 2023-12-19T00:15:48Z>\nStrange. passing None should be sufficient -- ScienceQA has such pure-text questions and it works fine there.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/llava/eval/model_vqa_science.py#L58\n</Comment>\n<Comment by AtsuMiyai at 2023-12-19T05:34:07Z>\n@haotian-liu  @chancharikmitra \r\nThanks for your reply. By referring to `model_vqa_science.py`, I found that I added DEFAULT_IMAGE_TOKEN to qs, which causes errors. By removing DEFAULT_IMAGE_TOKEN, this error can be solved.\r\nThanks for your feedback!\n</Comment>\n<Comment by copperwiring at 2024-04-25T11:19:06Z>\n@AtsuMiyai @haotian-liu  It still doesn't work for me. I have image files whose use I want to make optional.\r\n\r\nI updated the `run_llava.py`,[ line 114](https://github.com/haotian-liu/LLaVA/blob/3e337ad269da3245643a2724a1d694b5839c37f9/llava/eval/run_llava.py#L114) as\r\n\r\n> \r\n>     if args.image_file is not None:\r\n>         image_files = image_parser(args)\r\n>         images = load_images(image_files)\r\n>         image_sizes = [x.size for x in images]\r\n>         images_tensor = process_images(\r\n>                 images,\r\n>                 image_processor,\r\n>                 model.config\r\n>             ).to(model.device, dtype=torch.float16)\r\n>     else:\r\n>         images_tensor = None\r\n>         image_sizes = None\r\n> \r\n> \r\n>     input_ids = (\r\n>         tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\r\n>         .unsqueeze(0)\r\n>         .cuda()\r\n>     )\r\n> \r\n>     with torch.inference_mode():\r\n>         output_ids = model.generate(\r\n>             input_ids,\r\n>             images=None if images_tensor is None else images_tensor,\r\n>             image_sizes=image_sizes,\r\n>             do_sample=True if args.temperature > 0 else False,\r\n>             temperature=args.temperature,\r\n>             top_p=args.top_p,\r\n>             num_beams=args.num_beams,\r\n>             max_new_tokens=args.max_new_tokens,\r\n>             use_cache=True,\r\n>         )\r\n> \r\n\r\nError is like\r\n\r\n```\r\nndex: block: [120,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n/opt/conda/conda-bld/pytorch_1711403388920/work/aten/src/ATen/native/cuda/Indexing.cu:1290: indexSelectLargeIndex: block: [120,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n/opt/conda/conda-bld/pytorch_1711403388920/work/aten/src/ATen/native/cuda/Indexing.cu:1290: indexSelectLargeIndex: block: [120,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n/opt/conda/conda-bld/pytorch_1711403388920/work/aten/src/ATen/native/cuda/Indexing.cu:1290: indexSelectLargeIndex: block: [120,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n/opt/conda/conda-bld/pytorch_1711403388920/work/aten/src/ATen/native/cuda/Indexing.cu:1290: indexSelectLargeIndex: block: [120,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n/opt/conda/conda-bld/pytorch_1711403388920/work/aten/src/ATen/native/cuda/Indexing.cu:1290: indexSelectLargeIndex: block: [120,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n/opt/conda/conda-bld/pytorch_1711403388920/work/aten/src/ATen/native/cuda/Indexing.cu:1290: indexSelectLargeIndex: block: [120,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n```\n</Comment>\n<Comment by chancharikmitra at 2024-04-25T16:37:44Z>\n@copperwiring Although, I am no longer working on this issue. I think the solution is likely similar to before, except the library has been updated for 1.6. I would check the inputs to see if there is any image-specific tokens that don't need to be there anymore. Perhaps, others working more directly on this will have found the exact edit.\n</Comment>\n<Comment by AtsuMiyai at 2024-04-25T16:49:18Z>\nThanks! @chancharikmitra\r\n\r\n@copperwiring \r\nThanks for your question! \r\nCould you check whether `image_token_se` is added in L61-70 in `run_llava.py`?\r\n`image_token_se` is unnecessary for this case, so you can skip these processes in L61-70!\n</Comment>\n<Comment by copperwiring at 2024-04-26T12:41:15Z>\n@AtsuMiyai \r\nNot really because in following:\r\n\r\n```\r\n    qs = args.query\r\n    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\r\n    if IMAGE_PLACEHOLDER in qs:\r\n        if model.config.mm_use_im_start_end:\r\n            qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\r\n        else:\r\n            qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\r\n    else:\r\n        if model.config.mm_use_im_start_end:\r\n            qs = image_token_se + \"\\n\" + qs\r\n        else:\r\n            qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\r\n```\r\n\r\n`qs = args.query` doesn't have any image. Image is paased via `args.image_file` so the only line it processes in the if else loop is `qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs` which doesnt have `image_token_se`\r\n\r\nbut I do have `DEFAULT_IMAGE_TOKEN` (\r\nhttps://github.com/haotian-liu/LLaVA/blob/3e337ad269da3245643a2724a1d694b5839c37f9/llava/eval/run_llava.py#L70). We should remove this too? \r\n\r\nTangentially, what does `image_token_se`  and `DEFAULT_IMAGE_TOKEN` do btw?\n</Comment>\n<Comment by copperwiring at 2024-04-26T12:44:39Z>\nFrom what I see, code breaks here:\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/3e337ad269da3245643a2724a1d694b5839c37f9/llava/model/language_model/llava_llama.py#L135\r\n\r\nbut I cant see where the input goes wrong?\n</Comment>\n<Comment by copperwiring at 2024-04-26T18:21:21Z>\nNevermind. Removing `DEFAULT_IMAGE_TOKEN` indeed fixed it but if someone can explain me why that will be very helpful.\n</Comment>\n<Comment by CatcherInThePy at 2025-01-07T15:17:01Z>\n# Update:\r\nThis issue should be fixed now with the latest update of transformers library: https://github.com/huggingface/transformers/pull/34502\r\n\r\n---\r\nFor anyone having a similar issue with llava-next (1.6) huggingface models: \r\n\r\nIn my case, the problem was caused by the `line 874` in the `sitepackages\\transformers\\models\\llava_next\\modeling_llava_next.py\"`\r\n\r\n```\r\n    inputs_embeds = inputs_embeds.to(image_features.dtype)\r\n                                     ^^^^^^^^^^^^^^^^^^^^\r\n```\r\n\r\nWhere it gave the following error:\r\n\r\n> AttributeError: 'NoneType' object has no attribute 'dtype'\r\n\r\n\r\nAs a workaround solution, in order to stop the code from entering this block, I explicitly reset the legacy_processing to False at the `line 866`, which allows the code to directly run self.language_model with no image processing. The updated code block starting from line 866 looks like this:\r\n```\r\n        legacy_processing = False\r\n        if legacy_processing:\r\n            logger.warning_once(\r\n                \"Expanding inputs for image tokens in LLaVa-NeXT should be done in processing. \"\r\n                \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\r\n                \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\r\n                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\r\n            )\r\n            if input_ids.shape[1] != 1:\r\n                inputs_embeds = inputs_embeds.to(image_features.dtype)\r\n                inputs_embeds, attention_mask, position_ids, labels, _ = self._merge_input_ids_with_image_features(\r\n                    image_features,\r\n                    feature_lens,\r\n                    inputs_embeds,\r\n                    input_ids,\r\n                    attention_mask,\r\n                    position_ids,\r\n                    labels=labels,\r\n                )\r\n```\r\n\r\nBy doing so, I was able to get inference results with no image inputs, using the standard generation steps at [LLaVA - Batched Inference (Huggingface Docs)](https://huggingface.co/docs/transformers/main/en/model_doc/llava#batched-inference).\r\n\r\nDear @haotian-liu, I believe, a possible fix would be updating the code blocks inside `if legacy_processing:` at `line 866` to handle the case when image_features are None, as they are also initialized as such and updated if there are any pixel values.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 838,
    "state": "open",
    "created_by": "LiWentomng",
    "created_at": "2023-11-21T13:51:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/838</URL>\n\n<TITLE>The training process is paused  after 1 epoch in fine-tuning without any errors.</TITLE>\n\n<BODY>### Describe the issue\n\n\r\nHello, I encounter one strange issue. \r\n\r\nIn the fine-tuning stage,  training process is paused  after just 1 epoch without any errors. \r\n\r\nThe training  process remain staying in one iteration without any moving on.</BODY>\n\n<COMMENTS>\n<Comment by helloadish007 at 2023-11-22T04:11:55Z>\nHi @LiWentomng  , I'm also trying to finetune the LLaVA model. If you dont mind , can you please share the resources or source code for starting out ?\n</Comment>\n<Comment by aneet-javis at 2023-11-22T07:10:29Z>\nHi @LiWentomng. I'm also trying to fine-tune llava on my custom dataset, like @helloadish007.  Would be helpful if you could share the resources/source code to start with.\n</Comment>\n<Comment by cesarandreslopez at 2023-11-22T11:49:57Z>\nSame here. Trying to understand more how to fine-tune and train.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 837,
    "state": "open",
    "created_by": "Felix0805",
    "created_at": "2023-11-21T12:58:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/837</URL>\n\n<TITLE>为什么要重新初始化了lm_head</TITLE>\n\n<BODY>### Question\n\n在llava_llama.py中可以看到lm_head被重新初始化覆盖了llama的lm_head，为何要重新初始化呢，不能复用llama的lm_head，但是embedding复用了llama的embedding，请问有什么实验经验么</BODY>\n\n<COMMENTS>\n<Comment by Felix0805 at 2023-11-21T13:13:28Z>\n好像也是复用了llama的lm_head，为何要在LlavaLlamaForCausalLM类中再定义一下lm_head呢\n</Comment>\n<Comment by shipengai at 2023-12-06T06:16:26Z>\n+1\r\nsame question\r\n@haotian-liu hao\n</Comment>\n<Comment by 421zuoduan at 2024-03-26T11:44:09Z>\nsame question\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 836,
    "state": "open",
    "created_by": "DeepSleepCode",
    "created_at": "2023-11-21T08:54:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/836</URL>\n\n<TITLE>repetition penalty</TITLE>\n\n<BODY>### Question\n\nwhen i try to use repetition_penalty to avoid repeat answer, i met this error \"cuda error:device-side assert triggered\".  After my debug, i found that the input_ids include -200 which is default image token. i guess this is the reason of this \" cuda error:device-side assert triggered\". can  you fix this error?</BODY>\n\n<COMMENTS>\n<Comment by Jeckinchen at 2024-03-19T02:19:33Z>\nHello，have you solved this problem? I also encountered the problem of repeated responses after fine-tuning the model.\n</Comment>\n<Comment by redscv at 2024-05-12T06:43:03Z>\nI had the same error after set repetition_penalty > 0\n</Comment>\n<Comment by LanYu4qz at 2024-06-03T00:50:31Z>\nI have also encountered this issue. Who can solve it?\n</Comment>\n<Comment by DeepSleepCode at 2024-06-03T01:47:14Z>\n> I have also encountered this issue. Who can solve it?\r\nchange -200 to a new positive num\n</Comment>\n<Comment by LanYu4qz at 2024-06-03T07:22:56Z>\n> > I have also encountered this issue. Who can solve it?\r\n> > change -200 to a new positive num\r\n\r\nCould you give me some guidance on how to do it specifically?\n</Comment>\n<Comment by zhangsha1024 at 2024-06-11T06:49:19Z>\nsame here\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 835,
    "state": "open",
    "created_by": "taltlusty",
    "created_at": "2023-11-21T08:23:14Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/835</URL>\n\n<TITLE>llava_v1_5_mix665k dataset</TITLE>\n\n<BODY>### Describe the issue\n\nHello\r\nLooking at the dataset list, which dataset does the prompts with an empty model belong to?\r\nFor example:\r\n\r\n\"id\": \"wgByO4Y_0\",\r\n\"model\": \"\",\r\n\r\nThanks</BODY>\n\n<COMMENTS>\n<Comment by aneet-javis at 2023-11-22T07:11:20Z>\n@taltlusty Where did you get this dataset from? Didn't find in playground/data.\n</Comment>\n<Comment by taltlusty at 2023-11-22T08:43:23Z>\nThanks @aneet-javis \r\nThis is the published dataset for finetuning: https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_v1_5_mix665k.json\n</Comment>\n<Comment by CrazyBrick at 2023-11-30T13:39:55Z>\n> Thanks @aneet-javis This is the published dataset for finetuning: https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_v1_5_mix665k.json\r\n\r\n\r\nIt seems that I have browsed it under other issues before: Adding some plain text Q&A from llava_v1_5_mix665k to his custom dataset (image based Q&A) can improve his fine-tuning effect.\n</Comment>\n<Comment by zengxingchen at 2024-02-28T12:32:53Z>\nthe same. I also found that some ocr_vqa data do not exist in the downloaded data..........\n</Comment>\n<Comment by 421zuoduan at 2024-04-24T03:22:31Z>\nthe same. some ocr_vqa data do not exits\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 833,
    "state": "open",
    "created_by": "gapjialin",
    "created_at": "2023-11-21T04:25:50Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/833</URL>\n\n<TITLE>[Question] | ERROR | stderr | requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=10000): Max retries exceeded with url: /refresh_all_workers (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f16ba0cb340>: Failed to establish a new connection: [Errno 111] Connection refused'))</TITLE>\n\n<BODY>### Question\n\n用docker可以正常部署吗？</BODY>\n\n<COMMENTS>\n<Comment by gapjialin at 2023-11-21T10:27:15Z>\nFailed to establish a new connection: [Errno 111] Connection refused'))\n</Comment>\n<Comment by karenzimeng at 2023-11-30T09:22:46Z>\n请问是您是怎么解决 requests.exceptions.ConnectionError 问题的呢\n</Comment>\n<Comment by 459737087 at 2024-01-11T06:18:21Z>\nhello! did you solve it ? @gapjialin\n</Comment>\n<Comment by deadpipe at 2024-03-05T20:48:00Z>\nhi am also facing the same problem\r\nhave you found a solution? @gapjialin\n</Comment>\n<Comment by HongLouyemeng at 2024-04-24T11:36:32Z>\n你好 有解决方案吗\n</Comment>\n<Comment by Roserland at 2024-07-10T14:37:46Z>\nrun these two commands ONE BY ONE, you'd better run these commands **simultaneously** at 2 terminals\r\n\r\n`python -m llava.serve.controller --host 0.0.0.0 --port 10000`\r\n`python -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload`\n</Comment>\n<Comment by deadpipe at 2024-07-12T11:38:58Z>\nhttps://github.com/haotian-liu/LLaVA/issues/1230#issuecomment-2014765471 Temporary Fix\n</Comment>\n<Comment by g-makerr at 2025-02-17T03:30:14Z>\n先运行controller再运行gradio server\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 832,
    "state": "open",
    "created_by": "unmo",
    "created_at": "2023-11-21T04:04:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/832</URL>\n\n<TITLE>[Question] Does mm_projector parameter freeze?</TITLE>\n\n<BODY>### Question\n\n[finetune_task.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task.sh) does not have the --mm_projector_lr parameter, does this mean that mm_projector is frozen?\r\nThank you.</BODY>\n\n<COMMENTS>\n<Comment by Linziyang1999 at 2023-11-29T02:44:51Z>\n--mm_projector_lr is a parament to change learning rate of m_projector, its not mean whether tune mm_projector or not, its not frozen by default.\n</Comment>\n<Comment by unmo at 2023-11-29T02:58:59Z>\nThank you.\r\nAccording to the following implementation, the default is None, but how many lr would be in that case?\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/llava/train/train.py#L105C5-L105C20\n</Comment>\n<Comment by Linziyang1999 at 2023-11-29T03:56:28Z>\nYou can check llama/train/llava_trainer.py there is a customized optimizer\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 831,
    "state": "open",
    "created_by": "anjanakg",
    "created_at": "2023-11-20T21:46:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/831</URL>\n\n<TITLE>Fine-tune a LLaVa model for Visual question answering task on some custom set of images.</TITLE>\n\n<BODY>### Question\n\nI want to fine-tune a LLaVa model for Visual question answering task on some custom set of images. I wanted to know the Dataset format required for training and then fine-tuning. I found this github folder: https://github.com/haotian-liu/LLaVA/tree/main/scripts/v1_5 . And I found this is an example dataset: https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/detail_23k.json . But since I am new in these tasks still I didn't get a good idea about how to do it correctly. Can someone direct me?</BODY>\n\n<COMMENTS>\n<Comment by gyupro at 2023-11-21T00:53:34Z>\nI am not sure what you don't know. You can finetune with [the finetune script](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune.sh) with the llava format dataset.\n</Comment>\n<Comment by anjanakg at 2023-11-21T14:17:42Z>\n> I am not sure what you don't know. You can finetune with [the finetune script](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune.sh) with the llava format dataset.\r\n\r\nThank you so much! I am sorry, and I am confused. I know how to train CNN models. We need to access the image folders along with the labels. But here in those Github folders, I cannot see any image data folders. Can anyone tell me where we need to keep the image folder along with that json file for fine-tuning the LLaVA?\n</Comment>\n<Comment by gyupro at 2023-11-21T20:55:14Z>\n@anjanakg  oh I see.\r\nU can setup your dataset along with this script https://github.com/SkunkworksAI/BakLLaVA/blob/main/setup_finetune.sh .\r\nThe script is from another repo. You need to download the datasets from different websites and it's annoying. Using this script makes it easier\n</Comment>\n<Comment by anjanakg at 2023-11-21T21:09:25Z>\n> @anjanakg oh I see. U can setup your dataset along with this script https://github.com/SkunkworksAI/BakLLaVA/blob/main/setup_finetune.sh . The script is from another repo. You need to download the datasets from different websites and it's annoying. Using this script makes it easier\r\n\r\nOh...! Understood! Thank you so much! I will try in that way.\r\n\r\nSo, I need to create a json data file(with required format) for my images and keep the image folder somewhere on my local computer or cloud. And, then give the link to the folder and the json file as in that fine-tuning script. Am I correct? If not could you please correct me?\n</Comment>\n<Comment by Kikzter at 2024-01-15T13:30:52Z>\n@anjanakg Hi, hope you are doing well. I have certain doubts on training this model. if you don't mind can we connect regarding this. gmail md.nayeem18899@gmail.com\n</Comment>\n<Comment by Kikzter at 2024-01-15T21:06:09Z>\nHi @anjanakg , @gyupro ,\r\n\r\nCould you please help me out how to fine tuning the model using custom dataset. it would be better if we connect on it. \r\nemail: md.nayeem18899@gmail.com\r\n\r\nThank you\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 830,
    "state": "open",
    "created_by": "lzhhha",
    "created_at": "2023-11-20T17:06:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/830</URL>\n\n<TITLE>[Usage] SError: Unable to load weights from pytorch checkpoint file for './liuhaotian/llava-v1.5-13b/pytorch_model-00001-of-00003.bin' at './liuhaotian/llava-v1.5-13b/pytorch_model-00001-of-00003.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\npython run_llava.py\r\n```\r\n\r\nLog: \r\n```\r\nLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\r\n2023-11-21 00:57:56.933 [ERROR] Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\r\n2023-11-21 00:57:56.933 [ERROR] Traceback (most recent call last):\r\n2023-11-21 00:57:56.933 [ERROR]   File \"/root/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 460, in load_state_dict\r\n2023-11-21 00:57:56.934 [ERROR]     return torch.load(checkpoint_file, map_location=\"cpu\")\r\n2023-11-21 00:57:56.934 [ERROR]   File \"/root/.local/lib/python3.10/site-packages/torch/serialization.py\", line 797, in load\r\n2023-11-21 00:57:56.934 [ERROR]     with _open_zipfile_reader(opened_file) as opened_zipfile:\r\n2023-11-21 00:57:56.934 [ERROR]   File \"/root/.local/lib/python3.10/site-packages/torch/serialization.py\", line 283, in __init__\r\n2023-11-21 00:57:56.934 [ERROR]     super().__init__(torch._C.PyTorchFileReader(name_or_buffer))\r\n2023-11-21 00:57:56.934 [ERROR] RuntimeError: PytorchStreamReader failed reading zip archive: invalid header or archive is corrupted\r\n2023-11-21 00:57:56.934 [ERROR] \r\n2023-11-21 00:57:56.934 [ERROR] During handling of the above exception, another exception occurred:\r\n2023-11-21 00:57:56.934 [ERROR] \r\n2023-11-21 00:57:56.934 [ERROR] Traceback (most recent call last):\r\n2023-11-21 00:57:56.934 [ERROR]   File \"/root/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 464, in load_state_dict\r\n2023-11-21 00:57:56.934 [ERROR]     if f.read(7) == \"version\":\r\n2023-11-21 00:57:56.934 [ERROR]   File \"/root/.local/conda/envs/llava/lib/python3.10/codecs.py\", line 322, in decode\r\n2023-11-21 00:57:56.934 [ERROR]     (result, consumed) = self._buffer_decode(data, self.errors, final)\r\n2023-11-21 00:57:56.935 [ERROR] UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 128: invalid start byte\r\n2023-11-21 00:57:56.935 [ERROR] \r\n2023-11-21 00:57:56.935 [ERROR] During handling of the above exception, another exception occurred:\r\n2023-11-21 00:57:56.935 [ERROR] \r\n2023-11-21 00:57:56.935 [ERROR] Traceback (most recent call last):\r\n2023-11-21 00:57:56.935 [ERROR]   File \"/root/share/program/test_LVLM/LLaVA/llava/eval/run_llava.py\", line 157, in <module>\r\n2023-11-21 00:57:56.935 [ERROR]     eval_model(args)\r\n2023-11-21 00:57:56.935 [ERROR]   File \"/root/share/program/test_LVLM/LLaVA/llava/eval/run_llava.py\", line 56, in eval_model\r\n2023-11-21 00:57:56.935 [ERROR]     tokenizer, model, image_processor, context_len = load_pretrained_model(\r\n2023-11-21 00:57:56.935 [ERROR]   File \"/root/share/program/test_LVLM/LLaVA/llava/model/builder.py\", line 109, in load_pretrained_model\r\n2023-11-21 00:57:56.936 [ERROR]     model = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True,force_download=True, **kwargs)\r\n2023-11-21 00:57:56.936 [ERROR]   File \"/root/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2903, in from_pretrained\r\n2023-11-21 00:57:56.936 [ERROR]     ) = cls._load_pretrained_model(\r\n2023-11-21 00:57:56.936 [ERROR]   File \"/root/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3246, in _load_pretrained_model\r\n2023-11-21 00:57:56.937 [ERROR]     state_dict = load_state_dict(shard_file)\r\n2023-11-21 00:57:56.937 [ERROR]   File \"/root/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 476, in load_state_dict\r\n2023-11-21 00:57:56.937 [ERROR]     raise OSError(\r\n2023-11-21 00:57:56.937 [ERROR] OSError: Unable to load weights from pytorch checkpoint file for './liuhaotian/llava-v1.5-13b/pytorch_model-00001-of-00003.bin' at './liuhaotian/llava-v1.5-13b/pytorch_model-00001-of-00003.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\r\n```\r\n\r\nScreenshots:\r\naccelerate                0.21.0\r\naiofiles                  23.2.1\r\naiohttp                   3.9.0\r\naiosignal                 1.3.1\r\naltair                    5.1.2\r\nanyio                     3.7.1\r\nappdirs                   1.4.4\r\nasync-timeout             4.0.3\r\nattrs                     23.1.0\r\nbitsandbytes              0.41.0\r\ncertifi                   2023.11.17\r\ncharset-normalizer        3.3.2\r\nclick                     8.1.7\r\ncmake                     3.27.7\r\ncontourpy                 1.2.0\r\ncycler                    0.12.1\r\ndeepspeed                 0.9.5\r\ndocker-pycreds            0.4.0\r\neinops                    0.6.1\r\neinops-exts               0.0.4\r\nexceptiongroup            1.1.3\r\nfastapi                   0.104.1\r\nffmpy                     0.3.1\r\nfilelock                  3.13.1\r\nfonttools                 4.44.3\r\nfrozenlist                1.4.0\r\nfsspec                    2023.10.0\r\ngitdb                     4.0.11\r\nGitPython                 3.1.40\r\ngradio                    3.35.2\r\ngradio_client             0.2.9\r\nh11                       0.14.0\r\nhjson                     3.1.0\r\nhttpcore                  0.17.3\r\nhttpx                     0.24.0\r\nhuggingface-hub           0.19.4\r\nidna                      3.4\r\nJinja2                    3.1.2\r\njoblib                    1.3.2\r\njsonschema                4.20.0\r\njsonschema-specifications 2023.11.1\r\nkiwisolver                1.4.5\r\nlinkify-it-py             2.0.2\r\nlit                       17.0.5\r\nllava                     1.1.3        /root/share/program/test_LVLM/LLaVA\r\nmarkdown-it-py            2.2.0\r\nmarkdown2                 2.4.10\r\nMarkupSafe                2.1.3\r\nmatplotlib                3.8.2\r\nmdit-py-plugins           0.3.3\r\nmdurl                     0.1.2\r\nmpmath                    1.3.0\r\nmultidict                 6.0.4\r\nnetworkx                  3.2.1\r\nninja                     1.11.1.1\r\nnumpy                     1.26.2\r\nnvidia-cublas-cu11        11.10.3.66\r\nnvidia-cuda-cupti-cu11    11.7.101\r\nnvidia-cuda-nvrtc-cu11    11.7.99\r\nnvidia-cuda-runtime-cu11  11.7.99\r\nnvidia-cudnn-cu11         8.5.0.96\r\nnvidia-cufft-cu11         10.9.0.58\r\nnvidia-curand-cu11        10.2.10.91\r\nnvidia-cusolver-cu11      11.4.0.1\r\nnvidia-cusparse-cu11      11.7.4.91\r\nnvidia-nccl-cu11          2.14.3\r\nnvidia-nvtx-cu11          11.7.91\r\norjson                    3.9.10\r\npackaging                 23.2\r\npandas                    2.1.3\r\npeft                      0.4.0\r\nPillow                    10.1.0\r\npip                       23.3\r\nprotobuf                  4.25.1\r\npsutil                    5.9.6\r\npy-cpuinfo                9.0.0\r\npydantic                  1.10.13\r\npydub                     0.25.1\r\nPygments                  2.17.1\r\npyparsing                 3.1.1\r\npython-dateutil           2.8.2\r\npython-multipart          0.0.6\r\npytz                      2023.3.post1\r\nPyYAML                    6.0.1\r\nreferencing               0.31.0\r\nregex                     2023.10.3\r\nrequests                  2.31.0\r\nrpds-py                   0.13.0\r\nsafetensors               0.4.0\r\nscikit-learn              1.2.2\r\nscipy                     1.11.4\r\nsemantic-version          2.10.0\r\nsentencepiece             0.1.99\r\nsentry-sdk                1.35.0\r\nsetproctitle              1.3.3\r\nsetuptools                68.0.0\r\nshortuuid                 1.0.11\r\nsix                       1.16.0\r\nsmmap                     5.0.1\r\nsniffio                   1.3.0\r\nstarlette                 0.27.0\r\nsvgwrite                  1.4.3\r\nsympy                     1.12\r\nthreadpoolctl             3.2.0\r\ntimm                      0.6.13\r\ntokenizers                0.13.3\r\ntoolz                     0.12.0\r\ntorch                     2.0.1\r\ntorchvision               0.15.2\r\ntqdm                      4.66.1\r\ntransformers              4.31.0\r\ntriton                    2.0.0\r\ntyping_extensions         4.8.0\r\ntzdata                    2023.3\r\nuc-micro-py               1.0.2\r\nurllib3                   2.1.0\r\nuvicorn                   0.24.0.post1\r\nwandb                     0.16.0\r\nwavedrom                  2.0.3.post3\r\nwebsockets                12.0\r\nwheel                     0.41.2\r\nyarl                      1.9.2</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 829,
    "state": "open",
    "created_by": "Leon-Sander",
    "created_at": "2023-11-20T15:33:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/829</URL>\n\n<TITLE>[Usage] llava-7b 4bit quantization inference problem</TITLE>\n\n<BODY>### Issue:\r\nWhen loading the model in 4bit, I am facing this error: RuntimeError: expected scalar type Float but found Half.\r\n\r\n\r\n### Reproduce the code:\r\n```\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom io import BytesIO\r\nfrom PIL import Image\r\ndef load_image(image_file):\r\n    if image_file.startswith('http') or image_file.startswith('https'):\r\n        response = requests.get(image_file)\r\n        image = Image.open(BytesIO(response.content)).convert('RGB')\r\n    else:\r\n        image = Image.open(image_file).convert('RGB')\r\n    return image\r\n\r\nimport torch\r\n# Set the model path and name\r\nmodel_path = \"liuhaotian/llava-v1.5-7b\"\r\nmodel_name = \"llava-v1.5-7b\"\r\n\r\n# Load the model\r\ntokenizer, model, image_processor, _ = load_pretrained_model(model_path, model_name=model_name, model_base=None, load_4bit=True, device_map=\"auto\", device=\"cuda\")\r\n\r\n# Load your image\r\nimage_path = \"./gainer_schlo.JPG\"\r\nimage_data = load_image(image_path)\r\nimage_tensor = image_processor.preprocess(image_data, return_tensors='pt')['pixel_values'].cuda()\r\n\r\n# Prepare the text input\r\nprompt = \"Desribe what you see in the image.\"\r\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n#model = model.to(device)  \r\nimage_tensor = image_tensor.to(device)\r\n\r\n# Prepare the text input\r\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\r\n\r\n# Generate output\r\ngenerated_ids = model.generate(\r\n    input_ids=input_ids,\r\n    images=image_tensor,\r\n    max_new_tokens=256,  # specify the max number of new tokens\r\n    num_return_sequences=1  # specify the number of sequences to return\r\n)\r\n\r\n# Convert generated tokens to text\r\ngenerated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\r\nprint(generated_text)\r\n```\r\n\r\n### Traceback: \r\n```\r\nRuntimeError                              Traceback (most recent call last)\r\n[/test.ipynb](https://file+.vscode-resource.vscode-cdn.net/test.ipynb) Cell 42 line 3\r\n     [31](vscode-notebook-cell:/test.ipynb#X63sZmlsZQ%3D%3D?line=30) input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\r\n     [33](vscode-notebook-cell:/test.ipynb#X63sZmlsZQ%3D%3D?line=32) # Generate output\r\n---> [34](vscode-notebook-cell:/test.ipynb#X63sZmlsZQ%3D%3D?line=33) generated_ids = model.generate(\r\n     [35](vscode-notebook-cell:/test.ipynb#X63sZmlsZQ%3D%3D?line=34)     input_ids=input_ids,\r\n     [36](vscode-notebook-cell:/test.ipynb#X63sZmlsZQ%3D%3D?line=35)     images=image_tensor,\r\n     [37](vscode-notebook-cell:/test.ipynb#X63sZmlsZQ%3D%3D?line=36)     max_new_tokens=256,  # specify the max number of new tokens\r\n     [38](vscode-notebook-cell:/test.ipynb#X63sZmlsZQ%3D%3D?line=37)     num_return_sequences=1  # specify the number of sequences to return\r\n     [39](vscode-notebook-cell:/test.ipynb#X63sZmlsZQ%3D%3D?line=38) )\r\n     [41](vscode-notebook-cell:/test.ipynb#X63sZmlsZQ%3D%3D?line=40) # Convert generated tokens to text\r\n     [42](vscode-notebook-cell:/test.ipynb#X63sZmlsZQ%3D%3D?line=41) generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\r\n\r\nFile [~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/utils/_contextlib.py:115](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/utils/_contextlib.py:115), in context_decorator.<locals>.decorate_context(*args, **kwargs)\r\n    [112](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/utils/_contextlib.py:112) @functools.wraps(func)\r\n    [113](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/utils/_contextlib.py:113) def decorate_context(*args, **kwargs):\r\n    [114](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/utils/_contextlib.py:114)     with ctx_factory():\r\n--> [115](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/utils/_contextlib.py:115)         return func(*args, **kwargs)\r\n\r\nFile [~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:1538](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:1538), in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\r\n   [1532](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:1532)         raise ValueError(\r\n   [1533](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:1533)             \"num_return_sequences has to be 1 when doing greedy search, \"\r\n   [1534](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:1534)             f\"but is {generation_config.num_return_sequences}.\"\r\n   [1535](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:1535)         )\r\n   [1537](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:1537)     # 11. run greedy search\r\n-> [1538](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:1538)     return self.greedy_search(\r\n   [1539](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:1539)         input_ids,\r\n   [1540](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:1540)         logits_processor=logits_processor,\r\n   [1541](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:1541)         stopping_criteria=stopping_criteria,\r\n   [1542](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:1542)         pad_token_id=generation_config.pad_token_id,\r\n   [1543](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:1543)         eos_token_id=generation_config.eos_token_id,\r\n   [1544](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:1544)         output_scores=generation_config.output_scores,\r\n   [1545](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:1545)         return_dict_in_generate=generation_config.return_dict_in_generate,\r\n   [1546](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:1546)         synced_gpus=synced_gpus,\r\n   [1547](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:1547)         streamer=streamer,\r\n   [1548](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:1548)         **model_kwargs,\r\n   [1549](https://file+.vscode-resource.vscode-cdn.net/~/coRuntimeError: expected scalar type Float but found Halfde/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:1549)     )\r\n   [1551](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:1551) elif is_contrastive_search_gen_mode:\r\n   [1552](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:1552)     if generation_config.num_return_sequences > 1:\r\n\r\nFile [~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:2362](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:2362), in GenerationMixin.greedy_search(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\r\n   [2359](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:2359) model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\r\n   [2361](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:2361) # forward pass to get next token\r\n-> [2362](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:2362) outputs = self(\r\n   [2363](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:2363)     **model_inputs,\r\n   [2364](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:2364)     return_dict=True,\r\n   [2365](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:2365)     output_attentions=output_attentions,\r\n   [2366](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:2366)     output_hidden_states=output_hidden_states,\r\n   [2367](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:2367) )\r\n   [2369](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:2369) if synced_gpus and this_peer_finished:\r\n   [2370](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/generation/utils.py:2370)     continue  # don't waste resources running the code we don't need\r\n\r\nFile [~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1501](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1501), in Module._call_impl(self, *args, **kwargs)\r\n   [1496](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1496) # If we don't have any hooks, we want to skip the rest of the logic in\r\n   [1497](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1497) # this function, and just call forward.\r\n   [1498](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1498) if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\n   [1499](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1499)         or _global_backward_pre_hooks or _global_backward_hooks\r\n   [1500](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1500)         or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> [1501](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1501)     return forward_call(*args, **kwargs)\r\n   [1502](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1502) # Do not call functions when jit is used\r\n   [1503](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1503) full_backward_hooks, non_full_backward_hooks = [], []\r\n\r\nFile [~/code/LLaVA/venv_llava/lib/python3.10/site-packages/accelerate/hooks.py:165](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/accelerate/hooks.py:165), in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\r\n    [163](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/accelerate/hooks.py:163)         output = old_forward(*args, **kwargs)\r\n    [164](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/accelerate/hooks.py:164) else:\r\n--> [165](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/accelerate/hooks.py:165)     output = old_forward(*args, **kwargs)\r\n    [166](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/accelerate/hooks.py:166) return module._hf_hook.post_forward(module, output)\r\n\r\nFile [~/code/LLaVA/llava/model/language_model/llava_llama.py:88](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/llava/model/language_model/llava_llama.py:88), in LlavaLlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, return_dict)\r\n     [71](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/llava/model/language_model/llava_llama.py:71) if inputs_embeds is None:\r\n     [72](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/llava/model/language_model/llava_llama.py:72)     (\r\n     [73](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/llava/model/language_model/llava_llama.py:73)         input_ids,\r\n     [74](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/llava/model/language_model/llava_llama.py:74)         position_ids,\r\n   (...)\r\n     [85](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/llava/model/language_model/llava_llama.py:85)         images\r\n     [86](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/llava/model/language_model/llava_llama.py:86)     )\r\n---> [88](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/llava/model/language_model/llava_llama.py:88) return super().forward(\r\n     [89](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/llava/model/language_model/llava_llama.py:89)     input_ids=input_ids,\r\n     [90](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/llava/model/language_model/llava_llama.py:90)     attention_mask=attention_mask,\r\n     [91](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/llava/model/language_model/llava_llama.py:91)     position_ids=position_ids,\r\n     [92](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/llava/model/language_model/llava_llama.py:92)     past_key_values=past_key_values,\r\n     [93](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/llava/model/language_model/llava_llama.py:93)     inputs_embeds=inputs_embeds,\r\n     [94](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/llava/model/language_model/llava_llama.py:94)     labels=labels,\r\n     [95](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/llava/model/language_model/llava_llama.py:95)     use_cache=use_cache,\r\n     [96](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/llava/model/language_model/llava_llama.py:96)     output_attentions=output_attentions,\r\n     [97](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/llava/model/language_model/llava_llama.py:97)     output_hidden_states=output_hidden_states,\r\n     [98](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/llava/model/language_model/llava_llama.py:98)     return_dict=return_dict\r\n     [99](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/llava/model/language_model/llava_llama.py:99) )\r\n\r\nFile [~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:824](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:824), in LlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\r\n    [822](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:822)     logits = torch.cat(logits, dim=-1)\r\n    [823](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:823) else:\r\n--> [824](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:824)     logits = self.lm_head(hidden_states)\r\n    [825](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:825) logits = logits.float()\r\n    [827](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:827) loss = None\r\n\r\nFile [~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1501](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1501), in Module._call_impl(self, *args, **kwargs)\r\n   [1496](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1496) # If we don't have any hooks, we want to skip the rest of the logic in\r\n   [1497](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1497) # this function, and just call forward.\r\n   [1498](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1498) if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\n   [1499](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1499)         or _global_backward_pre_hooks or _global_backward_hooks\r\n   [1500](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1500)         or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> [1501](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1501)     return forward_call(*args, **kwargs)\r\n   [1502](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1502) # Do not call functions when jit is used\r\n   [1503](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1503) full_backward_hooks, non_full_backward_hooks = [], []\r\n\r\nFile [~/code/LLaVA/venv_llava/lib/python3.10/site-packages/accelerate/hooks.py:165](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/accelerate/hooks.py:165), in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\r\n    [163](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/accelerate/hooks.py:163)         output = old_forward(*args, **kwargs)\r\n    [164](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/accelerate/hooks.py:164) else:\r\n--> [165](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/accelerate/hooks.py:165)     output = old_forward(*args, **kwargs)\r\n    [166](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/accelerate/hooks.py:166) return module._hf_hook.post_forward(module, output)\r\n\r\nFile [~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/nn/modules/linear.py:114](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/nn/modules/linear.py:114), in Linear.forward(self, input)\r\n    [113](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/nn/modules/linear.py:113) def forward(self, input: Tensor) -> Tensor:\r\n--> [114](https://file+.vscode-resource.vscode-cdn.net/~/code/LLaVA/venv_llava/lib/python3.10/site-packages/torch/nn/modules/linear.py:114)     return F.linear(input, self.weight, self.bias)\r\n\r\nRuntimeError: expected scalar type Float but found Half\r\n\r\n```</BODY>\n\n<COMMENTS>\n<Comment by guzixian at 2024-01-10T07:48:39Z>\nwith torch.autocast(\"cuda\"): \r\n  trainer.train()\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 828,
    "state": "open",
    "created_by": "MrBlankness",
    "created_at": "2023-11-20T11:56:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/828</URL>\n\n<TITLE>[Question] Replace language model</TITLE>\n\n<BODY>### Question\n\nI would like to change the language model from llama to baihuan-13B. \r\nI update code in **LLaVA/lava/model/language_ Model/llava_ Llama.py**, such as replacing **LlamaModel** with **AutoModel**, but received the following error \r\n**ValueError: Unrecognized configuration class<class' transformers_ Modules XXXX. configuration_ Baichuan BaihuanConfig '>for this kind of AutoModel: LlavaLlamaForCausalLM**\r\nHow should I correct this error？\r\nThank you.</BODY>\n\n<COMMENTS>\n<Comment by tosiyuki at 2023-11-22T08:17:05Z>\nHow about using BaichuanModel with the following code?\r\nhttps://huggingface.co/baichuan-inc/Baichuan-13B-Base/blob/main/modeling_baichuan.py\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 827,
    "state": "open",
    "created_by": "Williamsunsir",
    "created_at": "2023-11-20T08:27:30Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/827</URL>\n\n<TITLE>[Usage] 我在一个A100 80GB下进行全模型训练，告诉我显存不够</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\nsh /notebooks/projects/git/LLaVA/scripts/v1_5/finetune_task.sh\r\n```\r\n\r\nLog: \r\n```\r\n# sh /notebooks/projects/git/LLaVA/scripts/v1_5/finetune_task.sh\r\n[2023-11-20 08:23:39,414] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n2023-11-20 08:23:41.034618: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-11-20 08:23:41.071501: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2023-11-20 08:23:41.071534: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2023-11-20 08:23:41.072821: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2023-11-20 08:23:41.078730: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-11-20 08:23:41.820408: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n[2023-11-20 08:23:42,801] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\n[2023-11-20 08:23:42,802] [INFO] [runner.py:555:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /notebooks/projects/git/LLaVA/llava/train/train_mem.py --deepspeed /notebooks/projects/git/LLaVA/scripts/zero3.json --model_name_or_path /notebooks/Data/hf-models/llava/llava-v1.5-7b --version v1 --data_path /notebooks/Data/llava/train/llava_v1_5_mix2k.json --image_folder /notebooks/Data/llava/train --vision_tower /notebooks/Data/hf-models/clip/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /notebooks/projects/git/LLaVA/checkpoints/llava-v1.5-7b-task --num_train_epochs 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to none\r\n[2023-11-20 08:23:44,121] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n2023-11-20 08:23:45.647684: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-11-20 08:23:45.684273: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2023-11-20 08:23:45.684298: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2023-11-20 08:23:45.685574: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2023-11-20 08:23:45.691474: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-11-20 08:23:46.418401: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n[2023-11-20 08:23:47,404] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.13.4-1\r\n[2023-11-20 08:23:47,404] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.13.4-1\r\n[2023-11-20 08:23:47,404] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\r\n[2023-11-20 08:23:47,404] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\r\n[2023-11-20 08:23:47,404] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.13.4-1+cuda11.7\r\n[2023-11-20 08:23:47,404] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.13.4-1+cuda11.7\r\n[2023-11-20 08:23:47,404] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.13.4-1\r\n[2023-11-20 08:23:47,404] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}\r\n[2023-11-20 08:23:47,404] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0\r\n[2023-11-20 08:23:47,404] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\r\n[2023-11-20 08:23:47,404] [INFO] [launch.py:163:main] dist_world_size=1\r\n[2023-11-20 08:23:47,404] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0\r\n[2023-11-20 08:23:50,220] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n2023-11-20 08:23:50.502602: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-11-20 08:23:50.539659: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2023-11-20 08:23:50.539684: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2023-11-20 08:23:50.540975: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2023-11-20 08:23:50.546813: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-11-20 08:23:51.348687: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n[2023-11-20 08:23:52,339] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-11-20 08:23:52,339] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-11-20 08:23:52,339] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n[2023-11-20 08:23:53,315] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 6.76B parameters\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.86s/it]\r\n[2023-11-20 08:24:06,582] [WARNING] [partition_parameters.py:836:_post_init_method] param `class_embedding` in CLIPVisionEmbeddings not on GPU so was not broadcasted from rank 0\r\n[2023-11-20 08:24:06,769] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 7.06B parameters\r\nFormatting inputs...Skip in lazy mode\r\nParameter Offload: Total persistent parameters: 599040 in 312 params\r\nTraceback (most recent call last):\r\n  File \"/notebooks/projects/git/LLaVA/llava/train/train_mem.py\", line 16, in <module>\r\n    train()\r\n  File \"/notebooks/projects/git/LLaVA/llava/train/train.py\", line 934, in train\r\n    trainer.train()\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1656, in _inner_training_loop\r\n    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 1198, in prepare\r\n    result = self._prepare_deepspeed(*args)\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 1537, in _prepare_deepspeed\r\n    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/__init__.py\", line 165, in initialize\r\n    engine = DeepSpeedEngine(args=args,\r\n  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py\", line 309, in __init__\r\n    self._configure_optimizer(optimizer, model_parameters)\r\n  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py\", line 1184, in _configure_optimizer\r\n    self.optimizer = self._configure_zero_optimizer(basic_optimizer)\r\n  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py\", line 1474, in _configure_zero_optimizer\r\n    optimizer = DeepSpeedZeroOptimizer_Stage3(\r\n  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/stage3.py\", line 300, in __init__\r\n    self._setup_for_real_optimizer()\r\n  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/stage3.py\", line 382, in _setup_for_real_optimizer\r\n    self.initialize_optimizer_states()\r\n  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/stage3.py\", line 892, in initialize_optimizer_states\r\n    self._optimizer_step(i)\r\n  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/stage3.py\", line 817, in _optimizer_step\r\n    self.optimizer.step()\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py\", line 69, in wrapper\r\n    return wrapped(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\", line 280, in wrapper\r\n    out = func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\", line 33, in _use_grad\r\n    ret = func(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\", line 171, in step\r\n    adamw(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\", line 321, in adamw\r\n    func(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\", line 564, in _multi_tensor_adamw\r\n    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.77 GiB (GPU 0; 79.15 GiB total capacity; 72.29 GiB already allocated; 2.91 GiB free; 75.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n[2023-11-20 08:24:22,443] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2442\r\n[2023-11-20 08:24:22,444] [ERROR] [launch.py:321:sigkill_handler] ['/usr/bin/python3', '-u', '/notebooks/projects/git/LLaVA/llava/train/train_mem.py', '--local_rank=0', '--deepspeed', '/notebooks/projects/git/LLaVA/scripts/zero3.json', '--model_name_or_path', '/notebooks/Data/hf-models/llava/llava-v1.5-7b', '--version', 'v1', '--data_path', '/notebooks/Data/llava/train/llava_v1_5_mix2k.json', '--image_folder', '/notebooks/Data/llava/train', '--vision_tower', '/notebooks/Data/hf-models/clip/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/notebooks/projects/git/LLaVA/checkpoints/llava-v1.5-7b-task', '--num_train_epochs', '1', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none'] exits with return code = 1\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by Linziyang1999 at 2023-11-21T10:44:47Z>\nchange finetune config to zero3_offload and reduce batch_size and increase gradient_accumulation_steps, i use this method and finialy run 7b finetune by using 50GB vram\n</Comment>\n<Comment by Williamsunsir at 2023-11-22T03:08:14Z>\n> change finetune config to zero3_offload and reduce batch_size and increase gradient_accumulation_steps, i use this method and finialy run 7b finetune by using 50GB vram\r\n\r\n非常感谢地你的建议，我通过使用zero3_offload.json 的配置，并且调大gradient_accumulation_steps 参数，但是我的系统内存在持续增长，并且很快占满，我该如何进行处理这个情况呢\r\n\r\n![微信截图_20231122110710](https://github.com/haotian-liu/LLaVA/assets/48049042/f24d032c-4c2e-4b30-aa0f-bd493ef743de)\n</Comment>\n<Comment by Linziyang1999 at 2023-11-22T03:36:57Z>\nyes it take a lot of ram, the workset memory is 232~252GiB during training\n</Comment>\n<Comment by Williamsunsir at 2023-11-22T03:39:54Z>\n> yes it take a lot of ram, the workset memory is 232~252GiB during training\r\n\r\n刚刚服务器卡死了，这么大的内存加载或许对我有些吃力，有没有其他的方式方法呢\n</Comment>\n<Comment by Linziyang1999 at 2023-11-22T06:43:47Z>\nhttps://huggingface.co/docs/transformers/v4.15.0/main_classes/deepspeed\r\ncheck this，you can reduce stage3_max_live_parameters: 1e9    stage3_max_reuse_distance: 1e9 \r\ni see below in huggingface deepspeed, hope this can help you ^^\r\nIf you are getting OOMs, because your model or activations don’t fit into the GPU memory and you have unutilized CPU memory offloading the optimizer states and parameters to CPU memory with \"device\": \"cpu\" may solve this limitation. If you don’t want to offload to CPU memory, use none instead of cpu for the device entry. Offloading to NVMe is discussed further down.\r\n\r\nPinned memory is enabled with pin_memory set to true. This feature can improve the throughput at the cost of making less memory available to other processes. Pinned memory is set aside to the specific process that requested it and its typically accessed much faster than normal CPU memory.\n</Comment>\n<Comment by Williamsunsir at 2023-11-22T09:59:31Z>\n好的，感谢你的建议\n</Comment>\n<Comment by 459737087 at 2024-01-12T06:53:37Z>\nwhen I use zero3_offload.json it reported wrong\r\n\r\n```\r\n[2024-01-12 06:48:37,198] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1201\r\n[2024-01-12 06:48:37,199] [ERROR] [launch.py:321:sigkill_handler] ['/usr/local/bin/python', '-u', 'llava/train/train.py', '--local_rank=0', '--deepspeed', './scripts/zero3_offload.json', '--model_name_or_path', '/output/LLaVA-vicuna-7b', '--version', 'v1', '--data_path', './data/blip_laion_cc_sbu_558k.json', '--image_folder', '/input4/', '--vision_tower', 'openai/clip-vit-large-patch14', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './checkpoints/llava-LLaVA-vicuna-7b-finetune', '--num_train_epochs', '1', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '2', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = -9\r\n```\n</Comment>\n<Comment by Linziyang1999 at 2024-01-12T13:26:11Z>\n> when I use zero3_offload.json it reported wrong\r\n> \r\n> ```\r\n> [2024-01-12 06:48:37,198] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1201\r\n> [2024-01-12 06:48:37,199] [ERROR] [launch.py:321:sigkill_handler] ['/usr/local/bin/python', '-u', 'llava/train/train.py', '--local_rank=0', '--deepspeed', './scripts/zero3_offload.json', '--model_name_or_path', '/output/LLaVA-vicuna-7b', '--version', 'v1', '--data_path', './data/blip_laion_cc_sbu_558k.json', '--image_folder', '/input4/', '--vision_tower', 'openai/clip-vit-large-patch14', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './checkpoints/llava-LLaVA-vicuna-7b-finetune', '--num_train_epochs', '1', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '2', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = -9\r\n> ```\r\n\r\nIt's out of memory , return code = -9 mean your vram/ram not enough\n</Comment>\n<Comment by yqy2001 at 2024-02-22T06:17:16Z>\n一人用中文一人用英文在这一问一答真的太搞笑了哈哈哈\r\n\r\nIt's really funny when one person uses Chinese and the other uses English to conduct conversations round after round. Hahaha\r\n\r\n“有没有其他的方式方法呢” -> \"check this, you can reduce .....\" -> \"好的，感谢\"\n</Comment>\n<Comment by Jeremy-lf at 2024-05-09T09:36:36Z>\n> > when I use zero3_offload.json it reported wrong\r\n> > ```\r\n> > [2024-01-12 06:48:37,198] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1201\r\n> > [2024-01-12 06:48:37,199] [ERROR] [launch.py:321:sigkill_handler] ['/usr/local/bin/python', '-u', 'llava/train/train.py', '--local_rank=0', '--deepspeed', './scripts/zero3_offload.json', '--model_name_or_path', '/output/LLaVA-vicuna-7b', '--version', 'v1', '--data_path', './data/blip_laion_cc_sbu_558k.json', '--image_folder', '/input4/', '--vision_tower', 'openai/clip-vit-large-patch14', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './checkpoints/llava-LLaVA-vicuna-7b-finetune', '--num_train_epochs', '1', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '2', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = -9\r\n> > ```\r\n> \r\n> It's out of memory , return code = -9 mean your vram/ram not enough\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/45171399/c1d82885-fc76-47ae-a6d9-65bd671c63a7)\r\nWhen i train pretraining, `sh scripts/v1_5/pretrain.sh`,  it encounter a error with return code = -8, how to solve it?  Thank you very much!!!!!\r\nENVS: A800 * 8, cuda11.6\n</Comment>\n<Comment by ThisisBillhe at 2024-07-02T06:29:44Z>\nsame OOM problem here when finetuning 7B models on a single A100-80G. The error log is exactly the same as @Williamsunsir . Theoretically, finetuning a 7B model takes 14G (model weights) + 28G (adam) VRAM under BF16. I wonder why using deepspeed lead to OOM? Is there any additional VRAM cost?\r\n\r\n> change finetune config to zero3_offload and reduce batch_size and increase gradient_accumulation_steps, i use this method and finialy run 7b finetune by using 50GB vram\n</Comment>\n<Comment by feiyangsuo at 2024-07-18T09:18:51Z>\n> same OOM problem here when finetuning 7B models on a single A100-80G. The error log is exactly the same as @Williamsunsir . Theoretically, finetuning a 7B model takes 14G (model weights) + 28G (adam) VRAM under BF16. I wonder why using deepspeed lead to OOM? Is there any additional VRAM cost?\r\n> \r\n> > change finetune config to zero3_offload and reduce batch_size and increase gradient_accumulation_steps, i use this method and finialy run 7b finetune by using 50GB vram\r\n\r\nFacing OOM issue too currently. Guess is that in fact it's not GPU OOM but CPU OOM. I'm using 1 A100-80G. If I use zero3.json the training will sometimes raise OOM. However once switched to zero3_offload.json, the training raises OOM every time just after started.\r\nLast time I changed `stage3_max_live_parameters` and `stage3_max_reuse_distance` in zero3.json from default 1e9 to 5e8 and the training is done with no error\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 826,
    "state": "open",
    "created_by": "shahwaiz638",
    "created_at": "2023-11-20T07:53:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/826</URL>\n\n<TITLE>[Question] Memory Size of LLaVA v1.5?</TITLE>\n\n<BODY>### Question\n\nI wanted to ask what is the memory required to train and test LLaVA? \r\nAnd what is the individual memory required by Lama 2?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 825,
    "state": "open",
    "created_by": "LinB203",
    "created_at": "2023-11-20T02:47:28Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/825</URL>\n\n<TITLE>[Discussion] We are contributing 🎉🎉🎉Video-LLaVA</TITLE>\n\n<BODY>### Discussion\n\nHello, esteemed LLaVA developer, thank you for contributing such robust code and data to the community.\r\n\r\nWe have extended LLaVA to [Video-LLaVA](https://github.com/PKU-YuanGroup/Video-LLaVA) to achieve advanced performance on MSRVTT,MSVD,TGIF,ACTIVITYNET.\r\n\r\nThank you again for your contributions to the large visual-language model!</BODY>\n\n<COMMENTS>\n<Comment by LinB203 at 2023-11-23T12:15:39Z>\n<img width=\"440\" alt=\"sota\" src=\"https://github.com/haotian-liu/LLaVA/assets/62638829/5bdf0407-3f1c-48ac-be02-503666303a3b\">\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 824,
    "state": "open",
    "created_by": "DragosDima96",
    "created_at": "2023-11-19T20:26:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/824</URL>\n\n<TITLE>[Question]  Multiple-Images Fine-Tunning</TITLE>\n\n<BODY>### Question\n\nI have seen that is now possible to do multiple images inference.\r\n\r\nHas anyone tried to do fine-tunning using multiple-images?</BODY>\n\n<COMMENTS>\n<Comment by SupritYoung at 2024-01-15T13:47:40Z>\nsame question +1\n</Comment>\n<Comment by LongYu-LY at 2024-02-04T09:58:41Z>\nI also have the same question\n</Comment>\n<Comment by HireTheHero at 2024-02-28T04:34:01Z>\nNot yet, but could #432 give us the basic idea?\r\nAccording to [finetune-by-yourself doc](https://github.com/haotian-liu/LasLaVA/blob/5d8f1760c08b7dfba3ae97b71cbd4c6f17d12dbd/docs/Finetune_Custom_Data.md#L4), we should use [train.py](https://github.com/haotian-liu/LLaVA/blob/main/llava/train/train.py) for this purpose.\r\nIn case of evaluation, I needed to 1) load multiple images to a list of tensors 2) insert special tokens to the prompt\r\nIf you wish to adapt a similar implementation, maybe the json file pointed by `--data_path` looks like this:\r\n```\r\n[\r\n  {\r\n    \"id\": \"997bb945-628d-4724-b370-b84de974a19f\",\r\n    \"image\": \"part-000001/997bb945-628d-4724-b370-b84de974a19f.jpg,path/to/second-image.jpg\",\r\n    \"conversations\": [\r\n      {\r\n        \"from\": \"human\",\r\n        \"value\": \"<image-placeholder>\\nWrite a prompt for Stable Diffusion to generate this image.\"\r\n      },\r\n      {\r\n        \"from\": \"gpt\",\r\n        \"value\": \"a beautiful painting of chernobyl by nekro, pascal blanche, john harris, greg rutkowski, sin jong hun, moebius, simon stalenhag. in style of cg art. ray tracing. cel shading. hyper detailed. realistic. ue 5. maya. octane render. \"\r\n      },\r\n      {\r\n        \"from\": \"human\",\r\n        \"value\": \"<image-placeholder>\\nWrite a prompt for Stable Diffusion to generate this image.\"\r\n      },\r\n      {\r\n        \"from\": \"gpt\",\r\n        \"value\": None\r\n      },\r\n    ]\r\n  },\r\n  ...\r\n]\r\n```\n</Comment>\n<Comment by mvsoom at 2024-04-15T18:46:41Z>\n@haotian-liu Do you have any idea if this would work?\n</Comment>\n<Comment by shure-dev at 2024-04-23T01:18:42Z>\nSame question\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 823,
    "state": "open",
    "created_by": "simplelifetime",
    "created_at": "2023-11-19T05:03:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/823</URL>\n\n<TITLE>Is there a seed for fine-tuning?</TITLE>\n\n<BODY>### Question\n\nI'm wondering if there is a seed for fine-tuning. Currently the reproduced results for second stage fine-tuning are all different from the results of official checkpoint. I'm certain that the training script is exactly the same as provided in this repo. I'm wondering if setting the different random seed can cause such problem.</BODY>\n\n<COMMENTS>\n<Comment by liyang-7 at 2023-11-27T03:07:52Z>\nSame problem.  The loss is different even with fixed seed in TrainingArguments. \r\nMay I ask if you have a method to set a fixed random seed to maintain consistency in the training process?\n</Comment>\n<Comment by xirui-li at 2024-09-09T23:52:22Z>\nSame problem\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 822,
    "state": "closed",
    "created_by": "Richar-Du",
    "created_at": "2023-11-19T03:51:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/822</URL>\n\n<TITLE>[Question] The processing method on bounding box of VG and RefCOCO are different?</TITLE>\n\n<BODY>### Question\r\n\r\nI have seen the issue in https://github.com/haotian-liu/LLaVA/issues/606, and I agree that the processing method of VG is the same as stated in https://github.com/haotian-liu/LLaVA/issues/606#issuecomment-1774076879. However, this processing method does not work for RefCOCO, following is an image in RefCOCO processed by expand2square:\r\n<img width=\"452\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/55051961/b366bcff-f4c3-4f4a-92df-3893511ef9a5\">\r\n\r\nAnd the following is the same image without processing by expand2square:\r\n<img width=\"544\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/55051961/433c0e81-9e21-46f4-b7e3-93bb94772da8\">\r\nIt seems that the second one is correct.\r\n\r\nSo I wonder if the processing methods used in RefCOCO and VG are different.</BODY>\n\n<COMMENTS>\n<Comment by Richar-Du at 2023-11-19T03:59:53Z>\nI misunderstand it, sorry for the interrupt.\n</Comment>\n<Comment by NorthSummer at 2023-12-17T14:14:29Z>\nHi, I still haven't got it; the normalized [xa, ya, xb, yb] seems correct for VG data after padding, but is not correct for coco data after padding, right?\n</Comment>\n<Comment by boyugou at 2024-05-18T00:46:27Z>\n> Hi, I still haven't got it; the normalized [xa, ya, xb, yb] seems correct for VG data after padding, but is not correct for coco data after padding, right?\n\nIs it true? I also haven't got whether the normalized coordinates in llava665k are adjusted for padding or not\n</Comment>\n<Comment by lzy37ld at 2024-05-18T00:53:44Z>\nSame\n</Comment>\n<Comment by MonolithFoundation at 2025-02-13T08:38:21Z>\nIt's totally messed up with the coordinates. we need better do no resize no padding and keeping the ratio\n</Comment>\n<Comment by boyugou at 2025-02-13T08:40:06Z>\n> It's totally messed up with the coordinates. we need better do no resize no padding and keeping the ratio\n\nI remember the author clarified the question somewhere. So I did understand the process later then.\n</Comment>\n<Comment by MonolithFoundation at 2025-02-13T08:43:03Z>\nCan u explain on how: the cooridnates processed in text ? (normalized into 0-1? according to what?)  The image actually being resized and padded.\n\nFor reference, qwen2 vl simply uses original x1y1x2y2 format without scaling, and original WH of raw image. Make sense more.\n</Comment>\n<Comment by boyugou at 2025-02-13T09:07:58Z>\n> Can u explain on how: the cooridnates processed in text ? (normalized into 0-1? according to what?) The image actually being resized and padded.\n> \n> For reference, qwen2 vl simply uses original x1y1x2y2 format without scaling, and original WH of raw image. Make sense more.\n\nI figured it out once and did the reversion for the data. But unfortunately I find I've deleted the related code. I'm sure Haotian clarified the image resizing somewhere in an issue, where I get to know how to deal with it.\n</Comment>\n<Comment by MonolithFoundation at 2025-02-13T09:12:28Z>\nDid u real understand how does it actually processed?\n</Comment>\n<Comment by boyugou at 2025-02-13T09:13:40Z>\n> Did u real understand how does it actually processed?\n\nofc? Otherwise why did I say 'I figured it out once and did the reversion for the data'\n</Comment>\n<Comment by MonolithFoundation at 2025-02-13T09:26:55Z>\nI mean, did u know how does LLava treated the input with resized and padding\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 821,
    "state": "open",
    "created_by": "microhu",
    "created_at": "2023-11-19T03:01:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/821</URL>\n\n<TITLE>[Question]  In SFT stage， Zero2 is slower than ZERO3</TITLE>\n\n<BODY>### Question\n\nWhen I runing SFT stage, I compare the zero2 andd zero3 and find that zero2 is slower than zero3, which is out of my understanding.  Is any special done in zero3 ?  @haotian-liu</BODY>\n\n<COMMENTS>\n<Comment by g-h-chen at 2025-01-31T14:50:39Z>\nsame issue here, any solution? @microhu  :)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 820,
    "state": "open",
    "created_by": "xiechengmude",
    "created_at": "2023-11-18T06:56:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/820</URL>\n\n<TITLE>[BUG]  Miss tokenizer match & TypeError: forward() got an unexpected keyword argument 'padding_mask'</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\nFile \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 921, in forward\r\n    layer_outputs = torch.utils.checkpoint.checkpoint(\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/_compile.py\", line 24, in inner\r\n    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 328, in _fn\r\n    return fn(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 17, in inner\r\n    return fn(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 451, in checkpoint\r\n    return CheckpointFunction.apply(function, preserve, *args)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/autograd/function.py\", line 539, in apply\r\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 230, in forward\r\n    outputs = run_function(*args)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 917, in custom_forward\r\n    return module(*inputs, past_key_value, output_attentions, padding_mask=padding_mask)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 635, in forward\r\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\nTypeError: forward() got an unexpected keyword argument 'padding_mask'\r\nTraceback (most recent call last):\r\n  File \"/workspace/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/workspace/LLaVA/llava/train/train.py\", line 933, in train\r\n    trainer.train()\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/transformers/trainer.py\", line 1591, in train\r\n    return inner_training_loop(\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/transformers/trainer.py\", line 1892, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/transformers/trainer.py\", line 2776, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/transformers/trainer.py\", line 2801, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1735, in forward\r\n    loss = self.module(*inputs, **kwargs)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/workspace/LLaVA/llava/model/language_model/llava_llama.py\", line 88, in forward\r\n    return super().forward(\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1038, in forward\r\n    outputs = self.model(\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 921, in forward\r\n    layer_outputs = torch.utils.checkpoint.checkpoint(\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/_compile.py\", line 24, in inner\r\n    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 328, in _fn\r\n    return fn(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 17, in inner\r\n    return fn(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 451, in checkpoint\r\n    return CheckpointFunction.apply(function, preserve, *args)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/autograd/function.py\", line 539, in apply\r\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 230, in forward\r\n    outputs = run_function(*args)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 917, in custom_forward\r\n    return module(*inputs, past_key_value, output_attentions, padding_mask=padding_mask)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 635, in forward\r\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1568, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\nTypeError: forward() got an unexpected keyword argument 'padding_mask'\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by unnikrishnanrnair at 2023-11-22T05:10:39Z>\nMaking sure the transformers version is correct with ```pip install transformers==4.31.0``` solved it for me.\n</Comment>\n<Comment by xiechengmude at 2023-12-04T13:53:19Z>\noh! thank you bro!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 819,
    "state": "open",
    "created_by": "zhaomin1995",
    "created_at": "2023-11-18T03:57:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/819</URL>\n\n<TITLE>AttributeError: 'Args' object has no attribute 'temperature'[Usage]</TITLE>\n\n<BODY>### Describe the issue\n\nHi, I followed the instructions in the README file for inference. But I got the error saying \"AttributeError: 'Args' object has no attribute 'temperature'\" when I tried to run the following command.\r\n\r\n```\r\nargs = type('Args', (), {\r\n    \"model_path\": model_path,\r\n    \"model_base\": None,\r\n    \"model_name\": get_model_name_from_path(model_path),\r\n    \"query\": prompt,\r\n    \"conv_mode\": None,\r\n    \"image_file\": image_file,\r\n    \"sep\": \",\",\r\n})()\r\n\r\neval_model(args)\r\n```</BODY>\n\n<COMMENTS>\n<Comment by Jiancong at 2023-11-18T11:24:41Z>\nUse this setting, you still miss two params.\r\n\r\nfrom llava.model.builder import load_pretrained_model\r\nfrom llava.mm_utils import get_model_name_from_path\r\nfrom llava.eval.run_llava import eval_model\r\n\r\nmodel_path = \"liuhaotian/llava-v1.5-7b\"\r\nprompt = \"What are the things I should be cautious about when I visit here?\"\r\nimage_file = \"https://llava-vl.github.io/static/images/view.jpg\"\r\n\r\nargs = type('Args', (), {\r\n    \"model_path\": model_path,\r\n    \"model_base\": None,\r\n    \"model_name\": get_model_name_from_path(model_path),\r\n    \"query\": prompt,\r\n    \"conv_mode\": None,\r\n    \"image_file\": image_file,\r\n    \"sep\": \",\",\r\n    \"temperature\": 1,\r\n    \"top_p\":1,\r\n    \"num_beams\": 1\r\n})()\r\n\r\neval_model(args)\n</Comment>\n<Comment by adityac94 at 2023-11-18T17:55:17Z>\nUsing this setting also I get \"AttributeError: 'Args' object has no attribute 'max_new_tokens'\" as error\n</Comment>\n<Comment by Muhammad4hmed at 2023-12-04T20:01:05Z>\nUse this default setting:\r\n```python\r\nargs = type('Args', (), {\r\n    \"model_path\": model_path,\r\n    \"model_base\": None,\r\n    \"model_name\": get_model_name_from_path(model_path),\r\n    \"query\": prompt,\r\n    \"conv_mode\": None,\r\n    \"image_file\": image_file,\r\n    \"sep\": \",\",\r\n    \"temperature\": 0.2,\r\n    \"top_p\":None,\r\n    \"num_beams\": 1,\r\n    'max_new_tokens': 512\r\n})()\r\n```\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 818,
    "state": "closed",
    "created_by": "czczup",
    "created_at": "2023-11-17T12:35:21Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/818</URL>\n\n<TITLE>[Question] Question about LLaVA bench</TITLE>\n\n<BODY>### Question\n\nHi, thanks for your awesome work.\r\nI have some question about the LLaVA Bench, which one is the score should I report in the paper?\r\n<img width=\"514\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/23737120/4a0ee8fc-b670-4e39-95d8-99d162340e64\"></BODY>\n\n<COMMENTS>\n<Comment by xiechengmude at 2023-11-17T16:42:35Z>\nLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n  0%|                                                                                                                                                                 | 0/1495 [00:00<?, ?it/s]\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/root/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/workspace/LLaVA/llava/eval/model_vqa_qbench.py\", line 122, in <module>\r\n    eval_model(args)\r\n  File \"/workspace/LLaVA/llava/eval/model_vqa_qbench.py\", line 78, in eval_model\r\n    image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].half().cuda()\r\nAttributeError: 'NoneType' object has no attribute 'preprocess'\n</Comment>\n<Comment by gapjialin at 2023-12-11T13:11:12Z>\n> LM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n> \r\n>     * This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n>       0%|                                                                                                                                                                 | 0/1495 [00:00<?, ?it/s]\r\n>       Traceback (most recent call last):\r\n>       File \"/root/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n>       return _run_code(code, main_globals, None,\r\n>       File \"/root/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n>       exec(code, run_globals)\r\n>       File \"/workspace/LLaVA/llava/eval/model_vqa_qbench.py\", line 122, in \r\n>       eval_model(args)\r\n>       File \"/workspace/LLaVA/llava/eval/model_vqa_qbench.py\", line 78, in eval_model\r\n>       image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].half().cuda()\r\n>       AttributeError: 'NoneType' object has no attribute 'preprocess'\r\n\r\nHi, I also encountered the same problem. How did you solve it?\n</Comment>\n<Comment by daiqing98 at 2023-12-24T13:05:34Z>\n> > LM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n> > ```\r\n> > * This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n> >   0%|                                                                                                                                                                 | 0/1495 [00:00<?, ?it/s]\r\n> >   Traceback (most recent call last):\r\n> >   File \"/root/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n> >   return _run_code(code, main_globals, None,\r\n> >   File \"/root/miniconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n> >   exec(code, run_globals)\r\n> >   File \"/workspace/LLaVA/llava/eval/model_vqa_qbench.py\", line 122, in \r\n> >   eval_model(args)\r\n> >   File \"/workspace/LLaVA/llava/eval/model_vqa_qbench.py\", line 78, in eval_model\r\n> >   image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].half().cuda()\r\n> >   AttributeError: 'NoneType' object has no attribute 'preprocess'\r\n> > ```\r\n> \r\n> Hi, I also encountered the same problem. How did you solve it?\r\n\r\nHi have you solved this problem? @gapjialin\n</Comment>\n<Comment by gaurav22verma at 2024-01-25T03:32:21Z>\n@daiqing98 you might want to check #444 – there's a naming convention that should be followed\n</Comment>\n<Comment by Dominic789654 at 2024-02-29T15:01:02Z>\n@czczup Hi, can you tell me which score to report?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 816,
    "state": "open",
    "created_by": "Sarahtorekryo",
    "created_at": "2023-11-17T05:21:21Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/816</URL>\n\n<TITLE>[Question] lora finetune loss=0.00</TITLE>\n\n<BODY>### Question\r\n\r\nHi Haotian~ I got training loss=0.00 when doing lora finetuning on custom data. I've checked that:\r\n\r\n1. full para finetuning loss is not zero\r\n2. custom data format is correct\r\n\r\nupdate:\r\nIt seems related to **model_max_length**, works all right when keep 2048. Does **model_max_length** means max input token length?\r\n\r\nDo you have any clue what's the cause? Thank a lot~ @haotian-liu</BODY>\n\n<COMMENTS>\n<Comment by aneet-javis at 2023-11-22T07:13:53Z>\n@Sarahtorekryo I'm also trying to fine-tune LLaVA on custom dataset. Could you share dataset format/resources or code to start with? Thanks.\n</Comment>\n<Comment by haotian-liu at 2023-11-28T19:48:16Z>\nmodel_max_length means how many tokens are in total, this includes everything in the conversation, and image tokens (576).\r\n\r\nAny content that is larger than model_max_length will be truncated.\n</Comment>\n<Comment by 20191864218 at 2024-03-06T01:35:34Z>\n> @Sarahtorekryo I'm also trying to fine-tune LLaVA on custom dataset. Could you share dataset format/resources or code to start with? Thanks.\r\n\r\nHi,I encountered the same issue. Have you managed to resolve it?\n</Comment>\n<Comment by Sprinter1999 at 2024-04-07T06:34:46Z>\nHi,I encountered the same issue. I input 3 images for SFT. Have you managed to resolve it? And I wonder where to set model_max_length?\n</Comment>\n<Comment by Sprinter1999 at 2024-04-07T07:14:33Z>\n> Hi,I encountered the same issue. I input more than 2 images for SFT. Have you managed to resolve it? And I wonder where to set model_max_length?\r\n\r\nOh I've figured it out, thank you guys.\n</Comment>\n<Comment by lqqyyy at 2024-05-11T15:46:59Z>\nsame issue.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 815,
    "state": "closed",
    "created_by": "Mingyang-Han",
    "created_at": "2023-11-16T12:20:51Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/815</URL>\n\n<TITLE>[Question] When run finetune.sh, train.py shows an error \"in preprocess_plain     assert len(source) == 2\"</TITLE>\n\n<BODY>### Question\n\nI print the source in sources, it  shows as below:\r\n[{'from': 'human', 'value': '<image>\\nWhat types of signs are in the image?'}, {'from': 'gpt', 'value': 'The image contains several wooden street signs at an intersection, with some of them giving directions to various destinations. There is also a street sign next to a parking lot with a FILA store, and a group of signs posted on a post.'}, {'from': 'human', 'value': 'How are the signs directing people in the picture?'}, {'from': 'gpt', 'value': 'The signs in the picture direct people by providing information about street names, distances, destinations or points of interest, and by guiding them towards specific locations or facilities like a parking lot or a FILA store at that location.'}, {'from': 'human', 'value': 'What is the purpose of these signs?'}, {'from': 'gpt', 'value': 'The purpose of these signs is to provide guidance and necessary information to pedestrians, drivers, and other people navigating the area. They help individuals find their way to specific places, understand road rules and regulations, and ensure a smooth and orderly flow of traffic. They also contribute to the accessibility and readability of a city or town, making it easier for residents and tourists alike to explore and navigate the surroundings.'}, {'from': 'human', 'value': 'What kind of store is the FILA store?'}, {'from': 'gpt', 'value': \"The FILA store is a retail outlet for the FILA brand, which is a sportswear company known for its footwear, clothing, and accessories. Established in Italy in 1911, FILA has become a popular brand worldwide for its sports and lifestyle products. The store typically sells athletic shoes, sports clothing, and accessories for various activities such as running, tennis, basketball, and training, as well as casual wear like t-shirts, hoodies, and sweatpants that showcase the brand's distinct style and logo.\"}]\r\n\r\nas the code in train.py:\r\ndef preprocess_plain(\r\n    sources: Sequence[str],\r\n    tokenizer: transformers.PreTrainedTokenizer,\r\n) -> Dict:\r\n    # add end signal and concatenate together\r\n    conversations = []\r\n    for source in sources:\r\n        # sft\r\n        print(source)\r\n        print(len(sources))\r\n        print(len(source))\r\n        print(\"-------\")\r\n        # source=source[0]\r\n        # print(source)\r\n        assert len(source) == 2\r\n        assert DEFAULT_IMAGE_TOKEN in source[0]['value']\r\n        source[0]['value'] = DEFAULT_IMAGE_TOKEN\r\n        conversation = source[0]['value'] + source[1]['value'] + conversation_lib.default_conversation.sep\r\n        conversations.append(conversation)\r\n    # tokenize conversations\r\n    input_ids = [tokenizer_image_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations]\r\n    targets = copy.deepcopy(input_ids)\r\n    for target, source in zip(targets, sources):\r\n        tokenized_len = len(tokenizer_image_token(source[0]['value'], tokenizer))\r\n        target[:tokenized_len] = IGNORE_INDEX\r\n    return dict(input_ids=input_ids, labels=targets)\r\n\r\nthe source's length is always not  as 2,  is it a wrong of my json file? I use llava_v1_5_mix665k.json as input.</BODY>\n\n<COMMENTS>\n<Comment by anonymous-atom at 2023-11-20T08:51:28Z>\n@Jiushanhuadao Can you let me know how you resolved this ? I am getting this error `assert DEFAULT_IMAGE_TOKEN in source[0]['value']`\n</Comment>\n<Comment by anonymous-atom at 2023-11-20T10:18:19Z>\nNow I am getting the same error as you `assert len(source) == 2\r\nAssertionError`\n</Comment>\n<Comment by Mingyang-Han at 2023-11-21T05:45:12Z>\n> Now I am getting the same error as you `assert len(source) == 2 AssertionError`\r\n\r\nI got the error because I use the old version finetune.sh and use a error json which is not suitable for the llava1.5-finetune.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 814,
    "state": "open",
    "created_by": "an1018",
    "created_at": "2023-11-16T12:00:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/814</URL>\n\n<TITLE>[Question] Output multiple options</TITLE>\n\n<BODY>### Question\n\nLike Seed-Bech dataset，answer with option's letter，but can only output one. I want to identify multiple objects in the image, how can I output multiple options？Thanks very much.\r\n\r\nfor example, the image includes dog and cat,\r\nA. dog\r\nB. cat\r\nC. panda\r\nD. pig\r\noutput：A, B</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 813,
    "state": "open",
    "created_by": "smzzl",
    "created_at": "2023-11-16T09:18:38Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/813</URL>\n\n<TITLE>Encountered problems when handling query with multiple images.</TITLE>\n\n<BODY>### Question\n\nEncountered an issue while processing inquiries involving multiple images in:\r\n\r\n **llava/model/llava_arch.py** at Line 119. `image_features = [x.flatten(0, 1).to(self.device) for x in image_features]`\r\n\r\nI'm questioning the use of flatten on tensor x at this point. Based on the subsequent code, it seems more appropriate to split it into a list rather then flatten it.\r\n\r\nIn my understanding, when `type(images) is list`  (line 114), each element in the ’images list‘ stores the images contained in the i-th sentence of the current batch. At this point, the ‘image_features list‘ stores the tensors resulting of 'self.encode_images' for each sentence in the batch. Therefore, the shape of x should be (i think) a tensor: [num, len, dim], where num represents the number of images for the current sentence. Flattening it combines num and len, causing issues when executing the code at line 178:\r\n\r\n`cur_image_features = image_features[cur_image_idx]`\r\n\r\nThe problem is that 'cur_image_idx' exceeds the range of image_features. \r\n\r\nThis is because 'cur_image_idx' can reach a maximum value equal to the sum of the total number of <image> tokens across all sentences. The preceding ‘flatten' results in each data entry providing only one image, limiting the maximum length of image_features to batch_size. This leads to errors when some sentences contain more than one <image>.\r\n\r\nI think that changing the flatten operation in Line 119 to create a list for all images, rather than concatenating all images of the same sentence in advance, is necessary to align with the use of ‘cur_image_idx‘.</BODY>\n\n<COMMENTS>\n<Comment by Yuki-Imajuku at 2023-11-21T05:54:39Z>\n+1\r\nThe process at the block from line 114 is just to concat all the images in the batch in order and encode them.\r\nHere is my understanding of the code and suggested modifications.\r\n```python\r\nif type(images) is list or images.ndim == 5:\r\n    concat_images = torch.cat([image for image in images], dim=0)  # -> (#images in a batch, C, H, W)\r\n    image_features = self.encode_images(concat_images)  # -> (#images in a batch, #patches per image, hidden_dim)\r\n    # split_sizes = [image.shape[0] for image in images]\r\n    # image_features = torch.split(image_features, split_sizes, dim=0)  # Divides into each sentences\r\n    # image_features = [x.flatten(0, 1).to(self.device) for x in image_features]  # -> (B, #images in a sentence * #patches, hidden_dim)\r\n    image_features = image_features.to(self.device)  # This might be needed\r\n```\n</Comment>\n<Comment by anonymous-atom at 2024-01-03T07:58:06Z>\n@Yuki-Imajuku @smzzl You got any solution for this then ?\n</Comment>\n<Comment by anonymous-atom at 2024-01-03T10:49:15Z>\n@haotian-liu could you please have a look at this issue ?\n</Comment>\n<Comment by smzzl at 2024-01-03T16:13:39Z>\n> @Yuki-Imajuku @smzzl You got any solution for this then ?\r\n\r\nNo, i use start and end token to rewrite it.\n</Comment>\n<Comment by anonymous-atom at 2024-01-03T16:43:15Z>\noh, I had to remove <image> from start/end of any question.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 812,
    "state": "closed",
    "created_by": "jaebbb",
    "created_at": "2023-11-16T00:38:55Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/812</URL>\n\n<TITLE>[Question] OMM error in V100*8 when pretrain</TITLE>\n\n<BODY>### Question\n\nHi, I'm trying to pretrain using xformer but I'm getting an omm error, do you know the solution?\r\nhere is my script code and log\r\nThank you\r\n\r\n```\r\nPROMPT_VERSION=plain\r\n########### DO NOT CHANGE ###########\r\n\r\ndeepspeed llava/train/train_xformers.py \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --model_name_or_path lmsys/vicuna-13b-v1.5 \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path ./playground/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json \\\r\n    --image_folder ./playground/data/LLaVA-Pretrain/images \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --tune_mm_mlp_adapter True \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 False \\\r\n    --output_dir ./checkpoints/llava-v1.5-13b-pretrain \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 24000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-3 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 False \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n```\r\n[2023-11-16 00:15:14,107] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-11-16 00:15:16,245] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\r\n[2023-11-16 00:15:16,245] [INFO] [runner.py:555:main] cmd = /opt/conda/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_xformers.py --deepspeed ./scripts/zero2.json --model_name_or_path lmsys/vicuna-13b-v1.5 --version plain --data_path ./playground/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json --image_folder ./playground/data/LLaVA-Pretrain/images --vision_tower openai/clip-vit-large-patch14 --tune_mm_mlp_adapter True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 False --output_dir ./checkpoints/llava-v1.5-13b-pretrain --num_train_epochs 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 24000 --save_total_limit 1 --learning_rate 2e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 False --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb\r\n[2023-11-16 00:15:17,506] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-11-16 00:15:19,460] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.13.4-1\r\n[2023-11-16 00:15:19,461] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.13.4-1\r\n[2023-11-16 00:15:19,461] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\r\n[2023-11-16 00:15:19,461] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\r\n[2023-11-16 00:15:19,461] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.13.4-1+cuda11.7\r\n[2023-11-16 00:15:19,461] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.13.4-1+cuda11.7\r\n[2023-11-16 00:15:19,461] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.13.4-1\r\n[2023-11-16 00:15:19,461] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\r\n[2023-11-16 00:15:19,461] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0\r\n[2023-11-16 00:15:19,461] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\r\n[2023-11-16 00:15:19,461] [INFO] [launch.py:163:main] dist_world_size=8\r\n[2023-11-16 00:15:19,461] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\r\n[2023-11-16 00:15:22,553] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-11-16 00:15:22,569] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-11-16 00:15:22,570] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-11-16 00:15:22,583] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-11-16 00:15:22,599] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-11-16 00:15:22,600] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-11-16 00:15:22,614] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[2023-11-16 00:15:22,656] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nERROR:root:xformers not found! Please install it before trying to use it.\r\nERROR:root:xformers not found! Please install it before trying to use it.\r\nERROR:root:xformers not found! Please install it before trying to use it.\r\nERROR:root:xformers not found! Please install it before trying to use it.\r\nERROR:root:xformers not found! Please install it before trying to use it.\r\nERROR:root:xformers not found! Please install it before trying to use it.\r\nERROR:root:xformers not found! Please install it before trying to use it.\r\n[2023-11-16 00:15:23,248] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-11-16 00:15:23,248] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-11-16 00:15:23,258] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-11-16 00:15:23,258] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-11-16 00:15:23,264] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-11-16 00:15:23,264] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-11-16 00:15:23,269] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-11-16 00:15:23,269] [INFO] [comm.py:594:init_distributed] cdb=None\r\nERROR:root:xformers not found! Please install it before trying to use it.\r\n[2023-11-16 00:15:23,303] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-11-16 00:15:23,303] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-11-16 00:15:23,304] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-11-16 00:15:23,304] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-11-16 00:15:23,310] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-11-16 00:15:23,310] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-11-16 00:15:23,310] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n[2023-11-16 00:15:23,382] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-11-16 00:15:23,382] [INFO] [comm.py:594:init_distributed] cdb=None\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nDownloading shards:   0%|                                                                                                                                                                  | 0/3 [00:00<?, ?it/s]You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nDownloading shards:   0%|                                                                                                                                                                  | 0/3 [00:00<?, ?it/s]You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nDownloading shards:   0%|                                                                                                                                                                  | 0/3 [00:00<?, ?it/s]You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nDownloading shards:   0%|                                                                                                                                                                  | 0/3 [00:00<?, ?it/sYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.                                | 0.00/9.95G [00:00<?, ?B/s]You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\r\npytorch_model-00001-of-00003.bin: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9.95G/9.95G [00:43<00:00, 231MB/s]pytorch_model-00002-of-00003.bin: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9.90G/9.90G [00:41<00:00, 237MB/s]pytorch_model-00003-of-00003.bin: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6.18G/6.18G [00:26<00:00, 237MB/s]Downloading shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:52<00:00, 37.38s/it]Downloading shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:51<00:00, 37.31s/it]Downloading shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:51<00:00, 37.32s/it]Downloading shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:51<00:00, 37.32s/it]Downloading shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:51<00:00, 37.18s/it]Downloading shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:51<00:00, 37.19s/it]Downloading shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:51<00:00, 37.32s/it]Downloading shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:51<00:00, 37.19s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:30<00:00, 10.29s/it](…)v1.5/resolve/main/generation_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 192/192 [00:00<00:00, 640kB/s](…)-v1.5/resolve/main/tokenizer_config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 749/749 [00:00<00:00, 3.72MB/s]tokenizer.model: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500k/500k [00:00<00:00, 30.2MB/s](…)1.5/resolve/main/special_tokens_map.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 438/438 [00:00<00:00, 1.44MB/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:33<00:00, 11.05s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:33<00:00, 11.03s/it](…)14/resolve/main/preprocessor_config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 316/316 [00:00<00:00, 1.14MB/s](…)t-large-patch14/resolve/main/config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.52k/4.52k [00:00<00:00, 15.6MB/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:37<00:00, 12.33s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:38<00:00, 12.68s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:37<00:00, 12.55s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:37<00:00, 12.56s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:38<00:00, 12.97s/it]model.safetensors: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.71G/1.71G [00:10<00:00, 162MB/s]Formatting inputs...Skip in lazy mode\r\nTraceback (most recent call last):\r\n  File \"/LLaVA/llava/train/train_xformers.py\", line 13, in <module>\r\n    train()\r\n  File \"/LLaVA/llava/train/train.py\", line 932, in train\r\n    trainer.train()\r\n  File \"/home/jaebb/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/home/jaebb/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 1656, in _inner_training_loop\r\n    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\r\n  File \"/home/jaebb/.local/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1198, in prepare\r\n    result = self._prepare_deepspeed(*args)\r\n  File \"/home/jaebb/.local/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1537, in _prepare_deepspeed\r\n    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\r\n  File \"/home/jaebb/.local/lib/python3.10/site-packages/deepspeed/__init__.py\", line 165, in initialize\r\n    engine = DeepSpeedEngine(args=args,\r\n  File \"/home/jaebb/.local/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 267, in __init__\r\n    self._configure_distributed_model(model)\r\n  File \"/home/jaebb/.local/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1048, in _configure_distributed_model\r\n    self.module.to(self.device)\r\n  File \"/home/jaebb/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 1900, in to\r\n    return super().to(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1145, in to\r\n    return self._apply(convert)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 797, in _apply\r\n    module._apply(fn)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 797, in _apply\r\n    module._apply(fn)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 797, in _apply\r\n    module._apply(fn)\r\n  [Previous line repeated 2 more times]\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 820, in _apply\r\n    param_applied = fn(param)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1143, in convert\r\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB (GPU 3; 31.74 GiB total capacity; 31.21 GiB already allocated; 97.38 MiB free; 31.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n```</BODY>\n\n<COMMENTS>\n<Comment by JerryDaHeLian at 2023-11-17T02:29:53Z>\n内存不够使！\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 811,
    "state": "open",
    "created_by": "fredshentu",
    "created_at": "2023-11-15T23:14:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/811</URL>\n\n<TITLE>[Question] is model parallelism supported for training?</TITLE>\n\n<BODY>### Question\n\nSay I have an cluster with 8 GPU but only 12G vram each, I can still train llava?\r\nIt seems that deepspeed can do a various of model parallelism (tensor parallelism, pipeline etc)\r\nI wonder if it is supported on LLaVA</BODY>\n\n<COMMENTS>\n<Comment by anonymous-atom at 2023-11-16T10:21:50Z>\nYes, they already used deepseed in the scripts.\r\ncheckout [scripts/v1_5/pretrain.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/pretrain.sh)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 810,
    "state": "open",
    "created_by": "Xyndra",
    "created_at": "2023-11-15T20:22:27Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/810</URL>\n\n<TITLE>IMPORTANT PROBLEM WITH CODE EXECUTION</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nWhilst trying to get something similar to [Draw A UI](https://github.com/SawyerHood/draw-a-ui/tree/main), I stumbled upon this critical problem where the AI can generate code which gets executed automatically. This could be used to create malicious web requests and links if someone gets the ai to say something dangerous.\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/71836523/b845c8fc-69b4-4a60-af27-7372f07f0677)</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 808,
    "state": "open",
    "created_by": "ZY123-GOOD",
    "created_at": "2023-11-15T13:42:32Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/808</URL>\n\n<TITLE>[Question] How to use the fine-tuned model？</TITLE>\n\n<BODY>### Question\n\nI have two questions.\r\n1.  I follow the instruction in scripts/v1.5 to pre-train and fine-tune the model. After pre-training, I get the mm_projector.bin; and after fine-tuning I get adapter_model.bin without new mm_projector.bin. I want to know is the projector frozen during the LoRa fine-tuning process.\r\n2. When `\"python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./liuhaotian --model-base /home/yaozhu/LLaVA/LLaVA_codes/vicuna\"`\r\nI got : \r\n\r\n```\r\nSome weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at /home/yaozhu/LLaVA/LLaVA_codes/vicuna and are newly initialized: ['model.mm_projector.0.weight', 'model.mm_projector.2.bias', 'model.mm_projector.2.weight', 'model.mm_projector.0.bias']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n```\r\n\r\nMay I ask how to correctly load the mm_projector?</BODY>\n\n<COMMENTS>\n<Comment by ZY123-GOOD at 2023-11-16T01:33:07Z>\nAfter carefully reading the code, the issue has been resolved. I'd like to double-check my understanding. \r\nDuring the pretraining phase, the mm_projector was trained. In the finetuning phase, both the mm_projector and the language model were fine-tuned, and the mm_projector was saved as non_lora_trainables.bin. \r\nWhen loading the model in the inference process, the language model was loaded in the first stage, without the mm_projector being loaded. In the second stage, when merging the adapter and the language model, the mm_projector was loaded from non_lora_trainables.bin. Is my understanding correct?\r\nThanks a lot.\n</Comment>\n<Comment by Eric-is-good at 2023-11-24T02:33:55Z>\nHow did you solve the problem? I am using my own language model, but there is also a situation where weight cannot be loaded\n</Comment>\n<Comment by curiousNick1 at 2023-12-20T08:10:18Z>\n@ZY123-GOOD  Can you please check my problem when loading merged-lora checkpoint? It seems that the merged checkpoint has the same format as Vicuna-v1.5-7b and it has no information about the vision tower as well as the mm-projector. After I add configs about Vision tower and projector(non_lora_trainable.bin) to the config file, there would be an error saying 'the weight is on the meta device'\r\nBut the code could run with the original llava projector(mm_projector.bin), I wonder if these two bin files has some differences?\n</Comment>\n<Comment by yuezih at 2023-12-27T00:39:09Z>\n@ZY123-GOOD @Eric-is-good @curiousNick1 Hi all, FYI:\r\nhttps://github.com/haotian-liu/LLaVA/issues/474#issuecomment-1760890593\r\n\r\n:)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 807,
    "state": "closed",
    "created_by": "zht8506",
    "created_at": "2023-11-15T11:47:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/807</URL>\n\n<TITLE>error when loading \"CLIP\"vision encoder</TITLE>\n\n<BODY>![image](https://github.com/haotian-liu/LLaVA/assets/81303574/8af401f4-f4dd-47da-8fb5-d11333621893)\r\nI use these code to load the weight, \"LLaVA-main/liuhaotian/llava-v1.5-7b\" and \"LLaVA-main/openai/clip-vit-large-patch14-336\". But i meet errors, as follow:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/81303574/ba5eaa2b-cc24-458b-b968-7b91d7efed89)\r\nhow to solve this issue? thank you.</BODY>\n\n<COMMENTS>\n<Comment by Norrrrrrr-lyn at 2024-12-02T15:54:55Z>\nHow did you solve the issue? I've encountered the same issue.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 806,
    "state": "open",
    "created_by": "laserwave",
    "created_at": "2023-11-15T10:58:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/806</URL>\n\n<TITLE>[Question] How did you calculate the metric of POPE benchmark for the LLaVA-1.5 model.</TITLE>\n\n<BODY>### Question\n\nI did not find any description in the paper. As it has three splits: adversarial, random and popular, I guess the score reported in the paper may be the average acc or F1 of these three splits.</BODY>\n\n<COMMENTS>\n<Comment by zhongshsh at 2024-02-05T01:39:08Z>\nhttps://github.com/haotian-liu/LLaVA/issues/626#issuecomment-1772025961\n</Comment>\n<Comment by 191220042 at 2024-06-04T14:30:11Z>\nwhere is pope/val2014\n</Comment>\n<Comment by cookiesupers22 at 2024-11-09T05:18:18Z>\n> where is pope/val2014\r\n\r\nDid you find out? I keep getting this error \r\nFileNotFoundError: [Errno 2] No such file or directory: '/content/LLaVA/playground/data/eval/pope/val2014/COCO_val2014_000000310196.jpg'\r\nwhen trying to reproduce the results of LLaVA on pope?\n</Comment>\n<Comment by heshandevaka at 2025-04-11T15:40:19Z>\n> where is pope/val2014\n\nI think you can download it from [here](https://cocodataset.org/#download)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 805,
    "state": "open",
    "created_by": "zohrehghaderi",
    "created_at": "2023-11-15T10:47:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/805</URL>\n\n<TITLE>Export ONNX 4 bit</TITLE>\n\n<BODY>### feature\n\nHi ,\r\n\r\nFirstly, I would like to express my gratitude for your released model. I attempted to export the 4-bit Llava model to ONNX, but encountered an error indicating that ONNX does not support it.\r\n\r\nes.py\", line 248, in forward\r\n    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)\r\n  File \"/home/zghaderi/miniconda/envs/test/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 579, in matmul_4bit\r\n    return MatMul4Bit.apply(A, B, out, bias, quant_state)\r\n  File \"/home/zghaderi/miniconda/envs/test/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\r\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\r\nRuntimeError: _Map_base::at\r\n\r\nDo you have any suggestions or plans to incorporate ONNX model export?</BODY>\n\n<COMMENTS>\n<Comment by woaichixihong at 2024-03-08T06:21:34Z>\nHello, have you successfully converted to onnx\n</Comment>\n<Comment by hunshi34 at 2024-04-20T09:04:04Z>\nsearching for  solutions\n</Comment>\n<Comment by zhangyu68 at 2024-08-19T09:07:04Z>\nsearching for solutions\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 804,
    "state": "open",
    "created_by": "henrycjh",
    "created_at": "2023-11-15T10:36:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/804</URL>\n\n<TITLE>[Question] A quick question about LLaVA1.5 paper</TITLE>\n\n<BODY>### Question\r\n\r\nThanks for your attention. My question is: \"For Visual Genome, we sample 10 annotations for images with additional annotations.\", how do you find the images with additional annotations in VG? I mean how do you get 86417 images out of 108K</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 803,
    "state": "closed",
    "created_by": "uyo9ko",
    "created_at": "2023-11-15T10:04:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/803</URL>\n\n<TITLE>[Question] Will randomly shuffling the data sample order in llava_v1_5_mix665k.json affect the final model's performance when fine-tuning?</TITLE>\n\n<BODY>### Question\n\nWill randomly shuffling the data sample order in llava_v1_5_mix665k.json affect the final model's performance when fine-tuning?\r\nIs the data order in llava_v1_5_mix665k.json intentionally set?</BODY>\n\n<COMMENTS>\n<Comment by uyo9ko at 2023-11-20T11:51:00Z>\nsorry, i find it is random shuffled\n</Comment>\n<Comment by jiazhen-code at 2024-09-15T04:41:41Z>\nThey don't seem to have been randomly shuffled.\r\n\r\n```python\r\n# data is read from that json.\r\nfor s, d in enumerate(data):\r\n    if 'ocr' in d['image']:\r\n        print(s)\r\n```\r\n\r\nResult:\r\n![image](https://github.com/user-attachments/assets/b161b07b-bd2b-44d5-bd26-91cfac019aa0)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 802,
    "state": "open",
    "created_by": "Vinaysukhesh98",
    "created_at": "2023-11-14T06:46:33Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/802</URL>\n\n<TITLE>HI Unable to compile lava model [Question]</TITLE>\n\n<BODY>### Question\n\ni have cloned the tokens while compiling  the model im getting below error:\r\npython3 -m mlc_llm.build --model $MODEL_NAME --target android --max-seq-len 768 --quantization q4f16_1\r\nTraceback (most recent call last):\r\n  File \"/home//miniconda3/envs/tvm-build-venv/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/p/miniconda3/envs/tvm-build-venv/lib/python3.8/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/lap/Documents/Vinay/Mob/mlc-llm/mlc_llm/build.py\", line 47, in <module>\r\n    main()\r\n  File \"/home/Documents/Vinay/Mob/mlc-llm/mlc_llm/build.py\", line 41, in main\r\n    parsed_args = core._parse_args(parsed_args)  # pylint: disable=protected-access\r\n  File \"/home//Documents/Vinay/Mob/mlc-llm/mlc_llm/core.py\", line 325, in _parse_args\r\n    parsed = _setup_model_path(parsed)\r\n  File \"/home//Documents/Vinay/Mob/mlc-llm/mlc_llm/core.py\", line 368, in _setup_model_path\r\n    validate_config(args.model_path)\r\n  File \"/home/Documents/Vinay/Mob/mlc-llm/mlc_llm/core.py\", line 411, in validate_config\r\n    assert (\r\nAssertionError: Model type llava not supported.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 801,
    "state": "open",
    "created_by": "YerongLi",
    "created_at": "2023-11-14T03:38:15Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/801</URL>\n\n<TITLE>[Question] How to post to the uvicorn server</TITLE>\n\n<BODY>### Question\n\nAfter I started the controller and model workers, how I can POST and feed image and text into it?\r\n\r\n![Screenshot from 2023-11-13 21-36-50](https://github.com/haotian-liu/LLaVA/assets/13112023/7f07d52f-5a0b-4312-8518-a264f1771253)\r\n![Screenshot from 2023-11-13 21-36-43](https://github.com/haotian-liu/LLaVA/assets/13112023/0bc4097f-dbe4-4463-854d-e8a59986e771)</BODY>\n\n<COMMENTS>\n<Comment by YerongLi at 2023-11-14T04:49:21Z>\nI am trying to do it with \r\n`curl -X POST -H \"Content-Type: application/json\" -d '{\"text\": \"What is this?\", \"model\":\"llava-v1.5-13b\"}' http://localhost:10000/worker_generate_stream --output \"result.txt\"`\n</Comment>\n<Comment by 459737087 at 2024-04-08T06:04:26Z>\nHi! Did you solve it ? @YerongLi\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 800,
    "state": "open",
    "created_by": "Eric-is-good",
    "created_at": "2023-11-14T02:39:59Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/800</URL>\n\n<TITLE>[Question] How to change the base language model, which part of codes should be changed?</TITLE>\n\n<BODY>### Question\n\nI want to use llama2 model, a bigger one, i use the model from huggingface. If i just change the model path, for example, just change the \"--model_name_or_path\" in pretrain.sh, can it also works?</BODY>\n\n<COMMENTS>\n<Comment by tosiyuki at 2023-11-17T08:50:29Z>\nLlama2 can also be loaded with LlamaForCausalLM, so maybe just changing \"--model_name_or_path\" is all that is needed.\r\nIf that doesn't work, I think you need to modify around the following code.\r\nhttps://github.com/haotian-liu/LLaVA/blob/80540fb4bf4dad118d87d42bd2fb55e6f3b96f16/llava/model/language_model/llava_llama.py#L40\r\nhttps://github.com/haotian-liu/LLaVA/blob/80540fb4bf4dad118d87d42bd2fb55e6f3b96f16/llava/model/llava_arch.py#L85\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 799,
    "state": "open",
    "created_by": "Qiulin-W",
    "created_at": "2023-11-14T02:32:49Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/799</URL>\n\n<TITLE>[Question] Model parameter not update?</TITLE>\n\n<BODY>### Question\n\nWhen I finetune the LLaVA1.5 model and try to print the model parameters in forward() function to monitor their update, the printed parameters seem not to be updated? Is it related to the usage of deepspeed zero3? Could you please give some explanation on this? Thanks!</BODY>\n\n<COMMENTS>\n<Comment by Qiulin-W at 2023-11-14T02:35:01Z>\nI found a related issue here: https://github.com/huggingface/transformers/issues/24860\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 797,
    "state": "open",
    "created_by": "beltonk",
    "created_at": "2023-11-12T19:30:08Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/797</URL>\n\n<TITLE>[Usage] Network Error while Inferencing with llava-v1.5-13b on Apple M1</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: It keep telling, \"NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.\" when click send with the example image & prompt. No detailed error is shown. Just \"Caught Unknown Error\", I tried to use try/except to catch the error, it appears to me it breaks at \"for new_text in streamer:\" in generate_stream(), where the queue.py will throw raise empty exception. llava-v1.5-7b works, so, it could be something wrong in loading model, not sure if \"model = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\" successful or not...\r\n\r\nI tried:\r\n\r\n- adding --device mps\r\n- ensuring/installing torch==2.1.0 torchvision==0.16.0\r\n- uninstalling bitsandbytes\r\n- trying various versions of transformers, including hugging face ones\r\n\r\nllava-v1.5-7b works correctly however.\r\n\r\nLog: \r\n```\r\n2023-11-13 03:08:24 | ERROR | stderr | \r\n2023-11-13 03:08:36 | INFO | model_worker | Register to controller\r\n2023-11-13 03:08:36 | ERROR | stderr | INFO:     Started server process [91636]\r\n2023-11-13 03:08:36 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2023-11-13 03:08:36 | ERROR | stderr | INFO:     Application startup complete.\r\n2023-11-13 03:08:36 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)\r\n2023-11-13 03:08:45 | INFO | stdout | INFO:     127.0.0.1:57618 - \"POST /worker_get_status HTTP/1.1\" 200 OK\r\n2023-11-13 03:08:51 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: None. global_counter: 0\r\n2023-11-13 03:08:56 | INFO | stdout | INFO:     127.0.0.1:57643 - \"POST /worker_get_status HTTP/1.1\" 200 OK\r\n2023-11-13 03:08:58 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n2023-11-13 03:08:58 | INFO | stdout | INFO:     127.0.0.1:57653 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\n2023-11-13 03:09:06 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n2023-11-13 03:09:13 | INFO | stdout | Caught Unknown Error\r\n```</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-13T01:24:16Z>\nHi, can you try the CLI inference, which can give a more explicit error message, as this looks like some model related issue?\r\n\r\nhttps://github.com/haotian-liu/LLaVA#cli-inference\n</Comment>\n<Comment by beltonk at 2023-11-13T02:43:18Z>\nI just tried CLI works well with 13b, just super slow, like zero to a few words in a minute... And controller/worker/gradio still not work. Same result can be always reproducible after clean reboot.\r\n\r\nCLI with 5b works well and fast, just like controller/worker/gradio does.\n</Comment>\n<Comment by beltonk at 2023-11-13T02:44:17Z>\nMine is M1 Max with 32GB RAM\n</Comment>\n<Comment by haotian-liu at 2023-11-13T02:45:32Z>\nYou can check activity monitor for the RAM usage. Maybe the 13B model is using too much memory and is using swap.\r\n\r\nIf above is the case, you can try https://github.com/oobabooga/text-generation-webui/pull/4305 for now before we officially support quantization on macOS / Windows.\n</Comment>\n<Comment by beltonk at 2023-11-13T03:13:34Z>\nI agree that should probably be RAM issue. I have monitored CPU/GPU/RAM while I ran them. Memory used was consistently marginal, somewhere between 29-31 GB for 13b, with consistently high memory pressure, same for both CLI & non-CLI, but only CLI can yield result.\r\n\r\nI tried llama.cpp's llava-cli example too, it works perfectly with pre-converted 7b models: f16, q5_k, q4_k, they all respond pretty fast. 13b's q4_k & q5_k also runs pretty fast too. Haven't tried 13b-f16 model yet. But it reveals that quantization will be the solution like you said.\r\n\r\nWill try https://github.com/oobabooga/text-generation-webui/pull/4305 later too.\r\n\r\nLook forward to quantization support! Amazing works here!\n</Comment>\n<Comment by beltonk at 2023-11-13T05:32:01Z>\nI just tried https://github.com/oobabooga/text-generation-webui, it is working, but very slow.\r\n\r\nI'm not sure I'm doing the right thing, I used pre-converted q4 q5 models from https://huggingface.co/mys/ggml_llava-v1.5-13b, and no issue with inference and with or without --load-in-4bit, just slow\r\n\r\nHowever, if I use --model liuhaotian_llava-v1.5-13b, it will say:\r\nImportError: Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes`\n</Comment>\n<Comment by YerongLi at 2023-11-14T03:12:19Z>\nHow did you post to the server after start the controller and the model worker?\n</Comment>\n<Comment by beltonk at 2023-11-14T14:55:52Z>\nSorry, not sure about your question?\r\n\r\nDo you mean the sequence of launching LLaVA? Controller->Model Worker->Gradio\r\n\r\nOr you mean the textgen command?I tried something like this:\r\npython server.py --model ggml-model-q4_k.gguf --multimodal-pipeline llava-v1.5-13b --load-in-4bit\r\npython server.py   --model liuhaotian_llava-v1.5-13b   --multimodal-pipeline llava-v1.5-13b  --disable_exllama  --load-in-4bit\r\npython server.py   --model liuhaotian_llava-v1.5-13b   --multimodal-pipeline llava-v1.5-13b  --load-in-4bit\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 796,
    "state": "open",
    "created_by": "papasanimohansrinivas",
    "created_at": "2023-11-12T08:50:48Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/796</URL>\n\n<TITLE>[Question] how to use inference with flash attention on llava 1.5 7b int4</TITLE>\n\n<BODY>### Question\n\nHi @haotian-liu , how to use model for inference using flash attention on llava 1.5 7b int4</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-13T01:26:24Z>\nHi, this may be implemented after we upgrade the `transformers` package to the latest, but currently we do have other priorities. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 795,
    "state": "closed",
    "created_by": "chae-won-kim",
    "created_at": "2023-11-12T05:01:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/795</URL>\n\n<TITLE>[Question] extract_video_frames.py missing for SEED dataset</TITLE>\n\n<BODY>### Question\n\nHi,\r\n\r\nYou guys mentioned in [evaluation.md](https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md#seed-bench) that you provide the code `extract_video_frames.py` to extract from videos, but the file is nowhere to be found. Can you please provide the code? Thanks!</BODY>\n\n<COMMENTS>\n<Comment by chae-won-kim at 2023-11-12T05:08:10Z>\nI found in under the eval.zip file the authors provided for those who need it\n</Comment>\n<Comment by ge-bin-hui at 2025-03-20T09:54:41Z>\n@chae-won-kim Hey bro! I still can not find the eval.zip, could you offer me the url?\n</Comment>\n<Comment by haiduo at 2025-07-22T13:18:54Z>\nHere [extract_video_frames.py](https://drive.google.com/file/d/1atZSBBrAX54yYpxtVVW33zFvcnaHeFPy/view)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 794,
    "state": "open",
    "created_by": "aa221",
    "created_at": "2023-11-12T04:10:47Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/794</URL>\n\n<TITLE>[Question] Trying to finetune LLAVA on a custom dataset without the use of a GPU.</TITLE>\n\n<BODY>### Question\n\nIs there a way I can download the checkpoints of the pre-trained LLAVA, and then fine-tune the data on my custom data set? I don't have a GPU so any tips would be appreciated.</BODY>\n\n<COMMENTS>\n<Comment by tosiyuki at 2023-11-17T09:15:29Z>\nThe dataset must be in the same format as LLaVA and \"--data_path\" in the training script must be replaced with your data path.\r\nIf you want to fine-tune the model after the second stage training, you will need to modify the training code slightly.\r\nhttps://github.com/haotian-liu/LLaVA/tree/main/scripts/v1_5\r\n\r\nBut to learn without GPU, the learning time is enormous...\n</Comment>\n<Comment by aneet-javis at 2023-11-22T07:16:31Z>\n@tosiyuki Could you please tell what is the format of LLaVA dataset? I'm trying to create a custom dataset for fine-tuning.\n</Comment>\n<Comment by tosiyuki at 2023-11-22T08:09:49Z>\n> @tosiyuki Could you please tell what is the format of LLaVA dataset? I'm trying to create a custom dataset for fine-tuning.\r\n\r\nYou can check data format here.\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md\n</Comment>\n<Comment by cherry956 at 2024-01-29T12:26:07Z>\n@tosiyuki Now I have my own dataset in the same format as LLaVA , So how I finetune llava-v1.5-7b with lora? I don't know the next I should do?\n</Comment>\n<Comment by linfei-mise at 2024-03-29T15:19:10Z>\n@cherry956 I have the same problem now, have you solved it please?\n</Comment>\n<Comment by SamuelSchmidgall at 2024-04-27T00:15:21Z>\nTry reading this @linfei-mise @cherry956 https://github.com/ubiai-incorporated/colab_repos/blob/master/LLAVA_finetuning.ipynb\n</Comment>\n<Comment by linfei-mise at 2024-05-24T08:24:32Z>\n@SamuelSchmidgall Thank you for your reply, I have resolved the issue.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 792,
    "state": "open",
    "created_by": "test16553",
    "created_at": "2023-11-11T09:26:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/792</URL>\n\n<TITLE>[Usage] How to load the quantized llava-7B model</TITLE>\n\n<BODY>### Describe the issue\n\nHello, how to use the following code to load the quantized llava-7B model, for example, to load llava-7B-4bit, how to modify the following code?</BODY>\n\n<COMMENTS>\n<Comment by tosiyuki at 2023-11-17T08:07:33Z>\nHere's how to load it in 4-bit and more.\r\nhttps://github.com/haotian-liu/LLaVA/blob/80540fb4bf4dad118d87d42bd2fb55e6f3b96f16/llava/model/builder.py#L26\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 791,
    "state": "open",
    "created_by": "molodec3",
    "created_at": "2023-11-10T20:54:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/791</URL>\n\n<TITLE>[Question] number of visual tokens</TITLE>\n\n<BODY>### Question\n\nHi! Can you please tell how many visual tokens are given to LM? Is it 576? If it's something else can you please show it in code where the output of clip is transformed except the projector?</BODY>\n\n<COMMENTS>\n<Comment by xmy0916 at 2023-11-12T08:58:25Z>\nhttps://github.com/haotian-liu/LLaVA/blob/main/llava/model/multimodal_encoder/clip_encoder.py#L32\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 790,
    "state": "open",
    "created_by": "smtabatabaie",
    "created_at": "2023-11-10T15:40:18Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/790</URL>\n\n<TITLE>Creating custom use cases based on specific images</TITLE>\n\n<BODY>### Question\n\nAwesome project, thanks very much.\r\nI wanted to ask if it is possible to provide special images for and make fine tune the model on those? for example, creating a manual for a specific tool or machine based on additional visual data. Kinda like RAG but for images.\r\nThanks</BODY>\n\n<COMMENTS>\n<Comment by anas-zafar at 2024-06-03T10:48:35Z>\nHi @smtabatabaie were you able to implement the RAG approach?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 788,
    "state": "open",
    "created_by": "StrangeTcy",
    "created_at": "2023-11-10T13:33:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/788</URL>\n\n<TITLE>[Usage] A path to the LoRA adapters isn't being handled properly</TITLE>\n\n<BODY>### Describe the issue\n\nFollowing the docs on LoRA models (https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md) we've run the following command:\r\n```\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path /root/raw_data_for_llava/raw_data_for_llava/LLaVAR/further_finetuning/lora --model-base meta-llama/Llama-2-7b-hf\r\n```\r\nand ran into the following error:\r\n```\r\n2023-11-10 13:00:47 | ERROR | stderr | \r\n2023-11-10 13:00:47 | INFO | stdout | Loading LoRA weights from /root/raw_data_for_llava/raw_data_for_llava/LLaVAR/further_finetuning/lora\r\n2023-11-10 13:00:47 | ERROR | stderr | Traceback (most recent call last):\r\n2023-11-10 13:00:47 | ERROR | stderr |   File \"/home/developer/anaconda3/envs/llava/lib/python3.10/site-packages/peft/utils/config.py\", line 177, in _get_peft_type\r\n2023-11-10 13:00:47 | ERROR | stderr |     config_file = hf_hub_download(\r\n2023-11-10 13:00:47 | ERROR | stderr |   File \"/home/developer/anaconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub-0.19.0-py3.8.egg/huggingface_hub/utils/_validators.py\", line 110, in _inner_fn\r\n2023-11-10 13:00:47 | ERROR | stderr |   File \"/home/developer/anaconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub-0.19.0-py3.8.egg/huggingface_hub/utils/_validators.py\", line 158, in validate_repo_id\r\n2023-11-10 13:00:47 | ERROR | stderr | huggingface_hub.utils._validators.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/root/raw_data_for_llava/raw_data_for_llava/LLaVAR/further_finetuning/lora'. Use `repo_type` argument if needed.\r\n2023-11-10 13:00:47 | ERROR | stderr | \r\n2023-11-10 13:00:47 | ERROR | stderr | During handling of the above exception, another exception occurred:\r\n2023-11-10 13:00:47 | ERROR | stderr | \r\n2023-11-10 13:00:47 | ERROR | stderr | Traceback (most recent call last):\r\n2023-11-10 13:00:47 | ERROR | stderr |   File \"/home/developer/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n2023-11-10 13:00:47 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n2023-11-10 13:00:47 | ERROR | stderr |   File \"/home/developer/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n2023-11-10 13:00:47 | ERROR | stderr |     exec(code, run_globals)\r\n2023-11-10 13:00:47 | ERROR | stderr |   File \"/home/developer/raw_data_for_llava/raw_data_for_llava/LLaVAR/LLaVA/llava/serve/model_worker.py\", line 275, in <module>\r\n2023-11-10 13:00:47 | ERROR | stderr |     worker = ModelWorker(args.controller_address,\r\n2023-11-10 13:00:47 | ERROR | stderr |   File \"/home/developer/raw_data_for_llava/raw_data_for_llava/LLaVAR/LLaVA/llava/serve/model_worker.py\", line 65, in __init__\r\n2023-11-10 13:00:47 | ERROR | stderr |     self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\r\n2023-11-10 13:00:47 | ERROR | stderr |   File \"/home/developer/raw_data_for_llava/raw_data_for_llava/LLaVAR/LLaVA/llava/model/builder.py\", line 115, in load_pretrained_model\r\n2023-11-10 13:00:47 | ERROR | stderr |     model = PeftModel.from_pretrained(model, model_path)\r\n2023-11-10 13:00:47 | ERROR | stderr |   File \"/home/developer/anaconda3/envs/llava/lib/python3.10/site-packages/peft/peft_model.py\", line 244, in from_pretrained\r\n2023-11-10 13:00:47 | ERROR | stderr |     PeftConfig._get_peft_type(\r\n2023-11-10 13:00:47 | ERROR | stderr |   File \"/home/developer/anaconda3/envs/llava/lib/python3.10/site-packages/peft/utils/config.py\", line 183, in _get_peft_type\r\n2023-11-10 13:00:47 | ERROR | stderr |     raise ValueError(f\"Can't find '{CONFIG_NAME}' at '{model_id}'\")\r\n2023-11-10 13:00:47 | ERROR | stderr | ValueError: Can't find 'adapter_config.json' at '/root/raw_data_for_llava/raw_data_for_llava/LLaVAR/further_finetuning/lora'\r\n```\r\n\r\nSo apparently something is going wrong with parsing the path to the adapter; after reviewing https://github.com/huggingface/huggingface_hub/blob/18d0ae2ebfcfc696c50a67e7dad8fa63f5f23068/src/huggingface_hub/utils/_validators.py#L123 and further \r\nmy guess is we're interpreting a `repo_type` as a `repo_id` or something similar.\r\nWhy might that be?</BODY>\n\n<COMMENTS>\n<Comment by sdip15fa at 2024-05-09T05:23:42Z>\ndid you solve the problem?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 787,
    "state": "open",
    "created_by": "yichaoshen-MS",
    "created_at": "2023-11-10T08:31:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/787</URL>\n\n<TITLE>[Question] OOM finetune full llava1.5 with bs1, but successfully finetune llava1.5 with lora bs 16 (80ggpu)</TITLE>\n\n<BODY>### Question\n\nWhen I use the official doc and script(https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task.sh), I can successfully finetune 13b llava1.5 with lora(bs 16; deepspeed 3;80g gpu), but failed in finetuning full 13b llava1.5 even bs1(OOM). Besides, I can also successfully finetune full 13b llava1.0 with same gpu and bs16. Is there any problem in official llava 1.5 scripts?</BODY>\n\n<COMMENTS>\n<Comment by naoto0804 at 2023-12-27T01:51:28Z>\nI have a similar issue using `finetune_task.sh` and still cannot figure out why;\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 786,
    "state": "open",
    "created_by": "jiaxiangc",
    "created_at": "2023-11-10T07:11:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/786</URL>\n\n<TITLE>[Question] Why none fine-tune llm test speed slowly？</TITLE>\n\n<BODY>### Question\n\nHello, when testing the performance of the model, I passed model-path as \"llava-13b\" and model-base as \"vicuna-13b\". I did this to test how the non-fine-tuned \"vicuña\" performs. However, I noticed that the inference speed becomes significantly slower using this approach. I would like to know the reason behind this and how to modify it.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 785,
    "state": "closed",
    "created_by": "mengjiexu",
    "created_at": "2023-11-10T06:04:06Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/785</URL>\n\n<TITLE>train finetune_qlora_7b.sh error: RuntimeError: expected scalar type Float but found BFloat16</TITLE>\n\n<BODY>### Question\n\nError is:\r\n```\r\nFormatting inputs...Skip in lazy mode\r\nRank: 0 partition count [1, 1, 1] and sizes[(319815680, False), (20971520, False), (8192, False)] \r\n  0%|          | 0/80000 [00:00<?, ?it/s]/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\nTraceback (most recent call last):\r\n  File \"/home/xmj/mycipan3/smartahc_algorithm/projects/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/home/xmj/mycipan3/smartahc_algorithm/projects/LLaVA/llava/train/train.py\", line 936, in train\r\n    trainer.train()\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2654, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2679, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1735, in forward\r\n    loss = self.module(*inputs, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/peft/peft_model.py\", line 922, in forward\r\n    return self.base_model(\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/xmj/mycipan3/smartahc_algorithm/projects/LLaVA/llava/model/language_model/llava_llama.py\", line 88, in forward\r\n    return super().forward(\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 806, in forward\r\n    outputs = self.model(\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 685, in forward\r\n    layer_outputs = torch.utils.checkpoint.checkpoint(\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\r\n    return CheckpointFunction.apply(function, preserve, *args)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\r\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 107, in forward\r\n    outputs = run_function(*args)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 681, in custom_forward\r\n    return module(*inputs, output_attentions, None)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 408, in forward\r\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\r\n    query_states = self.q_proj(hidden_states)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/peft/tuners/lora.py\", line 1076, in forward\r\n    self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\nRuntimeError: expected scalar type Float but found BFloat16\r\n```\r\nthe train config:\r\n```\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --bits 8 --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --model_name_or_path /home/xmj/mycipan3/smartahc_algorithm/projects/LLaVA/llava-v1.5-7b \\\r\n    --version v1 \\\r\n    --data_path /home/xmj/mycipan3/smartahc_algorithm/projects/LLaVA/llava_instruct_80k.json \\\r\n    --image_folder /home/xmj/mycipan2/COCO/train2017 \\\r\n    --vision_tower /home/xmj/mycipan3/smartahc_algorithm/projects/LLaVA/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter /home/xmj/mycipan3/smartahc_algorithm/projects/LLaVA/llava-v1.5-7b/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 2 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True\r\n```</BODY>\n\n<COMMENTS>\n<Comment by mengjiexu at 2023-11-10T06:05:53Z>\nthe training log is \r\n[log.txt](https://github.com/haotian-liu/LLaVA/files/13316131/log.txt)\n</Comment>\n<Comment by mengjiexu at 2023-11-10T06:45:00Z>\nI find the some of the linear layers' input becomes tf.float32, I don't know why, and  forced conversion to bfloat16 , then it can run.\n</Comment>\n<Comment by mengjiexu at 2023-11-10T07:40:12Z>\nthe param list shows after every lora.A.weight layer, the data willl become tf.float32:\r\n\r\nbase_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight ------- torch.Size([128, 4096]) torch.bfloat16\r\nbase_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight ------- torch.Size([4096, 128]) torch.bfloat16\r\nbase_model.model.model.layers.7.self_attn.k_proj.weight ------- torch.Size([4096, 4096]) torch.int8\r\nbase_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight ------- torch.Size([128, 4096]) torch.bfloat16\r\nbase_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight ------- torch.Size([4096, 128]) torch.bfloat16\r\nbase_model.model.model.layers.7.self_attn.v_proj.weight ------- torch.Size([4096, 4096]) torch.int8\r\nbase_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight ------- torch.Size([128, 4096]) torch.bfloat16\r\nbase_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight ------- torch.Size([4096, 128]) torch.bfloat16\r\nbase_model.model.model.layers.7.self_attn.o_proj.weight ------- torch.Size([4096, 4096]) torch.int8\r\nbase_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight ------- torch.Size([128, 4096]) torch.bfloat16\r\nbase_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight ------- torch.Size([4096, 128]) torch.bfloat16\r\n\r\nDoes someone know why?\n</Comment>\n<Comment by attnmamba at 2024-01-31T23:00:11Z>\n@mengjiexu How did you solve your problem?\n</Comment>\n<Comment by mengjiexu at 2024-02-01T03:16:59Z>\nhttps://github.com/haotian-liu/LLaVA/issues/785#issuecomment-1805189303\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 784,
    "state": "open",
    "created_by": "simplelifetime",
    "created_at": "2023-11-10T02:09:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/784</URL>\n\n<TITLE>about finetune_task_lora.sh</TITLE>\n\n<BODY>### Describe the issue\n\nWhat does mm_projector_lr mean in the training script. Does it indicate that the mm_projector will be tuned during lora training? But I don't see the mm_projector parameters saved in the result checkpoint files. Can you show how to load the model after lora fine-tuning on custom tasks?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-10T02:37:41Z>\nit means that the learning rate of the mm_projector and it should be saved in `non_lora_trainables.bin`\n</Comment>\n<Comment by ZY123-GOOD at 2023-11-15T13:47:10Z>\nhello. Do you know how to load the model after lora fine-tuning? I have encountered: `Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at /home/yaozhu/LLaVA/LLaVA_codes/vicuna and are newly initialized: ['model.mm_projector.0.weight', 'model.mm_projector.2.bias', 'model.mm_projector.2.weight', 'model.mm_projector.0.bias']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.`\n</Comment>\n<Comment by tosiyuki at 2023-11-17T08:44:03Z>\n> hello. Do you know how to load the model after lora fine-tuning? I have encountered: `Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at /home/yaozhu/LLaVA/LLaVA_codes/vicuna and are newly initialized: ['model.mm_projector.0.weight', 'model.mm_projector.2.bias', 'model.mm_projector.2.weight', 'model.mm_projector.0.bias'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.`\r\n\r\nThe following is how to LOAD mm_projector after tuning by LoRA.\r\nhttps://github.com/haotian-liu/LLaVA/blob/786aa6a19ea10edc6f574ad2e16276974e9aaa3a/llava/model/builder.py#L60-L75\n</Comment>\n<Comment by ZY123-GOOD at 2023-11-17T08:47:08Z>\nThanks!\n</Comment>\n<Comment by bluesky333 at 2023-12-01T18:40:49Z>\nI had this issue where finetune_task_lora.sh doesn't create mm_projector.bin which also limited my usage of the finetuned_lora model. (I cannot merge or use it for inference). I changed extract_mm_projector to extract mm_projector.bin from non_lora_trainables.bin.\n</Comment>\n<Comment by terminator123 at 2023-12-19T06:40:51Z>\n> I had this issue where finetune_task_lora.sh doesn't create mm_projector.bin which also limited my usage of the finetuned_lora model. (I cannot merge or use it for inference). I changed extract_mm_projector to extract mm_projector.bin from non_lora_trainables.bin.\r\n\r\ndou you known the non_lora_trainables.bin saves the merged mm_projector params or only the lora part of it ?\n</Comment>\n<Comment by cherry956 at 2024-01-29T12:41:21Z>\nI have my own dataset, now I want to use it to finetune LLava-v1.5-7b with lora. After finetune the model, I want to use it to predict. Can you tell me some detailed steps what should I do? I am contact it the first time. Thanks.\n</Comment>\n<Comment by mrseanryan at 2024-02-19T20:44:13Z>\n@cherry956  in case it helps -  this fork shows how to fine-tune v1.5\r\n\r\nhttps://github.com/mrseanryan/finetune_LLaVA\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 783,
    "state": "closed",
    "created_by": "mengjiexu",
    "created_at": "2023-11-10T02:00:25Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/783</URL>\n\n<TITLE>Error when training finetune_qlora_13b.sh :only Tensors of floating point and complex dtype can require gradients</TITLE>\n\n<BODY>### Describe the issue\n\nThe Error:\r\n`[2023-11-10 09:35:37,861] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:35<00:00, 11.70s/it]\r\nAdding LoRA adapters...\r\nTraceback (most recent call last):\r\n  File \"/home/xmj/mycipan3/smartahc_algorithm/projects/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/home/xmj/mycipan3/smartahc_algorithm/projects/LLaVA/llava/train/train.py\", line 874, in train\r\n    model.get_model().initialize_vision_modules(\r\n  File \"/home/xmj/mycipan3/smartahc_algorithm/projects/LLaVA/llava/model/llava_arch.py\", line 76, in initialize_vision_modules\r\n    p.requires_grad = True\r\nRuntimeError: only Tensors of floating point and complex dtype can require gradients`\r\n\r\nThe train script:\r\n`deepspeed llava/train/train_mem.py \\\r\n    --deepspeed ./scripts/zero2.json \\\r\n    --lora_enable True --bits 8 \\\r\n    --model_name_or_path LLaVA/llava-v1.5-13b \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path LLaVA/llava_instruct_80k.json \\\r\n    --image_folder COCO/train2017 \\\r\n    --vision_tower LLaVA/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter LLaVA/llava-v1.5-13b/mm_projector.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-$MODEL_VERSION-finetune_lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --gradient_accumulation_steps 32 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 100 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --dataloader_num_workers 4`\r\n\r\nWhy it shows this error? I used pip install the training environment and run the finetune_qlora_13b.sh.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-10T02:39:00Z>\nHi, does https://github.com/haotian-liu/LLaVA/commit/8467850a63aa0d6f47aa150c53aca4751f0d3d14 fix the issue?\n</Comment>\n<Comment by mengjiexu at 2023-11-10T03:32:07Z>\nyes, it fix the problem, but another error occus:\r\n  ```\r\n0%|                                                                                                                                                                                                                      | 0/80000 [00:00<?, ?it/s]/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\nTraceback (most recent call last):\r\n  File \"/home/xmj/mycipan3/smartahc_algorithm/projects/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/home/xmj/mycipan3/smartahc_algorithm/projects/LLaVA/llava/train/train.py\", line 933, in train\r\n    trainer.train()\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2654, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2679, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1735, in forward\r\n    loss = self.module(*inputs, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/peft/peft_model.py\", line 922, in forward\r\n    return self.base_model(\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/xmj/mycipan3/smartahc_algorithm/projects/LLaVA/llava/model/language_model/llava_llama.py\", line 88, in forward\r\n    return super().forward(\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 806, in forward\r\n    outputs = self.model(\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 685, in forward\r\n    layer_outputs = torch.utils.checkpoint.checkpoint(\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\r\n    return CheckpointFunction.apply(function, preserve, *args)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\r\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 107, in forward\r\n    outputs = run_function(*args)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 681, in custom_forward\r\n    return module(*inputs, output_attentions, None)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 408, in forward\r\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\r\n    query_states = self.q_proj(hidden_states)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/peft/tuners/lora.py\", line 1076, in forward\r\n    self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\nRuntimeError: expected scalar type Float but found BFloat16\r\n```\n</Comment>\n<Comment by mengjiexu at 2023-11-10T03:38:04Z>\nI turn off the bfloat16 use: --bf16 False , but it shows Error:\r\n\r\n```\r\nRank: 0 partition count [1, 1, 1] and sizes[(319815680, False), (20971520, False), (8192, False)] \r\n  0%|                                                                                                                                                                                                                      | 0/80000 [00:00<?, ?it/s]Traceback (most recent call last):\r\n  File \"/home/xmj/mycipan3/smartahc_algorithm/projects/LLaVA/llava/train/train_mem.py\", line 13, in <module>\r\n    train()\r\n  File \"/home/xmj/mycipan3/smartahc_algorithm/projects/LLaVA/llava/train/train.py\", line 933, in train\r\n    trainer.train()\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2654, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer.py\", line 2679, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1735, in forward\r\n    loss = self.module(*inputs, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/peft/peft_model.py\", line 922, in forward\r\n    return self.base_model(\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/xmj/mycipan3/smartahc_algorithm/projects/LLaVA/llava/model/language_model/llava_llama.py\", line 79, in forward\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/home/xmj/mycipan3/smartahc_algorithm/projects/LLaVA/llava/model/llava_arch.py\", line 129, in prepare_inputs_labels_for_multimodal\r\n    image_features = self.encode_images(images)\r\n  File \"/home/xmj/mycipan3/smartahc_algorithm/projects/LLaVA/llava/model/llava_arch.py\", line 103, in encode_images\r\n    image_features = self.get_model().get_vision_tower()(images)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/xmj/mycipan3/smartahc_algorithm/projects/LLaVA/llava/model/multimodal_encoder/clip_encoder.py\", line 48, in forward\r\n    image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 941, in forward\r\n    return self.vision_model(\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 867, in forward\r\n    hidden_states = self.pre_layrnorm(hidden_states)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/normalization.py\", line 190, in forward\r\n    return F.layer_norm(\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/functional.py\", line 2515, in layer_norm\r\n    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\r\nRuntimeError: expected scalar type Half but found Float\r\n```\n</Comment>\n<Comment by a2382625920 at 2024-01-08T08:54:47Z>\nHi, I'm experiencing the same error, has the error been resolved yet?\n</Comment>\n<Comment by hanoonaR at 2024-01-27T07:58:56Z>\nHi @haotian-liu , @mengjiexu \r\n\r\nDo you have any suggestions on how to solve this issue in qLoRA training with `--lora_r 128 --lora_alpha 256 --bits 8 --bf16 True --tf32 True`\r\n\r\nreturn F.linear(input, self.weight, self.bias)\r\nRuntimeError: expected scalar type Float but found BFloat16\r\n\r\nThank you in advance.\n</Comment>\n<Comment by mengjiexu at 2024-01-27T08:25:02Z>\n> Hi @haotian-liu , @mengjiexu\r\n> \r\n> Do you have any suggestions on how to solve this issue in qLoRA training with `--lora_r 128 --lora_alpha 256 --bits 8 --bf16 True --tf32 True`\r\n> \r\n> return F.linear(input, self.weight, self.bias) RuntimeError: expected scalar type Float but found BFloat16\r\n> \r\n> Thank you in advance.\r\n\r\nyou can add code below in the F.linear:\r\n`\r\nif self.weight.dtype != input.dtype:\r\n    input = input.to(self.weight.dtype)\r\n`\n</Comment>\n<Comment by attnmamba at 2024-02-01T03:01:55Z>\n@mengjiexu Did you finally solve the problem?\n</Comment>\n<Comment by mengjiexu at 2024-02-01T03:18:05Z>\nhttps://github.com/haotian-liu/LLaVA/issues/783#issuecomment-1913075499\n</Comment>\n<Comment by hanoonaR at 2024-02-01T07:05:32Z>\nThe best solution I could find for this issue is: https://github.com/huggingface/peft/pull/874/files.\n</Comment>\n<Comment by sudan94 at 2024-05-11T11:46:53Z>\nI encountered a similar issue when fine-tuning DistilBERT for classification task, and I managed to resolve it using the following configuration:\r\n\r\n```python\r\nbnb_config = BitsAndBytesConfig(\r\n  load_in_4bit=True,\r\n  bnb_4bit_use_double_quant=True,\r\n  bnb_4bit_quant_type=\"nf4\",\r\n  bnb_4bit_compute_dtype='int8',\r\n)\r\n```` \r\nThe addition of ```bnb_4bit_compute_dtype='int8'``` specifically resolves the issue by using int8 data type for computation.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 782,
    "state": "closed",
    "created_by": "jameszhou-gl",
    "created_at": "2023-11-09T19:02:03Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/782</URL>\n\n<TITLE>[Question] Where is the function body for model.generate()?</TITLE>\n\n<BODY>### Question\n\nHi authors, \r\nThanks for your great work.\r\n\r\nWhen I'm delving into the code implementation, I'm really curious where I can find the model.generate()?\r\nFor example, in [#L76](https://github.com/haotian-liu/LLaVA/blob/8467850a63aa0d6f47aa150c53aca4751f0d3d14/llava/eval/model_vqa_science.py#L76)\r\n```python\r\n with torch.inference_mode():\r\n      output_ids = model.generate(\r\n          input_ids,\r\n          images=images,\r\n          do_sample=True if args.temperature > 0 else False,\r\n          temperature=args.temperature,\r\n          max_new_tokens=1024,\r\n          use_cache=True,\r\n          stopping_criteria=stopping_criteria,\r\n      )\r\n```\r\nand such model is an instance from class [LlavaLlamaForCausalLM](https://github.com/haotian-liu/LLaVA/blob/8467850a63aa0d6f47aa150c53aca4751f0d3d14/llava/model/language_model/llava_llama.py#L40).\r\n\r\nCouldn't find the corresponding generate() function.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-09T20:07:06Z>\nPlease check out the official documentation from transformers. It's a subclass of GenerationMixin. Thanks.\r\n\r\nhttps://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationMixin\n</Comment>\n<Comment by DnkNju at 2024-10-02T11:03:34Z>\nvery grateful :)\r\nI searched the Chinese community for this question for months with no results until I tried google...\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 781,
    "state": "open",
    "created_by": "zhang-jr",
    "created_at": "2023-11-09T10:00:24Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/781</URL>\n\n<TITLE>[Feature request] Evaluation on the RefCOCO</TITLE>\n\n<BODY>### feature\n\nHello, do you have plans to evaluate the RefCOCO dataset?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 780,
    "state": "open",
    "created_by": "RicRicci22",
    "created_at": "2023-11-09T09:20:34Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/780</URL>\n\n<TITLE>[Usage] RuntimeError: probability tensor contains either `inf`, `nan` or element < 0</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: I pulled last commits from the repo, tried to run cli inference, the network is creating NaN as probability outputs.\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.cli --model-path liuhaotian/llava-v1.5-7b --image-file \"https://llava-vl.github.io/static/images/view.jpg\"     --load-4bit\r\n```\r\n\r\nI also tried to create a fresh environment, still same bug. \r\nI explored the token in input to the network, and I noticed a strange -200 token. Not sure if this is causing the issue, maybe someone can have a look? I'm trying to debug it and come here if I have news! \r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/riccardoricci/miniconda3/envs/chat_osm/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/riccardoricci/miniconda3/envs/chat_osm/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/media/data/Riccardo/chat_with_OSM/LLaVA/llava/serve/cli.py\", line 125, in <module>\r\n    main(args)\r\n  File \"/media/data/Riccardo/chat_with_OSM/LLaVA/llava/serve/cli.py\", line 95, in main\r\n    output_ids = model.generate(\r\n  File \"/home/riccardoricci/miniconda3/envs/chat_osm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/riccardoricci/miniconda3/envs/chat_osm/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1588, in generate\r\n    return self.sample(\r\n  File \"/home/riccardoricci/miniconda3/envs/chat_osm/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2678, in sample\r\n    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n```\r\n\r\n![image](https://github.com/haotian-liu/LLaVA/assets/44427504/18b080df-a642-4782-8839-6dfc0a06b764)</BODY>\n\n<COMMENTS>\n<Comment by crazycth at 2023-11-09T09:36:31Z>\n+1 , I met the same problem here , I finetune llava with lora and want to inference it with\r\n```\r\npython -m llava.serve.cli --model-path /root/code/LLaVA/checkpoints/llava-v1.5-13b-lora --image-file /root/code/LLaVA/pic.png --model-base FlagAlpha/Llama2-Chinese-7b-Chat\r\n```\r\n\r\nAnd i also met RuntimeError: probability tensor contains either `inf`, `nan` or element < 0\n</Comment>\n<Comment by RicRicci22 at 2023-11-09T09:45:49Z>\nFollow up, still I don't know the origin of the issue.. \r\n\r\nTried to do input_ids = torch.abs(input_ids) before generate in cli.py.\r\n\r\nInteresting fact: \r\nsize of input embeddings (before line 90 in llava_llama.py) changes.\r\n\r\nwith torch.abs()\r\ninput embeds -> torch.Size([1, 45, 4096])\r\n\r\nwithout\r\ninput embeds: torch.Size([1, 620, 4096])\r\n\r\nStill don't know the issue..\n</Comment>\n<Comment by RicRicci22 at 2023-11-09T10:11:20Z>\nJust discovered that token -200 is the image token.. So, my guess is that it is a tokenization problem! \r\n\r\nJust look at the size of the input_embeds with the \"correct tokens\"\n</Comment>\n<Comment by crazycth at 2023-11-09T10:44:49Z>\n-200 is the image token and don't change that\r\n\r\nIs the problem with weights ? have you tried lora and it works well?\n</Comment>\n<Comment by RicRicci22 at 2023-11-09T10:59:27Z>\nNo, the thing that makes me wonder is that I just cloned the repo, installed the packages and tried the cli inference. \r\n\r\nIt should be straightforward, but it is not in my case.. anyone else having the same problem? \r\n\r\nMy workstation has two gpus, and I encounter different behavior when doing \r\n'''\r\nexport CUDA_VISIBLE_DEVICES=0\r\n'''\r\nthe error above\r\n\r\n'''\r\nexport CUDA_VISIBLE_DEVICES=0,1\r\n'''\r\n\r\nError:\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\nStill I didn't find the solution..\n</Comment>\n<Comment by RicRicci22 at 2023-11-09T11:14:10Z>\nJust discovered that the gradio interface works fine if I do\r\nexport CUDA_VISIBLE_DEVICES=0 when starting the model worker. Otherwise it completely freezes the workstation.\r\n\r\nInstead in cli.py, after doing export CUDA_VISIBLE_DEVICES=0, I receive the above mentioned error: \r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\n</Comment>\n<Comment by ntoxeg at 2023-11-15T13:34:07Z>\nI had to add a bunch of arguments (missing in the README) to the arguments object and it turns out I get this error when `”num_beams” > 1`. Like this:\r\n```\r\n       \"temperature\": 0.2,\r\n       \"top_p\": 0.5,\r\n       \"num_beams\": 1,\r\n       \"max_new_tokens\": 300,\r\n```\n</Comment>\n<Comment by waltonfuture at 2024-04-17T09:14:20Z>\n@haotian-liu\r\nsame question. Can you help us on it?\n</Comment>\n<Comment by user074 at 2024-04-27T02:30:07Z>\nOkay. I think I solved it through change the dtype in cli.py, basically I changed `dtype=torch.float16` to `dtype=torch.bfloat16`. I think you also can change it to float32. Also you need to change the model dtype. For me I added `model.to(dtype=torch.bfloat16)` but there are probably more elegant way to handle this.\r\n\r\nThe issue is probably related to the float16 causing overflow as the [issue here](https://github.com/haotian-liu/LLaVA/issues/463#issuecomment-1788226258)\n</Comment>\n<Comment by ghazalsaheb at 2024-07-30T05:52:38Z>\nI have been facing the same issue.\n</Comment>\n<Comment by ghazalsaheb at 2024-08-06T19:46:00Z>\nUpdate: I was able to resolve the issue by changing the base model from hugging face's \"llava-hf/llava-1.5-7b-hf\"to \"liuhaotian/llava-v1.5-7b\". It resolved the NaN issue and the training performance got much better.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 779,
    "state": "open",
    "created_by": "created-Bi",
    "created_at": "2023-11-09T08:26:45Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/779</URL>\n\n<TITLE>[Usage] when I run finetune_qlora, 'WARNING: tokenization mismatch: 1 vs. 165. (ignored)' caused the loss to be 0!</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\n\r\nCommand:\r\n```\r\nPASTE THE COMMANDS HERE.\r\n```\r\n\r\nLog: \r\n```\r\nPASTE THE LOGS HERE.\r\n```\r\nenv:\r\nflash-attn                2.3.3                    pypi_0    pypi\r\ntorch                     2.0.1                    pypi_0    pypi\r\ntransformers              4.31.0                   pypi_0    pypi\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/28759055/d4a15781-27ed-4ac5-b4ea-fed38a367599)</BODY>\n\n<COMMENTS>\n<Comment by created-Bi at 2023-11-09T08:37:31Z>\ntokenizer version mismatch?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 778,
    "state": "open",
    "created_by": "andysingal",
    "created_at": "2023-11-09T03:11:30Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/778</URL>\n\n<TITLE>ERROR: Could not consume arg: val</TITLE>\n\n<BODY>### Describe the issue\n\nWhile running: https://colab.research.google.com/drive/1z_0zpvcJ11AaClAgRUcytzFYHy237fog?usp=sharing \r\ni am getting error:\r\n```\r\n!python /content/BakLLaVA/scripts/convert_sqa_to_llava.py \\\r\n    convert_to_llava \\\r\n    --base-dir /content/ScienceQA/tools/ScienceQA/data/scienceqa \\\r\n    --prompt-format \"QCM-LEA\" \\\r\n    --split {train,val,minival,test,minitest}\r\n\r\n```\r\ngives me:\r\n```\r\n```\r\nNumber of samples: 12726\r\nERROR: Could not consume arg: val\r\nUsage: convert_sqa_to_llava.py convert_to_llava --base-dir /content/ScienceQA/tools/ScienceQA/data/scienceqa --prompt-format QCM-LEA --split train -\r\n\r\nFor detailed information on this command, run:\r\n  convert_sqa_to_llava.py convert_to_llava --base-dir /content/ScienceQA/tools/ScienceQA/data/scienceqa --prompt-format QCM-LEA --split train - --help\r\n```</BODY>\n\n<COMMENTS>\n<Comment by White1973 at 2023-12-27T09:14:05Z>\nI meet the same error, how you solve it?\n</Comment>\n<Comment by ljianinggg at 2024-05-05T14:45:19Z>\n+1\n</Comment>\n<Comment by itsqyh at 2024-06-16T16:18:48Z>\nInstead of the original one-time split all kinds of data (--split{train,val,minival,test,minitest}) command line, try split one kind of data at a time (--split train, --split val.....) And in this case, you should enter the command line 5 times.\n</Comment>\n<Comment by itsqyh at 2024-06-16T16:19:05Z>\n> +1\r\n\r\nInstead of the original one-time split all kinds of data (--split{train,val,minival,test,minitest}) command line, try split one kind of data at a time (--split train, --split val.....) And in this case, you should enter the command line 5 times.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 777,
    "state": "open",
    "created_by": "unmo",
    "created_at": "2023-11-09T02:47:52Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/777</URL>\n\n<TITLE>[Question] DAPT(Domain-Adaptation Pretraining) method</TITLE>\n\n<BODY>### Question\n\nIs it possible to do Domain Adaptation instead of Task Adaptation?\r\nSpecifically, I want to use LLaVA as a starting checkpoint to train the language and images of the new domain.\r\nthank you.</BODY>\n\n<COMMENTS>\n<Comment by unmo at 2023-11-15T06:49:25Z>\nIs it impossible or is there some way to do DAPT.\n</Comment>\n<Comment by haotian-liu at 2023-11-15T16:02:01Z>\nI guess [LLaVA-Med](https://arxiv.org/abs/2306.00890) can be considered as DAPT?\n</Comment>\n<Comment by unmo at 2023-11-16T02:38:09Z>\nStage 1 is probably what it is.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/9719347/803e30bd-a78f-409a-871e-7d6f1ede8a38)\r\n\r\nSo can we use LLaVA 1.5(not vanila Vicuna) as a starting point for pre-training?\r\nIf so, would the procedure be to generate mm_projector.bin and use that to run the [finetune.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune.sh)?\n</Comment>\n<Comment by unmo at 2023-12-12T03:20:44Z>\nThis example appears to be done from pre-training using a large amount of data for domain information.\r\n\r\nI only have a small amount of domain data and I am not sure how it should be trained in that case.\r\n\r\nCan you please tell me what to do?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 776,
    "state": "open",
    "created_by": "jdy18",
    "created_at": "2023-11-08T18:34:05Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/776</URL>\n\n<TITLE>the finetuning strategy used for llava1.5:  freezing  LLM/Lora/Full training?</TITLE>\n\n<BODY>### Question\n\nCould you please introduce the finetuning strategy used for llava1.5? Is it employing weight freezing of the LLM, full LLM training, or utilizing the LoRA approach?\"</BODY>\n\n<COMMENTS>\n<Comment by Road2Redemption at 2023-11-16T01:58:49Z>\ni have the same question.\n</Comment>\n<Comment by tosiyuki at 2023-11-17T09:06:41Z>\nI read the paper and the code, I understand that the first stage pre-train is learned only Adapter. LLM and Vit are freezing.\r\nSecond stage, LLM and Adapter trained, Vit remains frozen.\r\nFor more information, please read the below and training script.\r\nhttps://github.com/haotian-liu/LLaVA/blob/80540fb4bf4dad118d87d42bd2fb55e6f3b96f16/llava/train/train.py#L890-L899\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 775,
    "state": "closed",
    "created_by": "mengjiexu",
    "created_at": "2023-11-08T08:17:16Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/775</URL>\n\n<TITLE>[Question] size mismatch for mm_projector</TITLE>\n\n<BODY>### Question\n\nthe error:\r\n'''\r\n```\r\nimage features: tensor([[[ 0.4863, -0.5620,  0.2654,  ...,  0.3267,  0.1172,  0.2654],\r\n         [ 0.2347, -0.2360,  0.3865,  ...,  0.6465,  0.9463,  0.1743],\r\n         [ 1.7607,  1.1641, -0.4541,  ...,  2.9082, -0.4727,  1.9062],\r\n         ...,\r\n         [ 1.6406,  1.4521,  0.0803,  ...,  2.3066, -0.6611,  1.5059],\r\n         [ 1.6182,  1.1221, -0.3940,  ...,  2.7715, -0.3125,  1.8516],\r\n         [ 0.2793,  0.3804,  0.5850,  ...,  0.5508,  0.4658,  0.1287]]],\r\n       device='cuda:0', dtype=torch.float16) torch.Size([1, 576, 1024])\r\nmm_projector: Sequential(\r\n  (0): Linear4bit(in_features=1024, out_features=5120, bias=True)\r\n  (1): GELU(approximate='none')\r\n  (2): Linear4bit(in_features=5120, out_features=5120, bias=True)\r\n)\r\nTraceback (most recent call last):\r\n  File \"/home/xmj/projects/LLaVA/llava/eval/test.py\", line 111, in <module>\r\n    output_ids = model.generate(\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1538, in generate\r\n    return self.greedy_search(\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2362, in greedy_search\r\n    outputs = self(\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/xmj/projects/LLaVA/llava/model/language_model/llava_llama.py\", line 79, in forward\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/home/xmj/projects/LLaVA/llava/model/llava_arch.py\", line 124, in prepare_inputs_labels_for_multimodal\r\n    image_features = self.encode_images(images)\r\n  File \"/home/xmj/projects/LLaVA/llava/model/llava_arch.py\", line 98, in encode_images\r\n    image_features = self.get_model().mm_projector(image_features)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 217, in forward\r\n    input = module(input)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/bitsandbytes/nn/modules.py\", line 248, in forward\r\n    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 579, in matmul_4bit\r\n    return MatMul4Bit.apply(A, B, out, bias, quant_state)\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\r\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\r\n  File \"/home/xmj/anaconda3/envs/llava/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 516, in forward\r\n    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, state).to(A.dtype).t(), bias)\r\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (576x1024 and 1x2621440)\r\n```\r\nthe origin code is:\r\n`\r\n\r\nclass LlavaMetaForCausalLM(ABC):\r\n\r\n    def encode_images(self, images):\r\n        image_features = self.get_model().get_vision_tower()(images)\r\n        print('image features:', image_features, image_features.shape)\r\n        print('mm_projector:', self.get_model().mm_projector)\r\n        image_features = self.get_model().mm_projector(image_features)\r\n        print('image features2:', image_features, image_features.shape)\r\n        return image_features\r\n\r\n`\r\nwhy the image features can't multiply with the mm_projector weights?</BODY>\n\n<COMMENTS>\n<Comment by devaansh100 at 2023-11-08T08:20:58Z>\nHello! I faced this error too. [This thread](https://github.com/haotian-liu/LLaVA/issues/744) helped me out\n</Comment>\n<Comment by mengjiexu at 2023-11-10T03:33:28Z>\nthis is because code wrong.\n</Comment>\n<Comment by haotian-liu at 2023-11-10T03:35:23Z>\n@mengjiexu \r\n\r\nCan you share how you fixed it? It would be beneficial to the community if we can incorporate the fix. Thanks.\n</Comment>\n<Comment by mengjiexu at 2023-11-15T13:09:20Z>\nI used myself code to load the mm_projector.bin again after the load_model func, so it is wrong, I remove the code then the model is normal.\n</Comment>\n<Comment by mikeogezi at 2023-11-22T19:30:03Z>\n@mengjiexu Could you share the code you used to reload mm_projector?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 774,
    "state": "closed",
    "created_by": "Yangr116",
    "created_at": "2023-11-08T08:09:46Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/774</URL>\n\n<TITLE>[Question]  RuntimeError: expected scalar type Float but found BFloat16</TITLE>\n\n<BODY>### Question\r\n\r\nI wanted to debug, and ran below code:\r\n```\r\npython3 -m pdb llava/train/train_mem.py \\\r\n    --lora_enable True \\\r\n    --model_name_or_path ./checkpoints/$MODEL_VERSION \\\r\n    --version $PROMPT_VERSION \\\r\n    --data_path ./playground/data/llava_instruct_80k.json \\\r\n    --image_folder /path/to/coco/train2017 \\\r\n    --vision_tower openai/clip-vit-large-patch14 \\\r\n    --pretrain_mm_mlp_adapter ./checkpoints/llava-$MODEL_VERSION-pretrain/mm_projector.bin \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-$MODEL_VERSION-finetune_lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --lazy_preprocess True \\\r\n    --dataloader_num_workers 4 \\\r\n    --report_to wandb\r\n```\r\nI got below error:\r\n```\r\nresult = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\r\nRuntimeError: expected scalar type Float but found BFloat16\r\n```\r\nIn this [line](https://github.com/huggingface/transformers/blob/be74b2ead69df1849ec62ac5c86c7d5dee663448/src/transformers/trainer.py#L1860), I checked the type of model and input. They had different types.\r\n![image](https://github.com/haotian-liu/LLaVA/assets/73805072/c38f1902-6859-49ce-bf68-7a6c1d281fba)\r\n\r\n\r\nHowever, when using deepspeed, it's OK.\r\nInstead of changing the original code of transformers.trainer, is there any great approach to solving this issue？</BODY>\n\n<COMMENTS>\n<Comment by Yangr116 at 2023-11-09T09:08:55Z>\nSolved. This may be caused by lora. After removing lora and turning off flash attention, it works.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 773,
    "state": "closed",
    "created_by": "csvlrs-lak",
    "created_at": "2023-11-08T07:28:50Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/773</URL>\n\n<TITLE>Clarification on pretrain_mm_mlp_adapter</TITLE>\n\n<BODY>### Question\n\nI've come across this argument called pretrain_mm_mlp_adapter in finetuning script,  I'd like to better understand its purpose and why it's being used.</BODY>\n\n<COMMENTS>\n<Comment by devaansh100 at 2023-11-08T08:18:55Z>\nThis flag is used to give the path to the .bin file for the weights mm_mlp_adapter trained on image-text pairs in the first stage. This is loaded and fine-tuned in the second stage\n</Comment>\n<Comment by csvlrs-lak at 2023-11-08T09:02:44Z>\nThanks @devaansh100 for you response, I have a question regarding the fine-tuning of the LLaVA-13B model with custom data. I have made the following changes to the provided script [finetune_lora](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_lora.sh) \r\n\r\n1. Changed the **model_name_or_path** from lmsys/vicuna-13b-v1.5 to liuhaotian/llava-v1.5-13b.\r\n2. Adjusted the values of **data_path** and **image_folder** to point to my custom data.\r\n\r\nHowever, I'm uncertain about which mm_projector.bin(**pretrain_mm_mlp_adapter** argument) file should be used in this fine-tuning process. Do I need to download it separately, and if so, from where?\"\n</Comment>\n<Comment by devaansh100 at 2023-11-09T03:58:56Z>\nMaybe [this thread](https://github.com/haotian-liu/LLaVA/issues/767) might help you\r\n\r\n> download it separately\r\n\r\nI think this model has not used lora(please check!), in which case the `mm_projector.bin` can be downloaded separately from [here](https://huggingface.co/liuhaotian/llava-v1.5-13b/tree/main).\n</Comment>\n<Comment by csvlrs-lak at 2023-11-09T04:01:57Z>\nThank you @devaansh100\n</Comment>\n<Comment by aneet-javis at 2023-11-24T07:01:17Z>\n@csvlrs-lak So for finetuning on our own custom data, should the model_path be set to lmsys/vicuna-13b-v1.5 or liuhaotian/llava-v1.5-13b? And what is the difference in terms of training/memory between the two? Thanks\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 772,
    "state": "open",
    "created_by": "jiaxiangc",
    "created_at": "2023-11-08T03:15:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/772</URL>\n\n<TITLE>[Question] Why VQAv2 always \"Execution time (sec.): None\"</TITLE>\n\n<BODY>### Question\n\nI follow the command to upload json file to the evaluation server, but the results are none. What is the reason?\r\nThanks.</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 771,
    "state": "open",
    "created_by": "jiaxiangc",
    "created_at": "2023-11-07T14:59:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/771</URL>\n\n<TITLE>[Question] \"Exception: Can't find testdev_balanced_questions.json\" when evaluate gqa.</TITLE>\n\n<BODY>### Question\n\nI modify the eval.py from https://gist.github.com/haotian-liu/db6eddc2a984b4cbcc8a7f26fd523187.\r\nBut I get this error.\r\nHow to solve it, thank you.</BODY>\n\n<COMMENTS>\n<Comment by wyxscir at 2024-03-26T06:35:41Z>\nhave you solve it? i get this error too\n</Comment>\n<Comment by Minato-Zackie at 2024-04-23T11:18:12Z>\nHello！You can choose \"Download Questions\" from https://cs.stanford.edu/people/dorarad/gqa/download.html.\r\nThe downloaded zip contains ’testdev_balanced_questions.json‘, and you can put it into './playground/data/eval/gqa/data'\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 770,
    "state": "open",
    "created_by": "unmo",
    "created_at": "2023-11-07T06:45:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/770</URL>\n\n<TITLE>[Question] What is the principle training scripts .</TITLE>\n\n<BODY>### Question\n\nWhat is the principle difference between [finetune_task.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task.sh), [finetune_lora.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_lora.sh) and [finetune_task_lora.sh](https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune_task_lora.sh)?</BODY>\n\n<COMMENTS>\n<Comment by tingxueronghua at 2023-11-07T07:06:04Z>\nIn my understanding, the script with a \"_task\" postfix means it uses llava-1.5 as initialization and is designed to further finetune it on other tasks. It is different from \"finetune.sh\", which is initialized using the model from the pretrained stage, and is designed to finetune on the 665k dataset.\r\n\r\n\"_lora\" means it is finetuned with lora.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 769,
    "state": "closed",
    "created_by": "unmo",
    "created_at": "2023-11-07T04:53:00Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/769</URL>\n\n<TITLE>[Usage] RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1!</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI execute cli command with my custom model. But I get a cuda error when I run it.\r\n\r\nCommand:\r\n```\r\npython -m llava.serve.cli \\\r\n    --model-base \"liuhaotian/llava-v1.5-13b\"\\\r\n    --model-path \"./checkpoints/llava1.5_13b_task_lora_epoch1\"\\\r\n    --image-file ./playground/data/LLaVA-Pretrain/images/test2/4.png\r\n```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/app/LLaVA/llava/serve/cli.py\", line 124, in <module>\r\n    main(args)\r\n  File \"/app/LLaVA/llava/serve/cli.py\", line 94, in main\r\n    output_ids = model.generate(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 1588, in generate\r\n    return self.sample(\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 2642, in sample\r\n    outputs = self(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/app/LLaVA/llava/model/language_model/llava_llama.py\", line 79, in forward\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/app/LLaVA/llava/model/llava_arch.py\", line 183, in prepare_inputs_labels_for_multimodal\r\n    cur_new_input_embeds = torch.cat(cur_new_input_embeds)\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument tensors in method wrapper_CUDA_cat)\r\n```</BODY>\n\n<COMMENTS>\n<Comment by DavidK0 at 2023-11-07T18:32:50Z>\nI had the same issue, adding the --device argument fixed it.\r\n\r\nEg.\r\npython -m llava.serve.cli \\\r\n    --model-base \"liuhaotian/llava-v1.5-13b\"\\\r\n    --model-path \"./checkpoints/llava1.5_13b_task_lora_epoch1\"\\\r\n    --image-file ./playground/data/LLaVA-Pretrain/images/test2/4.png\\\r\n    --device cuda:0\n</Comment>\n<Comment by rosiaaa at 2023-11-07T19:29:31Z>\n@DavidK0 Is this solution compatible with using multiple GPUs? It seems like that's just selecting one GPU. I think that the original poster might have been encountering this problem while intentionally trying to use both of their GPU's (cuda:0 and cuda:1) I encountered this error while trying to use multiple cuda devices because a single card did not have enough video ram.\r\n\r\n@unmo  for what it's worth I got around this by using the --load-4bit option to reduce memory footprint and then specify to use only one of my GPU's by prepending CUDA_VISIBLE_DEVICES=0. I'm using the demo interface not cli, but my command is this:\r\n\r\nCUDA_VISIBLE_DEVICES=0 python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./llava-v1.5-7b --load-4bit\n</Comment>\n<Comment by uyo9ko at 2023-11-08T01:40:01Z>\nWhy can't the new version '1.1.3' infer cli.py with multiple GPUs? I switched back to version '1.1.1', and it worked perfectly with no errors like 'Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1!'. However, when I use version '1.1.3', it raises this error.\n</Comment>\n<Comment by unmo at 2023-11-08T01:49:30Z>\nI found that new version is not work. So I used new main branch code, but don't know tag version.\r\nPrevious version code is good worked.\n</Comment>\n<Comment by uyo9ko at 2023-11-08T01:50:53Z>\n> I found that new version is not work. So I used new main branch code, but don't know tag version. Previous version code is good worked.\r\n\r\nYes. Can you help with this? @haotian-liu\n</Comment>\n<Comment by haotian-liu at 2023-11-08T01:51:08Z>\nSorry for the inconvenience. It should now be fixed in https://github.com/haotian-liu/LLaVA/commit/4a77fb46181b9354261648613b0b8b7eb35b815c\n</Comment>\n<Comment by uyo9ko at 2023-11-08T01:56:48Z>\n> Sorry for the inconvenience. It should now be fixed in [4a77fb4](https://github.com/haotian-liu/LLaVA/commit/4a77fb46181b9354261648613b0b8b7eb35b815c)\r\n\r\nIt's solved! Thank you!\n</Comment>\n<Comment by unmo at 2023-11-08T04:07:42Z>\nI resolved it. Thank you.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 768,
    "state": "open",
    "created_by": "devaansh100",
    "created_at": "2023-11-07T04:04:45Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/768</URL>\n\n<TITLE>[Question] Regarding Captioning Evaluation on Flickr30k</TITLE>\n\n<BODY>### Question\n\nHi, thanks for the great work! I have been trying to evaluate llava image captioning on Flickr30k, but I am not able to reproduce the results. While the original llava paper does not have these scores, some other works like [this](https://arxiv.org/abs/2306.09265) and [this](https://arxiv.org/abs/2309.03905) do. Both of them report the CIDEr score as 27.65.\r\n\r\nDue to the lack of eval scripts for captioning(unless I missed it!), I have used `llava/eval/model_vqa.py` keeping the same settings, however, I am getting near zero scores. This is due to the detailed caption produced by the model. Reducing the `max_new_tokens ` helps bring it closer to the expected score.\r\n\r\nI understand these are separate works, but by any chance would you be aware of the script and parameters used to get these scores? More specifically:\r\n1. I am interested in the checkpoint used. MPT seems to be giving better performance on this task than Vicuna\r\n2. The generations settings used\r\n\r\nThanks in advance!</BODY>\n\n<COMMENTS>\n<Comment by HenryHZY at 2023-11-08T20:59:11Z>\nHi @devaansh100 \r\nCan you share more Information about why \"MPT seems to be giving better performance on this task than Vicuna\"?\r\nFor CIDEr score 27.65, I think you can modify their evaluation scripts [link](https://github.com/OpenGVLab/Multi-Modality-Arena/tree/main/LVLM_evaluation), though I haven't tried it.\n</Comment>\n<Comment by devaansh100 at 2023-11-09T04:35:54Z>\n> MPT seems to be giving better performance on this task than Vicuna\r\n\r\nLet me get back to you on this! I am rechecking my evaluation and it actually seems to be the other way round, I might have made a typo. \r\n\r\n>  modify their evaluation scripts\r\n\r\nI did do that, however, setting `max_new_tokens` to 256 seems to be a bit too high. Lowering it to around 20, actually gives a CIDEr score of 36.14. Using 50 drops it down to 0.18. I am trying some intermediate hyperparameters too(all experiments using 1 beam, a temperature = 0.2 with a top_k = 1). This variation is where the confusion stems from.\r\n\r\nOn a side note, the current LLaVA model on hf uses 576 image tokens for the LLM(resizing image to 336x336 and using a patch size of 14) but [this paper](https://arxiv.org/abs/2309.03905) gets 27.65 by using 257 tokens. Is there a previous version of llava with such a setting?\n</Comment>\n<Comment by HenryHZY at 2023-11-09T08:04:47Z>\nThanks for your reply. I am not the maintainer of LLaVA. But I can provide some of my opinions.\r\n\r\nAs mentioned above:\r\n> 'max_new_tokens=20' with 'CIDEr=36.14' refers to LLaVA1.5. \r\n> 'max_new_tokens=16' (default setting of LVLM-eHub) with 'CIDEr= 27.65' refers to the previous LLaVA version, whcih uses 224x224 image resolution. \r\n\r\nI think it makes sense that LLaVA1.5 performs poorly in the image captioning task compared to other models. After all, LLaVA1.5 is mainly focused on the vqa task.\r\n\r\nBy the way, I haven't used the LVLM-eHub before. Is it available for you to release your LLaVA 1.5 evaluation script on Flickr30k? I am also interested in reproducing your Flickr30k result:)\n</Comment>\n<Comment by ursulalujun at 2023-11-30T07:02:19Z>\nHello~ @devaansh100 \r\nI noticed that the output of LLaVA1.5  can sometimes be much longer than GT when evaluating on Flickr30k. Although the content of outputs is correct, different lengths will lead to a decrease in similarity and CIDEr. I checked the paper and found this passage:\r\n> To address this, we propose using a single response formatting prompt that clearly indicates the output format, to be appended at the end of VQA questions when promoting short answers: Answer the question using a single word or phrase. We empirically show that when LLM is finetuned with such prompts, LLaVA is able to properly adjust the output format according to the user’s instructions, and does not require additional processing of the VQA data.\r\n\r\nThen I write a prompt similar to this example, \"Describe this image using one or more simple sentences\". The output length of LLaVA1.5 is indeed much shorter and CIDEr achieves 66.71.\n</Comment>\n<Comment by devaansh100 at 2023-11-30T07:27:34Z>\n@ursulalujun I see, thank you! However, while these prompts/hyperparameters work, there is naturally some test leakage that is happening. Moreover, it doesn't replicate the 27.65 in the previous works. But I guess this lack of standardization is an inherent problem with LLM evaluation🤷🏻‍♂️.\r\n\r\n@HenryHZY Sorry for the late response, I had missed this message! I did not write a special script. Rather, I converted the dataset into the format expected by `model_vqa.py`. The entries looked like this: \r\n\r\n```\r\n{\"image\": \"1007129816.jpg\", \"text\": \"Caption this image.\", \"question_id\": 0}\r\n{\"image\": \"1009434119.jpg\", \"text\": \"Caption this image.\", \"question_id\": 1}\r\n{\"image\": \"101362133.jpg\", \"text\": \"Caption this image.\", \"question_id\": 2}\r\n```\r\n\r\nFor CIDEr:\r\n```\r\nfrom pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\r\nfrom pycocoevalcap.bleu.bleu import Bleu\r\nfrom pycocoevalcap.meteor.meteor import Meteor\r\nfrom pycocoevalcap.rouge.rouge import Rouge\r\nfrom pycocoevalcap.cider.cider import Cider\r\nfrom pycocoevalcap.spice.spice import Spice\r\nimport pandas as pd\r\nimport json\r\nimport sys\r\nclass Evaluator:\r\n    def __init__(self) -> None:\r\n        self.tokenizer = PTBTokenizer()\r\n        self.scorer_list = [\r\n            (Cider(), \"CIDEr\"),\r\n            # (Meteor(), \"METEOR\")\r\n        ]\r\n        self.evaluation_report = {}\r\n\r\n    def do_the_thing(self, golden_reference, candidate_reference):\r\n        golden_reference = self.tokenizer.tokenize(golden_reference)\r\n        candidate_reference = self.tokenizer.tokenize(candidate_reference)\r\n        \r\n        # From this point, some variables are named as in the original code\r\n        # I have no idea why they name like these\r\n        # The original code: https://github.com/salaniz/pycocoevalcap/blob/a24f74c408c918f1f4ec34e9514bc8a76ce41ffd/eval.py#L51-L63\r\n        for scorer, method in self.scorer_list:\r\n            score, scores = scorer.compute_score(golden_reference, candidate_reference)\r\n            if isinstance(method, list):\r\n                for sc, scs, m in zip(score, scores, method):\r\n                    self.evaluation_report[m] = sc\r\n            else:\r\n                self.evaluation_report[method] = score\r\n\r\ndf = pd.read_csv('../Datasets/Flickr30k/flickr_annotations_30k.csv')\r\ndf = df[df['split'] == 'test']\r\ngolden_reference = []\r\nf = open(f'Flickr30k/{sys.argv[1]}.jsonl')\r\noutputs = [json.loads(x)['text'] for x in f.readlines()]\r\ncandidate_reference = []\r\nfor i, x in enumerate(df.iloc):\r\n    s = x['raw'][2:-2].replace('\"','').split(',')\r\n    golden_reference.append(s)\r\n    # print(outputs[i])\r\n    candidate_reference.append(outputs[i])\r\n\r\ngolden_reference = {k: [{'caption': x} for x in v] for k, v in enumerate(golden_reference)}\r\n\r\ncandidate_reference = {k: [{'caption': v}] for k, v in enumerate(candidate_reference)}\r\n# breakpoint()\r\nevaluator = Evaluator()\r\nevaluator.do_the_thing(golden_reference, candidate_reference)\r\n\r\nprint(evaluator.evaluation_report)\r\n```\r\n\r\nHope this helps!\n</Comment>\n<Comment by ursulalujun at 2023-11-30T09:30:22Z>\n@devaansh100 Yes, I agree with you! LLM is sensitive to prompt, and they can memory the prompt seen in the training stage. I have tried to use the prompt \"give me a short description of this image\", but it can not control the length of the output very well. By the way, I evaluated llava1.5 in ChEF, a newly published benchmark framework. https://github.com/OpenGVLab/LAMM/tree/ChEF\n</Comment>\n<Comment by duguodong7 at 2024-06-13T17:39:44Z>\n@ursulalujun \r\nwhen I use the prompt- \"Describe this image using one or more simple sentences\", and max_new_tokens=128, CIDEr achieves 76.5\n</Comment>\n<Comment by koookieee at 2024-10-22T18:01:14Z>\n> @ursulalujun when I use the prompt- \"Describe this image using one or more simple sentences\", and max_new_tokens=128, CIDEr achieves 76.5\r\n\r\nHi @duguodong7  I am actually trying to figure out the same but somehow i am always getting cider score of ~0 . Can you help me out\r\nThanks\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 767,
    "state": "open",
    "created_by": "TesiLin",
    "created_at": "2023-11-07T04:04:14Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/767</URL>\n\n<TITLE>[Question] Script question when pretrain with a model_base of LLaVA-1.5</TITLE>\n\n<BODY>### Question\n\nHi, thank you for your excellent work.\r\n\r\nI've been trying to **do the pre-training process based on LLaVA-1.5** to utilize the good performance of this new version. Could you please help to check if my understanding of the settings is correct?\r\n\r\nFrom my understanding, during pre-training I should:\r\n1) Set the \"**model_name_or_path**\" parameter to \"liuhaotian/llava-v1.5-13b\" to initialize the model weights of LLaVA-1.5.\r\n2) Set the \"**version**\" parameter to \"v1\".\r\n\r\nAbove will make it read the mm_mlp_adapter and LLM weights from liuhaotian/llava-v1.5-13b, freeze LLM part, start to train the adapter. Or should I add \"**pretrain_mm_mlp_adapter**\" parameter and download the checkpoints from somewhere? I did not find it alone in model zoo.\r\n\r\nDuring finetuning period, I should:\r\n1) Set the \"**model_name_or_path**\" parameter to pretrain stage checkpoints.\r\n2) Keep the \"**version**\" parameter to \"v1\".\r\n\r\nBtw, **how should I decide the value of parameter \"version\"**? Is there typically a relationship between it and the actual model version number?\r\n\r\nThanks for your help!</BODY>\n\n<COMMENTS>\n<Comment by devaansh100 at 2023-11-07T04:16:10Z>\nHi! I'm not the author but I may be able to help with some of the questions here.\r\n\r\n> Set the \"model_name_or_path\" parameter to \"liuhaotian/llava-v1.5-13b\" to initialize the model weights of LLaVA-1.5.\r\n\r\nThis will load a pre-trained and instruction tuned llava model. If you are pre-training from scratch, you do not need to load this. Instead, set `model_name_or_path` to some base LLM(like `lmsys/vicuna-7b-v1.5` or the MPT base model). Set the vision tower to a CLIP model(this repo suggests: `openai/clip-vit-large-patch14-336`) - doing so automatically initializes the mm_adapter. To train it, you can set the relevant flag to True.\r\n\r\n> Set the \"version\" parameter to \"v1\".\r\n\r\nTo the best of my knowledge, the version affects the conversation format that would be used during inference. This should be inline with your base LLM. Checkout `llava/conversations.py` to understand this better. \r\n\r\n> Set the \"model_name_or_path\" parameter to pretrain stage checkpoints\r\n\r\nNo, use the same `model_name_or_path`, since pertaining does not affect the LLM parameters. Also add the flags to tune the adapter, and enable lora. To load the pertained mm_adapter, use the `pretrain_mm_mlp_adapter` flag.\r\n\r\n> Keep the \"version\" parameter to \"v1\".\r\n> Is there typically a relationship between it and the actual model version number?\r\n\r\nNo relation that I am aware of. But the version should be the same during instruction tuning.\r\n\r\nPlease feel free to correct me if I anything I mentioned seems wrong!\n</Comment>\n<Comment by TesiLin at 2023-11-07T05:06:17Z>\nThanks @devaansh100 , it helps a lot. And I have checked the conversation templates, which make it more clear, thank you for your suggestion.\r\n\r\nI have two more things I'd like to confirm.\r\n\r\nI want to perform two-stage training on sub-domain tasks, based on the fine-tuned weights of LLaVA-1.5, just like what LLaVA-Med did. Should I directly load 'liuhaotian/llava-v1.5-13b' for this?\r\n\r\nThen, during fine tuning, by setting the `pretrain_mm_mlp_adapter,` new projector will override those of LLaVA-v1.5?\r\nThanks again.\n</Comment>\n<Comment by devaansh100 at 2023-11-07T05:29:47Z>\nI'm not completely sure how that would work. Ideally, to load the entire HuggingFace model, you would use `load_pretrained_model` from `llava/model/builder.py`. However, that is not used during training. \r\n\r\nOne hack that I can think of is to use whatever commands I mentioned in the previous message. Then you have a model in the original format(let's call this model A).\r\n\r\nPost that, add a function to reinitialize model A with the weights of `liuhaotian/llava-v1.5-13b`(loaded with the aforementioned function), then remove all unnecessary/redundant weights from memory. Expect this to be a bit slow though.\r\n\r\nIt's also worth trying to do this without creating model A(you will need to check if this affects other settings in `train.py`).\r\n\r\nFor fine-tuning, yes, just set that flag and load the relevant files from `output_dir`.\n</Comment>\n<Comment by TesiLin at 2023-11-07T05:55:25Z>\nI see. I will have a try.\r\nThanks again! Your reply truly helped me.\n</Comment>\n<Comment by unmo at 2023-11-21T01:29:23Z>\nHi \r\n> I want to perform two-stage training on sub-domain tasks, based on the fine-tuned weights of LLaVA-1.5, just like what LLaVA-Med did. Should I directly load 'liuhaotian/llava-v1.5-13b' for this?\r\n\r\nI am thinking the same thing. If you don't mind, could you share the script?\r\nThank you.\n</Comment>\n<Comment by 459737087 at 2024-01-11T11:26:56Z>\n@unmo Hi! did you solve it\n</Comment>\n<Comment by unmo at 2024-01-12T00:56:57Z>\nNo, I haven't solved it yet.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 766,
    "state": "closed",
    "created_by": "alfredplpl",
    "created_at": "2023-11-07T03:30:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/766</URL>\n\n<TITLE>[Question] Enhance LLaVA by new GPT-4V API</TITLE>\n\n<BODY>### Question\n\nHello,\r\n\r\nCan we enhance this model by GPT-4V API?\r\n\r\nFor example, I got the result of the image captioning:\r\n```txt\r\nDescrive this image in very highly detail.\r\n```\r\n![14CF3AC8-FDB6-479D-9A3B-C6DEA84AA7DC_4_5005_c](https://github.com/haotian-liu/LLaVA/assets/3625196/aaccb97d-39b0-4970-9e0f-7bc0514ea968)\r\n```txt\r\nThis image depicts a cartoon-style female character with distinct anthropomorphic features, primarily reminiscent of a rabbit. She has a vibrant design with a color scheme dominated by shades of green. Here are the detailed features:\\n\\n- **Head**: The character has large, expressive eyes with orange and yellow irises, and her face has a happy and friendly expression. There are two large green rabbit ears with a lighter green interior sticking up from her head. She has short green hair with lighter green highlights, suggesting a stylized bob haircut. The character sports a small green bow on her collar.\\n\\n- **Outfit**: She is wearing a short-sleeved white blouse with puffed sleeves accented with frills at the edges. Over the blouse, she wears green overalls with short pants. The overalls have pink buttons at the front and a purple hem around the leg openings. Underneath the collar of the blouse, a green ribbon with a bow is visible, providing a complementary accent to the overalls.\\n\\n- **Tail**: Protruding from her lower back is a long, bushy tail, colored in the same green shades as her hair and ears, suggesting a rabbit's tail that matches her character design.\\n\\n- **Legs and Footwear**: The character has bare legs, with simple skin shading that includes a barely noticeable blush of the knees and ankles, suggesting a softness. On her feet, she wears a pair of boots that match her overall color scheme — predominantly green, with pink-purple accents around the soles and a pink button on the side of each boot where a spiky leaf-like design protrudes, echoing a plant or rabbit-ear motif.\\n\\nThe character is standing in a frontal position with her arms relaxed at her sides, giving off a cheerful vibe. The background of the image is blank and white, drawing focus entirely to the character. The art style appears to be inspired by Japanese animation and could likely be associated with a character from manga, anime, or a video game.\r\n```\r\n\r\nThanks in advance.</BODY>\n\n<COMMENTS>\n<Comment by tingxueronghua at 2023-11-07T03:50:56Z>\nDo you mean distilling GPT-4V? I guess it is definitely beneficial to LLaVA. \r\n\r\nLLaVA's OCR ability is still weak. But I am not sure whether this is due to the vision tower or the training data.\n</Comment>\n<Comment by alfredplpl at 2023-11-07T03:57:06Z>\nYes, I am considering doing something similar to distillation. It might be good to do fine-tuning on my own, but I thought it might be better to have it done officially, given the opportunity.\n</Comment>\n<Comment by alfredplpl at 2023-11-07T05:14:58Z>\nFor example, the dataset is here:\r\nhttps://huggingface.co/datasets/alfredplpl/anime-with-gpt4v-caption-for-lora\n</Comment>\n<Comment by tingxueronghua at 2023-11-07T08:52:36Z>\noh, that is an interesting dataset. I think you can just design a series of prompts to instruct the GPT-4V to generate captions, and train the llava on such visual instruction following dataset.\n</Comment>\n<Comment by alfredplpl at 2023-11-07T15:55:47Z>\nI could create a lora adaptor with A6000x2.\r\nThen, I inference this image with the lora.\r\n\r\n![cute195](https://github.com/haotian-liu/LLaVA/assets/3625196/48153a11-1c04-4ed0-8a24-4f844900a61c)\r\n\r\n\"The image features a young girl with a cute and playful appearance. She is wearing a white and pink outfit, which includes a pink jacket and a pink bow in her hair. The girl is also wearing a backpack, which is visible on her back.  Additionally, there is a stuffed bunny, possibly a stuffed animal, sitting on her shoulder. The scene has a whimsical and charming vibe, with the girl's attire and the presence of the stuffed bunny creating a sense of innocence and youthfulness.\"\r\n\r\nI wanna use GPT-4V more.\n</Comment>\n<Comment by haotian-liu at 2023-11-07T21:41:10Z>\nThank you for the suggestions. We are working on planning such a dataset / model release. Unfortunately the GPT-4V API access is still capped at 100 requests, so we are preparing for a release after we are able to create a properly designed dataset with a broad knowledge coverage.\n</Comment>\n<Comment by haotian-liu at 2023-11-07T21:41:38Z>\n@alfredplpl \r\n\r\nHow would you like this one?\r\n\r\n> ![image](https://github.com/haotian-liu/LLaVA/assets/6631389/dd4f8778-6e1e-4688-89e2-b78719ec82e7)\r\n> Describe this image in very highly detail.\r\n> \r\n> This is a beautifully illustrated anime-style image featuring a young girl with a cute and whimsical aesthetic. The girl has pale skin and short, wavy blonde hair that falls just past her shoulders. Her hair is adorned with two large, circular pink accessories that resemble bows, one on the left side and the other on the right. She has large, expressive purple eyes with multiple shades, giving her a wide-eyed and innocent appearance. Her cheeks are adorned with two small, pink blush marks, and her small, straight nose is centered between her two eyes. The girl is wearing a loose-fitting, cream-colored jacket with a floral pattern that includes pink, orange, and green hues, along with small, white hearts scattered throughout. The jacket has a slightly ruffled hem and appears soft and comfortable. On her head, the girl is wearing a small, white animal-like hat that resembles a bunny or a small bear, with a pink inner lining and a black pompom at the top. In her arms, the girl is holding a small, white creature that resembles a chubby, pink bunny or a white, animal-like plushie with pink ears and a small, black nose. The creature has a content expression and is gazing up at the girl with a small, black dot representing an eye. The background is minimal, with a plain white rectangle that has a few simple, cute illustrations scattered around, including small, white flowers and a small, yellow star. The overall color palette is soft and warm, with a dominant use of pastel colors that gives the image a gentle and soothing feel.\n</Comment>\n<Comment by alfredplpl at 2023-11-08T02:33:09Z>\n@haotian-liu It seems nice. It can annotate more accurately than before fine-tuning. For example, it used to make more mistakes with the colors of clothes and eyes, but now it does so less frequently.\n</Comment>\n<Comment by gyupro at 2023-11-08T04:03:30Z>\n@alfredplpl  I was just wondering, I am doing similar kind of work to this, don't you think training vision tower would help to enhance these kind of tasks? Vision tower has been trained on real world images, feeding these images to LLAVA would be more improved by doing so\n</Comment>\n<Comment by alfredplpl at 2023-11-08T04:09:16Z>\n@gyupro I agree. It seems that photographs are predominantly used for learning, with fewer paintings being utilized. This pink image is a generated one and does not have copyright issues. It appears to be suitable for learning purposes.\r\nHowever, the Vision Tower you're talking about, as I envision modifying the ViT, seems like it would be quite labor-intensive because it wouldn't just end with instruction tuning.\n</Comment>\n<Comment by changtimwu at 2023-11-08T08:28:18Z>\nHey everyone! While this is a great idea, I recommend verifying the quality of the API output first. ChatGPT has had some issues in the past with inconsistent model performance.\r\n\r\nDuring the developer day, OpenAI introduced several variations of GPT-4.  I guess you're interested in trying out your idea with the `gpt-4-vision-preview` model.  Since OpenAI still refers to it as \"GPT-4 Turbo with vision\", the word `turbo` makes me wonder if it's a compressed or refined version of GPT-4.  As a result, its responses may not be as good as what we experienced with the `GPT-4` model on the official site.\n</Comment>\n<Comment by HenryHZY at 2023-11-08T20:40:08Z>\nGreat idea. I think there is a lot of researchers working on it. Just wait for something like ShareGPT-Vision.\n</Comment>\n<Comment by gyupro at 2023-11-09T01:11:15Z>\n@alfredplpl , I've been pondering over this labor-intensive issue and recently discovered the image quality from DALLE-3 provided by [GPT4](https://huggingface.co/datasets/laion/dalle-3-dataset). If we gather these images and prompts related to manga or anime, we could save considerable time and cut costs effectively. Does anyone have an estimate on how many images might influence the weight of the VIT encoder?\n</Comment>\n<Comment by alfredplpl at 2023-11-27T06:05:56Z>\nShareGPT4V is released. So, The goal is achieved. I will close this issue.\r\nhttps://sharegpt4v.github.io/\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 765,
    "state": "open",
    "created_by": "Anthony6197",
    "created_at": "2023-11-06T21:35:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/765</URL>\n\n<TITLE>[Question] Benchmarking on Video-QA</TITLE>\n\n<BODY>### Question\n\nDear LLaVA Developer Team,\r\n\r\nI must say the LMM is truly brilliant! 😊 I have a question: is LLaVA capable of performing video-QA? In other words, can the model accept a video or a set of sampled frames as input? We are currently working on creating a video-QA benchmark and are exploring the possibility of using LLaVA as one of our baseline models.\r\n\r\nThank you for your assistance in clarifying this.</BODY>\n\n<COMMENTS>\n<Comment by xmy0916 at 2023-11-07T02:58:48Z>\n@Anthony6197 I think the llava code support input format in image list according to this code. \r\n[https://github.com/haotian-liu/LLaVA/blob/main/llava/model/llava_arch.py#L114](https://github.com/haotian-liu/LLaVA/blob/main/llava/model/llava_arch.py#L114)\r\nwhat you should do is just modify the data process code.\n</Comment>\n<Comment by tingxueronghua at 2023-11-07T03:53:09Z>\n> @Anthony6197 I think the llava code support input format in image list according to this code. https://github.com/haotian-liu/LLaVA/blob/main/llava/model/llava_arch.py#L114 what you should do is just modify the data process code.\r\n\r\nBut I think the more sever problem is the lack of such data in the training dataset... By the way, I want to inquire whether there are any high-quality interleaved multi-image datasets?\n</Comment>\n<Comment by xmy0916 at 2023-11-07T04:05:32Z>\n@tingxueronghua \r\npretrain dataset: webvid-10M(video), blip_laion_cc_sbu_558k(image),Charades_v1(video)\r\nsft dataset: refer to videochat, llava, videollama, mplug, llavar\n</Comment>\n<Comment by xmy0916 at 2023-11-07T04:07:40Z>\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models#awesome-datasets\n</Comment>\n<Comment by tingxueronghua at 2023-11-07T08:49:21Z>\n@xmy0916 Thanks!\n</Comment>\n<Comment by jameszhou-gl at 2023-11-10T12:23:22Z>\n> > @Anthony6197 I think the llava code support input format in image list according to this code. https://github.com/haotian-liu/LLaVA/blob/main/llava/model/llava_arch.py#L114 what you should do is just modify the data process code.\r\n> \r\n> But I think the more sever problem is the lack of such data in the training dataset... By the way, I want to inquire whether there are any high-quality interleaved multi-image datasets?\r\n\r\nHi @tingxueronghua , have you implemented multiple images input and got some exact results?\n</Comment>\n<Comment by anas-zafar at 2024-08-05T05:41:45Z>\nHi @tingxueronghua, any luck with the multi image input?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 764,
    "state": "closed",
    "created_by": "zhengsipeng",
    "created_at": "2023-11-06T15:20:13Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/764</URL>\n\n<TITLE>Rreasonable loss of pre-training (Stage-1) loss.</TITLE>\n\n<BODY>### Question\n\nThank you for your wonderful work, \r\nMay I know the reasonble loss of LLaVA during the 1-st stage of pre-training?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-06T17:06:53Z>\nHi, we have provided all training logs in MODEL ZOO for LLaVA-1.5. You can check out the figure.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#llava-v15\r\nTraining logs: [wandb](https://api.wandb.ai/links/lht/6orh56wc).\n</Comment>\n<Comment by zhengsipeng at 2023-11-07T06:35:26Z>\nThanks!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 763,
    "state": "open",
    "created_by": "pipilurj",
    "created_at": "2023-11-06T12:24:21Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/763</URL>\n\n<TITLE>[Feature request] MistraL-7B support</TITLE>\n\n<BODY>### feature\n\nHi! Mistral-7B seems to be a great opensource LLM, do you have plans on integrating it into LLAVA? Thanks!</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-06T17:05:50Z>\nYes, we are working on integrating Mistral-7B to the code base, and since it requires a later transformers version, we are working to make sure that the existing code does not break when we upgrade, as there is a major change in terms of tokenization after we upgrade.\n</Comment>\n<Comment by liyang-7 at 2023-11-08T02:03:37Z>\nThere seems to be a problem with directly using the current tokenization to train Mistral-7B. \r\nCan you elaborate on what aspects need to be modified for Mistral-7B or when the code for training Mistral-7B can be open-sourced?\r\nThanks!\n</Comment>\n<Comment by liyang-7 at 2023-11-08T03:06:57Z>\nCan we exchange WeChat for further convenience communication in Mistral or Zephyr training?  ID：tiancaili7\n</Comment>\n<Comment by BlueBlueFF at 2023-11-30T05:38:21Z>\nany update？\n</Comment>\n<Comment by ChunyuanLI at 2023-11-30T07:09:32Z>\nhttps://huggingface.co/SkunkworksAI/BakLLaVA-1\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 762,
    "state": "closed",
    "created_by": "LetsGoFir",
    "created_at": "2023-11-06T11:50:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/762</URL>\n\n<TITLE>What does this `sep` do in your code?</TITLE>\n\n<BODY>Thanks for your wonderful work!\r\nCould you please tell me what does it do? I have no idea at all.\r\nhttps://github.com/haotian-liu/LLaVA/blob/eabb7102ea4c3c7dca5d34988eea6bf441920b6f/llava/train/train.py#L566</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-06T17:03:45Z>\nYou can imagine that this leverages the model's raw text completion capability on the large scale pretraining:\r\n\r\nGiven: <IMAGE>\r\nThe model predicts: <DESCRIPTION>\r\n\r\nBy doing this, we align <IMAGE> to the language model's distribution.\n</Comment>\n<Comment by LetsGoFir at 2023-11-07T06:19:38Z>\n@haotian-liu thanks for your reply! I have this confusion because I have many long datas and want to truncate them in the training.\r\nI think I should add the `sep` first and then truncate.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 761,
    "state": "open",
    "created_by": "zhuzilin",
    "created_at": "2023-11-06T10:58:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/761</URL>\n\n<TITLE>[Discussion] Change the RoPE positional encoding of the visual tokens</TITLE>\n\n<BODY>### Discussion\n\nThank you for your amazing work!\r\n\r\nWhile I'm trying to replicate it, I wonder if you have tried to set the RoPE position for the visual tokens? For example, all the image tokens can share the same positional id (instead of i, i+1, i+2, ...), so that the model could treat them equally.</BODY>\n\n<COMMENTS>\n<Comment by Camellia-hz at 2024-07-11T08:48:11Z>\n@zhuzilin Hello, I haven't looked at the code in detail yet, I think that since LLaVA uses Vicuna as the initial LLM model, the length of the sentence is fixed, so it shouldn't be possible to add ROPE for the visual tokens, the above is just my speculation, do you have any idea of how he did it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 759,
    "state": "closed",
    "created_by": "LetsGoFir",
    "created_at": "2023-11-06T09:32:02Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/759</URL>\n\n<TITLE>[Feature request] backward stuck with non-multimodal data</TITLE>\n\n<BODY>### feature\r\n\r\nThanks for your great work!\r\n\r\nBut I think code below cannot take vision tower into backward with mixed data(multimodal and non-multimodal), since I was still stuck with small batchsize after forward and got \"NCCL timeout\" finally.\r\nBig batchsize is OK, because there hardly to sample only non-multimodal data on one GPU, I think. I mixed them at a ratio 1:1.\r\n```\r\nif (cur_input_ids == IMAGE_TOKEN_INDEX).sum() == 0:\r\n       # multimodal LLM, but the current sample is not multimodal\r\n       cur_input_embeds = self.get_model().embed_tokens(cur_input_ids)\r\n       cur_input_embeds = cur_input_embeds + (0. * self.get_model().mm_projector(vision_tower.dummy_feature)).sum()\r\n```\r\nCould you please help me to fix it?</BODY>\n\n<COMMENTS>\n<Comment by LetsGoFir at 2023-11-06T10:13:06Z>\nfixed it by replacing above code with\r\n`0. * image_features[batch_idx]`\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 758,
    "state": "open",
    "created_by": "Priyansh-666",
    "created_at": "2023-11-06T09:21:36Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/758</URL>\n\n<TITLE>CUDA out of memory with 6GB VRAM 16 RAM , but works fine on 4 GB VRAM 8 GB RAM (slow but works)</TITLE>\n\n<BODY>### Describe the issue\n\nI am working with LLaVa on a simple picture on a GTX 1650 4 GB VRAM laptop with 8 GB RAM , and it works fine , thought its slow but it do give results\r\nwhen i run the same CLI command on my other PC that has RTX 4050 6 GB VRAM laptop and 16 GB RAM , it says CUDA out of memory since its allocating more than 6 GB\r\nI dont know what is the reason for this , one guess is that it is due to the batches it makes but i dont know how to set it for 6 GB</BODY>\n\n<COMMENTS>\n<Comment by krishna-exe at 2024-09-17T14:41:06Z>\nhello, i also have gtx 1650 4gb vram. which version of llava were u able to run on it?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 757,
    "state": "closed",
    "created_by": "guxm2021",
    "created_at": "2023-11-06T06:10:02Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/757</URL>\n\n<TITLE>No \"mm_projector_lr\" when fine-tuning llava-1.5-7b</TITLE>\n\n<BODY>### Describe the issue\n\nWhen I fine-tune the llava-1.5-7b using `scripts/v1_5/finetune_lora.sh` with minimal changes, here is my command\r\n\r\nCommand:\r\n```\r\ndeepspeed llava/train/train_mem.py \\\r\n    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\r\n    --deepspeed ./scripts/zero3.json \\\r\n    --model_name_or_path lmsys/vicuna-7b-v1.5 \\\r\n    --version v1 \\\r\n    --data_path ../LLaVA/playground/data/llava_v1_5_mix665k.json \\\r\n    --image_folder ../LLaVA/playground/data \\\r\n    --vision_tower openai/clip-vit-large-patch14-336 \\\r\n    --pretrain_mm_mlp_adapter ../LLaVA/checkpoints/llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5/mm_projector.bin \\\r\n    --mm_projector_type mlp2x_gelu \\\r\n    --mm_vision_select_layer -2 \\\r\n    --mm_use_im_start_end False \\\r\n    --mm_use_im_patch_token False \\\r\n    --image_aspect_ratio pad \\\r\n    --group_by_modality_length True \\\r\n    --bf16 True \\\r\n    --output_dir ./checkpoints/llava-v1.5-7b-lora \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 16 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 50000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-4 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --tf32 True \\\r\n    --model_max_length 2048 \\\r\n    --gradient_checkpointing True \\\r\n    --dataloader_num_workers 4 \\\r\n    --lazy_preprocess True \\\r\n    --report_to wandb\r\n```\r\n\r\nI encountered the value error: \r\nLog: \r\n```\r\nValueError: Some specified arguments are not used by the HfArgumentParser: ['--mm_projector_lr', '2e-5']\r\n```\r\n\r\nWhen I remove the argument `--mm_projector_lr 2e-5` in the command, the command can work. But I don't know whether I could get the correct model.</BODY>\n\n<COMMENTS>\n<Comment by Cooperx521 at 2024-03-11T08:50:45Z>\nHello, I have met the similar problem, do you know whether `--mm_projector_lr` works when setting `--learning_rate`? I'm confused about the priority of `--mm_projector_lr` and `--learning_rate 2e-4` @guxm2021 \r\n\r\n\r\n\r\n\r\n\r\n```\r\n            optimizer_cls, optimizer_kwargs = Trainer.get_optimizer_cls_and_kwargs(self.args)\r\n\r\n            self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)\r\n```\r\n@haotian-liu \r\nI just can not figure out whether `optimizer_grouped_parameters`  func in ` llava_trainer.py` works when `--learning_rate` is set\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 756,
    "state": "closed",
    "created_by": "yikun-chi",
    "created_at": "2023-11-06T02:49:02Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/756</URL>\n\n<TITLE>[Usage] Model eval.py Out of Memory error when running model eval on T4 but not RTX 3050Ti</TITLE>\n\n<BODY>### Describe the issue\n\nIssue: \r\n\r\nHi, I am trying to run the model_vqa.py for some test. My input only contains one image and one prompt. Right now on my own laptop (RTX 3050Ti) GPU, I was able to successfully run the model_vqa.py. But when I move the same input and prompt to a Vertex workbench with NVIDIA T4 GPU, I got the out of memory error. Also if I use the CLI inference process laid out in CLI Inference section, I was able to run and ask the prompt. \r\n\r\nGiven the model_vqa.py finished running on RTX 3050Ti, I am not sure why it would fail with T4 GPU. Any recommendation on how to start debugging this process would be appreciated. Thanks! \r\n\r\n\r\n \r\n\r\n\r\nCommand:\r\n```\r\npython model_vqa.py \\\r\n    --model-path ./checkpoints/llava-v1.5-7b \\\r\n    --question-file \\\r\n    playground/data/cellphone_screenshot_eval/prompt/cellphone_screenshot_questions.jsonl \\\r\n    --image-folder \\\r\n    playground/data/cellphone_screenshot_eval/images \\\r\n    --answers-file \\\r\n    playground/data/cellphone_screenshot_eval/answer.jsonl\r\n```\r\n\r\nLog: \r\n```\r\nLoading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:18<00:00, 39.11s/it]\r\n  0%|                                                                                                                                                           | 0/1 [00:03<?, ?it/s]\r\nTraceback (most recent call last):\r\n  File \"/home/jupyter/yc/LLaVA/model_vqa.py\", line 112, in <module>\r\n    eval_model(args)\r\n  File \"/home/jupyter/yc/LLaVA/model_vqa.py\", line 66, in eval_model\r\n    output_ids = model.generate(\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1588, in generate\r\n    return self.sample(\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2642, in sample\r\n    outputs = self(\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/jupyter/yc/LLaVA/llava/model/language_model/llava_llama.py\", line 78, in forward\r\n    outputs = self.model(\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 693, in forward\r\n    layer_outputs = decoder_layer(\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 408, in forward\r\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 346, in forward\r\n    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 14.58 GiB total capacity; 13.54 GiB already allocated; 13.38 MiB free; 13.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n```\r\n\r\nScreenshots:\r\nYou may attach screenshots if it better explains the issue.</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-06T03:05:59Z>\nIt should require 24GB to run with full precision. You can try using 4-bit quantization on T4 (and it will even fit 13B model).\r\n\r\nhttps://github.com/haotian-liu/LLaVA#launch-a-model-worker-4-bit-8-bit-inference-quantized\n</Comment>\n<Comment by yikun-chi at 2023-11-06T03:09:03Z>\nThanks for the response! I guess my confusion is how did the process finish in RTX 3050Ti, which only has 4GB of memory?\n</Comment>\n<Comment by haotian-liu at 2023-11-06T03:09:45Z>\nWere you running it on Linux or Windows?\n</Comment>\n<Comment by yikun-chi at 2023-11-06T03:10:47Z>\nLaptop is Windows, but the process was run in Ubuntu through WSL\n</Comment>\n<Comment by haotian-liu at 2023-11-06T03:11:52Z>\nI noticed that on Windows, it uses part of the CPU RAM as sort of SWAP for the VRAM. Not sure if this transfer towards WSL as well -- you may check when you run the program, does your CPU RAM goes up by around 10GB?\n</Comment>\n<Comment by yikun-chi at 2023-11-06T03:18:17Z>\nI see. Interesting property, that's probably it. . Yeah, the PC has 32 GB of RAM. Closing the issue. Thank you so much for your help!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 755,
    "state": "open",
    "created_by": "test16553",
    "created_at": "2023-11-06T02:36:23Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/755</URL>\n\n<TITLE>[Question] When loading the model, I encountered TypeError: not a string. How to solve it?</TITLE>\n\n<BODY>### Question\n\nI can load the model normally for inference using LLaVA-1.5-7B, but following the same steps for 13B, TypeError: not a string , which indicates that vocab_file is not a string, but an unsupported data type. How to solve this problem?</BODY>\n\n<COMMENTS>\n<Comment by test16553 at 2023-11-06T12:03:04Z>\n![报错截图](https://github.com/haotian-liu/LLaVA/assets/139971500/09be4a38-62b5-4a10-a2ed-1a0e8fcc2d31)\n</Comment>\n<Comment by haotian-liu at 2023-11-06T17:21:34Z>\nHi,\r\n\r\nSeems that you are loading the checkpoint from a local file, can you make sure that the tokenizer files are properly downloaded as well: https://github.com/haotian-liu/LLaVA/issues/718\r\n\r\nAlso, noticed that this is not in the conda env, package version conflicts can create weird issues as well, if the above does not work, please try install the environment following the instructions [here](https://github.com/haotian-liu/LLaVA#install).\n</Comment>\n<Comment by ds22058 at 2023-11-18T10:25:51Z>\n@test16553  Hi, I met the same problem. Have you solved it?\n</Comment>\n<Comment by yuezhao238 at 2023-12-27T20:35:34Z>\nmake sure tokenizer.model is in your folder~\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 754,
    "state": "closed",
    "created_by": "haotian-liu",
    "created_at": "2023-11-05T05:01:23Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/754</URL>\n\n<TITLE>Batch evaluation</TITLE>\n\n<BODY>**Update: Batch evaluation is supported with SGLang.**\r\n\r\nBatch eval example: https://github.com/sgl-project/sglang/tree/main/benchmark/llava_bench, which can be 5x faster on LLaVA bench.\r\n\r\nContinuous batching for serving: \r\nhttps://github.com/haotian-liu/LLaVA?tab=readme-ov-file#launch-a-sglang-worker\r\n\r\n---\r\n\r\nBatch eval has been the most wanted features and I have tried to create one for that [here](https://github.com/haotian-liu/LLaVA/blob/develop/llava/eval/model_vqa_batch.py) in the `dev` branch. Currently, we identified an issue with the script above, that can cause NaN in the generation.\r\n\r\nWe'll use this issue to track the status of the batch evaluation. \r\n\r\n- [x] NaN in batch eval\r\n- [ ] Minor inconsistency between different batch sizes\r\n- [ ] Improve efficiency</BODY>\n\n<COMMENTS>\n<Comment by shams2023 at 2023-11-05T06:51:15Z>\nBesides batch inference for VQA tasks, can batch inference be performed on image caption tasks?\n</Comment>\n<Comment by haotian-liu at 2023-11-05T11:13:27Z>\n@shams2023 Yes we will also support that.\n</Comment>\n<Comment by shams2023 at 2023-11-05T13:01:17Z>\n> @shams2023 Yes we will also support that.\r\n\r\nI really need to use this task to generate text descriptions for my lower resolution image dataset. I have used BLIP (after fine-tuning), but the results are not good, so I need to try using it.\n</Comment>\n<Comment by tweeter0830 at 2023-11-06T23:40:01Z>\nThis would be really great to have! thank you for working on it.\n</Comment>\n<Comment by HenryHZY at 2023-11-07T01:22:05Z>\n@haotian-liu Hi, batch evaluation is really important for VQAv2, which takes too much time. I think issue #675 has provided a solution to batch image captioning without evaluation. If would be great If you could support batch image captioning evaluation (e.g., CIDEr) for some traditional tasks (e.g., COCO and NoCaps).\n</Comment>\n<Comment by rabiulcste at 2023-11-09T02:33:59Z>\nI understand that the `generate()` function is not behaving as intended! I did some profiling a few weeks ago.\n</Comment>\n<Comment by dongzhiwu at 2023-11-15T00:59:36Z>\nhellow, is any new progress? Besides batch inference for VQA tasks, can batch inference be performed on image caption tasks?\n</Comment>\n<Comment by BrainWWW at 2023-11-21T09:36:09Z>\nI found in my experiments that using the same question, e.g. describe the image, the answers are basically consistent when using different batch sizes for inference. However, when each image uses a different question, the model outputs strange answers. May I ask why this problem occurs? thank you so much!\r\n\r\n{\"question_id\": 0, \"prompt\": \"What is the color of the two suitcases in the image?\", \"text\": \"The color of the two suitcases in the image is black.\", \"answer_id\": \"2sdWELixEN6LQ7BRen73b6\", \"model_id\": \"llava-v1.5-13b\", \"metadata\": {}}\r\n{\"question_id\": 1, \"prompt\": \"Analyze the image in a comprehensive and detailed manner.\", \"text\": \"The image features a close-up of a young man's face, with a focus on his eyes and lips. The man appears to be looking at the camera, and his eyes are slightly open. The image is a digital drawing or illustration, capturing the man's facial features in detail. The background is white, which further emphasizes the subject's face and expression.\", \"answer_id\": \"LcavJhTSXJLvaCaXUBvk9f\", \"model_id\": \"llava-v1.5-13b\", \"metadata\": {}}\r\n{\"question_id\": 2, \"prompt\": \"What potential factors could make these suitcases valuable?\", \"text\": \"**The two-pure-tubes-and-barges-in-the-cows-and-barges-in-the-cows-and-barges-in the-cows-and-barges-in the-cows-and-barges-in the-cows-and-barges-in the-cows-and-barges-in the-cows-and-barges-in the-cows-and-barges-in the-cows-and-barges-in the**\", \"answer_id\": \"RfX2ZosPS5npuMxrGszWkV\", \"model_id\": \"llava-v1.5-13b\", \"metadata\": {}}\r\n{\"question_id\": 3, \"prompt\": \"What are the main objects on the table in the image?\", \"text\": \"T**he, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and, the, and**,\", \"answer_id\": \"k9Lzv2yd5cj5684fRM9EDF\", \"model_id\": \"llava-v1.5-13b\", \"metadata\": {}}\r\n{\"question_id\": 4, \"prompt\": \"Describe the following image.\", \"text\": \"A black and white photo of a person wearing a black and white shirt. The person is wearing a black and white shirt.\", \"answer_id\": \"mJVswmZ4ctREnnwp6nnaUB\", \"model_id\": \"llava-v1.5-13b\", \"metadata\": {}}\r\n{\"question_id\": 5, \"prompt\": \"What activity might have recently taken place around this table and what could be its aftermath?\", \"text\": \"It is likely that a recent activity involving the use of the two broken wooden chopsticks took place around the table. The chopsticks are now broken and lying on the table, which suggests that they were used for eating or cooking and have since broken during the process. The aftermath of this situation could be that the person using the chopsticks might need to find an alternative method to eat or cook, as the broken chopsticks are no longer functional. **Additionally, the broken chopsticks may pose a safety hazard if not properly disposed of, as they could cause injury if**\", \"answer_id\": \"NaP9Dq8n2NRSMQrg69YUmN\", \"model_id\": \"llava-v1.5-13b\", \"metadata\": {}}\r\n{\"question_id\": 6, \"prompt\": \"What is the main focus of the image?\", \"text\": \"The main focus of the image is the two-pure-**white-fie-tie-pie-tie-pie-tie-pie-tie-pie-tie-pie-tie-pie-tie-pie-tie-pie-tie-pie-tie-pie-tie-pie-tie-pie-tie-pie-tie-pie-tie-pie-tie-pie-tie-pie-tie-pie-tie**\", \"answer_id\": \"FvY25Pqw6zued6Abeirwoh\", \"model_id\": \"llava-v1.5-13b\", \"metadata\": {}}\r\n{\"question_id\": 7, \"prompt\": \"What is this photo about'?\", \"text\": \"The image is a photo of a person's feet, which are the most visible part of the person's body. The person's feet are the main focus of the photo, and the person's body is the most visible part of the photo.\", \"answer_id\": \"jGL4pqaVXXYL3driouLDrW\", \"model_id\": \"llava-v1.5-13b\", \"metadata\": {}}\r\n{\"question_id\": 8, \"prompt\": \"What could be a reason for the cat's interest in the laptop?\", \"text\": \"The cat's interest in the laptop, which is a part of the laptop, **is a result of the cat's natural, and the cat's natural, and the cat's natural, and the cat's natural, and the cat's natural, and the cat's natural, and the cat's natural, and the cat's natural, and the cat's natural, and the cat's natural, and the cat's natural, and the cat's natural, and the cat's natural, and the cat's natural, and the cat's natural,** and the cat\", \"answer_id\": \"jawMBaRhvyz98K93QRqPap\", \"model_id\": \"llava-v1.5-13b\", \"metadata\": {}}\r\n{\"question_id\": 9, \"prompt\": \"What color is the dog in the image?\", \"text\": \"The dog in the image is a black dog, and the person is a white person.\", \"answer_id\": \"BHNjYWe8sVuPGLj4ikBG78\", \"model_id\": \"llava-v1.5-13b\", \"metadata\": {}}\r\n\"\"\"\n</Comment>\n<Comment by shams2023 at 2023-11-21T10:43:44Z>\n> hellow, is any new progress? Besides batch inference for VQA tasks, can batch inference be performed on image caption tasks?\r\n\r\nHave you completed the task for image captions?\n</Comment>\n<Comment by yanbai1993 at 2023-11-28T06:37:49Z>\nHi, When I use multiple GPU for multi-batch inference, I encounter the following error. However, when performing multi-batch inference on a single GPU (develop branch), or single-batch inference on multiple GPU (main branch), I do not encounter this problem. I look forward to your reply.\r\n\r\nTraceback (most recent call last):\r\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-perception/baiyan02/conda_env/llava_env/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-perception/baiyan02/conda_env/llava_env/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/workdir/baiyan02/2071702/70b4c7f21b349f3a43f519b683820c89/llava/eval/model_vqa_batch.py\", line 177, in <module>\r\n    eval_model(args)\r\n  File \"/workdir/baiyan02/2071702/70b4c7f21b349f3a43f519b683820c89/llava/eval/model_vqa_batch.py\", line 133, in eval_model\r\n    output_ids = model.generate(\r\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-perception/baiyan02/conda_env/llava_env/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/workdir/baiyan02/2071702/70b4c7f21b349f3a43f519b683820c89/llava/model/language_model/llava_llama.py\", line 121, in generate\r\n    ) = self.prepare_inputs_labels_for_multimodal(\r\n  File \"/workdir/baiyan02/2071702/70b4c7f21b349f3a43f519b683820c89/llava/model/llava_arch.py\", line 183, in prepare_inputs_labels_for_multimodal\r\n    cur_new_input_embeds = torch.cat(cur_new_input_embeds)\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument tensors in method wrapper_CUDA_cat)\r\n\r\n![2023-11-27 13-16-37屏幕截图](https://github.com/haotian-liu/LLaVA/assets/15922438/2cc2da43-f874-42e4-a02d-820f12d5c165\n</Comment>\n<Comment by ChantalMP at 2023-12-28T12:17:56Z>\nFor me, batch evaluation also gave nan values, after fine-tuning. Performing evaluation also with bfloat16 instead of float16 solved this for me. (I also fine-tuned using bf16 True). Like this, batchsize 1 and higher ones give very similar results.\n</Comment>\n<Comment by hkristof03 at 2024-01-07T20:48:57Z>\n@haotian-liu thank you for this amazing work. I just started getting familiar with this repository recently. I would like to point out a few things and also ask a question.\r\nYou provide an [example here](https://github.com/haotian-liu/LLaVA?tab=readme-ov-file#quick-start-with-huggingface), where you load the model once \r\n\r\n`tokenizer, model, image_processor, context_len = load_pretrained_model(\r\n    model_path=model_path,\r\n    model_base=None,\r\n    model_name=get_model_name_from_path(model_path)\r\n)`\r\n\r\nand then again when calling `eval_model(args)`. (The model is loaded twice into memory).\r\n\r\nI haven't dug deeper into batch inference but came here instead and I see it is not supported yet?\r\nWhat I don't understand, there is a [function to load multiple images.](https://github.com/haotian-liu/LLaVA/blob/9a26bd1435b4ac42c282757f2c16d34226575e96/llava/eval/run_llava.py#L43), which is passed to the model with a single prompt, I expected to generate multiple outputs from one prompt and multiple images. However the model generates one output even if I remove the [index here](https://github.com/haotian-liu/LLaVA/blob/9a26bd1435b4ac42c282757f2c16d34226575e96/llava/eval/run_llava.py#L135).\r\n\r\nI read through your papers but haven't found a case of multiple images passed to the model at once. Could you clarify and maybe comment on the batch inference issue?\n</Comment>\n<Comment by merrymercy at 2024-01-24T11:47:54Z>\nFor anyone interested in this issue, we collaborated with @haotian-liu and implemented a high-throughput inference server.\r\nYou can find examples of batch processing here https://github.com/sgl-project/sglang/tree/main/benchmark/llava_bench,\r\n which can be 5x faster on LLaVA bench.\n</Comment>\n<Comment by pseudotensor at 2024-02-08T09:39:32Z>\n@merrymercy Thanks.  I was about to try sglang but in the documentation on this repo it mentions the tokenizer needs to come from llava-hf on HF.  But right now there is only 7b for 1.6, none of the others for 1.6.  Is that intentional?  How should one proceed?  Thanks!\n</Comment>\n<Comment by pseudotensor at 2024-02-08T10:17:11Z>\n@haotian-liu Related, I notice if gradio server is hit with (say) 3 concurrent requests, the generation is about 3x slower.  It would be nice to try sglang, but from your docs it seems we need to have those other tokenizers in llava-hf?\n</Comment>\n<Comment by haotian-liu at 2024-02-09T17:03:35Z>\n@pseudotensor Sorry for the confusion.\r\n\r\nTokenizers (temporary): `llava-hf/llava-1.5-7b-hf`, `llava-hf/llava-1.5-13b-hf`, `liuhaotian/llava-v1.6-34b-tokenizer`.\r\n\r\nWe'll update the full repo to remove the need of tokenizers soon.\r\n\r\nAlso, you would need to use SGLang for the continuous batching and it does not have visible degradation in generation speed when batching.\n</Comment>\n<Comment by pseudotensor at 2024-02-10T02:44:39Z>\n> @pseudotensor Sorry for the confusion.\r\n> \r\n> Tokenizers (temporary): `llava-hf/llava-1.5-7b-hf`, `llava-hf/llava-1.5-13b-hf`, `liuhaotian/llava-v1.6-34b-tokenizer`.\r\n> \r\n> We'll update the full repo to remove the need of tokenizers soon.\r\n> \r\n> Also, you would need to use SGLang for the continuous batching and it does not have visible degradation in generation speed when batching.\r\n\r\nThanks. so just the 34b tokenizer for 1.6 for now.  I expect you'd rather do the planned removal of need for tokenizers than add the other tokenizers.\n</Comment>\n<Comment by haotian-liu at 2024-02-10T13:56:23Z>\n@pseudotensor \r\n\r\nYep we'll remove the need of doing so. Btw, the 7B/13B tokenizers are valid for 1.6 :)\n</Comment>\n<Comment by fisher75 at 2024-03-25T10:19:43Z>\nHi, I think for SGLang we need a feature for multiple rounds discussions for the ICL, or for few-shot. Can you provide a template for ICL in SGLang? currently the example is only one-time inference.\n</Comment>\n<Comment by fisher75 at 2024-04-15T15:32:47Z>\nNeed someone real hardcore to answer my question haha 👍 \r\n> ### Question\r\n> I saw at README saying those models are supported, got two questions: (1) what if I wanna use `llava-v1.6-vicuna-13b` or any other LLaVa models, is that possible? Thanks! (2) After I fine-tune or lora the existing models, how can I do a batch inference with it since in SGLang it looks that I need --model-path and --tokenizer-path to do batch inference.\r\n> \r\n> > LLaVA\r\n> > python3 -m sglang.launch_server --model-path liuhaotian/llava-v1.5-7b --tokenizer-path llava-hf/llava-1.5-7b-hf --chat-template vicuna_v1.1 --port 30000\r\n> > python3 -m sglang.launch_server --model-path liuhaotian/llava-v1.6-vicuna-7b --tokenizer-path llava-hf/llava-1.5-7b-hf --chat-template vicuna_v1.1 --port 30000\r\n> > python3 -m sglang.launch_server --model-path liuhaotian/llava-v1.6-34b --tokenizer-path liuhaotian/llava-v1.6-34b-tokenizer --port 3000\n</Comment>\n<Comment by dgarnitz at 2024-04-15T18:23:42Z>\nIs there any plan to make the batch inferencing available with standard hugging face code? I am trying to use serverless GPUs so running sgl's inference server is not going to work. \r\n\r\nRight now when I attempt to batch inference, it only runs inference over the first image in the batch repeatedly. For example, in the code below, the output is the same every time:\r\n\r\n```python\r\nasync def completion_stream(self, user_questions, images_data):\r\n    hf_logging.set_verbosity_info()\r\n    \r\n    # Prepare batch inputs\r\n    batch_inputs = []\r\n    for user_question, image_data in zip(user_questions, images_data):\r\n        image = Image.open(BytesIO(image_data))\r\n        prompt = f\"system\\nAnswer the questions.user\\n<image>\\n{user_question}assistant\\n\"\r\n        inputs = self.processor(prompt, image, return_tensors=\"pt\")\r\n        batch_inputs.append(inputs['input_ids'])\r\n    \r\n    # Concatenate all input_ids in a batch\r\n    batch_input_ids = torch.cat(batch_inputs, dim=0).to(\"cuda:0\")\r\n\r\n    # Perform batch inference\r\n    output = self.model.generate(input_ids=batch_input_ids, max_new_tokens=1536)\r\n    \r\n    # Decode each output in the batch and yield word by word\r\n    for o in output:\r\n        answer = self.processor.decode(o, skip_special_tokens=True)\r\n        words = answer.split()\r\n        for word in words:\r\n            yield word + ' '\r\n        yield '\\n'\r\n```\r\n\r\nAt the bare minimum, a much more detailed and well explained example of how to run the batching on sglang would be extremely helpful for helping to run and reverse engineer the code.\n</Comment>\n<Comment by Vignesh-Valaboju at 2024-05-31T18:30:32Z>\nHas anyone attempted batch inference without sglang? I am noticing that batch size is affecting the output. It looks like batch size is impacting the preprocessing and padding of the input tokenized sequences. When you use a batchsize > 1, all the token sequences are padded with 0 to be the same length. Llava doesn't understand this padding -- has anyone tried a work around?\n</Comment>\n<Comment by dacian7 at 2024-07-14T00:53:42Z>\n> Has anyone attempted batch inference without sglang? I am noticing that batch size is affecting the output. It looks like batch size is impacting the preprocessing and padding of the input tokenized sequences. When you use a batchsize > 1, all the token sequences are padded with 0 to be the same length. Llava doesn't understand this padding -- has anyone tried a work around?\r\n\r\n@Vignesh-Valaboju  Hi, same problem, have you solved this issue?\n</Comment>\n<Comment by XuGW-Kevin at 2024-07-29T12:58:51Z>\n@Vignesh-Valaboju @dacian7 #269 provides a feasible solution for this:\r\nChange the padding side to tokenizer.padding_side = \"left\", and modify KeywordsStoppingCriteria to make it support batch inference.\n</Comment>\n<Comment by CongYep at 2024-08-14T12:52:33Z>\n@XuGW-Kevin I do want to know in which file and on which line to modify tokenizer.padding_side = \"left\" and in which file modify KeywordsStoppingCriteria? Thank you.\n</Comment>\n<Comment by XuGW-Kevin at 2024-08-15T14:58:05Z>\nAdd on any line: model.llm.config.tokenizer_padding_side = \"left\"\r\nKeywordsStoppingCriteria: llava/mm_utils.py\n</Comment>\n<Comment by copperwiring at 2024-09-23T13:00:50Z>\n@XuGW-Kevin How did you support batch inference? Even thoug I updated keywordstopping uisng the issue you linked, I cant add model.llm.config.tokenizer_padding_side = \"left\"\r\n\r\nIt says `AttributeError: 'LlavaLlamaForCausalLM' object has no attribute 'llm'`. I then added simply `tokenizer.padding_side = \"left\"` as you suggested @CongYep \r\n\r\nThis is my code:\r\n\r\n```\r\n    for prompt in prompts_batch:\r\n        # Set args.query to the specific prompt in the batch\r\n        args.query = prompt\r\n\r\n        # Generate the prompt for each input in the batch, with the correct image handling\r\n        qs = get_prompt(args, model)\r\n\r\n        # Create a new conversation template for each prompt in the batch\r\n        conv = conv_templates[args.conv_mode].copy()\r\n        conv.append_message(conv.roles[0], qs)\r\n        conv.append_message(conv.roles[1], None)\r\n\r\n        # Add the complete prompt for this instance to the batch\r\n        batched_prompts.append(conv.get_prompt())\r\n\r\n    tokenizer.padding_side = \"left\"\r\n    # Tokenize the batch of prompts\r\n    tokenized_prompts = [\r\n        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0)\r\n        for prompt in batched_prompts\r\n    ]\r\n\r\n    input_ids = torch.cat(tokenized_prompts, dim=0).cuda()\r\n\r\n    # Process images if provided (batch image loading and processing)\r\n    if img_files_batch:\r\n        # For each batch, parse image files, load them, and process\r\n        image_files_batch = [image_parser(img_files, args.sep) for img_files in img_files_batch]\r\n        images = [load_images(image_files) for image_files in image_files_batch]\r\n        flat_images = [item for sublist in images for item in sublist]\r\n        images_tensor = process_images(flat_images, image_processor, model.config).to(model.device, dtype=torch.float16)\r\n        image_sizes = [img.size for img in flat_images]\r\n    else:\r\n        images_tensor = None\r\n        image_sizes = None\r\n    \r\n    attention_mask = torch.ones_like(input_ids)\r\n\r\n    with torch.inference_mode(), torch.cuda.amp.autocast():\r\n        outputs = model.forward(\r\n            input_ids=input_ids, \r\n            images=None if images_tensor is None else images_tensor,\r\n            image_sizes=image_sizes,\r\n            attention_mask=attention_mask\r\n            )\r\n    \r\n    logits = outputs.logits[:, -1, :]  # Get the logits for the last token position\r\n    probabilities = F.softmax(logits, dim=-1).squeeze()\r\n```\r\n\r\nBut it wont do concatenation at `input_ids = torch.cat(tokenized_prompts, dim=0).cuda()`\r\n\r\nError: `RuntimeError: Sizes of tensors must match except in dimension 0. Expected size 158 but got size 179 for tensor number 2 in the list.` I know input ids are different but then how do I pad them equally so that inference can happen in batch?\n</Comment>\n<Comment by copperwiring at 2024-09-23T13:34:07Z>\n@Vignesh-Valaboju I did left padding like this\r\n\r\n```\r\n# left padding\r\ndef left_pad_sequence_to_max_length(sequence, max_length, padding_value=0):\r\n    \"\"\"Pad a sequence to the desired max length.\"\"\"\r\n    if len(sequence) >= max_length:\r\n        return sequence\r\n    return torch.cat([torch.full((max_length - len(sequence),), padding_value, dtype=sequence.dtype), sequence])\r\n\r\n```\r\n```\r\n\r\n    # Tokenize the batch of prompts\r\n    tokenized_prompts = [\r\n        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0)\r\n        for prompt in batched_prompts\r\n    ]\r\n\r\n    # Determine the maximum length of input_ids in the batch\r\n    max_len = max([len(tokenized_prompt.squeeze()) for tokenized_prompt in tokenized_prompts])\r\n\r\n    # Pad the input_ids to the maximum length\r\n    padded_tokenized_ids= [left_pad_sequence_to_max_length(tokenized_prompt.squeeze(), max_len) for tokenized_prompt in tokenized_prompts]\r\n    batched_input_ids = torch.stack(padded_tokenized_ids).to(model.device)\r\n\r\n```\r\n```\r\nattention_mask = torch.ones_like(batched_input_ids)\r\n\r\n```\r\nand pass `attention_mask` to model.generate(), but results look completely wrong. Did it work for you?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 753,
    "state": "closed",
    "created_by": "pbenaim",
    "created_at": "2023-11-04T12:29:07Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/753</URL>\n\n<TITLE>[Feature request] Use LLaVA to recognise one specific person.</TITLE>\n\n<BODY>### feature\n\nLLaVA is very accurate to analyse one picture 👌\r\n\r\nI love it, because we can ask, any questions, in an other language than English, like me in French 😍\r\n\r\nSo, I think is it possible to recognise one specific person after some trainings. \r\n\r\nSo me question is, because I haven't any idea, how to do that?\r\n\r\nAny suggestions? Or is it impossible in the actual state of LLaVA... 😭</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-05T04:46:27Z>\nThat's totally possible, and you can do that by creating a custom dataset for that and finetuning LLaVA-1.5 on that dataset.\r\n\r\nhttps://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 752,
    "state": "closed",
    "created_by": "test16553",
    "created_at": "2023-11-04T06:37:22Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/752</URL>\n\n<TITLE>Hello, I would like to ask, if you want to use LLaVA-1.5 for inference without training, do you need to download the corresponding Projector weights after downloading checkpoints?</TITLE>\n\n<BODY>### Describe the issue\n\nHello, I would like to ask, if you want to use LLaVA-1.5 for inference without training, do you need to download the corresponding Projector weights after downloading checkpoints?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-05T01:06:23Z>\nHi, you do not need to download the projectors. They should NOT be used in any case, unless you want to *reproduce* LLaVA-1.5. Thanks.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 750,
    "state": "closed",
    "created_by": "wdrink",
    "created_at": "2023-11-03T14:04:58Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/750</URL>\n\n<TITLE>Could someone explain the effects of model_args.version? Will it have a big impact on finetuning performance? Thanks! [Question]</TITLE>\n\n<BODY>### Question\n\n_No response_</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 748,
    "state": "open",
    "created_by": "dydxdt",
    "created_at": "2023-11-03T11:01:40Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/748</URL>\n\n<TITLE>[Feature request] Support Multi-image input</TITLE>\n\n<BODY>### feature\n\nGreat job!\r\nWill LLava be able to support multiple images as input? Like other MLLMs,such as Qwen?</BODY>\n\n<COMMENTS>\n<Comment by shure-dev at 2023-11-04T08:57:42Z>\nHi, I also really need this feature\r\nThere is an issue for the same topic but not resolved https://github.com/haotian-liu/LLaVA/issues/197\r\nI want to know if this is possible or not for the current version by customizing the code\n</Comment>\n<Comment by shure-dev at 2023-11-05T01:26:48Z>\nThey just updated\r\nhttps://github.com/haotian-liu/LLaVA/pull/432\n</Comment>\n<Comment by dydxdt at 2023-11-10T07:11:24Z>\n> They just updated #432\r\n\r\nIt seems that it only supports evaluation not training? @shure-dev\n</Comment>\n<Comment by shure-dev at 2023-12-12T05:52:44Z>\nAccording to their comment, I think you are right. \r\n\r\nIn their training process,\r\nhttps://github.com/haotian-liu/LLaVA/pull/432#issuecomment-1826223545\r\n> Maybe the model learns one-to-one text-image mapping during contrastive training,\r\n\r\nIt's possible to give it multiple image in evaluation process however, it is not asured. \r\nAre you trying to train it by yourself?\r\n\r\nI hope this topic will have more attention.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 747,
    "state": "open",
    "created_by": "shams2023",
    "created_at": "2023-11-03T10:43:53Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/747</URL>\n\n<TITLE>Is it possible to perform batch caption generation operations on images in the image dataset?</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nThis is a great job, thank you for your contribution!\r\nMay I ask if this project can perform mass caption generation operations on images in the image dataset?</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 746,
    "state": "open",
    "created_by": "csvlrs-lak",
    "created_at": "2023-11-03T08:17:11Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/746</URL>\n\n<TITLE>facing issue while running finetuning script (v1.5 - lora) - AttributeError: 'DummyScheduler' object has no attribute 'get_last_lr'.</TITLE>\n\n<BODY>### Describe the issue\r\n\r\nIssue:\r\nAttributeError: 'DummyScheduler' object has no attribute 'get_last_lr'.\r\n\r\nCommand:\r\n```\r\nsh scripts/v1_5/finetune_lora.sh\r\n ```\r\n\r\nLog: \r\n```\r\nTraceback (most recent call last):\r\n  File \"llava/train/train.py\", line 956, in <module>\r\n    train()\r\n  File \"llava/train/train.py\", line 934, in train\r\n    trainer.train()\r\n  File \"/home/.local/lib/python3.8/site-packages/transformers/trainer.py\", line 1645, in train\r\n    return inner_training_loop(\r\n  File \"/home/.local/lib/python3.8/site-packages/transformers/trainer.py\", line 2011, in _inner_training_loop\r\n    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\r\n  File \"/home/.local/lib/python3.8/site-packages/transformers/trainer.py\", line 2292, in _maybe_log_save_evaluate\r\n    logs[\"learning_rate\"] = self._get_learning_rate()\r\n  File \"/home/.local/lib/python3.8/site-packages/transformers/trainer_pt_utils.py\", line 846, in _get_learning_rate\r\n    last_lr = self.lr_scheduler.get_last_lr()[0]\r\nAttributeError: 'DummyScheduler' object has no attribute 'get_last_lr'.\r\n```</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-05T04:47:28Z>\nHi, it seems that the environment is not in Conda and may have package version mismatch issues.\r\n\r\nPlease follow the instructions here to create a new environment for LLaVA: https://github.com/haotian-liu/LLaVA#install\n</Comment>\n<Comment by csvlrs-lak at 2023-11-06T05:03:34Z>\nThank you for the response @haotian-liu. I will try the fix suggested!!\n</Comment>\n<Comment by csvlrs-lak at 2023-11-06T09:30:01Z>\nworked!!\n</Comment>\n<Comment by csvlrs-lak at 2023-11-08T14:56:47Z>\nNot sure how, getting this error again!!\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 745,
    "state": "open",
    "created_by": "gombumsoo",
    "created_at": "2023-11-03T05:57:26Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/745</URL>\n\n<TITLE>[Question] How did you generate question in Detailed image description dataset?</TITLE>\n\n<BODY>### Question\r\n\r\nHi, when i read \"Visual Instruction Tuning\", I knew that the Detailed image description's questions were sampled from table9(Instructions for detailed image description) in paper. \r\nBut questions(user questions) from [detail_23k.json](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/detail_23k.json) don't match with table9. \r\nAccording to [here](https://github.com/haotian-liu/LLaVA/tree/main/playground/data/prompts/detail_description), when you generate Detailed image description by GPT-4, you don't mention about generating questions.\r\nI wonder how did you create those questions in detail_23k.json. \r\n\r\nThank you to read this</BODY>\n\n<COMMENTS>\n</COMMENTS>"
  },
  {
    "issue_number": 744,
    "state": "open",
    "created_by": "TonyUSTC",
    "created_at": "2023-11-03T02:29:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/744</URL>\n\n<TITLE>[Question] size mismatch for mm_projector</TITLE>\n\n<BODY>### Question\n\nwhen i run cmd below, got size mismatch.\r\nCUDA_VISIBLE_DEVICES=1 python -m llava.serve.cli --model-path liuhaotian/llava-v1.5-13b-lora --model-base liuhaotian/**vicuna-13b-v1.5** --image-file ./images/view.jpg --load-4bit\r\n\r\nraceback (most recent call last):\r\n  File \"/apdcephfs/share_1157259/users/tttaozhang/tools/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/apdcephfs/share_1157259/users/tttaozhang/tools/anaconda3/envs/llava/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/apdcephfs/private_tttaozhang/llm/LLaVA/llava/serve/cli_ttao.py\", line 132, in <module>\r\n    main(args)\r\n  File \"/apdcephfs/private_tttaozhang/llm/LLaVA/llava/serve/cli_ttao.py\", line 33, in main\r\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\r\n  File \"/apdcephfs/private_tttaozhang/llm/LLaVA/llava/model/builder.py\", line 72, in load_pretrained_model\r\n    model.load_state_dict(non_lora_trainables, strict=False)\r\n  File \"/apdcephfs/share_1157259/users/tttaozhang/tools/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2041, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for LlavaLlamaForCausalLM:\r\n        size mismatch for model.mm_projector.0.weight: copying a param with shape torch.Size([5120, 1024]) from checkpoint, the shape in current model is torch.Size([2621440, 1]).\r\n        size mismatch for model.mm_projector.2.weight: copying a param with shape torch.Size([5120, 5120]) from checkpoint, the shape in current model is torch.Size([13107200, 1]).</BODY>\n\n<COMMENTS>\n<Comment by TonyUSTC at 2023-11-03T02:52:16Z>\nremoved '--load-4bit', the code runs fine. However, how can I modify the code when using 4-bit quantization with lora weight?\n</Comment>\n<Comment by LumenYoung at 2023-11-03T10:44:08Z>\nI also have the same problem. I would like to believe that we didn't find the right model_base. But I can't find the instruction on the right one other than the information from the technical report.\n</Comment>\n<Comment by haotian-liu at 2023-11-05T00:58:27Z>\n> removed '--load-4bit', the code runs fine. However, how can I modify the code when using 4-bit quantization with lora weight?\r\n\r\nYou can first create merged lora weights, then load with 4bit: https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md#create-merged-checkpoints\r\n\r\nIt seems that QLora does not support this either: https://github.com/artidoro/qlora/blob/main/examples/guanaco_7B_demo_colab.ipynb\n</Comment>\n<Comment by LumenYoung at 2023-11-05T09:52:03Z>\n> > removed '--load-4bit', the code runs fine. However, how can I modify the code when using 4-bit quantization with lora weight?\r\n> \r\n> You can first create merged lora weights, then load with 4bit: https://github.com/haotian-liu/LLaVA/blob/main/docs/LoRA.md#create-merged-checkpoints\r\n> \r\n> It seems that QLora does not support this either: https://github.com/artidoro/qlora/blob/main/examples/guanaco_7B_demo_colab.ipynb\r\n\r\nHello Haotian, Thanks for the reply. But I would like to confirm which  is the base model of liuhaotian/llava-v1.5-13b-lora? Because there is no clear documentation on this. I could also open a PR for the documentation if you can provide a way for me to validate the model_base of lora models manually.\n</Comment>\n<Comment by haotian-liu at 2023-11-05T11:07:18Z>\nHi @LumenYoung The base model is Vicuna v1.5.\n</Comment>\n<Comment by LumenYoung at 2023-11-17T13:27:58Z>\n> Hi @LumenYoung The base model is Vicuna v1.5.\r\n\r\nThanks for the reply @haotian-liu , but the liuhaotian/vicuna-13b-v1.5 seems not existing. I checked the huggingface and the only similar one is lmsys/vicuna-13b-v1.5 or liuhaotian/llava-v1.5-mlp2x-336px-pretrain-vicuna-13b-v1.5. How exactly is the model base defined? \r\n\r\nApparently there is no liuhaotian/vicuna-13b-v1.5 at your huggingface repo right now. I wonder how @TonyUSTC  managed to run with the command he provides.\n</Comment>\n<Comment by LumenYoung at 2023-11-17T17:47:32Z>\nAs a follow up, the model base is lmsys/vicuna-13b-v1.5, should anyone was uncertain about the it.\n</Comment>\n<Comment by curiousNick1 at 2023-12-20T08:40:20Z>\n@haotian-liu Would you please check my problem when loading merged-lora checkpoint? It seems that the merged checkpoint has the same format as Vicuna-v1.5-7b and it has no information about the vision tower as well as the mm-projector. After I add configs about Vision tower and projector(non_lora_trainable.bin) to the config file, there would be an error saying 'ValueError:the weight is on the meta device, we need a 'value' to put in 0 ' when launching model worker.\r\nBut the code could run with the original llava projector(mm_projector.bin), I wonder if these two projector files has some differences?\r\nHow could I load the merged-lora checkpoint as well as the finetuned projector?\n</Comment>\n<Comment by zhyhome at 2024-03-11T12:09:13Z>\n****\r\n\r\n> @haotian-liu Would you please check my problem when loading merged-lora checkpoint? It seems that the merged checkpoint has the same format as Vicuna-v1.5-7b and it has no information about the vision tower as well as the mm-projector. After I add configs about Vision tower and projector(non_lora_trainable.bin) to the config file, there would be an error saying 'ValueError:the weight is on the meta device, we need a 'value' to put in 0 ' when launching model worker. But the code could run with the original llava projector(mm_projector.bin), I wonder if these two projector files has some differences? How could I load the merged-lora checkpoint as well as the finetuned projector?\r\nHow did you solve this problem?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 743,
    "state": "open",
    "created_by": "honoyomu",
    "created_at": "2023-11-02T23:05:42Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/743</URL>\n\n<TITLE>Got stuck on the gradio_web step on Windows</TITLE>\n\n<BODY>### Question\n\nHi there, I was following along the instruction of running LLaVA demo on Windows, and the link doesn't work. Could anyone help me with that? Really appreciate it!\r\nController: ![image](https://github.com/haotian-liu/LLaVA/assets/50334568/79a3c32f-e5a3-42e9-9d86-8505f926b4c1)\r\n\r\nWeb Server: \r\n![image](https://github.com/haotian-liu/LLaVA/assets/50334568/98137b2f-4276-4ff9-a776-102d3d1c20d7)\r\n\r\nDid I miss any important steps?</BODY>\n\n<COMMENTS>\n<Comment by CoreyJiang63 at 2023-11-03T07:32:11Z>\nSame problem!\n</Comment>\n<Comment by underworld02 at 2023-11-14T09:38:53Z>\n![屏幕截图 2023-11-14 165456](https://github.com/haotian-liu/LLaVA/assets/58507898/0bddfa5b-8075-4452-b060-dcb3747b8d50)\n</Comment>\n<Comment by FurkanGozukara at 2023-11-25T15:23:22Z>\nI made it work on Windows\r\n\r\nIncluding 4bit and 8bit inference as well\r\n\r\nHopefully it will be shared on my youtube channel with scripts https://www.youtube.com/SECourses\r\n\r\nIt will also have batch caption as well\r\n\r\nhttps://www.linkedin.com/posts/furkangozukara_alhamdulillah-finally-made-the-llava-batch-activity-7134191330854846464-MBo5?utm_source=share&utm_medium=member_desktop\n</Comment>\n<Comment by NQD44 at 2024-01-17T13:22:18Z>\n一样的问题\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 742,
    "state": "open",
    "created_by": "dhifafaz",
    "created_at": "2023-11-02T14:41:41Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/742</URL>\n\n<TITLE>I can't find my config.json file, and Non Lora Trainable</TITLE>\n\n<BODY>### Describe the issue\n\nIssue:\r\nI can't find my config.json file, and Non Lora Trainable\r\n\r\nCommand:\r\nThis is my command on training the llava model using lora on a specific task\r\n![image](https://github.com/haotian-liu/LLaVA/assets/54610871/fa6d6933-603d-4c8a-ba17-3b07b5006b99)\r\n\r\nScreenshots:\r\n![image](https://github.com/haotian-liu/LLaVA/assets/54610871/b3dc6df3-7fc0-4956-89c3-e9e422974b44)</BODY>\n\n<COMMENTS>\n<Comment by wuwu-C at 2024-04-20T13:10:22Z>\nHave you tried to solve this problem?\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 741,
    "state": "closed",
    "created_by": "pbenaim",
    "created_at": "2023-11-02T12:55:43Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/741</URL>\n\n<TITLE>[Feature request] Ouaaa !!! PhotoPrism need it !!! API need it too !!!</TITLE>\n\n<BODY>### feature\n\nHello at all.\r\n\r\nLLava is verally amazing, the demo work well on my RTX3080 ( 12 GB) with --load-4bit option ( https://github.com/haotian-liu/LLaVA#cli-inference )\r\n\r\nThanks a lot to Haotian Liu, for your amazing work !!!! I love It !!!\r\n\r\nSo...\r\nI want use it, for PhotoPrism ( https://www.photoprism.app :+1:  if you don't know is here : https://www.photoprism.app/) \r\n\r\nI need ( hope :+1: )  use LLaVA to find all keywords for my private pictures, with other treads of course, I'm not affraid to do that with another \"custom app\" to modify EXIF and describe it with like \"keys Words\"\r\n\r\nAnd after that ( LLava worked ) I can rescan all my pictures to add to describes will be avaible in PhotoPrism :+1: \r\n\r\nFor that, We need ( me of course, but other all want too ) , some API call to have json result.\r\n\r\nThis example call is not enough :-1: \r\n\r\npython -m llava.serve.cli \\\r\n    --model-path liuhaotian/llava-v1.5-7b \\\r\n    --image-file $1 \\\r\n    --load-4bit\r\n\r\n$1 is my image of course....\r\n\r\nSo we ( me ) need more arguments like, User \"Ask\" and \"path of json result\" and after we ( me ) can use it it... With call exit of course...\r\n\r\nMaybe, if I not clear, don't hesitate to ask me my recflection...\r\n\r\nI hope that many user \"like\" my post if we have the same wich...</BODY>\n\n<COMMENTS>\n<Comment by pbenaim at 2023-11-03T09:42:43Z>\nFind all I need here +1: [https://github.com/haotian-liu/LLaVA/issues/540](url)\r\n\r\ntcli.py must be in ../LLaVA/llava/serve\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 740,
    "state": "open",
    "created_by": "TonyUSTC",
    "created_at": "2023-11-02T11:30:44Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/740</URL>\n\n<TITLE>[Question] model_base：llava-v1.5-13b or vicuna-13b-v1.5?</TITLE>\n\n<BODY>### Question\n\nWhen using Lora weights for inference, should the model_base be chosen as llava-v1.5-13b or vicuna-13b-v1.5? What are the differences between them?</BODY>\n\n<COMMENTS>\n<Comment by haotian-liu at 2023-11-05T05:16:14Z>\nHi, you can think of LoRA as base model weights + finetuning. The model base should be the base you finetuned from. There are two cases.\r\n\r\n1. You finetune from Vicuna with LoRA on the LLaVA-v1.5-Mix-665K dataset -- you want to reproduce LLaVA-v1.5. Then the model base should be Vicuna.\r\n2. You finetune from LLaVA-v1.5 with LoRA on some custom datasets -- you want to customize LLaVA-v1.5 on your own dataset. Then the model base should be LLaVA.\n</Comment>\n<Comment by cherry956 at 2024-01-29T12:54:53Z>\nI am loading the weights of the llava-v1.5-7b from huggingface. Then store them in a folder and load them into LLava code. I also have my own dataset in the same as format, now I want to use it to finetune LLava-v1.5-7b with lora. if I need to run the finetune_task_lora.sh?\r\n![屏幕截图 2024-01-29 205245](https://github.com/haotian-liu/LLaVA/assets/144820412/44bd485b-3f0e-4a03-bf72-19a0a1938d60)\r\nHow should I modify the parameters?Thanks!!\n</Comment>\n<Comment by cherry956 at 2024-01-29T13:01:49Z>\nHere is the weight I load from huggingface\r\n![屏幕截图 2024-01-29 210050](https://github.com/haotian-liu/LLaVA/assets/144820412/f40c28af-3433-4953-9153-4ed3cc9cbbb2)\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 739,
    "state": "closed",
    "created_by": "ricard-inho",
    "created_at": "2023-11-02T10:54:04Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/739</URL>\n\n<TITLE>Exits with return code = -7 [Usage]</TITLE>\n\n<BODY>Issue: I try to run the `scripts/v1_5/fintetune_task.sh` on a 4xA100 40G and I get CUDA out of memory\r\n\r\nLog: \r\n```\r\nFile \"/opt/conda/lib/python3.11/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/transformers/trainer.py\", line 1656, in _inner_training_loop\r\n    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\r\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/accelerate/accelerator.py\", line 1198, in prepare\r\n    result = self._prepare_deepspeed(*args)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/accelerate/accelerator.py\", line 1537, in _prepare_deepspeed\r\n    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\r\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/deepspeed/__init__.py\", line 165, in initialize\r\n    engine = DeepSpeedEngine(args=args,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 309, in __init__\r\n    self._configure_optimizer(optimizer, model_parameters)\r\n  File \"/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 1184, in _configure_optimizer\r\n    self.optimizer = self._configure_zero_optimizer(basic_optimizer)\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 1474, in _configure_zero_optimizer\r\n    optimizer = DeepSpeedZeroOptimizer_Stage3(\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/stage3.py\", line 300, in __init__\r\n    self._setup_for_real_optimizer()\r\n  File \"/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/stage3.py\", line 382, in _setup_for_real_optimizer\r\n    self.initialize_optimizer_states()\r\n  File \"/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/stage3.py\", line 892, in initialize_optimizer_states    self._optimizer_step(i)\r\n  File \"/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/stage3.py\", line 817, in _optimizer_step\r\n    self.optimizer.step()\r\n  File \"/opt/conda/lib/python3.11/site-packages/torch/optim/lr_scheduler.py\", line 69, in wrapper\r\n    return wrapped(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 280, in wrapper\r\n    out = func(*args, **kwargs)\r\n          ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 33, in _use_grad\r\n    ret = func(self, *args, **kwargs)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/torch/optim/adamw.py\", line 171, in step\r\n    adamw(\r\n  File \"/opt/conda/lib/python3.11/site-packages/torch/optim/adamw.py\", line 321, in adamw\r\n    func(\r\n  File \"/opt/conda/lib/python3.11/site-packages/torch/optim/adamw.py\", line 564, in _multi_tensor_adamw\r\n    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.54 GiB (GPU 0; 39.41 GiB total capacity; 25.94 GiB already allocated; 1.37 GiB free; 31.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n[2023-11-02 18:47:58,884] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 14007\r\n[2023-11-02 18:47:58,885] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 14008\r\n[2023-11-02 18:47:59,501] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 14009\r\n[2023-11-02 18:48:00,156] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 14010\r\n[2023-11-02 18:48:00,931] [ERROR] [launch.py:321:sigkill_handler] ['/opt/conda/bin/python', '-u', 'custom_train.py', '--local_rank=3', '--deepspeed', './scripts/zero3.json'] exits with return code = 1\r\n```\r\n\r\nwhen I run the same command on 4xA100 80G machine I get the following error:\r\nLog: \r\n```\r\n[2023-11-02 19:48:06,677] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-11-02 19:48:06,677] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-11-02 19:48:06,846] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-11-02 19:48:06,846] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-11-02 19:48:06,846] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl[2023-11-02 19:48:06,909] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-11-02 19:48:06,909] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-11-02 19:48:06,965] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\r\n[2023-11-02 19:48:06,965] [INFO] [comm.py:594:init_distributed] cdb=None\r\n[2023-11-02 19:48:10,452] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1329\r\n[2023-11-02 19:48:10,452] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1330[2023-11-02 19:48:10,454] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1331\r\n[2023-11-02 19:48:10,455] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1332[2023-11-02 19:48:10,508] [ERROR] [launch.py:321:sigkill_handler] ['/opt/conda/bin/python', '-u', 'custom_train.py', '--local_rank=3', '--deepspeed', '/workspace/LLaVA/scripts/zero_offload.json'] exits with return code = -7\r\n```\r\n\r\nThis last error: `exits with return code = -7` doesn't give much information on what is wrong. Where or how could i check more about this error?\r\n\r\nThank you!</BODY>\n\n<COMMENTS>\n<Comment by ricard-inho at 2023-11-03T00:45:58Z>\nFor the second machine I found the solution [here](https://github.com/microsoft/DeepSpeed/issues/2897#issuecomment-1535798738). `shm_size` on docker needs to be bigger.\n</Comment>\n</COMMENTS>"
  },
  {
    "issue_number": 738,
    "state": "closed",
    "created_by": "wyd0817",
    "created_at": "2023-11-02T06:46:01Z",
    "labels": [],
    "text": "<URL>:https://github.com/haotian-liu/LLaVA/issues/738</URL>\n\n<TITLE>NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.</TITLE>\n\n<BODY>I am running docker on windows 11, running ubuntu22 inside docker.\r\n``` bash\r\npython -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 60000 --worker http://localhost:60000 --model-path /root/src/LLaVA/models/models--liuhaotian--llava-v1.5-7b/snapshots/12e054b30e8e061f423c7264bc97d4248232e965/\r\n``` \r\nI got the following error running llava with the above command\r\n\r\n``` bash\r\nroot@bd0f0ba1e85e:~# python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 60000 --worker http://localhost:60000 --model-path /root/src/LLaVA/models/models--liuhaotian--llava-v1.5-7b/snapshots/12e054b30e8e061f423c7264bc97d4248232e965/\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\", line 187, in _run_module_as_main\r\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\r\n  File \"/usr/lib/python3.10/runpy.py\", line 110, in _get_module_details\r\n    __import__(pkg_name)\r\n  File \"/root/src/LLaVA/llava/__init__.py\", line 1, in <module>\r\n    from .model import LlavaLlamaForCausalLM\r\n  File \"/root/src/LLaVA/llava/model/__init__.py\", line 1, in <module>\r\n    from .language_model.llava_llama import LlavaLlamaForCausalLM, LlavaConfig\r\n  File \"/root/src/LLaVA/llava/model/language_model/llava_llama.py\", line 22, in <module>\r\n    from transformers import AutoConfig, AutoModelForCausalLM, \\\r\nImportError: cannot import name 'LlamaConfig' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)\r\n```\r\nHere's my python and cuda version information:\r\n``` bash\r\nroot@bd0f0ba1e85e:~# python -V\r\nPython 3.10.12\r\nroot@bd0f0ba1e85e:~# nvcc -V\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2022 NVIDIA Corporation\r\nBuilt on Wed_Jun__8_16:49:14_PDT_2022\r\nCuda compilation tools, release 11.7, V11.7.99\r\nBuild cuda_11.7.r11.7/compiler.31442593_0\r\n``` \r\nHere's my pip list output:\r\n``` bash\r\n pip list\r\nPackage                    Version       Editable project location\r\n-------------------------- ------------- -------------------------\r\nabsl-py                    2.0.0\r\naccelerate                 0.21.0\r\naiofiles                   23.2.1\r\naiohttp                    3.8.6\r\naiosignal                  1.3.1\r\nale-py                     0.8.1\r\naltair                     5.1.2\r\nanyio                      3.7.1\r\nappdirs                    1.4.4\r\nasync-timeout              4.0.3\r\nattrs                      23.1.0\r\nAutoROM                    0.6.1\r\nAutoROM.accept-rom-license 0.6.1\r\nbitsandbytes               0.41.0\r\nblinker                    1.4\r\ncachetools                 5.3.2\r\ncertifi                    2023.7.22\r\ncharset-normalizer         3.3.1\r\nclick                      8.1.7\r\ncloudpickle                3.0.0\r\ncmake                      3.25.0\r\ncolorful                   0.5.5\r\ncontourpy                  1.1.1\r\ncryptography               3.4.8\r\ncycler                     0.12.1\r\ndbus-python                1.2.18\r\ndeepspeed                  0.9.5\r\ndistro                     1.7.0\r\ndocker-pycreds             0.4.0\r\neinops                     0.6.1\r\neinops-exts                0.0.4\r\nexceptiongroup             1.1.3\r\nFarama-Notifications       0.0.4\r\nfastapi                    0.104.0\r\nffmpy                      0.3.1\r\nfilelock                   3.13.0\r\nflash-attn                 2.3.3\r\nfonttools                  4.43.1\r\nfrozenlist                 1.4.0\r\nfsspec                     2023.10.0\r\ngitdb                      4.0.11\r\nGitPython                  3.1.40\r\ngoogle-auth                2.23.3\r\ngoogle-auth-oauthlib       1.1.0\r\ngradio                     3.35.2\r\ngradio_client              0.2.9\r\ngrpcio                     1.59.0\r\ngym                        0.26.2\r\ngym-notices                0.0.8\r\ngymnasium                  0.29.1\r\nh11                        0.14.0\r\nhjson                      3.1.0\r\nhttpcore                   0.17.3\r\nhttplib2                   0.20.2\r\nhttpx                      0.24.0\r\nhuggingface-hub            0.18.0\r\nidna                       3.4\r\nimportlib-metadata         4.6.4\r\nimportlib-resources        6.1.0\r\njapanize-matplotlib        1.1.3\r\njeepney                    0.7.1\r\nJinja2                     3.1.2\r\njoblib                     1.3.2\r\njsonschema                 4.19.1\r\njsonschema-specifications  2023.7.1\r\nkeyring                    23.5.0\r\nkiwisolver                 1.4.5\r\nlaunchpadlib               1.10.16\r\nlazr.restfulclient         0.14.4\r\nlazr.uri                   1.0.6\r\nlinkify-it-py              2.0.2\r\nlit                        15.0.7\r\nllava                      1.1.3         /root/src/LLaVA\r\nMarkdown                   3.5\r\nmarkdown-it-py             2.2.0\r\nmarkdown2                  2.4.10\r\nMarkupSafe                 2.1.3\r\nmatplotlib                 3.8.0\r\nmdit-py-plugins            0.3.3\r\nmdurl                      0.1.2\r\nmore-itertools             8.10.0\r\nmpmath                     1.3.0\r\nmultidict                  6.0.4\r\nnetworkx                   3.2\r\nninja                      1.11.1.1\r\nnumpy                      1.26.1\r\nnvidia-cublas-cu12         12.1.3.1\r\nnvidia-cuda-cupti-cu12     12.1.105\r\nnvidia-cuda-nvrtc-cu12     12.1.105\r\nnvidia-cuda-runtime-cu12   12.1.105\r\nnvidia-cudnn-cu12          8.9.2.26\r\nnvidia-cufft-cu12          11.0.2.54\r\nnvidia-curand-cu12         10.3.2.106\r\nnvidia-cusolver-cu12       11.4.5.107\r\nnvidia-cusparse-cu12       12.1.0.106\r\nnvidia-nccl-cu12           2.18.1\r\nnvidia-nvjitlink-cu12      12.3.52\r\nnvidia-nvtx-cu12           12.1.105\r\noauthlib                   3.2.2\r\nopencv-python-headless     4.8.1.78\r\norjson                     3.9.10\r\npackaging                  23.2\r\npandas                     2.1.2\r\npathtools                  0.1.2\r\npeft                       0.4.0\r\nPillow                     10.1.0\r\npip                        23.3.1\r\nprotobuf                   4.23.4\r\npsutil                     5.9.6\r\npy-cpuinfo                 9.0.0\r\npyasn1                     0.5.0\r\npyasn1-modules             0.3.0\r\npycairo                    1.20.1\r\npydantic                   1.10.13\r\npydub                      0.25.1\r\npygame                     2.5.2\r\nPygments                   2.16.1\r\nPyGObject                  3.42.1\r\nPyJWT                      2.3.0\r\npyparsing                  3.1.1\r\npython-apt                 2.4.0+ubuntu2\r\npython-dateutil            2.8.2\r\npython-multipart           0.0.6\r\npytz                       2023.3.post1\r\nPyYAML                     5.4.1\r\nreferencing                0.30.2\r\nregex                      2023.10.3\r\nrequests                   2.31.0\r\nrequests-oauthlib          1.3.1\r\nrich                       13.6.0\r\nrpds-py                    0.10.6\r\nrsa                        4.9\r\nsacremoses                 0.1.1\r\nsafetensors                0.4.0\r\nscikit-learn               1.2.2\r\nscikit-opt                 0.6.6\r\nscipy                      1.11.3\r\nSecretStorage              3.3.1\r\nsemantic-version           2.10.0\r\nsentencepiece              0.1.99\r\nsentry-sdk                 1.32.0\r\nsetproctitle               1.3.3\r\nsetuptools                 59.6.0\r\nShimmy                     1.1.0\r\nshortuuid                  1.0.11\r\nsix                        1.16.0\r\nsklearn                    0.0.post10\r\nsmmap                      5.0.1\r\nsniffio                    1.3.0\r\nstable-baselines3          2.1.0\r\nstarlette                  0.27.0\r\nsvgwrite                   1.4.3\r\nsympy                      1.12\r\nsystemd-python             234\r\ntensorboard                2.15.0\r\ntensorboard-data-server    0.7.2\r\nthreadpoolctl              3.2.0\r\ntimm                       0.6.13\r\ntokenizers                 0.12.1\r\ntoolz                      0.12.0\r\ntorch                      2.0.1+cu117\r\ntorchvision                0.15.2+cu117\r\ntqdm                       4.66.1\r\ntransformers               4.18.0\r\ntriton                     2.0.0\r\ntyping_extensions          4.8.0\r\ntzdata                     2023.3\r\nuc-micro-py                1.0.2\r\nurllib3                    2.0.7\r\nuvicorn                    0.23.2\r\nwadllib                    1.3.6\r\nwandb                      0.15.12\r\nwavedrom                   2.0.3.post3\r\nwebsockets                 12.0\r\nWerkzeug                   3.0.1\r\nwheel                      0.37.1\r\nyarl                       1.9.2\r\nzipp                       1.0.0\r\n```\r\n\r\nThank you in advance for your help!</BODY>\n\n<COMMENTS>\n<Comment by LYxiannv1124 at 2023-11-04T06:59:08Z>\nI also have the same problem.\n</Comment>\n<Comment by haotian-liu at 2023-11-04T19:17:59Z>\n`transformers               4.18.0`\r\n\r\nThe transformers version is off -- we need \"transfomers==4.31.0\"\n</Comment>\n<Comment by haotian-liu at 2023-11-05T00:16:14Z>\n`--model-path /root/src/LLaVA/models/models--liuhaotian--llava-v1.5-13b/snapshots/d64eb781be6876a5facc160ab1899281f59ef684/`\r\n\r\nDid you manually put your model here? If so, please rename it to llava-v1.5 (make sure it contains both v1.5 and llava)\n</Comment>\n<Comment by wyd0817 at 2023-11-05T00:16:36Z>\n> `transformers 4.18.0`\r\n> \r\n> The transformers version is off -- we need \"transfomers==4.31.0\"\r\n\r\nThanks for your help. I changed transferomers to 4.31.0, but still the same error. Sorry about the long error message.\r\n\r\n``` bash\r\nroot@8200cc374fa9:~# pip show transformers\r\nName: transformers\r\nVersion: 4.31.0\r\nSummary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\r\nHome-page: https://github.com/huggingface/transformers\r\nAuthor: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\r\nAuthor-email: transformers@huggingface.co\r\nLicense: Apache 2.0 License\r\nLocation: /usr/local/lib/python3.10/dist-packages\r\nRequires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\r\nRequired-by: llava, peft\r\nroot@8200cc374fa9:~# python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path /root/src/LLaVA/models/models--liuhaotian--llava-v1.5-13b/snapshots/d64eb781be6876a5facc160ab1899281f59ef684/\r\n2023-11-05 09:10:42 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='/root/src/LLaVA/models/models--liuhaotian--llava-v1.5-13b/snapshots/d64eb781be6876a5facc160ab1899281f59ef684/', model_base=None, model_name=None, device='cuda', multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=False)\r\n2023-11-05 09:10:42 | INFO | model_worker | Loading the model d64eb781be6876a5facc160ab1899281f59ef684 on worker 3f9a63 ...\r\nLoading checkpoint shards:   0%|   | 0/3 [00:00<?, ?it/s]\r\nLoading checkpoint shards:  33%|▎| 1/3 [00:22<00:44, 22.2\r\nLoading checkpoint shards:  67%|▋| 2/3 [00:45<00:22, 22.6\r\nLoading checkpoint shards: 100%|█| 3/3 [00:59<00:00, 18.6\r\nLoading checkpoint shards: 100%|█| 3/3 [00:59<00:00, 19.6\r\n2023-11-05 09:11:43 | ERROR | stderr | \r\n2023-11-05 09:11:43 | INFO | model_worker | Register to controller\r\n2023-11-05 09:11:43 | ERROR | stderr | INFO:     Started server process [12555]\r\n2023-11-05 09:11:43 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2023-11-05 09:11:43 | ERROR | stderr | INFO:     Application startup complete.\r\n2023-11-05 09:11:43 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)\r\n2023-11-05 09:11:51 | INFO | stdout | INFO:     127.0.0.1:39050 - \"POST /worker_get_status HTTP/1.1\" 200 OK\r\n2023-11-05 09:11:55 | INFO | model_worker | Send heart beat. Models: ['d64eb781be6876a5facc160ab1899281f59ef684']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n2023-11-05 09:11:55 | INFO | stdout | INFO:     127.0.0.1:39060 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [390,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [409,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n2023-11-05 09:11:56 | ERROR | stderr | Exception in thread Thread-3 (generate):\r\n2023-11-05 09:11:56 | ERROR | stderr | Traceback (most recent call last):\r\n2023-11-05 09:11:56 | ERROR | stderr |   File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\r\n2023-11-05 09:11:56 | ERROR | stderr |     self.run()\r\n2023-11-05 09:11:56 | ERROR | stderr |   File \"/usr/lib/python3.10/threading.py\", line 953, in run\r\n2023-11-05 09:11:56 | ERROR | stderr |     self._target(*self._args, **self._kwargs)\r\n2023-11-05 09:11:56 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n2023-11-05 09:11:56 | ERROR | stderr |     return func(*args, **kwargs)\r\n2023-11-05 09:11:56 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 1588, in generate\r\n2023-11-05 09:11:56 | ERROR | stderr |     return self.sample(\r\n2023-11-05 09:11:56 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 2642, in sample\r\n2023-11-05 09:11:56 | ERROR | stderr |     outputs = self(\r\n2023-11-05 09:11:56 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-11-05 09:11:56 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2023-11-05 09:11:56 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2023-11-05 09:11:56 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2023-11-05 09:11:56 | ERROR | stderr |   File \"/root/src/LLaVA/llava/model/language_model/llava_llama.py\", line 78, in forward\r\n2023-11-05 09:11:56 | ERROR | stderr |     outputs = self.model(\r\n2023-11-05 09:11:56 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-11-05 09:11:56 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2023-11-05 09:11:56 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 693, in forward\r\n2023-11-05 09:11:56 | ERROR | stderr |     layer_outputs = decoder_layer(\r\n2023-11-05 09:11:56 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-11-05 09:11:56 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2023-11-05 09:11:56 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2023-11-05 09:11:56 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2023-11-05 09:11:56 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 408, in forward\r\n2023-11-05 09:11:56 | ERROR | stderr |     hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n2023-11-05 09:11:56 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-11-05 09:11:56 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2023-11-05 09:11:56 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2023-11-05 09:11:56 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2023-11-05 09:11:56 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 305, in forward\r\n2023-11-05 09:11:56 | ERROR | stderr |     query_states = self.q_proj(hidden_states)\r\n2023-11-05 09:11:56 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n2023-11-05 09:11:56 | ERROR | stderr |     return forward_call(*args, **kwargs)\r\n2023-11-05 09:11:56 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\r\n2023-11-05 09:11:56 | ERROR | stderr |     output = old_forward(*args, **kwargs)\r\n2023-11-05 09:11:56 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\r\n2023-11-05 09:11:56 | ERROR | stderr |     return F.linear(input, self.weight, self.bias)\r\n2023-11-05 09:11:56 | ERROR | stderr | RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`\r\n2023-11-05 09:11:58 | INFO | model_worker | Send heart beat. Models: ['d64eb781be6876a5facc160ab1899281f59ef684']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n2023-11-05 09:12:11 | INFO | stdout | Caught Unknown Error\r\n2023-11-05 09:12:11 | INFO | model_worker | Send heart beat. Models: ['d64eb781be6876a5facc160ab1899281f59ef684']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n2023-11-05 09:12:13 | INFO | model_worker | Send heart beat. Models: ['d64eb781be6876a5facc160ab1899281f59ef684']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n^C2023-11-05 09:12:17 | ERROR | stderr | INFO:     Shutting down\r\n2023-11-05 09:12:17 | ERROR | stderr | INFO:     Waiting for application shutdown.\r\n2023-11-05 09:12:17 | ERROR | stderr | INFO:     Application shutdown complete.\r\n2023-11-05 09:12:17 | ERROR | stderr | INFO:     Finished server process [12555]\r\n2023-11-\r\n```\n</Comment>\n<Comment by wyd0817 at 2023-11-05T00:27:25Z>\n> `--model-path /root/src/LLaVA/models/models--liuhaotian--llava-v1.5-13b/snapshots/d64eb781be6876a5facc160ab1899281f59ef684/`\r\n> \r\n> Did you manually put your model here? If so, please rename it to llava-v1.5 (make sure it contains both v1.5 and llava)\r\n\r\nThis is my original directory structure\r\n``` bash\r\nroot@8200cc374fa9:~# cd /root/src/LLaVA/models/models--liuhaotian--llava-v1.5-13b/snapshots/\r\nroot@8200cc374fa9:~/src/LLaVA/models/models--liuhaotian--llava-v1.5-13b/snapshots# ls\r\nd64eb781be6876a5facc160ab1899281f59ef684\r\n```\r\n\r\nI changed the name to llava-v1.5.\r\n``` bash\r\nroot@8200cc374fa9:~/src/LLaVA/models/models--liuhaotian--llava-v1.5-13b/snapshots# tree\r\n.\r\n`-- llava-v1.5\r\n    |-- config.json -> ../../blobs/74d6ead6b92f34f32a10cc1d1301588dfa295fd7\r\n    |-- pytorch_model-00001-of-00003.bin -> ../../blobs/c64e0b62f8b154e7c8ceb1f95b39ae645d71070b6d00d76bff7b40f083c2d627\r\n    |-- pytorch_model-00002-of-00003.bin -> ../../blobs/13546a4b6616206edf83646d1caaf073a2d008dde4aa4b295a1ef4aef976b086\r\n    |-- pytorch_model-00003-of-00003.bin -> ../../blobs/0c5e0caccab8b461962252a0b75c8e80084d5e48f292ab65ff1a78c03a973fcd\r\n    |-- pytorch_model.bin.index.json -> ../../blobs/4d465e953a8a324474c318f5d319ddde575077ab\r\n    |-- special_tokens_map.json -> ../../blobs/14761dcf1466dc232bd41de9c21d4c617b15755e\r\n    |-- tokenizer.model -> ../../blobs/9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347\r\n    `-- tokenizer_config.json -> ../../blobs/740756b4bef305e27d0bb4d2e1a40dd8847797f7\r\n\r\n1 directory, 8 files\r\n```\r\n\r\nBut it does still have the same network error.\r\n\r\n``` bash\r\nroot@8200cc374fa9:~# python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path /root/src/LLaVA/models/models--liuhaotian--llava-v1.5-13b/snapshots/llava-v1.5                               \r\n2023-11-05 09:20:03 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='/root/src/LLaVA/models/models--liuhaotian--llava-v1.5-13b/snapshots/llava-v1.5', model_base=None, model_name=None, device='cuda', multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=False)\r\n2023-11-05 09:20:03 | INFO | model_worker | Loading the model llava-v1.5 on worker 4c794f ...\r\nLoading checkpoint shards:   0%|   | 0/3 [00:00<?, ?it/s]\r\nLoading checkpoint shards:  33%|▎| 1/3 [00:21<00:43, 21.9\r\nLoading checkpoint shards:  67%|▋| 2/3 [00:45<00:22, 22.8\r\nLoading checkpoint shards: 100%|█| 3/3 [00:59<00:00, 18.9\r\nLoading checkpoint shards: 100%|█| 3/3 [00:59<00:00, 19.9\r\n2023-11-05 09:21:04 | ERROR | stderr | \r\n(…)36/resolve/main/preprocessor_config.json:   0%| | 0.00\r\n(…)36/resolve/main/preprocessor_config.json: 100%|█| 316/\r\n2023-11-05 09:21:05 | ERROR | stderr | \r\npytorch_model.bin:   0%|     | 0.00/1.71G [00:00<?, ?B/s]\r\npytorch_model.bin:   1%| | 10.5M/1.71G [00:00<00:35, 47.9\r\npytorch_model.bin:   1%| | 21.0M/1.71G [00:00<00:35, 47.9\r\npytorch_model.bin:   2%| | 31.5M/1.71G [00:00<00:33, 49.9\r\npytorch_model.bin:   2%| | 41.9M/1.71G [00:00<00:34, 49.1\r\npytorch_model.bin:   3%| | 52.4M/1.71G [00:01<00:33, 49.4\r\n...\r\npytorch_model.bin:  99%|▉| 1.69G/1.71G [00:33<00:00, 54.7\r\npytorch_model.bin:  99%|▉| 1.70G/1.71G [00:33<00:00, 56.0\r\npytorch_model.bin: 100%|▉| 1.71G/1.71G [00:33<00:00, 55.1\r\npytorch_model.bin: 100%|█| 1.71G/1.71G [00:33<00:00, 50.9\r\n2023-11-05 09:21:39 | ERROR | stderr | \r\n2023-11-05 09:21:42 | INFO | model_worker | Register to controller\r\n2023-11-05 09:21:43 | ERROR | stderr | INFO:     Started server process [16031]\r\n2023-11-05 09:21:43 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2023-11-05 09:21:43 | ERROR | stderr | INFO:     Application startup complete.\r\n2023-11-05 09:21:43 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)\r\n2023-11-05 09:21:49 | INFO | stdout | INFO:     127.0.0.1:44838 - \"POST /worker_get_status HTTP/1.1\" 200 OK\r\n2023-11-05 09:21:53 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n2023-11-05 09:21:53 | INFO | stdout | INFO:     127.0.0.1:44852 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\n2023-11-05 09:21:57 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n2023-11-05 09:22:04 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n2023-11-05 09:22:13 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n2023-11-05 09:22:28 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n2023-11-05 09:22:43 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n2023-11-05 09:22:58 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n2023-11-05 09:23:13 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n2023-11-05 09:23:28 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n2023-11-05 09:23:43 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n2023-11-05 09:23:58 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n2023-11-05 09:24:13 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n2023-11-05 09:24:28 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n2023-11-05 09:24:43 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n2023-11-05 09:24:58 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n2023-11-05 09:25:13 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n2023-11-05 09:25:28 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n```\n</Comment>\n<Comment by haotian-liu at 2023-11-05T00:47:48Z>\n> 2023-11-05 09:21:39 | ERROR | stderr | \r\n> 2023-11-05 09:21:42 | INFO | model_worker | Register to controller\r\n> 2023-11-05 09:21:43 | ERROR | stderr | INFO:     Started server process [16031]\r\n> 2023-11-05 09:21:43 | ERROR | stderr | INFO:     Waiting for application startup.\r\n> 2023-11-05 09:21:43 | ERROR | stderr | INFO:     Application startup complete.\r\n> 2023-11-05 09:21:43 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)\r\n\r\n\r\nI don't know why it prints to the stderr, but this is not the error. Can you print out which terminal outputs the network error?\n</Comment>\n<Comment by wyd0817 at 2023-11-05T00:52:35Z>\n> > 2023-11-05 09:21:39 | ERROR | stderr |\r\n> > 2023-11-05 09:21:42 | INFO | model_worker | Register to controller\r\n> > 2023-11-05 09:21:43 | ERROR | stderr | INFO:     Started server process [16031]\r\n> > 2023-11-05 09:21:43 | ERROR | stderr | INFO:     Waiting for application startup.\r\n> > 2023-11-05 09:21:43 | ERROR | stderr | INFO:     Application startup complete.\r\n> > 2023-11-05 09:21:43 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)\r\n> \r\n> I don't know why it prints to the stderr, but this is not the error. Can you print out which terminal outputs the network error?\r\n\r\nSorry for the misunderstanding. In fact, the error occurred in the UI browser interface.\r\n<img width=\"743\" alt=\"image\" src=\"https://github.com/haotian-liu/LLaVA/assets/25527726/1cd3515a-9abb-402a-b2a4-af13c26d24e8\">\n</Comment>\n<Comment by haotian-liu at 2023-11-05T00:55:16Z>\n> 2023-11-05 09:21:53 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n> 2023-11-05 09:21:53 | INFO | stdout | INFO:     127.0.0.1:44852 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\n> 2023-11-05 09:21:57 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n> 2023-11-05 09:22:04 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n\r\nI noticed that semaphore becomes 4 and then returns to 5, which suggests that the request comes through successfully. It is possible that the first request takes some time to process as it needs to initialize something on the GPU.\r\n\r\nCan you try to press `regenerate` and see if it is always like this? And if so, can you see if there are other errors printed out in the terminal where you launch gradio?\r\n\r\nAlso, if you see the semaphore becomes 5->4, do you see gpu's utilization goes up (from idle to computing)?\n</Comment>\n<Comment by wyd0817 at 2023-11-05T01:43:18Z>\n> > 2023-11-05 09:21:53 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n> > 2023-11-05 09:21:53 | INFO | stdout | INFO:     127.0.0.1:44852 - \"POST /worker_generate_stream HTTP/1.1\" 200 OK\r\n> > 2023-11-05 09:21:57 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1\r\n> > 2023-11-05 09:22:04 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1\r\n> \r\n> I noticed that semaphore becomes 4 and then returns to 5, which suggests that the request comes through successfully. It is possible that the first request takes some time to process as it needs to initialize something on the GPU.\r\n> \r\n> Can you try to press `regenerate` and see if it is always like this? And if so, can you see if there are other errors printed out in the terminal where you launch gradio?\r\n> \r\n> Also, if you see the semaphore becomes 5->4, do you see gpu's utilization goes up (from idle to computing)?\r\n\r\nI've clicked regenerate three times and always get the same error. (Plus, my GPU is a 4090.)\r\n\r\nThis is the output of the terminal where gradio is started.\r\n\r\n```\r\nroot@8200cc374fa9:~# python -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload\r\n2023-11-05 08:18:59 | INFO | gradio_web_server | args: Namespace(host='0.0.0.0', port=None, controller_url='http://localhost:10000', concurrency_count=10, model_list_mode='reload', share=False, moderate=False, embed=False)\r\n2023-11-05 08:18:59 | INFO | gradio_web_server | Models: []\r\n2023-11-05 08:18:59 | INFO | gradio_web_server | Namespace(host='0.0.0.0', port=None, controller_url='http://localhost:10000', concurrency_count=10, model_list_mode='reload', share=False, moderate=False, embed=False)\r\n2023-11-05 08:19:00 | INFO | stdout | Running on local URL:  http://0.0.0.0:7860\r\n2023-11-05 08:19:00 | INFO | stdout | \r\n2023-11-05 08:19:00 | INFO | stdout | To create a public link, set `share=True` in `launch()`.\r\n2023-11-05 08:19:23 | INFO | gradio_web_server | load_demo. ip: 127.0.0.1\r\n2023-11-05 08:19:23 | INFO | gradio_web_server | Models: []\r\n2023-11-05 08:19:46 | INFO | gradio_web_server | load_demo. ip: 127.0.0.1\r\n2023-11-05 08:19:46 | INFO | gradio_web_server | Models: []\r\n2023-11-05 08:19:49 | INFO | gradio_web_server | load_demo. ip: 127.0.0.1\r\n2023-11-05 08:19:49 | INFO | gradio_web_server | Models: []\r\n2023-11-05 08:19:59 | INFO | gradio_web_server | load_demo. ip: 127.0.0.1\r\n2023-11-05 08:19:59 | INFO | gradio_web_server | Models: []\r\n2023-11-05 08:20:00 | INFO | gradio_web_server | load_demo. ip: 127.0.0.1\r\n2023-11-05 08:20:00 | INFO | gradio_web_server | Models: []\r\n2023-11-05 08:20:00 | INFO | gradio_web_server | load_demo. ip: 127.0.0.1\r\n2023-11-05 08:20:00 | INFO | gradio_web_server | Models: []\r\n2023-11-05 08:20:04 | INFO | gradio_web_server | load_demo. ip: 127.0.0.1\r\n2023-11-05 08:20:04 | INFO | gradio_web_server | Models: []\r\n2023-11-05 08:20:36 | INFO | gradio_web_server | load_demo. ip: 127.0.0.1\r\n2023-11-05 08:20:36 | INFO | gradio_web_server | Models: ['d64eb781be6876a5facc160ab1899281f59ef684']\r\n2023-11-05 08:20:38 | INFO | gradio_web_server | add_text. ip: 127.0.0.1. len: 33\r\n2023-11-05 08:20:39 | INFO | gradio_web_server | http_bot. ip: 127.0.0.1\r\n2023-11-05 08:20:39 | INFO | gradio_web_server | model_name: d64eb781be6876a5facc160ab1899281f59ef684, worker_addr: http://localhost:40000\r\n2023-11-05 08:20:39 | INFO | gradio_web_server | ==== request ====\r\n{'model': 'd64eb781be6876a5facc160ab1899281f59ef684', 'prompt': \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\\nWhat is unusual about this image? ASSISTANT:\", 'temperature': 0.2, 'top_p': 0.7, 'max_new_tokens': 512, 'stop': '</s>', 'images': \"List of 1 images: ['b939abf2c4553ce07e642170aee3a3d7']\"}\r\n2023-11-05 08:30:29 | INFO | gradio_web_server | load_demo. ip: 127.0.0.1\r\n2023-11-05 08:30:29 | INFO | gradio_web_server | Models: ['d64eb781be6876a5facc160ab1899281f59ef684']\r\n2023-11-05 08:30:31 | INFO | gradio_web_server | add_text. ip: 127.0.0.1. len: 65\r\n2023-11-05 08:30:32 | INFO | gradio_web_server | http_bot. ip: 127.0.0.1\r\n2023-11-05 08:30:32 | INFO | gradio_web_server | model_name: d64eb781be6876a5facc160ab1899281f59ef684, worker_addr: http://localhost:40000\r\n2023-11-05 08:30:32 | INFO | gradio_web_server | ==== request ====\r\n{'model': 'd64eb781be6876a5facc160ab1899281f59ef684', 'prompt': \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\\nWhat are the things I should be cautious about when I visit here? ASSISTANT:\", 'temperature': 0.2, 'top_p': 0.7, 'max_new_tokens': 512, 'stop': '</s>', 'images': \"List of 1 images: ['52b64b78584b687fa01c14fbac944d6f']\"}\r\n2023-11-05 09:11:51 | INFO | gradio_web_server | load_demo. ip: 127.0.0.1\r\n2023-11-05 09:11:51 | INFO | gradio_web_server | Models: ['d64eb781be6876a5facc160ab1899281f59ef684']\r\n2023-11-05 09:11:55 | INFO | gradio_web_server | add_text. ip: 127.0.0.1. len: 33\r\n2023-11-05 09:11:55 | INFO | gradio_web_server | http_bot. ip: 127.0.0.1\r\n2023-11-05 09:11:55 | INFO | gradio_web_server | model_name: d64eb781be6876a5facc160ab1899281f59ef684, worker_addr: http://localhost:40000\r\n2023-11-05 09:11:55 | INFO | gradio_web_server | ==== request ====\r\n{'model': 'd64eb781be6876a5facc160ab1899281f59ef684', 'prompt': \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\\nWhat is unusual about this image? ASSISTANT:\", 'temperature': 0.2, 'top_p': 0.7, 'max_new_tokens': 512, 'stop': '</s>', 'images': \"List of 1 images: ['b939abf2c4553ce07e642170aee3a3d7']\"}\r\n2023-11-05 09:20:47 | INFO | gradio_web_server | load_demo. ip: 127.0.0.1\r\n2023-11-05 09:20:47 | INFO | gradio_web_server | Models: []\r\n2023-11-05 09:20:49 | INFO | gradio_web_server | load_demo. ip: 127.0.0.1\r\n2023-11-05 09:20:49 | INFO | gradio_web_server | Models: []\r\n2023-11-05 09:20:53 | INFO | gradio_web_server | load_demo. ip: 127.0.0.1\r\n2023-11-05 09:20:53 | INFO | gradio_web_server | Models: []\r\n2023-11-05 09:21:49 | INFO | gradio_web_server | load_demo. ip: 127.0.0.1\r\n2023-11-05 09:21:49 | INFO | gradio_web_server | Models: ['llava-v1.5']\r\n2023-11-05 09:21:52 | INFO | gradio_web_server | add_text. ip: 127.0.0.1. len: 33\r\n2023-11-05 09:21:53 | INFO | gradio_web_server | http_bot. ip: 127.0.0.1\r\n2023-11-05 09:21:53 | INFO | gradio_web_server | model_name: llava-v1.5, worker_addr: http://localhost:40000\r\n2023-11-05 09:21:53 | INFO | gradio_web_server | ==== request ====\r\n{'model': 'llava-v1.5', 'prompt': \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\\nWhat is unusual about this image? ASSISTANT:\", 'temperature': 0.2, 'top_p': 0.7, 'max_new_tokens': 512, 'stop': '</s>', 'images': \"List of 1 images: ['b939abf2c4553ce07e642170aee3a3d7']\"}\r\n```\n</Comment>\n<Comment by ethanyanjiali at 2023-11-06T17:22:50Z>\nFor those who are looking for a solution, this is the key step for me\r\nhttps://github.com/haotian-liu/LLaVA/issues/738#issuecomment-1793588776\r\n\r\nI haven't looked into the code yet but my guess is that \"v1.5\" and \"llava\" are used somewhere in the code to decide some model hyperparameters, so you really need to make sure your directory name has these two words, not just in the path but the last directory that contains all the checkpoints.\n</Comment>\n<Comment by wyd0817 at 2023-11-08T01:36:42Z>\nAfter **_updating the version of the transformer_** and **_changing the name of the model_**, although I still have the same network error, after I **_restarted the computer_** (after doing the two steps above), LLaVa works fine, thank you.\n</Comment>\n</COMMENTS>"
  }
]