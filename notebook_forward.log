current file path llava/train/train.py
def train()
original parser
 HfArgumentParser(prog='ipykernel_launcher.py', usage=None, description=None, formatter_class=<class 'argparse.ArgumentDefaultsHelpFormatter'>, conflict_handler='error', add_help=True)
model_args
 ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')
data_args
 DataArguments(data_path='/workspaces/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=False, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')
training_args
 TrainingArguments(
_n_gpu=0,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
bits=16,
cache_dir=None,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=2,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=False,
double_quant=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
freeze_mm_mlp_adapter=False,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
group_by_modality_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0/runs/Oct01_04-18-40_833b9cbd99be,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_alpha=16,
lora_bias=none,
lora_dropout=0.05,
lora_enable=False,
lora_r=64,
lora_weight_path=,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mm_projector_lr=None,
model_max_length=2048,
mp_parameters=,
mpt_attn_impl=triton,
no_cuda=False,
num_train_epochs=1,
optim=adamw_torch,
optim_args=None,
output_dir=./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
quant_type=nf4,
ray_scope=last,
remove_unused_columns=False,
report_to=[],
resume_from_checkpoint=None,
run_name=./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0,
save_on_each_node=False,
save_safetensors=False,
save_steps=1,
save_strategy=steps,
save_total_limit=1,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=False,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
local_rank
 0
compute_dtype
 torch.bfloat16
bnb_model_from_pretrained_args
 {}
【COND】 bits=16
【COND】 vision_tower=openai/clip-vit-large-patch14-336
【ENTER】if model_args.vision_tower is not None:
【COND】 mpt_in_model_name_or_path=False
【COND】 not_mpt_in_model_name_or_path={'mpt' not in model_args.model_name_or_path}
【ENTER】else of if 'mpt' in model_args.model_name_or_path:
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.__init__(self, config)
self
 <class '__main__.LlavaLlamaForCausalLM'>
config
 LlavaConfig {
  "_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llava_llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.31.0",
  "use_cache": true,
  "vocab_size": 32000
}

current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaModel.__init__(self, config: LlamaConfig)
self
 <class '__main__.LlavaLlamaModel'>
config
 LlavaConfig {
  "_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llava_llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.31.0",
  "use_cache": true,
  "vocab_size": 32000
}

current file path llava/model/llava_arch.py
LlavaMetaModel.__init__(self, config)
config
 LlavaConfig {
  "_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llava_llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.31.0",
  "use_cache": true,
  "vocab_size": 32000
}

【COND】 mm_vision_tower=False
self.model
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
)
self.pretraining_tp
 1
self.vocab_size
 32000
self.lm_head
 Linear(in_features=2048, out_features=32000, bias=False)
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0 and are newly initialized: ['model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
model defined as LlavaLlamaForCausalLM 
 LlavaLlamaForCausalLM(
  (model): LlavaLlamaModel(
    (embed_tokens): Embedding(32000, 2048, padding_idx=0)
    (layers): ModuleList(
      (0-21): 22 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (k_proj): Linear(in_features=2048, out_features=256, bias=False)
          (v_proj): Linear(in_features=2048, out_features=256, bias=False)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)
)
【EXIT】else of if 'mpt' in model_args.model_name_or_path:
【EXIT】if model_args.vision_tower is not None:
【COND】 freeze_backbone=False
【COND】 bits=16
【COND】 gradient_checkpointing=True
【ENTER】if training_args.gradient_checkpointing:
【COND】 has_enable_input_require_grads=True
【ENTER】if hasattr(model, 'enable_input_require_grads'):
【EXIT】if hasattr(model, 'enable_input_require_grads'):
【EXIT】if training_args.gradient_checkpointing:
【COND】 lora_enable=False
【COND】 mpt_in_model_name_or_path=False
【COND】 not_mpt_in_model_name_or_path={'mpt' not in model_args.model_name_or_path}
【ENTER】else of if 'mpt' in model_args.model_name_or_path:
tokenizer defined by AutoTokenizer.from_pretrained 
 LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False)}, clean_up_tokenization_spaces=False)
【EXIT】else of if 'mpt' in model_args.model_name_or_path:
【COND】 version=plain
【ENTER】else of if model_args.version == 'v0' and elif 'v0.5':
【COND】 version_in_conv_templates=True
【ENTER】if model_args.version in conversation_lib.conv_templates:
conversation_lib.default_conversation set to plain
【EXIT】if model_args.version in conversation_lib.conv_templates:
【EXIT】else of if model_args.version == 'v0' and elif 'v0.5':
【COND】 vision_tower=openai/clip-vit-large-patch14-336
【ENTER】if model_args.vision_tower is not None:
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class '__main__.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
)
current file path llava/model/llava_arch.py
def initialize_vision_modules(self, model_args, fsdp=None)
model_args
 ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')
fsdp
 []
vision_tower from model_args
 openai/clip-vit-large-patch14-336
mm_vision_select_layer from model_args
 -2
mm_vision_select_feature from model_args
 patch
pretrain_mm_mlp_adapter from model_args
 None
self.config.mm_vision_tower
 openai/clip-vit-large-patch14-336
current file path llava/model/llava_arch.py
def get_vision_tower(self)
vision_tower (raw)
 None
type(vision_tower)
 <class 'NoneType'>
【COND】 type_vision_tower_is_list=False
vision_tower (return)
 None
【COND】 self.get_vision_tower()
 None
current file path llava/model/llava_arch.py
def get_vision_tower(self)
vision_tower (raw)
 None
type(vision_tower)
 <class 'NoneType'>
【COND】 type_vision_tower_is_list=False
vision_tower (return)
 None
【COND】 get_vision_tower_is_None=True
current file path llava/model/llava_arch.py
def get_vision_tower(self)
vision_tower (raw)
 None
type(vision_tower)
 <class 'NoneType'>
【COND】 type_vision_tower_is_list=False
vision_tower (return)
 None
【ENTER】if self.get_vision_tower() is None:
[ENTER] self.get_vision_tower() is None
current file path llava/llava/model/multimodal_encoder/builder.py
def build_vision_tower(vision_tower_cfg, **kwargs)
vision_tower_cfg
 ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')
kwargs
 {}
vision_tower from vision_tower_cfg
 openai/clip-vit-large-patch14-336
is_absolute_path_exists
 False
【COND】 is_absolute_path_exists=False vision_tower=openai/clip-vit-large-patch14-336
【ENTER】if is_absolute_path_exists or vision_tower.startswith('openai') or vision_tower.startswith('laion') or 'ShareGPT4V' in vision_tower:
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.__init__(self, vision_tower, args, delay_load=False)
self
 <class '__main__.CLIPVisionTower'>
vision_tower
 openai/clip-vit-large-patch14-336
args
 ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')
delay_load
 False
self.is_loaded
 False
self.vision_tower_name
 openai/clip-vit-large-patch14-336
self.select_layer
 -2
self.select_feature
 patch
【COND】 delay_load=False
【ENTER】if not delay_load:
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.load_model(self)
self
 <class '__main__.CLIPVisionTower'>
self.vision_tower_name
 openai/clip-vit-large-patch14-336
self.image_processor
 CLIPImageProcessor {
  "crop_size": {
    "height": 336,
    "width": 336
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "CLIPFeatureExtractor",
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 336
  }
}

self.vision_tower
 CLIPVisionModel(
  (vision_model): CLIPVisionTransformer(
    (embeddings): CLIPVisionEmbeddings(
      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
      (position_embedding): Embedding(577, 1024)
    )
    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (encoder): CLIPEncoder(
      (layers): ModuleList(
        (0-23): 24 x CLIPEncoderLayer(
          (self_attn): CLIPAttention(
            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): CLIPMLP(
            (activation_fn): QuickGELUActivation()
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
)
self.is_loaded
 True
result (return)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
【EXIT】if is_absolute_path_exists or vision_tower.startswith('openai') or vision_tower.startswith('laion') or 'ShareGPT4V' in vision_tower:
vision_tower after build_vision_tower
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
【COND】 fsdp
 []
【COND】 fsdp_is_not_None=True len_fsdp=0
【COND】 else_fsdp_is_not_None_and_len_fsdp_gt_0=True
【ENTER】else of if fsdp is not None and len(fsdp) > 0:
self.vision_tower
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
【EXIT】else of if fsdp is not None and len(fsdp) > 0:
【EXIT】if self.get_vision_tower() is None:
self.config.use_mm_proj set to True
self.config.mm_projector_type
 mlp2x_gelu
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.hidden_size(self)
self
 <class '__main__.CLIPVisionTower'>
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.config(self)
self
 <class '__main__.CLIPVisionTower'>
self.is_loaded
 True
【COND】 is_loaded=True
【ENTER】if self.is_loaded:
result (return)
 <class 'transformers.models.clip.configuration_clip.CLIPVisionConfig'>
【EXIT】if self.is_loaded:
result (return)
 CLIPVisionConfig {
  "_name_or_path": "openai/clip-vit-large-patch14-336",
  "attention_dropout": 0.0,
  "dropout": 0.0,
  "hidden_act": "quick_gelu",
  "hidden_size": 1024,
  "image_size": 336,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "model_type": "clip_vision_model",
  "num_attention_heads": 16,
  "num_channels": 3,
  "num_hidden_layers": 24,
  "patch_size": 14,
  "projection_dim": 768,
  "transformers_version": "4.31.0"
}

result (return), self.config.hidden_size
 1024
self.config.mm_hidden_size
 1024
self.config.mm_vision_select_layer
 -2
self.config.mm_vision_select_feature
 patch
self.config.mm_patch_merge_type
 flat
【COND】 mm_projector_is_None=True
【ENTER】if getattr(self, 'mm_projector', None) is None:
current file path llava/llava/model/multimodal_projector/builder.py
def build_vision_projector(config, delay_load=False, **kwargs)
config
 LlavaConfig {
  "_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "mm_hidden_size": 1024,
  "mm_patch_merge_type": "flat",
  "mm_projector_type": "mlp2x_gelu",
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14-336",
  "model_type": "llava_llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.31.0",
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}

delay_load
 False
kwargs
 {}
projector_type from config
 mlp2x_gelu
【COND】 projector_type
 mlp2x_gelu
【COND】mlp_gelu_match
 <re.Match object; span=(0, 10), match='mlp2x_gelu'>
【ENTER】if mlp_gelu_match:
mlp_depth from mlp_gelu_match.group(1)
 2
modules after first Linear
 [Linear(in_features=1024, out_features=2048, bias=True)]
modules before Sequential
 [Linear(in_features=1024, out_features=2048, bias=True), GELU(approximate='none'), Linear(in_features=2048, out_features=2048, bias=True)]
result (return)
 Sequential(
  (0): Linear(in_features=1024, out_features=2048, bias=True)
  (1): GELU(approximate='none')
  (2): Linear(in_features=2048, out_features=2048, bias=True)
)
【EXIT】if mlp_gelu_match:
self.mm_projector after build_vision_projector
 Sequential(
  (0): Linear(in_features=1024, out_features=2048, bias=True)
  (1): GELU(approximate='none')
  (2): Linear(in_features=2048, out_features=2048, bias=True)
)
mm_patch_merge_type
 flat
【COND】 unpad_in_mm_patch_merge_type=False
【EXIT】if getattr(self, 'mm_projector', None) is None:
【COND】 pretrain_mm_mlp_adapter_is_not_None=False
current file path llava/model/llava_arch.py
class LlavaMetaForCausalLM(ABC).get_vision_tower(self)
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class '__main__.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=2048, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=2048, out_features=2048, bias=True)
  )
)
current file path llava/model/llava_arch.py
def get_vision_tower(self)
vision_tower (raw)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
type(vision_tower)
 <class '__main__.CLIPVisionTower'>
【COND】 type_vision_tower_is_list=False
vision_tower (return)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
LlavaMetaForCausalLM(ABC).get_vision_tower(self) result (return)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
【COND】 tune_mm_mlp_adapter=True
【ENTER】if model_args.tune_mm_mlp_adapter:
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class '__main__.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=2048, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=2048, out_features=2048, bias=True)
  )
)
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class '__main__.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=2048, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=2048, out_features=2048, bias=True)
  )
)
model.get_model().mm_projector.parameters() <generator object Module.parameters at 0x78fb79913290>
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class '__main__.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=2048, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=2048, out_features=2048, bias=True)
  )
)
model.get_model().mm_projector.parameters() <generator object Module.parameters at 0x78fb79913290>
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class '__main__.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=2048, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=2048, out_features=2048, bias=True)
  )
)
model.get_model().mm_projector.parameters() <generator object Module.parameters at 0x78fb79913290>
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class '__main__.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=2048, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=2048, out_features=2048, bias=True)
  )
)
model.get_model().mm_projector.parameters() <generator object Module.parameters at 0x78fb79913290>
【EXIT】if model_args.tune_mm_mlp_adapter:
【COND】 freeze_mm_mlp_adapter=False
【COND】 bits=16
model_args.mm_use_im_start_end False
training_args.mm_projector_lr None
training_args.use_im_start_end False
model_args.mm_use_im_patch_token False
current file path llava/model/llava_arch.py
def initialize_vision_tokenizer(self, model_args, tokenizer)
model_args
 ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')
tokenizer
 LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)
【COND】 mm_use_im_patch_token=False
【EXIT】if model_args.vision_tower is not None:
【COND】 bits=16
current file path llava/train/train.py
def make_supervised_data_module(tokenizer, data_args)
tokenizer
 <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>
data_args
 DataArguments(data_path='/workspaces/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')
current file path llava/train/train.py
def LazySupervisedDataset.__init__(self, data_path, tokenizer, data_args)
data_path
 /workspaces/LLaVA/blip_laion_cc_sbu_1.json
tokenizer
 <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>
data_args
 DataArguments(data_path='/workspaces/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')
list_data_dict [{'id': '000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Give a brief description of the image.\n<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]}]
current file path llava/train/train.py
def rank0_print(*args)
args
 ('Formatting inputs...Skip in lazy mode',)
Formatting inputs...Skip in lazy mode
self.tokenizer
 LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)
self.list_data_dict
 [{'id': '000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Give a brief description of the image.\n<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]}]
self.data_args
 DataArguments(data_path='/workspaces/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')
train_dataset
 <__main__.LazySupervisedDataset object at 0x78fd3c994550>
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
len(train_dataset)
 1
data_collator
 DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))
def make_supervised_data_module: result (return)
 {'train_dataset': <__main__.LazySupervisedDataset object at 0x78fd3c994550>, 'eval_dataset': None, 'data_collator': DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))}
data_module
 {'train_dataset': <__main__.LazySupervisedDataset object at 0x78fd3c994550>, 'eval_dataset': None, 'data_collator': DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))}
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
trainer
 <__main__.LLaVATrainer object at 0x78fd3c9946a0>
【COND】list(pathlib.Path(training_args.output_dir).glob('checkpoint-*'))
 []
【ENTER】else of if list(pathlib.Path(training_args.output_dir).glob(checkpoint-*)):
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__getitem__(self, i)
i
 0
sources
 {'id': '000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Give a brief description of the image.\n<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]}
【COND】 isinstance(i, int): True
【ENTER】if isinstance(i, int):
sources (after)
 [{'id': '000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Give a brief description of the image.\n<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]}]
【EXIT】if isinstance(i, int):
【COND】 'image' in sources[0]: True
【ENTER】if 'image' in sources[0]:
image_file
 GCC_train_000406392.jpg
image_folder
 /workspaces/LLaVA/images/
processor
 CLIPImageProcessor {
  "crop_size": {
    "height": 336,
    "width": 336
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "CLIPFeatureExtractor",
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 336
  }
}

image_path
 /workspaces/LLaVA/images/GCC_train_000406392.jpg
Trying to open image...
Image opened successfully.
【COND】 self.data_args.image_aspect_ratio square
【ENTER】else (self.data_args.image_aspect_ratio != 'pad')
image (before)
 <PIL.Image.Image image mode=RGB size=224x224 at 0x78FD3C9A09A0>
image (after processor.preprocess)
 tensor([[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],
         [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],
         [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],
         ...,
         [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],

        [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],
         [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],
         [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],
         ...,
         [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],

        [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],
         [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],
         [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],
         ...,
         [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],
         [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],
         [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]])
sources (before preprocess_multimodal)
 [{'id': '000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Give a brief description of the image.\n<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]}]
current file path llava/train/train.py
def preprocess_multimodal(sources, data_args)
sources
 [[{'from': 'human', 'value': 'Give a brief description of the image.\n<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]
data_args
 DataArguments(data_path='/workspaces/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')
is_multimodal
 True
source current loop
 [{'from': 'human', 'value': 'Give a brief description of the image.\n<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]
sentence current loop
 {'from': 'human', 'value': 'Give a brief description of the image.\n<image>'}
【COND】 if DEFAULT_IMAGE_TOKEN in sentence['value']: True
sentence['value']
 Give a brief description of the image.
<image>
DEFAULT_IMAGE_TOKEN
 <image>
【ENTER】if DEFAULT_IMAGE_TOKEN in sentence['value']:
sentence current loop
 {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}
【COND】 if DEFAULT_IMAGE_TOKEN in sentence['value']: False
sentence['value']
 the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair
DEFAULT_IMAGE_TOKEN
 <image>
sources (final return)
 [[{'from': 'human', 'value': '<image>\nGive a brief description of the image.'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]
sources (after preprocess_multimodal)
 [[{'from': 'human', 'value': '<image>\nGive a brief description of the image.'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]
Calling preprocess...
current file path llava/train/train.py
def preprocess(sources, tokenizer, has_image=False)
sources
 [[{'from': 'human', 'value': '<image>\nGive a brief description of the image.'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]
tokenizer
 <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>
has_image
 True
current file path llava/train/train.py
def preprocess_plain(sources, tokenizer)
sources
 [[{'from': 'human', 'value': '<image>\nGive a brief description of the image.'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]
tokenizer
 <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>
conversations initial
 []
source current loop
 [{'from': 'human', 'value': '<image>\nGive a brief description of the image.'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]
conversation current loop
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
current file path llava/train/train.py
def LazySupervisedDataset.__len__(self)
 <image>the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair

conversations (final)
 ['<image>the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair\n']
current file path llava/mm_utils.py
def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None)
prompt
 <image>the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair

tokenizer
 LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)
image_token_index
 -200
return_tensors
 pt
input_ids
 [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
          411,  2654, 11315,    13])]
input_ids[0].shape
 torch.Size([24])
targets
 [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
          411,  2654, 11315,    13])]
targets[0].shape
 torch.Size([24])
sources
 [[{'from': 'human', 'value': '<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]
current file path llava/mm_utils.py
def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None)
prompt
 <image>
tokenizer
 LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)
image_token_index
 -200
return_tensors
 None
input_ids (return)
 [1, -200]
input_ids (return)
 [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
          411,  2654, 11315,    13])]
targets (return)
 [tensor([ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
          411,  2654, 11315,    13])]
data_dict (after preprocess)
 {'input_ids': [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
          411,  2654, 11315,    13])], 'labels': [tensor([ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
          411,  2654, 11315,    13])]}
【COND】 isinstance(i, int): True
current file path llava/train/train.py
def DataCollatorForSupervisedDataset.__call__(self, instances)
instances
 [{'input_ids': tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
          411,  2654, 11315,    13]), 'labels': tensor([ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
          411,  2654, 11315,    13]), 'image': tensor([[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],
         [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],
         [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],
         ...,
         [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],

        [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],
         [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],
         [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],
         ...,
         [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],

        [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],
         [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],
         [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],
         ...,
         [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],
         [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],
         [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]])}]
shape of each instance's input_ids and labels, and images(if any): [(torch.Size([24]), torch.Size([24]), torch.Size([3, 336, 336]))]
self.tokenizer.pad_token_id
 0
IGNORE_INDEX
 -100
input_ids.shape (after pad_sequence and truncate)
 torch.Size([1, 24])
input_ids (after pad_sequence and truncate)
 tensor([[    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
           411,  2654, 11315,    13]])
labels.shape (after pad_sequence and truncate)
 torch.Size([1, 24])
labels (after pad_sequence and truncate)
 tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
           411,  2654, 11315,    13]])
batch['images'].shape
 torch.Size([1, 3, 336, 336])
batch (return)
 {'input_ids': tensor([[    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
           411,  2654, 11315,    13]]), 'labels': tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
           411,  2654, 11315,    13]]), 'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True]]), 'images': tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],
          ...,
          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],

         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],
          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],
          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],
          ...,
          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],

         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],
          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],
          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],
          ...,
          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],
          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],
          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])}
shape of each batch's input_ids and labels, and images(if any): [(torch.Size([1, 24]), torch.Size([1, 24]), torch.Size([1, 3, 336, 336]))]
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, image_sizes, return_dict)
input_ids
 tensor([[    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
           411,  2654, 11315,    13]])
input_ids.shape
 torch.Size([1, 24])
attention_mask
 tensor([[True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True]])
position_ids
 None
past_key_values
 None
inputs_embeds
 None
labels
 tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
           411,  2654, 11315,    13]])
use_cache
 None
output_attentions
 None
output_hidden_states
 None
images
 tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],
          ...,
          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],

         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],
          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],
          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],
          ...,
          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],

         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],
          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],
          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],
          ...,
          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],
          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],
          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])
images.shape
 torch.Size([1, 3, 336, 336])
image_sizes
 None
return_dict
 None
【COND】 inputs_embeds_is_None=True
【ENTER】if inputs_embeds is None:
current file path llava/model/llava_arch.py
def LlavaMetaForCausalLM(ABC).prepare_inputs_labels_for_multimodal(self, input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes=None)
input_ids
 tensor([[    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
           411,  2654, 11315,    13]])
position_ids
 None
attention_mask
 tensor([[True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True]])
past_key_values
 None
labels
 tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
           411,  2654, 11315,    13]])
images
 tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],
          ...,
          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],

         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],
          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],
          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],
          ...,
          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],

         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],
          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],
          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],
          ...,
          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],
          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],
          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])
image_sizes
 None
current file path llava/model/llava_arch.py
class LlavaMetaForCausalLM(ABC).get_vision_tower(self)
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class '__main__.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=2048, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=2048, out_features=2048, bias=True)
  )
)
current file path llava/model/llava_arch.py
def get_vision_tower(self)
vision_tower (raw)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
type(vision_tower)
 <class '__main__.CLIPVisionTower'>
【COND】 type_vision_tower_is_list=False
vision_tower (return)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
LlavaMetaForCausalLM(ABC).get_vision_tower(self) result (return)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
vision_tower
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
【COND】 vision_tower_is_None=False images_is_None=False input_ids_shape_1_eq_1=False
【COND】type(images)
 <class 'torch.Tensor'>
【COND】images.ndim
 4
【ENTER】else of if type(images) is list or images.ndim == 5:
current file path llava/model/llava_arch.py
def LlavaMetaForCausalLM(ABC).encode_images(self, images)
images
 tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],
          ...,
          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],

         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],
          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],
          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],
          ...,
          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],

         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],
          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],
          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],
          ...,
          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],
          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],
          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class '__main__.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=2048, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=2048, out_features=2048, bias=True)
  )
)
current file path llava/model/llava_arch.py
def get_vision_tower(self)
vision_tower (raw)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
type(vision_tower)
 <class '__main__.CLIPVisionTower'>
【COND】 type_vision_tower_is_list=False
vision_tower (return)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.forward(self, images)
images shape
 torch.Size([1, 3, 336, 336])
images
 tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],
          ...,
          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],

         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],
          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],
          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],
          ...,
          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],

         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],
          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],
          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],
          ...,
          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],
          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],
          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])
images.shape
 torch.Size([1, 3, 336, 336])
【COND】 type_images_is_list=False
【ENTER】else (type(images) is not list):
original images
 tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],
          ...,
          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],
          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],

         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],
          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],
          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],
          ...,
          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],
          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],

         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],
          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],
          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],
          ...,
          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],
          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],
          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])
after process image_forward_outs
 <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.feature_select(self, image_forward_outs)
image_forward_outs
 BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.0879, -0.1445, -0.2949,  ...,  0.3906, -0.7188,  0.0356],
         [ 0.4629,  0.0078,  0.0859,  ..., -0.0815,  0.1113, -0.3906],
         [-0.2188,  1.0938,  1.0469,  ...,  0.2119, -0.3477, -0.8477],
         ...,
         [ 1.7500,  1.0859,  1.6719,  ..., -0.2891, -0.6797,  1.1016],
         [ 0.9141, -0.1050,  1.3594,  ..., -0.4141, -1.2031,  0.4043],
         [ 1.4531,  1.1641,  1.0156,  ..., -0.3867, -0.6094,  0.6719]]],
       dtype=torch.bfloat16), pooler_output=tensor([[ 0.1738, -0.1426, -0.6133,  ...,  0.9453, -1.4844,  0.0967]],
       dtype=torch.bfloat16), hidden_states=(tensor([[[ 0.0342, -0.0408, -0.1670,  ...,  0.3203, -0.1475, -0.0201],
         [-0.1187, -0.0493, -0.2656,  ...,  0.5859, -0.1318, -0.0098],
         [ 0.2393, -0.0437, -0.6172,  ...,  0.3438, -0.1445, -0.0098],
         ...,
         [ 0.1108, -0.0439, -0.1240,  ..., -0.0601, -0.1357, -0.0162],
         [ 0.1855, -0.0454, -0.1118,  ..., -0.0903, -0.1357, -0.0112],
         [ 0.0308, -0.0486, -0.0287,  ..., -0.0150, -0.1436, -0.0153]]],
       dtype=torch.bfloat16), tensor([[[ 0.0530,  0.1074, -0.1055,  ...,  0.1094,  0.0781,  0.0308],
         [ 0.0371,  0.0625, -0.1621,  ...,  0.3711, -0.0166, -0.0398],
         [ 0.2490,  0.0527, -0.4141,  ...,  0.1621,  0.1074,  0.1143],
         ...,
         [ 0.0698, -0.0698,  0.0410,  ..., -0.2988, -0.0195, -0.0435],
         [ 0.1904, -0.1289,  0.0171,  ..., -0.2988,  0.0146, -0.0525],
         [ 0.1396,  0.0537, -0.0117,  ..., -0.0732, -0.0156, -0.0083]]],
       dtype=torch.bfloat16), tensor([[[ 0.0408,  0.0098, -0.0869,  ...,  0.0674, -0.0288,  0.0486],
         [ 0.1133,  0.0054, -0.0918,  ...,  0.2412,  0.1250, -0.0337],
         [ 0.2070, -0.0588, -0.2832,  ...,  0.1523,  0.1006,  0.2051],
         ...,
         [-0.0547, -0.1973,  0.0098,  ..., -0.4551, -0.1118, -0.1055],
         [ 0.0205, -0.3027, -0.0288,  ..., -0.4922, -0.0635, -0.1338],
         [ 0.1348,  0.1006, -0.0991,  ..., -0.1621, -0.0354, -0.0364]]],
       dtype=torch.bfloat16), tensor([[[-0.0488,  0.0015, -0.0503,  ...,  0.0510,  0.0056,  0.0938],
         [ 0.1680, -0.0400, -0.0239,  ...,  0.2852,  0.3242, -0.0232],
         [ 0.2305,  0.0479, -0.0625,  ...,  0.1836,  0.1514,  0.3594],
         ...,
         [-0.1992, -0.2344, -0.0156,  ..., -0.4023, -0.1602, -0.0654],
         [-0.0586, -0.2539,  0.0259,  ..., -0.4316, -0.1309, -0.2139],
         [-0.0352,  0.3262, -0.1660,  ..., -0.1484,  0.0164, -0.0408]]],
       dtype=torch.bfloat16), tensor([[[-0.0933, -0.0166,  0.0188,  ...,  0.0811, -0.0063,  0.1025],
         [ 0.2500,  0.0225, -0.0117,  ...,  0.3145,  0.3789, -0.0177],
         [ 0.0527, -0.1914, -0.0776,  ...,  0.2598,  0.0928,  0.4062],
         ...,
         [-0.2158, -0.3867, -0.1240,  ..., -0.3145, -0.2236, -0.1523],
         [ 0.0273, -0.4551, -0.0640,  ..., -0.4160, -0.2324, -0.3340],
         [-0.0315,  0.3945, -0.0322,  ..., -0.0762, -0.0112, -0.0608]]],
       dtype=torch.bfloat16), tensor([[[-0.0942, -0.0542,  0.0381,  ...,  0.0903,  0.1113,  0.1201],
         [ 0.0859,  0.0083,  0.1484,  ...,  0.3008,  0.3086, -0.0874],
         [-0.0786, -0.2197,  0.0293,  ...,  0.3516,  0.0562,  0.4531],
         ...,
         [-0.1406, -0.4141,  0.0762,  ..., -0.2148, -0.1348, -0.1699],
         [ 0.0317, -0.5781,  0.1592,  ..., -0.3750, -0.2480, -0.2344],
         [-0.0894,  0.3516, -0.1387,  ..., -0.1084, -0.0137,  0.0728]]],
       dtype=torch.bfloat16), tensor([[[-0.0308,  0.0203,  0.0535,  ...,  0.0515,  0.1348,  0.0703],
         [-0.1006,  0.1211,  0.1147,  ...,  0.4551,  0.3164, -0.0635],
         [-0.0159, -0.1777,  0.0645,  ...,  0.4121, -0.0640,  0.4297],
         ...,
         [-0.2422, -0.3672,  0.2422,  ..., -0.2158, -0.2656, -0.2969],
         [-0.0569, -0.6250,  0.1982,  ..., -0.3145, -0.2832, -0.3008],
         [ 0.0088,  0.3867, -0.2041,  ..., -0.0801,  0.0625,  0.0391]]],
       dtype=torch.bfloat16), tensor([[[-0.0103,  0.0128,  0.0366,  ...,  0.0708,  0.0625,  0.0525],
         [-0.3164,  0.0747,  0.1641,  ...,  0.7266,  0.2988, -0.0088],
         [ 0.1602, -0.2061,  0.1816,  ...,  0.6445, -0.1777,  0.0928],
         ...,
         [-0.3145, -0.1406,  0.4531,  ..., -0.0132, -0.3164, -0.0225],
         [-0.1211, -0.5859,  0.3164,  ..., -0.1279, -0.3125, -0.1680],
         [ 0.0259,  0.3809, -0.0156,  ...,  0.0625,  0.1445,  0.1680]]],
       dtype=torch.bfloat16), tensor([[[-0.0405, -0.0493,  0.0444,  ...,  0.1738,  0.0366,  0.0183],
         [-0.1943, -0.0244,  0.0264,  ...,  0.8086,  0.1758, -0.1436],
         [ 0.0830, -0.3789,  0.3184,  ...,  0.8594, -0.1553, -0.1123],
         ...,
         [-0.3125, -0.2754,  0.4668,  ..., -0.1162, -0.4883,  0.2266],
         [-0.3027, -0.6406,  0.2930,  ..., -0.2217, -0.4023, -0.1172],
         [-0.0498,  0.3848, -0.3711,  ...,  0.0718, -0.1895, -0.0547]]],
       dtype=torch.bfloat16), tensor([[[-0.1196, -0.0562, -0.0332,  ...,  0.1270, -0.0732, -0.0430],
         [-0.0430, -0.1641, -0.0894,  ...,  0.8047,  0.3125, -0.0527],
         [ 0.0708, -0.3242,  0.3379,  ...,  0.8828, -0.2617,  0.0908],
         ...,
         [-0.3320, -0.4219,  0.1191,  ..., -0.2988, -0.6133,  0.4238],
         [-0.4551, -0.8320, -0.0947,  ..., -0.4375, -0.8203,  0.0026],
         [ 0.0371,  0.3203, -0.3223,  ..., -0.1279, -0.1338, -0.1406]]],
       dtype=torch.bfloat16), tensor([[[-0.0908, -0.0972,  0.0366,  ...,  0.1235, -0.0913,  0.0771],
         [ 0.1089,  0.1504, -0.1641,  ...,  0.5195,  0.3262,  0.1123],
         [ 0.0762,  0.0742,  0.1279,  ...,  0.8398, -0.1484,  0.0156],
         ...,
         [-0.3594, -0.3535,  0.2910,  ..., -0.1055, -0.6250,  0.3281],
         [-0.3555, -0.5977,  0.2578,  ..., -0.3359, -0.5781,  0.0245],
         [ 0.1494,  0.3438, -0.0767,  ..., -0.0713, -0.0029, -0.2080]]],
       dtype=torch.bfloat16), tensor([[[-0.0205, -0.0469,  0.0039,  ...,  0.0718, -0.0884,  0.1406],
         [ 0.0825,  0.2061, -0.0703,  ...,  0.5234,  0.5195,  0.0884],
         [ 0.0100,  0.1836,  0.0386,  ...,  0.9297, -0.2080, -0.0530],
         ...,
         [-0.2354, -0.3633,  0.2852,  ...,  0.0576, -0.5312,  0.0786],
         [-0.2227, -0.4297,  0.0977,  ..., -0.2578, -0.4961, -0.3027],
         [-0.0957,  0.2129,  0.2832,  ..., -0.0073, -0.1260, -0.3750]]],
       dtype=torch.bfloat16), tensor([[[-0.0339, -0.1240, -0.1602,  ...,  0.0713, -0.1895,  0.1230],
         [-0.0312,  0.2812, -0.2451,  ...,  0.5273,  0.3281,  0.1660],
         [-0.0645,  0.1914,  0.0801,  ...,  0.9336, -0.1758,  0.0018],
         ...,
         [-0.2734, -0.1035,  0.3262,  ..., -0.0674, -0.5469,  0.0713],
         [-0.0542, -0.5039,  0.1240,  ..., -0.2168, -0.5195, -0.5586],
         [-0.0098,  0.2012,  0.3828,  ..., -0.1279, -0.0874, -0.1807]]],
       dtype=torch.bfloat16), tensor([[[-0.0840, -0.0293, -0.0859,  ...,  0.1201, -0.2158,  0.1787],
         [-0.0459,  0.2490, -0.0205,  ...,  0.4004,  0.3223,  0.1426],
         [-0.1108,  0.1357, -0.0801,  ...,  0.7109, -0.2734, -0.2148],
         ...,
         [-0.2168,  0.0405,  0.2490,  ..., -0.0068, -0.1289,  0.0217],
         [ 0.0571, -0.3633,  0.0942,  ..., -0.2949, -0.1094, -0.4590],
         [ 0.2148,  0.1943,  0.3262,  ..., -0.1768,  0.0806, -0.2988]]],
       dtype=torch.bfloat16), tensor([[[-0.1719, -0.0371, -0.1592,  ...,  0.1055, -0.2656,  0.0127],
         [ 0.0337,  0.2451,  0.1396,  ...,  0.1426,  0.2158,  0.1562],
         [-0.1445,  0.1455, -0.2773,  ...,  0.3652, -0.1572, -0.1777],
         ...,
         [ 0.1152,  0.0645,  0.3359,  ..., -0.3438,  0.0118,  0.0430],
         [ 0.0591, -0.3750,  0.1045,  ..., -0.5664, -0.0610, -0.2119],
         [ 0.2578,  0.0405,  0.5352,  ..., -0.4746,  0.0566, -0.2871]]],
       dtype=torch.bfloat16), tensor([[[-0.1240, -0.1602, -0.0991,  ...,  0.1123, -0.2578,  0.1191],
         [-0.0027,  0.2715,  0.1855,  ..., -0.1982,  0.3320,  0.3789],
         [-0.3613,  0.2500, -0.1035,  ...,  0.1357, -0.0771,  0.0137],
         ...,
         [ 0.2578,  0.1133,  0.3965,  ...,  0.0923, -0.1484,  0.3008],
         [-0.0366, -0.4805,  0.2217,  ..., -0.0608, -0.3340, -0.1592],
         [ 0.3828,  0.0098,  0.5312,  ..., -0.4355, -0.0300, -0.0439]]],
       dtype=torch.bfloat16), tensor([[[-0.1157, -0.1279, -0.1006,  ..., -0.0732, -0.2637,  0.1235],
         [-0.0928,  0.4102,  0.2002,  ...,  0.0062,  0.2402,  0.4551],
         [-0.3125,  0.4824, -0.2100,  ...,  0.2832, -0.0742,  0.0564],
         ...,
         [ 0.3867,  0.3066,  0.2236,  ...,  0.2695, -0.1406,  0.2871],
         [-0.0635, -0.2080,  0.1748,  ...,  0.0251, -0.3496, -0.3457],
         [ 0.5234,  0.2676,  0.4531,  ..., -0.5312, -0.0742, -0.0635]]],
       dtype=torch.bfloat16), tensor([[[-0.1152,  0.1089,  0.0088,  ..., -0.1250, -0.3516,  0.1035],
         [-0.1348, -0.1104,  0.1709,  ..., -0.1475,  0.1621,  0.2812],
         [-0.1963,  0.2695, -0.3086,  ...,  0.1504,  0.0527, -0.1533],
         ...,
         [ 0.4785,  0.4980,  0.2236,  ...,  0.1719, -0.0986,  0.2500],
         [ 0.0625, -0.0176,  0.1934,  ..., -0.2207, -0.3770, -0.3965],
         [ 0.6055,  0.2334,  0.4629,  ..., -0.4199, -0.2832, -0.1426]]],
       dtype=torch.bfloat16), tensor([[[-0.1533,  0.1006,  0.0013,  ..., -0.0967, -0.2002,  0.0542],
         [-0.2363, -0.1387, -0.3262,  ..., -0.2988,  0.1406,  0.2891],
         [-0.2695,  0.3008, -0.2773,  ...,  0.0967,  0.2891, -0.1094],
         ...,
         [ 0.5508,  0.6797,  0.4336,  ...,  0.1338, -0.2422,  0.2656],
         [-0.1777, -0.0898, -0.0527,  ..., -0.1875, -0.3691, -0.5117],
         [ 0.5820,  0.2217,  0.2910,  ..., -0.4141, -0.2402, -0.0972]]],
       dtype=torch.bfloat16), tensor([[[-2.6367e-01,  1.0547e-01, -6.9336e-02,  ..., -5.2734e-02,
          -3.1250e-01,  1.6602e-02],
         [-3.7695e-01, -5.4688e-01, -3.8281e-01,  ...,  8.7891e-03,
           4.9805e-01,  8.5938e-02],
         [-4.5898e-01,  2.7344e-01, -8.0078e-02,  ...,  9.2773e-02,
           3.1641e-01, -6.5918e-02],
         ...,
         [ 5.3906e-01,  7.6172e-01,  4.7266e-01,  ...,  2.0703e-01,
          -2.8320e-01,  1.7383e-01],
         [-2.3828e-01, -4.2383e-01, -4.8828e-04,  ..., -1.7383e-01,
          -4.9609e-01, -3.5156e-01],
         [ 5.3516e-01,  1.7969e-01,  3.1641e-01,  ..., -2.1582e-01,
          -4.9219e-01, -2.4414e-03]]], dtype=torch.bfloat16), tensor([[[ 0.0703,  0.1992, -0.1992,  ..., -0.0293, -0.3535,  0.2363],
         [-0.2383, -0.2031, -0.0596,  ..., -0.1074,  0.4258,  0.0693],
         [-0.2188,  0.5391,  0.2070,  ...,  0.1021,  0.0771, -0.0388],
         ...,
         [ 0.8633,  0.7344,  0.5742,  ...,  0.1094, -0.3906,  0.2051],
         [ 0.2002, -0.5156,  0.1436,  ..., -0.2891, -0.6172, -0.3027],
         [ 0.9883,  0.4512,  0.5039,  ..., -0.3164, -0.5156,  0.2002]]],
       dtype=torch.bfloat16), tensor([[[ 0.0449,  0.2178, -0.2490,  ...,  0.1367, -0.3242,  0.4492],
         [-0.3594, -0.3027,  0.4336,  ...,  0.0020,  0.4023,  0.0400],
         [ 0.2754,  0.8047, -0.0332,  ...,  0.0479, -0.3086, -0.3945],
         ...,
         [ 1.6094,  1.1562,  0.6641,  ..., -0.5430, -0.8320,  0.4023],
         [ 1.1328, -0.2031,  0.7227,  ..., -0.9531, -0.7656, -0.3398],
         [ 1.8672,  0.9336,  0.6797,  ..., -0.9922, -0.4082,  0.0059]]],
       dtype=torch.bfloat16), tensor([[[ 0.0347,  0.3184, -0.2930,  ...,  0.3086, -0.3535,  0.1572],
         [ 0.2930, -0.0312,  0.3555,  ...,  0.3984,  0.5000, -0.2578],
         [ 0.0977,  0.7734,  0.4414,  ...,  0.4375, -0.1816, -0.5898],
         ...,
         [ 1.4297,  1.2031,  1.2656,  ..., -0.1641, -0.8711,  0.9180],
         [ 0.9609, -0.0039,  1.2344,  ..., -0.3906, -0.9922, -0.0430],
         [ 1.5625,  1.1016,  1.1250,  ..., -0.4961, -0.5000,  0.4141]]],
       dtype=torch.bfloat16), tensor([[[-0.0054,  0.2051, -0.7422,  ...,  0.4414, -0.5078, -0.1128],
         [ 0.1074,  0.1348, -0.4238,  ...,  0.1475,  0.3477, -0.4414],
         [-0.3164,  0.9922,  0.3301,  ...,  0.4297, -0.2090, -0.7070],
         ...,
         [ 1.6094,  0.9805,  0.8906,  ..., -0.2812, -1.0391,  1.2031],
         [ 0.8398, -0.3633,  0.6953,  ..., -0.2754, -1.2266,  0.4453],
         [ 1.5391,  0.7812,  0.3848,  ..., -0.2246, -0.7266,  0.8047]]],
       dtype=torch.bfloat16), tensor([[[ 0.0879, -0.1445, -0.2949,  ...,  0.3906, -0.7188,  0.0356],
         [ 0.4629,  0.0078,  0.0859,  ..., -0.0815,  0.1113, -0.3906],
         [-0.2188,  1.0938,  1.0469,  ...,  0.2119, -0.3477, -0.8477],
         ...,
         [ 1.7500,  1.0859,  1.6719,  ..., -0.2891, -0.6797,  1.1016],
         [ 0.9141, -0.1050,  1.3594,  ..., -0.4141, -1.2031,  0.4043],
         [ 1.4531,  1.1641,  1.0156,  ..., -0.3867, -0.6094,  0.6719]]],
       dtype=torch.bfloat16)), attentions=None)
image_features (after select_layer)
 <class 'torch.Tensor'>
image_features.shape
 torch.Size([1, 577, 1024])
【COND】 select_feature=patch
【ENTER】if self.select_feature == 'patch':
original image_features
 tensor([[[-0.0054,  0.2051, -0.7422,  ...,  0.4414, -0.5078, -0.1128],
         [ 0.1074,  0.1348, -0.4238,  ...,  0.1475,  0.3477, -0.4414],
         [-0.3164,  0.9922,  0.3301,  ...,  0.4297, -0.2090, -0.7070],
         ...,
         [ 1.6094,  0.9805,  0.8906,  ..., -0.2812, -1.0391,  1.2031],
         [ 0.8398, -0.3633,  0.6953,  ..., -0.2754, -1.2266,  0.4453],
         [ 1.5391,  0.7812,  0.3848,  ..., -0.2246, -0.7266,  0.8047]]],
       dtype=torch.bfloat16)
after process
 tensor([[[ 0.1074,  0.1348, -0.4238,  ...,  0.1475,  0.3477, -0.4414],
         [-0.3164,  0.9922,  0.3301,  ...,  0.4297, -0.2090, -0.7070],
         [ 1.2656, -0.0146, -0.6602,  ...,  0.2432,  0.3555,  0.3438],
         ...,
         [ 1.6094,  0.9805,  0.8906,  ..., -0.2812, -1.0391,  1.2031],
         [ 0.8398, -0.3633,  0.6953,  ..., -0.2754, -1.2266,  0.4453],
         [ 1.5391,  0.7812,  0.3848,  ..., -0.2246, -0.7266,  0.8047]]],
       dtype=torch.bfloat16)
【EXIT】if self.select_feature == 'patch':
selected image_feature shape
 torch.Size([1, 576, 1024])
image_features (return)
 tensor([[[ 0.1074,  0.1348, -0.4238,  ...,  0.1475,  0.3477, -0.4414],
         [-0.3164,  0.9922,  0.3301,  ...,  0.4297, -0.2090, -0.7070],
         [ 1.2656, -0.0146, -0.6602,  ...,  0.2432,  0.3555,  0.3438],
         ...,
         [ 1.6094,  0.9805,  0.8906,  ..., -0.2812, -1.0391,  1.2031],
         [ 0.8398, -0.3633,  0.6953,  ..., -0.2754, -1.2266,  0.4453],
         [ 1.5391,  0.7812,  0.3848,  ..., -0.2246, -0.7266,  0.8047]]],
       dtype=torch.bfloat16)
image_features.shape
 torch.Size([1, 576, 1024])
after process image_features
 <class 'torch.Tensor'>
【EXIT】else (type(images) is not list):
image_features (return)
 tensor([[[ 0.1074,  0.1348, -0.4238,  ...,  0.1475,  0.3477, -0.4414],
         [-0.3164,  0.9922,  0.3301,  ...,  0.4297, -0.2090, -0.7070],
         [ 1.2656, -0.0146, -0.6602,  ...,  0.2432,  0.3555,  0.3438],
         ...,
         [ 1.6094,  0.9805,  0.8906,  ..., -0.2812, -1.0391,  1.2031],
         [ 0.8398, -0.3633,  0.6953,  ..., -0.2754, -1.2266,  0.4453],
         [ 1.5391,  0.7812,  0.3848,  ..., -0.2246, -0.7266,  0.8047]]])
image_features.shape
 torch.Size([1, 576, 1024])
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class '__main__.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=2048, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=2048, out_features=2048, bias=True)
  )
)
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
image_features (return) shape
 torch.Size([1, 576, 2048])
image_features (return)
 tensor([[[ 0.0898,  0.0430, -0.0889,  ...,  0.1660, -0.0732, -0.0120],
         [-0.0435,  0.0300, -0.0217,  ..., -0.0062, -0.1621,  0.1230],
         [ 0.0344,  0.1689, -0.1279,  ...,  0.0069,  0.0737,  0.3105],
         ...,
         [ 0.2695,  0.1602, -0.1279,  ..., -0.0459,  0.2139,  0.0635],
         [ 0.1514,  0.1025, -0.1992,  ...,  0.2773,  0.1631, -0.0388],
         [ 0.1074, -0.0095, -0.0154,  ...,  0.1621,  0.2227, -0.0981]]],
       dtype=torch.bfloat16, grad_fn=<ViewBackward0>)
image_features after encode_images shape 
 torch.Size([1, 576, 2048])
image_features after encode_images
 tensor([[[ 0.0898,  0.0430, -0.0889,  ...,  0.1660, -0.0732, -0.0120],
         [-0.0435,  0.0300, -0.0217,  ..., -0.0062, -0.1621,  0.1230],
         [ 0.0344,  0.1689, -0.1279,  ...,  0.0069,  0.0737,  0.3105],
         ...,
         [ 0.2695,  0.1602, -0.1279,  ..., -0.0459,  0.2139,  0.0635],
         [ 0.1514,  0.1025, -0.1992,  ...,  0.2773,  0.1631, -0.0388],
         [ 0.1074, -0.0095, -0.0154,  ...,  0.1621,  0.2227, -0.0981]]],
       dtype=torch.bfloat16, grad_fn=<ViewBackward0>)
【EXIT】else of if type(images) is list or images.ndim == 5:
labels before
 tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
           411,  2654, 11315,    13]])
position_ids before
 None
attention_mask before
 tensor([[True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True]])
【ENTER】else of if attention_mask is None:
attention_mask（after）shape 
 torch.Size([1, 24])
attention_mask (after)
 tensor([[True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True]])
【EXIT】else of if attention_mask is None:
【ENTER】if position_ids is None:
position_ids (after) shape 
 torch.Size([24])
position_ids (after)
 tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23])
【EXIT】if position_ids is None:
【COND】 labels_is_None=False
input_ids after removing padding
 [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
          411,  2654, 11315,    13])]
labels after removing padding
 [tensor([ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
          411,  2654, 11315,    13])]
cur_input_ids shape
 torch.Size([24])
cur_input_ids
 tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
          411,  2654, 11315,    13])
【COND】num_images: tensor(1)
image_token_indices
 [-1, 1, 24]
len image_token_indices 3
cur_labels
 tensor([ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
        10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
          411,  2654, 11315,    13])
cur_input_ids_noim (after)
 [tensor([1]), tensor([  278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596,
        23425,   278,  3700,   322,  6567,   310,   263,  6114,   411,  2654,
        11315,    13])]
cur_labels_noim (after) 
 [tensor([-100]), tensor([  278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596,
        23425,   278,  3700,   322,  6567,   310,   263,  6114,   411,  2654,
        11315,    13])]
split_sizes
 [1, 22]
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class '__main__.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 2048, padding_idx=0)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=2048, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=2048, out_features=2048, bias=True)
  )
)
cur_input_embeds shape
 torch.Size([23, 2048])
cur_input_embeds
 tensor([[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,
         -6.5231e-04, -4.9973e-04],
        [ 7.0801e-03,  1.0452e-03,  6.0425e-03,  ...,  3.9673e-03,
          1.2817e-03, -1.1215e-03],
        [-2.2949e-02, -2.6226e-05,  6.8359e-03,  ..., -2.4658e-02,
         -9.4604e-03,  1.5869e-02],
        ...,
        [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,
         -8.3618e-03, -9.4604e-03],
        [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,
         -2.5177e-03, -8.0566e-03],
        [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,
         -7.3242e-04,  2.7924e-03]], requires_grad=True)
cur_input_embeds_no_im
 (tensor([[-0.0011,  0.0019, -0.0017,  ...,  0.0002, -0.0007, -0.0005]],
       grad_fn=<SplitWithSizesBackward0>), tensor([[ 7.0801e-03,  1.0452e-03,  6.0425e-03,  ...,  3.9673e-03,
          1.2817e-03, -1.1215e-03],
        [-2.2949e-02, -2.6226e-05,  6.8359e-03,  ..., -2.4658e-02,
         -9.4604e-03,  1.5869e-02],
        [-2.3499e-03,  1.4893e-02, -2.0447e-03,  ..., -8.6060e-03,
          2.3193e-03,  3.0670e-03],
        ...,
        [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,
         -8.3618e-03, -9.4604e-03],
        [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,
         -2.5177e-03, -8.0566e-03],
        [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,
         -7.3242e-04,  2.7924e-03]], grad_fn=<SplitWithSizesBackward0>))
【COND】 i=0 num_images=1
【ENTER】if i < num_images:
【EXIT】if i < num_images:
【COND】 i=1 num_images=1
cur_new_input_embeds (before cat) shape
 [torch.Size([1, 2048]), torch.Size([576, 2048]), torch.Size([22, 2048])]
cur_new_input_embeds (before cat)
 [tensor([[-0.0011,  0.0019, -0.0017,  ...,  0.0002, -0.0007, -0.0005]],
       grad_fn=<SplitWithSizesBackward0>), tensor([[ 0.0898,  0.0430, -0.0889,  ...,  0.1660, -0.0732, -0.0120],
        [-0.0435,  0.0300, -0.0217,  ..., -0.0062, -0.1621,  0.1230],
        [ 0.0344,  0.1689, -0.1279,  ...,  0.0069,  0.0737,  0.3105],
        ...,
        [ 0.2695,  0.1602, -0.1279,  ..., -0.0459,  0.2139,  0.0635],
        [ 0.1514,  0.1025, -0.1992,  ...,  0.2773,  0.1631, -0.0388],
        [ 0.1074, -0.0095, -0.0154,  ...,  0.1621,  0.2227, -0.0981]],
       dtype=torch.bfloat16, grad_fn=<SelectBackward0>), tensor([[ 7.0801e-03,  1.0452e-03,  6.0425e-03,  ...,  3.9673e-03,
          1.2817e-03, -1.1215e-03],
        [-2.2949e-02, -2.6226e-05,  6.8359e-03,  ..., -2.4658e-02,
         -9.4604e-03,  1.5869e-02],
        [-2.3499e-03,  1.4893e-02, -2.0447e-03,  ..., -8.6060e-03,
          2.3193e-03,  3.0670e-03],
        ...,
        [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,
         -8.3618e-03, -9.4604e-03],
        [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,
         -2.5177e-03, -8.0566e-03],
        [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,
         -7.3242e-04,  2.7924e-03]], grad_fn=<SplitWithSizesBackward0>)]
cur_new_labels (before cat) shape
 [torch.Size([1]), torch.Size([576]), torch.Size([22])]
cur_new_labels (before cat)
 [tensor([-100]), tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]), tensor([  278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596,
        23425,   278,  3700,   322,  6567,   310,   263,  6114,   411,  2654,
        11315,    13])]
cur_new_input_embeds (after cat) shape
 torch.Size([599, 2048])
cur_new_input_embeds (after cat)
 tensor([[-0.0011,  0.0019, -0.0017,  ...,  0.0002, -0.0007, -0.0005],
        [ 0.0898,  0.0430, -0.0889,  ...,  0.1660, -0.0732, -0.0120],
        [-0.0435,  0.0300, -0.0217,  ..., -0.0062, -0.1621,  0.1230],
        ...,
        [ 0.0212, -0.0227, -0.0142,  ..., -0.0028, -0.0084, -0.0095],
        [ 0.0037, -0.0036,  0.0090,  ..., -0.0137, -0.0025, -0.0081],
        [-0.0006, -0.0011, -0.0118,  ...,  0.0002, -0.0007,  0.0028]],
       grad_fn=<CatBackward0>)
cur_new_labels (after cat) shape
 torch.Size([599])
cur_new_labels (after cat)
 tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,
          297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,
          322,  6567,   310,   263,  6114,   411,  2654, 11315,    13])
new_input_embeds (so far) shape
 [torch.Size([599, 2048])]
new_input_embeds (so far)
 [tensor([[-0.0011,  0.0019, -0.0017,  ...,  0.0002, -0.0007, -0.0005],
        [ 0.0898,  0.0430, -0.0889,  ...,  0.1660, -0.0732, -0.0120],
        [-0.0435,  0.0300, -0.0217,  ..., -0.0062, -0.1621,  0.1230],
        ...,
        [ 0.0212, -0.0227, -0.0142,  ..., -0.0028, -0.0084, -0.0095],
        [ 0.0037, -0.0036,  0.0090,  ..., -0.0137, -0.0025, -0.0081],
        [-0.0006, -0.0011, -0.0118,  ...,  0.0002, -0.0007,  0.0028]],
       grad_fn=<CatBackward0>)]
new_labels (so far) shape
 [torch.Size([599])]
new_labels (so far)
 [tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,
          297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,
          322,  6567,   310,   263,  6114,   411,  2654, 11315,    13])]
【COND】 tokenizer_model_max_length_is_not_None=True
【ENTER】if tokenizer_model_max_length is not None:
【EXIT】if tokenizer_model_max_length is not None:
max_len
 599
batch_size
 1
new_labels_padded (before) shape
 torch.Size([1, 599])
new_labels_padded (before)
 tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]])
attention_mask (before) shape
 torch.Size([1, 599])
attention_mask (before)
 tensor([[False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False]])
position_ids (before) shape
 torch.Size([1, 599])
position_ids (before)
 tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
【COND】 padding_side=right cur_len=599 max_len=599
【ENTER】else (padding_side != 'left'):
new_input_embeds_padded (so far) shape
 [torch.Size([599, 2048])]
new_input_embeds_padded (so far)
 [tensor([[-0.0011,  0.0019, -0.0017,  ...,  0.0002, -0.0007, -0.0005],
        [ 0.0898,  0.0430, -0.0889,  ...,  0.1660, -0.0732, -0.0120],
        [-0.0435,  0.0300, -0.0217,  ..., -0.0062, -0.1621,  0.1230],
        ...,
        [ 0.0212, -0.0227, -0.0142,  ..., -0.0028, -0.0084, -0.0095],
        [ 0.0037, -0.0036,  0.0090,  ..., -0.0137, -0.0025, -0.0081],
        [-0.0006, -0.0011, -0.0118,  ...,  0.0002, -0.0007,  0.0028]],
       grad_fn=<CatBackward0>)]
new_labels_padded (so far) shape
 torch.Size([1, 599])
new_labels_padded (so far)
 tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,
           297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,
           322,  6567,   310,   263,  6114,   411,  2654, 11315,    13]])
attention_mask (so far) shape
 torch.Size([1, 599])
attention_mask (so far)
 tensor([[True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True]])
position_ids (so far) shape
 torch.Size([1, 599])
position_ids (so far)
 tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,
         140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,
         154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,
         168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,
         182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,
         196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,
         210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,
         224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,
         238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,
         252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,
         266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,
         280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,
         294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,
         308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,
         322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,
         336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,
         350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,
         364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,
         378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,
         392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,
         406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,
         420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,
         434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,
         448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,
         462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,
         476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,
         490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,
         504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517,
         518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531,
         532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,
         546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559,
         560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573,
         574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587,
         588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598]])
【EXIT】else (padding_side != 'left'):
new_input_embeds (after) shape
 torch.Size([1, 599, 2048])
new_input_embeds (after)
 tensor([[[-0.0011,  0.0019, -0.0017,  ...,  0.0002, -0.0007, -0.0005],
         [ 0.0898,  0.0430, -0.0889,  ...,  0.1660, -0.0732, -0.0120],
         [-0.0435,  0.0300, -0.0217,  ..., -0.0062, -0.1621,  0.1230],
         ...,
         [ 0.0212, -0.0227, -0.0142,  ..., -0.0028, -0.0084, -0.0095],
         [ 0.0037, -0.0036,  0.0090,  ..., -0.0137, -0.0025, -0.0081],
         [-0.0006, -0.0011, -0.0118,  ...,  0.0002, -0.0007,  0.0028]]],
       grad_fn=<StackBackward0>)
【COND】 _labels_is_None=False
【ENTER】else of if _labels is None:
new_labels (after)
 tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,
           297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,
           322,  6567,   310,   263,  6114,   411,  2654, 11315,    13]])
【EXIT】else of if _labels is None:
【COND】 _attention_mask_is_None=False
【ENTER】else of if _attention_mask is None:
attention_mask (after)2
 tensor([[True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True]])
【COND】 _position_ids_is_None=True
【ENTER】if _position_ids is None:
【EXIT】if _position_ids is None:
position_ids (return)
 None
attention_mask (return)
 tensor([[True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True]])
past_key_values (return)
 None
new_input_embeds (return)
 tensor([[[-0.0011,  0.0019, -0.0017,  ...,  0.0002, -0.0007, -0.0005],
         [ 0.0898,  0.0430, -0.0889,  ...,  0.1660, -0.0732, -0.0120],
         [-0.0435,  0.0300, -0.0217,  ..., -0.0062, -0.1621,  0.1230],
         ...,
         [ 0.0212, -0.0227, -0.0142,  ..., -0.0028, -0.0084, -0.0095],
         [ 0.0037, -0.0036,  0.0090,  ..., -0.0137, -0.0025, -0.0081],
         [-0.0006, -0.0011, -0.0118,  ...,  0.0002, -0.0007,  0.0028]]],
       grad_fn=<StackBackward0>)
new_labels (return)
 tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,
           297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,
           322,  6567,   310,   263,  6114,   411,  2654, 11315,    13]])
【EXIT】if inputs_embeds is None:
input_ids (after prepare_inputs_labels_for_multimodal)
 None
position_ids (after prepare_inputs_labels_for_multimodal)
 None
attention_mask shape (after prepare_inputs_labels_for_multimodal)
 torch.Size([1, 599])
attention_mask (after prepare_inputs_labels_for_multimodal)
 tensor([[True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True]])
past_key_values (after prepare_inputs_labels_for_multimodal)
 None
inputs_embeds shape (after prepare_inputs_labels_for_multimodal)
 torch.Size([1, 599, 2048])
inputs_embeds (after prepare_inputs_labels_for_multimodal)
 tensor([[[-0.0011,  0.0019, -0.0017,  ...,  0.0002, -0.0007, -0.0005],
         [ 0.0898,  0.0430, -0.0889,  ...,  0.1660, -0.0732, -0.0120],
         [-0.0435,  0.0300, -0.0217,  ..., -0.0062, -0.1621,  0.1230],
         ...,
         [ 0.0212, -0.0227, -0.0142,  ..., -0.0028, -0.0084, -0.0095],
         [ 0.0037, -0.0036,  0.0090,  ..., -0.0137, -0.0025, -0.0081],
         [-0.0006, -0.0011, -0.0118,  ...,  0.0002, -0.0007,  0.0028]]],
       grad_fn=<StackBackward0>)
labels shape (after prepare_inputs_labels_for_multimodal)
 torch.Size([1, 599])
labels (after prepare_inputs_labels_for_multimodal)
 tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,
           297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,
           322,  6567,   310,   263,  6114,   411,  2654, 11315,    13]])
Return of def LlavaLlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, image_sizes, return_dict)
logits tensor shape  LlavaLlamaForCausalLM.forward
 torch.Size([1, 599, 32000])
logits tensor (first 10 tokens)  LlavaLlamaForCausalLM.forward
 tensor([[-4.5938,  1.0156,  4.5000,  ..., -5.2500, -2.2344, -4.0000],
        [-4.7188, -3.9375,  5.6562,  ..., -4.1875, -5.5625, -1.3828],
        [-7.0312, -6.3125,  2.3438,  ..., -4.9375, -3.6250, -5.7812],
        ...,
        [-7.6562, -7.6875,  4.0938,  ..., -5.2812, -6.6875, -4.4375],
        [-7.8125, -7.8438,  1.6172,  ..., -2.3594, -7.0000, -4.1875],
        [-7.7500, -7.5000,  1.9375,  ..., -2.3438, -7.1250, -4.1875]],
       grad_fn=<SliceBackward0>)
loss (return)  LlavaLlamaForCausalLM.forward 
 tensor(8.8422, grad_fn=<NllLossBackward0>)
 current file path llava/train/llava_trainer.py
def _save_checkpoint(self, model, trial, metrics=None)
self _save_checkpoint
 <__main__.LLaVATrainer object at 0x78fd3c9946a0>
model _save_checkpoint
 LlavaLlamaForCausalLM(
  (model): LlavaLlamaModel(
    (embed_tokens): Embedding(32000, 2048, padding_idx=0)
    (layers): ModuleList(
      (0-21): 22 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (k_proj): Linear(in_features=2048, out_features=256, bias=False)
          (v_proj): Linear(in_features=2048, out_features=256, bias=False)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (vision_tower): CLIPVisionTower(
      (vision_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=2048, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=2048, out_features=2048, bias=True)
    )
  )
  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)
)
trial _save_checkpoint
 None
metrics _save_checkpoint
 None
【COND】tune_mm_mlp_adapter=True
【ENTER】if getattr(self.args, 'tune_mm_mlp_adapter', False):
checkpoint_folder = f"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}"
 checkpoint-1
run_dir = self._get_output_dir(trial=trial) ./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0
output_dir = os.path.join(run_dir, checkpoint_folder) ./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0/checkpoint-1
【COND】use_im_start_end=False
current file path llava/train/llava_trainer.py
def get_mm_adapter_state_maybe_zero_3(named_params, keys_to_match)
named_params get_mm_adapter_state_maybe_zero_3
 <generator object Module.named_parameters at 0x78fb79910120>
keys_to_match get_mm_adapter_state_maybe_zero_3
 ['mm_projector']
to_return = {k: t for k, t in named_params if any(key_match in k for key_match in keys_to_match)}
 {'model.mm_projector.0.weight': Parameter containing:
tensor([[ 0.0126,  0.0311,  0.0186,  ...,  0.0201,  0.0210, -0.0303],
        [ 0.0032,  0.0074, -0.0183,  ..., -0.0094,  0.0083,  0.0019],
        [-0.0034, -0.0060,  0.0156,  ..., -0.0307, -0.0066, -0.0051],
        ...,
        [-0.0117, -0.0134,  0.0284,  ...,  0.0069, -0.0162,  0.0101],
        [-0.0114, -0.0183, -0.0308,  ..., -0.0275, -0.0064,  0.0302],
        [ 0.0282,  0.0294,  0.0106,  ..., -0.0144,  0.0093,  0.0294]],
       requires_grad=True), 'model.mm_projector.0.bias': Parameter containing:
tensor([-0.0171,  0.0275, -0.0112,  ..., -0.0119, -0.0222, -0.0004],
       requires_grad=True), 'model.mm_projector.2.weight': Parameter containing:
tensor([[-0.0083,  0.0207,  0.0154,  ...,  0.0048, -0.0007,  0.0164],
        [-0.0192,  0.0175, -0.0188,  ...,  0.0085,  0.0070,  0.0070],
        [ 0.0182,  0.0204, -0.0099,  ..., -0.0004, -0.0055,  0.0122],
        ...,
        [-0.0065,  0.0134,  0.0134,  ...,  0.0083, -0.0020,  0.0205],
        [-0.0027,  0.0126, -0.0026,  ..., -0.0215,  0.0175, -0.0075],
        [-0.0197, -0.0134,  0.0135,  ...,  0.0185, -0.0042,  0.0207]],
       requires_grad=True), 'model.mm_projector.2.bias': Parameter containing:
tensor([ 0.0080,  0.0217,  0.0111,  ...,  0.0212,  0.0109, -0.0029],
       requires_grad=True)}
current file path llava/train/llava_trainer.py
def maybe_zero_3(param, ignore_status=False, name=None)
param maybe_zero_3
 Parameter containing:
tensor([[ 0.0126,  0.0311,  0.0186,  ...,  0.0201,  0.0210, -0.0303],
        [ 0.0032,  0.0074, -0.0183,  ..., -0.0094,  0.0083,  0.0019],
        [-0.0034, -0.0060,  0.0156,  ..., -0.0307, -0.0066, -0.0051],
        ...,
        [-0.0117, -0.0134,  0.0284,  ...,  0.0069, -0.0162,  0.0101],
        [-0.0114, -0.0183, -0.0308,  ..., -0.0275, -0.0064,  0.0302],
        [ 0.0282,  0.0294,  0.0106,  ..., -0.0144,  0.0093,  0.0294]],
       requires_grad=True)
ignore_status maybe_zero_3
 True
name maybe_zero_3
 model.mm_projector.0.weight
【COND】 hasattr_ds_id=False
【ENTER】else (not hasattr(param, 'ds_id')):
param (after else)
 tensor([[ 0.0126,  0.0311,  0.0186,  ...,  0.0201,  0.0210, -0.0303],
        [ 0.0032,  0.0074, -0.0183,  ..., -0.0094,  0.0083,  0.0019],
        [-0.0034, -0.0060,  0.0156,  ..., -0.0307, -0.0066, -0.0051],
        ...,
        [-0.0117, -0.0134,  0.0284,  ...,  0.0069, -0.0162,  0.0101],
        [-0.0114, -0.0183, -0.0308,  ..., -0.0275, -0.0064,  0.0302],
        [ 0.0282,  0.0294,  0.0106,  ..., -0.0144,  0.0093,  0.0294]])
【EXIT】else (not hasattr(param, 'ds_id')):
param (def maybe_zero_3 at llava_trainer.py return)
 tensor([[ 0.0126,  0.0311,  0.0186,  ...,  0.0201,  0.0210, -0.0303],
        [ 0.0032,  0.0074, -0.0183,  ..., -0.0094,  0.0083,  0.0019],
        [-0.0034, -0.0060,  0.0156,  ..., -0.0307, -0.0066, -0.0051],
        ...,
        [-0.0117, -0.0134,  0.0284,  ...,  0.0069, -0.0162,  0.0101],
        [-0.0114, -0.0183, -0.0308,  ..., -0.0275, -0.0064,  0.0302],
        [ 0.0282,  0.0294,  0.0106,  ..., -0.0144,  0.0093,  0.0294]])
current file path llava/train/llava_trainer.py
def maybe_zero_3(param, ignore_status=False, name=None)
param maybe_zero_3
 Parameter containing:
tensor([-0.0171,  0.0275, -0.0112,  ..., -0.0119, -0.0222, -0.0004],
       requires_grad=True)
ignore_status maybe_zero_3
 True
name maybe_zero_3
 model.mm_projector.0.bias
【COND】 hasattr_ds_id=False
【ENTER】else (not hasattr(param, 'ds_id')):
param (after else)
 tensor([-0.0171,  0.0275, -0.0112,  ..., -0.0119, -0.0222, -0.0004])
【EXIT】else (not hasattr(param, 'ds_id')):
param (def maybe_zero_3 at llava_trainer.py return)
 tensor([-0.0171,  0.0275, -0.0112,  ..., -0.0119, -0.0222, -0.0004])
current file path llava/train/llava_trainer.py
def maybe_zero_3(param, ignore_status=False, name=None)
param maybe_zero_3
 Parameter containing:
tensor([[-0.0083,  0.0207,  0.0154,  ...,  0.0048, -0.0007,  0.0164],
        [-0.0192,  0.0175, -0.0188,  ...,  0.0085,  0.0070,  0.0070],
        [ 0.0182,  0.0204, -0.0099,  ..., -0.0004, -0.0055,  0.0122],
        ...,
        [-0.0065,  0.0134,  0.0134,  ...,  0.0083, -0.0020,  0.0205],
        [-0.0027,  0.0126, -0.0026,  ..., -0.0215,  0.0175, -0.0075],
        [-0.0197, -0.0134,  0.0135,  ...,  0.0185, -0.0042,  0.0207]],
       requires_grad=True)
ignore_status maybe_zero_3
 True
name maybe_zero_3
 model.mm_projector.2.weight
【COND】 hasattr_ds_id=False
【ENTER】else (not hasattr(param, 'ds_id')):
param (after else)
 tensor([[-0.0083,  0.0207,  0.0154,  ...,  0.0048, -0.0007,  0.0164],
        [-0.0192,  0.0175, -0.0188,  ...,  0.0085,  0.0070,  0.0070],
        [ 0.0182,  0.0204, -0.0099,  ..., -0.0004, -0.0055,  0.0122],
        ...,
        [-0.0065,  0.0134,  0.0134,  ...,  0.0083, -0.0020,  0.0205],
        [-0.0027,  0.0126, -0.0026,  ..., -0.0215,  0.0175, -0.0075],
        [-0.0197, -0.0134,  0.0135,  ...,  0.0185, -0.0042,  0.0207]])
【EXIT】else (not hasattr(param, 'ds_id')):
param (def maybe_zero_3 at llava_trainer.py return)
 tensor([[-0.0083,  0.0207,  0.0154,  ...,  0.0048, -0.0007,  0.0164],
        [-0.0192,  0.0175, -0.0188,  ...,  0.0085,  0.0070,  0.0070],
        [ 0.0182,  0.0204, -0.0099,  ..., -0.0004, -0.0055,  0.0122],
        ...,
        [-0.0065,  0.0134,  0.0134,  ...,  0.0083, -0.0020,  0.0205],
        [-0.0027,  0.0126, -0.0026,  ..., -0.0215,  0.0175, -0.0075],
        [-0.0197, -0.0134,  0.0135,  ...,  0.0185, -0.0042,  0.0207]])
current file path llava/train/llava_trainer.py
def maybe_zero_3(param, ignore_status=False, name=None)
param maybe_zero_3
 Parameter containing:
tensor([ 0.0080,  0.0217,  0.0111,  ...,  0.0212,  0.0109, -0.0029],
       requires_grad=True)
ignore_status maybe_zero_3
 True
name maybe_zero_3
 model.mm_projector.2.bias
【COND】 hasattr_ds_id=False
【ENTER】else (not hasattr(param, 'ds_id')):
param (after else)
 tensor([ 0.0080,  0.0217,  0.0111,  ...,  0.0212,  0.0109, -0.0029])
【EXIT】else (not hasattr(param, 'ds_id')):
param (def maybe_zero_3 at llava_trainer.py return)
 tensor([ 0.0080,  0.0217,  0.0111,  ...,  0.0212,  0.0109, -0.0029])
to_return def get_mm_adapter_state_maybe_zero_3 
 {'model.mm_projector.0.weight': tensor([[ 0.0126,  0.0311,  0.0186,  ...,  0.0201,  0.0210, -0.0303],
        [ 0.0032,  0.0074, -0.0183,  ..., -0.0094,  0.0083,  0.0019],
        [-0.0034, -0.0060,  0.0156,  ..., -0.0307, -0.0066, -0.0051],
        ...,
        [-0.0117, -0.0134,  0.0284,  ...,  0.0069, -0.0162,  0.0101],
        [-0.0114, -0.0183, -0.0308,  ..., -0.0275, -0.0064,  0.0302],
        [ 0.0282,  0.0294,  0.0106,  ..., -0.0144,  0.0093,  0.0294]]), 'model.mm_projector.0.bias': tensor([-0.0171,  0.0275, -0.0112,  ..., -0.0119, -0.0222, -0.0004]), 'model.mm_projector.2.weight': tensor([[-0.0083,  0.0207,  0.0154,  ...,  0.0048, -0.0007,  0.0164],
        [-0.0192,  0.0175, -0.0188,  ...,  0.0085,  0.0070,  0.0070],
        [ 0.0182,  0.0204, -0.0099,  ..., -0.0004, -0.0055,  0.0122],
        ...,
        [-0.0065,  0.0134,  0.0134,  ...,  0.0083, -0.0020,  0.0205],
        [-0.0027,  0.0126, -0.0026,  ..., -0.0215,  0.0175, -0.0075],
        [-0.0197, -0.0134,  0.0135,  ...,  0.0185, -0.0042,  0.0207]]), 'model.mm_projector.2.bias': tensor([ 0.0080,  0.0217,  0.0111,  ...,  0.0212,  0.0109, -0.0029])}
to_return['model.mm_projector.0.weight'].shape
 torch.Size([2048, 1024])
to_return['model.mm_projector.0.bias'].shape
 torch.Size([2048])
to_return['model.mm_projector.2.weight'].shape
 torch.Size([2048, 2048])
to_return['model.mm_projector.2.bias'].shape
 torch.Size([2048])
【COND】local_rank=0
【ENTER】if self.args.local_rank == 0 or self.args.local_rank == -1:
【EXIT】if self.args.local_rank == 0 or self.args.local_rank == -1:
【EXIT】if getattr(self.args, 'tune_mm_mlp_adapter', False):
【EXIT】else of if list(pathlib.Path(training_args.output_dir).glob(checkpoint-*)):
model.config.use_cache = True True
【COND】lora_enable=False
【ENTER】else of if training_args.lora_enable:
trainer <__main__.LLaVATrainer object at 0x78fd3c9946a0>
current file path llava/train/train.py
def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str)
trainer safe_save_model_for_hf_trainer
 <class '__main__.LLaVATrainer'>
output_dir safe_save_model_for_hf_trainer
 ./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0
trainer.args safe_save_model_for_hf_trainer
 TrainingArguments(
_n_gpu=0,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
bits=16,
cache_dir=None,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=2,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=False,
double_quant=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
freeze_mm_mlp_adapter=False,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
group_by_modality_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0/runs/Oct01_04-18-40_833b9cbd99be,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_alpha=16,
lora_bias=none,
lora_dropout=0.05,
lora_enable=False,
lora_r=64,
lora_weight_path=,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mm_projector_lr=None,
model_max_length=2048,
mp_parameters=,
mpt_attn_impl=triton,
no_cuda=False,
num_train_epochs=1,
optim=adamw_torch,
optim_args=None,
output_dir=./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
quant_type=nf4,
ray_scope=last,
remove_unused_columns=False,
report_to=[],
resume_from_checkpoint=None,
run_name=./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0,
save_on_each_node=False,
save_safetensors=False,
save_steps=1,
save_strategy=steps,
save_total_limit=1,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=False,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
【COND】tune_mm_mlp_adapter= True
【ENTER】if getattr(trainer.args, 'tune_mm_mlp_adapter', False):
current file path llava/train/llava_trainer.py
def get_mm_adapter_state_maybe_zero_3(named_params, keys_to_match)
named_params get_mm_adapter_state_maybe_zero_3
 <generator object Module.named_parameters at 0x78fb79913370>
keys_to_match get_mm_adapter_state_maybe_zero_3
 ['mm_projector']
to_return = {k: t for k, t in named_params if any(key_match in k for key_match in keys_to_match)}
 {'model.mm_projector.0.weight': Parameter containing:
tensor([[ 0.0126,  0.0311,  0.0186,  ...,  0.0201,  0.0210, -0.0303],
        [ 0.0032,  0.0074, -0.0183,  ..., -0.0094,  0.0083,  0.0019],
        [-0.0034, -0.0060,  0.0156,  ..., -0.0307, -0.0066, -0.0051],
        ...,
        [-0.0117, -0.0134,  0.0284,  ...,  0.0069, -0.0162,  0.0101],
        [-0.0114, -0.0183, -0.0308,  ..., -0.0275, -0.0064,  0.0302],
        [ 0.0282,  0.0294,  0.0106,  ..., -0.0144,  0.0093,  0.0294]],
       requires_grad=True), 'model.mm_projector.0.bias': Parameter containing:
tensor([-0.0171,  0.0275, -0.0112,  ..., -0.0119, -0.0222, -0.0004],
       requires_grad=True), 'model.mm_projector.2.weight': Parameter containing:
tensor([[-0.0083,  0.0207,  0.0154,  ...,  0.0048, -0.0007,  0.0164],
        [-0.0192,  0.0175, -0.0188,  ...,  0.0085,  0.0070,  0.0070],
        [ 0.0182,  0.0204, -0.0099,  ..., -0.0004, -0.0055,  0.0122],
        ...,
        [-0.0065,  0.0134,  0.0134,  ...,  0.0083, -0.0020,  0.0205],
        [-0.0027,  0.0126, -0.0026,  ..., -0.0215,  0.0175, -0.0075],
        [-0.0197, -0.0134,  0.0135,  ...,  0.0185, -0.0042,  0.0207]],
       requires_grad=True), 'model.mm_projector.2.bias': Parameter containing:
tensor([ 0.0080,  0.0217,  0.0111,  ...,  0.0212,  0.0109, -0.0029],
       requires_grad=True)}
current file path llava/train/llava_trainer.py
def maybe_zero_3(param, ignore_status=False, name=None)
param maybe_zero_3
 Parameter containing:
tensor([[ 0.0126,  0.0311,  0.0186,  ...,  0.0201,  0.0210, -0.0303],
        [ 0.0032,  0.0074, -0.0183,  ..., -0.0094,  0.0083,  0.0019],
        [-0.0034, -0.0060,  0.0156,  ..., -0.0307, -0.0066, -0.0051],
        ...,
        [-0.0117, -0.0134,  0.0284,  ...,  0.0069, -0.0162,  0.0101],
        [-0.0114, -0.0183, -0.0308,  ..., -0.0275, -0.0064,  0.0302],
        [ 0.0282,  0.0294,  0.0106,  ..., -0.0144,  0.0093,  0.0294]],
       requires_grad=True)
ignore_status maybe_zero_3
 True
name maybe_zero_3
 model.mm_projector.0.weight
【COND】 hasattr_ds_id=False
【ENTER】else (not hasattr(param, 'ds_id')):
param (after else)
 tensor([[ 0.0126,  0.0311,  0.0186,  ...,  0.0201,  0.0210, -0.0303],
        [ 0.0032,  0.0074, -0.0183,  ..., -0.0094,  0.0083,  0.0019],
        [-0.0034, -0.0060,  0.0156,  ..., -0.0307, -0.0066, -0.0051],
        ...,
        [-0.0117, -0.0134,  0.0284,  ...,  0.0069, -0.0162,  0.0101],
        [-0.0114, -0.0183, -0.0308,  ..., -0.0275, -0.0064,  0.0302],
        [ 0.0282,  0.0294,  0.0106,  ..., -0.0144,  0.0093,  0.0294]])
【EXIT】else (not hasattr(param, 'ds_id')):
param (def maybe_zero_3 at llava_trainer.py return)
 tensor([[ 0.0126,  0.0311,  0.0186,  ...,  0.0201,  0.0210, -0.0303],
        [ 0.0032,  0.0074, -0.0183,  ..., -0.0094,  0.0083,  0.0019],
        [-0.0034, -0.0060,  0.0156,  ..., -0.0307, -0.0066, -0.0051],
        ...,
        [-0.0117, -0.0134,  0.0284,  ...,  0.0069, -0.0162,  0.0101],
        [-0.0114, -0.0183, -0.0308,  ..., -0.0275, -0.0064,  0.0302],
        [ 0.0282,  0.0294,  0.0106,  ..., -0.0144,  0.0093,  0.0294]])
current file path llava/train/llava_trainer.py
def maybe_zero_3(param, ignore_status=False, name=None)
param maybe_zero_3
 Parameter containing:
tensor([-0.0171,  0.0275, -0.0112,  ..., -0.0119, -0.0222, -0.0004],
       requires_grad=True)
ignore_status maybe_zero_3
 True
name maybe_zero_3
 model.mm_projector.0.bias
【COND】 hasattr_ds_id=False
【ENTER】else (not hasattr(param, 'ds_id')):
param (after else)
 tensor([-0.0171,  0.0275, -0.0112,  ..., -0.0119, -0.0222, -0.0004])
【EXIT】else (not hasattr(param, 'ds_id')):
param (def maybe_zero_3 at llava_trainer.py return)
 tensor([-0.0171,  0.0275, -0.0112,  ..., -0.0119, -0.0222, -0.0004])
current file path llava/train/llava_trainer.py
def maybe_zero_3(param, ignore_status=False, name=None)
param maybe_zero_3
 Parameter containing:
tensor([[-0.0083,  0.0207,  0.0154,  ...,  0.0048, -0.0007,  0.0164],
        [-0.0192,  0.0175, -0.0188,  ...,  0.0085,  0.0070,  0.0070],
        [ 0.0182,  0.0204, -0.0099,  ..., -0.0004, -0.0055,  0.0122],
        ...,
        [-0.0065,  0.0134,  0.0134,  ...,  0.0083, -0.0020,  0.0205],
        [-0.0027,  0.0126, -0.0026,  ..., -0.0215,  0.0175, -0.0075],
        [-0.0197, -0.0134,  0.0135,  ...,  0.0185, -0.0042,  0.0207]],
       requires_grad=True)
ignore_status maybe_zero_3
 True
name maybe_zero_3
 model.mm_projector.2.weight
【COND】 hasattr_ds_id=False
【ENTER】else (not hasattr(param, 'ds_id')):
param (after else)
 tensor([[-0.0083,  0.0207,  0.0154,  ...,  0.0048, -0.0007,  0.0164],
        [-0.0192,  0.0175, -0.0188,  ...,  0.0085,  0.0070,  0.0070],
        [ 0.0182,  0.0204, -0.0099,  ..., -0.0004, -0.0055,  0.0122],
        ...,
        [-0.0065,  0.0134,  0.0134,  ...,  0.0083, -0.0020,  0.0205],
        [-0.0027,  0.0126, -0.0026,  ..., -0.0215,  0.0175, -0.0075],
        [-0.0197, -0.0134,  0.0135,  ...,  0.0185, -0.0042,  0.0207]])
【EXIT】else (not hasattr(param, 'ds_id')):
param (def maybe_zero_3 at llava_trainer.py return)
 tensor([[-0.0083,  0.0207,  0.0154,  ...,  0.0048, -0.0007,  0.0164],
        [-0.0192,  0.0175, -0.0188,  ...,  0.0085,  0.0070,  0.0070],
        [ 0.0182,  0.0204, -0.0099,  ..., -0.0004, -0.0055,  0.0122],
        ...,
        [-0.0065,  0.0134,  0.0134,  ...,  0.0083, -0.0020,  0.0205],
        [-0.0027,  0.0126, -0.0026,  ..., -0.0215,  0.0175, -0.0075],
        [-0.0197, -0.0134,  0.0135,  ...,  0.0185, -0.0042,  0.0207]])
current file path llava/train/llava_trainer.py
def maybe_zero_3(param, ignore_status=False, name=None)
param maybe_zero_3
 Parameter containing:
tensor([ 0.0080,  0.0217,  0.0111,  ...,  0.0212,  0.0109, -0.0029],
       requires_grad=True)
ignore_status maybe_zero_3
 True
name maybe_zero_3
 model.mm_projector.2.bias
【COND】 hasattr_ds_id=False
【ENTER】else (not hasattr(param, 'ds_id')):
param (after else)
 tensor([ 0.0080,  0.0217,  0.0111,  ...,  0.0212,  0.0109, -0.0029])
【EXIT】else (not hasattr(param, 'ds_id')):
param (def maybe_zero_3 at llava_trainer.py return)
 tensor([ 0.0080,  0.0217,  0.0111,  ...,  0.0212,  0.0109, -0.0029])
to_return def get_mm_adapter_state_maybe_zero_3 
 {'model.mm_projector.0.weight': tensor([[ 0.0126,  0.0311,  0.0186,  ...,  0.0201,  0.0210, -0.0303],
        [ 0.0032,  0.0074, -0.0183,  ..., -0.0094,  0.0083,  0.0019],
        [-0.0034, -0.0060,  0.0156,  ..., -0.0307, -0.0066, -0.0051],
        ...,
        [-0.0117, -0.0134,  0.0284,  ...,  0.0069, -0.0162,  0.0101],
        [-0.0114, -0.0183, -0.0308,  ..., -0.0275, -0.0064,  0.0302],
        [ 0.0282,  0.0294,  0.0106,  ..., -0.0144,  0.0093,  0.0294]]), 'model.mm_projector.0.bias': tensor([-0.0171,  0.0275, -0.0112,  ..., -0.0119, -0.0222, -0.0004]), 'model.mm_projector.2.weight': tensor([[-0.0083,  0.0207,  0.0154,  ...,  0.0048, -0.0007,  0.0164],
        [-0.0192,  0.0175, -0.0188,  ...,  0.0085,  0.0070,  0.0070],
        [ 0.0182,  0.0204, -0.0099,  ..., -0.0004, -0.0055,  0.0122],
        ...,
        [-0.0065,  0.0134,  0.0134,  ...,  0.0083, -0.0020,  0.0205],
        [-0.0027,  0.0126, -0.0026,  ..., -0.0215,  0.0175, -0.0075],
        [-0.0197, -0.0134,  0.0135,  ...,  0.0185, -0.0042,  0.0207]]), 'model.mm_projector.2.bias': tensor([ 0.0080,  0.0217,  0.0111,  ...,  0.0212,  0.0109, -0.0029])}
to_return['model.mm_projector.0.weight'].shape
 torch.Size([2048, 1024])
to_return['model.mm_projector.0.bias'].shape
 torch.Size([2048])
to_return['model.mm_projector.2.weight'].shape
 torch.Size([2048, 2048])
to_return['model.mm_projector.2.bias'].shape
 torch.Size([2048])
current_folder = output_dir.split('/')[-1] llava-TinyLlama-1.1B-Chat-v1.0
parent_folder = os.path.dirname(output_dir)
 ./checkpoints
【COND】trainer.args.local_rank= 0
【ENTER】if trainer.args.local_rank == 0 or trainer.args.local_rank == -1:
Adapter weights saved to ./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0/mm_projector.bin
【EXIT】if trainer.args.local_rank == 0 or trainer.args.local_rank == -1:
【EXIT】if getattr(trainer.args, 'tune_mm_mlp_adapter', False):
【EXIT】else of if training_args.lora_enable: