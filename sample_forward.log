(llava) ubuntu@157-151-224-68:~/LLaVA$ bash scripts/v1_5/pretrain.sh
[2025-09-08 12:26:42,501] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-08 12:26:44,448] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-09-08 12:26:44,448] [INFO] [runner.py:555:main] cmd = /home/ubuntu/miniconda3/envs/llava/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero2.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version plain --data_path /home/ubuntu/llava-virginia/blip_laion_cc_sbu_1.json --image_folder /home/ubuntu/llava-virginia/images --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --tune_mm_mlp_adapter True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir ./checkpoints/llava-v1.5-7b-pretrain --num_train_epochs 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 1 --save_total_limit 1 --learning_rate 1e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 2 --lazy_preprocess True --report_to wandb
[2025-09-08 12:26:45,707] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-08 12:26:47,626] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2025-09-08 12:26:47,626] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-09-08 12:26:47,626] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-09-08 12:26:47,626] [INFO] [launch.py:163:main] dist_world_size=1
[2025-09-08 12:26:47,626] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-09-08 12:26:50,217] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-08 12:26:50,864] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2025-09-08 12:26:50,864] [INFO] [comm.py:594:init_distributed] cdb=None
[2025-09-08 12:26:50,864] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
config.json: 100%|██████████████████████████████████████████████████████████████████████| 615/615 [00:00<00:00, 5.15MB/s]
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
pytorch_model.bin.index.json: 26.8kB [00:00, 67.1MB/s]
pytorch_model-00001-of-00002.bin: 100%|██████████████████████████████████████████████| 9.98G/9.98G [00:11<00:00, 878MB/s]
pytorch_model-00002-of-00002.bin: 100%|██████████████████████████████████████████████| 3.50G/3.50G [00:05<00:00, 587MB/s]
Downloading shards: 100%|██████████████████████████████████████████████████████████████████| 2/2 [00:17<00:00,  8.70s/it]
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.__init__(self, config)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
config
 LlavaConfig {
  "_name_or_path": "lmsys/vicuna-7b-v1.5",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llava_llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "use_cache": true,
  "vocab_size": 32000
}

current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaModel.__init__(self, config: LlamaConfig)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaModel'>
config
 LlavaConfig {
  "_name_or_path": "lmsys/vicuna-7b-v1.5",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llava_llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "use_cache": true,
  "vocab_size": 32000
}

current file path llava/model/llava_arch.py
def __init__(self, config)
config
 LlavaConfig {
  "_name_or_path": "lmsys/vicuna-7b-v1.5",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llava_llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.25s/it]
generation_config.json: 100%|███████████████████████████████████████████████████████████| 162/162 [00:00<00:00, 1.12MB/s]
tokenizer_config.json: 100%|████████████████████████████████████████████████████████████| 749/749 [00:00<00:00, 5.50MB/s]
tokenizer.model: 100%|████████████████████████████████████████████████████████████████| 500k/500k [00:00<00:00, 4.73MB/s]
special_tokens_map.json: 100%|██████████████████████████████████████████████████████████| 438/438 [00:00<00:00, 2.84MB/s]
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 4096, padding_idx=0)
  (layers): ModuleList(
    (0-31): 32 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
)
current file path llava/model/llava_arch.py
def initialize_vision_modules(self, model_args, fsdp=None)
model_args
 ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')
fsdp
 []
current file path llava/model/llava_arch.py
def get_vision_tower(self)
vision_tower (raw)
 None
vision_tower (return)
 None
current file path llava/llava/model/multimodal_encoder/builder.py
def build_vision_tower(vision_tower_cfg, **kwargs)
vision_tower_cfg
 ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')
kwargs
 {}
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.__init__(self, vision_tower, args, delay_load=False)
self
 <class 'llava.model.multimodal_encoder.clip_encoder.CLIPVisionTower'>
vision_tower
 openai/clip-vit-large-patch14-336
args
 ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')
delay_load
 False
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.load_model(self)
self
 <class 'llava.model.multimodal_encoder.clip_encoder.CLIPVisionTower'>
preprocessor_config.json: 100%|█████████████████████████████████████████████████████████| 316/316 [00:00<00:00, 3.63MB/s]
config.json: 4.76kB [00:00, 32.3MB/s]
pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████| 1.71G/1.71G [00:02<00:00, 599MB/s]
result (return)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.hidden_size(self)
self
 <class 'llava.model.multimodal_encoder.clip_encoder.CLIPVisionTower'>
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.config(self)
self
 <class 'llava.model.multimodal_encoder.clip_encoder.CLIPVisionTower'>
result (return)
 CLIPVisionConfig {
  "_name_or_path": "openai/clip-vit-large-patch14-336",
  "attention_dropout": 0.0,
  "dropout": 0.0,
  "hidden_act": "quick_gelu",
  "hidden_size": 1024,
  "image_size": 336,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "model_type": "clip_vision_model",
  "num_attention_heads": 16,
  "num_channels": 3,
  "num_hidden_layers": 24,
  "patch_size": 14,
  "projection_dim": 768,
  "transformers_version": "4.31.0"
}

result (return)
 1024
current file path llava/llava/model/multimodal_projector/builder.py
def build_vision_projector(config, delay_load=False, **kwargs)
config
 LlavaConfig {
  "_name_or_path": "lmsys/vicuna-7b-v1.5",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_patch_merge_type": "flat",
  "mm_projector_type": "mlp2x_gelu",
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14-336",
  "model_type": "llava_llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "use_cache": false,
  "use_mm_proj": true,
  "vocab_size": 32000
}

delay_load
 False
kwargs
 {}
result (return)
 Sequential(
  (0): Linear(in_features=1024, out_features=4096, bias=True)
  (1): GELU(approximate='none')
  (2): Linear(in_features=4096, out_features=4096, bias=True)
)
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 4096, padding_idx=0)
  (layers): ModuleList(
    (0-31): 32 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=4096, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=4096, out_features=4096, bias=True)
  )
)
current file path llava/model/llava_arch.py
def get_vision_tower(self)
vision_tower (raw)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
vision_tower (return)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 4096, padding_idx=0)
  (layers): ModuleList(
    (0-31): 32 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=4096, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=4096, out_features=4096, bias=True)
  )
)
current file path llava/train/train.py
def rank0_print(*args)
args
 ('Formatting inputs...Skip in lazy mode',)
Formatting inputs...Skip in lazy mode
current file path llava/train/llava_trainer.py
def _get_train_sampler(self)
self
 <llava.train.llava_trainer.LLaVATrainer object at 0x79ca141a45e0>
current file path llava/train/llava_trainer.py
def create_optimizer(self)
self
 <llava.train.llava_trainer.LLaVATrainer object at 0x79ca141a45e0>
print(risk): print(self.args) disabled for safety
print(risk): print(opt_model) disabled for safety
print(risk): print(optimizer_grouped_parameters) disabled for safety
print(risk): print(optimizer_cls) disabled for safety
print(risk): print(optimizer_kwargs) disabled for safety
self.optimizer
 AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
current file path llava/train/llava_trainer.py
def create_optimizer(self)
self
 <llava.train.llava_trainer.LLaVATrainer object at 0x79ca141a45e0>
self.optimizer
 AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.0
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.0
    maximize: False
    weight_decay: 0.0
)
Rank: 0 partition count [1, 1] and sizes[(20971520, False), (8192, False)] 
wandb: (1) Create a W&B account
wandb: (2) Use an existing W&B account
wandb: (3) Don't visualize my results
wandb: Enter your choice: 3
wandb: You chose "Don't visualize my results"
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
  0%|                                                                                              | 0/1 [00:00<?, ?it/s]current file path llava/mm_utils.py
def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None)
prompt
 <image>the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair

tokenizer
 LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)
image_token_index
 -200
return_tensors
 pt
input_ids[0].shape
 torch.Size([24])
targets[0].shape
 torch.Size([24])
current file path llava/mm_utils.py
def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None)
prompt
 <image>
tokenizer
 LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)
image_token_index
 -200
return_tensors
 None
input_ids (return)
 [1, -200]
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, image_sizes, return_dict)
input_ids
 tensor([[    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
           411,  2654, 11315,    13]], device='cuda:0')
input_ids.shape
 torch.Size([1, 24])
attention_mask
 tensor([[True, True, True, True, True, True, True, True, True, True, True, True,
         True, True, True, True, True, True, True, True, True, True, True, True]],
       device='cuda:0')
position_ids
 None
past_key_values
 None
inputs_embeds
 None
labels
 tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,
         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,
           411,  2654, 11315,    13]], device='cuda:0')
use_cache
 None
output_attentions
 None
output_hidden_states
 None
images
 tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7109, -0.3613, -0.1279],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3906, -0.1719, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0112,  0.0471,  0.0908],
          ...,
          [-1.0312, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625]],

         [[ 0.3184,  0.3184,  0.3184,  ..., -0.3867, -0.0112,  0.2139],
          [ 0.3184,  0.3184,  0.3184,  ..., -0.0713,  0.1543,  0.3184],
          [ 0.3184,  0.3184,  0.3184,  ...,  0.2891,  0.3633,  0.4395],
          ...,
          [-1.0156, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000]],

         [[ 0.9648,  0.9648,  0.9648,  ...,  0.0981,  0.4531,  0.6680],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.3965,  0.6094,  0.7539],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.7539,  0.8086,  0.8359],
          ...,
          [-0.3711, -0.3848, -0.4004,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3711, -0.3711, -0.3848,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3848, -0.3711, -0.3711,  ..., -0.4277, -0.4277, -0.4277]]]],
       device='cuda:0', dtype=torch.bfloat16)
images.shape
 torch.Size([1, 3, 336, 336])
image_sizes
 None
return_dict
 None
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 4096, padding_idx=0)
  (layers): ModuleList(
    (0-31): 32 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=4096, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=4096, out_features=4096, bias=True)
  )
)
current file path llava/model/llava_arch.py
def get_vision_tower(self)
vision_tower (raw)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
vision_tower (return)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 4096, padding_idx=0)
  (layers): ModuleList(
    (0-31): 32 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=4096, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=4096, out_features=4096, bias=True)
  )
)
current file path llava/model/llava_arch.py
def get_vision_tower(self)
vision_tower (raw)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
vision_tower (return)
 CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.forward(self, images)
images
 tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7109, -0.3613, -0.1279],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.3906, -0.1719, -0.0259],
          [ 0.0325,  0.0325,  0.0325,  ..., -0.0112,  0.0471,  0.0908],
          ...,
          [-1.0312, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],
          [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625]],

         [[ 0.3184,  0.3184,  0.3184,  ..., -0.3867, -0.0112,  0.2139],
          [ 0.3184,  0.3184,  0.3184,  ..., -0.0713,  0.1543,  0.3184],
          [ 0.3184,  0.3184,  0.3184,  ...,  0.2891,  0.3633,  0.4395],
          ...,
          [-1.0156, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],
          [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000]],

         [[ 0.9648,  0.9648,  0.9648,  ...,  0.0981,  0.4531,  0.6680],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.3965,  0.6094,  0.7539],
          [ 0.9648,  0.9648,  0.9648,  ...,  0.7539,  0.8086,  0.8359],
          ...,
          [-0.3711, -0.3848, -0.4004,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3711, -0.3711, -0.3848,  ..., -0.4277, -0.4277, -0.4277],
          [-0.3848, -0.3711, -0.3711,  ..., -0.4277, -0.4277, -0.4277]]]],
       device='cuda:0', dtype=torch.bfloat16)
images.shape
 torch.Size([1, 3, 336, 336])
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.device(self)
self
 <class 'llava.model.multimodal_encoder.clip_encoder.CLIPVisionTower'>
result (return)
 cuda:0
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.dtype(self)
self
 <class 'llava.model.multimodal_encoder.clip_encoder.CLIPVisionTower'>
result (return)
 torch.bfloat16
current file path llava/llava/model/multimodal_encoder/clip_encoder.py
def CLIPVisionTower.feature_select(self, image_forward_outs)
image_forward_outs
 BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.3926, -0.0566, -0.2051,  ...,  0.4980, -0.7695, -0.0625],
         [ 0.3828, -0.2754,  0.3906,  ...,  0.0303,  0.1455,  0.2754],
         [-0.1113,  1.1250,  1.0859,  ...,  0.2656, -0.2520, -0.8203],
         ...,
         [ 1.9453,  0.9219,  1.6719,  ..., -0.2334, -0.5625,  1.2812],
         [ 1.0078, -0.2295,  1.3359,  ..., -0.5312, -0.9922,  0.3477],
         [ 1.6719,  0.9570,  1.1094,  ..., -0.7812, -0.3750,  0.9492]]],
       device='cuda:0', dtype=torch.bfloat16), pooler_output=tensor([[ 0.7617,  0.0408, -0.4414,  ...,  1.1250, -1.6016, -0.1167]],
       device='cuda:0', dtype=torch.bfloat16), hidden_states=(tensor([[[ 0.0342, -0.0408, -0.1670,  ...,  0.3203, -0.1475, -0.0201],
         [-0.1167, -0.0491, -0.2637,  ...,  0.5859, -0.1318, -0.0099],
         [ 0.2451, -0.0437, -0.6133,  ...,  0.3477, -0.1445, -0.0098],
         ...,
         [ 0.1074, -0.0439, -0.1309,  ..., -0.0601, -0.1357, -0.0164],
         [ 0.1797, -0.0452, -0.1074,  ..., -0.0811, -0.1348, -0.0111],
         [ 0.0266, -0.0486, -0.0238,  ..., -0.0114, -0.1445, -0.0155]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.0513,  0.1045, -0.1074,  ...,  0.1152,  0.0723,  0.0322],
         [ 0.0391,  0.0635, -0.1592,  ...,  0.3496, -0.0361, -0.0442],
         [ 0.2480,  0.0557, -0.4141,  ...,  0.1582,  0.0967,  0.1113],
         ...,
         [ 0.0640, -0.0659,  0.0269,  ..., -0.2852, -0.0283, -0.0420],
         [ 0.1729, -0.1162,  0.0181,  ..., -0.2891, -0.0049, -0.0574],
         [ 0.0977,  0.0244, -0.0283,  ..., -0.0527,  0.0107, -0.0073]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.0415,  0.0073, -0.0889,  ...,  0.0664, -0.0334,  0.0476],
         [ 0.1201,  0.0059, -0.0898,  ...,  0.2324,  0.1152, -0.0430],
         [ 0.2119, -0.0593, -0.2832,  ...,  0.1514,  0.0977,  0.1992],
         ...,
         [-0.0527, -0.1914,  0.0000,  ..., -0.4453, -0.1162, -0.1050],
         [ 0.0117, -0.2793, -0.0059,  ..., -0.4883, -0.0732, -0.1416],
         [ 0.0815,  0.0679, -0.0757,  ..., -0.1523,  0.0603, -0.0087]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-4.3701e-02, -1.9531e-03, -5.1270e-02,  ...,  4.8096e-02,
           2.4414e-04,  9.4727e-02],
         [ 1.8262e-01, -3.6133e-02, -1.7090e-02,  ...,  2.8516e-01,
           3.0078e-01, -2.8809e-02],
         [ 2.5000e-01,  3.6133e-02, -6.6406e-02,  ...,  1.9141e-01,
           1.5039e-01,  3.4766e-01],
         ...,
         [-1.9336e-01, -2.1484e-01, -3.6621e-02,  ..., -4.1016e-01,
          -1.5430e-01, -5.4688e-02],
         [-4.7852e-02, -2.7539e-01,  3.8574e-02,  ..., -4.1992e-01,
          -1.3672e-01, -2.3242e-01],
         [-7.3242e-02,  2.7930e-01, -1.6895e-01,  ..., -1.5430e-01,
           1.7969e-01,  5.5176e-02]]], device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0791, -0.0151,  0.0153,  ...,  0.0811, -0.0127,  0.1055],
         [ 0.2617,  0.0234, -0.0078,  ...,  0.3105,  0.3457, -0.0234],
         [ 0.0728, -0.2021, -0.0781,  ...,  0.2695,  0.0840,  0.3906],
         ...,
         [-0.2119, -0.3672, -0.1543,  ..., -0.3223, -0.2314, -0.1309],
         [ 0.0337, -0.4688, -0.0571,  ..., -0.3926, -0.2500, -0.2852],
         [-0.0190,  0.3535, -0.1060,  ..., -0.1089,  0.1133,  0.0396]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0791, -0.0437,  0.0469,  ...,  0.0986,  0.1113,  0.1328],
         [ 0.0850,  0.0151,  0.1533,  ...,  0.2988,  0.2598, -0.1006],
         [-0.0603, -0.2324,  0.0549,  ...,  0.3633,  0.0493,  0.4473],
         ...,
         [-0.1055, -0.3867,  0.0304,  ..., -0.2012, -0.1152, -0.1406],
         [ 0.0718, -0.5859,  0.1582,  ..., -0.3438, -0.2617, -0.1680],
         [ 0.0227,  0.3047, -0.1895,  ..., -0.1270,  0.1348,  0.1660]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0229,  0.0454,  0.0537,  ...,  0.0559,  0.1309,  0.0645],
         [-0.1396,  0.1514,  0.0649,  ...,  0.4512,  0.2656, -0.1074],
         [-0.0283, -0.1748,  0.0381,  ...,  0.4082, -0.0879,  0.4023],
         ...,
         [-0.2500, -0.3066,  0.1836,  ..., -0.2266, -0.3008, -0.3145],
         [-0.0181, -0.6250,  0.2070,  ..., -0.3047, -0.2773, -0.2246],
         [ 0.0781,  0.3477, -0.2637,  ..., -0.1079,  0.1768,  0.1074]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 3.2959e-02,  1.5381e-02,  5.6396e-02,  ...,  7.9102e-02,
           8.3984e-02,  5.3223e-02],
         [-3.9062e-01,  6.4453e-02,  5.8594e-02,  ...,  7.2656e-01,
           2.1973e-01, -8.3496e-02],
         [ 1.5137e-01, -2.0605e-01,  1.8945e-01,  ...,  6.5625e-01,
          -2.4023e-01,  3.1250e-02],
         ...,
         [-2.4414e-01, -1.2207e-01,  4.0625e-01,  ..., -1.3184e-02,
          -3.9844e-01, -4.2969e-02],
         [ 1.3184e-02, -6.0156e-01,  3.4766e-01,  ..., -1.0156e-01,
          -3.2812e-01, -1.1475e-01],
         [ 1.3770e-01,  2.8516e-01, -4.8828e-04,  ...,  2.9053e-02,
           3.0078e-01,  1.9043e-01]]], device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.0078, -0.0359,  0.0630,  ...,  0.1738,  0.0801,  0.0483],
         [-0.3105, -0.0420, -0.0649,  ...,  0.8047,  0.1406, -0.2080],
         [ 0.0747, -0.4062,  0.3340,  ...,  0.8594, -0.1592, -0.1582],
         ...,
         [-0.2324, -0.2832,  0.4414,  ..., -0.1187, -0.5625,  0.2139],
         [-0.2002, -0.6484,  0.3379,  ..., -0.2012, -0.3828, -0.0962],
         [ 0.0078,  0.3164, -0.3223,  ...,  0.0223, -0.0674,  0.0049]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0293, -0.0152,  0.0259,  ...,  0.1309,  0.0183, -0.0393],
         [-0.1050, -0.1914, -0.1230,  ...,  0.8047,  0.3242, -0.1162],
         [ 0.0601, -0.3457,  0.3594,  ...,  0.8906, -0.2422,  0.0596],
         ...,
         [-0.2451, -0.4023,  0.0840,  ..., -0.3184, -0.6719,  0.3848],
         [-0.3750, -0.7930, -0.0449,  ..., -0.4004, -0.7734,  0.0133],
         [ 0.1377,  0.2539, -0.3672,  ..., -0.1914, -0.0449, -0.0830]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0199, -0.0488,  0.0977,  ...,  0.1201, -0.0154,  0.0889],
         [ 0.0820,  0.1895, -0.1914,  ...,  0.5625,  0.3359,  0.0737],
         [ 0.0618,  0.0371,  0.1416,  ...,  0.8516, -0.0825,  0.0029],
         ...,
         [-0.3281, -0.3242,  0.2539,  ..., -0.1562, -0.6289,  0.3203],
         [-0.3145, -0.5742,  0.2559,  ..., -0.3203, -0.5352,  0.0498],
         [ 0.2266,  0.2480, -0.0879,  ..., -0.1699,  0.0913, -0.1147]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.0635, -0.0757,  0.0557,  ...,  0.0669,  0.0361,  0.0664],
         [ 0.0525,  0.2217, -0.0425,  ...,  0.5039,  0.5273,  0.0464],
         [ 0.0085,  0.1768,  0.0605,  ...,  0.9219, -0.1309, -0.0786],
         ...,
         [-0.2471, -0.3516,  0.2539,  ...,  0.0156, -0.5000,  0.0684],
         [-0.2246, -0.4219,  0.0957,  ..., -0.2578, -0.4688, -0.3086],
         [-0.0059,  0.1660,  0.3242,  ..., -0.0918,  0.0000, -0.3086]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0020, -0.1914, -0.1504,  ...,  0.0952, -0.0635,  0.0156],
         [-0.0620,  0.2363, -0.1914,  ...,  0.5000,  0.3008,  0.1992],
         [-0.0479,  0.2188,  0.0669,  ...,  0.9297, -0.1436, -0.0142],
         ...,
         [-0.2891, -0.1006,  0.3105,  ..., -0.1040, -0.5195,  0.0713],
         [-0.0388, -0.5039,  0.1133,  ..., -0.2266, -0.5117, -0.5586],
         [ 0.0127,  0.1758,  0.3340,  ..., -0.2500,  0.0188, -0.1582]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0957, -0.1621, -0.0874,  ...,  0.1128, -0.0439,  0.0625],
         [-0.0347,  0.1836,  0.0596,  ...,  0.3828,  0.2559,  0.2090],
         [-0.0791,  0.1621, -0.0967,  ...,  0.7031, -0.2305, -0.2109],
         ...,
         [-0.2236,  0.0317,  0.2412,  ..., -0.0254, -0.0986, -0.0027],
         [ 0.0630, -0.3438,  0.0820,  ..., -0.2891, -0.1074, -0.4551],
         [ 0.2578,  0.2363,  0.3359,  ..., -0.2559,  0.1455, -0.2344]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0840, -0.1104, -0.1099,  ...,  0.1074, -0.0608, -0.0303],
         [ 0.0471,  0.1641,  0.2354,  ...,  0.1045,  0.1836,  0.2070],
         [-0.1152,  0.1582, -0.2969,  ...,  0.3242, -0.1279, -0.1875],
         ...,
         [ 0.1152,  0.0225,  0.3242,  ..., -0.3711,  0.0330,  0.0049],
         [ 0.0859, -0.3750,  0.0898,  ..., -0.5625, -0.0723, -0.2197],
         [ 0.2871,  0.1045,  0.5117,  ..., -0.5391,  0.1758, -0.2070]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0874, -0.1621, -0.0811,  ...,  0.1396, -0.0361,  0.0850],
         [ 0.0552,  0.2070,  0.2715,  ..., -0.2266,  0.3477,  0.4434],
         [-0.2910,  0.2559, -0.1426,  ...,  0.1074, -0.0417,  0.0420],
         ...,
         [ 0.3047,  0.0737,  0.3789,  ...,  0.0664, -0.1260,  0.3086],
         [-0.0255, -0.4805,  0.2227,  ..., -0.0139, -0.3477, -0.1689],
         [ 0.3887,  0.0400,  0.5898,  ..., -0.5078,  0.0264,  0.0015]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-2.0508e-02, -1.2793e-01, -1.0938e-01,  ...,  3.9062e-03,
          -7.0312e-02,  1.4648e-01],
         [-1.9531e-02,  3.3203e-01,  2.5000e-01,  ..., -1.2207e-04,
           2.6562e-01,  5.1172e-01],
         [-2.6367e-01,  4.5898e-01, -2.5586e-01,  ...,  2.8711e-01,
          -3.0762e-02,  9.8633e-02],
         ...,
         [ 4.4531e-01,  2.8906e-01,  2.2559e-01,  ...,  2.5586e-01,
          -1.2891e-01,  2.9492e-01],
         [-7.8125e-02, -2.1582e-01,  2.0312e-01,  ...,  5.5664e-02,
          -3.8281e-01, -3.7891e-01],
         [ 5.5469e-01,  2.4609e-01,  5.4688e-01,  ..., -5.5469e-01,
           2.5391e-02,  1.0547e-01]]], device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.0444,  0.0947,  0.0210,  ..., -0.1357, -0.2324,  0.0718],
         [-0.0559, -0.1777,  0.2119,  ..., -0.1641,  0.3203,  0.3984],
         [-0.1748,  0.2471, -0.3320,  ...,  0.0986,  0.1021, -0.0972],
         ...,
         [ 0.5234,  0.4746,  0.2051,  ...,  0.1660, -0.0527,  0.2793],
         [ 0.0400,  0.0029,  0.1826,  ..., -0.2158, -0.3750, -0.3984],
         [ 0.5469,  0.2656,  0.4375,  ..., -0.5234, -0.1641,  0.0054]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.1318,  0.0918, -0.0099,  ..., -0.0718, -0.0293,  0.0776],
         [-0.1289, -0.1719, -0.1719,  ..., -0.2773,  0.2715,  0.4258],
         [-0.2227,  0.2695, -0.3027,  ...,  0.0540,  0.3203, -0.0581],
         ...,
         [ 0.5547,  0.6562,  0.4102,  ...,  0.1245, -0.1777,  0.3164],
         [-0.1562, -0.1143, -0.0254,  ..., -0.2119, -0.3633, -0.5078],
         [ 0.5273,  0.2178,  0.1523,  ..., -0.5000, -0.0762,  0.1221]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[-0.1719,  0.1270, -0.0293,  ...,  0.0132, -0.1270, -0.0059],
         [-0.2500, -0.6719, -0.2773,  ..., -0.0088,  0.6289,  0.1934],
         [-0.5195,  0.1963, -0.1387,  ...,  0.0435,  0.4082, -0.0762],
         ...,
         [ 0.5391,  0.7188,  0.4590,  ...,  0.2910, -0.1982,  0.2793],
         [-0.1108, -0.4355,  0.0649,  ..., -0.1396, -0.4512, -0.3320],
         [ 0.6250,  0.0820,  0.2100,  ..., -0.2158, -0.3320,  0.3203]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.2207,  0.3359, -0.2021,  ..., -0.0181, -0.1562,  0.1973],
         [-0.2256, -0.4062, -0.0879,  ..., -0.2031,  0.4688,  0.2012],
         [-0.2539,  0.4688,  0.2051,  ...,  0.0532,  0.1289,  0.0061],
         ...,
         [ 0.7891,  0.7305,  0.5352,  ...,  0.2344, -0.3203,  0.3359],
         [ 0.2324, -0.5312,  0.1602,  ..., -0.2402, -0.5781, -0.2695],
         [ 1.1016,  0.2715,  0.3008,  ..., -0.2754, -0.5703,  0.6055]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.2412,  0.2930, -0.2734,  ...,  0.2412, -0.2119,  0.3594],
         [-0.3008, -0.4062,  0.5195,  ..., -0.0208,  0.5156,  0.1719],
         [ 0.3242,  0.8516,  0.0449,  ..., -0.0198, -0.3672, -0.2520],
         ...,
         [ 1.4766,  1.2656,  1.0312,  ..., -0.4336, -0.7344,  0.4648],
         [ 1.0781, -0.0664,  1.0781,  ..., -0.9922, -0.6836, -0.4004],
         [ 1.7891,  0.8359,  0.8945,  ..., -1.1172, -0.3340,  0.3203]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.1650,  0.3223, -0.2383,  ...,  0.3867, -0.3438,  0.0703],
         [-0.0684, -0.1299,  0.4023,  ...,  0.3145,  0.4844,  0.3926],
         [ 0.1445,  0.8867,  0.5703,  ...,  0.4219, -0.1836, -0.4023],
         ...,
         [ 1.4062,  1.1797,  1.4688,  ...,  0.0469, -0.6367,  1.0234],
         [ 0.9453, -0.0439,  1.4062,  ..., -0.3633, -0.7383, -0.1465],
         [ 1.5000,  0.9766,  0.9531,  ..., -0.6289, -0.3906,  0.8281]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.2236,  0.2432, -0.5938,  ...,  0.4863, -0.5273, -0.2041],
         [-0.0469, -0.1836, -0.0273,  ...,  0.3535,  0.3750,  0.3047],
         [-0.2598,  1.1484,  0.4844,  ...,  0.4961, -0.1719, -0.5117],
         ...,
         [ 1.7188,  0.9688,  0.8828,  ..., -0.2441, -0.8672,  1.3047],
         [ 0.7891, -0.3984,  0.6797,  ..., -0.3594, -0.9922,  0.3164],
         [ 1.5000,  0.6250,  0.3672,  ..., -0.5469, -0.4902,  0.9766]]],
       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.3926, -0.0566, -0.2051,  ...,  0.4980, -0.7695, -0.0625],
         [ 0.3828, -0.2754,  0.3906,  ...,  0.0303,  0.1455,  0.2754],
         [-0.1113,  1.1250,  1.0859,  ...,  0.2656, -0.2520, -0.8203],
         ...,
         [ 1.9453,  0.9219,  1.6719,  ..., -0.2334, -0.5625,  1.2812],
         [ 1.0078, -0.2295,  1.3359,  ..., -0.5312, -0.9922,  0.3477],
         [ 1.6719,  0.9570,  1.1094,  ..., -0.7812, -0.3750,  0.9492]]],
       device='cuda:0', dtype=torch.bfloat16)), attentions=None)
image_features.shape
 torch.Size([1, 577, 1024])
image_features (return)
 tensor([[[-0.0469, -0.1836, -0.0273,  ...,  0.3535,  0.3750,  0.3047],
         [-0.2598,  1.1484,  0.4844,  ...,  0.4961, -0.1719, -0.5117],
         [ 1.0625, -0.0635, -0.3730,  ...,  0.0220,  0.0820,  0.4805],
         ...,
         [ 1.7188,  0.9688,  0.8828,  ..., -0.2441, -0.8672,  1.3047],
         [ 0.7891, -0.3984,  0.6797,  ..., -0.3594, -0.9922,  0.3164],
         [ 1.5000,  0.6250,  0.3672,  ..., -0.5469, -0.4902,  0.9766]]],
       device='cuda:0', dtype=torch.bfloat16)
image_features.shape
 torch.Size([1, 576, 1024])
image_features (return)
 tensor([[[-0.0469, -0.1836, -0.0273,  ...,  0.3535,  0.3750,  0.3047],
         [-0.2598,  1.1484,  0.4844,  ...,  0.4961, -0.1719, -0.5117],
         [ 1.0625, -0.0635, -0.3730,  ...,  0.0220,  0.0820,  0.4805],
         ...,
         [ 1.7188,  0.9688,  0.8828,  ..., -0.2441, -0.8672,  1.3047],
         [ 0.7891, -0.3984,  0.6797,  ..., -0.3594, -0.9922,  0.3164],
         [ 1.5000,  0.6250,  0.3672,  ..., -0.5469, -0.4902,  0.9766]]],
       device='cuda:0', dtype=torch.bfloat16)
image_features.shape
 torch.Size([1, 576, 1024])
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 4096, padding_idx=0)
  (layers): ModuleList(
    (0-31): 32 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=4096, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=4096, out_features=4096, bias=True)
  )
)
current file path llava/llava/model/language_model/llava_llama.py
def LlavaLlamaForCausalLM.get_model(self)
self
 <class 'llava.model.language_model.llava_llama.LlavaLlamaForCausalLM'>
self.model (return)
 LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 4096, padding_idx=0)
  (layers): ModuleList(
    (0-31): 32 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=4096, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=4096, out_features=4096, bias=True)
  )
)
result (return)
 CausalLMOutputWithPast(loss=tensor(8.8108, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[  0.8242,   0.1855,  -0.7031,  ...,   1.6719,   2.6719,   1.1875],
         [ -9.2500,  -3.9844,   6.6875,  ...,  -5.0000,  -7.3125,  -5.8438],
         [-10.5000,  -3.8438,   4.0625,  ...,  -8.2500, -11.2500,  -9.8750],
         ...,
         [ -6.3750,  -1.6406,   8.5000,  ...,  -1.0781,  -3.5156,  -0.7539],
         [ -6.5625,  -2.3906,   8.1875,  ...,  -1.3828,  -3.6406,  -1.5234],
         [ -5.9375,   0.1416,   5.0312,  ...,  -3.7500,  -3.3594,  -2.2344]]],
       device='cuda:0', grad_fn=<ToCopyBackward0>), past_key_values=None, hidden_states=None, attentions=None)
{'loss': 8.8108, 'learning_rate': 0.001, 'epoch': 1.0}                                                                   
100%|██████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.53s/it]current file path llava/train/llava_trainer.py
def _save_checkpoint(self, model, trial, metrics=None)
self
 <llava.train.llava_trainer.LLaVATrainer object at 0x79ca141a45e0>
model
 DeepSpeedEngine(
  (module): LlavaLlamaForCausalLM(
    (model): LlavaLlamaModel(
      (embed_tokens): Embedding(32000, 4096, padding_idx=0)
      (layers): ModuleList(
        (0-31): 32 x LlamaDecoderLayer(
          (self_attn): LlamaAttention(
            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
            (act_fn): SiLUActivation()
          )
          (input_layernorm): LlamaRMSNorm()
          (post_attention_layernorm): LlamaRMSNorm()
        )
      )
      (norm): LlamaRMSNorm()
      (vision_tower): CLIPVisionTower(
        (vision_tower): CLIPVisionModel(
          (vision_model): CLIPVisionTransformer(
            (embeddings): CLIPVisionEmbeddings(
              (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
              (position_embedding): Embedding(577, 1024)
            )
            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder): CLIPEncoder(
              (layers): ModuleList(
                (0-23): 24 x CLIPEncoderLayer(
                  (self_attn): CLIPAttention(
                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  )
                  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (mlp): CLIPMLP(
                    (activation_fn): QuickGELUActivation()
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (mm_projector): Sequential(
        (0): Linear(in_features=1024, out_features=4096, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=4096, out_features=4096, bias=True)
      )
    )
    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
  )
)
trial
 None
metrics
 None
current file path llava/train/llava_trainer.py
def get_mm_adapter_state_maybe_zero_3(named_params, keys_to_match)
named_params
 <generator object Module.named_parameters at 0x79ca1421e7a0>
keys_to_match
 ['mm_projector', 'vision_resampler']
current file path llava/train/llava_trainer.py
def maybe_zero_3(param, ignore_status=False, name=None)
param
 Parameter containing:
tensor([[ 1.5137e-02,  2.2217e-02, -1.7700e-02,  ...,  2.1515e-03,
          1.9897e-02,  4.2725e-03],
        [ 1.9653e-02, -2.4170e-02, -1.2634e-02,  ..., -2.0386e-02,
          2.9907e-02, -1.7853e-03],
        [ 3.7432e-05,  3.1128e-02, -1.8799e-02,  ...,  5.5847e-03,
          6.9885e-03, -1.6479e-02],
        ...,
        [-3.3569e-03,  1.3977e-02, -5.4016e-03,  ..., -2.7710e-02,
          2.9785e-02, -2.9907e-03],
        [ 3.0396e-02, -2.4658e-02,  2.0874e-02,  ..., -2.6367e-02,
          7.6904e-03, -9.1553e-03],
        [-1.7212e-02,  2.9144e-03,  1.4832e-02,  ..., -2.0508e-02,
          7.7515e-03, -1.3672e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
ignore_status
 True
name
 model.mm_projector.0.weight
param (to return)
 tensor([[ 1.5137e-02,  2.2217e-02, -1.7700e-02,  ...,  2.1515e-03,
          1.9897e-02,  4.2725e-03],
        [ 1.9653e-02, -2.4170e-02, -1.2634e-02,  ..., -2.0386e-02,
          2.9907e-02, -1.7853e-03],
        [ 3.7432e-05,  3.1128e-02, -1.8799e-02,  ...,  5.5847e-03,
          6.9885e-03, -1.6479e-02],
        ...,
        [-3.3569e-03,  1.3977e-02, -5.4016e-03,  ..., -2.7710e-02,
          2.9785e-02, -2.9907e-03],
        [ 3.0396e-02, -2.4658e-02,  2.0874e-02,  ..., -2.6367e-02,
          7.6904e-03, -9.1553e-03],
        [-1.7212e-02,  2.9144e-03,  1.4832e-02,  ..., -2.0508e-02,
          7.7515e-03, -1.3672e-02]], dtype=torch.bfloat16)
current file path llava/train/llava_trainer.py
def maybe_zero_3(param, ignore_status=False, name=None)
param
 Parameter containing:
tensor([-0.0097, -0.0259,  0.0199,  ...,  0.0193,  0.0199, -0.0299],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
ignore_status
 True
name
 model.mm_projector.0.bias
param (to return)
 tensor([-0.0097, -0.0259,  0.0199,  ...,  0.0193,  0.0199, -0.0299],
       dtype=torch.bfloat16)
current file path llava/train/llava_trainer.py
def maybe_zero_3(param, ignore_status=False, name=None)
param
 Parameter containing:
tensor([[-0.0155, -0.0023, -0.0103,  ..., -0.0099,  0.0075, -0.0039],
        [-0.0126,  0.0082, -0.0026,  ...,  0.0015,  0.0042, -0.0017],
        [ 0.0093, -0.0073, -0.0087,  ...,  0.0004,  0.0153, -0.0055],
        ...,
        [ 0.0036,  0.0115,  0.0111,  ..., -0.0104,  0.0102, -0.0137],
        [-0.0070, -0.0139, -0.0061,  ...,  0.0048, -0.0146,  0.0084],
        [-0.0105,  0.0012,  0.0070,  ..., -0.0139, -0.0094,  0.0007]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
ignore_status
 True
name
 model.mm_projector.2.weight
param (to return)
 tensor([[-0.0155, -0.0023, -0.0103,  ..., -0.0099,  0.0075, -0.0039],
        [-0.0126,  0.0082, -0.0026,  ...,  0.0015,  0.0042, -0.0017],
        [ 0.0093, -0.0073, -0.0087,  ...,  0.0004,  0.0153, -0.0055],
        ...,
        [ 0.0036,  0.0115,  0.0111,  ..., -0.0104,  0.0102, -0.0137],
        [-0.0070, -0.0139, -0.0061,  ...,  0.0048, -0.0146,  0.0084],
        [-0.0105,  0.0012,  0.0070,  ..., -0.0139, -0.0094,  0.0007]],
       dtype=torch.bfloat16)
current file path llava/train/llava_trainer.py
def maybe_zero_3(param, ignore_status=False, name=None)
param
 Parameter containing:
tensor([-0.0034, -0.0085,  0.0103,  ..., -0.0152,  0.0149,  0.0013],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
ignore_status
 True
name
 model.mm_projector.2.bias
param (to return)
 tensor([-0.0034, -0.0085,  0.0103,  ..., -0.0152,  0.0149,  0.0013],
       dtype=torch.bfloat16)
to_return
 {'model.mm_projector.0.weight': tensor([[ 1.5137e-02,  2.2217e-02, -1.7700e-02,  ...,  2.1515e-03,
          1.9897e-02,  4.2725e-03],
        [ 1.9653e-02, -2.4170e-02, -1.2634e-02,  ..., -2.0386e-02,
          2.9907e-02, -1.7853e-03],
        [ 3.7432e-05,  3.1128e-02, -1.8799e-02,  ...,  5.5847e-03,
          6.9885e-03, -1.6479e-02],
        ...,
        [-3.3569e-03,  1.3977e-02, -5.4016e-03,  ..., -2.7710e-02,
          2.9785e-02, -2.9907e-03],
        [ 3.0396e-02, -2.4658e-02,  2.0874e-02,  ..., -2.6367e-02,
          7.6904e-03, -9.1553e-03],
        [-1.7212e-02,  2.9144e-03,  1.4832e-02,  ..., -2.0508e-02,
          7.7515e-03, -1.3672e-02]], dtype=torch.bfloat16), 'model.mm_projector.0.bias': tensor([-0.0097, -0.0259,  0.0199,  ...,  0.0193,  0.0199, -0.0299],
       dtype=torch.bfloat16), 'model.mm_projector.2.weight': tensor([[-0.0155, -0.0023, -0.0103,  ..., -0.0099,  0.0075, -0.0039],
        [-0.0126,  0.0082, -0.0026,  ...,  0.0015,  0.0042, -0.0017],
        [ 0.0093, -0.0073, -0.0087,  ...,  0.0004,  0.0153, -0.0055],
        ...,
        [ 0.0036,  0.0115,  0.0111,  ..., -0.0104,  0.0102, -0.0137],
        [-0.0070, -0.0139, -0.0061,  ...,  0.0048, -0.0146,  0.0084],
        [-0.0105,  0.0012,  0.0070,  ..., -0.0139, -0.0094,  0.0007]],
       dtype=torch.bfloat16), 'model.mm_projector.2.bias': tensor([-0.0034, -0.0085,  0.0103,  ..., -0.0152,  0.0149,  0.0013],
       dtype=torch.bfloat16)}
{'train_runtime': 92.9918, 'train_samples_per_second': 0.011, 'train_steps_per_second': 0.011, 'train_loss': 8.810822486877441, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.75s/it]
current file path llava/train/train.py
def get_mm_adapter_state_maybe_zero_3(named_params, keys_to_match)
named_params
 <generator object Module.named_parameters at 0x79ca1421eb20>
keys_to_match
 ['mm_projector']
current file path llava/train/train.py
def maybe_zero_3(param, ignore_status=False, name=None)
param
 Parameter containing:
tensor([[ 1.5137e-02,  2.2217e-02, -1.7700e-02,  ...,  2.1515e-03,
          1.9897e-02,  4.2725e-03],
        [ 1.9653e-02, -2.4170e-02, -1.2634e-02,  ..., -2.0386e-02,
          2.9907e-02, -1.7853e-03],
        [ 3.7432e-05,  3.1128e-02, -1.8799e-02,  ...,  5.5847e-03,
          6.9885e-03, -1.6479e-02],
        ...,
        [-3.3569e-03,  1.3977e-02, -5.4016e-03,  ..., -2.7710e-02,
          2.9785e-02, -2.9907e-03],
        [ 3.0396e-02, -2.4658e-02,  2.0874e-02,  ..., -2.6367e-02,
          7.6904e-03, -9.1553e-03],
        [-1.7212e-02,  2.9144e-03,  1.4832e-02,  ..., -2.0508e-02,
          7.7515e-03, -1.3672e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
param.shape
 torch.Size([4096, 1024])
ignore_status
 True
name
 None
param (return)
 tensor([[ 1.5137e-02,  2.2217e-02, -1.7700e-02,  ...,  2.1515e-03,
          1.9897e-02,  4.2725e-03],
        [ 1.9653e-02, -2.4170e-02, -1.2634e-02,  ..., -2.0386e-02,
          2.9907e-02, -1.7853e-03],
        [ 3.7432e-05,  3.1128e-02, -1.8799e-02,  ...,  5.5847e-03,
          6.9885e-03, -1.6479e-02],
        ...,
        [-3.3569e-03,  1.3977e-02, -5.4016e-03,  ..., -2.7710e-02,
          2.9785e-02, -2.9907e-03],
        [ 3.0396e-02, -2.4658e-02,  2.0874e-02,  ..., -2.6367e-02,
          7.6904e-03, -9.1553e-03],
        [-1.7212e-02,  2.9144e-03,  1.4832e-02,  ..., -2.0508e-02,
          7.7515e-03, -1.3672e-02]], dtype=torch.bfloat16)
param (return).shape
 torch.Size([4096, 1024])
current file path llava/train/train.py
def maybe_zero_3(param, ignore_status=False, name=None)
param
 Parameter containing:
tensor([-0.0097, -0.0259,  0.0199,  ...,  0.0193,  0.0199, -0.0299],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
param.shape
 torch.Size([4096])
ignore_status
 True
name
 None
param (return)
 tensor([-0.0097, -0.0259,  0.0199,  ...,  0.0193,  0.0199, -0.0299],
       dtype=torch.bfloat16)
param (return).shape
 torch.Size([4096])
current file path llava/train/train.py
def maybe_zero_3(param, ignore_status=False, name=None)
param
 Parameter containing:
tensor([[-0.0155, -0.0023, -0.0103,  ..., -0.0099,  0.0075, -0.0039],
        [-0.0126,  0.0082, -0.0026,  ...,  0.0015,  0.0042, -0.0017],
        [ 0.0093, -0.0073, -0.0087,  ...,  0.0004,  0.0153, -0.0055],
        ...,
        [ 0.0036,  0.0115,  0.0111,  ..., -0.0104,  0.0102, -0.0137],
        [-0.0070, -0.0139, -0.0061,  ...,  0.0048, -0.0146,  0.0084],
        [-0.0105,  0.0012,  0.0070,  ..., -0.0139, -0.0094,  0.0007]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
param.shape
 torch.Size([4096, 4096])
ignore_status
 True
name
 None
param (return)
 tensor([[-0.0155, -0.0023, -0.0103,  ..., -0.0099,  0.0075, -0.0039],
        [-0.0126,  0.0082, -0.0026,  ...,  0.0015,  0.0042, -0.0017],
        [ 0.0093, -0.0073, -0.0087,  ...,  0.0004,  0.0153, -0.0055],
        ...,
        [ 0.0036,  0.0115,  0.0111,  ..., -0.0104,  0.0102, -0.0137],
        [-0.0070, -0.0139, -0.0061,  ...,  0.0048, -0.0146,  0.0084],
        [-0.0105,  0.0012,  0.0070,  ..., -0.0139, -0.0094,  0.0007]],
       dtype=torch.bfloat16)
param (return).shape
 torch.Size([4096, 4096])
current file path llava/train/train.py
def maybe_zero_3(param, ignore_status=False, name=None)
param
 Parameter containing:
tensor([-0.0034, -0.0085,  0.0103,  ..., -0.0152,  0.0149,  0.0013],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
param.shape
 torch.Size([4096])
ignore_status
 True
name
 None
param (return)
 tensor([-0.0034, -0.0085,  0.0103,  ..., -0.0152,  0.0149,  0.0013],
       dtype=torch.bfloat16)
param (return).shape
 torch.Size([4096])
to_return (return)
 {'model.mm_projector.0.weight': tensor([[ 1.5137e-02,  2.2217e-02, -1.7700e-02,  ...,  2.1515e-03,
          1.9897e-02,  4.2725e-03],
        [ 1.9653e-02, -2.4170e-02, -1.2634e-02,  ..., -2.0386e-02,
          2.9907e-02, -1.7853e-03],
        [ 3.7432e-05,  3.1128e-02, -1.8799e-02,  ...,  5.5847e-03,
          6.9885e-03, -1.6479e-02],
        ...,
        [-3.3569e-03,  1.3977e-02, -5.4016e-03,  ..., -2.7710e-02,
          2.9785e-02, -2.9907e-03],
        [ 3.0396e-02, -2.4658e-02,  2.0874e-02,  ..., -2.6367e-02,
          7.6904e-03, -9.1553e-03],
        [-1.7212e-02,  2.9144e-03,  1.4832e-02,  ..., -2.0508e-02,
          7.7515e-03, -1.3672e-02]], dtype=torch.bfloat16), 'model.mm_projector.0.bias': tensor([-0.0097, -0.0259,  0.0199,  ...,  0.0193,  0.0199, -0.0299],
       dtype=torch.bfloat16), 'model.mm_projector.2.weight': tensor([[-0.0155, -0.0023, -0.0103,  ..., -0.0099,  0.0075, -0.0039],
        [-0.0126,  0.0082, -0.0026,  ...,  0.0015,  0.0042, -0.0017],
        [ 0.0093, -0.0073, -0.0087,  ...,  0.0004,  0.0153, -0.0055],
        ...,
        [ 0.0036,  0.0115,  0.0111,  ..., -0.0104,  0.0102, -0.0137],
        [-0.0070, -0.0139, -0.0061,  ...,  0.0048, -0.0146,  0.0084],
        [-0.0105,  0.0012,  0.0070,  ..., -0.0139, -0.0094,  0.0007]],
       dtype=torch.bfloat16), 'model.mm_projector.2.bias': tensor([-0.0034, -0.0085,  0.0103,  ..., -0.0152,  0.0149,  0.0013],
       dtype=torch.bfloat16)}
to_return['model.mm_projector.0.weight'].shape
 torch.Size([4096, 1024])
to_return['model.mm_projector.0.bias'].shape
 torch.Size([4096])
to_return['model.mm_projector.2.weight'].shape
 torch.Size([4096, 4096])
to_return['model.mm_projector.2.bias'].shape
 torch.Size([4096])
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                    train/epoch ▁▁
wandb:              train/global_step ▁▁
wandb:            train/learning_rate ▁
wandb:                     train/loss ▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                    train/epoch 1.0
wandb:              train/global_step 1
wandb:            train/learning_rate 0.001
wandb:                     train/loss 8.8108
wandb:               train/total_flos 998098534400.0
wandb:               train/train_loss 8.81082
wandb:            train/train_runtime 92.9918
wandb: train/train_samples_per_second 0.011
wandb:   train/train_steps_per_second 0.011
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/ubuntu/LLaVA/wandb/offline-run-20250908_123008-i6nxmla5
wandb: Find logs at: ./wandb/offline-run-20250908_123008-i6nxmla5/logs
[2025-09-08 12:30:15,857] [INFO] [launch.py:347:main] Process 6400 exits successfully.
(llava) ubuntu@157-151-224-68:~/LLaVA$ 