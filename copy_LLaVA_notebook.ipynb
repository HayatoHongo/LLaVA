{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MgqvUKsdFkEz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File \"<string>\", line 1\n",
      "    import torch, transformers; print(torch.__version__}, transformers.__version__)\n",
      "                                                       ^\n",
      "SyntaxError: closing parenthesis '}' does not match opening parenthesis '('\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import torch, transformers; print(torch.__version__}, transformers.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C8j7yWUt4N-C"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RmGhXE625w7R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "[2025-10-04 03:56:58,560] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/wandb/sdk/launch/builder/build.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n",
    "    version: Optional[str] = field(default=\"v0\")\n",
    "    freeze_backbone: bool = field(default=False)\n",
    "    tune_mm_mlp_adapter: bool = field(default=False)\n",
    "    vision_tower: Optional[str] = field(default=None) # default to None\n",
    "    mm_vision_select_layer: Optional[int] = field(default=-1)   # default to the last layer\n",
    "    pretrain_mm_mlp_adapter: Optional[str] = field(default=None)\n",
    "    mm_projector_type: Optional[str] = field(default='linear')\n",
    "    mm_use_im_start_end: bool = field(default=False)\n",
    "    mm_use_im_patch_token: bool = field(default=True)\n",
    "    mm_patch_merge_type: Optional[str] = field(default='flat')\n",
    "    mm_vision_select_feature: Optional[str] = field(default=\"patch\")\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(default=None,\n",
    "                           metadata={\"help\": \"Path to the training data.\"})\n",
    "    lazy_preprocess: bool = False\n",
    "    is_multimodal: bool = False\n",
    "    image_folder: Optional[str] = field(default=None)\n",
    "    image_aspect_ratio: str = 'square'\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    remove_unused_columns: bool = field(default=False)\n",
    "    freeze_mm_mlp_adapter: bool = field(default=False)\n",
    "    mpt_attn_impl: Optional[str] = field(default=\"triton\")\n",
    "    model_max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\n",
    "            \"help\":\n",
    "            \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
    "        },\n",
    "    )\n",
    "    double_quant: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Compress the quantization statistics through double quantization.\"}\n",
    "    )\n",
    "    quant_type: str = field(\n",
    "        default=\"nf4\",\n",
    "        metadata={\"help\": \"Quantization data type to use. Should be one of `fp4` or `nf4`.\"}\n",
    "    )\n",
    "    bits: int = field(\n",
    "        default=16,\n",
    "        metadata={\"help\": \"How many bits to use.\"}\n",
    "    )\n",
    "    lora_enable: bool = False\n",
    "    lora_r: int = 64\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_weight_path: str = \"\"\n",
    "    lora_bias: str = \"none\"\n",
    "    mm_projector_lr: Optional[float] = None\n",
    "    group_by_modality_length: bool = field(default=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DTvjg0PP7za1"
   },
   "outputs": [],
   "source": [
    "from transformers import HfArgumentParser\n",
    "\n",
    "args_dict = {\n",
    "    #\"deepspeed\": \"./scripts/zero2.json\",\n",
    "    \"model_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    \"version\": \"plain\",\n",
    "    \"data_path\": \"/workspaces/LLaVA/CC3M_1.json\",\n",
    "    \"image_folder\": \"/workspaces/LLaVA/images/\",\n",
    "    \"vision_tower\": \"openai/clip-vit-large-patch14-336\",\n",
    "    \"mm_projector_type\": \"mlp2x_gelu\",\n",
    "    \"tune_mm_mlp_adapter\": True,\n",
    "    \"mm_vision_select_layer\": -2,\n",
    "    \"mm_use_im_start_end\": False,\n",
    "    \"mm_use_im_patch_token\": False,\n",
    "    \"bf16\": True,\n",
    "    \"output_dir\": \"./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0\",\n",
    "\n",
    "    # TrainingArguments 相当\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 1,\n",
    "    \"save_total_limit\": 1,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"weight_decay\": 0.0, # I don't know why 0.0\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"logging_steps\": 1,\n",
    "    \"tf32\": False, # switched from True for TinyLlama\n",
    "    \"model_max_length\": 2048,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"dataloader_num_workers\": 2,\n",
    "    \"lazy_preprocess\": True,\n",
    "    \"report_to\": \"none\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dBX7g2DD-xpR",
    "outputId": "627a283d-b903-45db-a0c0-eec1b51ca0eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_args\n",
      " ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
      "data_args\n",
      " DataArguments(data_path='/workspaces/LLaVA/CC3M_1.json', lazy_preprocess=True, is_multimodal=False, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')\n",
      "training_args\n",
      " TrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "bits=16,\n",
      "cache_dir=None,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=2,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "double_quant=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "freeze_mm_mlp_adapter=False,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "group_by_modality_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0/runs/Oct04_03-56-59_4987b947cf96,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1,\n",
      "logging_strategy=steps,\n",
      "lora_alpha=16,\n",
      "lora_bias=none,\n",
      "lora_dropout=0.05,\n",
      "lora_enable=False,\n",
      "lora_r=64,\n",
      "lora_weight_path=,\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mm_projector_lr=None,\n",
      "model_max_length=2048,\n",
      "mp_parameters=,\n",
      "mpt_attn_impl=triton,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "quant_type=nf4,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=1,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=False,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_dict(args_dict)\n",
    "print(\"model_args\\n\", model_args)\n",
    "print(\"data_args\\n\", data_args)\n",
    "print(\"training_args\\n\", training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oC2rbSk5QtcX"
   },
   "outputs": [],
   "source": [
    "# Model Constants\n",
    "IGNORE_INDEX = -100\n",
    "IMAGE_TOKEN_INDEX = -200\n",
    "DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "DEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\n",
    "DEFAULT_IM_START_TOKEN = \"<im_start>\"\n",
    "DEFAULT_IM_END_TOKEN = \"<im_end>\"\n",
    "IMAGE_PLACEHOLDER = \"<image-placeholder>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "J_5thB_J_WEi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_rank\n",
      " 0\n",
      "compute_dtype\n",
      " torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "local_rank = training_args.local_rank\n",
    "print(\"local_rank\\n\", local_rank)\n",
    "compute_dtype = (torch.float16 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n",
    "print(\"compute_dtype\\n\", compute_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jFp9VBciHEhH"
   },
   "outputs": [],
   "source": [
    "from transformers import CLIPVisionModel, CLIPImageProcessor, CLIPVisionConfig\n",
    "import torch.nn as nn\n",
    "# __init__\n",
    "# load_model\n",
    "\n",
    "# result = CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n",
    "class CLIPVisionTower(nn.Module):\n",
    "    def __init__(self, vision_tower, args):\n",
    "        # result = CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n",
    "        print(\"current file path\", \"llava/llava/model/multimodal_encoder/clip_encoder.py\")\n",
    "        print(\"def CLIPVisionTower.__init__(self, vision_tower, args\")\n",
    "        print(\"self\\n\", type(self))\n",
    "        print(\"vision_tower\\n\", vision_tower) # openai/clip-vit-large-patch14-336\n",
    "        print(\"args\\n\", args) # ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_loaded = False\n",
    "\n",
    "        print(\"self.is_loaded\\n\", self.is_loaded) # False\n",
    "\n",
    "        self.vision_tower_name = vision_tower\n",
    "        print(\"self.vision_tower_name\\n\", self.vision_tower_name) # openai/clip-vit-large-patch14-336\n",
    "        self.select_layer = args.mm_vision_select_layer\n",
    "        print(\"self.select_layer\\n\", self.select_layer) # -2\n",
    "        self.select_feature = getattr(args, 'mm_vision_select_feature', 'patch')\n",
    "        print(\"self.select_feature\\n\", self.select_feature) # patch\n",
    "\n",
    "    def load_model(self):\n",
    "\n",
    "        print(\"current file path\", \"llava/llava/model/multimodal_encoder/clip_encoder.py\")\n",
    "        print(\"def CLIPVisionTower.load_model(self)\")\n",
    "        print(\"self\\n\", type(self))\n",
    "        print(\"self.vision_tower_name\\n\", self.vision_tower_name) # openai/clip-vit-large-patch14-336\n",
    "        self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name)\n",
    "        print(\"self.image_processor\\n\", self.image_processor)\n",
    "        \"\"\"\n",
    "        CLIPImageProcessor {\n",
    "        \"crop_size\": {\n",
    "            \"height\": 336,\n",
    "            \"width\": 336\n",
    "        },\n",
    "        \"do_center_crop\": true,\n",
    "        \"do_convert_rgb\": true,\n",
    "        \"do_normalize\": true,\n",
    "        \"do_rescale\": true,\n",
    "        \"do_resize\": true,\n",
    "        \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n",
    "        \"image_mean\": [\n",
    "            0.48145466,\n",
    "            0.4578275,\n",
    "            0.40821073\n",
    "        ],\n",
    "        \"image_processor_type\": \"CLIPImageProcessor\",\n",
    "        \"image_std\": [\n",
    "            0.26862954,\n",
    "            0.26130258,\n",
    "            0.27577711\n",
    "        ],\n",
    "        \"resample\": 3,\n",
    "        \"rescale_factor\": 0.00392156862745098,\n",
    "        \"size\": {\n",
    "            \"shortest_edge\": 336\n",
    "        }\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name)\n",
    "        print(\"self.vision_tower\\n\", self.vision_tower)\n",
    "        \"\"\"\n",
    "        CLIPVisionModel(\n",
    "        (vision_model): CLIPVisionTransformer(\n",
    "            (embeddings): CLIPVisionEmbeddings(\n",
    "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "            (position_embedding): Embedding(577, 1024)\n",
    "            )\n",
    "            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "            (encoder): CLIPEncoder(\n",
    "            (layers): ModuleList(\n",
    "                (0-23): 24 x CLIPEncoderLayer(\n",
    "                (self_attn): CLIPAttention(\n",
    "                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                (mlp): CLIPMLP(\n",
    "                    (activation_fn): QuickGELUActivation()\n",
    "                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                )\n",
    "            )\n",
    "            )\n",
    "            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        )\n",
    "        )\n",
    "        \"\"\"\n",
    "        self.vision_tower.requires_grad_(False)\n",
    "\n",
    "        self.is_loaded = True\n",
    "        print(\"self.is_loaded\\n\", self.is_loaded) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n"
     ]
    }
   ],
   "source": [
    "print(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/llava/model/multimodal_encoder/clip_encoder.py\n",
      "def CLIPVisionTower.__init__(self, vision_tower, args\n",
      "self\n",
      " <class '__main__.CLIPVisionTower'>\n",
      "vision_tower\n",
      " openai/clip-vit-large-patch14-336\n",
      "args\n",
      " ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
      "self.is_loaded\n",
      " False\n",
      "self.vision_tower_name\n",
      " openai/clip-vit-large-patch14-336\n",
      "self.select_layer\n",
      " -2\n",
      "self.select_feature\n",
      " patch\n"
     ]
    }
   ],
   "source": [
    "vision_tower_name = \"openai/clip-vit-large-patch14-336\"\n",
    "CLIPVisionTower_model = CLIPVisionTower(vision_tower_name, args=model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/llava/model/multimodal_encoder/clip_encoder.py\n",
      "def CLIPVisionTower.load_model(self)\n",
      "self\n",
      " <class '__main__.CLIPVisionTower'>\n",
      "self.vision_tower_name\n",
      " openai/clip-vit-large-patch14-336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.image_processor\n",
      " CLIPImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 336,\n",
      "    \"width\": 336\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 336\n",
      "  }\n",
      "}\n",
      "\n",
      "self.vision_tower\n",
      " CLIPVisionModel(\n",
      "  (vision_model): CLIPVisionTransformer(\n",
      "    (embeddings): CLIPVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "      (position_embedding): Embedding(577, 1024)\n",
      "    )\n",
      "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "self.is_loaded\n",
      " True\n"
     ]
    }
   ],
   "source": [
    "CLIPVisionTower_model.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(CLIPVisionTower_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPVisionModel(\n",
      "  (vision_model): CLIPVisionTransformer(\n",
      "    (embeddings): CLIPVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "      (position_embedding): Embedding(577, 1024)\n",
      "    )\n",
      "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(CLIPVisionTower_model.vision_tower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPVisionConfig {\n",
      "  \"_name_or_path\": \"openai/clip-vit-large-patch14-336\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"dropout\": 0.0,\n",
      "  \"hidden_act\": \"quick_gelu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": 336,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"model_type\": \"clip_vision_model\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"patch_size\": 14,\n",
      "  \"projection_dim\": 768,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(CLIPVisionTower_model.vision_tower.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5mz6UWifHfTq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def build_vision_tower(model_args, **kwargs):\n",
    "    # vision_tower = build_vision_tower(model_args)\n",
    "    print(\"current file path\", \"llava/llava/model/multimodal_encoder/builder.py\")\n",
    "    print(\"def build_vision_tower(model_args, **kwargs)\")\n",
    "    print(\"model_args\\n\", model_args) # ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
    "    print(\"kwargs\\n\", kwargs) # {}\n",
    "    vision_tower = model_args.vision_tower\n",
    "    print(\"vision_tower from model_args\\n\", vision_tower) # openai/clip-vit-large-patch14-336\n",
    "    CLIPVisionTower_model = CLIPVisionTower(vision_tower, args=model_args, **kwargs)\n",
    "    CLIPVisionTower_model.load_model()\n",
    "    print(\"CLIPVisionTower_model\\n\", CLIPVisionTower_model)\n",
    "    print(\"【EXIT】if is_absolute_path_exists or vision_tower.startswith('openai') or vision_tower.startswith('laion') or 'ShareGPT4V' in vision_tower:\")\n",
    "    return CLIPVisionTower_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/llava/model/multimodal_encoder/builder.py\n",
      "def build_vision_tower(model_args, **kwargs)\n",
      "model_args\n",
      " ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
      "kwargs\n",
      " {}\n",
      "vision_tower from model_args\n",
      " openai/clip-vit-large-patch14-336\n",
      "current file path llava/llava/model/multimodal_encoder/clip_encoder.py\n",
      "def CLIPVisionTower.__init__(self, vision_tower, args\n",
      "self\n",
      " <class '__main__.CLIPVisionTower'>\n",
      "vision_tower\n",
      " openai/clip-vit-large-patch14-336\n",
      "args\n",
      " ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
      "self.is_loaded\n",
      " False\n",
      "self.vision_tower_name\n",
      " openai/clip-vit-large-patch14-336\n",
      "self.select_layer\n",
      " -2\n",
      "self.select_feature\n",
      " patch\n",
      "current file path llava/llava/model/multimodal_encoder/clip_encoder.py\n",
      "def CLIPVisionTower.load_model(self)\n",
      "self\n",
      " <class '__main__.CLIPVisionTower'>\n",
      "self.vision_tower_name\n",
      " openai/clip-vit-large-patch14-336\n",
      "self.image_processor\n",
      " CLIPImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 336,\n",
      "    \"width\": 336\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 336\n",
      "  }\n",
      "}\n",
      "\n",
      "self.vision_tower\n",
      " CLIPVisionModel(\n",
      "  (vision_model): CLIPVisionTransformer(\n",
      "    (embeddings): CLIPVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "      (position_embedding): Embedding(577, 1024)\n",
      "    )\n",
      "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "self.is_loaded\n",
      " True\n",
      "CLIPVisionTower_model\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "【EXIT】if is_absolute_path_exists or vision_tower.startswith('openai') or vision_tower.startswith('laion') or 'ShareGPT4V' in vision_tower:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPVisionTower(\n",
       "  (vision_tower): CLIPVisionModel(\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "        (position_embedding): Embedding(577, 1024)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_vision_tower(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def build_vision_projector(config, delay_load=False, **kwargs):\n",
    "    # 2層MLPをハードコーディング（configからサイズのみ取得）\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(config.mm_hidden_size, config.hidden_size),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(config.hidden_size, config.hidden_size),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mm_hidden_size\": 1024,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "# 公式 LLaMA-2-7B の config をロード\n",
    "TinyLlama_config = AutoConfig.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "TinyLlama_config.mm_hidden_size = CLIPVisionTower_model.vision_tower.config.hidden_size\n",
    "print(TinyLlama_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "projector = build_vision_projector(TinyLlama_config)\n",
    "print(projector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(CLIPVisionTower_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mro(cls):\n",
    "    print(f\"MRO for {cls.__name__}:\\n\")\n",
    "    for i, c in enumerate(cls.mro()):\n",
    "        print(f\"{i:2d}: {c.__module__}.{c.__name__}\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "VjkgxnbKFGan"
   },
   "outputs": [],
   "source": [
    "# LlavaBaseModel\n",
    "# __init__\n",
    "# get_vision_tower\n",
    "# initialize_vision_modules\n",
    "# unpad_image\n",
    "\n",
    "class LlavaBaseModel:  # LLaMA, MPT などの全てのLLM用の共通機能をまとめたクラス\n",
    "\n",
    "    # LlavaLlamaModelの__init__によって呼び出される\n",
    "    def __init__(self):\n",
    "\n",
    "        print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "        print(\"LlavaBaseModel.__init__(self, config)\")\n",
    "        # 「LlavaLlamaModelの__init__によって呼び出された時は」LlamaModelの__init_を呼び出す\n",
    "        super(LlavaBaseModel, self).__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "T_YguuBGN-3R"
   },
   "outputs": [],
   "source": [
    "# transformers.models.llama.modeling_llama.LlamaModel\n",
    "# URL: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py\n",
    "from transformers import LlamaModel\n",
    "\n",
    "# LlavaBaseModelにLlamaの初期化クラスをくっつけたいので、LLamaModelをMROに追加。\n",
    "# MROだと、LlavaLlamaModel > LlavaBaseModel > LlamaModel の順に解決される。\n",
    "class LlavaLlamaModel(LlavaBaseModel, LlamaModel): \n",
    "    def __init__(self):\n",
    "\n",
    "        print(\"current file path\", \"llava/llava/model/language_model/llava_llama.py\")\n",
    "        print(\"def LlavaLlamaModel.__init__(self, config: LlamaConfig)\")\n",
    "        print(\"self\\n\", type(self))\n",
    "        # LlavaBaseModelの__init__を呼ぶ\n",
    "        super(LlavaLlamaModel, self).__init__() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRO for LlavaBaseModel:\n",
      "\n",
      " 0: __main__.LlavaBaseModel\n",
      " 1: builtins.object\n",
      "\n",
      "\n",
      "\n",
      "MRO for LlavaLlamaModel:\n",
      "\n",
      " 0: __main__.LlavaLlamaModel\n",
      " 1: __main__.LlavaBaseModel\n",
      " 2: transformers.models.llama.modeling_llama.LlamaModel\n",
      " 3: transformers.models.llama.modeling_llama.LlamaPreTrainedModel\n",
      " 4: transformers.modeling_utils.PreTrainedModel\n",
      " 5: torch.nn.modules.module.Module\n",
      " 6: transformers.modeling_utils.ModuleUtilsMixin\n",
      " 7: transformers.generation.utils.GenerationMixin\n",
      " 8: transformers.utils.hub.PushToHubMixin\n",
      " 9: builtins.object\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_mro(LlavaBaseModel)\n",
    "print_mro(LlavaLlamaModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlavaBaseModel\n",
    "# __init__\n",
    "# get_vision_tower\n",
    "# initialize_vision_modules\n",
    "# unpad_image\n",
    "\n",
    "class LlavaBaseModel:  # LLaMA, MPT などの全てのLLM用の共通機能をまとめたクラス\n",
    "\n",
    "    # LlavaLlamaModelの__init__によって呼び出される\n",
    "    def __init__(self, config):\n",
    "\n",
    "        print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "        print(\"LlavaBaseModel.__init__(self, config)\")\n",
    "        print(\"config\\n\", config)\n",
    "        # 「LlavaLlamaModelの__init__によって呼び出された時は」LlamaModelの__init_を呼び出す\n",
    "        super(LlavaBaseModel, self).__init__(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers.models.llama.modeling_llama.LlamaModel\n",
    "# URL: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py\n",
    "from transformers import LlamaConfig, LlamaModel\n",
    "\n",
    "class LlavaLlamaConfig(LlamaConfig):\n",
    "    model_type = \"llava_llama\"\n",
    "\n",
    "# LlavaBaseModelにLlamaの初期化クラスをくっつけたいので、LLamaModelをMROに追加。\n",
    "# MROだと、LlavaLlamaModel > LlavaBaseModel > LlamaModel の順に解決される。\n",
    "class LlavaLlamaModel(LlavaBaseModel, LlamaModel): \n",
    "    config_class = LlavaLlamaConfig # from_pretrained の時にここが読みこまれる\n",
    "\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "\n",
    "        print(\"current file path\", \"llava/llava/model/language_model/llava_llama.py\")\n",
    "        print(\"def LlavaLlamaModel.__init__(self, config: LlamaConfig)\")\n",
    "        print(\"self\\n\", type(self))\n",
    "        print(\"config\\n\", config)\n",
    "        # LlavaBaseModelの__init__を呼ぶ\n",
    "        super(LlavaLlamaModel, self).__init__(config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "V9uRX7reN0gI"
   },
   "outputs": [],
   "source": [
    "# LlavaBaseForCausalLM\n",
    "# get_vision_tower\n",
    "# encode_images\n",
    "# prepare_inputs_labels_for_multimodal\n",
    "# initialize_vision_tokenizer\n",
    "\n",
    "# 以下のLlavaBaseForCausalLMのselfは、継承先のLlavaLlamaForCausalLMのselfが使用される\n",
    "\n",
    "class LlavaBaseForCausalLM:\n",
    "\n",
    "    def get_vision_tower(self):\n",
    "        print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "        print(\"class LlavaBaseForCausalLM(ABC).get_vision_tower(self)\")\n",
    "        result = self.get_model().get_vision_tower()\n",
    "        print(\"LlavaBaseForCausalLM(ABC).get_vision_tower(self) result (return)\\n\", result)\n",
    "        \"\"\"\n",
    "        CLIPVisionTower(\n",
    "        (vision_tower): CLIPVisionModel(\n",
    "            (vision_model): CLIPVisionTransformer(\n",
    "            (embeddings): CLIPVisionEmbeddings(\n",
    "                (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "                (position_embedding): Embedding(577, 1024)\n",
    "            )\n",
    "            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "            (encoder): CLIPEncoder(\n",
    "                (layers): ModuleList(\n",
    "                (0-23): 24 x CLIPEncoderLayer(\n",
    "                    (self_attn): CLIPAttention(\n",
    "                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    )\n",
    "                    (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                    (mlp): CLIPMLP(\n",
    "                    (activation_fn): QuickGELUActivation()\n",
    "                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "                    )\n",
    "                    (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                )\n",
    "                )\n",
    "            )\n",
    "            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "            )\n",
    "        )\n",
    "        )\n",
    "        \"\"\"\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "t4yAIwyeOEWE"
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "class LlavaLlamaForCausalLM(LlamaForCausalLM, LlavaBaseForCausalLM):\n",
    "    config_class = LlavaLlamaConfig # from_pretrainedの際にここが読み込まれる\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        print(\"current file path\", \"llava/llava/model/language_model/llava_llama.py\")\n",
    "        print(\"def LlavaLlamaForCausalLM.__init__(self, config)\")\n",
    "        print(\"self\\n\", type(self))\n",
    "        print(\"config\\n\", config)\n",
    "        super(LlamaForCausalLM, self).__init__(config)\n",
    "        # LlamaForCausalLMのself.modelをLlavaLlamaModelに置き換える\n",
    "        self.model = LlavaLlamaModel(config)\n",
    "        print(\"self.model\\n\", self.model)\n",
    "        \"\"\"\n",
    "        self.model\n",
    "        LlavaLlamaModel(\n",
    "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
    "        (layers): ModuleList(\n",
    "            (0-31): 32 x LlamaDecoderLayer(\n",
    "            (self_attn): LlamaAttention(\n",
    "                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "                (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "                (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "                (rotary_emb): LlamaRotaryEmbedding()\n",
    "            )\n",
    "            (mlp): LlamaMLP(\n",
    "                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
    "                (act_fn): SiLUActivation()\n",
    "            )\n",
    "            (input_layernorm): LlamaRMSNorm()\n",
    "            (post_attention_layernorm): LlamaRMSNorm()\n",
    "            )\n",
    "        )\n",
    "        (norm): LlamaRMSNorm()\n",
    "        )\n",
    "        \"\"\"\n",
    "        #print(\"self.pretraining_tp\\n\", self.pretraining_tp) # 1\n",
    "        #print(\"self.vocab_size\\n\", self.vocab_size) # 32_000\n",
    "        #print(\"self.lm_head\\n\", self.lm_head) # Linear(in_features=4096, out_features=32000, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        #self.post_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "RlMP4qUg0QtV",
    "outputId": "12f73ecd-9922-4d31-a0cf-a0e8bc5d88fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import AutoConfig\\n\\n# まず config.json をロードして Config クラスを自動判別\\nconfig = AutoConfig.from_pretrained(\\n    model_args.model_name_or_path,\\n    cache_dir=training_args.cache_dir\\n)\\n\\nprint(\"model_args.model_name_or_path\\n\", model_args.model_name_or_path)\\nprint(\"training_args.cache_dir\\n\", training_args.cache_dir)\\nprint(\"\")\\nprint(\"Loaded config:\\n\", config)\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from transformers import AutoConfig\n",
    "\n",
    "# まず config.json をロードして Config クラスを自動判別\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir\n",
    ")\n",
    "\n",
    "print(\"model_args.model_name_or_path\\n\", model_args.model_name_or_path)\n",
    "print(\"training_args.cache_dir\\n\", training_args.cache_dir)\n",
    "print(\"\")\n",
    "print(\"Loaded config:\\n\", config)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QwLz_ubtVlti",
    "outputId": "680f292f-9756-45d4-cb80-af9692475a34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRO for LlavaBaseForCausalLM:\n",
      "\n",
      " 0: __main__.LlavaBaseForCausalLM\n",
      " 1: builtins.object\n",
      "\n",
      "\n",
      "\n",
      "MRO for LlavaLlamaForCausalLM:\n",
      "\n",
      " 0: __main__.LlavaLlamaForCausalLM\n",
      " 1: transformers.models.llama.modeling_llama.LlamaForCausalLM\n",
      " 2: transformers.models.llama.modeling_llama.LlamaPreTrainedModel\n",
      " 3: transformers.modeling_utils.PreTrainedModel\n",
      " 4: torch.nn.modules.module.Module\n",
      " 5: transformers.modeling_utils.ModuleUtilsMixin\n",
      " 6: transformers.generation.utils.GenerationMixin\n",
      " 7: transformers.utils.hub.PushToHubMixin\n",
      " 8: __main__.LlavaBaseForCausalLM\n",
      " 9: builtins.object\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_mro(LlavaBaseForCausalLM)\n",
    "print_mro(LlavaLlamaForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "UJE07TY6Jb5K",
    "outputId": "091b04d2-adbc-49c3-ef2a-50181c53f793"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.__init__(self, config)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "config\n",
      " LlavaLlamaConfig {\n",
      "  \"_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llava_llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaModel.__init__(self, config: LlamaConfig)\n",
      "self\n",
      " <class '__main__.LlavaLlamaModel'>\n",
      "config\n",
      " LlavaLlamaConfig {\n",
      "  \"_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llava_llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "current file path llava/model/llava_arch.py\n",
      "LlavaBaseModel.__init__(self, config)\n",
      "config\n",
      " LlavaLlamaConfig {\n",
      "  \"_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llava_llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "self.model\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0 and are newly initialized: ['model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "LlavaLlamaForCausalLM_model = LlavaLlamaForCausalLM.from_pretrained(model_args.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "wodQZ9rWSyZN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlavaLlamaForCausalLM_model\n",
      " LlavaLlamaForCausalLM(\n",
      "  (model): LlavaLlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-21): 22 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"LlavaLlamaForCausalLM_model\\n\", LlavaLlamaForCausalLM_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "z5c1aEs4gf5L"
   },
   "outputs": [],
   "source": [
    "LlavaLlamaForCausalLM_model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "l9az2bPNccDC"
   },
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from typing import List\n",
    "from enum import auto, Enum\n",
    "\n",
    "class SeparatorStyle(Enum):\n",
    "    \"\"\"Different separator style.\"\"\"\n",
    "    SINGLE = auto()\n",
    "    TWO = auto()\n",
    "    MPT = auto()\n",
    "    PLAIN = auto()\n",
    "    LLAMA_2 = auto()\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Conversation:\n",
    "    \"\"\"A class that keeps all conversation history.\"\"\"\n",
    "    system: str\n",
    "    roles: List[str]\n",
    "    messages: List[List[str]]\n",
    "    offset: int\n",
    "    sep_style: SeparatorStyle = SeparatorStyle.SINGLE\n",
    "    sep: str = \"###\"\n",
    "    sep2: str = None\n",
    "    version: str = \"Unknown\"\n",
    "    skip_next: bool = False\n",
    "\n",
    "\n",
    "conv_llava_plain = Conversation(\n",
    "    system=\"\",\n",
    "    roles=(\"\", \"\"),\n",
    "    messages=(\n",
    "    ),\n",
    "    offset=0,\n",
    "    sep_style=SeparatorStyle.PLAIN,\n",
    "    sep=\"\\n\",\n",
    ")\n",
    "\n",
    "\n",
    "conv_templates = {\n",
    "    \"plain\": conv_llava_plain,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "YSs2_yNVOpOD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport inspect\\nprint(inspect.getattr_static(LlamaForCausalLM, \"from_pretrained\"))\\nprint(inspect.getattr_static(LlamaForCausalLM, \"enable_input_require_grads\"))\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import inspect\n",
    "print(inspect.getattr_static(LlamaForCausalLM, \"from_pretrained\"))\n",
    "print(inspect.getattr_static(LlamaForCausalLM, \"enable_input_require_grads\"))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "None\n",
      "2048\n"
     ]
    }
   ],
   "source": [
    "print(model_args.model_name_or_path)\n",
    "print(training_args.cache_dir)\n",
    "print(training_args.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "od0OYu7yBesv"
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    model_max_length=training_args.model_max_length, \n",
    "    padding_side=\"right\", # 実際にパディングを行うのは tokenizer か DataCollator のどちらかだが、padding_side 自体はtokenizer側で行う\n",
    "    use_fast=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "M71wf9CYD_Y7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token: </s>\n",
      "pad_token_id: 2\n",
      "unk_token: <unk>\n",
      "unk_token_id: 0\n",
      "tokenizer\n",
      " LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False)}, clean_up_tokenization_spaces=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"pad_token:\", tokenizer.pad_token)\n",
    "print(\"pad_token_id:\", tokenizer.pad_token_id)\n",
    "print(\"unk_token:\", tokenizer.unk_token) # unkownの略です。類似の単語と間違えないでください。\n",
    "print(\"unk_token_id:\", tokenizer.unk_token_id)\n",
    "print(\"tokenizer\\n\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "5TrZtJFODFWz"
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Vbf0h2NSDoUG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token: <unk>\n",
      "pad_token_id: 0\n",
      "unk_token: <unk>\n",
      "unk_token_id: 0\n",
      "tokenizer\n",
      " LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"pad_token:\", tokenizer.pad_token)\n",
    "print(\"pad_token_id:\", tokenizer.pad_token_id)\n",
    "print(\"unk_token:\", tokenizer.unk_token)\n",
    "print(\"unk_token_id:\", tokenizer.unk_token_id)\n",
    "print(\"tokenizer\\n\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "kGtPYpfKDOKW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_args.version\n",
      " plain\n",
      "default_conversation\n",
      " Conversation(system='', roles=('', ''), messages=(), offset=0, sep_style=<SeparatorStyle.PLAIN: 4>, sep='\\n', sep2=None, version='Unknown', skip_next=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"model_args.version\\n\", model_args.version)\n",
    "default_conversation = conv_templates[model_args.version]\n",
    "print(\"default_conversation\\n\", default_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "csi6Xe_-G0_G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_args.vision_tower\n",
      " openai/clip-vit-large-patch14-336\n"
     ]
    }
   ],
   "source": [
    "print(\"model_args.vision_tower\\n\", model_args.vision_tower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpri2EiEIatU"
   },
   "outputs": [],
   "source": [
    "# LlavaLlamaForCausalLMの関数。LlavaLlamaModelをgetする\n",
    "def get_model(self):\n",
    "\n",
    "    print(\"current file path\", \"llava/llava/model/language_model/llava_llama.py\")\n",
    "    print(\"def LlavaLlamaForCausalLM.get_model(self)\")\n",
    "    print(\"self\\n\", type(self))\n",
    "    print(\"self.model (return)\\n\", self.model)\n",
    "    return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "RPVqYUnjJHoF"
   },
   "outputs": [],
   "source": [
    "LlavaLlamaForCausalLM.get_model = get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Iqqp22MDJSJ-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.get_model(self)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "self.model (return)\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "initial_model = LlavaLlamaForCausalLM_model.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "4c_UQLwdPhjd"
   },
   "outputs": [],
   "source": [
    "def config(self):\n",
    "\n",
    "    print(\"current file path\", \"llava/llava/model/multimodal_encoder/clip_encoder.py\")\n",
    "    print(\"def CLIPVisionTower.config(self)\")\n",
    "    print(\"self\\n\", type(self))\n",
    "    print(\"self.is_loaded\\n\", self.is_loaded) # True\n",
    "    print(f\"【COND】 is_loaded={self.is_loaded}\")\n",
    "    if self.is_loaded:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】if self.is_loaded:\")\n",
    "        result = self.vision_tower.config\n",
    "        print(\"result (return)\\n\", type(result))\n",
    "        print(\"【EXIT】if self.is_loaded:\")\n",
    "    else:\n",
    "      pass\n",
    "    print(\"result (return)\\n\", result)\n",
    "    \"\"\"\n",
    "    CLIPVisionConfig {\n",
    "    \"_name_or_path\": \"openai/clip-vit-large-patch14-336\",\n",
    "    \"attention_dropout\": 0.0,\n",
    "    \"dropout\": 0.0,\n",
    "    \"hidden_act\": \"quick_gelu\",\n",
    "    \"hidden_size\": 1024,\n",
    "    \"image_size\": 336,\n",
    "    \"initializer_factor\": 1.0,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 4096,\n",
    "    \"layer_norm_eps\": 1e-05,\n",
    "    \"model_type\": \"clip_vision_model\",\n",
    "    \"num_attention_heads\": 16,\n",
    "    \"num_channels\": 3,\n",
    "    \"num_hidden_layers\": 24,\n",
    "    \"patch_size\": 14,\n",
    "    \"projection_dim\": 768,\n",
    "    \"transformers_version\": \"4.31.0\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "XG8ocYNiPpGB"
   },
   "outputs": [],
   "source": [
    "CLIPVisionTower.config = property(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "HrBNMbAm39zA"
   },
   "outputs": [],
   "source": [
    "def initialize_vision_modules(self, model_args, fsdp=None):\n",
    "\n",
    "  print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "  print(\"def initialize_vision_modules(self, model_args, fsdp=None)\")\n",
    "  print(\"model_args\\n\", model_args) #  ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
    "  print(\"fsdp\\n\", fsdp) # []\n",
    "  vision_tower = model_args.vision_tower\n",
    "  print(\"vision_tower from model_args\\n\", vision_tower) # openai/clip-vit-large-patch14-336\n",
    "  mm_vision_select_layer = model_args.mm_vision_select_layer\n",
    "  print(\"mm_vision_select_layer from model_args\\n\", mm_vision_select_layer) # -2\n",
    "  mm_vision_select_feature = model_args.mm_vision_select_feature\n",
    "  print(\"mm_vision_select_feature from model_args\\n\", mm_vision_select_feature) # patch\n",
    "  pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n",
    "  print(\"pretrain_mm_mlp_adapter from model_args\\n\", pretrain_mm_mlp_adapter) # None\n",
    "  mm_patch_merge_type = model_args.mm_patch_merge_type\n",
    "  # 下記はself.config.mm_vision_towerに関するもの。self.vision_towerは依然としてNone\n",
    "  self.config.mm_vision_tower = vision_tower\n",
    "  print(\"self.config.mm_vision_tower\\n\", self.config.mm_vision_tower) # None\n",
    "\n",
    "  print(\"【COND】 self.get_vision_tower()\\n\", self.get_vision_tower()) # None\n",
    "  print(f\"【COND】 get_vision_tower_is_None={self.get_vision_tower() is None}\")\n",
    "  if self.get_vision_tower() is None:\n",
    "      #【ENTER】self.vision_tower, self.get_vision_towerはNoneなのでこの分岐に入る。\n",
    "      print(\"【ENTER】if self.get_vision_tower() is None:\")\n",
    "      print(\"[ENTER] self.get_vision_tower() is None\")\n",
    "      # build_vision_tower(model_args) はちょっと奥の依存関係が深い\n",
    "      vision_tower = build_vision_tower(model_args)\n",
    "      print(\"vision_tower after build_vision_tower\\n\", vision_tower)\n",
    "      \"\"\"\n",
    "      CLIPVisionTower(\n",
    "      (vision_tower): CLIPVisionModel(\n",
    "      (vision_model): CLIPVisionTransformer(\n",
    "          (embeddings): CLIPVisionEmbeddings(\n",
    "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "          (position_embedding): Embedding(577, 1024)\n",
    "          )\n",
    "          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "          (encoder): CLIPEncoder(\n",
    "          (layers): ModuleList(\n",
    "              (0-23): 24 x CLIPEncoderLayer(\n",
    "              (self_attn): CLIPAttention(\n",
    "                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "              )\n",
    "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "              (mlp): CLIPMLP(\n",
    "                  (activation_fn): QuickGELUActivation()\n",
    "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "              )\n",
    "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "              )\n",
    "          )\n",
    "          )\n",
    "          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "      )\n",
    "      )\n",
    "      )\n",
    "      \"\"\"\n",
    "      # 分散学習(FSDP)を使うかどうか. 今回は [] 空のリストとなるので、Noneではないが、len(fsdp) == 0\n",
    "      print(\"【COND】 fsdp\\n\", fsdp) # []\n",
    "      print(f\"【COND】 fsdp_is_not_None={fsdp is not None} len_fsdp={len(fsdp) if fsdp is not None else 'N/A'}\") # fsdp_is_not_None=True len_fsdp=0\n",
    "      if fsdp is not None and len(fsdp) > 0:\n",
    "        pass\n",
    "      else:\n",
    "          # 【ENTER】else of if fsdp is not None and len(fsdp) > 0:\n",
    "          print(\"【COND】 else_fsdp_is_not_None_and_len_fsdp_gt_0=True\")\n",
    "          print(\"【ENTER】else of if fsdp is not None and len(fsdp) > 0:\")\n",
    "          self.vision_tower = vision_tower\n",
    "          print(\"self.vision_tower\\n\", self.vision_tower)\n",
    "          \"\"\"\n",
    "          CLIPVisionTower(\n",
    "          (vision_tower): CLIPVisionModel(\n",
    "              (vision_model): CLIPVisionTransformer(\n",
    "              (embeddings): CLIPVisionEmbeddings(\n",
    "                  (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "                  (position_embedding): Embedding(577, 1024)\n",
    "              )\n",
    "              (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "              (encoder): CLIPEncoder(\n",
    "                  (layers): ModuleList(\n",
    "                  (0-23): 24 x CLIPEncoderLayer(\n",
    "                      (self_attn): CLIPAttention(\n",
    "                      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                      )\n",
    "                      (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                      (mlp): CLIPMLP(\n",
    "                      (activation_fn): QuickGELUActivation()\n",
    "                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "                      )\n",
    "                      (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                  )\n",
    "                  )\n",
    "              )\n",
    "              (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "              )\n",
    "          )\n",
    "          )\n",
    "          \"\"\"\n",
    "          print(\"【EXIT】else of if fsdp is not None and len(fsdp) > 0:\")\n",
    "\n",
    "      print(\"【EXIT】if self.get_vision_tower() is None:\")\n",
    "  else:\n",
    "    pass\n",
    "\n",
    "  self.config.use_mm_proj = True\n",
    "  print(\"self.config.use_mm_proj set to True\") # True\n",
    "  self.config.mm_projector_type = getattr(model_args, 'mm_projector_type', 'linear')\n",
    "  print(\"self.config.mm_projector_type\\n\", self.config.mm_projector_type) # mlp2x_gelu\n",
    "  self.config.mm_hidden_size = vision_tower.config.hidden_size\n",
    "  print(\"self.config.mm_hidden_size\\n\", self.config.mm_hidden_size) # 1024\n",
    "  self.config.mm_vision_select_layer = mm_vision_select_layer\n",
    "  print(\"self.config.mm_vision_select_layer\\n\", self.config.mm_vision_select_layer) # -2\n",
    "  self.config.mm_vision_select_feature = mm_vision_select_feature\n",
    "  print(\"self.config.mm_vision_select_feature\\n\", self.config.mm_vision_select_feature) # patch\n",
    "  self.config.mm_patch_merge_type = mm_patch_merge_type\n",
    "  print(\"self.config.mm_patch_merge_type\\n\", self.config.mm_patch_merge_type) # flat\n",
    "\n",
    "  # mm_projector_is_None=True\n",
    "  print(f\"【COND】 mm_projector_is_None={getattr(self, 'mm_projector', None) is None}\")\n",
    "  if getattr(self, 'mm_projector', None) is None:\n",
    "      # 【ENTER】\n",
    "      print(\"【ENTER】if getattr(self, 'mm_projector', None) is None:\")\n",
    "      self.mm_projector = build_vision_projector(self.config)\n",
    "      \"\"\"\n",
    "      Sequential(\n",
    "        (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
    "        (1): GELU(approximate='none')\n",
    "        (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
    "      )\n",
    "      \"\"\"\n",
    "      print(\"self.mm_projector after build_vision_projector\\n\", self.mm_projector)\n",
    "      print(\"mm_patch_merge_type\\n\", mm_patch_merge_type) # flat\n",
    "      print(f\"【COND】 unpad_in_mm_patch_merge_type={'unpad' in mm_patch_merge_type}\")\n",
    "      if 'unpad' in mm_patch_merge_type:\n",
    "        pass\n",
    "      print(\"【EXIT】if getattr(self, 'mm_projector', None) is None:\")\n",
    "  else:\n",
    "    pass\n",
    "\n",
    "  print(f\"【COND】 pretrain_mm_mlp_adapter_is_not_None={pretrain_mm_mlp_adapter is not None}\")\n",
    "  if pretrain_mm_mlp_adapter is not None:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "e6_2dvDMkr20"
   },
   "outputs": [],
   "source": [
    "LlavaBaseModel.initialize_vision_modules = initialize_vision_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "qT_vXMXBlP2a"
   },
   "outputs": [],
   "source": [
    "def get_vision_tower(self):\n",
    "\n",
    "    print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "    print(\"def get_vision_tower(self)\")\n",
    "    vision_tower = getattr(self, 'vision_tower', None)\n",
    "    print(\"vision_tower (raw)\\n\", vision_tower)\n",
    "    \"\"\"\n",
    "    CLIPVisionTower(\n",
    "    (vision_tower): CLIPVisionModel(\n",
    "        (vision_model): CLIPVisionTransformer(\n",
    "        (embeddings): CLIPVisionEmbeddings(\n",
    "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "            (position_embedding): Embedding(577, 1024)\n",
    "        )\n",
    "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        (encoder): CLIPEncoder(\n",
    "            (layers): ModuleList(\n",
    "            (0-23): 24 x CLIPEncoderLayer(\n",
    "                (self_attn): CLIPAttention(\n",
    "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                (mlp): CLIPMLP(\n",
    "                (activation_fn): QuickGELUActivation()\n",
    "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "            )\n",
    "            )\n",
    "        )\n",
    "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        )\n",
    "    )\n",
    "    )\n",
    "    \"\"\"\n",
    "    print(\"type(vision_tower)\\n\", type(vision_tower))\n",
    "    print(f\"【COND】 type_vision_tower_is_list={type(vision_tower) is list}\")  # False\n",
    "    if type(vision_tower) is list:\n",
    "        # 【SKIP】\n",
    "        print(\"【ENTER】if type(vision_tower) is list:\")\n",
    "        vision_tower = vision_tower[0]\n",
    "        print(\"【EXIT】if type(vision_tower) is list:\")\n",
    "    print(\"vision_tower (return)\\n\", vision_tower)\n",
    "    \"\"\"\n",
    "    vision_tower (return)\n",
    "    CLIPVisionTower(\n",
    "    (vision_tower): CLIPVisionModel(\n",
    "        (vision_model): CLIPVisionTransformer(\n",
    "        (embeddings): CLIPVisionEmbeddings(\n",
    "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "            (position_embedding): Embedding(577, 1024)\n",
    "        )\n",
    "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        (encoder): CLIPEncoder(\n",
    "            (layers): ModuleList(\n",
    "            (0-23): 24 x CLIPEncoderLayer(\n",
    "                (self_attn): CLIPAttention(\n",
    "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                (mlp): CLIPMLP(\n",
    "                (activation_fn): QuickGELUActivation()\n",
    "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "            )\n",
    "            )\n",
    "        )\n",
    "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        )\n",
    "    )\n",
    "    )\n",
    "    \"\"\"\n",
    "    return vision_tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "iabxtpWflUf9"
   },
   "outputs": [],
   "source": [
    "LlavaBaseModel.get_vision_tower = get_vision_tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RW9YLSmBT4FG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/model/llava_arch.py\n",
      "def initialize_vision_modules(self, model_args, fsdp=None)\n",
      "model_args\n",
      " ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
      "fsdp\n",
      " []\n",
      "vision_tower from model_args\n",
      " openai/clip-vit-large-patch14-336\n",
      "mm_vision_select_layer from model_args\n",
      " -2\n",
      "mm_vision_select_feature from model_args\n",
      " patch\n",
      "pretrain_mm_mlp_adapter from model_args\n",
      " None\n",
      "self.config.mm_vision_tower\n",
      " openai/clip-vit-large-patch14-336\n",
      "current file path llava/model/llava_arch.py\n",
      "def get_vision_tower(self)\n",
      "vision_tower (raw)\n",
      " None\n",
      "type(vision_tower)\n",
      " <class 'NoneType'>\n",
      "【COND】 type_vision_tower_is_list=False\n",
      "vision_tower (return)\n",
      " None\n",
      "【COND】 self.get_vision_tower()\n",
      " None\n",
      "current file path llava/model/llava_arch.py\n",
      "def get_vision_tower(self)\n",
      "vision_tower (raw)\n",
      " None\n",
      "type(vision_tower)\n",
      " <class 'NoneType'>\n",
      "【COND】 type_vision_tower_is_list=False\n",
      "vision_tower (return)\n",
      " None\n",
      "【COND】 get_vision_tower_is_None=True\n",
      "current file path llava/model/llava_arch.py\n",
      "def get_vision_tower(self)\n",
      "vision_tower (raw)\n",
      " None\n",
      "type(vision_tower)\n",
      " <class 'NoneType'>\n",
      "【COND】 type_vision_tower_is_list=False\n",
      "vision_tower (return)\n",
      " None\n",
      "【ENTER】if self.get_vision_tower() is None:\n",
      "[ENTER] self.get_vision_tower() is None\n",
      "current file path llava/llava/model/multimodal_encoder/builder.py\n",
      "def build_vision_tower(model_args, **kwargs)\n",
      "model_args\n",
      " ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
      "kwargs\n",
      " {}\n",
      "vision_tower from model_args\n",
      " openai/clip-vit-large-patch14-336\n",
      "current file path llava/llava/model/multimodal_encoder/clip_encoder.py\n",
      "def CLIPVisionTower.__init__(self, vision_tower, args\n",
      "self\n",
      " <class '__main__.CLIPVisionTower'>\n",
      "vision_tower\n",
      " openai/clip-vit-large-patch14-336\n",
      "args\n",
      " ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
      "self.is_loaded\n",
      " False\n",
      "self.vision_tower_name\n",
      " openai/clip-vit-large-patch14-336\n",
      "self.select_layer\n",
      " -2\n",
      "self.select_feature\n",
      " patch\n",
      "current file path llava/llava/model/multimodal_encoder/clip_encoder.py\n",
      "def CLIPVisionTower.load_model(self)\n",
      "self\n",
      " <class '__main__.CLIPVisionTower'>\n",
      "self.vision_tower_name\n",
      " openai/clip-vit-large-patch14-336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.image_processor\n",
      " CLIPImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 336,\n",
      "    \"width\": 336\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 336\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_model.initialize_vision_modules(\n",
    "    model_args=model_args,\n",
    "    fsdp=training_args.fsdp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3-wDbzZfnfD"
   },
   "outputs": [],
   "source": [
    "vision_tower = model.get_vision_tower()\n",
    "print(\"vision_tower\\n\", vision_tower)\n",
    "vision_tower.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)\n",
    "\n",
    "data_args.image_processor = vision_tower.image_processor\n",
    "print(\"data_args.image_processor\\n\", data_args.image_processor)\n",
    "data_args.is_multimodal = True\n",
    "print(\"data_args.is_multimodal\\n\", data_args.is_multimodal) # True\n",
    "\n",
    "model.config.image_aspect_ratio = data_args.image_aspect_ratio\n",
    "print(\"model.config.image_aspect_ratio\\n\", model.config.image_aspect_ratio) # square\n",
    "model.config.tokenizer_padding_side = tokenizer.padding_side\n",
    "print(\"model.config.tokenizer_padding_side\\n\", model.config.tokenizer_padding_side) # right\n",
    "model.config.tokenizer_model_max_length = tokenizer.model_max_length\n",
    "print(\"model.config.tokenizer_model_max_length\\n\", model.config.tokenizer_model_max_length) # 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBa5nAaWi1M7"
   },
   "outputs": [],
   "source": [
    "model.config.tune_mm_mlp_adapter = training_args.tune_mm_mlp_adapter = model_args.tune_mm_mlp_adapter\n",
    "print(f\"【COND】 tune_mm_mlp_adapter={model_args.tune_mm_mlp_adapter}\") # True\n",
    "if model_args.tune_mm_mlp_adapter:\n",
    "    # 【ENTER】 tune_mm_mlp_adapter=True なので、この分岐に入る\n",
    "    print(\"【ENTER】if model_args.tune_mm_mlp_adapter:\")\n",
    "    # モデル全体の全パラメータを「学習不可（requires_grad=False）」にする\n",
    "    # これで通常の重みは全て凍結される\n",
    "    model.requires_grad_(False)\n",
    "    for p in model.get_model().mm_projector.parameters():\n",
    "        # mm_projector（画像特徴量→テキスト特徴量への変換層）の全パラメータだけを「学習可能（requires_grad=True）」に戻す\n",
    "        # これで mm_projector のみ学習されることになる\n",
    "        print(\"model.get_model().mm_projector.parameters()\", model.get_model().mm_projector.parameters())\n",
    "        p.requires_grad = True\n",
    "    print(\"【EXIT】if model_args.tune_mm_mlp_adapter:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lsCavfasZCY"
   },
   "outputs": [],
   "source": [
    "model.config.freeze_mm_mlp_adapter = training_args.freeze_mm_mlp_adapter\n",
    "print(f\"【COND】 freeze_mm_mlp_adapter={training_args.freeze_mm_mlp_adapter}\") # False\n",
    "if training_args.freeze_mm_mlp_adapter:\n",
    "  pass\n",
    "\n",
    "print(f\"【COND】 bits={training_args.bits}\") # 16\n",
    "if training_args.bits in [4, 8]:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjMWFAC_um1_"
   },
   "outputs": [],
   "source": [
    "def initialize_vision_tokenizer(self, model_args, tokenizer):\n",
    "    print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "    print(\"def initialize_vision_tokenizer(self, model_args, tokenizer)\")\n",
    "    print(\"model_args\\n\", model_args) # ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
    "    print(\"tokenizer\\n\", tokenizer) # LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)\n",
    "\n",
    "    print(f\"【COND】 mm_use_im_patch_token={model_args.mm_use_im_patch_token}\") # False\n",
    "    if model_args.mm_use_im_patch_token:\n",
    "      pass\n",
    "\n",
    "    if model_args.mm_use_im_start_end: # False\n",
    "      pass\n",
    "\n",
    "    elif model_args.mm_use_im_patch_token: # False\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WnJGUH-zu5lX"
   },
   "outputs": [],
   "source": [
    "LlavaLlamaForCausalLM.initialize_vision_tokenizer = initialize_vision_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lj1_WjcpsnXB"
   },
   "outputs": [],
   "source": [
    "model.config.mm_use_im_start_end = data_args.mm_use_im_start_end = model_args.mm_use_im_start_end\n",
    "print(\"model_args.mm_use_im_start_end\", model_args.mm_use_im_start_end)\n",
    "model.config.mm_projector_lr = training_args.mm_projector_lr\n",
    "print(\"training_args.mm_projector_lr\", training_args.mm_projector_lr)\n",
    "training_args.use_im_start_end = model_args.mm_use_im_start_end\n",
    "print(\"training_args.use_im_start_end\", training_args.use_im_start_end)\n",
    "model.config.mm_use_im_patch_token = model_args.mm_use_im_patch_token\n",
    "print(\"model_args.mm_use_im_patch_token\", model_args.mm_use_im_patch_token)\n",
    "model.initialize_vision_tokenizer(model_args, tokenizer=tokenizer)\n",
    "print(\"【EXIT】if model_args.vision_tower is not None:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLUf4wqcxKI4"
   },
   "outputs": [],
   "source": [
    "def rank0_print(*args):\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def rank0_print(*args)\")\n",
    "    print(\"args\\n\", args) # ('Formatting inputs...Skip in lazy mode',)\n",
    "    if local_rank == 0:\n",
    "        print(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "839mYcaPw2CZ"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class LazySupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str,\n",
    "                 tokenizer: transformers.PreTrainedTokenizer,\n",
    "                 data_args: DataArguments):\n",
    "\n",
    "        print(\"current file path\", \"llava/train/train.py\")\n",
    "        print(\"def LazySupervisedDataset.__init__(self, data_path, tokenizer, data_args)\")\n",
    "        print(\"data_path\\n\", data_path) # /content/LLaVA/blip_laion_cc_sbu_1.json\n",
    "        print(\"tokenizer\\n\", type(tokenizer)) # <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
    "        print(\"data_args\\n\", data_args) # DataArguments(data_path='/content/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/LLaVA/images', image_aspect_ratio='square')\n",
    "        super(LazySupervisedDataset, self).__init__()\n",
    "        list_data_dict = json.load(open(data_path, \"r\"))\n",
    "        # 今回は1サンプルだけなのでprintしても危険ではない\n",
    "        print(\"list_data_dict\", list_data_dict)\n",
    "\n",
    "        rank0_print(\"Formatting inputs...Skip in lazy mode\") # Formatting inputs...Skip in lazy mode\n",
    "        self.tokenizer = tokenizer\n",
    "        print(\"self.tokenizer\\n\", self.tokenizer)\n",
    "        self.list_data_dict = list_data_dict\n",
    "        print(\"self.list_data_dict\\n\", self.list_data_dict)\n",
    "        self.data_args = data_args\n",
    "        print(\"self.data_args\\n\", self.data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hv3nmqgMwVWa"
   },
   "outputs": [],
   "source": [
    "def __len__(self):\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def LazySupervisedDataset.__len__(self)\")\n",
    "    return len(self.list_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64qMycFgwWrA"
   },
   "outputs": [],
   "source": [
    "LazySupervisedDataset.__len__ = __len__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2Th7s50xen5"
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from typing import Dict\n",
    "\n",
    "# Trainer > def _get_dataloader > dataloader_params = {...\"collate_fn\": data_collator,...}\n",
    "# self.accelerator.prepare(DataLoader(dataset, **dataloader_params)) で呼ばれる\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        print(\"current file path\", \"llava/train/train.py\")\n",
    "        print(\"def DataCollatorForSupervisedDataset.__call__(self, instances)\")\n",
    "        print(\"instances\\n\", instances)\n",
    "        #  [(torch.Size([24]), torch.Size([24]), torch.Size([3, 336, 336]))]\n",
    "        print(\"shape of each instance's input_ids and labels, and images(if any):\", [(x['input_ids'].shape, x['labels'].shape, x.get('image', None).shape if 'image' in x else None) for x in instances])\n",
    "        # データローダーが None を返すことがあるので、Noneのサンプルを除外。\n",
    "        instances = [x for x in instances if x is not None]\n",
    "        # input_idsとlabelsのそれぞれについてリストを作成。タプルをつくる。\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances]\n",
    "                                  for key in (\"input_ids\", \"labels\"))\n",
    "        # input_idsはtokenizerのpad_token_id(0)でパディング\n",
    "        print(\"self.tokenizer.pad_token_id\\n\", self.tokenizer.pad_token_id)\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids,\n",
    "            batch_first=True,\n",
    "            padding_value=self.tokenizer.pad_token_id)\n",
    "        # labelsはIGNORE_INDEX(-100)でパディング\n",
    "        print(\"IGNORE_INDEX\\n\", IGNORE_INDEX)\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels,\n",
    "                                                 batch_first=True,\n",
    "                                                 padding_value=IGNORE_INDEX)\n",
    "        input_ids = input_ids[:, :self.tokenizer.model_max_length]\n",
    "        print(\"input_ids.shape (after pad_sequence and truncate)\\n\", input_ids.shape)\n",
    "        print(\"input_ids (after pad_sequence and truncate)\\n\", input_ids)\n",
    "        labels = labels[:, :self.tokenizer.model_max_length]\n",
    "        print(\"labels.shape (after pad_sequence and truncate)\\n\", labels.shape)\n",
    "        print(\"labels (after pad_sequence and truncate)\\n\", labels)\n",
    "        # .ne() は \"not equal\" → pad_token_id(=0) じゃない部分を 1、pad 部分を 0 にする。モデルが pad 部分を読まないように制御するマスクです。\n",
    "        batch = dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "        if 'image' in instances[0]:\n",
    "            images = [instance['image'] for instance in instances]\n",
    "            if all(x is not None and x.shape == images[0].shape for x in images):\n",
    "                batch['images'] = torch.stack(images)\n",
    "            else:\n",
    "                batch['images'] = images\n",
    "            print(\"batch['images'].shape\\n\", batch['images'].shape)\n",
    "        \n",
    "        print(\"batch (return)\\n\", batch)\n",
    "        print(\"shape of each batch's input_ids and labels, and images(if any):\", [(batch['input_ids'].shape, batch['labels'].shape, batch.get('images', None).shape if 'images' in batch else None)])\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QjNj6B85wWA6"
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer,\n",
    "                                data_args) -> Dict:\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def make_supervised_data_module(tokenizer, data_args)\")\n",
    "    print(\"tokenizer\\n\", type(tokenizer))\n",
    "    print(\"data_args\\n\", data_args) #  DataArguments(data_path='/content/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/LLaVA/images', image_aspect_ratio='square')\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    train_dataset = LazySupervisedDataset(tokenizer=tokenizer,\n",
    "                                data_path=data_args.data_path,\n",
    "                                data_args=data_args)\n",
    "    print(\"train_dataset\\n\", train_dataset) # <llava.train.train.LazySupervisedDataset object at 0x7ed6341f4880>\n",
    "    print(\"len(train_dataset)\\n\", len(train_dataset)) # 1\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "    print(\"data_collator\\n\", data_collator) # DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))\n",
    "    result = dict(train_dataset=train_dataset,\n",
    "                  eval_dataset=None,\n",
    "                  data_collator=data_collator)\n",
    "    print(\"def make_supervised_data_module: result (return)\\n\", result) # {'train_dataset': <llava.train.train.LazySupervisedDataset object at 0x7ed6341f4880>, 'eval_dataset': None, 'data_collator': DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TlaRN8vG5p99",
    "outputId": "a52c09b2-84d7-4d0b-b5b1-1f94af461408"
   },
   "outputs": [],
   "source": [
    "data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n",
    "print(\"data_module\\n\", data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "id": "z8F6sqQdxsMO",
    "outputId": "92194c34-0765-4928-9cda-23c51af0a337"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from transformers.trainer import (\n",
    "    is_sagemaker_mp_enabled,\n",
    "    get_parameter_names,\n",
    "    has_length,\n",
    "    ALL_LAYERNORM_LAYERS,\n",
    "    ShardedDDPOption,\n",
    "    logger,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None):\n",
    "\n",
    "    print(\"current file path\", \"llava/mm_utils.py\")\n",
    "    print(\"def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None)\")\n",
    "    print(\"prompt\\n\", prompt) # <image>the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair\n",
    "    print(\"tokenizer\\n\", tokenizer) #  LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)\n",
    "    print(\"image_token_index\\n\", image_token_index) # -200\n",
    "    print(\"return_tensors\\n\", return_tensors) # pt\n",
    "    prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split('<image>')]\n",
    "\n",
    "    def insert_separator(X, sep):\n",
    "        return [ele for sublist in zip(X, [sep]*len(X)) for ele in sublist][:-1]\n",
    "\n",
    "    input_ids = []\n",
    "    offset = 0\n",
    "    if len(prompt_chunks) > 0 and len(prompt_chunks[0]) > 0 and prompt_chunks[0][0] == tokenizer.bos_token_id:\n",
    "        offset = 1\n",
    "        input_ids.append(prompt_chunks[0][0])\n",
    "\n",
    "    for x in insert_separator(prompt_chunks, [image_token_index] * (offset + 1)):\n",
    "        input_ids.extend(x[offset:])\n",
    "\n",
    "    if return_tensors is not None:\n",
    "        if return_tensors == 'pt':\n",
    "            return torch.tensor(input_ids, dtype=torch.long)\n",
    "        raise ValueError(f'Unsupported tensor type: {return_tensors}')\n",
    "    print(\"input_ids (return)\\n\", input_ids)\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def preprocess_plain(\n",
    "    sources: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def preprocess_plain(sources, tokenizer)\")\n",
    "    print(\"sources\\n\", sources) # [[{'from': 'human', 'value': '<image>\\nGive a brief description of the image.'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]\n",
    "    print(\"tokenizer\\n\", type(tokenizer)) # <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
    "    # add end signal and concatenate together\n",
    "    conversations = []\n",
    "    print(\"conversations initial\\n\", conversations) # []\n",
    "    for source in sources:\n",
    "        print(\"source current loop\\n\", source) \n",
    "        assert len(source) == 2\n",
    "        assert DEFAULT_IMAGE_TOKEN in source[0]['value']\n",
    "        source[0]['value'] = DEFAULT_IMAGE_TOKEN\n",
    "        conversation = source[0]['value'] + source[1]['value'] + default_conversation.sep\n",
    "        print(\"conversation current loop\\n\", conversation)\n",
    "        conversations.append(conversation)\n",
    "    print(\"conversations (final)\\n\", conversations) #  ['<image>the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair\\n']\n",
    "    # tokenize conversations\n",
    "    input_ids = [tokenizer_image_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations]\n",
    "    print(\"input_ids\\n\", input_ids) # [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114, 411,  2654, 11315,    13])]\n",
    "    for idx, tensor in enumerate(input_ids):\n",
    "        if hasattr(tensor, 'shape'):\n",
    "            print(f\"input_ids[{idx}].shape\\n\", tensor.shape) # torch.Size([24])\n",
    "    targets = copy.deepcopy(input_ids)\n",
    "    print(\"targets\\n\", targets) # [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114, 411,  2654, 11315,    13])]\n",
    "    for idx, tensor in enumerate(targets):\n",
    "        if hasattr(tensor, 'shape'):\n",
    "            print(f\"targets[{idx}].shape\\n\", tensor.shape) # torch.Size([24])\n",
    "    print(\"sources\\n\", sources) # [[{'from': 'human', 'value': '<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]\n",
    "    for target, source in zip(targets, sources):\n",
    "        tokenized_len = len(tokenizer_image_token(source[0]['value'], tokenizer)) # prompt <image>\n",
    "        target[:tokenized_len] = IGNORE_INDEX\n",
    "\n",
    "    print(\"input_ids (return)\\n\", input_ids) # [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114, 411,  2654, 11315,    13])]\n",
    "    print(\"targets (return)\\n\", targets) #  [tensor([ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114, 411,  2654, 11315,    13])]\n",
    "    return dict(input_ids=input_ids, labels=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_speaker_and_signal(header, source, get_conversation=True):\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def _add_speaker_and_signal(header, source, get_conversation=True)\")\n",
    "    print(\"header _add_speaker_and_signal\\n\", header)\n",
    "    print(\"source _add_speaker_and_signal\\n\", source)\n",
    "    print(\"get_conversation _add_speaker_and_signal\\n\", get_conversation)\n",
    "    \"\"\"Add speaker and start/end signal on each round.\"\"\"\n",
    "    BEGIN_SIGNAL = \"### \"\n",
    "    END_SIGNAL = \"\\n\"\n",
    "    conversation = header\n",
    "    for sentence in source:\n",
    "        from_str = sentence[\"from\"]\n",
    "        if from_str.lower() == \"human\":\n",
    "            from_str = default_conversation.roles[0]\n",
    "        elif from_str.lower() == \"gpt\":\n",
    "            from_str = default_conversation.roles[1]\n",
    "        else:\n",
    "            from_str = 'unknown'\n",
    "        sentence[\"value\"] = (BEGIN_SIGNAL + from_str + \": \" +\n",
    "                             sentence[\"value\"] + END_SIGNAL)\n",
    "        if get_conversation:\n",
    "            conversation += sentence[\"value\"]\n",
    "    conversation += BEGIN_SIGNAL\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_fn(strings: Sequence[str],\n",
    "                 tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def _tokenize_fn(strings, tokenizer)\")\n",
    "    print(\"strings _tokenize_fn\\n\", strings)\n",
    "    print(\"tokenizer _tokenize_fn\\n\", type(tokenizer))\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        ) for text in strings\n",
    "    ]\n",
    "    input_ids = labels = [\n",
    "        tokenized.input_ids[0] for tokenized in tokenized_list\n",
    "    ]\n",
    "    for idx, tensor in enumerate(input_ids):\n",
    "        if hasattr(tensor, 'shape'):\n",
    "            print(f\"input_ids[{idx}].shape\\n\", tensor.shape)\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item()\n",
    "        for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mask_targets(target, tokenized_lens, speakers):\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def _mask_targets(target, tokenized_lens, speakers)\")\n",
    "    print(\"target\\n\", target)\n",
    "    print(\"tokenized_lens\\n\", tokenized_lens)\n",
    "    print(\"speakers\\n\", speakers)\n",
    "    # cur_idx = 0\n",
    "    cur_idx = tokenized_lens[0]\n",
    "    tokenized_lens = tokenized_lens[1:]\n",
    "    target[:cur_idx] = IGNORE_INDEX\n",
    "    for tokenized_len, speaker in zip(tokenized_lens, speakers):\n",
    "        if speaker == \"human\":\n",
    "            target[cur_idx+2:cur_idx + tokenized_len] = IGNORE_INDEX\n",
    "        cur_idx += tokenized_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    has_image: bool = False\n",
    ") -> Dict:\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def preprocess(sources, tokenizer, has_image=False)\")\n",
    "    print(\"sources\\n\", sources) # [[{'from': 'human', 'value': '<image>\\nGive a brief description of the image.'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]\n",
    "    print(\"tokenizer\\n\", type(tokenizer)) # <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
    "    print(\"has_image\\n\", has_image) # True\n",
    "    \"\"\"\n",
    "    Given a list of sources, each is a conversation list. This transform:\n",
    "    1. Add signal '### ' at the beginning each sentence, with end signal '\\n';\n",
    "    2. Concatenate conversations together;\n",
    "    3. Tokenize the concatenated conversation;\n",
    "    4. Make a deepcopy as the target. Mask human words with IGNORE_INDEX.\n",
    "    \"\"\"\n",
    "    if default_conversation.sep_style == SeparatorStyle.PLAIN:\n",
    "        return preprocess_plain(sources, tokenizer) # True\n",
    "    # add end signal and concatenate together\n",
    "    conversations = []\n",
    "    for source in sources:\n",
    "        header = f\"{default_conversation.system}\\n\\n\"\n",
    "        conversation = _add_speaker_and_signal(header, source)\n",
    "        conversations.append(conversation)\n",
    "    # tokenize conversations\n",
    "    def get_tokenize_len(prompts):\n",
    "        return [len(tokenizer_image_token(prompt, tokenizer)) for prompt in prompts]\n",
    "\n",
    "    if has_image:\n",
    "        input_ids = [tokenizer_image_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations]\n",
    "        for idx, tensor in enumerate(input_ids):\n",
    "            if hasattr(tensor, 'shape'):\n",
    "                print(f\"input_ids[{idx}].shape\\n\", tensor.shape)\n",
    "    else:\n",
    "        conversations_tokenized = _tokenize_fn(conversations, tokenizer)\n",
    "        input_ids = conversations_tokenized[\"input_ids\"]\n",
    "\n",
    "    targets = copy.deepcopy(input_ids)\n",
    "    if isinstance(targets, list):\n",
    "        for idx, tensor in enumerate(targets):\n",
    "            if hasattr(tensor, 'shape'):\n",
    "                print(f\"targets[{idx}].shape\\n\", tensor.shape)\n",
    "    elif hasattr(targets, 'shape'):\n",
    "        print(\"targets.shape\\n\", targets.shape)\n",
    "    for target, source in zip(targets, sources):\n",
    "        if has_image:\n",
    "            tokenized_lens = get_tokenize_len([header] + [s[\"value\"] for s in source])\n",
    "        else:\n",
    "            tokenized_lens = _tokenize_fn([header] + [s[\"value\"] for s in source], tokenizer)[\"input_ids_lens\"]\n",
    "        speakers = [sentence[\"from\"] for sentence in source]\n",
    "        _mask_targets(target, tokenized_lens, speakers)\n",
    "\n",
    "    print(\"return dict(input_ids=input_ids, labels=targets)\\n\", dict(input_ids=input_ids, labels=targets))\n",
    "    return dict(input_ids=input_ids, labels=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_multimodal(\n",
    "    sources: Sequence[str],\n",
    "    data_args: DataArguments\n",
    ") -> Dict:\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def preprocess_multimodal(sources, data_args)\")\n",
    "    print(\"sources\\n\", sources) # [[{'from': 'human', 'value': 'Give a brief description of the image.\\n<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]\n",
    "    print(\"data_args\\n\", data_args) # DataArguments(data_path='/content/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/LLaVA/images', image_aspect_ratio='square')\n",
    "    is_multimodal = data_args.is_multimodal \n",
    "    print(\"is_multimodal\\n\", is_multimodal) # True\n",
    "    if not is_multimodal:\n",
    "        pass\n",
    "\n",
    "    for source in sources:\n",
    "        print(\"source current loop\\n\", source)\n",
    "        for sentence in source:\n",
    "            print(\"sentence current loop\\n\", sentence)\n",
    "            print(\"【COND】 if DEFAULT_IMAGE_TOKEN in sentence['value']:\", DEFAULT_IMAGE_TOKEN in sentence['value'])\n",
    "            print(\"sentence['value']\\n\", sentence['value'])\n",
    "            print(\"DEFAULT_IMAGE_TOKEN\\n\", DEFAULT_IMAGE_TOKEN)\n",
    "            if DEFAULT_IMAGE_TOKEN in sentence['value']:\n",
    "                print(\"【ENTER】if DEFAULT_IMAGE_TOKEN in sentence['value']:\")\n",
    "                sentence['value'] = sentence['value'].replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
    "                sentence['value'] = DEFAULT_IMAGE_TOKEN + '\\n' + sentence['value']\n",
    "                sentence['value'] = sentence['value'].strip()\n",
    "                if \"mmtag\" in default_conversation.version:\n",
    "                    sentence['value'] = sentence['value'].replace(DEFAULT_IMAGE_TOKEN, '<Image>' + DEFAULT_IMAGE_TOKEN + '</Image>')\n",
    "            replace_token = DEFAULT_IMAGE_TOKEN\n",
    "            if data_args.mm_use_im_start_end:\n",
    "                replace_token = DEFAULT_IM_START_TOKEN + replace_token + DEFAULT_IM_END_TOKEN\n",
    "            sentence[\"value\"] = sentence[\"value\"].replace(DEFAULT_IMAGE_TOKEN, replace_token)\n",
    "    print(\"sources (final return)\\n\", sources)\n",
    "    return sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from PIL import Image\n",
    "\n",
    "# Trainer > def _get_dataloader > dataloader = self.accelerator.prepare(DataLoader(dataset, **dataloader_params))\n",
    "def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def LazySupervisedDataset.__getitem__(self, i)\")\n",
    "    print(\"i\\n\", i) # 0\n",
    "    sources = self.list_data_dict[i]\n",
    "    print(\"sources\\n\", sources)\n",
    "    print(\"【COND】 isinstance(i, int):\", isinstance(i, int))\n",
    "    if isinstance(i, int):\n",
    "        print(\"【ENTER】if isinstance(i, int):\")\n",
    "        sources = [sources]\n",
    "        print(\"sources (after)\\n\", sources)\n",
    "        print(\"【EXIT】if isinstance(i, int):\")\n",
    "    assert len(sources) == 1, \"Don't know why it is wrapped to a list\"  # FIXME\n",
    "    print(\"【COND】 'image' in sources[0]:\", 'image' in sources[0])\n",
    "    if 'image' in sources[0]:\n",
    "        print(\"【ENTER】if 'image' in sources[0]:\")\n",
    "        image_file = self.list_data_dict[i]['image']\n",
    "        print(\"image_file\\n\", image_file)\n",
    "        image_folder = self.data_args.image_folder\n",
    "        print(\"image_folder\\n\", image_folder)\n",
    "        processor = self.data_args.image_processor\n",
    "        print(\"processor\\n\", processor)\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        print(\"image_path\\n\", image_path)\n",
    "        try:\n",
    "            print(\"Trying to open image...\")\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            print(\"Image opened successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image: {e}\")\n",
    "            # 画像がなければこのサンプルはスキップ\n",
    "            print(\"Skipping this sample due to image loading error.\")\n",
    "            return None \n",
    "        print(\"【COND】 self.data_args.image_aspect_ratio\", self.data_args.image_aspect_ratio) # square\n",
    "        if self.data_args.image_aspect_ratio == 'pad':\n",
    "            pass\n",
    "        else:\n",
    "            print(\"【ENTER】else (self.data_args.image_aspect_ratio != 'pad')\")\n",
    "            print(\"image (before)\\n\", image)\n",
    "            image = processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n",
    "            print(\"image (after processor.preprocess)\\n\", image)\n",
    "        print(\"sources (before preprocess_multimodal)\\n\", sources)\n",
    "        sources = preprocess_multimodal(\n",
    "            copy.deepcopy([e[\"conversations\"] for e in sources]),\n",
    "            self.data_args)\n",
    "        print(\"sources (after preprocess_multimodal)\\n\", sources)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print(\"Calling preprocess...\")\n",
    "    data_dict = preprocess(\n",
    "        sources,\n",
    "        self.tokenizer,\n",
    "        has_image=('image' in self.list_data_dict[i]))\n",
    "    print(\"data_dict (after preprocess)\\n\", data_dict)\n",
    "    print(\"【COND】 isinstance(i, int):\", isinstance(i, int))\n",
    "    if isinstance(i, int):\n",
    "        data_dict = dict(input_ids=data_dict[\"input_ids\"][0],\n",
    "                            labels=data_dict[\"labels\"][0])\n",
    "\n",
    "    # image exist in the data\n",
    "    if 'image' in self.list_data_dict[i]:\n",
    "        data_dict['image'] = image\n",
    "    elif self.data_args.is_multimodal:\n",
    "        # image does not exist in the data, but the model is multimodal\n",
    "        crop_size = self.data_args.image_processor.crop_size\n",
    "        data_dict['image'] = torch.zeros(3, crop_size['height'], crop_size['width'])\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LazySupervisedDataset.__getitem__ = __getitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    model_max_length=training_args.model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "\n",
    "print(\"tokenizer\\n\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LazySupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path, data_args=data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_dict = train_dataset.__getitem__(0)\n",
    "print(\"sample_data_dict\\n\", sample_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = [sample_data_dict]\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "batch = data_collator(instances)\n",
    "print(\"batch\\n\", batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = batch['images']\n",
    "print(\"images shape\\n\", images.shape) # torch.Size([1, 3, 336, 336])\n",
    "print(\"images\\n\", images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images(self, images):\n",
    "    print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "    print(\"def LlavaMetaForCausalLM(ABC).encode_images(self, images)\")\n",
    "    print(\"images\\n\", images)\n",
    "    image_features = self.get_model().get_vision_tower()(images)\n",
    "    image_features = self.get_model().mm_projector(image_features)\n",
    "    print(\"image_features (return) shape\\n\", image_features.shape)\n",
    "    print(\"image_features (return)\\n\", image_features)\n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LlavaMetaForCausalLM.encode_images = encode_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_forward_outs から、指定した層の特徴量 (B, 577, 1024) を取り出したのち、パッチ特徴量 (B, 576, 1024) のみを返す。\n",
    "def feature_select(self, image_forward_outs):\n",
    "\n",
    "    print(\"current file path\", \"llava/llava/model/multimodal_encoder/clip_encoder.py\")\n",
    "    print(\"def CLIPVisionTower.feature_select(self, image_forward_outs)\")\n",
    "    print(\"image_forward_outs\\n\", image_forward_outs) # 24層のtuple\n",
    "    image_features = image_forward_outs.hidden_states[self.select_layer]\n",
    "    print(\"image_features (after select_layer)\\n\", type(image_features))\n",
    "    if hasattr(image_features, 'shape'):\n",
    "        print(\"image_features.shape\\n\", image_features.shape) # torch.Size([1, 577, 1024])\n",
    "    print(f\"【COND】 select_feature={self.select_feature}\") # patch\n",
    "    if self.select_feature == 'patch':\n",
    "        print(\"【ENTER】if self.select_feature == 'patch':\")\n",
    "        print(\"original image_features\\n\", image_features)\n",
    "        \"\"\"\n",
    "        tensor([[[ 0.2236,  0.2432, -0.5938,  ...,  0.4863, -0.5273, -0.2041],\n",
    "                [-0.0469, -0.1836, -0.0273,  ...,  0.3535,  0.3750,  0.3047],\n",
    "                [-0.2598,  1.1484,  0.4844,  ...,  0.4961, -0.1719, -0.5117],\n",
    "                ...,\n",
    "                [ 1.7188,  0.9688,  0.8828,  ..., -0.2441, -0.8672,  1.3047],\n",
    "                [ 0.7891, -0.3984,  0.6797,  ..., -0.3594, -0.9922,  0.3164],\n",
    "                [ 1.5000,  0.6250,  0.3672,  ..., -0.5469, -0.4902,  0.9766]]],\n",
    "            device='cuda:0', dtype=torch.bfloat16)\n",
    "        \"\"\"\n",
    "        image_features = image_features[:, 1:]\n",
    "        print(\"after process\\n\", image_features)\n",
    "        \"\"\"\n",
    "        tensor([[[-0.0469, -0.1836, -0.0273,  ...,  0.3535,  0.3750,  0.3047],\n",
    "                [-0.2598,  1.1484,  0.4844,  ...,  0.4961, -0.1719, -0.5117],\n",
    "                [ 1.0625, -0.0635, -0.3730,  ...,  0.0220,  0.0820,  0.4805],\n",
    "                ...,\n",
    "                [ 1.7188,  0.9688,  0.8828,  ..., -0.2441, -0.8672,  1.3047],\n",
    "                [ 0.7891, -0.3984,  0.6797,  ..., -0.3594, -0.9922,  0.3164],\n",
    "                [ 1.5000,  0.6250,  0.3672,  ..., -0.5469, -0.4902,  0.9766]]],\n",
    "            device='cuda:0', dtype=torch.bfloat16)\n",
    "        \"\"\"\n",
    "        print(\"【EXIT】if self.select_feature == 'patch':\")\n",
    "    elif self.select_feature == 'cls_patch':\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "    print(\"selected image_feature shape\\n\", image_features.shape) \n",
    "    print(\"image_features (return)\\n\", image_features)\n",
    "    \"\"\"\n",
    "    image_features (return)\n",
    "    tensor([[[-0.0469, -0.1836, -0.0273,  ...,  0.3535,  0.3750,  0.3047],\n",
    "            [-0.2598,  1.1484,  0.4844,  ...,  0.4961, -0.1719, -0.5117],\n",
    "            [ 1.0625, -0.0635, -0.3730,  ...,  0.0220,  0.0820,  0.4805],\n",
    "            ...,\n",
    "            [ 1.7188,  0.9688,  0.8828,  ..., -0.2441, -0.8672,  1.3047],\n",
    "            [ 0.7891, -0.3984,  0.6797,  ..., -0.3594, -0.9922,  0.3164],\n",
    "            [ 1.5000,  0.6250,  0.3672,  ..., -0.5469, -0.4902,  0.9766]]],\n",
    "        device='cuda:0', dtype=torch.bfloat16)\n",
    "    \"\"\"\n",
    "    if hasattr(image_features, 'shape'):\n",
    "        print(\"image_features.shape\\n\", image_features.shape) # torch.Size([1, 576, 1024])\n",
    "    return image_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIPVisionTower.feature_select = feature_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() \n",
    "def forward(self, images):\n",
    "\n",
    "    print(\"current file path\", \"llava/llava/model/multimodal_encoder/clip_encoder.py\")\n",
    "    print(\"def CLIPVisionTower.forward(self, images)\")\n",
    "    print(\"images shape\\n\", images.shape) # torch.Size([1, 3, 336, 336])\n",
    "    print(\"images\\n\", images)\n",
    "    \n",
    "    if hasattr(images, 'shape'):\n",
    "        print(\"images.shape\\n\", images.shape) # torch.Size([1, 3, 336, 336])\n",
    "    print(f\"【COND】 type_images_is_list={type(images) is list}\") # False\n",
    "    if type(images) is list:\n",
    "        pass\n",
    "    else:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】else (type(images) is not list):\")\n",
    "        print(\"original images\\n\", images)\n",
    "        image_forward_outs = self.vision_tower(images.to(device=self.vision_tower.device, dtype=self.vision_tower.dtype), output_hidden_states=True)\n",
    "        print(\"after process image_forward_outs\\n\", type(image_forward_outs)) # 24層のtuple\n",
    "        image_features = self.feature_select(image_forward_outs).to(images.dtype)\n",
    "        print(\"after process image_features\\n\", type(image_features)) # <class 'torch.Tensor'>\n",
    "        print(\"【EXIT】else (type(images) is not list):\")\n",
    "\n",
    "    print(\"image_features (return)\\n\", image_features)\n",
    "    \"\"\"\n",
    "    image_features (return)\n",
    "    tensor([[[-0.0469, -0.1836, -0.0273,  ...,  0.3535,  0.3750,  0.3047],\n",
    "            [-0.2598,  1.1484,  0.4844,  ...,  0.4961, -0.1719, -0.5117],\n",
    "            [ 1.0625, -0.0635, -0.3730,  ...,  0.0220,  0.0820,  0.4805],\n",
    "            ...,\n",
    "            [ 1.7188,  0.9688,  0.8828,  ..., -0.2441, -0.8672,  1.3047],\n",
    "            [ 0.7891, -0.3984,  0.6797,  ..., -0.3594, -0.9922,  0.3164],\n",
    "            [ 1.5000,  0.6250,  0.3672,  ..., -0.5469, -0.4902,  0.9766]]],\n",
    "        device='cuda:0', dtype=torch.bfloat16)\n",
    "    \"\"\"\n",
    "    if hasattr(image_features, 'shape'):\n",
    "        print(\"image_features.shape\\n\", image_features.shape) # \n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIPVisionTower.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = model.encode_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs_labels_for_multimodal(\n",
    "    self, input_ids, position_ids, attention_mask, past_key_values, labels,\n",
    "    images, image_sizes=None\n",
    "):\n",
    "    print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "    \"\"\"\n",
    "    llava/llava/model/language_model/llava_llama.py\n",
    "    \"\"\"\n",
    "    print(\"def LlavaMetaForCausalLM(ABC).prepare_inputs_labels_for_multimodal(self, input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes=None)\")  # not found\n",
    "    print(\"input_ids\\n\", input_ids)\n",
    "    \"\"\"\n",
    "    tensor([[    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "             10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "               411,  2654, 11315,    13]])\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"position_ids\\n\", position_ids)  # None\n",
    "    print(\"attention_mask\\n\", attention_mask)\n",
    "    \"\"\"\n",
    "    tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True]])\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"past_key_values\\n\", past_key_values)  # None\n",
    "    print(\"labels\\n\", labels)\n",
    "    \"\"\"\n",
    "    tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "             10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "               411,  2654, 11315,    13]])\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"images\\n\", images)\n",
    "    \"\"\"\n",
    "    tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
    "              [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
    "              [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
    "              ...,\n",
    "              [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
    "              [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
    "              [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
    "    \n",
    "             [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
    "              [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
    "              [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
    "              ...,\n",
    "              [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
    "              [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
    "              [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
    "    \n",
    "             [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
    "              [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
    "              [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
    "              ...,\n",
    "              [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
    "              [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
    "              [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"image_sizes\\n\", image_sizes)  # None\n",
    "    vision_tower = self.get_vision_tower()\n",
    "    print(\"vision_tower\\n\", vision_tower)\n",
    "    \"\"\"\n",
    "    CLIPVisionTower(\n",
    "      (vision_tower): CLIPVisionModel(\n",
    "        (vision_model): CLIPVisionTransformer(\n",
    "          (embeddings): CLIPVisionEmbeddings(\n",
    "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "            (position_embedding): Embedding(577, 1024)\n",
    "          )\n",
    "          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "          (encoder): CLIPEncoder(\n",
    "            (layers): ModuleList(\n",
    "              (0-23): 24 x CLIPEncoderLayer(\n",
    "                (self_attn): CLIPAttention(\n",
    "                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                (mlp): CLIPMLP(\n",
    "                  (activation_fn): QuickGELUActivation()\n",
    "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "              )\n",
    "            )\n",
    "          )\n",
    "          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"【COND】 vision_tower_is_None={vision_tower is None} images_is_None={images is None} input_ids_shape_1_eq_1={input_ids.shape[1] == 1}\")\n",
    "    if vision_tower is None or images is None or input_ids.shape[1] == 1:\n",
    "        pass\n",
    "\n",
    "    print(\"【COND】type(images)\\n\", type(images))  # <class 'torch.Tensor'>\n",
    "    print(\"【COND】images.ndim\\n\", images.ndim)  # 4\n",
    "    if type(images) is list or images.ndim == 5:\n",
    "        pass\n",
    "    else:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】else of if type(images) is list or images.ndim == 5:\")\n",
    "        image_features = self.encode_images(images)\n",
    "        print(\"image_features after encode_images shape \\n\", image_features.shape)  # torch.Size([1, 576, 2048])\n",
    "        print(\"image_features after encode_images\\n\", image_features)\n",
    "        \"\"\"\n",
    "        tensor([[[-0.1943,  0.1157, -0.0747,  ...,  0.0027, -0.1691, -0.3439],\n",
    "                 [ 0.0437,  0.1717, -0.0998,  ...,  0.0930, -0.1386, -0.0731],\n",
    "                 [-0.0505,  0.1592, -0.0982,  ...,  0.0866, -0.1123, -0.2177],\n",
    "                 ...,\n",
    "                 [-0.0182,  0.0850, -0.0556,  ...,  0.0622, -0.1969,  0.0129],\n",
    "                 [-0.0651,  0.0586, -0.1218,  ..., -0.0614, -0.1158, -0.0104],\n",
    "                 [ 0.0863,  0.0081, -0.1651,  ..., -0.2040, -0.0455,  0.0618]]],\n",
    "               grad_fn=<ViewBackward0>)\n",
    "        \"\"\"\n",
    "        print(\"【EXIT】else of if type(images) is list or images.ndim == 5:\")\n",
    "\n",
    "    # TODO: image start / end is not implemented here to support pretraining.\n",
    "    if getattr(self.config, 'tune_mm_mlp_adapter', False) and getattr(self.config, 'mm_use_im_start_end', False):\n",
    "        print(\"【ENTER】if getattr(self.config, 'tune_mm_mlp_adapter', False) and getattr(self.config, 'mm_use_im_start_end', False):\")  # not found\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Let's just add dummy tensors if they do not exist,\n",
    "    # it is a headache to deal with None all the time.\n",
    "    # But it is not ideal, and if you have a better idea,\n",
    "    # please open an issue / submit a PR, thanks.\n",
    "\n",
    "    print(\"labels before\\n\", labels)\n",
    "    \"\"\"\n",
    "    tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "             10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "               411,  2654, 11315,    13]])\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"position_ids before\\n\", position_ids)  # None\n",
    "\n",
    "    print(\"attention_mask before\\n\", attention_mask)\n",
    "    \"\"\"\n",
    "    tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True]])\n",
    "    \"\"\"\n",
    "\n",
    "    _labels = labels\n",
    "    _position_ids = position_ids\n",
    "    _attention_mask = attention_mask\n",
    "    if attention_mask is None:\n",
    "        pass\n",
    "    else:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】else of if attention_mask is None:\")\n",
    "        attention_mask = attention_mask.bool()\n",
    " \n",
    "        print(\"attention_mask（after）shape \\n\", attention_mask.shape)  # torch.Size([1, 24])\n",
    "        print(\"attention_mask (after)\\n\", attention_mask)\n",
    "        \"\"\"\n",
    "        tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True]])\n",
    "        \"\"\"\n",
    "        print(\"【EXIT】else of if attention_mask is None:\")\n",
    "    if position_ids is None:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】if position_ids is None:\")\n",
    "        position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "        print(\"position_ids (after) shape \\n\", position_ids.shape)  # torch.Size([24])\n",
    "        print(\"position_ids (after)\\n\", position_ids)\n",
    "        \"\"\"\n",
    "        tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
    "                18, 19, 20, 21, 22, 23])\n",
    "        \"\"\"\n",
    "        print(\"【EXIT】if position_ids is None:\")\n",
    "    print(f\"【COND】 labels_is_None={labels is None}\")\n",
    "    if labels is None:\n",
    "        pass\n",
    "\n",
    "    # remove the padding using attention_mask -- FIXME\n",
    "    _input_ids = input_ids\n",
    "    input_ids = [cur_input_ids[cur_attention_mask] for cur_input_ids, cur_attention_mask in zip(input_ids, attention_mask)]\n",
    "    labels = [cur_labels[cur_attention_mask] for cur_labels, cur_attention_mask in zip(labels, attention_mask)]\n",
    "    print(\"input_ids after removing padding\\n\", input_ids)\n",
    "    \"\"\"\n",
    "    [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "            10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "              411,  2654, 11315,    13])]\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"labels after removing padding\\n\", labels)\n",
    "    \"\"\"\n",
    "    [tensor([ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "            10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "              411,  2654, 11315,    13])]\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    new_input_embeds = []\n",
    "    new_labels = []\n",
    "    cur_image_idx = 0\n",
    "    for batch_idx, cur_input_ids in enumerate(input_ids):\n",
    "        print(\"cur_input_ids shape\\n\", cur_input_ids.shape)   # torch.Size([24])\n",
    "        print(\"cur_input_ids\\n\", cur_input_ids)\n",
    "        \"\"\"\n",
    "        tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "                10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "                  411,  2654, 11315,    13])\n",
    "        \"\"\"\n",
    "        num_images = (cur_input_ids == IMAGE_TOKEN_INDEX).sum()\n",
    "        print(\"【COND】num_images:\", num_images)  # tensor(1)\n",
    "        if num_images == 0:\n",
    "            print(\"【ENTER】if num_images == 0:\")\n",
    "            cur_image_features = image_features[cur_image_idx]\n",
    "            cur_input_embeds_1 = self.get_model().embed_tokens(cur_input_ids)\n",
    "            cur_input_embeds = torch.cat([cur_input_embeds_1, cur_image_features[0:0]], dim=0)\n",
    "            new_input_embeds.append(cur_input_embeds)\n",
    "            new_labels.append(labels[batch_idx])\n",
    "            cur_image_idx += 1\n",
    "            print(\"【EXIT】if num_images == 0:\")\n",
    "            continue\n",
    "\n",
    "        image_token_indices = [-1] + torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)[0].tolist() + [cur_input_ids.shape[0]]\n",
    "        print(\"image_token_indices\\n\", image_token_indices)  # [-1, 1, 24]\n",
    "        print(\"len image_token_indices\", len(image_token_indices))   # 3\n",
    "        cur_input_ids_noim = []\n",
    "        cur_labels = labels[batch_idx]\n",
    "        print(\"cur_labels\\n\", cur_labels)\n",
    "        \"\"\"\n",
    "        tensor([ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "                10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "                  411,  2654, 11315,    13])\n",
    "        \"\"\"\n",
    "        cur_labels_noim = []\n",
    "        for i in range(len(image_token_indices) - 1): # 2回ループ。1回目 START から IMAGE_TOKEN_INDEXの手前まで、2回目はIMAGE_TOKEN_INDEX より先から 最後まで\n",
    "            cur_input_ids_noim.append(cur_input_ids[image_token_indices[i]+1:image_token_indices[i+1]])\n",
    "            cur_labels_noim.append(cur_labels[image_token_indices[i]+1:image_token_indices[i+1]])\n",
    "        print(\"cur_input_ids_noim (after)\\n\", cur_input_ids_noim)\n",
    "        \"\"\"\n",
    "        [tensor([1]), tensor([  278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596,\n",
    "                23425,   278,  3700,   322,  6567,   310,   263,  6114,   411,  2654,\n",
    "                11315,    13])]\n",
    "        \"\"\"\n",
    "        print(\"cur_labels_noim (after) \\n\", cur_labels_noim)\n",
    "        \"\"\"\n",
    "        [tensor([-100]), tensor([  278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596,\n",
    "                23425,   278,  3700,   322,  6567,   310,   263,  6114,   411,  2654,\n",
    "                11315,    13])]\n",
    "        \"\"\"\n",
    "        split_sizes = [x.shape[0] for x in cur_labels_noim]\n",
    "        print(\"split_sizes\\n\", split_sizes)  # [1, 22]\n",
    "        cur_input_embeds = self.get_model().embed_tokens(torch.cat(cur_input_ids_noim))\n",
    "        print(\"cur_input_embeds shape\\n\", cur_input_embeds.shape)  # torch.Size([23, 2048])\n",
    "        print(\"cur_input_embeds\\n\", cur_input_embeds)\n",
    "        \"\"\"\n",
    "        tensor([[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
    "                 -6.5231e-04, -4.9973e-04],\n",
    "                [ 7.0801e-03,  1.0452e-03,  6.0425e-03,  ...,  3.9673e-03,\n",
    "                  1.2817e-03, -1.1215e-03],\n",
    "                [-2.2949e-02, -2.6226e-05,  6.8359e-03,  ..., -2.4658e-02,\n",
    "                 -9.4604e-03,  1.5869e-02],\n",
    "                ...,\n",
    "                [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
    "                 -8.3618e-03, -9.4604e-03],\n",
    "                [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
    "                 -2.5177e-03, -8.0566e-03],\n",
    "                [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
    "                 -7.3242e-04,  2.7924e-03]], requires_grad=True)\n",
    "        \"\"\"\n",
    "        cur_input_embeds_no_im = torch.split(cur_input_embeds, split_sizes, dim=0)\n",
    "        print(\"cur_input_embeds_no_im\\n\", cur_input_embeds_no_im)\n",
    "        \"\"\"\n",
    "        (tensor([[-0.0011,  0.0019, -0.0017,  ...,  0.0002, -0.0007, -0.0005]],\n",
    "               grad_fn=<SplitWithSizesBackward0>), tensor([[ 7.0801e-03,  1.0452e-03,  6.0425e-03,  ...,  3.9673e-03,\n",
    "                  1.2817e-03, -1.1215e-03],\n",
    "                [-2.2949e-02, -2.6226e-05,  6.8359e-03,  ..., -2.4658e-02,\n",
    "                 -9.4604e-03,  1.5869e-02],\n",
    "                [-2.3499e-03,  1.4893e-02, -2.0447e-03,  ..., -8.6060e-03,\n",
    "                  2.3193e-03,  3.0670e-03],\n",
    "                ...,\n",
    "                [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
    "                 -8.3618e-03, -9.4604e-03],\n",
    "                [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
    "                 -2.5177e-03, -8.0566e-03],\n",
    "                [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
    "                 -7.3242e-04,  2.7924e-03]], grad_fn=<SplitWithSizesBackward0>))\n",
    "        \"\"\"\n",
    "        cur_new_input_embeds = []\n",
    "        cur_new_labels = []\n",
    "\n",
    "        for i in range(num_images + 1):\n",
    "            cur_new_input_embeds.append(cur_input_embeds_no_im[i])\n",
    "            cur_new_labels.append(cur_labels_noim[i])\n",
    "            print(f\"【COND】 i={i} num_images={num_images}\")\n",
    "            if i < num_images:\n",
    "                print(\"【ENTER】if i < num_images:\")\n",
    "                cur_image_features = image_features[cur_image_idx]\n",
    "                cur_image_idx += 1\n",
    "                cur_new_input_embeds.append(cur_image_features)\n",
    "                cur_new_labels.append(torch.full((cur_image_features.shape[0],), IGNORE_INDEX, device=cur_labels.device, dtype=cur_labels.dtype))\n",
    "                print(\"【EXIT】if i < num_images:\")\n",
    "\n",
    "        print(\"cur_new_input_embeds (before cat) shape\\n\", [x.shape for x in cur_new_input_embeds])\n",
    "        \"\"\"\n",
    "        [torch.Size([1, 2048]), torch.Size([576, 2048]), torch.Size([22, 2048])]\n",
    "        \"\"\"\n",
    "        print(\"cur_new_input_embeds (before cat)\\n\", cur_new_input_embeds)\n",
    "        \"\"\"\n",
    "        [tensor([[-0.0011,  0.0019, -0.0017,  ...,  0.0002, -0.0007, -0.0005]],\n",
    "               grad_fn=<SplitWithSizesBackward0>), tensor([[-0.1943,  0.1157, -0.0747,  ...,  0.0027, -0.1691, -0.3439],\n",
    "                [ 0.0437,  0.1717, -0.0998,  ...,  0.0930, -0.1386, -0.0731],\n",
    "                [-0.0505,  0.1592, -0.0982,  ...,  0.0866, -0.1123, -0.2177],\n",
    "                ...,\n",
    "                [-0.0182,  0.0850, -0.0556,  ...,  0.0622, -0.1969,  0.0129],\n",
    "                [-0.0651,  0.0586, -0.1218,  ..., -0.0614, -0.1158, -0.0104],\n",
    "                [ 0.0863,  0.0081, -0.1651,  ..., -0.2040, -0.0455,  0.0618]],\n",
    "               grad_fn=<SelectBackward0>), tensor([[ 7.0801e-03,  1.0452e-03,  6.0425e-03,  ...,  3.9673e-03,\n",
    "                  1.2817e-03, -1.1215e-03],\n",
    "                [-2.2949e-02, -2.6226e-05,  6.8359e-03,  ..., -2.4658e-02,\n",
    "                 -9.4604e-03,  1.5869e-02],\n",
    "                [-2.3499e-03,  1.4893e-02, -2.0447e-03,  ..., -8.6060e-03,\n",
    "                  2.3193e-03,  3.0670e-03],\n",
    "                ...,\n",
    "                [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
    "                 -8.3618e-03, -9.4604e-03],\n",
    "                [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
    "                 -2.5177e-03, -8.0566e-03],\n",
    "                [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
    "                 -7.3242e-04,  2.7924e-03]], grad_fn=<SplitWithSizesBackward0>)]\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"cur_new_labels (before cat) shape\\n\", [x.shape for x in cur_new_labels])\n",
    "        \"\"\"\n",
    "        [torch.Size([1]), torch.Size([576]), torch.Size([22])]\n",
    "        \"\"\"\n",
    "        print(\"cur_new_labels (before cat)\\n\", cur_new_labels)\n",
    "        \"\"\"\n",
    "        [tensor([-100]), tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]), tensor([  278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596,\n",
    "                23425,   278,  3700,   322,  6567,   310,   263,  6114,   411,  2654,\n",
    "                11315,    13])]\n",
    "        \"\"\"\n",
    "        cur_new_input_embeds = [x.to(self.device) for x in cur_new_input_embeds]\n",
    "\n",
    "        cur_new_input_embeds = torch.cat(cur_new_input_embeds)\n",
    "        cur_new_labels = torch.cat(cur_new_labels)\n",
    "\n",
    "        print(\"cur_new_input_embeds (after cat) shape\\n\", cur_new_input_embeds.shape)  # torch.Size([599, 2048])\n",
    "        print(\"cur_new_input_embeds (after cat)\\n\", cur_new_input_embeds)\n",
    "        \"\"\"\n",
    "        tensor([[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
    "                 -6.5231e-04, -4.9973e-04],\n",
    "                [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
    "                 -1.6907e-01, -3.4387e-01],\n",
    "                [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
    "                 -1.3859e-01, -7.3106e-02],\n",
    "                ...,\n",
    "                [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
    "                 -8.3618e-03, -9.4604e-03],\n",
    "                [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
    "                 -2.5177e-03, -8.0566e-03],\n",
    "                [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
    "                 -7.3242e-04,  2.7924e-03]], grad_fn=<CatBackward0>)\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"cur_new_labels (after cat) shape\\n\", cur_new_labels.shape)  # torch.Size([599])\n",
    "        print(\"cur_new_labels (after cat)\\n\", cur_new_labels)\n",
    "        \"\"\"\n",
    "        tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
    "                  297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
    "                  322,  6567,   310,   263,  6114,   411,  2654, 11315,    13])\n",
    "        \"\"\"\n",
    "\n",
    "        new_input_embeds.append(cur_new_input_embeds)\n",
    "        new_labels.append(cur_new_labels)\n",
    "        print(\"new_input_embeds (so far) shape\\n\", [x.shape for x in new_input_embeds])  # [torch.Size([599, 2048])]\n",
    "        print(\"new_input_embeds (so far)\\n\", new_input_embeds)\n",
    "        \"\"\"\n",
    "        [tensor([[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
    "                 -6.5231e-04, -4.9973e-04],\n",
    "                [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
    "                 -1.6907e-01, -3.4387e-01],\n",
    "                [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
    "                 -1.3859e-01, -7.3106e-02],\n",
    "                ...,\n",
    "                [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
    "                 -8.3618e-03, -9.4604e-03],\n",
    "                [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
    "                 -2.5177e-03, -8.0566e-03],\n",
    "                [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
    "                 -7.3242e-04,  2.7924e-03]], grad_fn=<CatBackward0>)]\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"new_labels (so far) shape\\n\", [x.shape for x in new_labels])  # [torch.Size([599])]\n",
    "        print(\"new_labels (so far)\\n\", new_labels)\n",
    "        \"\"\"\n",
    "        [tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
    "                  297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
    "                  322,  6567,   310,   263,  6114,   411,  2654, 11315,    13])]\n",
    "        \"\"\"\n",
    "\n",
    "    # Truncate sequences to max length as image embeddings can make the sequence longer\n",
    "    tokenizer_model_max_length = getattr(self.config, 'tokenizer_model_max_length', None)\n",
    "    print(f\"【COND】 tokenizer_model_max_length_is_not_None={tokenizer_model_max_length is not None}\")\n",
    "    if tokenizer_model_max_length is not None:\n",
    "        print(\"【ENTER】if tokenizer_model_max_length is not None:\")\n",
    "        new_input_embeds = [x[:tokenizer_model_max_length] for x in new_input_embeds]\n",
    "        new_labels = [x[:tokenizer_model_max_length] for x in new_labels]\n",
    "        print(\"【EXIT】if tokenizer_model_max_length is not None:\")\n",
    "\n",
    "    # Combine them\n",
    "    max_len = max(x.shape[0] for x in new_input_embeds)\n",
    "    print(\"max_len\\n\", max_len)  # 599\n",
    "    batch_size = len(new_input_embeds)\n",
    "    print(\"batch_size\\n\", batch_size)  # 1\n",
    "\n",
    "    new_input_embeds_padded = []\n",
    "    new_labels_padded = torch.full((batch_size, max_len), IGNORE_INDEX, dtype=new_labels[0].dtype, device=new_labels[0].device)\n",
    "    print(\"new_labels_padded (before) shape\\n\", new_labels_padded.shape)  # torch.Size([1, 599])\n",
    "    print(\"new_labels_padded (before)\\n\", new_labels_padded)\n",
    "    \"\"\"\n",
    "    tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]])\n",
    "    \"\"\"\n",
    "    attention_mask = torch.zeros((batch_size, max_len), dtype=attention_mask.dtype, device=attention_mask.device)\n",
    "    print(\"attention_mask (before) shape\\n\", attention_mask.shape)  # torch.Size([1, 599])\n",
    "    print(\"attention_mask (before)\\n\", attention_mask)\n",
    "    \"\"\"\n",
    "    tensor([[False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False]])\n",
    "    \"\"\"\n",
    "    position_ids = torch.zeros((batch_size, max_len), dtype=position_ids.dtype, device=position_ids.device)\n",
    "    print(\"position_ids (before) shape\\n\", position_ids.shape)  # torch.Size([1, 599])\n",
    "    print(\"position_ids (before)\\n\", position_ids)\n",
    "    \"\"\"\n",
    "    tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "    \"\"\"\n",
    "\n",
    "    for i, (cur_new_embed, cur_new_labels) in enumerate(zip(new_input_embeds, new_labels)):\n",
    "        cur_len = cur_new_embed.shape[0]\n",
    "        print(f\"【COND】 padding_side={getattr(self.config, 'tokenizer_padding_side', 'right')} cur_len={cur_len} max_len={max_len}\")\n",
    "        if getattr(self.config, 'tokenizer_padding_side', 'right') == \"left\":\n",
    "            pass\n",
    "        else:\n",
    "            print(\"【ENTER】else (padding_side != 'left'):\")\n",
    "            #【ENTER】\n",
    "            new_input_embeds_padded.append(torch.cat((\n",
    "                cur_new_embed,\n",
    "                torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device)\n",
    "            ), dim=0))\n",
    "            if cur_len > 0:\n",
    "                # :cur_len に、代入\n",
    "                new_labels_padded[i, :cur_len] = cur_new_labels \n",
    "                attention_mask[i, :cur_len] = True\n",
    "                position_ids[i, :cur_len] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\n",
    "            print(\"new_input_embeds_padded (so far) shape\\n\", [x.shape for x in new_input_embeds_padded])  # [torch.Size([599, 2048])]\n",
    "            print(\"new_input_embeds_padded (so far)\\n\", new_input_embeds_padded)\n",
    "            \"\"\"\n",
    "            [tensor([[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
    "                     -6.5231e-04, -4.9973e-04],\n",
    "                    [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
    "                     -1.6907e-01, -3.4387e-01],\n",
    "                    [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
    "                     -1.3859e-01, -7.3106e-02],\n",
    "                    ...,\n",
    "                    [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
    "                     -8.3618e-03, -9.4604e-03],\n",
    "                    [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
    "                     -2.5177e-03, -8.0566e-03],\n",
    "                    [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
    "                     -7.3242e-04,  2.7924e-03]], grad_fn=<CatBackward0>)]\n",
    "            \"\"\"\n",
    "\n",
    "            print(\"new_labels_padded (so far) shape\\n\", new_labels_padded.shape)  # torch.Size([1, 599])\n",
    "            print(\"new_labels_padded (so far)\\n\", new_labels_padded)\n",
    "            \"\"\"\n",
    "            tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
    "                       297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
    "                       322,  6567,   310,   263,  6114,   411,  2654, 11315,    13]])\n",
    "            \"\"\"\n",
    "\n",
    "            print(\"attention_mask (so far) shape\\n\", attention_mask.shape)  # torch.Size([1, 599])\n",
    "            print(\"attention_mask (so far)\\n\", attention_mask)\n",
    "            \"\"\"\n",
    "            tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True]])\n",
    "            \"\"\"\n",
    "\n",
    "            print(\"position_ids (so far) shape\\n\", position_ids.shape)  # torch.Size([1, 599])\n",
    "            print(\"position_ids (so far)\\n\", position_ids)\n",
    "            \"\"\"\n",
    "            tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
    "                      14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
    "                      28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
    "                      42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
    "                      56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
    "                      70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
    "                      84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
    "                      98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
    "                     112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
    "                     126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
    "                     140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
    "                     154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
    "                     168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
    "                     182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
    "                     196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
    "                     210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
    "                     224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
    "                     238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
    "                     252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
    "                     266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
    "                     280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
    "                     294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
    "                     308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
    "                     322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
    "                     336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
    "                     350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
    "                     364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
    "                     378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
    "                     392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
    "                     406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
    "                     420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
    "                     434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
    "                     448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
    "                     462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
    "                     476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
    "                     490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
    "                     504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517,\n",
    "                     518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531,\n",
    "                     532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
    "                     546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559,\n",
    "                     560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573,\n",
    "                     574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587,\n",
    "                     588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598]])\n",
    "            \"\"\"\n",
    "            print(\"【EXIT】else (padding_side != 'left'):\")\n",
    "\n",
    "    new_input_embeds = torch.stack(new_input_embeds_padded, dim=0)\n",
    "    print(\"new_input_embeds (after) shape\\n\", new_input_embeds.shape)  # torch.Size([1, 599, 2048])\n",
    "    print(\"new_input_embeds (after)\\n\", new_input_embeds)\n",
    "    \"\"\"\n",
    "    tensor([[[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
    "              -6.5231e-04, -4.9973e-04],\n",
    "             [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
    "              -1.6907e-01, -3.4387e-01],\n",
    "             [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
    "              -1.3859e-01, -7.3106e-02],\n",
    "             ...,\n",
    "             [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
    "              -8.3618e-03, -9.4604e-03],\n",
    "             [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
    "              -2.5177e-03, -8.0566e-03],\n",
    "             [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
    "              -7.3242e-04,  2.7924e-03]]], grad_fn=<StackBackward0>)\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"【COND】 _labels_is_None={_labels is None}\") \n",
    "    if _labels is None:\n",
    "        #【SKIP】\n",
    "        print(\"【ENTER】if _labels is None:\")\n",
    "        new_labels = None\n",
    "        print(\"【EXIT】if _labels is None:\")\n",
    "    else:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】else of if _labels is None:\")\n",
    "        new_labels = new_labels_padded\n",
    "        print(\"new_labels (after)\\n\", new_labels)\n",
    "        \"\"\"\n",
    "        tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
    "                   297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
    "                   322,  6567,   310,   263,  6114,   411,  2654, 11315,    13]])\n",
    "        \"\"\"\n",
    "        print(\"【EXIT】else of if _labels is None:\")\n",
    "\n",
    "    print(f\"【COND】 _attention_mask_is_None={_attention_mask is None}\") \n",
    "    if _attention_mask is None:\n",
    "        # 【SKIP】\n",
    "        print(\"【ENTER】if _attention_mask is None:\")\n",
    "        attention_mask = None\n",
    "        print(\"【EXIT】if _attention_mask is None:\")\n",
    "    else:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】else of if _attention_mask is None:\")\n",
    "        attention_mask = attention_mask.to(dtype=_attention_mask.dtype)\n",
    "        print(\"attention_mask (after)2\\n\", attention_mask)\n",
    "        \"\"\"\n",
    "        tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True]])\n",
    "        \"\"\"\n",
    "\n",
    "    print(f\"【COND】 _position_ids_is_None={_position_ids is None}\")\n",
    "    if _position_ids is None:\n",
    "        print(\"【ENTER】if _position_ids is None:\")\n",
    "        position_ids = None\n",
    "        print(\"【EXIT】if _position_ids is None:\")\n",
    "\n",
    "    print(\"position_ids (return)\\n\", position_ids)  # None\n",
    "    print(\"attention_mask (return)\\n\", attention_mask)\n",
    "    \"\"\"\n",
    "    tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True]])\n",
    "    \"\"\"\n",
    "    print(\"past_key_values (return)\\n\", past_key_values)  # None\n",
    "    print(\"new_input_embeds (return)\\n\", new_input_embeds)\n",
    "    \"\"\"\n",
    "    tensor([[[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
    "              -6.5231e-04, -4.9973e-04],\n",
    "             [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
    "              -1.6907e-01, -3.4387e-01],\n",
    "             [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
    "              -1.3859e-01, -7.3106e-02],\n",
    "             ...,\n",
    "             [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
    "              -8.3618e-03, -9.4604e-03],\n",
    "             [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
    "              -2.5177e-03, -8.0566e-03],\n",
    "             [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
    "              -7.3242e-04,  2.7924e-03]]], grad_fn=<StackBackward0>)\n",
    "    \"\"\"\n",
    "    print(\"new_labels (return)\\n\", new_labels)\n",
    "    \"\"\"\n",
    "    tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
    "               297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
    "               322,  6567,   310,   263,  6114,   411,  2654, 11315,    13]])\n",
    "    \"\"\"\n",
    "    return None, position_ids, attention_mask, past_key_values, new_input_embeds, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LlavaMetaForCausalLM.prepare_inputs_labels_for_multimodal = prepare_inputs_labels_for_multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model_device\\n\", model.device) # cpu\n",
    "\n",
    "input_ids = batch['input_ids'].to(device=model.device)\n",
    "print(\"input_ids shape\\n\", input_ids.shape) # torch.Size([1, 24])\n",
    "print(\"input_ids\\n\", input_ids)\n",
    "\"\"\"\n",
    "tensor([[    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "           411,  2654, 11315,    13]])\n",
    "\"\"\"\n",
    "\n",
    "attention_mask = batch['attention_mask'].to(device=model.device)\n",
    "print(\"attention_mask shape\\n\", attention_mask.shape) # torch.Size([1, 24])\n",
    "print(\"attention_mask\\n\", attention_mask)\n",
    "\"\"\"\n",
    "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True]])\n",
    "\"\"\"\n",
    "\n",
    "labels = batch['labels'].to(device=model.device)\n",
    "print(\"labels shape\\n\", labels.shape) # torch.Size([1, 3, 336, 336])\n",
    "print(\"labels\\n\", labels)\n",
    "\"\"\"\n",
    "tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "           411,  2654, 11315,    13]])\n",
    "\"\"\"\n",
    "\n",
    "images = batch['images'].to(device=model.device)\n",
    "print(\"images shape\\n\", images.shape)\n",
    "print(\"images\\n\", images)\n",
    "\"\"\"\n",
    "tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
    "          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
    "          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
    "          ...,\n",
    "          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
    "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
    "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
    "\n",
    "         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
    "          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
    "          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
    "          ...,\n",
    "          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
    "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
    "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
    "\n",
    "         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
    "          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
    "          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
    "          ...,\n",
    "          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
    "          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
    "          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids = None\n",
    "past_key_values = None\n",
    "image_sizes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from print_factory.print_factory import run_and_capture, embed_print_outputs\n",
    "\n",
    "logs, mapping = run_and_capture(\n",
    "    model.prepare_inputs_labels_for_multimodal,\n",
    "    input_ids,\n",
    "    position_ids,\n",
    "    attention_mask,\n",
    "    past_key_values,\n",
    "    labels,\n",
    "    images,\n",
    "    image_sizes\n",
    ")\n",
    "\n",
    "with open(\"print_factory/original_code.py\") as f:\n",
    "    code = f.read()\n",
    "\n",
    "new_code = embed_print_outputs(code, mapping)\n",
    "\n",
    "with open(\"print_factory/after_code.py\", \"w\") as f:\n",
    "    f.write(new_code)\n",
    "\n",
    "print(\"done\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    input_ids,\n",
    "    position_ids,\n",
    "    attention_mask,\n",
    "    past_key_values,\n",
    "    inputs_embeds,\n",
    "    labels\n",
    ") = model.prepare_inputs_labels_for_multimodal(\n",
    "    input_ids,\n",
    "    position_ids,\n",
    "    attention_mask,\n",
    "    past_key_values,\n",
    "    labels,\n",
    "    images,\n",
    "    image_sizes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(\n",
    "    self,\n",
    "    input_ids: torch.LongTensor = None,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    labels: Optional[torch.LongTensor] = None,\n",
    "    use_cache: Optional[bool] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    images: Optional[torch.FloatTensor] = None,\n",
    "    image_sizes: Optional[List[List[int]]] = None,\n",
    "    return_dict: Optional[bool] = None,\n",
    ") -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "\n",
    "    print(\"current file path\", \"llava/llava/model/language_model/llava_llama.py\")\n",
    "    print(\"def LlavaLlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, image_sizes, return_dict)\")\n",
    "    print(\"input_ids\\n\", input_ids)\n",
    "    \"\"\"\n",
    "    tensor([[    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "            10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "            411,  2654, 11315,    13]], device='cuda:0')        \n",
    "    \"\"\"\n",
    "    if hasattr(input_ids, 'shape'):\n",
    "        print(\"input_ids.shape\\n\", input_ids.shape) # torch.Size([1, 24])\n",
    "    print(\"attention_mask\\n\", attention_mask)\n",
    "    \"\"\"\n",
    "    tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "            True, True, True, True, True, True, True, True, True, True, True, True]],\n",
    "        device='cuda:0')\n",
    "    \"\"\"\n",
    "    print(\"position_ids\\n\", position_ids) # None\n",
    "    print(\"past_key_values\\n\", past_key_values) # None\n",
    "    print(\"inputs_embeds\\n\", inputs_embeds) # None\n",
    "    if hasattr(inputs_embeds, 'shape'):\n",
    "        print(\"inputs_embeds.shape\\n\", inputs_embeds.shape)\n",
    "    print(\"labels\\n\", labels)\n",
    "    \"\"\"\n",
    "    tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "            10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "            411,  2654, 11315,    13]], device='cuda:0')\n",
    "    \"\"\"\n",
    "    print(\"use_cache\\n\", use_cache) # None\n",
    "    print(\"output_attentions\\n\", output_attentions) # None\n",
    "    print(\"output_hidden_states\\n\", output_hidden_states) # None\n",
    "    print(\"images\\n\", images)\n",
    "    \"\"\"\n",
    "    tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7109, -0.3613, -0.1279],\n",
    "            [ 0.0325,  0.0325,  0.0325,  ..., -0.3906, -0.1719, -0.0259],\n",
    "            [ 0.0325,  0.0325,  0.0325,  ..., -0.0112,  0.0471,  0.0908],\n",
    "            ...,\n",
    "            [-1.0312, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],\n",
    "            [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],\n",
    "            [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625]],\n",
    "\n",
    "            [[ 0.3184,  0.3184,  0.3184,  ..., -0.3867, -0.0112,  0.2139],\n",
    "            [ 0.3184,  0.3184,  0.3184,  ..., -0.0713,  0.1543,  0.3184],\n",
    "            [ 0.3184,  0.3184,  0.3184,  ...,  0.2891,  0.3633,  0.4395],\n",
    "            ...,\n",
    "            [-1.0156, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],\n",
    "            [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],\n",
    "            [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000]],\n",
    "\n",
    "            [[ 0.9648,  0.9648,  0.9648,  ...,  0.0981,  0.4531,  0.6680],\n",
    "            [ 0.9648,  0.9648,  0.9648,  ...,  0.3965,  0.6094,  0.7539],\n",
    "            [ 0.9648,  0.9648,  0.9648,  ...,  0.7539,  0.8086,  0.8359],\n",
    "            ...,\n",
    "            [-0.3711, -0.3848, -0.4004,  ..., -0.4277, -0.4277, -0.4277],\n",
    "            [-0.3711, -0.3711, -0.3848,  ..., -0.4277, -0.4277, -0.4277],\n",
    "            [-0.3848, -0.3711, -0.3711,  ..., -0.4277, -0.4277, -0.4277]]]],\n",
    "        device='cuda:0', dtype=torch.bfloat16)\n",
    "    \"\"\"\n",
    "    if hasattr(images, 'shape'):\n",
    "        print(\"images.shape\\n\", images.shape) # torch.Size([1, 3, 336, 336])\n",
    "    print(\"image_sizes\\n\", image_sizes) # None\n",
    "    print(\"return_dict\\n\", return_dict) # None\n",
    "\n",
    "    print(f\"【COND】 inputs_embeds_is_None={inputs_embeds is None}\") # True\n",
    "    if inputs_embeds is None:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】if inputs_embeds is None:\")\n",
    "        (\n",
    "            input_ids,\n",
    "            position_ids,\n",
    "            attention_mask,\n",
    "            past_key_values,\n",
    "            inputs_embeds,\n",
    "            labels\n",
    "        ) = self.prepare_inputs_labels_for_multimodal(\n",
    "            input_ids,\n",
    "            position_ids,\n",
    "            attention_mask,\n",
    "            past_key_values,\n",
    "            labels,\n",
    "            images,\n",
    "            image_sizes\n",
    "        )\n",
    "        print(\"【EXIT】if inputs_embeds is None:\")\n",
    "\n",
    "    print(\"input_ids (after prepare_inputs_labels_for_multimodal)\\n\", input_ids)\n",
    "\n",
    "    print(\"position_ids (after prepare_inputs_labels_for_multimodal)\\n\", position_ids)\n",
    "\n",
    "    print(\"attention_mask shape (after prepare_inputs_labels_for_multimodal)\\n\", attention_mask.shape)\n",
    "    print(\"attention_mask (after prepare_inputs_labels_for_multimodal)\\n\", attention_mask)\n",
    "\n",
    "\n",
    "    print(\"past_key_values (after prepare_inputs_labels_for_multimodal)\\n\", past_key_values)\n",
    "\n",
    "    print(\"inputs_embeds shape (after prepare_inputs_labels_for_multimodal)\\n\", None if inputs_embeds is None else inputs_embeds.shape)\n",
    "    print(\"inputs_embeds (after prepare_inputs_labels_for_multimodal)\\n\", inputs_embeds)\n",
    "\n",
    "    print(\"labels shape (after prepare_inputs_labels_for_multimodal)\\n\", labels.shape)\n",
    "    print(\"labels (after prepare_inputs_labels_for_multimodal)\\n\", labels)\n",
    "\n",
    "    #  LlamaForCausalLM.forward(self, ...)で明示\n",
    "    # Trainer > def train > def inner_training_loop > def training_step > model(**inputs) > model.forward\n",
    "    result = LlamaForCausalLM.forward(\n",
    "        self,\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_values=past_key_values,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        labels=labels,\n",
    "        use_cache=use_cache,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict\n",
    "    )\n",
    "    print(\"Return of def LlavaLlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, image_sizes, return_dict)\")\n",
    "    #print(\"result of LlavaLlamaForCausalLM.forward (return)\\n\", result)\n",
    "    print(\"logits tensor shape  LlavaLlamaForCausalLM.forward\\n\", result.logits.shape) # torch.Size([1, 599, 32000])\n",
    "    print(\"logits tensor (first 10 tokens)  LlavaLlamaForCausalLM.forward\\n\", result.logits[0, :10, :])\n",
    "    \"\"\"\n",
    "    tensor([[-4.6822,  0.9866,  4.5126,  ..., -5.2010, -2.1646, -4.2286],\n",
    "            [ 3.2882,  3.7620,  1.9036,  ...,  1.6998,  1.9076,  3.4061],\n",
    "            [-7.5813, -7.5340,  3.8222,  ..., -1.3140, -6.1645, -1.1844],\n",
    "            ...,\n",
    "            [-8.3741, -7.6768,  0.9916,  ..., -7.2707, -8.4552, -4.9956],\n",
    "            [-7.4367, -7.1454,  3.2786,  ..., -5.6713, -6.4151, -3.9531],\n",
    "            [-7.0483, -6.7871,  4.4183,  ..., -7.2721, -7.3837, -4.2513]],\n",
    "        grad_fn=<SliceBackward0>)\n",
    "    \"\"\"\n",
    "    print(\"loss (return)  LlavaLlamaForCausalLM.forward \\n\", result.loss) # tensor(9.2224, grad_fn=<NllLossBackward0>)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LlavaLlamaForCausalLM.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"input_ids\": batch['input_ids'].to(device=model.device),\n",
    "    \"attention_mask\": batch['attention_mask'].to(device=model.device),\n",
    "    \"labels\": batch['labels'].to(device=model.device),\n",
    "    \"images\": batch['images'].to(device=model.device),\n",
    "    \"position_ids\": None,\n",
    "    \"past_key_values\": None,\n",
    "    \"image_sizes\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "logs, mapping = run_and_capture(\n",
    "    model.forward,\n",
    "    **inputs\n",
    ")\n",
    "\n",
    "with open(\"print_factory/original_code.py\") as f:\n",
    "    code = f.read()\n",
    "\n",
    "new_code = embed_print_outputs(code, mapping)\n",
    "\n",
    "with open(\"print_factory/after_code.py\", \"w\") as f:\n",
    "    f.write(new_code)\n",
    "\n",
    "print(\"done\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_zero_3(param, ignore_status=False, name=None):\n",
    "\n",
    "    print(\"current file path\", \"llava/train/llava_trainer.py\")\n",
    "    print(\"def maybe_zero_3(param, ignore_status=False, name=None)\")\n",
    "    print(\"param maybe_zero_3\\n\", param)\n",
    "    print(\"ignore_status maybe_zero_3\\n\", ignore_status)\n",
    "    print(\"name maybe_zero_3\\n\", name)\n",
    "    from deepspeed import zero\n",
    "    from deepspeed.runtime.zero.partition_parameters import ZeroParamStatus\n",
    "    print(f\"【COND】 hasattr_ds_id={hasattr(param, 'ds_id')}\") # 【COND】 hasattr_ds_id=False\n",
    "    if hasattr(param, \"ds_id\"): # TinyLLaVAではdeepspeedを使用しないのでSKIP\n",
    "        print(\"【ENTER】if hasattr(param, 'ds_id'):\") \n",
    "        print(f\"【COND】 ds_status={getattr(param, 'ds_status', None)}, ignore_status={ignore_status}\")\n",
    "        if param.ds_status == ZeroParamStatus.NOT_AVAILABLE:\n",
    "            print(\"【ENTER】if param.ds_status == ZeroParamStatus.NOT_AVAILABLE:\")\n",
    "            print(f\"【COND】 ignore_status={ignore_status}\")\n",
    "            if not ignore_status:\n",
    "                print(\"【ENTER】if not ignore_status:\")\n",
    "                print(name, 'no ignore status')\n",
    "                print(\"【EXIT】if not ignore_status:\")\n",
    "            print(\"【EXIT】if param.ds_status == ZeroParamStatus.NOT_AVAILABLE:\")\n",
    "        with zero.GatheredParameters([param]):\n",
    "            param = param.data.detach().cpu().clone()\n",
    "            print(\"param (after GatheredParameters)\\n\", param)\n",
    "        print(\"【EXIT】if hasattr(param, 'ds_id'):\")\n",
    "    else:\n",
    "        print(\"【ENTER】else (not hasattr(param, 'ds_id')):\") # ENTER\n",
    "        param = param.detach().cpu().clone()\n",
    "        print(\"param (after else)\\n\", param)\n",
    "        print(\"【EXIT】else (not hasattr(param, 'ds_id')):\")\n",
    "    print(\"param (def maybe_zero_3 at llava_trainer.py return)\\n\", param)\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mm_adapter_state_maybe_zero_3(named_params, keys_to_match):\n",
    "\n",
    "    print(\"current file path\", \"llava/train/llava_trainer.py\")\n",
    "    print(\"def get_mm_adapter_state_maybe_zero_3(named_params, keys_to_match)\")\n",
    "    print(\"named_params get_mm_adapter_state_maybe_zero_3\\n\", named_params)\n",
    "    print(\"keys_to_match get_mm_adapter_state_maybe_zero_3\\n\", keys_to_match)\n",
    "    to_return = {k: t for k, t in named_params if any(key_match in k for key_match in keys_to_match)}\n",
    "    print(\"to_return = {k: t for k, t in named_params if any(key_match in k for key_match in keys_to_match)}\\n\", to_return)\n",
    "    to_return = {k: maybe_zero_3(v, ignore_status=True, name=k).cpu() for k, v in to_return.items()}\n",
    "    print(\"to_return def get_mm_adapter_state_maybe_zero_3 \\n\", to_return)\n",
    "\n",
    "    for k, v in to_return.items():\n",
    "        if hasattr(v, 'shape'):\n",
    "            print(f\"to_return['{k}'].shape\\n\", v.shape)\n",
    "            \n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_match = ['mm_projector']\n",
    "weight_to_save = get_mm_adapter_state_maybe_zero_3(model.named_parameters(), keys_to_match)\n",
    "print(\"weight_to_save\\n\", weight_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = training_args.output_dir\n",
    "print(\"output_dir\", output_dir)\n",
    "model.config.save_pretrained(output_dir)\n",
    "torch.save(weight_to_save, os.path.join(output_dir, f'mm_projector.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaVATrainer(Trainer):\n",
    "    # Trainer > _inner_training_loop > _maybe_log_save_evaluate > self._save_checkpoint(model, trial)\n",
    "    def _save_checkpoint(self, model, trial, metrics=None):\n",
    "\n",
    "        print(\"current file path\", \"llava/train/llava_trainer.py\")\n",
    "        print(\"def _save_checkpoint(self, model, trial, metrics=None)\")\n",
    "        print(\"self _save_checkpoint\\n\", self) # <llava.train.llava_trainer.LLaVATrainer object at 0x7ed6341f4490>\n",
    "        print(\"model _save_checkpoint\\n\", model)\n",
    "        print(\"trial _save_checkpoint\\n\", trial) # None\n",
    "        print(\"metrics _save_checkpoint\\n\", metrics) # None\n",
    "        print(f\"【COND】tune_mm_mlp_adapter={getattr(self.args, 'tune_mm_mlp_adapter', False)}\") # True\n",
    "        if getattr(self.args, 'tune_mm_mlp_adapter', False):\n",
    "            # 【ENTER】\n",
    "            print(\"【ENTER】if getattr(self.args, 'tune_mm_mlp_adapter', False):\")\n",
    "            from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "            checkpoint_folder = f\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\"\n",
    "            print(\"checkpoint_folder = f\\\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\\\"\\n\", checkpoint_folder)\n",
    "\n",
    "            run_dir = self._get_output_dir(trial=trial)\n",
    "            print(\"run_dir = self._get_output_dir(trial=trial)\", run_dir)\n",
    "            output_dir = os.path.join(run_dir, checkpoint_folder)\n",
    "            print(\"output_dir = os.path.join(run_dir, checkpoint_folder)\", output_dir)\n",
    "\n",
    "            # Only save Adapter\n",
    "            keys_to_match = ['mm_projector'] # 'vision_resampler'\n",
    "            print(f\"【COND】use_im_start_end={getattr(self.args, 'use_im_start_end', False)}\") # False\n",
    "            if getattr(self.args, \"use_im_start_end\", False):\n",
    "                pass\n",
    "\n",
    "            weight_to_save = get_mm_adapter_state_maybe_zero_3(self.model.named_parameters(), keys_to_match)\n",
    "\n",
    "            print(f\"【COND】local_rank={self.args.local_rank}\") # 0\n",
    "            if self.args.local_rank == 0 or self.args.local_rank == -1:\n",
    "                # 【ENTER】\n",
    "                print(\"【ENTER】if self.args.local_rank == 0 or self.args.local_rank == -1:\")\n",
    "                self.model.config.save_pretrained(output_dir)\n",
    "                torch.save(weight_to_save, os.path.join(output_dir, f'mm_projector.bin'))\n",
    "                print(\"【EXIT】if self.args.local_rank == 0 or self.args.local_rank == -1:\")\n",
    "            print(\"【EXIT】if getattr(self.args, 'tune_mm_mlp_adapter', False):\")\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer,\n",
    "                                   output_dir: str):\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str)\")\n",
    "    print(\"trainer safe_save_model_for_hf_trainer\\n\", type(trainer)) # <class 'llava.train.llava_trainer.LLaVATrainer'>\n",
    "    print(\"output_dir safe_save_model_for_hf_trainer\\n\", output_dir) # ./checkpoints/llava-v1.5-7b-pretrain\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "\n",
    "    print(\"trainer.args safe_save_model_for_hf_trainer\\n\", trainer.args) # TrainingArguments(...\n",
    "    print(\"【COND】tune_mm_mlp_adapter=\", getattr(trainer.args, \"tune_mm_mlp_adapter\", False)) # True\n",
    "    if getattr(trainer.args, \"tune_mm_mlp_adapter\", False):\n",
    "        # Only save Adapter\n",
    "        print(\"【ENTER】if getattr(trainer.args, 'tune_mm_mlp_adapter', False):\") # 【ENTER】\n",
    "        keys_to_match = ['mm_projector']\n",
    "        if getattr(trainer.args, \"use_im_start_end\", False):\n",
    "            print(\"【ENTER】if getattr(trainer.args, 'use_im_start_end', False):\")\n",
    "            keys_to_match.extend(['embed_tokens', 'embed_in'])\n",
    "            print(\"【EXIT】if getattr(trainer.args, 'use_im_start_end', False):\")\n",
    "\n",
    "        weight_to_save = get_mm_adapter_state_maybe_zero_3(trainer.model.named_parameters(), keys_to_match)\n",
    "        if hasattr(weight_to_save, 'shape'):\n",
    "            print(\"【ENTER】if hasattr(weight_to_save, 'shape'):\")\n",
    "            print(\"weight_to_save.shape\\n\", weight_to_save.shape)\n",
    "            print(\"【EXIT】if hasattr(weight_to_save, 'shape'):\")\n",
    "        trainer.model.config.save_pretrained(output_dir)\n",
    "\n",
    "        current_folder = output_dir.split('/')[-1]\n",
    "        parent_folder = os.path.dirname(output_dir)\n",
    "        print(\"current_folder = output_dir.split('/')[-1]\", current_folder) # checkpoint-xxx or llava-v1.5-7b-pretrain\n",
    "        print(\"parent_folder = os.path.dirname(output_dir)\\n\", parent_folder) # ./checkpoints\n",
    "        print(\"【COND】trainer.args.local_rank=\", trainer.args.local_rank) # 0 or -1\n",
    "        if trainer.args.local_rank == 0 or trainer.args.local_rank == -1:\n",
    "            print(\"【ENTER】if trainer.args.local_rank == 0 or trainer.args.local_rank == -1:\")\n",
    "            if current_folder.startswith('checkpoint-'):\n",
    "                print(\"【ENTER】if current_folder.startswith('checkpoint-'):\")\n",
    "                mm_projector_folder = os.path.join(parent_folder, \"mm_projector\")\n",
    "                os.makedirs(mm_projector_folder, exist_ok=True)\n",
    "                torch.save(weight_to_save, os.path.join(mm_projector_folder, f'{current_folder}.bin'))\n",
    "                print(f\"Adapter weights saved to {os.path.join(mm_projector_folder, f'{current_folder}.bin')}\")\n",
    "                print(\"【EXIT】if current_folder.startswith('checkpoint-'):\")\n",
    "            else:\n",
    "                torch.save(weight_to_save, os.path.join(output_dir, f'mm_projector.bin'))\n",
    "                print(f\"Adapter weights saved to {os.path.join(output_dir, f'mm_projector.bin')}\")\n",
    "            print(\"【EXIT】if trainer.args.local_rank == 0 or trainer.args.local_rank == -1:\")\n",
    "        print(\"【EXIT】if getattr(trainer.args, 'tune_mm_mlp_adapter', False):\")\n",
    "        return\n",
    "\n",
    "    if trainer.deepspeed:\n",
    "        print(\"【ENTER】if trainer.deepspeed:\") # 【SKIP】\n",
    "        torch.cuda.synchronize()\n",
    "        trainer.save_model(output_dir)\n",
    "        print(\"【EXIT】if trainer.deepspeed:\")\n",
    "        return\n",
    "\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        print(\"【ENTER】if trainer.args.should_save:\") # 【SKIP】\n",
    "        cpu_state_dict = {\n",
    "            key: value.cpu() for key, value in state_dict.items()\n",
    "        }\n",
    "        for key, value in cpu_state_dict.items():\n",
    "            if hasattr(value, 'shape'):\n",
    "                print(f\"cpu_state_dict['{key}'].shape\\n\", value.shape)\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n",
    "        print(\"【EXIT】if trainer.args.should_save:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgAHEvYzuB0U"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "def train():\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def train()\")\n",
    "    global local_rank\n",
    "\n",
    "    parser = transformers.HfArgumentParser(\n",
    "        (ModelArguments, DataArguments, TrainingArguments))\n",
    "    print(\"original parser\\n\", parser)\n",
    "    model_args, data_args, training_args = parser.parse_dict(args_dict)\n",
    "    print(\"model_args\\n\", model_args)\n",
    "    print(\"data_args\\n\", data_args)\n",
    "    print(\"training_args\\n\", training_args)\n",
    "    local_rank = training_args.local_rank\n",
    "    print(\"local_rank\\n\", local_rank)\n",
    "    compute_dtype = (torch.float16 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n",
    "    print(\"compute_dtype\\n\", compute_dtype)\n",
    "    bnb_model_from_pretrained_args = {}\n",
    "    print(\"bnb_model_from_pretrained_args\\n\", bnb_model_from_pretrained_args)\n",
    "    # 【SKIP】bfloat16 なので 以下の if 文はスキップされる\n",
    "    print(f\"【COND】 bits={training_args.bits}\")\n",
    "    if training_args.bits in [4, 8]:\n",
    "      pass\n",
    "\n",
    "    print(f\"【COND】 vision_tower={model_args.vision_tower}\")\n",
    "    # 【ENTER】 vision_tower=openai/clip-vit-large-patch14-336 なので、この分岐に入る\n",
    "    if model_args.vision_tower is not None:\n",
    "        print(\"【ENTER】if model_args.vision_tower is not None:\")\n",
    "        print(f\"【COND】 mpt_in_model_name_or_path={'mpt' in model_args.model_name_or_path}\")\n",
    "        #【SKIP】model_args.model_name_or_path に mptは含まれていないので、この分岐はskipされる\n",
    "        if 'mpt' in model_args.model_name_or_path:\n",
    "          pass\n",
    "\n",
    "        #【ENTER】 model_args.model_name_or_path に mptは含まれていないので、この分岐に入る\n",
    "        else:\n",
    "            print(\"【COND】 not_mpt_in_model_name_or_path={'mpt' not in model_args.model_name_or_path}\")\n",
    "            print(\"【ENTER】else of if 'mpt' in model_args.model_name_or_path:\")\n",
    "            # PreTrainedModel.from_pretrained\n",
    "            model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "                model_args.model_name_or_path,\n",
    "                cache_dir=training_args.cache_dir,\n",
    "                **bnb_model_from_pretrained_args\n",
    "            )\n",
    "            print(\"model defined as LlavaLlamaForCausalLM \\n\", model)\n",
    "            print(\"【EXIT】else of if 'mpt' in model_args.model_name_or_path:\")\n",
    "        print(\"【EXIT】if model_args.vision_tower is not None:\")\n",
    "    # 【SKIP】 vision_tower=clip-vit-large-patch14-336 なので、この分岐には入らない\n",
    "    else:\n",
    "      pass\n",
    "\n",
    "    print(f\"【COND】 freeze_backbone={model_args.freeze_backbone}\")\n",
    "    # 【SKIP】 freeze_backbone=False なので、この分岐はskipされる\n",
    "    if model_args.freeze_backbone:\n",
    "        pass\n",
    "\n",
    "    # 【SKIP】 bfloat16 なので 以下の if 文はスキップされる\n",
    "    print(f\"【COND】 bits={training_args.bits}\")\n",
    "    if training_args.bits in [4, 8]:\n",
    "      pass\n",
    "\n",
    "    print(f\"【COND】 gradient_checkpointing={training_args.gradient_checkpointing}\")\n",
    "    # 【ENTER】 gradient_checkpointing=True なので、この分岐に入る\n",
    "    if training_args.gradient_checkpointing:\n",
    "        print(\"【ENTER】if training_args.gradient_checkpointing:\")\n",
    "        print(f\"【COND】 has_enable_input_require_grads={hasattr(model, 'enable_input_require_grads')}\")\n",
    "        # 【ENTER】 model に enable_input_require_grads メソッドがあるので、この分岐に入る\n",
    "        if hasattr(model, \"enable_input_require_grads\"):\n",
    "            print(\"【ENTER】if hasattr(model, 'enable_input_require_grads'):\")\n",
    "            # PreTrainedModel.enable_input_require_grads\n",
    "            # 元々 全ての重みについて True\n",
    "            model.enable_input_require_grads()\n",
    "            print(\"【EXIT】if hasattr(model, 'enable_input_require_grads'):\")\n",
    "        # 【SKIP】 model に enable_input_require_grads メソッドがあるので、この分岐はskipされる\n",
    "        else:\n",
    "          pass\n",
    "\n",
    "        print(\"【EXIT】if training_args.gradient_checkpointing:\")\n",
    "\n",
    "    print(f\"【COND】 lora_enable={training_args.lora_enable}\")\n",
    "    # 【SKIP】 lora_enable=False なので、この分岐はskipされる\n",
    "    if training_args.lora_enable:\n",
    "      pass\n",
    "\n",
    "    print(f\"【COND】 mpt_in_model_name_or_path={'mpt' in model_args.model_name_or_path}\")\n",
    "    # 【SKIP】model_args.model_name_or_path に mptは含まれていないので、この分岐はskipされる\n",
    "    if 'mpt' in model_args.model_name_or_path:\n",
    "      pass\n",
    "\n",
    "    #【ENTER】 model_args.model_name_or_path に mptは含まれていないので、この分岐に入る\n",
    "    else:\n",
    "        print(\"【COND】 not_mpt_in_model_name_or_path={'mpt' not in model_args.model_name_or_path}\")\n",
    "        print(\"【ENTER】else of if 'mpt' in model_args.model_name_or_path:\")\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            cache_dir=training_args.cache_dir,\n",
    "            model_max_length=training_args.model_max_length,\n",
    "            padding_side=\"right\",\n",
    "            use_fast=False,\n",
    "        )\n",
    "        print(\"tokenizer defined by AutoTokenizer.from_pretrained \\n\", tokenizer)\n",
    "        print(\"【EXIT】else of if 'mpt' in model_args.model_name_or_path:\")\n",
    "\n",
    "    print(f\"【COND】 version={model_args.version}\")\n",
    "    # 【SKIP】 version=plain なので、この分岐はskipされる\n",
    "    if model_args.version == \"v0\":\n",
    "      pass\n",
    "\n",
    "    # 【SKIP】 version=plain なので、この分岐はskipされる\n",
    "    elif model_args.version == \"v0.5\":\n",
    "      pass\n",
    "    # 【ENTER】 version=plain なので、この分岐に入る\n",
    "    else:\n",
    "        print(\"【ENTER】else of if model_args.version == 'v0' and elif 'v0.5':\")\n",
    "        tokenizer.pad_token = tokenizer.unk_token\n",
    "        print(f\"【COND】 version_in_conv_templates={model_args.version in conv_templates}\")\n",
    "        # 【ENTER】 model_args.version=plain は conversation_lib.conv_templates に含まれている（\"plain\": conv_llava_plain）ので、この分岐に入る\n",
    "        if model_args.version in conv_templates:\n",
    "            print(\"【ENTER】if model_args.version in conversation_lib.conv_templates:\")\n",
    "            default_conversation = conv_templates[model_args.version]\n",
    "            print(f\"conversation_lib.default_conversation set to {model_args.version}\")\n",
    "            print(\"【EXIT】if model_args.version in conversation_lib.conv_templates:\")\n",
    "        # 【SKIP】 model_args.version=plain は conversation_lib.conv_templates に含まれているので、この分岐はskipされる\n",
    "        else:\n",
    "          pass\n",
    "        print(\"【EXIT】else of if model_args.version == 'v0' and elif 'v0.5':\")\n",
    "\n",
    "    print(f\"【COND】 vision_tower={model_args.vision_tower}\")\n",
    "    # 【ENTER】 vision_tower=openai/clip-vit-large-patch14-336 なので、この分岐に入る\n",
    "    if model_args.vision_tower is not None:\n",
    "        print(\"【ENTER】if model_args.vision_tower is not None:\")\n",
    "        model.get_model().initialize_vision_modules(\n",
    "            model_args=model_args,\n",
    "            fsdp=training_args.fsdp\n",
    "        )\n",
    "\n",
    "        vision_tower = model.get_vision_tower()\n",
    "        vision_tower.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)\n",
    "\n",
    "        data_args.image_processor = vision_tower.image_processor\n",
    "        data_args.is_multimodal = True\n",
    "\n",
    "        model.config.image_aspect_ratio = data_args.image_aspect_ratio\n",
    "        model.config.tokenizer_padding_side = tokenizer.padding_side\n",
    "        model.config.tokenizer_model_max_length = tokenizer.model_max_length\n",
    "\n",
    "        model.config.tune_mm_mlp_adapter = training_args.tune_mm_mlp_adapter = model_args.tune_mm_mlp_adapter\n",
    "        print(f\"【COND】 tune_mm_mlp_adapter={model_args.tune_mm_mlp_adapter}\") # True\n",
    "        if model_args.tune_mm_mlp_adapter:\n",
    "            # 【ENTER】 tune_mm_mlp_adapter=True なので、この分岐に入る\n",
    "            print(\"【ENTER】if model_args.tune_mm_mlp_adapter:\")\n",
    "            # モデル全体の全パラメータを「学習不可（requires_grad=False）」にする\n",
    "            # これで通常の重みは全て凍結される\n",
    "            model.requires_grad_(False)\n",
    "            for p in model.get_model().mm_projector.parameters():\n",
    "                # mm_projector（画像特徴量→テキスト特徴量への変換層）の全パラメータだけを「学習可能（requires_grad=True）」に戻す\n",
    "                # これで mm_projector のみ学習されることになる\n",
    "                print(\"model.get_model().mm_projector.parameters()\", model.get_model().mm_projector.parameters())\n",
    "                p.requires_grad = True\n",
    "            print(\"【EXIT】if model_args.tune_mm_mlp_adapter:\")\n",
    "\n",
    "        model.config.freeze_mm_mlp_adapter = training_args.freeze_mm_mlp_adapter\n",
    "        print(f\"【COND】 freeze_mm_mlp_adapter={training_args.freeze_mm_mlp_adapter}\") # False\n",
    "        if training_args.freeze_mm_mlp_adapter:\n",
    "          pass\n",
    "\n",
    "        print(f\"【COND】 bits={training_args.bits}\") # 16\n",
    "        if training_args.bits in [4, 8]:\n",
    "          pass\n",
    "\n",
    "        model.config.mm_use_im_start_end = data_args.mm_use_im_start_end = model_args.mm_use_im_start_end\n",
    "        print(\"model_args.mm_use_im_start_end\", model_args.mm_use_im_start_end)\n",
    "        model.config.mm_projector_lr = training_args.mm_projector_lr\n",
    "        print(\"training_args.mm_projector_lr\", training_args.mm_projector_lr)\n",
    "        training_args.use_im_start_end = model_args.mm_use_im_start_end\n",
    "        print(\"training_args.use_im_start_end\", training_args.use_im_start_end)\n",
    "        model.config.mm_use_im_patch_token = model_args.mm_use_im_patch_token\n",
    "        print(\"model_args.mm_use_im_patch_token\", model_args.mm_use_im_patch_token)\n",
    "        model.initialize_vision_tokenizer(model_args, tokenizer=tokenizer)\n",
    "        print(\"【EXIT】if model_args.vision_tower is not None:\")\n",
    "\n",
    "    print(f\"【COND】 bits={training_args.bits}\") # 16\n",
    "    if training_args.bits in [4, 8]:\n",
    "        pass\n",
    "\n",
    "    data_module = make_supervised_data_module(tokenizer=tokenizer,\n",
    "                                              data_args=data_args)\n",
    "    print(\"data_module\\n\", data_module) # {'train_dataset': <llava.train.train.LazySupervisedDataset object at 0x7ed6341f4880>, 'eval_dataset': None, 'data_collator': DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))}\n",
    "\n",
    "    trainer = LLaVATrainer(model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    args=training_args,\n",
    "                    **data_module)\n",
    "    print(\"trainer\\n\", trainer) # <llava.train.llava_trainer.LLaVATrainer object at 0x7ed6341f4490>\n",
    "\n",
    "    print(\"【COND】list(pathlib.Path(training_args.output_dir).glob('checkpoint-*'))\\n\", list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\"))) # [PosixPath('checkpoints/llava-v1.5-7b-pretrain/checkpoint-250'), PosixPath('checkpoints/llava-v1.5-7b-pretrain/checkpoint-1')]\n",
    "    if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】if list(pathlib.Path(training_args.output_dir).glob(checkpoint-*)):\")\n",
    "        trainer.train(resume_from_checkpoint=False)\n",
    "        print(\"【EXIT】if list(pathlib.Path(training_args.output_dir).glob(checkpoint-*)):\")\n",
    "    else:\n",
    "        print(\"【ENTER】else of if list(pathlib.Path(training_args.output_dir).glob(checkpoint-*)):\")\n",
    "        trainer.train()\n",
    "        print(\"【EXIT】else of if list(pathlib.Path(training_args.output_dir).glob(checkpoint-*)):\")\n",
    "    trainer.save_state()\n",
    "\n",
    "    model.config.use_cache = True\n",
    "    print(\"model.config.use_cache = True\", model.config.use_cache) # True\n",
    "\n",
    "    print(f\"【COND】lora_enable={training_args.lora_enable}\") # False\n",
    "    if training_args.lora_enable:\n",
    "      pass\n",
    "    else:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】else of if training_args.lora_enable:\")\n",
    "        print(\"trainer\", trainer) # <class 'llava.train.llava_trainer.LLaVATrainer'>\n",
    "        safe_save_model_for_hf_trainer(trainer=trainer,\n",
    "                                       output_dir=training_args.output_dir)\n",
    "        print(\"【EXIT】else of if training_args.lora_enable:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
