{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MgqvUKsdFkEz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117 4.31.0\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import torch, transformers; print(torch.__version__, transformers.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C8j7yWUt4N-C"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RmGhXE625w7R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "[2025-10-06 00:51:44,882] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/wandb/sdk/launch/builder/build.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n",
    "    version: Optional[str] = field(default=\"v0\")\n",
    "    freeze_backbone: bool = field(default=False)\n",
    "    tune_mm_mlp_adapter: bool = field(default=False)\n",
    "    vision_tower: Optional[str] = field(default=None) # default to None\n",
    "    mm_vision_select_layer: Optional[int] = field(default=-1)   # default to the last layer\n",
    "    pretrain_mm_mlp_adapter: Optional[str] = field(default=None)\n",
    "    mm_projector_type: Optional[str] = field(default='linear')\n",
    "    mm_use_im_start_end: bool = field(default=False)\n",
    "    mm_use_im_patch_token: bool = field(default=True)\n",
    "    mm_patch_merge_type: Optional[str] = field(default='flat')\n",
    "    mm_vision_select_feature: Optional[str] = field(default=\"patch\")\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(default=None,\n",
    "                           metadata={\"help\": \"Path to the training data.\"})\n",
    "    lazy_preprocess: bool = False\n",
    "    is_multimodal: bool = False\n",
    "    image_folder: Optional[str] = field(default=None)\n",
    "    image_aspect_ratio: str = 'square'\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    remove_unused_columns: bool = field(default=False)\n",
    "    freeze_mm_mlp_adapter: bool = field(default=False)\n",
    "    mpt_attn_impl: Optional[str] = field(default=\"triton\")\n",
    "    model_max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\n",
    "            \"help\":\n",
    "            \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
    "        },\n",
    "    )\n",
    "    double_quant: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Compress the quantization statistics through double quantization.\"}\n",
    "    )\n",
    "    quant_type: str = field(\n",
    "        default=\"nf4\",\n",
    "        metadata={\"help\": \"Quantization data type to use. Should be one of `fp4` or `nf4`.\"}\n",
    "    )\n",
    "    bits: int = field(\n",
    "        default=16,\n",
    "        metadata={\"help\": \"How many bits to use.\"}\n",
    "    )\n",
    "    lora_enable: bool = False\n",
    "    lora_r: int = 64\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_weight_path: str = \"\"\n",
    "    lora_bias: str = \"none\"\n",
    "    mm_projector_lr: Optional[float] = None\n",
    "    group_by_modality_length: bool = field(default=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DTvjg0PP7za1"
   },
   "outputs": [],
   "source": [
    "from transformers import HfArgumentParser\n",
    "\n",
    "args_dict = {\n",
    "    #\"deepspeed\": \"./scripts/zero2.json\",\n",
    "    \"model_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    \"version\": \"plain\",\n",
    "    \"data_path\": \"/workspaces/LLaVA/CC3M_2.json\",\n",
    "    \"image_folder\": \"/workspaces/LLaVA/images/\",\n",
    "    \"vision_tower\": \"openai/clip-vit-large-patch14-336\",\n",
    "    \"mm_projector_type\": \"mlp2x_gelu\",\n",
    "    \"tune_mm_mlp_adapter\": True,\n",
    "    \"mm_vision_select_layer\": -2,\n",
    "    \"mm_use_im_start_end\": False,\n",
    "    \"mm_use_im_patch_token\": False,\n",
    "    \"bf16\": True,\n",
    "    \"output_dir\": \"./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0\",\n",
    "\n",
    "    # TrainingArguments 相当\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 1,\n",
    "    \"save_total_limit\": 1,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"weight_decay\": 0.0, # I don't know why 0.0\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"logging_steps\": 1,\n",
    "    \"tf32\": False, # switched from True for TinyLlama\n",
    "    \"model_max_length\": 2048,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"dataloader_num_workers\": 2,\n",
    "    \"lazy_preprocess\": True,\n",
    "    \"report_to\": \"none\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dBX7g2DD-xpR",
    "outputId": "627a283d-b903-45db-a0c0-eec1b51ca0eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_args\n",
      " ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
      "data_args\n",
      " DataArguments(data_path='/workspaces/LLaVA/CC3M_2.json', lazy_preprocess=True, is_multimodal=False, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')\n",
      "training_args\n",
      " TrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "bits=16,\n",
      "cache_dir=None,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=2,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "double_quant=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "freeze_mm_mlp_adapter=False,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "group_by_modality_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0/runs/Oct06_00-51-46_3d7e264c2fcc,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1,\n",
      "logging_strategy=steps,\n",
      "lora_alpha=16,\n",
      "lora_bias=none,\n",
      "lora_dropout=0.05,\n",
      "lora_enable=False,\n",
      "lora_r=64,\n",
      "lora_weight_path=,\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mm_projector_lr=None,\n",
      "model_max_length=2048,\n",
      "mp_parameters=,\n",
      "mpt_attn_impl=triton,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "quant_type=nf4,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./checkpoints/llava-TinyLlama-1.1B-Chat-v1.0,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=1,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=False,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_dict(args_dict)\n",
    "print(\"model_args\\n\", model_args)\n",
    "print(\"data_args\\n\", data_args)\n",
    "print(\"training_args\\n\", training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oC2rbSk5QtcX"
   },
   "outputs": [],
   "source": [
    "# Model Constants\n",
    "IGNORE_INDEX = -100\n",
    "IMAGE_TOKEN_INDEX = -200\n",
    "DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "DEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\n",
    "DEFAULT_IM_START_TOKEN = \"<im_start>\"\n",
    "DEFAULT_IM_END_TOKEN = \"<im_end>\"\n",
    "IMAGE_PLACEHOLDER = \"<image-placeholder>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "J_5thB_J_WEi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_rank\n",
      " 0\n",
      "compute_dtype\n",
      " torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "local_rank = training_args.local_rank\n",
    "print(\"local_rank\\n\", local_rank)\n",
    "compute_dtype = (torch.float16 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n",
    "print(\"compute_dtype\\n\", compute_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jFp9VBciHEhH"
   },
   "outputs": [],
   "source": [
    "from transformers import CLIPVisionModel, CLIPImageProcessor, CLIPVisionConfig\n",
    "import torch.nn as nn\n",
    "# __init__\n",
    "# load_model\n",
    "\n",
    "# result = CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n",
    "class CLIPVisionTower(nn.Module):\n",
    "    def __init__(self, vision_tower, args):\n",
    "        # result = CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n",
    "        print(\"current file path\", \"llava/llava/model/multimodal_encoder/clip_encoder.py\")\n",
    "        print(\"def CLIPVisionTower.__init__(self, vision_tower, args\")\n",
    "        print(\"self\\n\", type(self))\n",
    "        print(\"vision_tower\\n\", vision_tower) # openai/clip-vit-large-patch14-336\n",
    "        print(\"args\\n\", args) # ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_loaded = False\n",
    "\n",
    "        print(\"self.is_loaded\\n\", self.is_loaded) # False\n",
    "\n",
    "        self.vision_tower_name = vision_tower\n",
    "        print(\"self.vision_tower_name\\n\", self.vision_tower_name) # openai/clip-vit-large-patch14-336\n",
    "        self.select_layer = args.mm_vision_select_layer\n",
    "        print(\"self.select_layer\\n\", self.select_layer) # -2\n",
    "        self.select_feature = getattr(args, 'mm_vision_select_feature', 'patch')\n",
    "        print(\"self.select_feature\\n\", self.select_feature) # patch\n",
    "\n",
    "    def load_model(self):\n",
    "\n",
    "        print(\"current file path\", \"llava/llava/model/multimodal_encoder/clip_encoder.py\")\n",
    "        print(\"def CLIPVisionTower.load_model(self)\")\n",
    "        print(\"self\\n\", type(self))\n",
    "        print(\"self.vision_tower_name\\n\", self.vision_tower_name) # openai/clip-vit-large-patch14-336\n",
    "        self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name)\n",
    "        print(\"self.image_processor\\n\", self.image_processor)\n",
    "        \"\"\"\n",
    "        CLIPImageProcessor {\n",
    "        \"crop_size\": {\n",
    "            \"height\": 336,\n",
    "            \"width\": 336\n",
    "        },\n",
    "        \"do_center_crop\": true,\n",
    "        \"do_convert_rgb\": true,\n",
    "        \"do_normalize\": true,\n",
    "        \"do_rescale\": true,\n",
    "        \"do_resize\": true,\n",
    "        \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n",
    "        \"image_mean\": [\n",
    "            0.48145466,\n",
    "            0.4578275,\n",
    "            0.40821073\n",
    "        ],\n",
    "        \"image_processor_type\": \"CLIPImageProcessor\",\n",
    "        \"image_std\": [\n",
    "            0.26862954,\n",
    "            0.26130258,\n",
    "            0.27577711\n",
    "        ],\n",
    "        \"resample\": 3,\n",
    "        \"rescale_factor\": 0.00392156862745098,\n",
    "        \"size\": {\n",
    "            \"shortest_edge\": 336\n",
    "        }\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name)\n",
    "        print(\"self.vision_tower\\n\", self.vision_tower)\n",
    "        \"\"\"\n",
    "        CLIPVisionModel(\n",
    "        (vision_model): CLIPVisionTransformer(\n",
    "            (embeddings): CLIPVisionEmbeddings(\n",
    "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "            (position_embedding): Embedding(577, 1024)\n",
    "            )\n",
    "            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "            (encoder): CLIPEncoder(\n",
    "            (layers): ModuleList(\n",
    "                (0-23): 24 x CLIPEncoderLayer(\n",
    "                (self_attn): CLIPAttention(\n",
    "                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                (mlp): CLIPMLP(\n",
    "                    (activation_fn): QuickGELUActivation()\n",
    "                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                )\n",
    "            )\n",
    "            )\n",
    "            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        )\n",
    "        )\n",
    "        \"\"\"\n",
    "        self.vision_tower.requires_grad_(False)\n",
    "\n",
    "        self.is_loaded = True\n",
    "        print(\"self.is_loaded\\n\", self.is_loaded) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n"
     ]
    }
   ],
   "source": [
    "print(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/llava/model/multimodal_encoder/clip_encoder.py\n",
      "def CLIPVisionTower.__init__(self, vision_tower, args\n",
      "self\n",
      " <class '__main__.CLIPVisionTower'>\n",
      "vision_tower\n",
      " openai/clip-vit-large-patch14-336\n",
      "args\n",
      " ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
      "self.is_loaded\n",
      " False\n",
      "self.vision_tower_name\n",
      " openai/clip-vit-large-patch14-336\n",
      "self.select_layer\n",
      " -2\n",
      "self.select_feature\n",
      " patch\n"
     ]
    }
   ],
   "source": [
    "vision_tower_name = \"openai/clip-vit-large-patch14-336\"\n",
    "CLIPVisionTower_model = CLIPVisionTower(vision_tower_name, args=model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/llava/model/multimodal_encoder/clip_encoder.py\n",
      "def CLIPVisionTower.load_model(self)\n",
      "self\n",
      " <class '__main__.CLIPVisionTower'>\n",
      "self.vision_tower_name\n",
      " openai/clip-vit-large-patch14-336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.image_processor\n",
      " CLIPImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 336,\n",
      "    \"width\": 336\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 336\n",
      "  }\n",
      "}\n",
      "\n",
      "self.vision_tower\n",
      " CLIPVisionModel(\n",
      "  (vision_model): CLIPVisionTransformer(\n",
      "    (embeddings): CLIPVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "      (position_embedding): Embedding(577, 1024)\n",
      "    )\n",
      "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "self.is_loaded\n",
      " True\n"
     ]
    }
   ],
   "source": [
    "CLIPVisionTower_model.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(CLIPVisionTower_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPVisionModel(\n",
      "  (vision_model): CLIPVisionTransformer(\n",
      "    (embeddings): CLIPVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "      (position_embedding): Embedding(577, 1024)\n",
      "    )\n",
      "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(CLIPVisionTower_model.vision_tower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPVisionConfig {\n",
      "  \"_name_or_path\": \"openai/clip-vit-large-patch14-336\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"dropout\": 0.0,\n",
      "  \"hidden_act\": \"quick_gelu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": 336,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"model_type\": \"clip_vision_model\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"patch_size\": 14,\n",
      "  \"projection_dim\": 768,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(CLIPVisionTower_model.vision_tower.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5mz6UWifHfTq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def build_vision_tower(model_args, **kwargs):\n",
    "    # vision_tower = build_vision_tower(model_args)\n",
    "    print(\"current file path\", \"llava/llava/model/multimodal_encoder/builder.py\")\n",
    "    print(\"def build_vision_tower(model_args, **kwargs)\")\n",
    "    print(\"model_args\\n\", model_args) # ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
    "    print(\"kwargs\\n\", kwargs) # {}\n",
    "    vision_tower = model_args.vision_tower\n",
    "    print(\"vision_tower from model_args\\n\", vision_tower) # openai/clip-vit-large-patch14-336\n",
    "    CLIPVisionTower_model = CLIPVisionTower(vision_tower, args=model_args, **kwargs)\n",
    "    CLIPVisionTower_model.load_model()\n",
    "    print(\"CLIPVisionTower_model\\n\", CLIPVisionTower_model)\n",
    "    print(\"【EXIT】if is_absolute_path_exists or vision_tower.startswith('openai') or vision_tower.startswith('laion') or 'ShareGPT4V' in vision_tower:\")\n",
    "    return CLIPVisionTower_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/llava/model/multimodal_encoder/builder.py\n",
      "def build_vision_tower(model_args, **kwargs)\n",
      "model_args\n",
      " ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
      "kwargs\n",
      " {}\n",
      "vision_tower from model_args\n",
      " openai/clip-vit-large-patch14-336\n",
      "current file path llava/llava/model/multimodal_encoder/clip_encoder.py\n",
      "def CLIPVisionTower.__init__(self, vision_tower, args\n",
      "self\n",
      " <class '__main__.CLIPVisionTower'>\n",
      "vision_tower\n",
      " openai/clip-vit-large-patch14-336\n",
      "args\n",
      " ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
      "self.is_loaded\n",
      " False\n",
      "self.vision_tower_name\n",
      " openai/clip-vit-large-patch14-336\n",
      "self.select_layer\n",
      " -2\n",
      "self.select_feature\n",
      " patch\n",
      "current file path llava/llava/model/multimodal_encoder/clip_encoder.py\n",
      "def CLIPVisionTower.load_model(self)\n",
      "self\n",
      " <class '__main__.CLIPVisionTower'>\n",
      "self.vision_tower_name\n",
      " openai/clip-vit-large-patch14-336\n",
      "self.image_processor\n",
      " CLIPImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 336,\n",
      "    \"width\": 336\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 336\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.vision_tower\n",
      " CLIPVisionModel(\n",
      "  (vision_model): CLIPVisionTransformer(\n",
      "    (embeddings): CLIPVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "      (position_embedding): Embedding(577, 1024)\n",
      "    )\n",
      "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "self.is_loaded\n",
      " True\n",
      "CLIPVisionTower_model\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "【EXIT】if is_absolute_path_exists or vision_tower.startswith('openai') or vision_tower.startswith('laion') or 'ShareGPT4V' in vision_tower:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPVisionTower(\n",
       "  (vision_tower): CLIPVisionModel(\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "        (position_embedding): Embedding(577, 1024)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_vision_tower(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def build_vision_projector(config, delay_load=False, **kwargs):\n",
    "    # 2層MLPをハードコーディング（configからサイズのみ取得）\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(config.mm_hidden_size, config.hidden_size),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(config.hidden_size, config.hidden_size),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mm_hidden_size\": 1024,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "# 公式 LLaMA-2-7B の config をロード\n",
    "TinyLlama_config = AutoConfig.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "TinyLlama_config.mm_hidden_size = CLIPVisionTower_model.vision_tower.config.hidden_size\n",
    "print(TinyLlama_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "projector = build_vision_projector(TinyLlama_config)\n",
    "print(projector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(CLIPVisionTower_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mro(cls):\n",
    "    print(f\"MRO for {cls.__name__}:\\n\")\n",
    "    for i, c in enumerate(cls.mro()):\n",
    "        print(f\"{i:2d}: {c.__module__}.{c.__name__}\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "VjkgxnbKFGan"
   },
   "outputs": [],
   "source": [
    "# LlavaBaseModel\n",
    "# __init__\n",
    "# get_vision_tower\n",
    "# initialize_vision_modules\n",
    "# unpad_image\n",
    "\n",
    "class LlavaBaseModel:  # LLaMA, MPT などの全てのLLM用の共通機能をまとめたクラス\n",
    "\n",
    "    # LlavaLlamaModelの__init__によって呼び出される\n",
    "    def __init__(self):\n",
    "\n",
    "        print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "        print(\"LlavaBaseModel.__init__(self, config)\")\n",
    "        # 「LlavaLlamaModelの__init__によって呼び出された時は」LlamaModelの__init_を呼び出す\n",
    "        super(LlavaBaseModel, self).__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "T_YguuBGN-3R"
   },
   "outputs": [],
   "source": [
    "# transformers.models.llama.modeling_llama.LlamaModel\n",
    "# URL: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py\n",
    "from transformers import LlamaModel\n",
    "\n",
    "# LlavaBaseModelにLlamaの初期化クラスをくっつけたいので、LLamaModelをMROに追加。\n",
    "# MROだと、LlavaLlamaModel > LlavaBaseModel > LlamaModel の順に解決される。\n",
    "class LlavaLlamaModel(LlavaBaseModel, LlamaModel): \n",
    "    def __init__(self):\n",
    "\n",
    "        print(\"current file path\", \"llava/llava/model/language_model/llava_llama.py\")\n",
    "        print(\"def LlavaLlamaModel.__init__(self, config: LlamaConfig)\")\n",
    "        print(\"self\\n\", type(self))\n",
    "        # LlavaBaseModelの__init__を呼ぶ\n",
    "        super(LlavaLlamaModel, self).__init__() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRO for LlavaBaseModel:\n",
      "\n",
      " 0: __main__.LlavaBaseModel\n",
      " 1: builtins.object\n",
      "\n",
      "\n",
      "\n",
      "MRO for LlavaLlamaModel:\n",
      "\n",
      " 0: __main__.LlavaLlamaModel\n",
      " 1: __main__.LlavaBaseModel\n",
      " 2: transformers.models.llama.modeling_llama.LlamaModel\n",
      " 3: transformers.models.llama.modeling_llama.LlamaPreTrainedModel\n",
      " 4: transformers.modeling_utils.PreTrainedModel\n",
      " 5: torch.nn.modules.module.Module\n",
      " 6: transformers.modeling_utils.ModuleUtilsMixin\n",
      " 7: transformers.generation.utils.GenerationMixin\n",
      " 8: transformers.utils.hub.PushToHubMixin\n",
      " 9: builtins.object\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_mro(LlavaBaseModel)\n",
    "print_mro(LlavaLlamaModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlavaBaseModel\n",
    "# __init__\n",
    "# get_vision_tower\n",
    "# initialize_vision_modules\n",
    "# unpad_image\n",
    "\n",
    "class LlavaBaseModel:  # LLaMA, MPT などの全てのLLM用の共通機能をまとめたクラス\n",
    "\n",
    "    # LlavaLlamaModelの__init__によって呼び出される\n",
    "    def __init__(self, config):\n",
    "\n",
    "        print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "        print(\"LlavaBaseModel.__init__(self, config)\")\n",
    "        print(\"config\\n\", config)\n",
    "        # 「LlavaLlamaModelの__init__によって呼び出された時は」LlamaModelの__init_を呼び出す\n",
    "        super(LlavaBaseModel, self).__init__(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers.models.llama.modeling_llama.LlamaModel\n",
    "# URL: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py\n",
    "from transformers import LlamaConfig, LlamaModel\n",
    "\n",
    "class LlavaLlamaConfig(LlamaConfig):\n",
    "    model_type = \"llava_llama\"\n",
    "\n",
    "# LlavaBaseModelにLlamaの初期化クラスをくっつけたいので、LLamaModelをMROに追加。\n",
    "# MROだと、LlavaLlamaModel > LlavaBaseModel > LlamaModel の順に解決される。\n",
    "class LlavaLlamaModel(LlavaBaseModel, LlamaModel): \n",
    "    config_class = LlavaLlamaConfig # from_pretrained の時にここが読みこまれる\n",
    "\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "\n",
    "        print(\"current file path\", \"llava/llava/model/language_model/llava_llama.py\")\n",
    "        print(\"def LlavaLlamaModel.__init__(self, config: LlamaConfig)\")\n",
    "        print(\"self\\n\", type(self))\n",
    "        print(\"config\\n\", config)\n",
    "        # LlavaBaseModelの__init__を呼ぶ\n",
    "        super(LlavaLlamaModel, self).__init__(config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "V9uRX7reN0gI"
   },
   "outputs": [],
   "source": [
    "# LlavaBaseForCausalLM\n",
    "# get_vision_tower\n",
    "# encode_images\n",
    "# prepare_inputs_labels_for_multimodal\n",
    "# initialize_vision_tokenizer\n",
    "\n",
    "# 以下のLlavaBaseForCausalLMのselfは、継承先のLlavaLlamaForCausalLMのselfが使用される\n",
    "\n",
    "class LlavaBaseForCausalLM:\n",
    "\n",
    "    def get_vision_tower(self):\n",
    "        print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "        print(\"class LlavaBaseForCausalLM(ABC).get_vision_tower(self)\")\n",
    "        result = self.get_model().get_vision_tower()\n",
    "        print(\"LlavaBaseForCausalLM(ABC).get_vision_tower(self) result (return)\\n\", result)\n",
    "        \"\"\"\n",
    "        CLIPVisionTower(\n",
    "        (vision_tower): CLIPVisionModel(\n",
    "            (vision_model): CLIPVisionTransformer(\n",
    "            (embeddings): CLIPVisionEmbeddings(\n",
    "                (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "                (position_embedding): Embedding(577, 1024)\n",
    "            )\n",
    "            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "            (encoder): CLIPEncoder(\n",
    "                (layers): ModuleList(\n",
    "                (0-23): 24 x CLIPEncoderLayer(\n",
    "                    (self_attn): CLIPAttention(\n",
    "                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    )\n",
    "                    (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                    (mlp): CLIPMLP(\n",
    "                    (activation_fn): QuickGELUActivation()\n",
    "                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "                    )\n",
    "                    (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                )\n",
    "                )\n",
    "            )\n",
    "            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "            )\n",
    "        )\n",
    "        )\n",
    "        \"\"\"\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "t4yAIwyeOEWE"
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "class LlavaLlamaForCausalLM(LlamaForCausalLM, LlavaBaseForCausalLM):\n",
    "    config_class = LlavaLlamaConfig # from_pretrainedの際にここが読み込まれる\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        print(\"current file path\", \"llava/llava/model/language_model/llava_llama.py\")\n",
    "        print(\"def LlavaLlamaForCausalLM.__init__(self, config)\")\n",
    "        print(\"self\\n\", type(self))\n",
    "        print(\"config\\n\", config)\n",
    "        super(LlamaForCausalLM, self).__init__(config)\n",
    "        # LlamaForCausalLMのself.modelをLlavaLlamaModelに置き換える\n",
    "        self.model = LlavaLlamaModel(config)\n",
    "        print(\"self.model\\n\", self.model)\n",
    "        \"\"\"\n",
    "        self.model\n",
    "        LlavaLlamaModel(\n",
    "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
    "        (layers): ModuleList(\n",
    "            (0-31): 32 x LlamaDecoderLayer(\n",
    "            (self_attn): LlamaAttention(\n",
    "                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "                (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "                (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "                (rotary_emb): LlamaRotaryEmbedding()\n",
    "            )\n",
    "            (mlp): LlamaMLP(\n",
    "                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
    "                (act_fn): SiLUActivation()\n",
    "            )\n",
    "            (input_layernorm): LlamaRMSNorm()\n",
    "            (post_attention_layernorm): LlamaRMSNorm()\n",
    "            )\n",
    "        )\n",
    "        (norm): LlamaRMSNorm()\n",
    "        )\n",
    "        \"\"\"\n",
    "        #print(\"self.pretraining_tp\\n\", self.pretraining_tp) # 1\n",
    "        #print(\"self.vocab_size\\n\", self.vocab_size) # 32_000\n",
    "        #print(\"self.lm_head\\n\", self.lm_head) # Linear(in_features=4096, out_features=32000, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        #self.post_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "RlMP4qUg0QtV",
    "outputId": "12f73ecd-9922-4d31-a0cf-a0e8bc5d88fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import AutoConfig\\n\\n# まず config.json をロードして Config クラスを自動判別\\nconfig = AutoConfig.from_pretrained(\\n    model_args.model_name_or_path,\\n    cache_dir=training_args.cache_dir\\n)\\n\\nprint(\"model_args.model_name_or_path\\n\", model_args.model_name_or_path)\\nprint(\"training_args.cache_dir\\n\", training_args.cache_dir)\\nprint(\"\")\\nprint(\"Loaded config:\\n\", config)\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from transformers import AutoConfig\n",
    "\n",
    "# まず config.json をロードして Config クラスを自動判別\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir\n",
    ")\n",
    "\n",
    "print(\"model_args.model_name_or_path\\n\", model_args.model_name_or_path)\n",
    "print(\"training_args.cache_dir\\n\", training_args.cache_dir)\n",
    "print(\"\")\n",
    "print(\"Loaded config:\\n\", config)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QwLz_ubtVlti",
    "outputId": "680f292f-9756-45d4-cb80-af9692475a34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRO for LlavaBaseForCausalLM:\n",
      "\n",
      " 0: __main__.LlavaBaseForCausalLM\n",
      " 1: builtins.object\n",
      "\n",
      "\n",
      "\n",
      "MRO for LlavaLlamaForCausalLM:\n",
      "\n",
      " 0: __main__.LlavaLlamaForCausalLM\n",
      " 1: transformers.models.llama.modeling_llama.LlamaForCausalLM\n",
      " 2: transformers.models.llama.modeling_llama.LlamaPreTrainedModel\n",
      " 3: transformers.modeling_utils.PreTrainedModel\n",
      " 4: torch.nn.modules.module.Module\n",
      " 5: transformers.modeling_utils.ModuleUtilsMixin\n",
      " 6: transformers.generation.utils.GenerationMixin\n",
      " 7: transformers.utils.hub.PushToHubMixin\n",
      " 8: __main__.LlavaBaseForCausalLM\n",
      " 9: builtins.object\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_mro(LlavaBaseForCausalLM)\n",
    "print_mro(LlavaLlamaForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "UJE07TY6Jb5K",
    "outputId": "091b04d2-adbc-49c3-ef2a-50181c53f793"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.__init__(self, config)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "config\n",
      " LlavaLlamaConfig {\n",
      "  \"_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llava_llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaModel.__init__(self, config: LlamaConfig)\n",
      "self\n",
      " <class '__main__.LlavaLlamaModel'>\n",
      "config\n",
      " LlavaLlamaConfig {\n",
      "  \"_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llava_llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "current file path llava/model/llava_arch.py\n",
      "LlavaBaseModel.__init__(self, config)\n",
      "config\n",
      " LlavaLlamaConfig {\n",
      "  \"_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llava_llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "self.model\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0 and are newly initialized: ['model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "LlavaLlamaForCausalLM_model = LlavaLlamaForCausalLM.from_pretrained(model_args.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "wodQZ9rWSyZN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlavaLlamaForCausalLM_model\n",
      " LlavaLlamaForCausalLM(\n",
      "  (model): LlavaLlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-21): 22 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"LlavaLlamaForCausalLM_model\\n\", LlavaLlamaForCausalLM_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "z5c1aEs4gf5L"
   },
   "outputs": [],
   "source": [
    "LlavaLlamaForCausalLM_model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "l9az2bPNccDC"
   },
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from typing import List\n",
    "from enum import auto, Enum\n",
    "\n",
    "class SeparatorStyle(Enum):\n",
    "    \"\"\"Different separator style.\"\"\"\n",
    "    SINGLE = auto()\n",
    "    TWO = auto()\n",
    "    MPT = auto()\n",
    "    PLAIN = auto()\n",
    "    LLAMA_2 = auto()\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Conversation:\n",
    "    \"\"\"A class that keeps all conversation history.\"\"\"\n",
    "    system: str\n",
    "    roles: List[str]\n",
    "    messages: List[List[str]]\n",
    "    offset: int\n",
    "    sep_style: SeparatorStyle = SeparatorStyle.SINGLE\n",
    "    sep: str = \"###\"\n",
    "    sep2: str = None\n",
    "    version: str = \"Unknown\"\n",
    "    skip_next: bool = False\n",
    "\n",
    "\n",
    "conv_llava_plain = Conversation(\n",
    "    system=\"\",\n",
    "    roles=(\"\", \"\"),\n",
    "    messages=(\n",
    "    ),\n",
    "    offset=0,\n",
    "    sep_style=SeparatorStyle.PLAIN,\n",
    "    sep=\"\\n\",\n",
    ")\n",
    "\n",
    "\n",
    "conv_templates = {\n",
    "    \"plain\": conv_llava_plain,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "YSs2_yNVOpOD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport inspect\\nprint(inspect.getattr_static(LlamaForCausalLM, \"from_pretrained\"))\\nprint(inspect.getattr_static(LlamaForCausalLM, \"enable_input_require_grads\"))\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import inspect\n",
    "print(inspect.getattr_static(LlamaForCausalLM, \"from_pretrained\"))\n",
    "print(inspect.getattr_static(LlamaForCausalLM, \"enable_input_require_grads\"))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "None\n",
      "2048\n"
     ]
    }
   ],
   "source": [
    "print(model_args.model_name_or_path)\n",
    "print(training_args.cache_dir)\n",
    "print(training_args.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "od0OYu7yBesv"
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    model_max_length=training_args.model_max_length, \n",
    "    padding_side=\"right\", # 実際にパディングを行うのは tokenizer か DataCollator のどちらかだが、padding_side 自体はtokenizer側で行う\n",
    "    use_fast=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False)}, clean_up_tokenization_spaces=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "M71wf9CYD_Y7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token: </s>\n",
      "pad_token_id: 2\n",
      "unk_token: <unk>\n",
      "unk_token_id: 0\n",
      "tokenizer\n",
      " LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False)}, clean_up_tokenization_spaces=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"pad_token:\", tokenizer.pad_token)\n",
    "print(\"pad_token_id:\", tokenizer.pad_token_id)\n",
    "print(\"unk_token:\", tokenizer.unk_token) # unkownの略です。類似の単語と間違えないでください。\n",
    "print(\"unk_token_id:\", tokenizer.unk_token_id)\n",
    "print(\"tokenizer\\n\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "5TrZtJFODFWz"
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "Vbf0h2NSDoUG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token: <unk>\n",
      "pad_token_id: 0\n",
      "unk_token: <unk>\n",
      "unk_token_id: 0\n",
      "tokenizer\n",
      " LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"pad_token:\", tokenizer.pad_token)\n",
    "print(\"pad_token_id:\", tokenizer.pad_token_id)\n",
    "print(\"unk_token:\", tokenizer.unk_token)\n",
    "print(\"unk_token_id:\", tokenizer.unk_token_id)\n",
    "print(\"tokenizer\\n\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "kGtPYpfKDOKW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_args.version\n",
      " plain\n",
      "default_conversation\n",
      " Conversation(system='', roles=('', ''), messages=(), offset=0, sep_style=<SeparatorStyle.PLAIN: 4>, sep='\\n', sep2=None, version='Unknown', skip_next=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"model_args.version\\n\", model_args.version)\n",
    "default_conversation = conv_templates[model_args.version]\n",
    "print(\"default_conversation\\n\", default_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "csi6Xe_-G0_G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_args.vision_tower\n",
      " openai/clip-vit-large-patch14-336\n"
     ]
    }
   ],
   "source": [
    "print(\"model_args.vision_tower\\n\", model_args.vision_tower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "gpri2EiEIatU"
   },
   "outputs": [],
   "source": [
    "# LlavaLlamaForCausalLMの関数。LlavaLlamaModelをgetする\n",
    "def get_model(self):\n",
    "\n",
    "    print(\"current file path\", \"llava/llava/model/language_model/llava_llama.py\")\n",
    "    print(\"def LlavaLlamaForCausalLM.get_model(self)\")\n",
    "    print(\"self\\n\", type(self))\n",
    "    print(\"self.model (return)\\n\", self.model)\n",
    "    return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "RPVqYUnjJHoF"
   },
   "outputs": [],
   "source": [
    "LlavaLlamaForCausalLM.get_model = get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "Iqqp22MDJSJ-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.get_model(self)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "self.model (return)\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "LlavaLlamaModel_model = LlavaLlamaForCausalLM_model.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "4c_UQLwdPhjd"
   },
   "outputs": [],
   "source": [
    "def config(self):\n",
    "\n",
    "    print(\"current file path\", \"llava/llava/model/multimodal_encoder/clip_encoder.py\")\n",
    "    print(\"def CLIPVisionTower.config(self)\")\n",
    "    print(\"self\\n\", type(self))\n",
    "    print(\"self.is_loaded\\n\", self.is_loaded) # True\n",
    "    print(f\"【COND】 is_loaded={self.is_loaded}\")\n",
    "    if self.is_loaded:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】if self.is_loaded:\")\n",
    "        result = self.vision_tower.config\n",
    "        print(\"result (return)\\n\", type(result))\n",
    "        print(\"【EXIT】if self.is_loaded:\")\n",
    "    else:\n",
    "      pass\n",
    "    print(\"result (return)\\n\", result)\n",
    "    \"\"\"\n",
    "    CLIPVisionConfig {\n",
    "    \"_name_or_path\": \"openai/clip-vit-large-patch14-336\",\n",
    "    \"attention_dropout\": 0.0,\n",
    "    \"dropout\": 0.0,\n",
    "    \"hidden_act\": \"quick_gelu\",\n",
    "    \"hidden_size\": 1024,\n",
    "    \"image_size\": 336,\n",
    "    \"initializer_factor\": 1.0,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 4096,\n",
    "    \"layer_norm_eps\": 1e-05,\n",
    "    \"model_type\": \"clip_vision_model\",\n",
    "    \"num_attention_heads\": 16,\n",
    "    \"num_channels\": 3,\n",
    "    \"num_hidden_layers\": 24,\n",
    "    \"patch_size\": 14,\n",
    "    \"projection_dim\": 768,\n",
    "    \"transformers_version\": \"4.31.0\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "XG8ocYNiPpGB"
   },
   "outputs": [],
   "source": [
    "CLIPVisionTower.config = property(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "HrBNMbAm39zA"
   },
   "outputs": [],
   "source": [
    "def initialize_vision_modules(self, model_args, fsdp=None):\n",
    "\n",
    "  print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "  print(\"def initialize_vision_modules(self, model_args, fsdp=None)\")\n",
    "  print(\"model_args\\n\", model_args) #  ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
    "  print(\"fsdp\\n\", fsdp) # []\n",
    "  vision_tower = model_args.vision_tower\n",
    "  print(\"vision_tower from model_args\\n\", vision_tower) # openai/clip-vit-large-patch14-336\n",
    "  mm_vision_select_layer = model_args.mm_vision_select_layer\n",
    "  print(\"mm_vision_select_layer from model_args\\n\", mm_vision_select_layer) # -2\n",
    "  mm_vision_select_feature = model_args.mm_vision_select_feature\n",
    "  print(\"mm_vision_select_feature from model_args\\n\", mm_vision_select_feature) # patch\n",
    "  pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n",
    "  print(\"pretrain_mm_mlp_adapter from model_args\\n\", pretrain_mm_mlp_adapter) # None\n",
    "  mm_patch_merge_type = model_args.mm_patch_merge_type\n",
    "  # 下記はself.config.mm_vision_towerに関するもの。self.vision_towerは依然としてNone\n",
    "  self.config.mm_vision_tower = vision_tower\n",
    "  print(\"self.config.mm_vision_tower\\n\", self.config.mm_vision_tower) # None\n",
    "\n",
    "  print(\"【COND】 self.get_vision_tower()\\n\", self.get_vision_tower()) # None\n",
    "  print(f\"【COND】 get_vision_tower_is_None={self.get_vision_tower() is None}\")\n",
    "  if self.get_vision_tower() is None:\n",
    "      #【ENTER】self.vision_tower, self.get_vision_towerはNoneなのでこの分岐に入る。\n",
    "      print(\"【ENTER】if self.get_vision_tower() is None:\")\n",
    "      print(\"[ENTER] self.get_vision_tower() is None\")\n",
    "      # build_vision_tower(model_args) はちょっと奥の依存関係が深い\n",
    "      vision_tower = build_vision_tower(model_args)\n",
    "      print(\"vision_tower after build_vision_tower\\n\", vision_tower)\n",
    "      \"\"\"\n",
    "      CLIPVisionTower(\n",
    "      (vision_tower): CLIPVisionModel(\n",
    "      (vision_model): CLIPVisionTransformer(\n",
    "          (embeddings): CLIPVisionEmbeddings(\n",
    "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "          (position_embedding): Embedding(577, 1024)\n",
    "          )\n",
    "          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "          (encoder): CLIPEncoder(\n",
    "          (layers): ModuleList(\n",
    "              (0-23): 24 x CLIPEncoderLayer(\n",
    "              (self_attn): CLIPAttention(\n",
    "                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "              )\n",
    "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "              (mlp): CLIPMLP(\n",
    "                  (activation_fn): QuickGELUActivation()\n",
    "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "              )\n",
    "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "              )\n",
    "          )\n",
    "          )\n",
    "          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "      )\n",
    "      )\n",
    "      )\n",
    "      \"\"\"\n",
    "      # 分散学習(FSDP)を使うかどうか. 今回は [] 空のリストとなるので、Noneではないが、len(fsdp) == 0\n",
    "      print(\"【COND】 fsdp\\n\", fsdp) # []\n",
    "      print(f\"【COND】 fsdp_is_not_None={fsdp is not None} len_fsdp={len(fsdp) if fsdp is not None else 'N/A'}\") # fsdp_is_not_None=True len_fsdp=0\n",
    "      if fsdp is not None and len(fsdp) > 0:\n",
    "        pass\n",
    "      else:\n",
    "          # 【ENTER】else of if fsdp is not None and len(fsdp) > 0:\n",
    "          print(\"【COND】 else_fsdp_is_not_None_and_len_fsdp_gt_0=True\")\n",
    "          print(\"【ENTER】else of if fsdp is not None and len(fsdp) > 0:\")\n",
    "          self.vision_tower = vision_tower\n",
    "          print(\"self.vision_tower\\n\", self.vision_tower)\n",
    "          \"\"\"\n",
    "          CLIPVisionTower(\n",
    "          (vision_tower): CLIPVisionModel(\n",
    "              (vision_model): CLIPVisionTransformer(\n",
    "              (embeddings): CLIPVisionEmbeddings(\n",
    "                  (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "                  (position_embedding): Embedding(577, 1024)\n",
    "              )\n",
    "              (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "              (encoder): CLIPEncoder(\n",
    "                  (layers): ModuleList(\n",
    "                  (0-23): 24 x CLIPEncoderLayer(\n",
    "                      (self_attn): CLIPAttention(\n",
    "                      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                      )\n",
    "                      (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                      (mlp): CLIPMLP(\n",
    "                      (activation_fn): QuickGELUActivation()\n",
    "                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "                      )\n",
    "                      (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                  )\n",
    "                  )\n",
    "              )\n",
    "              (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "              )\n",
    "          )\n",
    "          )\n",
    "          \"\"\"\n",
    "          print(\"【EXIT】else of if fsdp is not None and len(fsdp) > 0:\")\n",
    "\n",
    "      print(\"【EXIT】if self.get_vision_tower() is None:\")\n",
    "  else:\n",
    "    pass\n",
    "\n",
    "  self.config.use_mm_proj = True\n",
    "  print(\"self.config.use_mm_proj set to True\") # True\n",
    "  self.config.mm_projector_type = getattr(model_args, 'mm_projector_type', 'linear')\n",
    "  print(\"self.config.mm_projector_type\\n\", self.config.mm_projector_type) # mlp2x_gelu\n",
    "  self.config.mm_hidden_size = vision_tower.config.hidden_size\n",
    "  print(\"self.config.mm_hidden_size\\n\", self.config.mm_hidden_size) # 1024\n",
    "  self.config.mm_vision_select_layer = mm_vision_select_layer\n",
    "  print(\"self.config.mm_vision_select_layer\\n\", self.config.mm_vision_select_layer) # -2\n",
    "  self.config.mm_vision_select_feature = mm_vision_select_feature\n",
    "  print(\"self.config.mm_vision_select_feature\\n\", self.config.mm_vision_select_feature) # patch\n",
    "  self.config.mm_patch_merge_type = mm_patch_merge_type\n",
    "  print(\"self.config.mm_patch_merge_type\\n\", self.config.mm_patch_merge_type) # flat\n",
    "\n",
    "  # mm_projector_is_None=True\n",
    "  print(f\"【COND】 mm_projector_is_None={getattr(self, 'mm_projector', None) is None}\")\n",
    "  if getattr(self, 'mm_projector', None) is None:\n",
    "      # 【ENTER】\n",
    "      print(\"【ENTER】if getattr(self, 'mm_projector', None) is None:\")\n",
    "      self.mm_projector = build_vision_projector(self.config)\n",
    "      \"\"\"\n",
    "      Sequential(\n",
    "        (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
    "        (1): GELU(approximate='none')\n",
    "        (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
    "      )\n",
    "      \"\"\"\n",
    "      print(\"self.mm_projector after build_vision_projector\\n\", self.mm_projector)\n",
    "      print(\"mm_patch_merge_type\\n\", mm_patch_merge_type) # flat\n",
    "      print(f\"【COND】 unpad_in_mm_patch_merge_type={'unpad' in mm_patch_merge_type}\")\n",
    "      if 'unpad' in mm_patch_merge_type:\n",
    "        pass\n",
    "      print(\"【EXIT】if getattr(self, 'mm_projector', None) is None:\")\n",
    "  else:\n",
    "    pass\n",
    "\n",
    "  print(f\"【COND】 pretrain_mm_mlp_adapter_is_not_None={pretrain_mm_mlp_adapter is not None}\")\n",
    "  if pretrain_mm_mlp_adapter is not None:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "e6_2dvDMkr20"
   },
   "outputs": [],
   "source": [
    "LlavaBaseModel.initialize_vision_modules = initialize_vision_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "qT_vXMXBlP2a"
   },
   "outputs": [],
   "source": [
    "def get_vision_tower(self):\n",
    "\n",
    "    print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "    print(\"def get_vision_tower(self)\")\n",
    "    vision_tower = getattr(self, 'vision_tower', None)\n",
    "    print(\"vision_tower (raw)\\n\", vision_tower)\n",
    "    \"\"\"\n",
    "    CLIPVisionTower(\n",
    "    (vision_tower): CLIPVisionModel(\n",
    "        (vision_model): CLIPVisionTransformer(\n",
    "        (embeddings): CLIPVisionEmbeddings(\n",
    "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "            (position_embedding): Embedding(577, 1024)\n",
    "        )\n",
    "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        (encoder): CLIPEncoder(\n",
    "            (layers): ModuleList(\n",
    "            (0-23): 24 x CLIPEncoderLayer(\n",
    "                (self_attn): CLIPAttention(\n",
    "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                (mlp): CLIPMLP(\n",
    "                (activation_fn): QuickGELUActivation()\n",
    "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "            )\n",
    "            )\n",
    "        )\n",
    "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        )\n",
    "    )\n",
    "    )\n",
    "    \"\"\"\n",
    "    print(\"type(vision_tower)\\n\", type(vision_tower))\n",
    "    print(f\"【COND】 type_vision_tower_is_list={type(vision_tower) is list}\")  # False\n",
    "    if type(vision_tower) is list:\n",
    "        # 【SKIP】\n",
    "        print(\"【ENTER】if type(vision_tower) is list:\")\n",
    "        vision_tower = vision_tower[0]\n",
    "        print(\"【EXIT】if type(vision_tower) is list:\")\n",
    "    print(\"vision_tower (return)\\n\", vision_tower)\n",
    "    \"\"\"\n",
    "    vision_tower (return)\n",
    "    CLIPVisionTower(\n",
    "    (vision_tower): CLIPVisionModel(\n",
    "        (vision_model): CLIPVisionTransformer(\n",
    "        (embeddings): CLIPVisionEmbeddings(\n",
    "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "            (position_embedding): Embedding(577, 1024)\n",
    "        )\n",
    "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        (encoder): CLIPEncoder(\n",
    "            (layers): ModuleList(\n",
    "            (0-23): 24 x CLIPEncoderLayer(\n",
    "                (self_attn): CLIPAttention(\n",
    "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                (mlp): CLIPMLP(\n",
    "                (activation_fn): QuickGELUActivation()\n",
    "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "            )\n",
    "            )\n",
    "        )\n",
    "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        )\n",
    "    )\n",
    "    )\n",
    "    \"\"\"\n",
    "    return vision_tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "iabxtpWflUf9"
   },
   "outputs": [],
   "source": [
    "LlavaBaseModel.get_vision_tower = get_vision_tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "RW9YLSmBT4FG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/model/llava_arch.py\n",
      "def initialize_vision_modules(self, model_args, fsdp=None)\n",
      "model_args\n",
      " ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
      "fsdp\n",
      " []\n",
      "vision_tower from model_args\n",
      " openai/clip-vit-large-patch14-336\n",
      "mm_vision_select_layer from model_args\n",
      " -2\n",
      "mm_vision_select_feature from model_args\n",
      " patch\n",
      "pretrain_mm_mlp_adapter from model_args\n",
      " None\n",
      "self.config.mm_vision_tower\n",
      " openai/clip-vit-large-patch14-336\n",
      "current file path llava/model/llava_arch.py\n",
      "def get_vision_tower(self)\n",
      "vision_tower (raw)\n",
      " None\n",
      "type(vision_tower)\n",
      " <class 'NoneType'>\n",
      "【COND】 type_vision_tower_is_list=False\n",
      "vision_tower (return)\n",
      " None\n",
      "【COND】 self.get_vision_tower()\n",
      " None\n",
      "current file path llava/model/llava_arch.py\n",
      "def get_vision_tower(self)\n",
      "vision_tower (raw)\n",
      " None\n",
      "type(vision_tower)\n",
      " <class 'NoneType'>\n",
      "【COND】 type_vision_tower_is_list=False\n",
      "vision_tower (return)\n",
      " None\n",
      "【COND】 get_vision_tower_is_None=True\n",
      "current file path llava/model/llava_arch.py\n",
      "def get_vision_tower(self)\n",
      "vision_tower (raw)\n",
      " None\n",
      "type(vision_tower)\n",
      " <class 'NoneType'>\n",
      "【COND】 type_vision_tower_is_list=False\n",
      "vision_tower (return)\n",
      " None\n",
      "【ENTER】if self.get_vision_tower() is None:\n",
      "[ENTER] self.get_vision_tower() is None\n",
      "current file path llava/llava/model/multimodal_encoder/builder.py\n",
      "def build_vision_tower(model_args, **kwargs)\n",
      "model_args\n",
      " ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
      "kwargs\n",
      " {}\n",
      "vision_tower from model_args\n",
      " openai/clip-vit-large-patch14-336\n",
      "current file path llava/llava/model/multimodal_encoder/clip_encoder.py\n",
      "def CLIPVisionTower.__init__(self, vision_tower, args\n",
      "self\n",
      " <class '__main__.CLIPVisionTower'>\n",
      "vision_tower\n",
      " openai/clip-vit-large-patch14-336\n",
      "args\n",
      " ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
      "self.is_loaded\n",
      " False\n",
      "self.vision_tower_name\n",
      " openai/clip-vit-large-patch14-336\n",
      "self.select_layer\n",
      " -2\n",
      "self.select_feature\n",
      " patch\n",
      "current file path llava/llava/model/multimodal_encoder/clip_encoder.py\n",
      "def CLIPVisionTower.load_model(self)\n",
      "self\n",
      " <class '__main__.CLIPVisionTower'>\n",
      "self.vision_tower_name\n",
      " openai/clip-vit-large-patch14-336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.image_processor\n",
      " CLIPImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 336,\n",
      "    \"width\": 336\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 336\n",
      "  }\n",
      "}\n",
      "\n",
      "self.vision_tower\n",
      " CLIPVisionModel(\n",
      "  (vision_model): CLIPVisionTransformer(\n",
      "    (embeddings): CLIPVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "      (position_embedding): Embedding(577, 1024)\n",
      "    )\n",
      "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "self.is_loaded\n",
      " True\n",
      "CLIPVisionTower_model\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "【EXIT】if is_absolute_path_exists or vision_tower.startswith('openai') or vision_tower.startswith('laion') or 'ShareGPT4V' in vision_tower:\n",
      "vision_tower after build_vision_tower\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "【COND】 fsdp\n",
      " []\n",
      "【COND】 fsdp_is_not_None=True len_fsdp=0\n",
      "【COND】 else_fsdp_is_not_None_and_len_fsdp_gt_0=True\n",
      "【ENTER】else of if fsdp is not None and len(fsdp) > 0:\n",
      "self.vision_tower\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "【EXIT】else of if fsdp is not None and len(fsdp) > 0:\n",
      "【EXIT】if self.get_vision_tower() is None:\n",
      "self.config.use_mm_proj set to True\n",
      "self.config.mm_projector_type\n",
      " mlp2x_gelu\n",
      "current file path llava/llava/model/multimodal_encoder/clip_encoder.py\n",
      "def CLIPVisionTower.config(self)\n",
      "self\n",
      " <class '__main__.CLIPVisionTower'>\n",
      "self.is_loaded\n",
      " True\n",
      "【COND】 is_loaded=True\n",
      "【ENTER】if self.is_loaded:\n",
      "result (return)\n",
      " <class 'transformers.models.clip.configuration_clip.CLIPVisionConfig'>\n",
      "【EXIT】if self.is_loaded:\n",
      "result (return)\n",
      " CLIPVisionConfig {\n",
      "  \"_name_or_path\": \"openai/clip-vit-large-patch14-336\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"dropout\": 0.0,\n",
      "  \"hidden_act\": \"quick_gelu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": 336,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"model_type\": \"clip_vision_model\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"patch_size\": 14,\n",
      "  \"projection_dim\": 768,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n",
      "self.config.mm_hidden_size\n",
      " 1024\n",
      "self.config.mm_vision_select_layer\n",
      " -2\n",
      "self.config.mm_vision_select_feature\n",
      " patch\n",
      "self.config.mm_patch_merge_type\n",
      " flat\n",
      "【COND】 mm_projector_is_None=True\n",
      "【ENTER】if getattr(self, 'mm_projector', None) is None:\n",
      "self.mm_projector after build_vision_projector\n",
      " Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      ")\n",
      "mm_patch_merge_type\n",
      " flat\n",
      "【COND】 unpad_in_mm_patch_merge_type=False\n",
      "【EXIT】if getattr(self, 'mm_projector', None) is None:\n",
      "【COND】 pretrain_mm_mlp_adapter_is_not_None=False\n"
     ]
    }
   ],
   "source": [
    "LlavaLlamaModel_model.initialize_vision_modules(\n",
    "    model_args=model_args,\n",
    "    fsdp=training_args.fsdp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "w3-wDbzZfnfD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/model/llava_arch.py\n",
      "class LlavaBaseForCausalLM(ABC).get_vision_tower(self)\n",
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.get_model(self)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "self.model (return)\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      "  (vision_tower): CLIPVisionTower(\n",
      "    (vision_tower): CLIPVisionModel(\n",
      "      (vision_model): CLIPVisionTransformer(\n",
      "        (embeddings): CLIPVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "          (position_embedding): Embedding(577, 1024)\n",
      "        )\n",
      "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder): CLIPEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-23): 24 x CLIPEncoderLayer(\n",
      "              (self_attn): CLIPAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): CLIPMLP(\n",
      "                (activation_fn): QuickGELUActivation()\n",
      "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mm_projector): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n",
      "current file path llava/model/llava_arch.py\n",
      "def get_vision_tower(self)\n",
      "vision_tower (raw)\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "type(vision_tower)\n",
      " <class '__main__.CLIPVisionTower'>\n",
      "【COND】 type_vision_tower_is_list=False\n",
      "vision_tower (return)\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "LlavaBaseForCausalLM(ABC).get_vision_tower(self) result (return)\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "vision_tower\n",
      " CLIPVisionTower(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "data_args.image_processor\n",
      " CLIPImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 336,\n",
      "    \"width\": 336\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 336\n",
      "  }\n",
      "}\n",
      "\n",
      "data_args.is_multimodal\n",
      " True\n",
      "model.config.image_aspect_ratio\n",
      " square\n",
      "model.config.tokenizer_padding_side\n",
      " right\n",
      "model.config.tokenizer_model_max_length\n",
      " 2048\n"
     ]
    }
   ],
   "source": [
    "vision_tower = LlavaLlamaForCausalLM_model.get_vision_tower()\n",
    "print(\"vision_tower\\n\", vision_tower)\n",
    "vision_tower.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)\n",
    "\n",
    "data_args.image_processor = vision_tower.image_processor\n",
    "print(\"data_args.image_processor\\n\", data_args.image_processor)\n",
    "data_args.is_multimodal = True\n",
    "print(\"data_args.is_multimodal\\n\", data_args.is_multimodal) # True\n",
    "\n",
    "LlavaLlamaForCausalLM_model.config.image_aspect_ratio = data_args.image_aspect_ratio\n",
    "print(\"model.config.image_aspect_ratio\\n\", LlavaLlamaForCausalLM_model.config.image_aspect_ratio) # square\n",
    "LlavaLlamaForCausalLM_model.config.tokenizer_padding_side = tokenizer.padding_side\n",
    "print(\"model.config.tokenizer_padding_side\\n\", LlavaLlamaForCausalLM_model.config.tokenizer_padding_side) # right\n",
    "LlavaLlamaForCausalLM_model.config.tokenizer_model_max_length = tokenizer.model_max_length\n",
    "print(\"model.config.tokenizer_model_max_length\\n\", LlavaLlamaForCausalLM_model.config.tokenizer_model_max_length) # 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "fBa5nAaWi1M7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【COND】 tune_mm_mlp_adapter=True\n",
      "【ENTER】if model_args.tune_mm_mlp_adapter:\n",
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.get_model(self)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "self.model (return)\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      "  (vision_tower): CLIPVisionTower(\n",
      "    (vision_tower): CLIPVisionModel(\n",
      "      (vision_model): CLIPVisionTransformer(\n",
      "        (embeddings): CLIPVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "          (position_embedding): Embedding(577, 1024)\n",
      "        )\n",
      "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder): CLIPEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-23): 24 x CLIPEncoderLayer(\n",
      "              (self_attn): CLIPAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): CLIPMLP(\n",
      "                (activation_fn): QuickGELUActivation()\n",
      "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mm_projector): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n",
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.get_model(self)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "self.model (return)\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      "  (vision_tower): CLIPVisionTower(\n",
      "    (vision_tower): CLIPVisionModel(\n",
      "      (vision_model): CLIPVisionTransformer(\n",
      "        (embeddings): CLIPVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "          (position_embedding): Embedding(577, 1024)\n",
      "        )\n",
      "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder): CLIPEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-23): 24 x CLIPEncoderLayer(\n",
      "              (self_attn): CLIPAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): CLIPMLP(\n",
      "                (activation_fn): QuickGELUActivation()\n",
      "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mm_projector): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n",
      "model.get_model().mm_projector.parameters() <generator object Module.parameters at 0x7f985a33ef10>\n",
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.get_model(self)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "self.model (return)\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      "  (vision_tower): CLIPVisionTower(\n",
      "    (vision_tower): CLIPVisionModel(\n",
      "      (vision_model): CLIPVisionTransformer(\n",
      "        (embeddings): CLIPVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "          (position_embedding): Embedding(577, 1024)\n",
      "        )\n",
      "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder): CLIPEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-23): 24 x CLIPEncoderLayer(\n",
      "              (self_attn): CLIPAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): CLIPMLP(\n",
      "                (activation_fn): QuickGELUActivation()\n",
      "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mm_projector): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n",
      "model.get_model().mm_projector.parameters() <generator object Module.parameters at 0x7f985a33ef10>\n",
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.get_model(self)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "self.model (return)\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      "  (vision_tower): CLIPVisionTower(\n",
      "    (vision_tower): CLIPVisionModel(\n",
      "      (vision_model): CLIPVisionTransformer(\n",
      "        (embeddings): CLIPVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "          (position_embedding): Embedding(577, 1024)\n",
      "        )\n",
      "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder): CLIPEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-23): 24 x CLIPEncoderLayer(\n",
      "              (self_attn): CLIPAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): CLIPMLP(\n",
      "                (activation_fn): QuickGELUActivation()\n",
      "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mm_projector): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n",
      "model.get_model().mm_projector.parameters() <generator object Module.parameters at 0x7f985a33ef10>\n",
      "current file path llava/llava/model/language_model/llava_llama.py\n",
      "def LlavaLlamaForCausalLM.get_model(self)\n",
      "self\n",
      " <class '__main__.LlavaLlamaForCausalLM'>\n",
      "self.model (return)\n",
      " LlavaLlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-21): 22 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      "  (vision_tower): CLIPVisionTower(\n",
      "    (vision_tower): CLIPVisionModel(\n",
      "      (vision_model): CLIPVisionTransformer(\n",
      "        (embeddings): CLIPVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "          (position_embedding): Embedding(577, 1024)\n",
      "        )\n",
      "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder): CLIPEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-23): 24 x CLIPEncoderLayer(\n",
      "              (self_attn): CLIPAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): CLIPMLP(\n",
      "                (activation_fn): QuickGELUActivation()\n",
      "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mm_projector): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n",
      "model.get_model().mm_projector.parameters() <generator object Module.parameters at 0x7f985a33ef10>\n",
      "【EXIT】if model_args.tune_mm_mlp_adapter:\n"
     ]
    }
   ],
   "source": [
    "LlavaLlamaForCausalLM_model.config.tune_mm_mlp_adapter = model_args.tune_mm_mlp_adapter\n",
    "training_args.tune_mm_mlp_adapter = model_args.tune_mm_mlp_adapter\n",
    "print(f\"【COND】 tune_mm_mlp_adapter={model_args.tune_mm_mlp_adapter}\") # True\n",
    "if model_args.tune_mm_mlp_adapter:\n",
    "    # 【ENTER】 tune_mm_mlp_adapter=True なので、この分岐に入る\n",
    "    print(\"【ENTER】if model_args.tune_mm_mlp_adapter:\")\n",
    "    # モデル全体の全パラメータを「学習不可（requires_grad=False）」にする\n",
    "    # これで通常の重みは全て凍結される\n",
    "    LlavaLlamaForCausalLM_model.requires_grad_(False)\n",
    "    for p in LlavaLlamaForCausalLM_model.get_model().mm_projector.parameters():\n",
    "        # mm_projector（画像特徴量→テキスト特徴量への変換層）の全パラメータだけを「学習可能（requires_grad=True）」に戻す\n",
    "        # これで mm_projector のみ学習されることになる\n",
    "        print(\"model.get_model().mm_projector.parameters()\", LlavaLlamaForCausalLM_model.get_model().mm_projector.parameters())\n",
    "        p.requires_grad = True\n",
    "    print(\"【EXIT】if model_args.tune_mm_mlp_adapter:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "lj1_WjcpsnXB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.mm_projector_lr None\n",
      "model_args.mm_use_im_patch_token False\n"
     ]
    }
   ],
   "source": [
    "LlavaLlamaForCausalLM_model.config.mm_use_im_start_end = model_args.mm_use_im_start_end\n",
    "data_args.mm_use_im_start_end = model_args.mm_use_im_start_end\n",
    "training_args.mm_use_im_start_end = model_args.mm_use_im_start_end\n",
    "\n",
    "LlavaLlamaForCausalLM_model.config.mm_projector_lr = training_args.mm_projector_lr\n",
    "print(\"training_args.mm_projector_lr\", training_args.mm_projector_lr)\n",
    "\n",
    "LlavaLlamaForCausalLM_model.config.mm_use_im_patch_token = model_args.mm_use_im_patch_token\n",
    "print(\"model_args.mm_use_im_patch_token\", model_args.mm_use_im_patch_token)\n",
    "#LlavaLlamaForCausalLM_model.initialize_vision_tokenizer(model_args, tokenizer=tokenizer)\n",
    "#print(\"【EXIT】if model_args.vision_tower is not None:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "zLUf4wqcxKI4"
   },
   "outputs": [],
   "source": [
    "def rank0_print(*args):\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def rank0_print(*args)\")\n",
    "    print(\"args\\n\", args) # ('Formatting inputs...Skip in lazy mode',)\n",
    "    if local_rank == 0:\n",
    "        print(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "839mYcaPw2CZ"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "# URL: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/dataset.py\n",
    "class LazySupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str,\n",
    "                 tokenizer: transformers.PreTrainedTokenizer,\n",
    "                 data_args: DataArguments):\n",
    "\n",
    "        print(\"current file path\", \"llava/train/train.py\")\n",
    "        print(\"def LazySupervisedDataset.__init__(self, data_path, tokenizer, data_args)\")\n",
    "        print(\"data_path\\n\", data_path) # /content/LLaVA/blip_laion_cc_sbu_1.json\n",
    "        print(\"tokenizer\\n\", type(tokenizer)) # <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
    "        print(\"data_args\\n\", data_args) # DataArguments(data_path='/content/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/LLaVA/images', image_aspect_ratio='square')\n",
    "        super(LazySupervisedDataset, self).__init__()\n",
    "        list_data_dict = json.load(open(data_path, \"r\"))\n",
    "        # 今回は1サンプルだけなのでprintしても危険ではない\n",
    "        print(\"list_data_dict\", list_data_dict)\n",
    "\n",
    "        rank0_print(\"Formatting inputs...Skip in lazy mode\") # Formatting inputs...Skip in lazy mode\n",
    "        self.tokenizer = tokenizer\n",
    "        print(\"self.tokenizer\\n\", self.tokenizer)\n",
    "        self.list_data_dict = list_data_dict\n",
    "        print(\"self.list_data_dict\\n\", self.list_data_dict)\n",
    "        self.data_args = data_args\n",
    "        print(\"self.data_args\\n\", self.data_args)\n",
    "    \n",
    "    def __len__(self):\n",
    "\n",
    "        print(\"current file path\", \"llava/train/train.py\")\n",
    "        print(\"def LazySupervisedDataset.__len__(self)\")\n",
    "        return len(self.list_data_dict)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRO for LazySupervisedDataset:\n",
      "\n",
      " 0: __main__.LazySupervisedDataset\n",
      " 1: torch.utils.data.dataset.Dataset\n",
      " 2: typing.Generic\n",
      " 3: builtins.object\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_mro(LazySupervisedDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer\n",
      " LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)\n"
     ]
    }
   ],
   "source": [
    "# だいぶ間が空いているので復習\n",
    "\"\"\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    model_max_length=training_args.model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(\"tokenizer\\n\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/train/train.py\n",
      "def LazySupervisedDataset.__init__(self, data_path, tokenizer, data_args)\n",
      "data_path\n",
      " /workspaces/LLaVA/CC3M_2.json\n",
      "tokenizer\n",
      " <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "data_args\n",
      " DataArguments(data_path='/workspaces/LLaVA/CC3M_2.json', lazy_preprocess=True, is_multimodal=True, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')\n",
      "list_data_dict [{'id': 'GCC_train_000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': '<image>\\nShare a concise interpretation of the image provided.'}, {'from': 'gpt', 'value': 'water pollution in the city'}]}, {'id': 'GCC_train_000196242', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Render a clear and concise summary of the photo.\\n<image>'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]}]\n",
      "current file path llava/train/train.py\n",
      "def rank0_print(*args)\n",
      "args\n",
      " ('Formatting inputs...Skip in lazy mode',)\n",
      "Formatting inputs...Skip in lazy mode\n",
      "self.tokenizer\n",
      " LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)\n",
      "self.list_data_dict\n",
      " [{'id': 'GCC_train_000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': '<image>\\nShare a concise interpretation of the image provided.'}, {'from': 'gpt', 'value': 'water pollution in the city'}]}, {'id': 'GCC_train_000196242', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Render a clear and concise summary of the photo.\\n<image>'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]}]\n",
      "self.data_args\n",
      " DataArguments(data_path='/workspaces/LLaVA/CC3M_2.json', lazy_preprocess=True, is_multimodal=True, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')\n"
     ]
    }
   ],
   "source": [
    "train_dataset = LazySupervisedDataset(data_path=data_args.data_path, tokenizer=tokenizer, data_args=data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'GCC_train_000406392',\n",
       " 'image': 'GCC_train_000406392.jpg',\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': '<image>\\nShare a concise interpretation of the image provided.'},\n",
       "  {'from': 'gpt', 'value': 'water pollution in the city'}]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.list_data_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_sample\n",
      " {'id': 'GCC_train_000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': '<image>\\nShare a concise interpretation of the image provided.'}, {'from': 'gpt', 'value': 'water pollution in the city'}]}\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "data_sample = train_dataset.list_data_dict[i]\n",
    "print(\"data_sample\\n\", data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_file\n",
      " GCC_train_000406392.jpg\n",
      "image_folder\n",
      " /workspaces/LLaVA/images/\n",
      "image_path\n",
      " /workspaces/LLaVA/images/GCC_train_000406392.jpg\n"
     ]
    }
   ],
   "source": [
    "if 'image' in data_sample:\n",
    "    image_file = data_sample['image']\n",
    "    print(\"image_file\\n\", image_file)\n",
    "    image_folder = train_dataset.data_args.image_folder\n",
    "    print(\"image_folder\\n\", image_folder)\n",
    "    image_path = os.path.join(image_folder, image_file)\n",
    "    print(\"image_path\\n\", image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_processor\n",
      " CLIPImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 336,\n",
      "    \"width\": 336\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 336\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_processor = train_dataset.data_args.image_processor\n",
    "print(\"image_processor\\n\", image_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=224x224>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open(image_path).convert('RGB')\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_tensor_shape\n",
      " torch.Size([3, 336, 336])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
       "         [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
       "         [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
       "         ...,\n",
       "         [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
       "         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
       "         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
       "\n",
       "        [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
       "         [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
       "         [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
       "         ...,\n",
       "         [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
       "         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
       "         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
       "\n",
       "        [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
       "         [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
       "         [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
       "         ...,\n",
       "         [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
       "         [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
       "         [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URL: https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/image_processing_clip.py\n",
    "image = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n",
    "print(\"image_tensor_shape\\n\", image.shape)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "data_sample = train_dataset.list_data_dict[i]\n",
    "\n",
    "if isinstance(i, int):\n",
    "    data_sample_list = [data_sample]\n",
    "if 'image' in data_sample:\n",
    "    image_file = data_sample['image']\n",
    "    image_folder = train_dataset.data_args.image_folder\n",
    "    image_path = os.path.join(image_folder, image_file)\n",
    "    image_processor = train_dataset.data_args.image_processor\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load image {image_path}: {e}\")\n",
    "        raise e\n",
    "\n",
    "    image = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialog_pair\n",
      " [{'from': 'human', 'value': '<image>\\nShare a concise interpretation of the image provided.'}, {'from': 'gpt', 'value': 'water pollution in the city'}]\n",
      "dialog_pair_list\n",
      " [[{'from': 'human', 'value': '<image>\\nShare a concise interpretation of the image provided.'}, {'from': 'gpt', 'value': 'water pollution in the city'}]]\n"
     ]
    }
   ],
   "source": [
    "# i = 0 \n",
    "import copy\n",
    "dialog_pair = copy.deepcopy(data_sample[\"conversations\"])\n",
    "print(\"dialog_pair\\n\", dialog_pair)\n",
    "# __getitem__ では 1サンプルずつ取り出すので、基本的にリストの長さは1\n",
    "# pair は1組だが、リストにした方が便利な時があるのでリスト形式に変換する。\n",
    "# リストといっても、リスト形式にしたという意味で、複数サンプルがあるわけではないので注意\n",
    "# dialog_pair 自体がリストなので、二重リストになる。\n",
    "dialog_pair_list = [dialog_pair]\n",
    "print(\"dialog_pair_list\\n\", dialog_pair_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dialog_pair_list)\n",
      " 1\n"
     ]
    }
   ],
   "source": [
    "# 見づらいので、もう一度見やすく定義する\n",
    "dialog_pair_list = [[\n",
    "    {'from': 'human', 'value': 'Render a clear and concise summary of the photo.\\n<image>'},\n",
    "    {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}\n",
    "]]\n",
    "\n",
    "print(\"len(dialog_pair_list)\\n\", len(dialog_pair_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_args\n",
      " DataArguments(data_path='/workspaces/LLaVA/CC3M_2.json', lazy_preprocess=True, is_multimodal=True, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')\n"
     ]
    }
   ],
   "source": [
    "print(\"data_args\\n\", data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_multimodal\n",
      " True\n"
     ]
    }
   ],
   "source": [
    "is_multimodal = data_args.is_multimodal\n",
    "print(\"is_multimodal\\n\", is_multimodal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair current loop\n",
      " [{'from': 'human', 'value': 'Render a clear and concise summary of the photo.\\n<image>'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]\n"
     ]
    }
   ],
   "source": [
    "dialog_pair = dialog_pair_list[0]\n",
    "print(\"pair current loop\\n\", dialog_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_dialog = dialog_pair[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence['value']\n",
      " Render a clear and concise summary of the photo.\n",
      "<image>\n",
      "\n",
      "DEFAULT_IMAGE_TOKEN\n",
      " <image>\n",
      "【COND】 if DEFAULT_IMAGE_TOKEN in sentence['value']: True\n"
     ]
    }
   ],
   "source": [
    "print(\"sentence['value']\\n\", human_dialog['value'])\n",
    "print(\"\\nDEFAULT_IMAGE_TOKEN\\n\", DEFAULT_IMAGE_TOKEN)\n",
    "print(\"【COND】 if DEFAULT_IMAGE_TOKEN in sentence['value']:\", DEFAULT_IMAGE_TOKEN in human_dialog['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_dialog['value'] after remove DEFAULT_IMAGE_TOKEN\n",
      " Render a clear and concise summary of the photo.\n",
      "===\n",
      "human_dialog['value'] after adding DEFAULT_IMAGE_TOKEN to the front\n",
      " <image>\n",
      "Render a clear and concise summary of the photo.\n",
      "===\n",
      "human_dialog['value'] after strip()\n",
      " <image>\n",
      "Render a clear and concise summary of the photo.\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "# strip() は先頭と末尾の空白文字（スペース、タブ、改行など）をすべて削除するPythonの標準メソッドです。\n",
    "# repleace で <image> (DEFAULT_IMAGE_TOKEN) を空文字に置換したあとに、strip()を適用して削除する。\n",
    "\n",
    "human_dialog['value'] = human_dialog['value'].replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
    "print(\"human_dialog['value'] after remove DEFAULT_IMAGE_TOKEN\\n\", human_dialog['value'])\n",
    "print(\"===\")\n",
    "\n",
    "human_dialog['value'] = DEFAULT_IMAGE_TOKEN + '\\n' + human_dialog['value']\n",
    "print(\"human_dialog['value'] after adding DEFAULT_IMAGE_TOKEN to the front\\n\", human_dialog['value'])\n",
    "print(\"===\")\n",
    "\n",
    "human_dialog['value'] = human_dialog['value'].strip()\n",
    "print(\"human_dialog['value'] after strip()\\n\", human_dialog['value'])\n",
    "print(\"===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_dialog['value']\n",
      " <image>\n",
      "Render a clear and concise summary of the photo.\n",
      "\n",
      "DEFAULT_IMAGE_TOKEN\n",
      " <image>\n",
      "【COND】 if DEFAULT_IMAGE_TOKEN in human_dialog['value']: True\n",
      "human_dialog['value'] after remove DEFAULT_IMAGE_TOKEN\n",
      " Render a clear and concise summary of the photo.\n",
      "===\n",
      "human_dialog['value'] after adding DEFAULT_IMAGE_TOKEN to the front\n",
      " <image>\n",
      "Render a clear and concise summary of the photo.\n",
      "===\n",
      "human_dialog['value'] after strip()\n",
      " <image>\n",
      "Render a clear and concise summary of the photo.\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "print(\"human_dialog['value']\\n\", human_dialog['value'])\n",
    "print(\"\\nDEFAULT_IMAGE_TOKEN\\n\", DEFAULT_IMAGE_TOKEN)\n",
    "print(\"【COND】 if DEFAULT_IMAGE_TOKEN in human_dialog['value']:\", DEFAULT_IMAGE_TOKEN in human_dialog['value'])\n",
    "if DEFAULT_IMAGE_TOKEN in human_dialog['value']: # False\n",
    "    # 【SKIP】\n",
    "    human_dialog['value'] = human_dialog['value'].replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
    "    print(\"human_dialog['value'] after remove DEFAULT_IMAGE_TOKEN\\n\", human_dialog['value'])\n",
    "    print(\"===\")\n",
    "\n",
    "    human_dialog['value'] = DEFAULT_IMAGE_TOKEN + '\\n' + human_dialog['value']\n",
    "    print(\"human_dialog['value'] after adding DEFAULT_IMAGE_TOKEN to the front\\n\", human_dialog['value'])\n",
    "    print(\"===\")\n",
    "\n",
    "    human_dialog['value'] = human_dialog['value'].strip()\n",
    "    print(\"human_dialog['value'] after strip()\\n\", human_dialog['value'])\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_dialog current loop\n",
      " {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}\n"
     ]
    }
   ],
   "source": [
    "gpt_dialog = dialog_pair[1]\n",
    "print(\"gpt_dialog current loop\\n\", gpt_dialog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_dialog['value']\n",
      " illustration of a summer holiday in bright colors .\n",
      "\n",
      "DEFAULT_IMAGE_TOKEN\n",
      " <image>\n",
      "【COND】 if DEFAULT_IMAGE_TOKEN in gpt_dialog['value']: False\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "print(\"gpt_dialog['value']\\n\", gpt_dialog['value'])\n",
    "print(\"\\nDEFAULT_IMAGE_TOKEN\\n\", DEFAULT_IMAGE_TOKEN)\n",
    "print(\"【COND】 if DEFAULT_IMAGE_TOKEN in gpt_dialog['value']:\", DEFAULT_IMAGE_TOKEN in gpt_dialog['value'])\n",
    "if DEFAULT_IMAGE_TOKEN in gpt_dialog['value']: # False\n",
    "    # 【SKIP】\n",
    "    gpt_dialog['value'] = gpt_dialog['value'].replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
    "    print(\"gpt_dialog['value'] after remove DEFAULT_IMAGE_TOKEN\\n\", gpt_dialog['value'])\n",
    "    print(\"===\")\n",
    "\n",
    "    gpt_dialog['value'] = DEFAULT_IMAGE_TOKEN + '\\n' + gpt_dialog['value']\n",
    "    print(\"gpt_dialog['value'] after adding DEFAULT_IMAGE_TOKEN\\n\", gpt_dialog['value'])\n",
    "    print(\"===\")\n",
    "\n",
    "    gpt_dialog['value'] = gpt_dialog['value'].strip()\n",
    "    print(\"gpt_dialog['value'] after strip()\\n\", gpt_dialog['value'])\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from typing import Dict\n",
    "\n",
    "def preprocess_multimodal(\n",
    "    dialog_pair: Sequence[str],\n",
    "    data_args: DataArguments\n",
    ") -> Dict:\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def preprocess_multimodal(dialog_pair, data_args)\")\n",
    "    print(\"dialog_pair\\n\", dialog_pair) # [{'from': 'human', 'value': 'Give a brief description of the image.\\n<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]\n",
    "    print(\"data_args\\n\", data_args) # DataArguments(data_path='/content/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/LLaVA/images', image_aspect_ratio='square')\n",
    "    for dialog in dialog_pair:\n",
    "        if DEFAULT_IMAGE_TOKEN in dialog['value']:\n",
    "            dialog['value'] = dialog['value'].replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
    "            dialog['value'] = DEFAULT_IMAGE_TOKEN + '\\n' + dialog['value']\n",
    "            dialog['value'] = dialog['value'].strip()\n",
    "    return dialog_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_pair= [\n",
    "    {'from': 'human', 'value': 'Render a clear and concise summary of the photo.\\n<image>'},\n",
    "    {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/train/train.py\n",
      "def preprocess_multimodal(dialog_pair, data_args)\n",
      "dialog_pair\n",
      " [{'from': 'human', 'value': 'Render a clear and concise summary of the photo.\\n<image>'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]\n",
      "data_args\n",
      " DataArguments(data_path='/workspaces/LLaVA/CC3M_2.json', lazy_preprocess=True, is_multimodal=True, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')\n",
      "dialog_pair\n",
      " [{'from': 'human', 'value': '<image>\\nRender a clear and concise summary of the photo.'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]\n"
     ]
    }
   ],
   "source": [
    "dialog_pair = preprocess_multimodal(dialog_pair, data_args)\n",
    "print(\"dialog_pair\\n\", dialog_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 他のサンプルでも試してみる\n",
    "dialog_pair = [\n",
    "    {'from': 'human', 'value': '<image>\\nShare a concise interpretation of the image provided.'},\n",
    "    {'from': 'gpt', 'value': 'water pollution in the city'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/train/train.py\n",
      "def preprocess_multimodal(dialog_pair, data_args)\n",
      "dialog_pair\n",
      " [{'from': 'human', 'value': '<image>\\nShare a concise interpretation of the image provided.'}, {'from': 'gpt', 'value': 'water pollution in the city'}]\n",
      "data_args\n",
      " DataArguments(data_path='/workspaces/LLaVA/CC3M_2.json', lazy_preprocess=True, is_multimodal=True, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')\n",
      "dialog_pair\n",
      " [{'from': 'human', 'value': '<image>\\nShare a concise interpretation of the image provided.'}, {'from': 'gpt', 'value': 'water pollution in the city'}]\n"
     ]
    }
   ],
   "source": [
    "dialog_pair = preprocess_multimodal(dialog_pair, data_args)\n",
    "print(\"dialog_pair\\n\", dialog_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "hv3nmqgMwVWa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\ndef __len__(self):\\n\\n    print(\"current file path\", \"llava/train/train.py\")\\n    print(\"def LazySupervisedDataset.__len__(self)\")\\n    return len(self.list_data_dict)\\nLazySupervisedDataset.__len__ = __len__\\n'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "def __len__(self):\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def LazySupervisedDataset.__len__(self)\")\n",
    "    return len(self.list_data_dict)\n",
    "LazySupervisedDataset.__len__ = __len__\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: <image>illustration of a summer holiday in bright colors . \n",
      "\n",
      "tokenizer: LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)\n",
      "image_token_index: -200\n",
      "return_tensors: pt\n"
     ]
    }
   ],
   "source": [
    "prompt = \"<image>illustration of a summer holiday in bright colors . \\n\"\n",
    "image_token_index = IMAGE_TOKEN_INDEX # -200\n",
    "return_tensors = \"pt\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "# 例: TinyLlama用\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "print(\"prompt:\", prompt)\n",
    "print(\"tokenizer:\", tokenizer)\n",
    "print(\"image_token_index:\", image_token_index)\n",
    "print(\"return_tensors:\", return_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_before_image\n",
      " \n",
      "text_after_image\n",
      " illustration of a summer holiday in bright colors . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_before_image, text_after_image = prompt.split(\"<image>\")\n",
    "print(\"text_before_image\\n\", text_before_image) # Nothing\n",
    "print(\"text_after_image\\n\", text_after_image) # caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids_before_image\n",
      " [1]\n",
      "ids_after_image\n",
      " [1, 8632, 362, 310, 263, 11801, 8753, 22394, 297, 11785, 11955, 869, 29871, 13]\n",
      "input_ids\n",
      " [1, -200, 8632, 362, 310, 263, 11801, 8753, 22394, 297, 11785, 11955, 869, 29871, 13]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer(chunk).input_ids の 1 は、<s> という、BOS (Beginning Of Sentence) トークンのIDです。\n",
    "# 全く何もない場合でも、<s> という 特殊トークンとして扱われます。\n",
    "ids_before_image = tokenizer(text_before_image).input_ids\n",
    "print(\"ids_before_image\\n\", ids_before_image)\n",
    "ids_after_image = tokenizer(text_after_image).input_ids\n",
    "print(\"ids_after_image\\n\", ids_after_image)\n",
    "\n",
    "# [テキスト前] + [画像トークン] + [テキスト後]\n",
    "ids_after_image_no_bos = ids_after_image[1:]\n",
    "input_ids = ids_before_image + [image_token_index] + ids_after_image_no_bos\n",
    "print(\"input_ids\\n\", input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_ids\n",
      " tensor([    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
      "        11785, 11955,   869, 29871,    13])\n"
     ]
    }
   ],
   "source": [
    "if return_tensors == \"pt\":\n",
    "    tokenized_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "\n",
    "print(\"tokenized_ids\\n\", tokenized_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None):\n",
    "    # <image> の前後で分割\n",
    "    text_before_image, text_after_image = prompt.split(\"<image>\")\n",
    "\n",
    "    # トークナイズ\n",
    "    ids_before_image = tokenizer(text_before_image).input_ids\n",
    "    ids_after_image = tokenizer(text_after_image).input_ids\n",
    "    ids_after_image_no_bos = ids_after_image[1:]\n",
    "\n",
    "    # [テキスト前] + [画像トークン] + [テキスト後]\n",
    "    input_ids = ids_before_image + [image_token_index] + ids_after_image_no_bos\n",
    "\n",
    "    if return_tensors == \"pt\":\n",
    "        return torch.tensor(input_ids, dtype=torch.long)\n",
    "    return input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: <image>illustration of a summer holiday in bright colors .\n",
      "'\n",
      "tokenizer: LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)\n",
      "image_token_index: -200\n",
      "return_tensors: pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
       "        11785, 11955,   869,    13, 29915])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"<image>illustration of a summer holiday in bright colors .\\n'\"\n",
    "image_token_index = IMAGE_TOKEN_INDEX # -200\n",
    "return_tensors = \"pt\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "# 例: TinyLlama用\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "print(\"prompt:\", prompt)\n",
    "print(\"tokenizer:\", tokenizer)\n",
    "print(\"image_token_index:\", image_token_index)\n",
    "print(\"return_tensors:\", return_tensors)\n",
    "\n",
    "tokenizer_image_token(prompt, tokenizer, image_token_index, return_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk\n",
      " \n",
      "tokenizer(chunk)\n",
      " {'input_ids': [1], 'attention_mask': [1]}\n",
      "tokenizer(chunk).input_ids\n",
      " [1]\n",
      "\n",
      "chunk\n",
      " illustration of a summer holiday in bright colors .\n",
      "tokenizer(chunk)\n",
      " {'input_ids': [1, 8632, 362, 310, 263, 11801, 8753, 22394, 297, 11785, 11955, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "tokenizer(chunk).input_ids\n",
      " [1, 8632, 362, 310, 263, 11801, 8753, 22394, 297, 11785, 11955, 869]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# tokenizer(chunk).input_ids の 1 は、<s> という、BOS (Beginning Of Sentence) トークンのIDです。\n",
    "# 全く何もない場合でも、<s> という 特殊トークンとして扱われます。\n",
    "for chunk in prompt.split(\"<image>\"):\n",
    "    print(\"chunk\\n\", chunk)\n",
    "    print(\"tokenizer(chunk)\\n\", tokenizer(chunk))\n",
    "    print(\"tokenizer(chunk).input_ids\\n\", tokenizer(chunk).input_ids)\n",
    "    print(\"\")\n",
    "\n",
    "# セル2：<image>で分割してトークン化\n",
    "prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split(\"<image>\")]\n",
    "print(\"prompt_chunks =\", prompt_chunks)\n",
    "\n",
    "print(\"prompt_chunks\\n\", prompt_chunks)\n",
    "print(\"prompt_chunks[0]\\n\", prompt_chunks[0])\n",
    "print(\"prompt_chunks[0][0]\\n\", prompt_chunks[0][0])\n",
    "\n",
    "# ======================\n",
    "# セル3：画像トークンを間に挟む処理（関数なし）\n",
    "# ======================\n",
    "input_ids = []\n",
    "offset = 0\n",
    "\n",
    "# 最初のチャンクがBOSトークンで始まっていたら、その分をスキップ\n",
    "if len(prompt_chunks) > 0 and len(prompt_chunks[0]) > 0 and prompt_chunks[0][0] == tokenizer.bos_token_id:\n",
    "    offset = 1\n",
    "    input_ids.append(prompt_chunks[0][0])\n",
    "print(\"offset =\", offset)\n",
    "print(\"input_ids\\n\", input_ids)\n",
    "\n",
    "# チャンクと画像トークンを交互に並べる\n",
    "for i, chunk in enumerate(prompt_chunks):\n",
    "    # チャンクの実体を追加\n",
    "    print(\"current chunk\\n\", chunk)\n",
    "    input_ids.extend(chunk[offset:])\n",
    "    print(\"input_ids\\n\", input_ids)\n",
    "    # 最後以外には画像トークンを挿入\n",
    "    if i < len(prompt_chunks) - 1: # len(prompt_chunks) -1  = 2 -1 = 1 より、i=0\n",
    "        print(\"i\", i)\n",
    "        input_ids.extend([image_token_index])\n",
    "        print(f\"--> insert image_token_index ({image_token_index}) between chunks\")\n",
    "\n",
    "print(\"統合後の input_ids:\", input_ids)\n",
    "\n",
    "\n",
    "# ======================\n",
    "# セル4：return_tensors の処理\n",
    "# ======================\n",
    "import torch\n",
    "\n",
    "if return_tensors == \"pt\":\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "\n",
    "print(\"最終出力:\")\n",
    "print(input_ids)\n",
    "\n",
    "# セル4：offset処理と最初の BOS トークン確認\n",
    "input_ids = []\n",
    "offset = 0\n",
    "\n",
    "if len(prompt_chunks) > 0 and len(prompt_chunks[0]) > 0 and prompt_chunks[0][0] == tokenizer.bos_token_id:\n",
    "    offset = 1\n",
    "    input_ids.append(prompt_chunks[0][0])\n",
    "\n",
    "print(\"offset =\", offset)\n",
    "print(\"初期input_ids =\", input_ids)\n",
    "\n",
    "# セル6：return_tensorsの処理\n",
    "import torch\n",
    "\n",
    "if return_tensors is not None:\n",
    "    if return_tensors == \"pt\":\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported tensor type: {return_tensors}\")\n",
    "\n",
    "print(\"最終出力:\")\n",
    "print(input_ids)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None):\\n\\n    print(\"current file path\", \"llava/mm_utils.py\")\\n    print(\"def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None)\")\\n    print(\"prompt\\n\", prompt) # <image>the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair\\n    print(\"tokenizer\\n\", tokenizer) #  LlamaTokenizer(name_or_path=\\'lmsys/vicuna-7b-v1.5\\', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side=\\'right\\', truncation_side=\\'right\\', special_tokens={\\'bos_token\\': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), \\'eos_token\\': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), \\'unk_token\\': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), \\'pad_token\\': \\'<unk>\\'}, clean_up_tokenization_spaces=False)\\n    print(\"image_token_index\\n\", image_token_index) # -200\\n    print(\"return_tensors\\n\", return_tensors) # pt\\n    prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split(\\'<image>\\')]\\n\\n    def insert_separator(X, sep):\\n        return [ele for sublist in zip(X, [sep]*len(X)) for ele in sublist][:-1]\\n\\n    input_ids = []\\n    offset = 0\\n    if len(prompt_chunks) > 0 and len(prompt_chunks[0]) > 0 and prompt_chunks[0][0] == tokenizer.bos_token_id:\\n        offset = 1\\n        input_ids.append(prompt_chunks[0][0])\\n\\n    for x in insert_separator(prompt_chunks, [image_token_index] * (offset + 1)):\\n        input_ids.extend(x[offset:])\\n\\n    if return_tensors is not None:\\n        if return_tensors == \\'pt\\':\\n            return torch.tensor(input_ids, dtype=torch.long)\\n        raise ValueError(f\\'Unsupported tensor type: {return_tensors}\\')\\n    print(\"input_ids (return)\\n\", input_ids)\\n    return input_ids\\n'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None):\n",
    "\n",
    "    print(\"current file path\", \"llava/mm_utils.py\")\n",
    "    print(\"def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None)\")\n",
    "    print(\"prompt\\n\", prompt) # <image>the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair\n",
    "    print(\"tokenizer\\n\", tokenizer) #  LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)\n",
    "    print(\"image_token_index\\n\", image_token_index) # -200\n",
    "    print(\"return_tensors\\n\", return_tensors) # pt\n",
    "    prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split('<image>')]\n",
    "\n",
    "    def insert_separator(X, sep):\n",
    "        return [ele for sublist in zip(X, [sep]*len(X)) for ele in sublist][:-1]\n",
    "\n",
    "    input_ids = []\n",
    "    offset = 0\n",
    "    if len(prompt_chunks) > 0 and len(prompt_chunks[0]) > 0 and prompt_chunks[0][0] == tokenizer.bos_token_id:\n",
    "        offset = 1\n",
    "        input_ids.append(prompt_chunks[0][0])\n",
    "\n",
    "    for x in insert_separator(prompt_chunks, [image_token_index] * (offset + 1)):\n",
    "        input_ids.extend(x[offset:])\n",
    "\n",
    "    if return_tensors is not None:\n",
    "        if return_tensors == 'pt':\n",
    "            return torch.tensor(input_ids, dtype=torch.long)\n",
    "        raise ValueError(f'Unsupported tensor type: {return_tensors}')\n",
    "    print(\"input_ids (return)\\n\", input_ids)\n",
    "    return input_ids\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_pair_list = [[\n",
    "    {'from': 'human', 'value': '<image>\\nRender a clear and concise summary of the photo.'},\n",
    "    {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def preprocess_plain(\n",
    "    dialog_pair: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def preprocess_plain(dialog_pair, tokenizer)\")\n",
    "    print(\"dialog_pair\\n\", dialog_pair) # [[{'from': 'human', 'value': '<image>\\nGive a brief description of the image.'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]\n",
    "    print(\"tokenizer\\n\", type(tokenizer)) # <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
    "    # add end signal and concatenate together\n",
    "    \n",
    "    human_dialog = dialog_pair[0]['value']\n",
    "    print(\"human_dialog\\n\", human_dialog) # <image>\\nGive a brief description of the image.\n",
    "    gpt_dialog = dialog_pair[1]['value']\n",
    "    print(\"gpt_dialog\\n\", gpt_dialog) # the divine queen in her elaborate\n",
    "    default_conversation = conv_templates[\"plain\"] # hard coding\n",
    "    sep_token = default_conversation.sep # separator_token \"\\n\"\n",
    "\n",
    "    prompt = DEFAULT_IMAGE_TOKEN + gpt_dialog + sep_token\n",
    "    print(\"prompt\\n\", prompt)\n",
    "    input_ids = tokenizer_image_token(prompt=prompt, tokenizer=tokenizer, return_tensors='pt')\n",
    "    target_ids = copy.deepcopy(input_ids)\n",
    "\n",
    "\n",
    "    tokenized_len = len(tokenizer_image_token(DEFAULT_IMAGE_TOKEN, tokenizer)) # prompt <image>\n",
    "    print(\"tokenized_len\\n\", tokenized_len) # 2\n",
    "    target_ids[:tokenized_len] = IGNORE_INDEX\n",
    "\n",
    "    # list処理はその後に重複があるのでコメントアウト\n",
    "    #input_ids_list = [input_ids]\n",
    "    #target_ids_list = [target_ids]\n",
    "\n",
    "    data_dict = dict(input_ids=input_ids, labels=target_ids)\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    has_image: bool = False\n",
    ") -> Dict:\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def preprocess(sources, tokenizer, has_image=False)\")\n",
    "    print(\"sources\\n\", sources) # [[{'from': 'human', 'value': '<image>\\nGive a brief description of the image.'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]\n",
    "    print(\"tokenizer\\n\", type(tokenizer)) # <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
    "    print(\"has_image\\n\", has_image) # True\n",
    "    \n",
    "    # Given a list of sources, each is a conversation list. This transform:\n",
    "    # 1. Add signal '### ' at the beginning each sentence, with end signal '\\n';\n",
    "    # 2. Concatenate conversations together;\n",
    "    # 3. Tokenize the concatenated conversation;\n",
    "    # 4. Make a deepcopy as the target. Mask human words with IGNORE_INDEX.\n",
    "    if default_conversation.sep_style == SeparatorStyle.PLAIN:\n",
    "        return preprocess_plain(sources, tokenizer) # True\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/train/train.py\n",
      "def preprocess_plain(dialog_pair_list, tokenizer)\n",
      "dialog_pair_list\n",
      " [[{'from': 'human', 'value': '<image>\\nRender a clear and concise summary of the photo.'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]]\n",
      "tokenizer\n",
      " <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "human_dialog\n",
      " <image>\n",
      "Render a clear and concise summary of the photo.\n",
      "gpt_dialog\n",
      " illustration of a summer holiday in bright colors .\n",
      "prompt\n",
      " <image>illustration of a summer holiday in bright colors .\n",
      "\n",
      "tokenized_len\n",
      " 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
       "         11785, 11955,   869,    13]),\n",
       " 'labels': tensor([ -100,  -100,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
       "         11785, 11955,   869,    13])}"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialog_pair_list = [[\n",
    "    {'from': 'human', 'value': '<image>\\nRender a clear and concise summary of the photo.'},\n",
    "    {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}\n",
    "]]\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "# 例: TinyLlama用\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "data_dict = preprocess_plain(dialog_pair_list, tokenizer)\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from PIL import Image\n",
    "\n",
    "# Trainer > def _get_dataloader > dataloader = self.accelerator.prepare(DataLoader(dataset, **dataloader_params))\n",
    "def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def LazySupervisedDataset.__getitem__(self, i)\")\n",
    "    print(\"i\\n\", i) # 0\n",
    "    data_sample = self.list_data_dict[i]\n",
    "    dialog_pair = copy.deepcopy(data_sample[\"conversations\"])\n",
    "    \n",
    "    # 画像+テキストのサンプル\n",
    "    if 'image' in data_sample:\n",
    "        image_file = data_sample['image']\n",
    "        image_folder = self.data_args.image_folder\n",
    "        image_processor = self.data_args.image_processor      \n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        print(\"image_path\\n\", image_path)\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image: {e}\")\n",
    "            # 画像ファイルのパスがなければこのサンプルは無効としてスキップ\n",
    "            print(\"Skipping this sample due to image loading error.\")\n",
    "            return None\n",
    "        image = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n",
    "        dialog_pair = preprocess_multimodal(dialog_pair, self.data_args)\n",
    "        data_dict = preprocess_plain(dialog_pair, self.tokenizer)\n",
    "    # テキストのみのサンプル\n",
    "    else:\n",
    "        dialog_pair = copy.deepcopy(data_sample[\"conversations\"])\n",
    "        data_dict = preprocess_plain(dialog_pair, self.tokenizer)\n",
    "\n",
    "    print(\"Calling preprocess...\")\n",
    "    data_dict = preprocess_plain(dialog_pair, self.tokenizer)\n",
    "    print(\"data_dict (after preprocess)\\n\", data_dict)\n",
    "\n",
    "    # image exist in the data\n",
    "    if 'image' in data_sample:\n",
    "        data_dict['image'] = image\n",
    "    elif self.data_args.is_multimodal:\n",
    "        # image does not exist in the data, but the model is multimodal\n",
    "        # 完全な黒の画像をダミー画像として生成して添える\n",
    "        crop_size = self.data_args.image_processor.crop_size\n",
    "        data_dict['image'] = torch.zeros(3, crop_size['height'], crop_size['width'])\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import copy\n",
    "\n",
    "def preprocess_plain(s\n",
    "    sources: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def preprocess_plain(sources, tokenizer)\")\n",
    "    print(\"sources\\n\", sources) # [[{'from': 'human', 'value': '<image>\\nGive a brief description of the image.'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]\n",
    "    print(\"tokenizer\\n\", type(tokenizer)) # <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
    "    # add end signal and concatenate together\n",
    "    conversations = []\n",
    "    print(\"conversations initial\\n\", conversations) # []\n",
    "    for source in sources:\n",
    "        print(\"source current loop\\n\", source) \n",
    "        assert len(source) == 2\n",
    "        assert DEFAULT_IMAGE_TOKEN in source[0]['value']\n",
    "        source[0]['value'] = DEFAULT_IMAGE_TOKEN\n",
    "        conversation = source[0]['value'] + source[1]['value'] + default_conversation.sep\n",
    "        print(\"conversation current loop\\n\", conversation)\n",
    "        conversations.append(conversation)\n",
    "    print(\"conversations (final)\\n\", conversations) #  ['<image>the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair\\n']\n",
    "    # tokenize conversations\n",
    "    input_ids = [tokenizer_image_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations]\n",
    "    print(\"input_ids\\n\", input_ids) # [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114, 411,  2654, 11315,    13])]\n",
    "    for idx, tensor in enumerate(input_ids):\n",
    "        if hasattr(tensor, 'shape'):\n",
    "            print(f\"input_ids[{idx}].shape\\n\", tensor.shape) # torch.Size([24])\n",
    "    targets = copy.deepcopy(input_ids)\n",
    "    print(\"targets\\n\", targets) # [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114, 411,  2654, 11315,    13])]\n",
    "    for idx, tensor in enumerate(targets):\n",
    "        if hasattr(tensor, 'shape'):\n",
    "            print(f\"targets[{idx}].shape\\n\", tensor.shape) # torch.Size([24])\n",
    "    print(\"sources\\n\", sources) # [[{'from': 'human', 'value': '<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]\n",
    "    for target, source in zip(targets, sources):\n",
    "        tokenized_len = len(tokenizer_image_token(source[0]['value'], tokenizer)) # prompt <image>\n",
    "        target[:tokenized_len] = IGNORE_INDEX\n",
    "\n",
    "    print(\"input_ids (return)\\n\", input_ids) # [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114, 411,  2654, 11315,    13])]\n",
    "    print(\"targets (return)\\n\", targets) #  [tensor([ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114, 411,  2654, 11315,    13])]\n",
    "    return dict(input_ids=input_ids, labels=targets)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 擬似データの input_ids, labels\n",
    "input_ids0 = torch.tensor([1, -200, 4094, 21180, 918, 297, 278, 4272, 13])\n",
    "labels0 = torch.tensor([-100, -100, 4094, 21180, 918, 297, 278, 4272, 13])\n",
    "image0 = torch.randn(3, 336, 336)\n",
    "\n",
    "input_ids1 = torch.tensor([1, -200, 8632, 362, 310, 263, 11801, 8753, 22394, 297, 11785, 11955, 869, 13])\n",
    "labels1 = torch.tensor([-100, -100, 8632, 362, 310, 263, 11801, 8753, 22394, 297, 11785, 11955, 869, 13])\n",
    "image1 = torch.randn(3, 336, 336)\n",
    "\n",
    "instances = [\n",
    "    {\n",
    "        \"input_ids\": input_ids0,\n",
    "        \"labels\": labels0,\n",
    "        \"image\": image0\n",
    "    },\n",
    "    {\n",
    "        \"input_ids\": input_ids1 ,\n",
    "        \"labels\": labels1,\n",
    "        \"image\": image1\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids0: <s> <image> water poll ution in the city \n",
      "\n",
      "input_ids1: <s> <image> illustr ation of a summer hol iday in bright colors . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 例: TinyLlama用\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "def custom_decode(input_ids, tokenizer):\n",
    "    # -200 など特殊トークンは手動で置換\n",
    "    tokens = []\n",
    "    for id in input_ids.tolist():\n",
    "        if id == -200:\n",
    "            tokens.append(\"<image>\")\n",
    "        elif id == -100:\n",
    "            tokens.append(\"<IGNORE>\")\n",
    "        else:\n",
    "            tokens.append(tokenizer.decode([id], skip_special_tokens=False))\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "print(\"input_ids0:\", custom_decode(input_ids0, tokenizer))\n",
    "print(\"input_ids1:\", custom_decode(input_ids1, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([    1,  -200,  4094, 21180,   918,   297,   278,  4272,    13]),\n",
       "  'labels': tensor([ -100,  -100,  4094, 21180,   918,   297,   278,  4272,    13]),\n",
       "  'image': tensor([[[-1.8835, -0.3822, -2.5373,  ..., -0.7053, -0.9337,  1.1468],\n",
       "           [-1.2558,  2.5180, -0.4216,  ...,  0.8925, -0.5908,  0.3425],\n",
       "           [-2.6587,  0.0197, -1.3906,  ...,  0.9975, -0.4158,  0.8519],\n",
       "           ...,\n",
       "           [ 0.2711, -1.3692,  0.1377,  ...,  0.2996, -1.0353, -0.6736],\n",
       "           [ 0.0296,  1.7625, -0.9573,  ...,  0.6216,  1.0572, -1.3407],\n",
       "           [-0.2703, -0.5290, -0.3409,  ...,  1.2841,  0.6272,  0.8975]],\n",
       "  \n",
       "          [[-0.1837, -0.5841,  1.1485,  ..., -0.2424,  0.4561, -0.4998],\n",
       "           [-0.6487,  0.0952,  0.3311,  ..., -1.0449, -0.3937, -0.8867],\n",
       "           [ 3.2370,  0.2340,  0.4478,  ...,  0.0451, -1.9259, -0.3385],\n",
       "           ...,\n",
       "           [ 0.3318, -0.2168,  0.2938,  ..., -0.4289,  0.9286, -0.2047],\n",
       "           [ 1.2384, -0.8697,  0.3743,  ..., -0.4930,  1.5127,  0.9692],\n",
       "           [ 0.1569,  0.0087,  0.6907,  ...,  0.1936, -0.0832,  0.1761]],\n",
       "  \n",
       "          [[-0.1304,  1.1641,  1.2552,  ...,  0.7838,  1.4909,  1.1663],\n",
       "           [ 2.2470,  0.4694, -1.3782,  ...,  0.6459, -0.7242, -2.2146],\n",
       "           [-1.7083, -0.0737, -0.6669,  ..., -0.5856,  0.1070, -0.9377],\n",
       "           ...,\n",
       "           [-0.6248, -1.0099,  0.4351,  ..., -0.0610, -0.0414,  1.5906],\n",
       "           [ 2.1607,  0.3265,  1.8075,  ...,  1.8937, -1.0137, -2.3498],\n",
       "           [ 0.0790, -2.3247,  1.5815,  ..., -0.2514,  0.6149, -0.1566]]])},\n",
       " {'input_ids': tensor([    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
       "          11785, 11955,   869,    13]),\n",
       "  'labels': tensor([ -100,  -100,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
       "          11785, 11955,   869,    13]),\n",
       "  'image': tensor([[[-1.0478, -1.4113,  0.7405,  ...,  0.3304,  1.0833,  1.3053],\n",
       "           [ 0.5359,  1.2097, -1.7203,  ...,  0.4078,  0.9025,  0.9674],\n",
       "           [-0.2481, -2.4219,  0.1939,  ...,  0.3531,  0.4591, -0.0878],\n",
       "           ...,\n",
       "           [-0.5736, -0.1429, -1.0400,  ...,  0.5142,  0.9681,  0.3935],\n",
       "           [-0.2472, -0.4690, -0.0548,  ...,  0.9243, -1.5814, -0.7341],\n",
       "           [ 0.4793,  0.2714,  0.1649,  ...,  0.2690, -0.4838,  0.3442]],\n",
       "  \n",
       "          [[-0.1226, -1.5070,  0.1656,  ...,  0.3575,  0.3321,  1.9848],\n",
       "           [ 0.3799,  1.4754, -0.8209,  ..., -0.2339,  0.8225,  0.9976],\n",
       "           [ 0.8180,  0.2029,  1.0558,  ...,  1.0203,  1.4675, -0.8886],\n",
       "           ...,\n",
       "           [-1.5128,  1.4707, -0.6439,  ..., -0.4760,  1.0265,  1.6681],\n",
       "           [-0.6419,  0.7795,  0.2935,  ...,  0.8649,  1.4358, -0.7028],\n",
       "           [-0.9246, -0.5351, -0.7028,  ...,  0.2828, -0.1938,  0.3045]],\n",
       "  \n",
       "          [[-0.8412, -0.0623, -1.1485,  ..., -1.3483,  0.3278,  1.1276],\n",
       "           [ 2.6520, -1.6820,  0.5106,  ..., -0.4740, -1.2570,  1.8308],\n",
       "           [-0.8365,  0.5188, -2.4893,  ..., -0.6288, -0.5259, -0.9685],\n",
       "           ...,\n",
       "           [-0.7418, -1.9129,  2.4009,  ..., -0.4762,  0.9375,  0.3237],\n",
       "           [-1.5323,  1.0716,  0.4702,  ...,  0.0510, -0.3231,  0.8547],\n",
       "           [-1.0641, -1.8975,  1.4758,  ..., -0.2257,  0.3994, -0.1101]]])}]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データローダーが None を返すことがあるので、Noneのサンプルを除外。\n",
    "instances = [x for x in instances if x is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, labels = tuple([instance[key] for instance in instances] \n",
    "                          for key in (\"input_ids\", \"labels\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      " [tensor([    1,  -200,  4094, 21180,   918,   297,   278,  4272,    13]), tensor([    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
      "        11785, 11955,   869,    13])]\n",
      "labels\n",
      " [tensor([ -100,  -100,  4094, 21180,   918,   297,   278,  4272,    13]), tensor([ -100,  -100,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
      "        11785, 11955,   869,    13])]\n"
     ]
    }
   ],
   "source": [
    "print(\"input_ids\\n\", input_ids)\n",
    "print(\"labels\\n\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token_id\n",
      " 0\n",
      "input_ids\n",
      " tensor([[    1,  -200,  4094, 21180,   918,   297,   278,  4272,    13,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
      "         11785, 11955,   869,    13]])\n"
     ]
    }
   ],
   "source": [
    "# input_idsはtokenizerのpad_token_id(0)でパディング\n",
    "print(\"tokenizer.pad_token_id\\n\", tokenizer.pad_token_id)\n",
    "input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "    input_ids,\n",
    "    batch_first=True,\n",
    "    padding_value=tokenizer.pad_token_id\n",
    ")\n",
    "print(\"input_ids\\n\", input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IGNORE_INDEX\n",
      " -100\n",
      "labels\n",
      " tensor([[ -100,  -100,  4094, 21180,   918,   297,   278,  4272,    13,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
      "         11785, 11955,   869,    13]])\n"
     ]
    }
   ],
   "source": [
    "# labelsはIGNORE_INDEX(-100)でパディング\n",
    "print(\"IGNORE_INDEX\\n\", IGNORE_INDEX)\n",
    "labels = torch.nn.utils.rnn.pad_sequence(\n",
    "    labels,\n",
    "    batch_first=True,\n",
    "    padding_value=IGNORE_INDEX\n",
    ")\n",
    "print(\"labels\\n\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = input_ids[:, :tokenizer.model_max_length]\n",
    "labels = labels[:, :tokenizer.model_max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      " tensor([[    1,  -200,  4094, 21180,   918,   297,   278,  4272,    13,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
      "         11785, 11955,   869,    13]])\n",
      "labels\n",
      " tensor([[ -100,  -100,  4094, 21180,   918,   297,   278,  4272,    13,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
      "         11785, 11955,   869,    13]])\n"
     ]
    }
   ],
   "source": [
    "print(\"input_ids\\n\", input_ids)\n",
    "print(\"labels\\n\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_mask\n",
      " tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "         False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "attention_mask=input_ids.ne(tokenizer.pad_token_id)\n",
    "print(\"attention_mask\\n\", attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = dict(\n",
    "    input_ids=input_ids,\n",
    "    labels=labels,\n",
    "    attention_mask=attention_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,  -200,  4094, 21180,   918,   297,   278,  4272,    13,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
       "          11785, 11955,   869,    13]]),\n",
       " 'labels': tensor([[ -100,  -100,  4094, 21180,   918,   297,   278,  4272,    13,  -100,\n",
       "           -100,  -100,  -100,  -100],\n",
       "         [ -100,  -100,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
       "          11785, 11955,   869,    13]]),\n",
       " 'attention_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "          False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True]])}"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'image' in instances[0]:\n",
    "    # 画像リストを抽出\n",
    "    images = [instance['image'] for instance in instances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[-1.8835, -0.3822, -2.5373,  ..., -0.7053, -0.9337,  1.1468],\n",
       "          [-1.2558,  2.5180, -0.4216,  ...,  0.8925, -0.5908,  0.3425],\n",
       "          [-2.6587,  0.0197, -1.3906,  ...,  0.9975, -0.4158,  0.8519],\n",
       "          ...,\n",
       "          [ 0.2711, -1.3692,  0.1377,  ...,  0.2996, -1.0353, -0.6736],\n",
       "          [ 0.0296,  1.7625, -0.9573,  ...,  0.6216,  1.0572, -1.3407],\n",
       "          [-0.2703, -0.5290, -0.3409,  ...,  1.2841,  0.6272,  0.8975]],\n",
       " \n",
       "         [[-0.1837, -0.5841,  1.1485,  ..., -0.2424,  0.4561, -0.4998],\n",
       "          [-0.6487,  0.0952,  0.3311,  ..., -1.0449, -0.3937, -0.8867],\n",
       "          [ 3.2370,  0.2340,  0.4478,  ...,  0.0451, -1.9259, -0.3385],\n",
       "          ...,\n",
       "          [ 0.3318, -0.2168,  0.2938,  ..., -0.4289,  0.9286, -0.2047],\n",
       "          [ 1.2384, -0.8697,  0.3743,  ..., -0.4930,  1.5127,  0.9692],\n",
       "          [ 0.1569,  0.0087,  0.6907,  ...,  0.1936, -0.0832,  0.1761]],\n",
       " \n",
       "         [[-0.1304,  1.1641,  1.2552,  ...,  0.7838,  1.4909,  1.1663],\n",
       "          [ 2.2470,  0.4694, -1.3782,  ...,  0.6459, -0.7242, -2.2146],\n",
       "          [-1.7083, -0.0737, -0.6669,  ..., -0.5856,  0.1070, -0.9377],\n",
       "          ...,\n",
       "          [-0.6248, -1.0099,  0.4351,  ..., -0.0610, -0.0414,  1.5906],\n",
       "          [ 2.1607,  0.3265,  1.8075,  ...,  1.8937, -1.0137, -2.3498],\n",
       "          [ 0.0790, -2.3247,  1.5815,  ..., -0.2514,  0.6149, -0.1566]]]),\n",
       " tensor([[[-1.0478, -1.4113,  0.7405,  ...,  0.3304,  1.0833,  1.3053],\n",
       "          [ 0.5359,  1.2097, -1.7203,  ...,  0.4078,  0.9025,  0.9674],\n",
       "          [-0.2481, -2.4219,  0.1939,  ...,  0.3531,  0.4591, -0.0878],\n",
       "          ...,\n",
       "          [-0.5736, -0.1429, -1.0400,  ...,  0.5142,  0.9681,  0.3935],\n",
       "          [-0.2472, -0.4690, -0.0548,  ...,  0.9243, -1.5814, -0.7341],\n",
       "          [ 0.4793,  0.2714,  0.1649,  ...,  0.2690, -0.4838,  0.3442]],\n",
       " \n",
       "         [[-0.1226, -1.5070,  0.1656,  ...,  0.3575,  0.3321,  1.9848],\n",
       "          [ 0.3799,  1.4754, -0.8209,  ..., -0.2339,  0.8225,  0.9976],\n",
       "          [ 0.8180,  0.2029,  1.0558,  ...,  1.0203,  1.4675, -0.8886],\n",
       "          ...,\n",
       "          [-1.5128,  1.4707, -0.6439,  ..., -0.4760,  1.0265,  1.6681],\n",
       "          [-0.6419,  0.7795,  0.2935,  ...,  0.8649,  1.4358, -0.7028],\n",
       "          [-0.9246, -0.5351, -0.7028,  ...,  0.2828, -0.1938,  0.3045]],\n",
       " \n",
       "         [[-0.8412, -0.0623, -1.1485,  ..., -1.3483,  0.3278,  1.1276],\n",
       "          [ 2.6520, -1.6820,  0.5106,  ..., -0.4740, -1.2570,  1.8308],\n",
       "          [-0.8365,  0.5188, -2.4893,  ..., -0.6288, -0.5259, -0.9685],\n",
       "          ...,\n",
       "          [-0.7418, -1.9129,  2.4009,  ..., -0.4762,  0.9375,  0.3237],\n",
       "          [-1.5323,  1.0716,  0.4702,  ...,  0.0510, -0.3231,  0.8547],\n",
       "          [-1.0641, -1.8975,  1.4758,  ..., -0.2257,  0.3994, -0.1101]]])]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_same_shape = all(\n",
    "    x is not None and x.shape == images[0].shape for x in images\n",
    ")\n",
    "\n",
    "all_same_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像をバッチ化 or リストのまま格納\n",
    "if all_same_shape:\n",
    "    batch['images'] = torch.stack(images)\n",
    "else:\n",
    "    # リストのまま渡すと困るのではないか？この疑問は未解決\n",
    "    batch['images'] = images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.8835, -0.3822, -2.5373,  ..., -0.7053, -0.9337,  1.1468],\n",
       "          [-1.2558,  2.5180, -0.4216,  ...,  0.8925, -0.5908,  0.3425],\n",
       "          [-2.6587,  0.0197, -1.3906,  ...,  0.9975, -0.4158,  0.8519],\n",
       "          ...,\n",
       "          [ 0.2711, -1.3692,  0.1377,  ...,  0.2996, -1.0353, -0.6736],\n",
       "          [ 0.0296,  1.7625, -0.9573,  ...,  0.6216,  1.0572, -1.3407],\n",
       "          [-0.2703, -0.5290, -0.3409,  ...,  1.2841,  0.6272,  0.8975]],\n",
       "\n",
       "         [[-0.1837, -0.5841,  1.1485,  ..., -0.2424,  0.4561, -0.4998],\n",
       "          [-0.6487,  0.0952,  0.3311,  ..., -1.0449, -0.3937, -0.8867],\n",
       "          [ 3.2370,  0.2340,  0.4478,  ...,  0.0451, -1.9259, -0.3385],\n",
       "          ...,\n",
       "          [ 0.3318, -0.2168,  0.2938,  ..., -0.4289,  0.9286, -0.2047],\n",
       "          [ 1.2384, -0.8697,  0.3743,  ..., -0.4930,  1.5127,  0.9692],\n",
       "          [ 0.1569,  0.0087,  0.6907,  ...,  0.1936, -0.0832,  0.1761]],\n",
       "\n",
       "         [[-0.1304,  1.1641,  1.2552,  ...,  0.7838,  1.4909,  1.1663],\n",
       "          [ 2.2470,  0.4694, -1.3782,  ...,  0.6459, -0.7242, -2.2146],\n",
       "          [-1.7083, -0.0737, -0.6669,  ..., -0.5856,  0.1070, -0.9377],\n",
       "          ...,\n",
       "          [-0.6248, -1.0099,  0.4351,  ..., -0.0610, -0.0414,  1.5906],\n",
       "          [ 2.1607,  0.3265,  1.8075,  ...,  1.8937, -1.0137, -2.3498],\n",
       "          [ 0.0790, -2.3247,  1.5815,  ..., -0.2514,  0.6149, -0.1566]]],\n",
       "\n",
       "\n",
       "        [[[-1.0478, -1.4113,  0.7405,  ...,  0.3304,  1.0833,  1.3053],\n",
       "          [ 0.5359,  1.2097, -1.7203,  ...,  0.4078,  0.9025,  0.9674],\n",
       "          [-0.2481, -2.4219,  0.1939,  ...,  0.3531,  0.4591, -0.0878],\n",
       "          ...,\n",
       "          [-0.5736, -0.1429, -1.0400,  ...,  0.5142,  0.9681,  0.3935],\n",
       "          [-0.2472, -0.4690, -0.0548,  ...,  0.9243, -1.5814, -0.7341],\n",
       "          [ 0.4793,  0.2714,  0.1649,  ...,  0.2690, -0.4838,  0.3442]],\n",
       "\n",
       "         [[-0.1226, -1.5070,  0.1656,  ...,  0.3575,  0.3321,  1.9848],\n",
       "          [ 0.3799,  1.4754, -0.8209,  ..., -0.2339,  0.8225,  0.9976],\n",
       "          [ 0.8180,  0.2029,  1.0558,  ...,  1.0203,  1.4675, -0.8886],\n",
       "          ...,\n",
       "          [-1.5128,  1.4707, -0.6439,  ..., -0.4760,  1.0265,  1.6681],\n",
       "          [-0.6419,  0.7795,  0.2935,  ...,  0.8649,  1.4358, -0.7028],\n",
       "          [-0.9246, -0.5351, -0.7028,  ...,  0.2828, -0.1938,  0.3045]],\n",
       "\n",
       "         [[-0.8412, -0.0623, -1.1485,  ..., -1.3483,  0.3278,  1.1276],\n",
       "          [ 2.6520, -1.6820,  0.5106,  ..., -0.4740, -1.2570,  1.8308],\n",
       "          [-0.8365,  0.5188, -2.4893,  ..., -0.6288, -0.5259, -0.9685],\n",
       "          ...,\n",
       "          [-0.7418, -1.9129,  2.4009,  ..., -0.4762,  0.9375,  0.3237],\n",
       "          [-1.5323,  1.0716,  0.4702,  ...,  0.0510, -0.3231,  0.8547],\n",
       "          [-1.0641, -1.8975,  1.4758,  ..., -0.2257,  0.3994, -0.1101]]]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,  -200,  4094, 21180,   918,   297,   278,  4272,    13,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
       "          11785, 11955,   869,    13]]),\n",
       " 'labels': tensor([[ -100,  -100,  4094, 21180,   918,   297,   278,  4272,    13,  -100,\n",
       "           -100,  -100,  -100,  -100],\n",
       "         [ -100,  -100,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
       "          11785, 11955,   869,    13]]),\n",
       " 'attention_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "          False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True]]),\n",
       " 'images': tensor([[[[-1.8835, -0.3822, -2.5373,  ..., -0.7053, -0.9337,  1.1468],\n",
       "           [-1.2558,  2.5180, -0.4216,  ...,  0.8925, -0.5908,  0.3425],\n",
       "           [-2.6587,  0.0197, -1.3906,  ...,  0.9975, -0.4158,  0.8519],\n",
       "           ...,\n",
       "           [ 0.2711, -1.3692,  0.1377,  ...,  0.2996, -1.0353, -0.6736],\n",
       "           [ 0.0296,  1.7625, -0.9573,  ...,  0.6216,  1.0572, -1.3407],\n",
       "           [-0.2703, -0.5290, -0.3409,  ...,  1.2841,  0.6272,  0.8975]],\n",
       " \n",
       "          [[-0.1837, -0.5841,  1.1485,  ..., -0.2424,  0.4561, -0.4998],\n",
       "           [-0.6487,  0.0952,  0.3311,  ..., -1.0449, -0.3937, -0.8867],\n",
       "           [ 3.2370,  0.2340,  0.4478,  ...,  0.0451, -1.9259, -0.3385],\n",
       "           ...,\n",
       "           [ 0.3318, -0.2168,  0.2938,  ..., -0.4289,  0.9286, -0.2047],\n",
       "           [ 1.2384, -0.8697,  0.3743,  ..., -0.4930,  1.5127,  0.9692],\n",
       "           [ 0.1569,  0.0087,  0.6907,  ...,  0.1936, -0.0832,  0.1761]],\n",
       " \n",
       "          [[-0.1304,  1.1641,  1.2552,  ...,  0.7838,  1.4909,  1.1663],\n",
       "           [ 2.2470,  0.4694, -1.3782,  ...,  0.6459, -0.7242, -2.2146],\n",
       "           [-1.7083, -0.0737, -0.6669,  ..., -0.5856,  0.1070, -0.9377],\n",
       "           ...,\n",
       "           [-0.6248, -1.0099,  0.4351,  ..., -0.0610, -0.0414,  1.5906],\n",
       "           [ 2.1607,  0.3265,  1.8075,  ...,  1.8937, -1.0137, -2.3498],\n",
       "           [ 0.0790, -2.3247,  1.5815,  ..., -0.2514,  0.6149, -0.1566]]],\n",
       " \n",
       " \n",
       "         [[[-1.0478, -1.4113,  0.7405,  ...,  0.3304,  1.0833,  1.3053],\n",
       "           [ 0.5359,  1.2097, -1.7203,  ...,  0.4078,  0.9025,  0.9674],\n",
       "           [-0.2481, -2.4219,  0.1939,  ...,  0.3531,  0.4591, -0.0878],\n",
       "           ...,\n",
       "           [-0.5736, -0.1429, -1.0400,  ...,  0.5142,  0.9681,  0.3935],\n",
       "           [-0.2472, -0.4690, -0.0548,  ...,  0.9243, -1.5814, -0.7341],\n",
       "           [ 0.4793,  0.2714,  0.1649,  ...,  0.2690, -0.4838,  0.3442]],\n",
       " \n",
       "          [[-0.1226, -1.5070,  0.1656,  ...,  0.3575,  0.3321,  1.9848],\n",
       "           [ 0.3799,  1.4754, -0.8209,  ..., -0.2339,  0.8225,  0.9976],\n",
       "           [ 0.8180,  0.2029,  1.0558,  ...,  1.0203,  1.4675, -0.8886],\n",
       "           ...,\n",
       "           [-1.5128,  1.4707, -0.6439,  ..., -0.4760,  1.0265,  1.6681],\n",
       "           [-0.6419,  0.7795,  0.2935,  ...,  0.8649,  1.4358, -0.7028],\n",
       "           [-0.9246, -0.5351, -0.7028,  ...,  0.2828, -0.1938,  0.3045]],\n",
       " \n",
       "          [[-0.8412, -0.0623, -1.1485,  ..., -1.3483,  0.3278,  1.1276],\n",
       "           [ 2.6520, -1.6820,  0.5106,  ..., -0.4740, -1.2570,  1.8308],\n",
       "           [-0.8365,  0.5188, -2.4893,  ..., -0.6288, -0.5259, -0.9685],\n",
       "           ...,\n",
       "           [-0.7418, -1.9129,  2.4009,  ..., -0.4762,  0.9375,  0.3237],\n",
       "           [-1.5323,  1.0716,  0.4702,  ...,  0.0510, -0.3231,  0.8547],\n",
       "           [-1.0641, -1.8975,  1.4758,  ..., -0.2257,  0.3994, -0.1101]]]])}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch['images'].shape\n",
      " torch.Size([2, 3, 336, 336])\n"
     ]
    }
   ],
   "source": [
    "# 画像のshapeを出力\n",
    "if isinstance(batch['images'], torch.Tensor):\n",
    "    print(\"batch['images'].shape\\n\", batch['images'].shape)\n",
    "else:\n",
    "    print(\"batch['images'] is a list, first shape:\", images[0].shape if images else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_collator とは、複数のサンプルをまとめてデータをバッチ化するもの、という意味です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "id": "K2Th7s50xen5"
   },
   "outputs": [],
   "source": [
    "# Trainer > def _get_dataloader > dataloader_params = {...\"collate_fn\": data_collator,...}\n",
    "# self.accelerator.prepare(DataLoader(dataset, **dataloader_params)) で呼ばれる\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        print(\"current file path\", \"llava/train/train.py\")\n",
    "        print(\"def DataCollatorForSupervisedDataset.__call__(self, instances)\")\n",
    "        print(\"instances\\n\", instances)\n",
    "        #  [(torch.Size([24]), torch.Size([24]), torch.Size([3, 336, 336]))]\n",
    "        print(\"shape of each instance's input_ids and labels, and images(if any):\", [(x['input_ids'].shape, x['labels'].shape, x.get('image', None).shape if 'image' in x else None) for x in instances])\n",
    "        # データローダーが None を返すことがあるので、Noneのサンプルを除外。\n",
    "        instances = [x for x in instances if x is not None]\n",
    "        # input_idsとlabelsのそれぞれについてリストを作成。タプルをつくる。\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances]\n",
    "                                  for key in (\"input_ids\", \"labels\"))\n",
    "        # input_idsはtokenizerのpad_token_id(0)でパディング\n",
    "        print(\"self.tokenizer.pad_token_id\\n\", self.tokenizer.pad_token_id)\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids,\n",
    "            batch_first=True,\n",
    "            padding_value=self.tokenizer.pad_token_id)\n",
    "        # labelsはIGNORE_INDEX(-100)でパディング\n",
    "        print(\"IGNORE_INDEX\\n\", IGNORE_INDEX)\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels,\n",
    "                                                 batch_first=True,\n",
    "                                                 padding_value=IGNORE_INDEX)\n",
    "        input_ids = input_ids[:, :self.tokenizer.model_max_length]\n",
    "        print(\"input_ids.shape (after pad_sequence and truncate)\\n\", input_ids.shape)\n",
    "        print(\"input_ids (after pad_sequence and truncate)\\n\", input_ids)\n",
    "        labels = labels[:, :self.tokenizer.model_max_length]\n",
    "        print(\"labels.shape (after pad_sequence and truncate)\\n\", labels.shape)\n",
    "        print(\"labels (after pad_sequence and truncate)\\n\", labels)\n",
    "        # .ne() は \"not equal\" → pad_token_id(=0) じゃない部分を 1、pad 部分を 0 にする。モデルが pad 部分を読まないように制御するマスクです。\n",
    "        attention_mask=input_ids.ne(self.tokenizer.pad_token_id)\n",
    "        \n",
    "        batch = dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        if 'image' in instances[0]:\n",
    "            images = [instance['image'] for instance in instances]\n",
    "            if all(x is not None and x.shape == images[0].shape for x in images):\n",
    "                batch['images'] = torch.stack(images)\n",
    "            else:\n",
    "                batch['images'] = images\n",
    "            print(\"batch['images'].shape\\n\", batch['images'].shape)\n",
    "        \n",
    "        print(\"batch (return)\\n\", batch)\n",
    "        print(\"shape of each batch's input_ids and labels, and images(if any):\", [(batch['input_ids'].shape, batch['labels'].shape, batch.get('images', None).shape if 'images' in batch else None)])\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "id": "QjNj6B85wWA6"
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer,\n",
    "                                data_args) -> Dict:\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def make_supervised_data_module(tokenizer, data_args)\")\n",
    "    print(\"tokenizer\\n\", type(tokenizer))\n",
    "    print(\"data_args\\n\", data_args) #  DataArguments(data_path='/content/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/LLaVA/images', image_aspect_ratio='square')\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    train_dataset = LazySupervisedDataset(tokenizer=tokenizer,\n",
    "                                data_path=data_args.data_path,\n",
    "                                data_args=data_args)\n",
    "    print(\"train_dataset\\n\", train_dataset) # <llava.train.train.LazySupervisedDataset object at 0x7ed6341f4880>\n",
    "    print(\"len(train_dataset)\\n\", len(train_dataset)) # 1\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "    print(\"data_collator\\n\", data_collator) # DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))\n",
    "    data_module = dict(train_dataset=train_dataset,\n",
    "                  eval_dataset=None,\n",
    "                  data_collator=data_collator)\n",
    "    print(\"def make_supervised_data_module: result (return)\\n\", data_module) # {'train_dataset': <llava.train.train.LazySupervisedDataset object at 0x7ed6341f4880>, 'eval_dataset': None, 'data_collator': DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))}\n",
    "    return data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TlaRN8vG5p99",
    "outputId": "a52c09b2-84d7-4d0b-b5b1-1f94af461408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/train/train.py\n",
      "def make_supervised_data_module(tokenizer, data_args)\n",
      "tokenizer\n",
      " <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "data_args\n",
      " DataArguments(data_path='/workspaces/LLaVA/CC3M_2.json', lazy_preprocess=True, is_multimodal=True, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')\n",
      "current file path llava/train/train.py\n",
      "def LazySupervisedDataset.__init__(self, data_path, tokenizer, data_args)\n",
      "data_path\n",
      " /workspaces/LLaVA/CC3M_2.json\n",
      "tokenizer\n",
      " <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "data_args\n",
      " DataArguments(data_path='/workspaces/LLaVA/CC3M_2.json', lazy_preprocess=True, is_multimodal=True, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')\n",
      "list_data_dict [{'id': 'GCC_train_000196242', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Render a clear and concise summary of the photo.\\n<image>'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]}, {'id': 'GCC_train_000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': '<image>\\nShare a concise interpretation of the image provided.'}, {'from': 'gpt', 'value': 'water pollution in the city'}]}]\n",
      "current file path llava/train/train.py\n",
      "def rank0_print(*args)\n",
      "args\n",
      " ('Formatting inputs...Skip in lazy mode',)\n",
      "Formatting inputs...Skip in lazy mode\n",
      "self.tokenizer\n",
      " LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)\n",
      "self.list_data_dict\n",
      " [{'id': 'GCC_train_000196242', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Render a clear and concise summary of the photo.\\n<image>'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]}, {'id': 'GCC_train_000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': '<image>\\nShare a concise interpretation of the image provided.'}, {'from': 'gpt', 'value': 'water pollution in the city'}]}]\n",
      "self.data_args\n",
      " DataArguments(data_path='/workspaces/LLaVA/CC3M_2.json', lazy_preprocess=True, is_multimodal=True, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')\n",
      "train_dataset\n",
      " <__main__.LazySupervisedDataset object at 0x7f985100efb0>\n",
      "current file path llava/train/train.py\n",
      "def LazySupervisedDataset.__len__(self)\n",
      "len(train_dataset)\n",
      " 2\n",
      "data_collator\n",
      " DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))\n",
      "def make_supervised_data_module: result (return)\n",
      " {'train_dataset': <__main__.LazySupervisedDataset object at 0x7f985100efb0>, 'eval_dataset': None, 'data_collator': DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))}\n",
      "data_module\n",
      " {'train_dataset': <__main__.LazySupervisedDataset object at 0x7f985100efb0>, 'eval_dataset': None, 'data_collator': DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))}\n"
     ]
    }
   ],
   "source": [
    "data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n",
    "print(\"data_module\\n\", data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "LazySupervisedDataset.__getitem__ = __getitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer\n",
      " LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False)}, clean_up_tokenization_spaces=False)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    model_max_length=training_args.model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "\n",
    "print(\"tokenizer\\n\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/train/train.py\n",
      "def LazySupervisedDataset.__init__(self, data_path, tokenizer, data_args)\n",
      "data_path\n",
      " /workspaces/LLaVA/CC3M_2.json\n",
      "tokenizer\n",
      " <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "data_args\n",
      " DataArguments(data_path='/workspaces/LLaVA/CC3M_2.json', lazy_preprocess=True, is_multimodal=True, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')\n",
      "list_data_dict [{'id': 'GCC_train_000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': '<image>\\nShare a concise interpretation of the image provided.'}, {'from': 'gpt', 'value': 'water pollution in the city'}]}, {'id': 'GCC_train_000196242', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Render a clear and concise summary of the photo.\\n<image>'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]}]\n",
      "current file path llava/train/train.py\n",
      "def rank0_print(*args)\n",
      "args\n",
      " ('Formatting inputs...Skip in lazy mode',)\n",
      "Formatting inputs...Skip in lazy mode\n",
      "self.tokenizer\n",
      " LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False)}, clean_up_tokenization_spaces=False)\n",
      "self.list_data_dict\n",
      " [{'id': 'GCC_train_000406392', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': '<image>\\nShare a concise interpretation of the image provided.'}, {'from': 'gpt', 'value': 'water pollution in the city'}]}, {'id': 'GCC_train_000196242', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Render a clear and concise summary of the photo.\\n<image>'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]}]\n",
      "self.data_args\n",
      " DataArguments(data_path='/workspaces/LLaVA/CC3M_2.json', lazy_preprocess=True, is_multimodal=True, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')\n"
     ]
    }
   ],
   "source": [
    "train_dataset = LazySupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path, data_args=data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/train/train.py\n",
      "def LazySupervisedDataset.__getitem__(self, i)\n",
      "i\n",
      " 1\n",
      "sources\n",
      " {'id': 'GCC_train_000196242', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Render a clear and concise summary of the photo.\\n<image>'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]}\n",
      "【COND】 isinstance(i, int): True\n",
      "【ENTER】if isinstance(i, int):\n",
      "sources (after)\n",
      " [{'id': 'GCC_train_000196242', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Render a clear and concise summary of the photo.\\n<image>'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]}]\n",
      "【EXIT】if isinstance(i, int):\n",
      "【COND】 'image' in sources[0]: True\n",
      "【ENTER】if 'image' in sources[0]:\n",
      "image_file\n",
      " GCC_train_000406392.jpg\n",
      "image_folder\n",
      " /workspaces/LLaVA/images/\n",
      "processor\n",
      " CLIPImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 336,\n",
      "    \"width\": 336\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 336\n",
      "  }\n",
      "}\n",
      "\n",
      "image_path\n",
      " /workspaces/LLaVA/images/GCC_train_000406392.jpg\n",
      "Trying to open image...\n",
      "Image opened successfully.\n",
      "【COND】 self.data_args.image_aspect_ratio square\n",
      "【ENTER】else (self.data_args.image_aspect_ratio != 'pad')\n",
      "image (before)\n",
      " <PIL.Image.Image image mode=RGB size=224x224 at 0x7FEDA69AC2E0>\n",
      "image (after processor.preprocess)\n",
      " tensor([[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "         [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "         [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "         ...,\n",
      "         [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "        [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "         [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "         [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "         ...,\n",
      "         [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "        [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "         [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "         [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "         ...,\n",
      "         [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "         [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "         [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]])\n",
      "sources (before preprocess_multimodal)\n",
      " [{'id': 'GCC_train_000196242', 'image': 'GCC_train_000406392.jpg', 'conversations': [{'from': 'human', 'value': 'Render a clear and concise summary of the photo.\\n<image>'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]}]\n",
      "current file path llava/train/train.py\n",
      "def preprocess_multimodal(sources, data_args)\n",
      "sources\n",
      " [[{'from': 'human', 'value': 'Render a clear and concise summary of the photo.\\n<image>'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]]\n",
      "data_args\n",
      " DataArguments(data_path='/workspaces/LLaVA/CC3M_2.json', lazy_preprocess=True, is_multimodal=True, image_folder='/workspaces/LLaVA/images/', image_aspect_ratio='square')\n",
      "is_multimodal\n",
      " True\n",
      "source current loop\n",
      " [{'from': 'human', 'value': 'Render a clear and concise summary of the photo.\\n<image>'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]\n",
      "sentence current loop\n",
      " {'from': 'human', 'value': 'Render a clear and concise summary of the photo.\\n<image>'}\n",
      "【COND】 if DEFAULT_IMAGE_TOKEN in sentence['value']: True\n",
      "sentence['value']\n",
      " Render a clear and concise summary of the photo.\n",
      "<image>\n",
      "DEFAULT_IMAGE_TOKEN\n",
      " <image>\n",
      "【ENTER】if DEFAULT_IMAGE_TOKEN in sentence['value']:\n",
      "sentence current loop\n",
      " {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}\n",
      "【COND】 if DEFAULT_IMAGE_TOKEN in sentence['value']: False\n",
      "sentence['value']\n",
      " illustration of a summer holiday in bright colors .\n",
      "DEFAULT_IMAGE_TOKEN\n",
      " <image>\n",
      "sources (final return)\n",
      " [[{'from': 'human', 'value': '<image>\\nRender a clear and concise summary of the photo.'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]]\n",
      "sources (after preprocess_multimodal)\n",
      " [[{'from': 'human', 'value': '<image>\\nRender a clear and concise summary of the photo.'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]]\n",
      "Calling preprocess...\n",
      "current file path llava/train/train.py\n",
      "def preprocess(sources, tokenizer, has_image=False)\n",
      "sources\n",
      " [[{'from': 'human', 'value': '<image>\\nRender a clear and concise summary of the photo.'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]]\n",
      "tokenizer\n",
      " <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "has_image\n",
      " True\n",
      "current file path llava/train/train.py\n",
      "def preprocess_plain(sources, tokenizer)\n",
      "sources\n",
      " [[{'from': 'human', 'value': '<image>\\nRender a clear and concise summary of the photo.'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]]\n",
      "tokenizer\n",
      " <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "conversations initial\n",
      " []\n",
      "source current loop\n",
      " [{'from': 'human', 'value': '<image>\\nRender a clear and concise summary of the photo.'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]\n",
      "conversation current loop\n",
      " <image>illustration of a summer holiday in bright colors .\n",
      "\n",
      "conversations (final)\n",
      " ['<image>illustration of a summer holiday in bright colors .\\n']\n",
      "current file path llava/mm_utils.py\n",
      "def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None)\n",
      "prompt\n",
      " <image>illustration of a summer holiday in bright colors .\n",
      "\n",
      "tokenizer\n",
      " LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False)}, clean_up_tokenization_spaces=False)\n",
      "image_token_index\n",
      " -200\n",
      "return_tensors\n",
      " pt\n",
      "input_ids\n",
      " [tensor([    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
      "        11785, 11955,   869,    13])]\n",
      "input_ids[0].shape\n",
      " torch.Size([14])\n",
      "targets\n",
      " [tensor([    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
      "        11785, 11955,   869,    13])]\n",
      "targets[0].shape\n",
      " torch.Size([14])\n",
      "sources\n",
      " [[{'from': 'human', 'value': '<image>'}, {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}]]\n",
      "current file path llava/mm_utils.py\n",
      "def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None)\n",
      "prompt\n",
      " <image>\n",
      "tokenizer\n",
      " LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False)}, clean_up_tokenization_spaces=False)\n",
      "image_token_index\n",
      " -200\n",
      "return_tensors\n",
      " None\n",
      "input_ids (return)\n",
      " [1, -200]\n",
      "input_ids (return)\n",
      " [tensor([    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
      "        11785, 11955,   869,    13])]\n",
      "targets (return)\n",
      " [tensor([ -100,  -100,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
      "        11785, 11955,   869,    13])]\n",
      "data_dict (after preprocess)\n",
      " {'input_ids': [tensor([    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
      "        11785, 11955,   869,    13])], 'labels': [tensor([ -100,  -100,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
      "        11785, 11955,   869,    13])]}\n",
      "【COND】 isinstance(i, int): True\n",
      "sample_data_dict\n",
      " {'input_ids': tensor([    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
      "        11785, 11955,   869,    13]), 'labels': tensor([ -100,  -100,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
      "        11785, 11955,   869,    13]), 'image': tensor([[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "         [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "         [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "         ...,\n",
      "         [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "        [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "         [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "         [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "         ...,\n",
      "         [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "        [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "         [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "         [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "         ...,\n",
      "         [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "         [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "         [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]])}\n"
     ]
    }
   ],
   "source": [
    "sample_data_dict = train_dataset.__getitem__(1)\n",
    "print(\"sample_data_dict\\n\", sample_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file path llava/train/train.py\n",
      "def DataCollatorForSupervisedDataset.__call__(self, instances)\n",
      "instances\n",
      " [{'input_ids': tensor([    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
      "        11785, 11955,   869,    13]), 'labels': tensor([ -100,  -100,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
      "        11785, 11955,   869,    13]), 'image': tensor([[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "         [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "         [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "         ...,\n",
      "         [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "         [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "        [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "         [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "         [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "         ...,\n",
      "         [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "         [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "        [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "         [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "         [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "         ...,\n",
      "         [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "         [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "         [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]])}]\n",
      "shape of each instance's input_ids and labels, and images(if any): [(torch.Size([14]), torch.Size([14]), torch.Size([3, 336, 336]))]\n",
      "self.tokenizer.pad_token_id\n",
      " 2\n",
      "IGNORE_INDEX\n",
      " -100\n",
      "input_ids.shape (after pad_sequence and truncate)\n",
      " torch.Size([1, 14])\n",
      "input_ids (after pad_sequence and truncate)\n",
      " tensor([[    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
      "         11785, 11955,   869,    13]])\n",
      "labels.shape (after pad_sequence and truncate)\n",
      " torch.Size([1, 14])\n",
      "labels (after pad_sequence and truncate)\n",
      " tensor([[ -100,  -100,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
      "         11785, 11955,   869,    13]])\n",
      "batch['images'].shape\n",
      " torch.Size([1, 3, 336, 336])\n",
      "batch (return)\n",
      " {'input_ids': tensor([[    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
      "         11785, 11955,   869,    13]]), 'labels': tensor([[ -100,  -100,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
      "         11785, 11955,   869,    13]]), 'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True]]), 'images': tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "          ...,\n",
      "          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "          ...,\n",
      "          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "          ...,\n",
      "          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])}\n",
      "shape of each batch's input_ids and labels, and images(if any): [(torch.Size([1, 14]), torch.Size([1, 14]), torch.Size([1, 3, 336, 336]))]\n",
      "batch\n",
      " {'input_ids': tensor([[    1,  -200,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
      "         11785, 11955,   869,    13]]), 'labels': tensor([[ -100,  -100,  8632,   362,   310,   263, 11801,  8753, 22394,   297,\n",
      "         11785, 11955,   869,    13]]), 'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True]]), 'images': tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "          ...,\n",
      "          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "          ...,\n",
      "          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "          ...,\n",
      "          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])}\n"
     ]
    }
   ],
   "source": [
    "instances = [sample_data_dict]\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "batch = data_collator(instances)\n",
    "print(\"batch\\n\", batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape\n",
      " torch.Size([1, 3, 336, 336])\n",
      "images\n",
      " tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
      "          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
      "          ...,\n",
      "          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
      "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
      "\n",
      "         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
      "          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
      "          ...,\n",
      "          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
      "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
      "\n",
      "         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
      "          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
      "          ...,\n",
      "          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
      "          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])\n"
     ]
    }
   ],
   "source": [
    "images = batch['images']\n",
    "print(\"images shape\\n\", images.shape) # torch.Size([1, 3, 336, 336])\n",
    "print(\"images\\n\", images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images(self, images):\n",
    "    print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "    print(\"def LlavaMetaForCausalLM(ABC).encode_images(self, images)\")\n",
    "    print(\"images\\n\", images)\n",
    "    image_features = self.get_model().get_vision_tower()(images)\n",
    "    image_features = self.get_model().mm_projector(image_features)\n",
    "    print(\"image_features (return) shape\\n\", image_features.shape)\n",
    "    print(\"image_features (return)\\n\", image_features)\n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LlavaBasicForCausalLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mLlavaBasicForCausalLM\u001b[49m\u001b[38;5;241m.\u001b[39mencode_images \u001b[38;5;241m=\u001b[39m encode_images\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LlavaBasicForCausalLM' is not defined"
     ]
    }
   ],
   "source": [
    "LlavaBasicForCausalLM.encode_images = encode_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_forward_outs から、指定した層の特徴量 (B, 577, 1024) を取り出したのち、パッチ特徴量 (B, 576, 1024) のみを返す。\n",
    "def feature_select(self, image_forward_outs):\n",
    "\n",
    "    print(\"current file path\", \"llava/llava/model/multimodal_encoder/clip_encoder.py\")\n",
    "    print(\"def CLIPVisionTower.feature_select(self, image_forward_outs)\")\n",
    "    print(\"image_forward_outs\\n\", image_forward_outs) # 24層のtuple\n",
    "    image_features = image_forward_outs.hidden_states[self.select_layer]\n",
    "    print(\"image_features (after select_layer)\\n\", type(image_features))\n",
    "    if hasattr(image_features, 'shape'):\n",
    "        print(\"image_features.shape\\n\", image_features.shape) # torch.Size([1, 577, 1024])\n",
    "    print(f\"【COND】 select_feature={self.select_feature}\") # patch\n",
    "    if self.select_feature == 'patch':\n",
    "        print(\"【ENTER】if self.select_feature == 'patch':\")\n",
    "        print(\"original image_features\\n\", image_features)\n",
    "        \"\"\"\n",
    "        tensor([[[ 0.2236,  0.2432, -0.5938,  ...,  0.4863, -0.5273, -0.2041],\n",
    "                [-0.0469, -0.1836, -0.0273,  ...,  0.3535,  0.3750,  0.3047],\n",
    "                [-0.2598,  1.1484,  0.4844,  ...,  0.4961, -0.1719, -0.5117],\n",
    "                ...,\n",
    "                [ 1.7188,  0.9688,  0.8828,  ..., -0.2441, -0.8672,  1.3047],\n",
    "                [ 0.7891, -0.3984,  0.6797,  ..., -0.3594, -0.9922,  0.3164],\n",
    "                [ 1.5000,  0.6250,  0.3672,  ..., -0.5469, -0.4902,  0.9766]]],\n",
    "            device='cuda:0', dtype=torch.bfloat16)\n",
    "        \"\"\"\n",
    "        image_features = image_features[:, 1:]\n",
    "        print(\"after process\\n\", image_features)\n",
    "        \"\"\"\n",
    "        tensor([[[-0.0469, -0.1836, -0.0273,  ...,  0.3535,  0.3750,  0.3047],\n",
    "                [-0.2598,  1.1484,  0.4844,  ...,  0.4961, -0.1719, -0.5117],\n",
    "                [ 1.0625, -0.0635, -0.3730,  ...,  0.0220,  0.0820,  0.4805],\n",
    "                ...,\n",
    "                [ 1.7188,  0.9688,  0.8828,  ..., -0.2441, -0.8672,  1.3047],\n",
    "                [ 0.7891, -0.3984,  0.6797,  ..., -0.3594, -0.9922,  0.3164],\n",
    "                [ 1.5000,  0.6250,  0.3672,  ..., -0.5469, -0.4902,  0.9766]]],\n",
    "            device='cuda:0', dtype=torch.bfloat16)\n",
    "        \"\"\"\n",
    "        print(\"【EXIT】if self.select_feature == 'patch':\")\n",
    "    elif self.select_feature == 'cls_patch':\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "    print(\"selected image_feature shape\\n\", image_features.shape) \n",
    "    print(\"image_features (return)\\n\", image_features)\n",
    "    \"\"\"\n",
    "    image_features (return)\n",
    "    tensor([[[-0.0469, -0.1836, -0.0273,  ...,  0.3535,  0.3750,  0.3047],\n",
    "            [-0.2598,  1.1484,  0.4844,  ...,  0.4961, -0.1719, -0.5117],\n",
    "            [ 1.0625, -0.0635, -0.3730,  ...,  0.0220,  0.0820,  0.4805],\n",
    "            ...,\n",
    "            [ 1.7188,  0.9688,  0.8828,  ..., -0.2441, -0.8672,  1.3047],\n",
    "            [ 0.7891, -0.3984,  0.6797,  ..., -0.3594, -0.9922,  0.3164],\n",
    "            [ 1.5000,  0.6250,  0.3672,  ..., -0.5469, -0.4902,  0.9766]]],\n",
    "        device='cuda:0', dtype=torch.bfloat16)\n",
    "    \"\"\"\n",
    "    if hasattr(image_features, 'shape'):\n",
    "        print(\"image_features.shape\\n\", image_features.shape) # torch.Size([1, 576, 1024])\n",
    "    return image_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIPVisionTower.feature_select = feature_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() \n",
    "def forward(self, images):\n",
    "\n",
    "    print(\"current file path\", \"llava/llava/model/multimodal_encoder/clip_encoder.py\")\n",
    "    print(\"def CLIPVisionTower.forward(self, images)\")\n",
    "    print(\"images shape\\n\", images.shape) # torch.Size([1, 3, 336, 336])\n",
    "    print(\"images\\n\", images)\n",
    "    \n",
    "    if hasattr(images, 'shape'):\n",
    "        print(\"images.shape\\n\", images.shape) # torch.Size([1, 3, 336, 336])\n",
    "    print(f\"【COND】 type_images_is_list={type(images) is list}\") # False\n",
    "    if type(images) is list:\n",
    "        pass\n",
    "    else:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】else (type(images) is not list):\")\n",
    "        print(\"original images\\n\", images)\n",
    "        image_forward_outs = self.vision_tower(images.to(device=self.vision_tower.device, dtype=self.vision_tower.dtype), output_hidden_states=True)\n",
    "        print(\"after process image_forward_outs\\n\", type(image_forward_outs)) # 24層のtuple\n",
    "        image_features = self.feature_select(image_forward_outs).to(images.dtype)\n",
    "        print(\"after process image_features\\n\", type(image_features)) # <class 'torch.Tensor'>\n",
    "        print(\"【EXIT】else (type(images) is not list):\")\n",
    "\n",
    "    print(\"image_features (return)\\n\", image_features)\n",
    "    \"\"\"\n",
    "    image_features (return)\n",
    "    tensor([[[-0.0469, -0.1836, -0.0273,  ...,  0.3535,  0.3750,  0.3047],\n",
    "            [-0.2598,  1.1484,  0.4844,  ...,  0.4961, -0.1719, -0.5117],\n",
    "            [ 1.0625, -0.0635, -0.3730,  ...,  0.0220,  0.0820,  0.4805],\n",
    "            ...,\n",
    "            [ 1.7188,  0.9688,  0.8828,  ..., -0.2441, -0.8672,  1.3047],\n",
    "            [ 0.7891, -0.3984,  0.6797,  ..., -0.3594, -0.9922,  0.3164],\n",
    "            [ 1.5000,  0.6250,  0.3672,  ..., -0.5469, -0.4902,  0.9766]]],\n",
    "        device='cuda:0', dtype=torch.bfloat16)\n",
    "    \"\"\"\n",
    "    if hasattr(image_features, 'shape'):\n",
    "        print(\"image_features.shape\\n\", image_features.shape) # \n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIPVisionTower.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = model.encode_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs_labels_for_multimodal(\n",
    "    self, input_ids, position_ids, attention_mask, past_key_values, labels,\n",
    "    images, image_sizes=None\n",
    "):\n",
    "    print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "    \"\"\"\n",
    "    llava/llava/model/language_model/llava_llama.py\n",
    "    \"\"\"\n",
    "    print(\"def LlavaMetaForCausalLM(ABC).prepare_inputs_labels_for_multimodal(self, input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes=None)\")  # not found\n",
    "    print(\"input_ids\\n\", input_ids)\n",
    "    \"\"\"\n",
    "    tensor([[    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "             10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "               411,  2654, 11315,    13]])\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"position_ids\\n\", position_ids)  # None\n",
    "    print(\"attention_mask\\n\", attention_mask)\n",
    "    \"\"\"\n",
    "    tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True]])\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"past_key_values\\n\", past_key_values)  # None\n",
    "    print(\"labels\\n\", labels)\n",
    "    \"\"\"\n",
    "    tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "             10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "               411,  2654, 11315,    13]])\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"images\\n\", images)\n",
    "    \"\"\"\n",
    "    tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
    "              [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
    "              [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
    "              ...,\n",
    "              [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
    "              [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
    "              [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
    "    \n",
    "             [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
    "              [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
    "              [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
    "              ...,\n",
    "              [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
    "              [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
    "              [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
    "    \n",
    "             [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
    "              [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
    "              [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
    "              ...,\n",
    "              [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
    "              [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
    "              [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"image_sizes\\n\", image_sizes)  # None\n",
    "    vision_tower = self.get_vision_tower()\n",
    "    print(\"vision_tower\\n\", vision_tower)\n",
    "    \"\"\"\n",
    "    CLIPVisionTower(\n",
    "      (vision_tower): CLIPVisionModel(\n",
    "        (vision_model): CLIPVisionTransformer(\n",
    "          (embeddings): CLIPVisionEmbeddings(\n",
    "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "            (position_embedding): Embedding(577, 1024)\n",
    "          )\n",
    "          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "          (encoder): CLIPEncoder(\n",
    "            (layers): ModuleList(\n",
    "              (0-23): 24 x CLIPEncoderLayer(\n",
    "                (self_attn): CLIPAttention(\n",
    "                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                (mlp): CLIPMLP(\n",
    "                  (activation_fn): QuickGELUActivation()\n",
    "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "              )\n",
    "            )\n",
    "          )\n",
    "          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"【COND】 vision_tower_is_None={vision_tower is None} images_is_None={images is None} input_ids_shape_1_eq_1={input_ids.shape[1] == 1}\")\n",
    "    if vision_tower is None or images is None or input_ids.shape[1] == 1:\n",
    "        pass\n",
    "\n",
    "    print(\"【COND】type(images)\\n\", type(images))  # <class 'torch.Tensor'>\n",
    "    print(\"【COND】images.ndim\\n\", images.ndim)  # 4\n",
    "    if type(images) is list or images.ndim == 5:\n",
    "        pass\n",
    "    else:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】else of if type(images) is list or images.ndim == 5:\")\n",
    "        image_features = self.encode_images(images)\n",
    "        print(\"image_features after encode_images shape \\n\", image_features.shape)  # torch.Size([1, 576, 2048])\n",
    "        print(\"image_features after encode_images\\n\", image_features)\n",
    "        \"\"\"\n",
    "        tensor([[[-0.1943,  0.1157, -0.0747,  ...,  0.0027, -0.1691, -0.3439],\n",
    "                 [ 0.0437,  0.1717, -0.0998,  ...,  0.0930, -0.1386, -0.0731],\n",
    "                 [-0.0505,  0.1592, -0.0982,  ...,  0.0866, -0.1123, -0.2177],\n",
    "                 ...,\n",
    "                 [-0.0182,  0.0850, -0.0556,  ...,  0.0622, -0.1969,  0.0129],\n",
    "                 [-0.0651,  0.0586, -0.1218,  ..., -0.0614, -0.1158, -0.0104],\n",
    "                 [ 0.0863,  0.0081, -0.1651,  ..., -0.2040, -0.0455,  0.0618]]],\n",
    "               grad_fn=<ViewBackward0>)\n",
    "        \"\"\"\n",
    "        print(\"【EXIT】else of if type(images) is list or images.ndim == 5:\")\n",
    "\n",
    "    # TODO: image start / end is not implemented here to support pretraining.\n",
    "    if getattr(self.config, 'tune_mm_mlp_adapter', False) and getattr(self.config, 'mm_use_im_start_end', False):\n",
    "        print(\"【ENTER】if getattr(self.config, 'tune_mm_mlp_adapter', False) and getattr(self.config, 'mm_use_im_start_end', False):\")  # not found\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Let's just add dummy tensors if they do not exist,\n",
    "    # it is a headache to deal with None all the time.\n",
    "    # But it is not ideal, and if you have a better idea,\n",
    "    # please open an issue / submit a PR, thanks.\n",
    "\n",
    "    print(\"labels before\\n\", labels)\n",
    "    \"\"\"\n",
    "    tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "             10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "               411,  2654, 11315,    13]])\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"position_ids before\\n\", position_ids)  # None\n",
    "\n",
    "    print(\"attention_mask before\\n\", attention_mask)\n",
    "    \"\"\"\n",
    "    tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True]])\n",
    "    \"\"\"\n",
    "\n",
    "    _labels = labels\n",
    "    _position_ids = position_ids\n",
    "    _attention_mask = attention_mask\n",
    "    if attention_mask is None:\n",
    "        pass\n",
    "    else:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】else of if attention_mask is None:\")\n",
    "        attention_mask = attention_mask.bool()\n",
    " \n",
    "        print(\"attention_mask（after）shape \\n\", attention_mask.shape)  # torch.Size([1, 24])\n",
    "        print(\"attention_mask (after)\\n\", attention_mask)\n",
    "        \"\"\"\n",
    "        tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True]])\n",
    "        \"\"\"\n",
    "        print(\"【EXIT】else of if attention_mask is None:\")\n",
    "    if position_ids is None:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】if position_ids is None:\")\n",
    "        position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "        print(\"position_ids (after) shape \\n\", position_ids.shape)  # torch.Size([24])\n",
    "        print(\"position_ids (after)\\n\", position_ids)\n",
    "        \"\"\"\n",
    "        tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
    "                18, 19, 20, 21, 22, 23])\n",
    "        \"\"\"\n",
    "        print(\"【EXIT】if position_ids is None:\")\n",
    "    print(f\"【COND】 labels_is_None={labels is None}\")\n",
    "    if labels is None:\n",
    "        pass\n",
    "\n",
    "    # remove the padding using attention_mask -- FIXME\n",
    "    _input_ids = input_ids\n",
    "    input_ids = [cur_input_ids[cur_attention_mask] for cur_input_ids, cur_attention_mask in zip(input_ids, attention_mask)]\n",
    "    labels = [cur_labels[cur_attention_mask] for cur_labels, cur_attention_mask in zip(labels, attention_mask)]\n",
    "    print(\"input_ids after removing padding\\n\", input_ids)\n",
    "    \"\"\"\n",
    "    [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "            10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "              411,  2654, 11315,    13])]\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"labels after removing padding\\n\", labels)\n",
    "    \"\"\"\n",
    "    [tensor([ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "            10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "              411,  2654, 11315,    13])]\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    new_input_embeds = []\n",
    "    new_labels = []\n",
    "    cur_image_idx = 0\n",
    "    for batch_idx, cur_input_ids in enumerate(input_ids):\n",
    "        print(\"cur_input_ids shape\\n\", cur_input_ids.shape)   # torch.Size([24])\n",
    "        print(\"cur_input_ids\\n\", cur_input_ids)\n",
    "        \"\"\"\n",
    "        tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "                10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "                  411,  2654, 11315,    13])\n",
    "        \"\"\"\n",
    "        num_images = (cur_input_ids == IMAGE_TOKEN_INDEX).sum()\n",
    "        print(\"【COND】num_images:\", num_images)  # tensor(1)\n",
    "        if num_images == 0:\n",
    "            print(\"【ENTER】if num_images == 0:\")\n",
    "            cur_image_features = image_features[cur_image_idx]\n",
    "            cur_input_embeds_1 = self.get_model().embed_tokens(cur_input_ids)\n",
    "            cur_input_embeds = torch.cat([cur_input_embeds_1, cur_image_features[0:0]], dim=0)\n",
    "            new_input_embeds.append(cur_input_embeds)\n",
    "            new_labels.append(labels[batch_idx])\n",
    "            cur_image_idx += 1\n",
    "            print(\"【EXIT】if num_images == 0:\")\n",
    "            continue\n",
    "\n",
    "        image_token_indices = [-1] + torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)[0].tolist() + [cur_input_ids.shape[0]]\n",
    "        print(\"image_token_indices\\n\", image_token_indices)  # [-1, 1, 24]\n",
    "        print(\"len image_token_indices\", len(image_token_indices))   # 3\n",
    "        cur_input_ids_noim = []\n",
    "        cur_labels = labels[batch_idx]\n",
    "        print(\"cur_labels\\n\", cur_labels)\n",
    "        \"\"\"\n",
    "        tensor([ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "                10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "                  411,  2654, 11315,    13])\n",
    "        \"\"\"\n",
    "        cur_labels_noim = []\n",
    "        for i in range(len(image_token_indices) - 1): # 2回ループ。1回目 START から IMAGE_TOKEN_INDEXの手前まで、2回目はIMAGE_TOKEN_INDEX より先から 最後まで\n",
    "            cur_input_ids_noim.append(cur_input_ids[image_token_indices[i]+1:image_token_indices[i+1]])\n",
    "            cur_labels_noim.append(cur_labels[image_token_indices[i]+1:image_token_indices[i+1]])\n",
    "        print(\"cur_input_ids_noim (after)\\n\", cur_input_ids_noim)\n",
    "        \"\"\"\n",
    "        [tensor([1]), tensor([  278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596,\n",
    "                23425,   278,  3700,   322,  6567,   310,   263,  6114,   411,  2654,\n",
    "                11315,    13])]\n",
    "        \"\"\"\n",
    "        print(\"cur_labels_noim (after) \\n\", cur_labels_noim)\n",
    "        \"\"\"\n",
    "        [tensor([-100]), tensor([  278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596,\n",
    "                23425,   278,  3700,   322,  6567,   310,   263,  6114,   411,  2654,\n",
    "                11315,    13])]\n",
    "        \"\"\"\n",
    "        split_sizes = [x.shape[0] for x in cur_labels_noim]\n",
    "        print(\"split_sizes\\n\", split_sizes)  # [1, 22]\n",
    "        cur_input_embeds = self.get_model().embed_tokens(torch.cat(cur_input_ids_noim))\n",
    "        print(\"cur_input_embeds shape\\n\", cur_input_embeds.shape)  # torch.Size([23, 2048])\n",
    "        print(\"cur_input_embeds\\n\", cur_input_embeds)\n",
    "        \"\"\"\n",
    "        tensor([[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
    "                 -6.5231e-04, -4.9973e-04],\n",
    "                [ 7.0801e-03,  1.0452e-03,  6.0425e-03,  ...,  3.9673e-03,\n",
    "                  1.2817e-03, -1.1215e-03],\n",
    "                [-2.2949e-02, -2.6226e-05,  6.8359e-03,  ..., -2.4658e-02,\n",
    "                 -9.4604e-03,  1.5869e-02],\n",
    "                ...,\n",
    "                [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
    "                 -8.3618e-03, -9.4604e-03],\n",
    "                [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
    "                 -2.5177e-03, -8.0566e-03],\n",
    "                [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
    "                 -7.3242e-04,  2.7924e-03]], requires_grad=True)\n",
    "        \"\"\"\n",
    "        cur_input_embeds_no_im = torch.split(cur_input_embeds, split_sizes, dim=0)\n",
    "        print(\"cur_input_embeds_no_im\\n\", cur_input_embeds_no_im)\n",
    "        \"\"\"\n",
    "        (tensor([[-0.0011,  0.0019, -0.0017,  ...,  0.0002, -0.0007, -0.0005]],\n",
    "               grad_fn=<SplitWithSizesBackward0>), tensor([[ 7.0801e-03,  1.0452e-03,  6.0425e-03,  ...,  3.9673e-03,\n",
    "                  1.2817e-03, -1.1215e-03],\n",
    "                [-2.2949e-02, -2.6226e-05,  6.8359e-03,  ..., -2.4658e-02,\n",
    "                 -9.4604e-03,  1.5869e-02],\n",
    "                [-2.3499e-03,  1.4893e-02, -2.0447e-03,  ..., -8.6060e-03,\n",
    "                  2.3193e-03,  3.0670e-03],\n",
    "                ...,\n",
    "                [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
    "                 -8.3618e-03, -9.4604e-03],\n",
    "                [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
    "                 -2.5177e-03, -8.0566e-03],\n",
    "                [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
    "                 -7.3242e-04,  2.7924e-03]], grad_fn=<SplitWithSizesBackward0>))\n",
    "        \"\"\"\n",
    "        cur_new_input_embeds = []\n",
    "        cur_new_labels = []\n",
    "\n",
    "        for i in range(num_images + 1):\n",
    "            cur_new_input_embeds.append(cur_input_embeds_no_im[i])\n",
    "            cur_new_labels.append(cur_labels_noim[i])\n",
    "            print(f\"【COND】 i={i} num_images={num_images}\")\n",
    "            if i < num_images:\n",
    "                print(\"【ENTER】if i < num_images:\")\n",
    "                cur_image_features = image_features[cur_image_idx]\n",
    "                cur_image_idx += 1\n",
    "                cur_new_input_embeds.append(cur_image_features)\n",
    "                cur_new_labels.append(torch.full((cur_image_features.shape[0],), IGNORE_INDEX, device=cur_labels.device, dtype=cur_labels.dtype))\n",
    "                print(\"【EXIT】if i < num_images:\")\n",
    "\n",
    "        print(\"cur_new_input_embeds (before cat) shape\\n\", [x.shape for x in cur_new_input_embeds])\n",
    "        \"\"\"\n",
    "        [torch.Size([1, 2048]), torch.Size([576, 2048]), torch.Size([22, 2048])]\n",
    "        \"\"\"\n",
    "        print(\"cur_new_input_embeds (before cat)\\n\", cur_new_input_embeds)\n",
    "        \"\"\"\n",
    "        [tensor([[-0.0011,  0.0019, -0.0017,  ...,  0.0002, -0.0007, -0.0005]],\n",
    "               grad_fn=<SplitWithSizesBackward0>), tensor([[-0.1943,  0.1157, -0.0747,  ...,  0.0027, -0.1691, -0.3439],\n",
    "                [ 0.0437,  0.1717, -0.0998,  ...,  0.0930, -0.1386, -0.0731],\n",
    "                [-0.0505,  0.1592, -0.0982,  ...,  0.0866, -0.1123, -0.2177],\n",
    "                ...,\n",
    "                [-0.0182,  0.0850, -0.0556,  ...,  0.0622, -0.1969,  0.0129],\n",
    "                [-0.0651,  0.0586, -0.1218,  ..., -0.0614, -0.1158, -0.0104],\n",
    "                [ 0.0863,  0.0081, -0.1651,  ..., -0.2040, -0.0455,  0.0618]],\n",
    "               grad_fn=<SelectBackward0>), tensor([[ 7.0801e-03,  1.0452e-03,  6.0425e-03,  ...,  3.9673e-03,\n",
    "                  1.2817e-03, -1.1215e-03],\n",
    "                [-2.2949e-02, -2.6226e-05,  6.8359e-03,  ..., -2.4658e-02,\n",
    "                 -9.4604e-03,  1.5869e-02],\n",
    "                [-2.3499e-03,  1.4893e-02, -2.0447e-03,  ..., -8.6060e-03,\n",
    "                  2.3193e-03,  3.0670e-03],\n",
    "                ...,\n",
    "                [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
    "                 -8.3618e-03, -9.4604e-03],\n",
    "                [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
    "                 -2.5177e-03, -8.0566e-03],\n",
    "                [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
    "                 -7.3242e-04,  2.7924e-03]], grad_fn=<SplitWithSizesBackward0>)]\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"cur_new_labels (before cat) shape\\n\", [x.shape for x in cur_new_labels])\n",
    "        \"\"\"\n",
    "        [torch.Size([1]), torch.Size([576]), torch.Size([22])]\n",
    "        \"\"\"\n",
    "        print(\"cur_new_labels (before cat)\\n\", cur_new_labels)\n",
    "        \"\"\"\n",
    "        [tensor([-100]), tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "                -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]), tensor([  278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596,\n",
    "                23425,   278,  3700,   322,  6567,   310,   263,  6114,   411,  2654,\n",
    "                11315,    13])]\n",
    "        \"\"\"\n",
    "        cur_new_input_embeds = [x.to(self.device) for x in cur_new_input_embeds]\n",
    "\n",
    "        cur_new_input_embeds = torch.cat(cur_new_input_embeds)\n",
    "        cur_new_labels = torch.cat(cur_new_labels)\n",
    "\n",
    "        print(\"cur_new_input_embeds (after cat) shape\\n\", cur_new_input_embeds.shape)  # torch.Size([599, 2048])\n",
    "        print(\"cur_new_input_embeds (after cat)\\n\", cur_new_input_embeds)\n",
    "        \"\"\"\n",
    "        tensor([[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
    "                 -6.5231e-04, -4.9973e-04],\n",
    "                [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
    "                 -1.6907e-01, -3.4387e-01],\n",
    "                [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
    "                 -1.3859e-01, -7.3106e-02],\n",
    "                ...,\n",
    "                [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
    "                 -8.3618e-03, -9.4604e-03],\n",
    "                [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
    "                 -2.5177e-03, -8.0566e-03],\n",
    "                [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
    "                 -7.3242e-04,  2.7924e-03]], grad_fn=<CatBackward0>)\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"cur_new_labels (after cat) shape\\n\", cur_new_labels.shape)  # torch.Size([599])\n",
    "        print(\"cur_new_labels (after cat)\\n\", cur_new_labels)\n",
    "        \"\"\"\n",
    "        tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
    "                  297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
    "                  322,  6567,   310,   263,  6114,   411,  2654, 11315,    13])\n",
    "        \"\"\"\n",
    "\n",
    "        new_input_embeds.append(cur_new_input_embeds)\n",
    "        new_labels.append(cur_new_labels)\n",
    "        print(\"new_input_embeds (so far) shape\\n\", [x.shape for x in new_input_embeds])  # [torch.Size([599, 2048])]\n",
    "        print(\"new_input_embeds (so far)\\n\", new_input_embeds)\n",
    "        \"\"\"\n",
    "        [tensor([[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
    "                 -6.5231e-04, -4.9973e-04],\n",
    "                [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
    "                 -1.6907e-01, -3.4387e-01],\n",
    "                [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
    "                 -1.3859e-01, -7.3106e-02],\n",
    "                ...,\n",
    "                [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
    "                 -8.3618e-03, -9.4604e-03],\n",
    "                [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
    "                 -2.5177e-03, -8.0566e-03],\n",
    "                [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
    "                 -7.3242e-04,  2.7924e-03]], grad_fn=<CatBackward0>)]\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"new_labels (so far) shape\\n\", [x.shape for x in new_labels])  # [torch.Size([599])]\n",
    "        print(\"new_labels (so far)\\n\", new_labels)\n",
    "        \"\"\"\n",
    "        [tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                 -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
    "                  297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
    "                  322,  6567,   310,   263,  6114,   411,  2654, 11315,    13])]\n",
    "        \"\"\"\n",
    "\n",
    "    # Truncate sequences to max length as image embeddings can make the sequence longer\n",
    "    tokenizer_model_max_length = getattr(self.config, 'tokenizer_model_max_length', None)\n",
    "    print(f\"【COND】 tokenizer_model_max_length_is_not_None={tokenizer_model_max_length is not None}\")\n",
    "    if tokenizer_model_max_length is not None:\n",
    "        print(\"【ENTER】if tokenizer_model_max_length is not None:\")\n",
    "        new_input_embeds = [x[:tokenizer_model_max_length] for x in new_input_embeds]\n",
    "        new_labels = [x[:tokenizer_model_max_length] for x in new_labels]\n",
    "        print(\"【EXIT】if tokenizer_model_max_length is not None:\")\n",
    "\n",
    "    # Combine them\n",
    "    max_len = max(x.shape[0] for x in new_input_embeds)\n",
    "    print(\"max_len\\n\", max_len)  # 599\n",
    "    batch_size = len(new_input_embeds)\n",
    "    print(\"batch_size\\n\", batch_size)  # 1\n",
    "\n",
    "    new_input_embeds_padded = []\n",
    "    new_labels_padded = torch.full((batch_size, max_len), IGNORE_INDEX, dtype=new_labels[0].dtype, device=new_labels[0].device)\n",
    "    print(\"new_labels_padded (before) shape\\n\", new_labels_padded.shape)  # torch.Size([1, 599])\n",
    "    print(\"new_labels_padded (before)\\n\", new_labels_padded)\n",
    "    \"\"\"\n",
    "    tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "             -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]])\n",
    "    \"\"\"\n",
    "    attention_mask = torch.zeros((batch_size, max_len), dtype=attention_mask.dtype, device=attention_mask.device)\n",
    "    print(\"attention_mask (before) shape\\n\", attention_mask.shape)  # torch.Size([1, 599])\n",
    "    print(\"attention_mask (before)\\n\", attention_mask)\n",
    "    \"\"\"\n",
    "    tensor([[False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False, False,\n",
    "             False, False, False, False, False, False, False, False, False]])\n",
    "    \"\"\"\n",
    "    position_ids = torch.zeros((batch_size, max_len), dtype=position_ids.dtype, device=position_ids.device)\n",
    "    print(\"position_ids (before) shape\\n\", position_ids.shape)  # torch.Size([1, 599])\n",
    "    print(\"position_ids (before)\\n\", position_ids)\n",
    "    \"\"\"\n",
    "    tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "    \"\"\"\n",
    "\n",
    "    for i, (cur_new_embed, cur_new_labels) in enumerate(zip(new_input_embeds, new_labels)):\n",
    "        cur_len = cur_new_embed.shape[0]\n",
    "        print(f\"【COND】 padding_side={getattr(self.config, 'tokenizer_padding_side', 'right')} cur_len={cur_len} max_len={max_len}\")\n",
    "        if getattr(self.config, 'tokenizer_padding_side', 'right') == \"left\":\n",
    "            pass\n",
    "        else:\n",
    "            print(\"【ENTER】else (padding_side != 'left'):\")\n",
    "            #【ENTER】\n",
    "            new_input_embeds_padded.append(torch.cat((\n",
    "                cur_new_embed,\n",
    "                torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device)\n",
    "            ), dim=0))\n",
    "            if cur_len > 0:\n",
    "                # :cur_len に、代入\n",
    "                new_labels_padded[i, :cur_len] = cur_new_labels \n",
    "                attention_mask[i, :cur_len] = True\n",
    "                position_ids[i, :cur_len] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\n",
    "            print(\"new_input_embeds_padded (so far) shape\\n\", [x.shape for x in new_input_embeds_padded])  # [torch.Size([599, 2048])]\n",
    "            print(\"new_input_embeds_padded (so far)\\n\", new_input_embeds_padded)\n",
    "            \"\"\"\n",
    "            [tensor([[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
    "                     -6.5231e-04, -4.9973e-04],\n",
    "                    [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
    "                     -1.6907e-01, -3.4387e-01],\n",
    "                    [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
    "                     -1.3859e-01, -7.3106e-02],\n",
    "                    ...,\n",
    "                    [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
    "                     -8.3618e-03, -9.4604e-03],\n",
    "                    [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
    "                     -2.5177e-03, -8.0566e-03],\n",
    "                    [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
    "                     -7.3242e-04,  2.7924e-03]], grad_fn=<CatBackward0>)]\n",
    "            \"\"\"\n",
    "\n",
    "            print(\"new_labels_padded (so far) shape\\n\", new_labels_padded.shape)  # torch.Size([1, 599])\n",
    "            print(\"new_labels_padded (so far)\\n\", new_labels_padded)\n",
    "            \"\"\"\n",
    "            tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                      -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
    "                       297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
    "                       322,  6567,   310,   263,  6114,   411,  2654, 11315,    13]])\n",
    "            \"\"\"\n",
    "\n",
    "            print(\"attention_mask (so far) shape\\n\", attention_mask.shape)  # torch.Size([1, 599])\n",
    "            print(\"attention_mask (so far)\\n\", attention_mask)\n",
    "            \"\"\"\n",
    "            tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                     True, True, True, True, True, True, True, True, True, True, True]])\n",
    "            \"\"\"\n",
    "\n",
    "            print(\"position_ids (so far) shape\\n\", position_ids.shape)  # torch.Size([1, 599])\n",
    "            print(\"position_ids (so far)\\n\", position_ids)\n",
    "            \"\"\"\n",
    "            tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
    "                      14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
    "                      28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
    "                      42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
    "                      56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
    "                      70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
    "                      84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
    "                      98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
    "                     112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
    "                     126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
    "                     140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
    "                     154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
    "                     168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
    "                     182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
    "                     196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
    "                     210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
    "                     224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
    "                     238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
    "                     252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
    "                     266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
    "                     280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
    "                     294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
    "                     308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
    "                     322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
    "                     336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
    "                     350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
    "                     364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
    "                     378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
    "                     392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
    "                     406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
    "                     420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
    "                     434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
    "                     448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
    "                     462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
    "                     476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
    "                     490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
    "                     504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517,\n",
    "                     518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531,\n",
    "                     532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
    "                     546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559,\n",
    "                     560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573,\n",
    "                     574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587,\n",
    "                     588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598]])\n",
    "            \"\"\"\n",
    "            print(\"【EXIT】else (padding_side != 'left'):\")\n",
    "\n",
    "    new_input_embeds = torch.stack(new_input_embeds_padded, dim=0)\n",
    "    print(\"new_input_embeds (after) shape\\n\", new_input_embeds.shape)  # torch.Size([1, 599, 2048])\n",
    "    print(\"new_input_embeds (after)\\n\", new_input_embeds)\n",
    "    \"\"\"\n",
    "    tensor([[[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
    "              -6.5231e-04, -4.9973e-04],\n",
    "             [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
    "              -1.6907e-01, -3.4387e-01],\n",
    "             [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
    "              -1.3859e-01, -7.3106e-02],\n",
    "             ...,\n",
    "             [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
    "              -8.3618e-03, -9.4604e-03],\n",
    "             [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
    "              -2.5177e-03, -8.0566e-03],\n",
    "             [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
    "              -7.3242e-04,  2.7924e-03]]], grad_fn=<StackBackward0>)\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"【COND】 _labels_is_None={_labels is None}\") \n",
    "    if _labels is None:\n",
    "        #【SKIP】\n",
    "        print(\"【ENTER】if _labels is None:\")\n",
    "        new_labels = None\n",
    "        print(\"【EXIT】if _labels is None:\")\n",
    "    else:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】else of if _labels is None:\")\n",
    "        new_labels = new_labels_padded\n",
    "        print(\"new_labels (after)\\n\", new_labels)\n",
    "        \"\"\"\n",
    "        tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "                  -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
    "                   297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
    "                   322,  6567,   310,   263,  6114,   411,  2654, 11315,    13]])\n",
    "        \"\"\"\n",
    "        print(\"【EXIT】else of if _labels is None:\")\n",
    "\n",
    "    print(f\"【COND】 _attention_mask_is_None={_attention_mask is None}\") \n",
    "    if _attention_mask is None:\n",
    "        # 【SKIP】\n",
    "        print(\"【ENTER】if _attention_mask is None:\")\n",
    "        attention_mask = None\n",
    "        print(\"【EXIT】if _attention_mask is None:\")\n",
    "    else:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】else of if _attention_mask is None:\")\n",
    "        attention_mask = attention_mask.to(dtype=_attention_mask.dtype)\n",
    "        print(\"attention_mask (after)2\\n\", attention_mask)\n",
    "        \"\"\"\n",
    "        tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "                 True, True, True, True, True, True, True, True, True, True, True]])\n",
    "        \"\"\"\n",
    "\n",
    "    print(f\"【COND】 _position_ids_is_None={_position_ids is None}\")\n",
    "    if _position_ids is None:\n",
    "        print(\"【ENTER】if _position_ids is None:\")\n",
    "        position_ids = None\n",
    "        print(\"【EXIT】if _position_ids is None:\")\n",
    "\n",
    "    print(\"position_ids (return)\\n\", position_ids)  # None\n",
    "    print(\"attention_mask (return)\\n\", attention_mask)\n",
    "    \"\"\"\n",
    "    tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "             True, True, True, True, True, True, True, True, True, True, True]])\n",
    "    \"\"\"\n",
    "    print(\"past_key_values (return)\\n\", past_key_values)  # None\n",
    "    print(\"new_input_embeds (return)\\n\", new_input_embeds)\n",
    "    \"\"\"\n",
    "    tensor([[[-1.0910e-03,  1.9302e-03, -1.6632e-03,  ...,  1.9932e-04,\n",
    "              -6.5231e-04, -4.9973e-04],\n",
    "             [-1.9428e-01,  1.1569e-01, -7.4740e-02,  ...,  2.6653e-03,\n",
    "              -1.6907e-01, -3.4387e-01],\n",
    "             [ 4.3680e-02,  1.7172e-01, -9.9813e-02,  ...,  9.3004e-02,\n",
    "              -1.3859e-01, -7.3106e-02],\n",
    "             ...,\n",
    "             [ 2.1240e-02, -2.2705e-02, -1.4221e-02,  ..., -2.8229e-03,\n",
    "              -8.3618e-03, -9.4604e-03],\n",
    "             [ 3.7079e-03, -3.6011e-03,  9.0332e-03,  ..., -1.3672e-02,\n",
    "              -2.5177e-03, -8.0566e-03],\n",
    "             [-6.3705e-04, -1.0605e-03, -1.1841e-02,  ...,  2.1935e-04,\n",
    "              -7.3242e-04,  2.7924e-03]]], grad_fn=<StackBackward0>)\n",
    "    \"\"\"\n",
    "    print(\"new_labels (return)\\n\", new_labels)\n",
    "    \"\"\"\n",
    "    tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "              -100,  -100,  -100,  -100,  -100,  -100,  -100,   278, 25616, 26624,\n",
    "               297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,\n",
    "               322,  6567,   310,   263,  6114,   411,  2654, 11315,    13]])\n",
    "    \"\"\"\n",
    "    return None, position_ids, attention_mask, past_key_values, new_input_embeds, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LlavaMetaForCausalLM.prepare_inputs_labels_for_multimodal = prepare_inputs_labels_for_multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model_device\\n\", model.device) # cpu\n",
    "\n",
    "input_ids = batch['input_ids'].to(device=model.device)\n",
    "print(\"input_ids shape\\n\", input_ids.shape) # torch.Size([1, 24])\n",
    "print(\"input_ids\\n\", input_ids)\n",
    "\"\"\"\n",
    "tensor([[    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "           411,  2654, 11315,    13]])\n",
    "\"\"\"\n",
    "\n",
    "attention_mask = batch['attention_mask'].to(device=model.device)\n",
    "print(\"attention_mask shape\\n\", attention_mask.shape) # torch.Size([1, 24])\n",
    "print(\"attention_mask\\n\", attention_mask)\n",
    "\"\"\"\n",
    "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True]])\n",
    "\"\"\"\n",
    "\n",
    "labels = batch['labels'].to(device=model.device)\n",
    "print(\"labels shape\\n\", labels.shape) # torch.Size([1, 3, 336, 336])\n",
    "print(\"labels\\n\", labels)\n",
    "\"\"\"\n",
    "tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "         10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "           411,  2654, 11315,    13]])\n",
    "\"\"\"\n",
    "\n",
    "images = batch['images'].to(device=model.device)\n",
    "print(\"images shape\\n\", images.shape)\n",
    "print(\"images\\n\", images)\n",
    "\"\"\"\n",
    "tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7120, -0.3616, -0.1280],\n",
    "          [ 0.0325,  0.0325,  0.0325,  ..., -0.3908, -0.1718, -0.0259],\n",
    "          [ 0.0325,  0.0325,  0.0325,  ..., -0.0113,  0.0471,  0.0909],\n",
    "          ...,\n",
    "          [-1.0331, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
    "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623],\n",
    "          [-1.0477, -1.0331, -1.0331,  ..., -1.0623, -1.0623, -1.0623]],\n",
    "\n",
    "         [[ 0.3190,  0.3190,  0.3190,  ..., -0.3864, -0.0112,  0.2139],\n",
    "          [ 0.3190,  0.3190,  0.3190,  ..., -0.0712,  0.1539,  0.3190],\n",
    "          [ 0.3190,  0.3190,  0.3190,  ...,  0.2890,  0.3640,  0.4390],\n",
    "          ...,\n",
    "          [-1.0167, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
    "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017],\n",
    "          [-1.0317, -1.0167, -1.0167,  ..., -1.0017, -1.0017, -1.0017]],\n",
    "\n",
    "         [[ 0.9656,  0.9656,  0.9656,  ...,  0.0982,  0.4537,  0.6670],\n",
    "          [ 0.9656,  0.9656,  0.9656,  ...,  0.3968,  0.6101,  0.7523],\n",
    "          [ 0.9656,  0.9656,  0.9656,  ...,  0.7523,  0.8092,  0.8377],\n",
    "          ...,\n",
    "          [-0.3711, -0.3853, -0.3995,  ..., -0.4279, -0.4279, -0.4279],\n",
    "          [-0.3711, -0.3711, -0.3853,  ..., -0.4279, -0.4279, -0.4279],\n",
    "          [-0.3853, -0.3711, -0.3711,  ..., -0.4279, -0.4279, -0.4279]]]])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids = None\n",
    "past_key_values = None\n",
    "image_sizes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from print_factory.print_factory import run_and_capture, embed_print_outputs\n",
    "\n",
    "logs, mapping = run_and_capture(\n",
    "    model.prepare_inputs_labels_for_multimodal,\n",
    "    input_ids,\n",
    "    position_ids,\n",
    "    attention_mask,\n",
    "    past_key_values,\n",
    "    labels,\n",
    "    images,\n",
    "    image_sizes\n",
    ")\n",
    "\n",
    "with open(\"print_factory/original_code.py\") as f:\n",
    "    code = f.read()\n",
    "\n",
    "new_code = embed_print_outputs(code, mapping)\n",
    "\n",
    "with open(\"print_factory/after_code.py\", \"w\") as f:\n",
    "    f.write(new_code)\n",
    "\n",
    "print(\"done\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    input_ids,\n",
    "    position_ids,\n",
    "    attention_mask,\n",
    "    past_key_values,\n",
    "    inputs_embeds,\n",
    "    labels\n",
    ") = model.prepare_inputs_labels_for_multimodal(\n",
    "    input_ids,\n",
    "    position_ids,\n",
    "    attention_mask,\n",
    "    past_key_values,\n",
    "    labels,\n",
    "    images,\n",
    "    image_sizes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(\n",
    "    self,\n",
    "    input_ids: torch.LongTensor = None,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    labels: Optional[torch.LongTensor] = None,\n",
    "    use_cache: Optional[bool] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    images: Optional[torch.FloatTensor] = None,\n",
    "    image_sizes: Optional[List[List[int]]] = None,\n",
    "    return_dict: Optional[bool] = None,\n",
    ") -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "\n",
    "    print(\"current file path\", \"llava/llava/model/language_model/llava_llama.py\")\n",
    "    print(\"def LlavaLlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, image_sizes, return_dict)\")\n",
    "    print(\"input_ids\\n\", input_ids)\n",
    "    \"\"\"\n",
    "    tensor([[    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "            10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "            411,  2654, 11315,    13]], device='cuda:0')        \n",
    "    \"\"\"\n",
    "    if hasattr(input_ids, 'shape'):\n",
    "        print(\"input_ids.shape\\n\", input_ids.shape) # torch.Size([1, 24])\n",
    "    print(\"attention_mask\\n\", attention_mask)\n",
    "    \"\"\"\n",
    "    tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "            True, True, True, True, True, True, True, True, True, True, True, True]],\n",
    "        device='cuda:0')\n",
    "    \"\"\"\n",
    "    print(\"position_ids\\n\", position_ids) # None\n",
    "    print(\"past_key_values\\n\", past_key_values) # None\n",
    "    print(\"inputs_embeds\\n\", inputs_embeds) # None\n",
    "    if hasattr(inputs_embeds, 'shape'):\n",
    "        print(\"inputs_embeds.shape\\n\", inputs_embeds.shape)\n",
    "    print(\"labels\\n\", labels)\n",
    "    \"\"\"\n",
    "    tensor([[ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879,\n",
    "            10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114,\n",
    "            411,  2654, 11315,    13]], device='cuda:0')\n",
    "    \"\"\"\n",
    "    print(\"use_cache\\n\", use_cache) # None\n",
    "    print(\"output_attentions\\n\", output_attentions) # None\n",
    "    print(\"output_hidden_states\\n\", output_hidden_states) # None\n",
    "    print(\"images\\n\", images)\n",
    "    \"\"\"\n",
    "    tensor([[[[ 0.0325,  0.0325,  0.0325,  ..., -0.7109, -0.3613, -0.1279],\n",
    "            [ 0.0325,  0.0325,  0.0325,  ..., -0.3906, -0.1719, -0.0259],\n",
    "            [ 0.0325,  0.0325,  0.0325,  ..., -0.0112,  0.0471,  0.0908],\n",
    "            ...,\n",
    "            [-1.0312, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],\n",
    "            [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625],\n",
    "            [-1.0469, -1.0312, -1.0312,  ..., -1.0625, -1.0625, -1.0625]],\n",
    "\n",
    "            [[ 0.3184,  0.3184,  0.3184,  ..., -0.3867, -0.0112,  0.2139],\n",
    "            [ 0.3184,  0.3184,  0.3184,  ..., -0.0713,  0.1543,  0.3184],\n",
    "            [ 0.3184,  0.3184,  0.3184,  ...,  0.2891,  0.3633,  0.4395],\n",
    "            ...,\n",
    "            [-1.0156, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],\n",
    "            [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000],\n",
    "            [-1.0312, -1.0156, -1.0156,  ..., -1.0000, -1.0000, -1.0000]],\n",
    "\n",
    "            [[ 0.9648,  0.9648,  0.9648,  ...,  0.0981,  0.4531,  0.6680],\n",
    "            [ 0.9648,  0.9648,  0.9648,  ...,  0.3965,  0.6094,  0.7539],\n",
    "            [ 0.9648,  0.9648,  0.9648,  ...,  0.7539,  0.8086,  0.8359],\n",
    "            ...,\n",
    "            [-0.3711, -0.3848, -0.4004,  ..., -0.4277, -0.4277, -0.4277],\n",
    "            [-0.3711, -0.3711, -0.3848,  ..., -0.4277, -0.4277, -0.4277],\n",
    "            [-0.3848, -0.3711, -0.3711,  ..., -0.4277, -0.4277, -0.4277]]]],\n",
    "        device='cuda:0', dtype=torch.bfloat16)\n",
    "    \"\"\"\n",
    "    if hasattr(images, 'shape'):\n",
    "        print(\"images.shape\\n\", images.shape) # torch.Size([1, 3, 336, 336])\n",
    "    print(\"image_sizes\\n\", image_sizes) # None\n",
    "    print(\"return_dict\\n\", return_dict) # None\n",
    "\n",
    "    print(f\"【COND】 inputs_embeds_is_None={inputs_embeds is None}\") # True\n",
    "    if inputs_embeds is None:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】if inputs_embeds is None:\")\n",
    "        (\n",
    "            input_ids,\n",
    "            position_ids,\n",
    "            attention_mask,\n",
    "            past_key_values,\n",
    "            inputs_embeds,\n",
    "            labels\n",
    "        ) = self.prepare_inputs_labels_for_multimodal(\n",
    "            input_ids,\n",
    "            position_ids,\n",
    "            attention_mask,\n",
    "            past_key_values,\n",
    "            labels,\n",
    "            images,\n",
    "            image_sizes\n",
    "        )\n",
    "        print(\"【EXIT】if inputs_embeds is None:\")\n",
    "\n",
    "    print(\"input_ids (after prepare_inputs_labels_for_multimodal)\\n\", input_ids)\n",
    "\n",
    "    print(\"position_ids (after prepare_inputs_labels_for_multimodal)\\n\", position_ids)\n",
    "\n",
    "    print(\"attention_mask shape (after prepare_inputs_labels_for_multimodal)\\n\", attention_mask.shape)\n",
    "    print(\"attention_mask (after prepare_inputs_labels_for_multimodal)\\n\", attention_mask)\n",
    "\n",
    "\n",
    "    print(\"past_key_values (after prepare_inputs_labels_for_multimodal)\\n\", past_key_values)\n",
    "\n",
    "    print(\"inputs_embeds shape (after prepare_inputs_labels_for_multimodal)\\n\", None if inputs_embeds is None else inputs_embeds.shape)\n",
    "    print(\"inputs_embeds (after prepare_inputs_labels_for_multimodal)\\n\", inputs_embeds)\n",
    "\n",
    "    print(\"labels shape (after prepare_inputs_labels_for_multimodal)\\n\", labels.shape)\n",
    "    print(\"labels (after prepare_inputs_labels_for_multimodal)\\n\", labels)\n",
    "\n",
    "    #  LlamaForCausalLM.forward(self, ...)で明示\n",
    "    # Trainer > def train > def inner_training_loop > def training_step > model(**inputs) > model.forward\n",
    "    result = LlamaForCausalLM.forward(\n",
    "        self,\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_values=past_key_values,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        labels=labels,\n",
    "        use_cache=use_cache,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict\n",
    "    )\n",
    "    print(\"Return of def LlavaLlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, image_sizes, return_dict)\")\n",
    "    #print(\"result of LlavaLlamaForCausalLM.forward (return)\\n\", result)\n",
    "    print(\"logits tensor shape  LlavaLlamaForCausalLM.forward\\n\", result.logits.shape) # torch.Size([1, 599, 32000])\n",
    "    print(\"logits tensor (first 10 tokens)  LlavaLlamaForCausalLM.forward\\n\", result.logits[0, :10, :])\n",
    "    \"\"\"\n",
    "    tensor([[-4.6822,  0.9866,  4.5126,  ..., -5.2010, -2.1646, -4.2286],\n",
    "            [ 3.2882,  3.7620,  1.9036,  ...,  1.6998,  1.9076,  3.4061],\n",
    "            [-7.5813, -7.5340,  3.8222,  ..., -1.3140, -6.1645, -1.1844],\n",
    "            ...,\n",
    "            [-8.3741, -7.6768,  0.9916,  ..., -7.2707, -8.4552, -4.9956],\n",
    "            [-7.4367, -7.1454,  3.2786,  ..., -5.6713, -6.4151, -3.9531],\n",
    "            [-7.0483, -6.7871,  4.4183,  ..., -7.2721, -7.3837, -4.2513]],\n",
    "        grad_fn=<SliceBackward0>)\n",
    "    \"\"\"\n",
    "    print(\"loss (return)  LlavaLlamaForCausalLM.forward \\n\", result.loss) # tensor(9.2224, grad_fn=<NllLossBackward0>)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LlavaLlamaForCausalLM.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"input_ids\": batch['input_ids'].to(device=model.device),\n",
    "    \"attention_mask\": batch['attention_mask'].to(device=model.device),\n",
    "    \"labels\": batch['labels'].to(device=model.device),\n",
    "    \"images\": batch['images'].to(device=model.device),\n",
    "    \"position_ids\": None,\n",
    "    \"past_key_values\": None,\n",
    "    \"image_sizes\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "logs, mapping = run_and_capture(\n",
    "    model.forward,\n",
    "    **inputs\n",
    ")\n",
    "\n",
    "with open(\"print_factory/original_code.py\") as f:\n",
    "    code = f.read()\n",
    "\n",
    "new_code = embed_print_outputs(code, mapping)\n",
    "\n",
    "with open(\"print_factory/after_code.py\", \"w\") as f:\n",
    "    f.write(new_code)\n",
    "\n",
    "print(\"done\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_zero_3(param, ignore_status=False, name=None):\n",
    "\n",
    "    print(\"current file path\", \"llava/train/llava_trainer.py\")\n",
    "    print(\"def maybe_zero_3(param, ignore_status=False, name=None)\")\n",
    "    print(\"param maybe_zero_3\\n\", param)\n",
    "    print(\"ignore_status maybe_zero_3\\n\", ignore_status)\n",
    "    print(\"name maybe_zero_3\\n\", name)\n",
    "    from deepspeed import zero\n",
    "    from deepspeed.runtime.zero.partition_parameters import ZeroParamStatus\n",
    "    print(f\"【COND】 hasattr_ds_id={hasattr(param, 'ds_id')}\") # 【COND】 hasattr_ds_id=False\n",
    "    if hasattr(param, \"ds_id\"): # TinyLLaVAではdeepspeedを使用しないのでSKIP\n",
    "        print(\"【ENTER】if hasattr(param, 'ds_id'):\") \n",
    "        print(f\"【COND】 ds_status={getattr(param, 'ds_status', None)}, ignore_status={ignore_status}\")\n",
    "        if param.ds_status == ZeroParamStatus.NOT_AVAILABLE:\n",
    "            print(\"【ENTER】if param.ds_status == ZeroParamStatus.NOT_AVAILABLE:\")\n",
    "            print(f\"【COND】 ignore_status={ignore_status}\")\n",
    "            if not ignore_status:\n",
    "                print(\"【ENTER】if not ignore_status:\")\n",
    "                print(name, 'no ignore status')\n",
    "                print(\"【EXIT】if not ignore_status:\")\n",
    "            print(\"【EXIT】if param.ds_status == ZeroParamStatus.NOT_AVAILABLE:\")\n",
    "        with zero.GatheredParameters([param]):\n",
    "            param = param.data.detach().cpu().clone()\n",
    "            print(\"param (after GatheredParameters)\\n\", param)\n",
    "        print(\"【EXIT】if hasattr(param, 'ds_id'):\")\n",
    "    else:\n",
    "        print(\"【ENTER】else (not hasattr(param, 'ds_id')):\") # ENTER\n",
    "        param = param.detach().cpu().clone()\n",
    "        print(\"param (after else)\\n\", param)\n",
    "        print(\"【EXIT】else (not hasattr(param, 'ds_id')):\")\n",
    "    print(\"param (def maybe_zero_3 at llava_trainer.py return)\\n\", param)\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mm_adapter_state_maybe_zero_3(named_params, keys_to_match):\n",
    "\n",
    "    print(\"current file path\", \"llava/train/llava_trainer.py\")\n",
    "    print(\"def get_mm_adapter_state_maybe_zero_3(named_params, keys_to_match)\")\n",
    "    print(\"named_params get_mm_adapter_state_maybe_zero_3\\n\", named_params)\n",
    "    print(\"keys_to_match get_mm_adapter_state_maybe_zero_3\\n\", keys_to_match)\n",
    "    to_return = {k: t for k, t in named_params if any(key_match in k for key_match in keys_to_match)}\n",
    "    print(\"to_return = {k: t for k, t in named_params if any(key_match in k for key_match in keys_to_match)}\\n\", to_return)\n",
    "    to_return = {k: maybe_zero_3(v, ignore_status=True, name=k).cpu() for k, v in to_return.items()}\n",
    "    print(\"to_return def get_mm_adapter_state_maybe_zero_3 \\n\", to_return)\n",
    "\n",
    "    for k, v in to_return.items():\n",
    "        if hasattr(v, 'shape'):\n",
    "            print(f\"to_return['{k}'].shape\\n\", v.shape)\n",
    "            \n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_match = ['mm_projector']\n",
    "weight_to_save = get_mm_adapter_state_maybe_zero_3(model.named_parameters(), keys_to_match)\n",
    "print(\"weight_to_save\\n\", weight_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = training_args.output_dir\n",
    "print(\"output_dir\", output_dir)\n",
    "model.config.save_pretrained(output_dir)\n",
    "torch.save(weight_to_save, os.path.join(output_dir, f'mm_projector.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaVATrainer(Trainer):\n",
    "    # Trainer > _inner_training_loop > _maybe_log_save_evaluate > self._save_checkpoint(model, trial)\n",
    "    def _save_checkpoint(self, model, trial, metrics=None):\n",
    "\n",
    "        print(\"current file path\", \"llava/train/llava_trainer.py\")\n",
    "        print(\"def _save_checkpoint(self, model, trial, metrics=None)\")\n",
    "        print(\"self _save_checkpoint\\n\", self) # <llava.train.llava_trainer.LLaVATrainer object at 0x7ed6341f4490>\n",
    "        print(\"model _save_checkpoint\\n\", model)\n",
    "        print(\"trial _save_checkpoint\\n\", trial) # None\n",
    "        print(\"metrics _save_checkpoint\\n\", metrics) # None\n",
    "        print(f\"【COND】tune_mm_mlp_adapter={getattr(self.args, 'tune_mm_mlp_adapter', False)}\") # True\n",
    "        if getattr(self.args, 'tune_mm_mlp_adapter', False):\n",
    "            # 【ENTER】\n",
    "            print(\"【ENTER】if getattr(self.args, 'tune_mm_mlp_adapter', False):\")\n",
    "            from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "            checkpoint_folder = f\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\"\n",
    "            print(\"checkpoint_folder = f\\\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\\\"\\n\", checkpoint_folder)\n",
    "\n",
    "            run_dir = self._get_output_dir(trial=trial)\n",
    "            print(\"run_dir = self._get_output_dir(trial=trial)\", run_dir)\n",
    "            output_dir = os.path.join(run_dir, checkpoint_folder)\n",
    "            print(\"output_dir = os.path.join(run_dir, checkpoint_folder)\", output_dir)\n",
    "\n",
    "            # Only save Adapter\n",
    "            keys_to_match = ['mm_projector'] # 'vision_resampler'\n",
    "            print(f\"【COND】mm_use_im_start_end={getattr(self.args, 'mm_use_im_start_end', False)}\") # False\n",
    "            if getattr(self.args, \"mm_use_im_start_end\", False):\n",
    "                pass\n",
    "\n",
    "            weight_to_save = get_mm_adapter_state_maybe_zero_3(self.model.named_parameters(), keys_to_match)\n",
    "\n",
    "            print(f\"【COND】local_rank={self.args.local_rank}\") # 0\n",
    "            if self.args.local_rank == 0 or self.args.local_rank == -1:\n",
    "                # 【ENTER】\n",
    "                print(\"【ENTER】if self.args.local_rank == 0 or self.args.local_rank == -1:\")\n",
    "                self.model.config.save_pretrained(output_dir)\n",
    "                torch.save(weight_to_save, os.path.join(output_dir, f'mm_projector.bin'))\n",
    "                print(\"【EXIT】if self.args.local_rank == 0 or self.args.local_rank == -1:\")\n",
    "            print(\"【EXIT】if getattr(self.args, 'tune_mm_mlp_adapter', False):\")\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer,\n",
    "                                   output_dir: str):\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str)\")\n",
    "    print(\"trainer safe_save_model_for_hf_trainer\\n\", type(trainer)) # <class 'llava.train.llava_trainer.LLaVATrainer'>\n",
    "    print(\"output_dir safe_save_model_for_hf_trainer\\n\", output_dir) # ./checkpoints/llava-v1.5-7b-pretrain\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "\n",
    "    print(\"trainer.args safe_save_model_for_hf_trainer\\n\", trainer.args) # TrainingArguments(...\n",
    "    print(\"【COND】tune_mm_mlp_adapter=\", getattr(trainer.args, \"tune_mm_mlp_adapter\", False)) # True\n",
    "    if getattr(trainer.args, \"tune_mm_mlp_adapter\", False):\n",
    "        # Only save Adapter\n",
    "        print(\"【ENTER】if getattr(trainer.args, 'tune_mm_mlp_adapter', False):\") # 【ENTER】\n",
    "        keys_to_match = ['mm_projector']\n",
    "        if getattr(trainer.args, \"use_im_start_end\", False):\n",
    "            print(\"【ENTER】if getattr(trainer.args, 'use_im_start_end', False):\")\n",
    "            keys_to_match.extend(['embed_tokens', 'embed_in'])\n",
    "            print(\"【EXIT】if getattr(trainer.args, 'use_im_start_end', False):\")\n",
    "\n",
    "        weight_to_save = get_mm_adapter_state_maybe_zero_3(trainer.model.named_parameters(), keys_to_match)\n",
    "        if hasattr(weight_to_save, 'shape'):\n",
    "            print(\"【ENTER】if hasattr(weight_to_save, 'shape'):\")\n",
    "            print(\"weight_to_save.shape\\n\", weight_to_save.shape)\n",
    "            print(\"【EXIT】if hasattr(weight_to_save, 'shape'):\")\n",
    "        trainer.model.config.save_pretrained(output_dir)\n",
    "\n",
    "        current_folder = output_dir.split('/')[-1]\n",
    "        parent_folder = os.path.dirname(output_dir)\n",
    "        print(\"current_folder = output_dir.split('/')[-1]\", current_folder) # checkpoint-xxx or llava-v1.5-7b-pretrain\n",
    "        print(\"parent_folder = os.path.dirname(output_dir)\\n\", parent_folder) # ./checkpoints\n",
    "        print(\"【COND】trainer.args.local_rank=\", trainer.args.local_rank) # 0 or -1\n",
    "        if trainer.args.local_rank == 0 or trainer.args.local_rank == -1:\n",
    "            print(\"【ENTER】if trainer.args.local_rank == 0 or trainer.args.local_rank == -1:\")\n",
    "            if current_folder.startswith('checkpoint-'):\n",
    "                print(\"【ENTER】if current_folder.startswith('checkpoint-'):\")\n",
    "                mm_projector_folder = os.path.join(parent_folder, \"mm_projector\")\n",
    "                os.makedirs(mm_projector_folder, exist_ok=True)\n",
    "                torch.save(weight_to_save, os.path.join(mm_projector_folder, f'{current_folder}.bin'))\n",
    "                print(f\"Adapter weights saved to {os.path.join(mm_projector_folder, f'{current_folder}.bin')}\")\n",
    "                print(\"【EXIT】if current_folder.startswith('checkpoint-'):\")\n",
    "            else:\n",
    "                torch.save(weight_to_save, os.path.join(output_dir, f'mm_projector.bin'))\n",
    "                print(f\"Adapter weights saved to {os.path.join(output_dir, f'mm_projector.bin')}\")\n",
    "            print(\"【EXIT】if trainer.args.local_rank == 0 or trainer.args.local_rank == -1:\")\n",
    "        print(\"【EXIT】if getattr(trainer.args, 'tune_mm_mlp_adapter', False):\")\n",
    "        return\n",
    "\n",
    "    if trainer.deepspeed:\n",
    "        print(\"【ENTER】if trainer.deepspeed:\") # 【SKIP】\n",
    "        torch.cuda.synchronize()\n",
    "        trainer.save_model(output_dir)\n",
    "        print(\"【EXIT】if trainer.deepspeed:\")\n",
    "        return\n",
    "\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        print(\"【ENTER】if trainer.args.should_save:\") # 【SKIP】\n",
    "        cpu_state_dict = {\n",
    "            key: value.cpu() for key, value in state_dict.items()\n",
    "        }\n",
    "        for key, value in cpu_state_dict.items():\n",
    "            if hasattr(value, 'shape'):\n",
    "                print(f\"cpu_state_dict['{key}'].shape\\n\", value.shape)\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n",
    "        print(\"【EXIT】if trainer.args.should_save:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgAHEvYzuB0U"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "def train():\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def train()\")\n",
    "    global local_rank\n",
    "\n",
    "    parser = transformers.HfArgumentParser(\n",
    "        (ModelArguments, DataArguments, TrainingArguments))\n",
    "    print(\"original parser\\n\", parser)\n",
    "    model_args, data_args, training_args = parser.parse_dict(args_dict)\n",
    "    print(\"model_args\\n\", model_args)\n",
    "    print(\"data_args\\n\", data_args)\n",
    "    print(\"training_args\\n\", training_args)\n",
    "    local_rank = training_args.local_rank\n",
    "    print(\"local_rank\\n\", local_rank)\n",
    "    compute_dtype = (torch.float16 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n",
    "    print(\"compute_dtype\\n\", compute_dtype)\n",
    "    bnb_model_from_pretrained_args = {}\n",
    "    print(\"bnb_model_from_pretrained_args\\n\", bnb_model_from_pretrained_args)\n",
    "    # 【SKIP】bfloat16 なので 以下の if 文はスキップされる\n",
    "    print(f\"【COND】 bits={training_args.bits}\")\n",
    "    if training_args.bits in [4, 8]:\n",
    "      pass\n",
    "\n",
    "    print(f\"【COND】 vision_tower={model_args.vision_tower}\")\n",
    "    # 【ENTER】 vision_tower=openai/clip-vit-large-patch14-336 なので、この分岐に入る\n",
    "    if model_args.vision_tower is not None:\n",
    "        print(\"【ENTER】if model_args.vision_tower is not None:\")\n",
    "        print(f\"【COND】 mpt_in_model_name_or_path={'mpt' in model_args.model_name_or_path}\")\n",
    "        #【SKIP】model_args.model_name_or_path に mptは含まれていないので、この分岐はskipされる\n",
    "        if 'mpt' in model_args.model_name_or_path:\n",
    "          pass\n",
    "\n",
    "        #【ENTER】 model_args.model_name_or_path に mptは含まれていないので、この分岐に入る\n",
    "        else:\n",
    "            print(\"【COND】 not_mpt_in_model_name_or_path={'mpt' not in model_args.model_name_or_path}\")\n",
    "            print(\"【ENTER】else of if 'mpt' in model_args.model_name_or_path:\")\n",
    "            # PreTrainedModel.from_pretrained\n",
    "            model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "                model_args.model_name_or_path,\n",
    "                cache_dir=training_args.cache_dir,\n",
    "                **bnb_model_from_pretrained_args\n",
    "            )\n",
    "            print(\"model defined as LlavaLlamaForCausalLM \\n\", model)\n",
    "            print(\"【EXIT】else of if 'mpt' in model_args.model_name_or_path:\")\n",
    "        print(\"【EXIT】if model_args.vision_tower is not None:\")\n",
    "    # 【SKIP】 vision_tower=clip-vit-large-patch14-336 なので、この分岐には入らない\n",
    "    else:\n",
    "      pass\n",
    "\n",
    "    print(f\"【COND】 freeze_backbone={model_args.freeze_backbone}\")\n",
    "    # 【SKIP】 freeze_backbone=False なので、この分岐はskipされる\n",
    "    if model_args.freeze_backbone:\n",
    "        pass\n",
    "\n",
    "    # 【SKIP】 bfloat16 なので 以下の if 文はスキップされる\n",
    "    print(f\"【COND】 bits={training_args.bits}\")\n",
    "    if training_args.bits in [4, 8]:\n",
    "      pass\n",
    "\n",
    "    print(f\"【COND】 gradient_checkpointing={training_args.gradient_checkpointing}\")\n",
    "    # 【ENTER】 gradient_checkpointing=True なので、この分岐に入る\n",
    "    if training_args.gradient_checkpointing:\n",
    "        print(\"【ENTER】if training_args.gradient_checkpointing:\")\n",
    "        print(f\"【COND】 has_enable_input_require_grads={hasattr(model, 'enable_input_require_grads')}\")\n",
    "        # 【ENTER】 model に enable_input_require_grads メソッドがあるので、この分岐に入る\n",
    "        if hasattr(model, \"enable_input_require_grads\"):\n",
    "            print(\"【ENTER】if hasattr(model, 'enable_input_require_grads'):\")\n",
    "            # PreTrainedModel.enable_input_require_grads\n",
    "            # 元々 全ての重みについて True\n",
    "            model.enable_input_require_grads()\n",
    "            print(\"【EXIT】if hasattr(model, 'enable_input_require_grads'):\")\n",
    "        # 【SKIP】 model に enable_input_require_grads メソッドがあるので、この分岐はskipされる\n",
    "        else:\n",
    "          pass\n",
    "\n",
    "        print(\"【EXIT】if training_args.gradient_checkpointing:\")\n",
    "\n",
    "    print(f\"【COND】 lora_enable={training_args.lora_enable}\")\n",
    "    # 【SKIP】 lora_enable=False なので、この分岐はskipされる\n",
    "    if training_args.lora_enable:\n",
    "      pass\n",
    "\n",
    "    print(f\"【COND】 mpt_in_model_name_or_path={'mpt' in model_args.model_name_or_path}\")\n",
    "    # 【SKIP】model_args.model_name_or_path に mptは含まれていないので、この分岐はskipされる\n",
    "    if 'mpt' in model_args.model_name_or_path:\n",
    "      pass\n",
    "\n",
    "    #【ENTER】 model_args.model_name_or_path に mptは含まれていないので、この分岐に入る\n",
    "    else:\n",
    "        print(\"【COND】 not_mpt_in_model_name_or_path={'mpt' not in model_args.model_name_or_path}\")\n",
    "        print(\"【ENTER】else of if 'mpt' in model_args.model_name_or_path:\")\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            cache_dir=training_args.cache_dir,\n",
    "            model_max_length=training_args.model_max_length,\n",
    "            padding_side=\"right\",\n",
    "            use_fast=False,\n",
    "        )\n",
    "        print(\"tokenizer defined by AutoTokenizer.from_pretrained \\n\", tokenizer)\n",
    "        print(\"【EXIT】else of if 'mpt' in model_args.model_name_or_path:\")\n",
    "\n",
    "    print(f\"【COND】 version={model_args.version}\")\n",
    "    # 【SKIP】 version=plain なので、この分岐はskipされる\n",
    "    if model_args.version == \"v0\":\n",
    "      pass\n",
    "\n",
    "    # 【SKIP】 version=plain なので、この分岐はskipされる\n",
    "    elif model_args.version == \"v0.5\":\n",
    "      pass\n",
    "    # 【ENTER】 version=plain なので、この分岐に入る\n",
    "    else:\n",
    "        print(\"【ENTER】else of if model_args.version == 'v0' and elif 'v0.5':\")\n",
    "        tokenizer.pad_token = tokenizer.unk_token\n",
    "        print(f\"【COND】 version_in_conv_templates={model_args.version in conv_templates}\")\n",
    "        # 【ENTER】 model_args.version=plain は conversation_lib.conv_templates に含まれている（\"plain\": conv_llava_plain）ので、この分岐に入る\n",
    "        if model_args.version in conv_templates:\n",
    "            print(\"【ENTER】if model_args.version in conversation_lib.conv_templates:\")\n",
    "            default_conversation = conv_templates[model_args.version]\n",
    "            print(f\"conversation_lib.default_conversation set to {model_args.version}\")\n",
    "            print(\"【EXIT】if model_args.version in conversation_lib.conv_templates:\")\n",
    "        # 【SKIP】 model_args.version=plain は conversation_lib.conv_templates に含まれているので、この分岐はskipされる\n",
    "        else:\n",
    "          pass\n",
    "        print(\"【EXIT】else of if model_args.version == 'v0' and elif 'v0.5':\")\n",
    "\n",
    "    print(f\"【COND】 vision_tower={model_args.vision_tower}\")\n",
    "    # 【ENTER】 vision_tower=openai/clip-vit-large-patch14-336 なので、この分岐に入る\n",
    "    if model_args.vision_tower is not None:\n",
    "        print(\"【ENTER】if model_args.vision_tower is not None:\")\n",
    "        model.get_model().initialize_vision_modules(\n",
    "            model_args=model_args,\n",
    "            fsdp=training_args.fsdp\n",
    "        )\n",
    "\n",
    "        vision_tower = model.get_vision_tower()\n",
    "        vision_tower.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)\n",
    "\n",
    "        data_args.image_processor = vision_tower.image_processor\n",
    "        data_args.is_multimodal = True\n",
    "\n",
    "        model.config.image_aspect_ratio = data_args.image_aspect_ratio\n",
    "        model.config.tokenizer_padding_side = tokenizer.padding_side\n",
    "        model.config.tokenizer_model_max_length = tokenizer.model_max_length\n",
    "\n",
    "        model.config.tune_mm_mlp_adapter = training_args.tune_mm_mlp_adapter = model_args.tune_mm_mlp_adapter\n",
    "        print(f\"【COND】 tune_mm_mlp_adapter={model_args.tune_mm_mlp_adapter}\") # True\n",
    "        if model_args.tune_mm_mlp_adapter:\n",
    "            # 【ENTER】 tune_mm_mlp_adapter=True なので、この分岐に入る\n",
    "            print(\"【ENTER】if model_args.tune_mm_mlp_adapter:\")\n",
    "            # モデル全体の全パラメータを「学習不可（requires_grad=False）」にする\n",
    "            # これで通常の重みは全て凍結される\n",
    "            model.requires_grad_(False)\n",
    "            for p in model.get_model().mm_projector.parameters():\n",
    "                # mm_projector（画像特徴量→テキスト特徴量への変換層）の全パラメータだけを「学習可能（requires_grad=True）」に戻す\n",
    "                # これで mm_projector のみ学習されることになる\n",
    "                print(\"model.get_model().mm_projector.parameters()\", model.get_model().mm_projector.parameters())\n",
    "                p.requires_grad = True\n",
    "            print(\"【EXIT】if model_args.tune_mm_mlp_adapter:\")\n",
    "\n",
    "        model.config.freeze_mm_mlp_adapter = training_args.freeze_mm_mlp_adapter\n",
    "        print(f\"【COND】 freeze_mm_mlp_adapter={training_args.freeze_mm_mlp_adapter}\") # False\n",
    "        if training_args.freeze_mm_mlp_adapter:\n",
    "          pass\n",
    "\n",
    "        print(f\"【COND】 bits={training_args.bits}\") # 16\n",
    "        if training_args.bits in [4, 8]:\n",
    "          pass\n",
    "\n",
    "        model.config.mm_use_im_start_end = data_args.mm_use_im_start_end = model_args.mm_use_im_start_end\n",
    "        print(\"model_args.mm_use_im_start_end\", model_args.mm_use_im_start_end)\n",
    "        model.config.mm_projector_lr = training_args.mm_projector_lr\n",
    "        print(\"training_args.mm_projector_lr\", training_args.mm_projector_lr)\n",
    "        training_args.use_im_start_end = model_args.mm_use_im_start_end\n",
    "        print(\"training_args.use_im_start_end\", training_args.use_im_start_end)\n",
    "        model.config.mm_use_im_patch_token = model_args.mm_use_im_patch_token\n",
    "        print(\"model_args.mm_use_im_patch_token\", model_args.mm_use_im_patch_token)\n",
    "        #model.initialize_vision_tokenizer(model_args, tokenizer=tokenizer)\n",
    "        #print(\"【EXIT】if model_args.vision_tower is not None:\")\n",
    "\n",
    "    print(f\"【COND】 bits={training_args.bits}\") # 16\n",
    "    if training_args.bits in [4, 8]:\n",
    "        pass\n",
    "\n",
    "    data_module = make_supervised_data_module(tokenizer=tokenizer,\n",
    "                                              data_args=data_args)\n",
    "    print(\"data_module\\n\", data_module) # {'train_dataset': <llava.train.train.LazySupervisedDataset object at 0x7ed6341f4880>, 'eval_dataset': None, 'data_collator': DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))}\n",
    "\n",
    "    trainer = LLaVATrainer(model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    args=training_args,\n",
    "                    **data_module)\n",
    "    print(\"trainer\\n\", trainer) # <llava.train.llava_trainer.LLaVATrainer object at 0x7ed6341f4490>\n",
    "\n",
    "    print(\"【COND】list(pathlib.Path(training_args.output_dir).glob('checkpoint-*'))\\n\", list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\"))) # [PosixPath('checkpoints/llava-v1.5-7b-pretrain/checkpoint-250'), PosixPath('checkpoints/llava-v1.5-7b-pretrain/checkpoint-1')]\n",
    "    if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】if list(pathlib.Path(training_args.output_dir).glob(checkpoint-*)):\")\n",
    "        trainer.train(resume_from_checkpoint=False)\n",
    "        print(\"【EXIT】if list(pathlib.Path(training_args.output_dir).glob(checkpoint-*)):\")\n",
    "    else:\n",
    "        print(\"【ENTER】else of if list(pathlib.Path(training_args.output_dir).glob(checkpoint-*)):\")\n",
    "        trainer.train()\n",
    "        print(\"【EXIT】else of if list(pathlib.Path(training_args.output_dir).glob(checkpoint-*)):\")\n",
    "    trainer.save_state()\n",
    "\n",
    "    model.config.use_cache = True\n",
    "    print(\"model.config.use_cache = True\", model.config.use_cache) # True\n",
    "\n",
    "    print(f\"【COND】lora_enable={training_args.lora_enable}\") # False\n",
    "    if training_args.lora_enable:\n",
    "      pass\n",
    "    else:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】else of if training_args.lora_enable:\")\n",
    "        print(\"trainer\", trainer) # <class 'llava.train.llava_trainer.LLaVATrainer'>\n",
    "        safe_save_model_for_hf_trainer(trainer=trainer,\n",
    "                                       output_dir=training_args.output_dir)\n",
    "        print(\"【EXIT】else of if training_args.lora_enable:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
