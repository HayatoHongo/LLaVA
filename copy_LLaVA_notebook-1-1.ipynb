{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Ampere architecture or cpu. Hopper architecture does not support this old environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "git clone https://github.com/HayatoHongo/LLaVA.git\n",
    "cd LLaVA/\n",
    "\n",
    "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh\n",
    "bash ~/miniconda.sh # Enter, yes, Enter, Enter\n",
    "\n",
    "source ~/.bashrc\n",
    "source ~/miniconda3/etc/profile.d/conda.sh\n",
    "conda --version\n",
    "conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/main\n",
    "conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/r\n",
    "conda create -n llava python=3.10.12 -y\n",
    "\n",
    "conda activate llava\n",
    "\n",
    "pip --version # make sure you use pip 25.1\n",
    "cd LLaVA/\n",
    "pip install -e .\n",
    "pip install -e \".[train]\"\n",
    "pip install flash-attn==2.3.4 --no-build-isolation\n",
    "pip install ipykernel\n",
    "\n",
    "# Jupyter 用のカーネルを建てる\n",
    "python -m ipykernel install --user \\\n",
    "  --name llava \\\n",
    "  --display-name \"Jupyter (llava)\"\n",
    "\n",
    "python -c \"import deepspeed; print(deepspeed.__version__)\" # make sure you are using deepspeed 0.9.5\n",
    "\n",
    "# そもそも作成したJupyter用のカーネルが認識されていないので、Ctrl+Shift+P (Cmd+Shift+P for Mac)でReload Window\n",
    "# 一度SSHを切って再接続する。\n",
    "# 再度仮想環境をactivate\n",
    "conda activate llava\n",
    "\n",
    "# カーネルでjupyter用のカーネルを選択する。Python環境ではないことに注意。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "ModuleNotFoundError: No module named 'transformers'\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import torch, transformers; print(torch.__version__, transformers.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RmGhXE625w7R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-28 10:14:53,862] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/wandb/sdk/launch/builder/build.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "/home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/wandb/sdk/launch/builder/build.py:11: UserWarning: Module wandb was already imported from /home/ubuntu/miniconda3/envs/llava/lib/python3.10/site-packages/wandb/__init__.py, but /home/ubuntu/LLaVA is being added to sys.path\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n",
    "    version: Optional[str] = field(default=\"v0\")\n",
    "    freeze_backbone: bool = field(default=False)\n",
    "    tune_mm_mlp_adapter: bool = field(default=False)\n",
    "    vision_tower: Optional[str] = field(default=None) # default to None\n",
    "    mm_vision_select_layer: Optional[int] = field(default=-1)   # default to the last layer\n",
    "    pretrain_mm_mlp_adapter: Optional[str] = field(default=None)\n",
    "    mm_projector_type: Optional[str] = field(default='linear')\n",
    "    mm_use_im_start_end: bool = field(default=False)\n",
    "    mm_use_im_patch_token: bool = field(default=True)\n",
    "    mm_patch_merge_type: Optional[str] = field(default='flat')\n",
    "    mm_vision_select_feature: Optional[str] = field(default=\"patch\")\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(default=None,\n",
    "                           metadata={\"help\": \"Path to the training data.\"})\n",
    "    lazy_preprocess: bool = False\n",
    "    is_multimodal: bool = False\n",
    "    image_folder: Optional[str] = field(default=None)\n",
    "    image_aspect_ratio: str = 'square'\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    remove_unused_columns: bool = field(default=False)\n",
    "    freeze_mm_mlp_adapter: bool = field(default=False)\n",
    "    mpt_attn_impl: Optional[str] = field(default=\"triton\")\n",
    "    model_max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\n",
    "            \"help\":\n",
    "            \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
    "        },\n",
    "    )\n",
    "    double_quant: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Compress the quantization statistics through double quantization.\"}\n",
    "    )\n",
    "    quant_type: str = field(\n",
    "        default=\"nf4\",\n",
    "        metadata={\"help\": \"Quantization data type to use. Should be one of `fp4` or `nf4`.\"}\n",
    "    )\n",
    "    bits: int = field(\n",
    "        default=16,\n",
    "        metadata={\"help\": \"How many bits to use.\"}\n",
    "    )\n",
    "    lora_enable: bool = False\n",
    "    lora_r: int = 64\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_weight_path: str = \"\"\n",
    "    lora_bias: str = \"none\"\n",
    "    mm_projector_lr: Optional[float] = None\n",
    "    group_by_modality_length: bool = field(default=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DTvjg0PP7za1"
   },
   "outputs": [],
   "source": [
    "from transformers import HfArgumentParser\n",
    "\n",
    "args_dict = {\n",
    "    #\"deepspeed\": \"./scripts/zero2.json\",\n",
    "    \"model_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    \"version\": \"plain\",\n",
    "    \"data_path\": \"/workspaces/LLaVA/CC3M_2.json\",\n",
    "    \"image_folder\": \"/workspaces/LLaVA/images/\",\n",
    "    \"vision_tower\": \"openai/clip-vit-large-patch14-336\",\n",
    "    \"mm_projector_type\": \"mlp2x_gelu\",\n",
    "    \"tune_mm_mlp_adapter\": True,\n",
    "    \"mm_vision_select_layer\": -2,\n",
    "    \"mm_use_im_start_end\": False,\n",
    "    \"mm_use_im_patch_token\": False,\n",
    "    \"bf16\": True,\n",
    "    \"output_dir\": \"./checkpoints/Tinyllava-1.1B-pretrain\",\n",
    "\n",
    "    # TrainingArguments 相当\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 1,\n",
    "    \"save_total_limit\": 1,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"weight_decay\": 0.0, # I don't know why 0.0\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"logging_steps\": 1,\n",
    "    \"tf32\": False, # switched from True for TinyLlama\n",
    "    \"model_max_length\": 2048,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"dataloader_num_workers\": 2,\n",
    "    \"lazy_preprocess\": True,\n",
    "    \"report_to\": \"none\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dBX7g2DD-xpR",
    "outputId": "627a283d-b903-45db-a0c0-eec1b51ca0eb"
   },
   "outputs": [],
   "source": [
    "parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_dict(args_dict)\n",
    "print(\"model_args\\n\", model_args)\n",
    "print(\"data_args\\n\", data_args)\n",
    "print(\"training_args\\n\", training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oC2rbSk5QtcX"
   },
   "outputs": [],
   "source": [
    "# Model Constants\n",
    "IGNORE_INDEX = -100\n",
    "IMAGE_TOKEN_INDEX = -200\n",
    "DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "DEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\n",
    "DEFAULT_IM_START_TOKEN = \"<im_start>\"\n",
    "DEFAULT_IM_END_TOKEN = \"<im_end>\"\n",
    "IMAGE_PLACEHOLDER = \"<image-placeholder>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_5thB_J_WEi"
   },
   "outputs": [],
   "source": [
    "local_rank = training_args.local_rank\n",
    "print(\"local_rank\\n\", local_rank)\n",
    "compute_dtype = (torch.float16 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n",
    "print(\"compute_dtype\\n\", compute_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFp9VBciHEhH"
   },
   "outputs": [],
   "source": [
    "from transformers import CLIPVisionModel, CLIPImageProcessor, CLIPVisionConfig\n",
    "import torch.nn as nn\n",
    "# __init__\n",
    "# load_model\n",
    "\n",
    "# result = CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n",
    "class CLIPVisionTower(nn.Module):\n",
    "    def __init__(self, vision_tower, args):\n",
    "        # result = CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n",
    "        print(\"current file path\", \"llava/llava/model/multimodal_encoder/clip_encoder.py\")\n",
    "        print(\"def CLIPVisionTower.__init__(self, vision_tower, args\")\n",
    "        print(\"self\\n\", type(self))\n",
    "        print(\"vision_tower\\n\", vision_tower) # openai/clip-vit-large-patch14-336\n",
    "        print(\"args\\n\", args) # ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_loaded = False\n",
    "\n",
    "        print(\"self.is_loaded\\n\", self.is_loaded) # False\n",
    "\n",
    "        self.vision_tower_name = vision_tower\n",
    "        print(\"self.vision_tower_name\\n\", self.vision_tower_name) # openai/clip-vit-large-patch14-336\n",
    "        self.select_layer = args.mm_vision_select_layer\n",
    "        print(\"self.select_layer\\n\", self.select_layer) # -2\n",
    "        self.select_feature = getattr(args, 'mm_vision_select_feature', 'patch')\n",
    "        print(\"self.select_feature\\n\", self.select_feature) # patch\n",
    "\n",
    "    def load_model(self):\n",
    "\n",
    "        print(\"current file path\", \"llava/llava/model/multimodal_encoder/clip_encoder.py\")\n",
    "        print(\"def CLIPVisionTower.load_model(self)\")\n",
    "        print(\"self\\n\", type(self))\n",
    "        print(\"self.vision_tower_name\\n\", self.vision_tower_name) # openai/clip-vit-large-patch14-336\n",
    "        self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name)\n",
    "        print(\"self.image_processor\\n\", self.image_processor)\n",
    "        \"\"\"\n",
    "        CLIPImageProcessor {\n",
    "        \"crop_size\": {\n",
    "            \"height\": 336,\n",
    "            \"width\": 336\n",
    "        },\n",
    "        \"do_center_crop\": true,\n",
    "        \"do_convert_rgb\": true,\n",
    "        \"do_normalize\": true,\n",
    "        \"do_rescale\": true,\n",
    "        \"do_resize\": true,\n",
    "        \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n",
    "        \"image_mean\": [\n",
    "            0.48145466,\n",
    "            0.4578275,\n",
    "            0.40821073\n",
    "        ],\n",
    "        \"image_processor_type\": \"CLIPImageProcessor\",\n",
    "        \"image_std\": [\n",
    "            0.26862954,\n",
    "            0.26130258,\n",
    "            0.27577711\n",
    "        ],\n",
    "        \"resample\": 3,\n",
    "        \"rescale_factor\": 0.00392156862745098,\n",
    "        \"size\": {\n",
    "            \"shortest_edge\": 336\n",
    "        }\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name)\n",
    "        print(\"self.vision_tower\\n\", self.vision_tower)\n",
    "        \"\"\"\n",
    "        CLIPVisionModel(\n",
    "        (vision_model): CLIPVisionTransformer(\n",
    "            (embeddings): CLIPVisionEmbeddings(\n",
    "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "            (position_embedding): Embedding(577, 1024)\n",
    "            )\n",
    "            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "            (encoder): CLIPEncoder(\n",
    "            (layers): ModuleList(\n",
    "                (0-23): 24 x CLIPEncoderLayer(\n",
    "                (self_attn): CLIPAttention(\n",
    "                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                (mlp): CLIPMLP(\n",
    "                    (activation_fn): QuickGELUActivation()\n",
    "                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                )\n",
    "            )\n",
    "            )\n",
    "            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        )\n",
    "        )\n",
    "        \"\"\"\n",
    "        self.vision_tower.requires_grad_(False)\n",
    "\n",
    "        self.is_loaded = True\n",
    "        print(\"self.is_loaded\\n\", self.is_loaded) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_tower_name = \"openai/clip-vit-large-patch14-336\"\n",
    "CLIPVisionTower_model = CLIPVisionTower(vision_tower_name, args=model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIPVisionTower_model.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CLIPVisionTower_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CLIPVisionTower_model.vision_tower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CLIPVisionTower_model.vision_tower.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mz6UWifHfTq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def build_vision_tower(model_args, **kwargs):\n",
    "    # vision_tower = build_vision_tower(model_args)\n",
    "    print(\"current file path\", \"llava/llava/model/multimodal_encoder/builder.py\")\n",
    "    print(\"def build_vision_tower(model_args, **kwargs)\")\n",
    "    print(\"model_args\\n\", model_args) # ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
    "    print(\"kwargs\\n\", kwargs) # {}\n",
    "    vision_tower = model_args.vision_tower\n",
    "    print(\"vision_tower from model_args\\n\", vision_tower) # openai/clip-vit-large-patch14-336\n",
    "    CLIPVisionTower_model = CLIPVisionTower(vision_tower, args=model_args, **kwargs)\n",
    "    CLIPVisionTower_model.load_model()\n",
    "    print(\"CLIPVisionTower_model\\n\", CLIPVisionTower_model)\n",
    "    print(\"【EXIT】if is_absolute_path_exists or vision_tower.startswith('openai') or vision_tower.startswith('laion') or 'ShareGPT4V' in vision_tower:\")\n",
    "    return CLIPVisionTower_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_vision_tower(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def build_vision_projector(config, delay_load=False, **kwargs):\n",
    "    # 2層MLPをハードコーディング（configからサイズのみ取得）\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(config.mm_hidden_size, config.hidden_size),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(config.hidden_size, config.hidden_size),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "# 公式 LLaMA-2-7B の config をロード\n",
    "TinyLlama_config = AutoConfig.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "TinyLlama_config.mm_hidden_size = CLIPVisionTower_model.vision_tower.config.hidden_size\n",
    "print(TinyLlama_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = build_vision_projector(TinyLlama_config)\n",
    "print(projector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(CLIPVisionTower_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mro(cls):\n",
    "    print(f\"MRO for {cls.__name__}:\\n\")\n",
    "    for i, c in enumerate(cls.mro()):\n",
    "        print(f\"{i:2d}: {c.__module__}.{c.__name__}\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjkgxnbKFGan"
   },
   "outputs": [],
   "source": [
    "# LlavaBaseModel\n",
    "# __init__\n",
    "# get_vision_tower\n",
    "# initialize_vision_modules\n",
    "# unpad_image\n",
    "\n",
    "class LlavaBaseModel:  # LLaMA, MPT などの全てのLLM用の共通機能をまとめたクラス\n",
    "\n",
    "    # LlavaLlamaModelの__init__によって呼び出される\n",
    "    def __init__(self):\n",
    "\n",
    "        print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "        print(\"LlavaBaseModel.__init__(self, config)\")\n",
    "        # 「LlavaLlamaModelの__init__によって呼び出された時は」LlamaModelの__init_を呼び出す\n",
    "        super(LlavaBaseModel, self).__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_YguuBGN-3R"
   },
   "outputs": [],
   "source": [
    "# transformers.models.llama.modeling_llama.LlamaModel\n",
    "# URL: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py\n",
    "from transformers import LlamaModel\n",
    "\n",
    "# LlavaBaseModelにLlamaの初期化クラスをくっつけたいので、LLamaModelをMROに追加。\n",
    "# MROだと、LlavaLlamaModel > LlavaBaseModel > LlamaModel の順に解決される。\n",
    "class LlavaLlamaModel(LlavaBaseModel, LlamaModel): \n",
    "    def __init__(self):\n",
    "\n",
    "        print(\"current file path\", \"llava/llava/model/language_model/llava_llama.py\")\n",
    "        print(\"def LlavaLlamaModel.__init__(self, config: LlamaConfig)\")\n",
    "        print(\"self\\n\", type(self))\n",
    "        # LlavaBaseModelの__init__を呼ぶ\n",
    "        super(LlavaLlamaModel, self).__init__() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mro(LlavaBaseModel)\n",
    "print_mro(LlavaLlamaModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlavaBaseModel\n",
    "# __init__\n",
    "# get_vision_tower\n",
    "# initialize_vision_modules\n",
    "# unpad_image\n",
    "\n",
    "class LlavaBaseModel:  # LLaMA, MPT などの全てのLLM用の共通機能をまとめたクラス\n",
    "\n",
    "    # LlavaLlamaModelの__init__によって呼び出される\n",
    "    def __init__(self, config):\n",
    "\n",
    "        print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "        print(\"LlavaBaseModel.__init__(self, config)\")\n",
    "        print(\"config\\n\", config)\n",
    "        # 「LlavaLlamaModelの__init__によって呼び出された時は」LlamaModelの__init_を呼び出す\n",
    "        super(LlavaBaseModel, self).__init__(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers.models.llama.modeling_llama.LlamaModel\n",
    "# URL: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py\n",
    "from transformers import LlamaConfig, LlamaModel\n",
    "\n",
    "class LlavaLlamaConfig(LlamaConfig):\n",
    "    model_type = \"llava_llama\"\n",
    "\n",
    "# LlavaBaseModelにLlamaの初期化クラスをくっつけたいので、LLamaModelをMROに追加。\n",
    "# MROだと、LlavaLlamaModel > LlavaBaseModel > LlamaModel の順に解決される。\n",
    "class LlavaLlamaModel(LlavaBaseModel, LlamaModel): \n",
    "    config_class = LlavaLlamaConfig # from_pretrained の時にここが読みこまれる\n",
    "\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "\n",
    "        print(\"current file path\", \"llava/llava/model/language_model/llava_llama.py\")\n",
    "        print(\"def LlavaLlamaModel.__init__(self, config: LlamaConfig)\")\n",
    "        print(\"self\\n\", type(self))\n",
    "        print(\"config\\n\", config)\n",
    "        # LlavaBaseModelの__init__を呼ぶ\n",
    "        super(LlavaLlamaModel, self).__init__(config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9uRX7reN0gI"
   },
   "outputs": [],
   "source": [
    "# LlavaBaseForCausalLM\n",
    "# get_vision_tower\n",
    "# encode_images\n",
    "# prepare_inputs_labels_for_multimodal\n",
    "# initialize_vision_tokenizer\n",
    "\n",
    "# 以下のLlavaBaseForCausalLMのselfは、継承先のLlavaLlamaForCausalLMのselfが使用される\n",
    "\n",
    "class LlavaBaseForCausalLM:\n",
    "\n",
    "    def get_vision_tower(self):\n",
    "        print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "        print(\"class LlavaBaseForCausalLM(ABC).get_vision_tower(self)\")\n",
    "        result = self.get_model().get_vision_tower()\n",
    "        print(\"LlavaBaseForCausalLM(ABC).get_vision_tower(self) result (return)\\n\", result)\n",
    "        \"\"\"\n",
    "        CLIPVisionTower(\n",
    "        (vision_tower): CLIPVisionModel(\n",
    "            (vision_model): CLIPVisionTransformer(\n",
    "            (embeddings): CLIPVisionEmbeddings(\n",
    "                (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "                (position_embedding): Embedding(577, 1024)\n",
    "            )\n",
    "            (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "            (encoder): CLIPEncoder(\n",
    "                (layers): ModuleList(\n",
    "                (0-23): 24 x CLIPEncoderLayer(\n",
    "                    (self_attn): CLIPAttention(\n",
    "                    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                    )\n",
    "                    (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                    (mlp): CLIPMLP(\n",
    "                    (activation_fn): QuickGELUActivation()\n",
    "                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "                    )\n",
    "                    (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                )\n",
    "                )\n",
    "            )\n",
    "            (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "            )\n",
    "        )\n",
    "        )\n",
    "        \"\"\"\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4yAIwyeOEWE"
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "class LlavaLlamaForCausalLM(LlamaForCausalLM, LlavaBaseForCausalLM):\n",
    "    config_class = LlavaLlamaConfig # from_pretrainedの際にここが読み込まれる\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        print(\"current file path\", \"llava/llava/model/language_model/llava_llama.py\")\n",
    "        print(\"def LlavaLlamaForCausalLM.__init__(self, config)\")\n",
    "        print(\"self\\n\", type(self))\n",
    "        print(\"config\\n\", config)\n",
    "        super(LlamaForCausalLM, self).__init__(config)\n",
    "        # LlamaForCausalLMのself.modelをLlavaLlamaModelに置き換える\n",
    "        self.model = LlavaLlamaModel(config)\n",
    "        print(\"self.model\\n\", self.model)\n",
    "        # LlavaLlamaModelの初期化のあと、LlavaBaseModelの初期化も呼ばれる。\n",
    "        self.pretraining_tp = config.pretraining_tp\n",
    "        \"\"\"\n",
    "        self.model\n",
    "        LlavaLlamaModel(\n",
    "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
    "        (layers): ModuleList(\n",
    "            (0-31): 32 x LlamaDecoderLayer(\n",
    "            (self_attn): LlamaAttention(\n",
    "                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "                (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "                (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "                (rotary_emb): LlamaRotaryEmbedding()\n",
    "            )\n",
    "            (mlp): LlamaMLP(\n",
    "                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
    "                (act_fn): SiLUActivation()\n",
    "            )\n",
    "            (input_layernorm): LlamaRMSNorm()\n",
    "            (post_attention_layernorm): LlamaRMSNorm()\n",
    "            )\n",
    "        )\n",
    "        (norm): LlamaRMSNorm()\n",
    "        )\n",
    "        \"\"\"\n",
    "        #print(\"self.pretraining_tp\\n\", self.pretraining_tp) # 1\n",
    "        #print(\"self.vocab_size\\n\", self.vocab_size) # 32_000\n",
    "        #print(\"self.lm_head\\n\", self.lm_head) # Linear(in_features=4096, out_features=32000, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        #self.post_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "RlMP4qUg0QtV",
    "outputId": "12f73ecd-9922-4d31-a0cf-a0e8bc5d88fb"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from transformers import AutoConfig\n",
    "\n",
    "# まず config.json をロードして Config クラスを自動判別\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir\n",
    ")\n",
    "\n",
    "print(\"model_args.model_name_or_path\\n\", model_args.model_name_or_path)\n",
    "print(\"training_args.cache_dir\\n\", training_args.cache_dir)\n",
    "print(\"\")\n",
    "print(\"Loaded config:\\n\", config)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QwLz_ubtVlti",
    "outputId": "680f292f-9756-45d4-cb80-af9692475a34"
   },
   "outputs": [],
   "source": [
    "print_mro(LlavaBaseForCausalLM)\n",
    "print_mro(LlavaLlamaForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "UJE07TY6Jb5K",
    "outputId": "091b04d2-adbc-49c3-ef2a-50181c53f793"
   },
   "outputs": [],
   "source": [
    "LlavaLlamaForCausalLM_model = LlavaLlamaForCausalLM.from_pretrained(model_args.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wodQZ9rWSyZN"
   },
   "outputs": [],
   "source": [
    "print(\"LlavaLlamaForCausalLM_model\\n\", LlavaLlamaForCausalLM_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5c1aEs4gf5L"
   },
   "outputs": [],
   "source": [
    "LlavaLlamaForCausalLM_model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9az2bPNccDC"
   },
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from typing import List\n",
    "from enum import auto, Enum\n",
    "\n",
    "class SeparatorStyle(Enum):\n",
    "    \"\"\"Different separator style.\"\"\"\n",
    "    SINGLE = auto()\n",
    "    TWO = auto()\n",
    "    MPT = auto()\n",
    "    PLAIN = auto()\n",
    "    LLAMA_2 = auto()\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Conversation:\n",
    "    \"\"\"A class that keeps all conversation history.\"\"\"\n",
    "    system: str\n",
    "    roles: List[str]\n",
    "    messages: List[List[str]]\n",
    "    offset: int\n",
    "    sep_style: SeparatorStyle = SeparatorStyle.SINGLE\n",
    "    sep: str = \"###\"\n",
    "    sep2: str = None\n",
    "    version: str = \"Unknown\"\n",
    "    skip_next: bool = False\n",
    "\n",
    "\n",
    "conv_llava_plain = Conversation(\n",
    "    system=\"\",\n",
    "    roles=(\"\", \"\"),\n",
    "    messages=(\n",
    "    ),\n",
    "    offset=0,\n",
    "    sep_style=SeparatorStyle.PLAIN,\n",
    "    sep=\"\\n\",\n",
    ")\n",
    "\n",
    "\n",
    "conv_templates = {\n",
    "    \"plain\": conv_llava_plain,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSs2_yNVOpOD"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import inspect\n",
    "print(inspect.getattr_static(LlamaForCausalLM, \"from_pretrained\"))\n",
    "print(inspect.getattr_static(LlamaForCausalLM, \"enable_input_require_grads\"))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_args.model_name_or_path)\n",
    "print(training_args.cache_dir)\n",
    "print(training_args.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "od0OYu7yBesv"
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    model_max_length=training_args.model_max_length, \n",
    "    padding_side=\"right\", # 実際にパディングを行うのは tokenizer か DataCollator のどちらかだが、padding_side 自体はtokenizer側で行う\n",
    "    use_fast=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M71wf9CYD_Y7"
   },
   "outputs": [],
   "source": [
    "print(\"pad_token:\", tokenizer.pad_token)\n",
    "print(\"pad_token_id:\", tokenizer.pad_token_id)\n",
    "print(\"unk_token:\", tokenizer.unk_token) # unkownの略です。類似の単語と間違えないでください。\n",
    "print(\"unk_token_id:\", tokenizer.unk_token_id)\n",
    "print(\"tokenizer\\n\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TrZtJFODFWz"
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vbf0h2NSDoUG"
   },
   "outputs": [],
   "source": [
    "print(\"pad_token:\", tokenizer.pad_token)\n",
    "print(\"pad_token_id:\", tokenizer.pad_token_id)\n",
    "print(\"unk_token:\", tokenizer.unk_token)\n",
    "print(\"unk_token_id:\", tokenizer.unk_token_id)\n",
    "print(\"tokenizer\\n\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGtPYpfKDOKW"
   },
   "outputs": [],
   "source": [
    "print(\"model_args.version\\n\", model_args.version)\n",
    "default_conversation = conv_templates[model_args.version]\n",
    "print(\"default_conversation\\n\", default_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csi6Xe_-G0_G"
   },
   "outputs": [],
   "source": [
    "print(\"model_args.vision_tower\\n\", model_args.vision_tower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpri2EiEIatU"
   },
   "outputs": [],
   "source": [
    "# LlavaLlamaForCausalLMの関数。LlavaLlamaModelをgetする\n",
    "def get_model(self):\n",
    "\n",
    "    print(\"current file path\", \"llava/llava/model/language_model/llava_llama.py\")\n",
    "    print(\"def LlavaLlamaForCausalLM.get_model(self)\")\n",
    "    print(\"self\\n\", type(self))\n",
    "    print(\"self.model (return)\\n\", self.model)\n",
    "    return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RPVqYUnjJHoF"
   },
   "outputs": [],
   "source": [
    "LlavaLlamaForCausalLM.get_model = get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iqqp22MDJSJ-"
   },
   "outputs": [],
   "source": [
    "LlavaLlamaModel_model = LlavaLlamaForCausalLM_model.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4c_UQLwdPhjd"
   },
   "outputs": [],
   "source": [
    "def config(self):\n",
    "\n",
    "    print(\"current file path\", \"llava/llava/model/multimodal_encoder/clip_encoder.py\")\n",
    "    print(\"def CLIPVisionTower.config(self)\")\n",
    "    print(\"self\\n\", type(self))\n",
    "    print(\"self.is_loaded\\n\", self.is_loaded) # True\n",
    "    print(f\"【COND】 is_loaded={self.is_loaded}\")\n",
    "    if self.is_loaded:\n",
    "        # 【ENTER】\n",
    "        print(\"【ENTER】if self.is_loaded:\")\n",
    "        result = self.vision_tower.config\n",
    "        print(\"result (return)\\n\", type(result))\n",
    "        print(\"【EXIT】if self.is_loaded:\")\n",
    "    else:\n",
    "      pass\n",
    "    print(\"result (return)\\n\", result)\n",
    "    \"\"\"\n",
    "    CLIPVisionConfig {\n",
    "    \"_name_or_path\": \"openai/clip-vit-large-patch14-336\",\n",
    "    \"attention_dropout\": 0.0,\n",
    "    \"dropout\": 0.0,\n",
    "    \"hidden_act\": \"quick_gelu\",\n",
    "    \"hidden_size\": 1024,\n",
    "    \"image_size\": 336,\n",
    "    \"initializer_factor\": 1.0,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 4096,\n",
    "    \"layer_norm_eps\": 1e-05,\n",
    "    \"model_type\": \"clip_vision_model\",\n",
    "    \"num_attention_heads\": 16,\n",
    "    \"num_channels\": 3,\n",
    "    \"num_hidden_layers\": 24,\n",
    "    \"patch_size\": 14,\n",
    "    \"projection_dim\": 768,\n",
    "    \"transformers_version\": \"4.31.0\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XG8ocYNiPpGB"
   },
   "outputs": [],
   "source": [
    "CLIPVisionTower.config = property(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrBNMbAm39zA"
   },
   "outputs": [],
   "source": [
    "def initialize_vision_modules(self, model_args, fsdp=None):\n",
    "\n",
    "  print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "  print(\"def initialize_vision_modules(self, model_args, fsdp=None)\")\n",
    "  print(\"model_args\\n\", model_args) #  ModelArguments(model_name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch')\n",
    "  print(\"fsdp\\n\", fsdp) # []\n",
    "  vision_tower = model_args.vision_tower\n",
    "  print(\"vision_tower from model_args\\n\", vision_tower) # openai/clip-vit-large-patch14-336\n",
    "  mm_vision_select_layer = model_args.mm_vision_select_layer\n",
    "  print(\"mm_vision_select_layer from model_args\\n\", mm_vision_select_layer) # -2\n",
    "  mm_vision_select_feature = model_args.mm_vision_select_feature\n",
    "  print(\"mm_vision_select_feature from model_args\\n\", mm_vision_select_feature) # patch\n",
    "  pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n",
    "  print(\"pretrain_mm_mlp_adapter from model_args\\n\", pretrain_mm_mlp_adapter) # None\n",
    "  mm_patch_merge_type = model_args.mm_patch_merge_type\n",
    "  # 下記はself.config.mm_vision_towerに関するもの。self.vision_towerは依然としてNone\n",
    "  self.config.mm_vision_tower = vision_tower\n",
    "  print(\"self.config.mm_vision_tower\\n\", self.config.mm_vision_tower) # None\n",
    "\n",
    "  print(\"【COND】 self.get_vision_tower()\\n\", self.get_vision_tower()) # None\n",
    "  print(f\"【COND】 get_vision_tower_is_None={self.get_vision_tower() is None}\")\n",
    "  if self.get_vision_tower() is None:\n",
    "      #【ENTER】self.vision_tower, self.get_vision_towerはNoneなのでこの分岐に入る。\n",
    "      print(\"【ENTER】if self.get_vision_tower() is None:\")\n",
    "      print(\"[ENTER] self.get_vision_tower() is None\")\n",
    "      # build_vision_tower(model_args) はちょっと奥の依存関係が深い\n",
    "      vision_tower = build_vision_tower(model_args)\n",
    "      print(\"vision_tower after build_vision_tower\\n\", vision_tower)\n",
    "      \"\"\"\n",
    "      CLIPVisionTower(\n",
    "      (vision_tower): CLIPVisionModel(\n",
    "      (vision_model): CLIPVisionTransformer(\n",
    "          (embeddings): CLIPVisionEmbeddings(\n",
    "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "          (position_embedding): Embedding(577, 1024)\n",
    "          )\n",
    "          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "          (encoder): CLIPEncoder(\n",
    "          (layers): ModuleList(\n",
    "              (0-23): 24 x CLIPEncoderLayer(\n",
    "              (self_attn): CLIPAttention(\n",
    "                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "              )\n",
    "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "              (mlp): CLIPMLP(\n",
    "                  (activation_fn): QuickGELUActivation()\n",
    "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "              )\n",
    "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "              )\n",
    "          )\n",
    "          )\n",
    "          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "      )\n",
    "      )\n",
    "      )\n",
    "      \"\"\"\n",
    "      # 分散学習(FSDP)を使うかどうか. 今回は [] 空のリストとなるので、Noneではないが、len(fsdp) == 0\n",
    "      print(\"【COND】 fsdp\\n\", fsdp) # []\n",
    "      print(f\"【COND】 fsdp_is_not_None={fsdp is not None} len_fsdp={len(fsdp) if fsdp is not None else 'N/A'}\") # fsdp_is_not_None=True len_fsdp=0\n",
    "      if fsdp is not None and len(fsdp) > 0:\n",
    "        pass\n",
    "      else:\n",
    "          # 【ENTER】else of if fsdp is not None and len(fsdp) > 0:\n",
    "          print(\"【COND】 else_fsdp_is_not_None_and_len_fsdp_gt_0=True\")\n",
    "          print(\"【ENTER】else of if fsdp is not None and len(fsdp) > 0:\")\n",
    "          self.vision_tower = vision_tower\n",
    "          print(\"self.vision_tower\\n\", self.vision_tower)\n",
    "          \"\"\"\n",
    "          CLIPVisionTower(\n",
    "          (vision_tower): CLIPVisionModel(\n",
    "              (vision_model): CLIPVisionTransformer(\n",
    "              (embeddings): CLIPVisionEmbeddings(\n",
    "                  (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "                  (position_embedding): Embedding(577, 1024)\n",
    "              )\n",
    "              (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "              (encoder): CLIPEncoder(\n",
    "                  (layers): ModuleList(\n",
    "                  (0-23): 24 x CLIPEncoderLayer(\n",
    "                      (self_attn): CLIPAttention(\n",
    "                      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                      )\n",
    "                      (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                      (mlp): CLIPMLP(\n",
    "                      (activation_fn): QuickGELUActivation()\n",
    "                      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "                      )\n",
    "                      (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                  )\n",
    "                  )\n",
    "              )\n",
    "              (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "              )\n",
    "          )\n",
    "          )\n",
    "          \"\"\"\n",
    "          print(\"【EXIT】else of if fsdp is not None and len(fsdp) > 0:\")\n",
    "\n",
    "      print(\"【EXIT】if self.get_vision_tower() is None:\")\n",
    "  else:\n",
    "    pass\n",
    "\n",
    "  self.config.use_mm_proj = True\n",
    "  print(\"self.config.use_mm_proj set to True\") # True\n",
    "  self.config.mm_projector_type = getattr(model_args, 'mm_projector_type', 'linear')\n",
    "  print(\"self.config.mm_projector_type\\n\", self.config.mm_projector_type) # mlp2x_gelu\n",
    "  self.config.mm_hidden_size = vision_tower.config.hidden_size\n",
    "  print(\"self.config.mm_hidden_size\\n\", self.config.mm_hidden_size) # 1024\n",
    "  self.config.mm_vision_select_layer = mm_vision_select_layer\n",
    "  print(\"self.config.mm_vision_select_layer\\n\", self.config.mm_vision_select_layer) # -2\n",
    "  self.config.mm_vision_select_feature = mm_vision_select_feature\n",
    "  print(\"self.config.mm_vision_select_feature\\n\", self.config.mm_vision_select_feature) # patch\n",
    "  self.config.mm_patch_merge_type = mm_patch_merge_type\n",
    "  print(\"self.config.mm_patch_merge_type\\n\", self.config.mm_patch_merge_type) # flat\n",
    "\n",
    "  # mm_projector_is_None=True\n",
    "  print(f\"【COND】 mm_projector_is_None={getattr(self, 'mm_projector', None) is None}\")\n",
    "  if getattr(self, 'mm_projector', None) is None:\n",
    "      # 【ENTER】\n",
    "      print(\"【ENTER】if getattr(self, 'mm_projector', None) is None:\")\n",
    "      self.mm_projector = build_vision_projector(self.config)\n",
    "      \"\"\"\n",
    "      Sequential(\n",
    "        (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
    "        (1): GELU(approximate='none')\n",
    "        (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
    "      )\n",
    "      \"\"\"\n",
    "      print(\"self.mm_projector after build_vision_projector\\n\", self.mm_projector)\n",
    "      print(\"mm_patch_merge_type\\n\", mm_patch_merge_type) # flat\n",
    "      print(f\"【COND】 unpad_in_mm_patch_merge_type={'unpad' in mm_patch_merge_type}\")\n",
    "      if 'unpad' in mm_patch_merge_type:\n",
    "        pass\n",
    "      print(\"【EXIT】if getattr(self, 'mm_projector', None) is None:\")\n",
    "  else:\n",
    "    pass\n",
    "\n",
    "  print(f\"【COND】 pretrain_mm_mlp_adapter_is_not_None={pretrain_mm_mlp_adapter is not None}\")\n",
    "  if pretrain_mm_mlp_adapter is not None:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6_2dvDMkr20"
   },
   "outputs": [],
   "source": [
    "LlavaBaseModel.initialize_vision_modules = initialize_vision_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qT_vXMXBlP2a"
   },
   "outputs": [],
   "source": [
    "def get_vision_tower(self):\n",
    "\n",
    "    print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "    print(\"def get_vision_tower(self)\")\n",
    "    vision_tower = getattr(self, 'vision_tower', None)\n",
    "    print(\"vision_tower (raw)\\n\", vision_tower)\n",
    "    \"\"\"\n",
    "    CLIPVisionTower(\n",
    "    (vision_tower): CLIPVisionModel(\n",
    "        (vision_model): CLIPVisionTransformer(\n",
    "        (embeddings): CLIPVisionEmbeddings(\n",
    "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "            (position_embedding): Embedding(577, 1024)\n",
    "        )\n",
    "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        (encoder): CLIPEncoder(\n",
    "            (layers): ModuleList(\n",
    "            (0-23): 24 x CLIPEncoderLayer(\n",
    "                (self_attn): CLIPAttention(\n",
    "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                (mlp): CLIPMLP(\n",
    "                (activation_fn): QuickGELUActivation()\n",
    "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "            )\n",
    "            )\n",
    "        )\n",
    "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        )\n",
    "    )\n",
    "    )\n",
    "    \"\"\"\n",
    "    print(\"type(vision_tower)\\n\", type(vision_tower))\n",
    "    print(f\"【COND】 type_vision_tower_is_list={type(vision_tower) is list}\")  # False\n",
    "    if type(vision_tower) is list:\n",
    "        # 【SKIP】\n",
    "        print(\"【ENTER】if type(vision_tower) is list:\")\n",
    "        vision_tower = vision_tower[0]\n",
    "        print(\"【EXIT】if type(vision_tower) is list:\")\n",
    "    print(\"vision_tower (return)\\n\", vision_tower)\n",
    "    \"\"\"\n",
    "    vision_tower (return)\n",
    "    CLIPVisionTower(\n",
    "    (vision_tower): CLIPVisionModel(\n",
    "        (vision_model): CLIPVisionTransformer(\n",
    "        (embeddings): CLIPVisionEmbeddings(\n",
    "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
    "            (position_embedding): Embedding(577, 1024)\n",
    "        )\n",
    "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        (encoder): CLIPEncoder(\n",
    "            (layers): ModuleList(\n",
    "            (0-23): 24 x CLIPEncoderLayer(\n",
    "                (self_attn): CLIPAttention(\n",
    "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "                (mlp): CLIPMLP(\n",
    "                (activation_fn): QuickGELUActivation()\n",
    "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "                )\n",
    "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "            )\n",
    "            )\n",
    "        )\n",
    "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        )\n",
    "    )\n",
    "    )\n",
    "    \"\"\"\n",
    "    return vision_tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iabxtpWflUf9"
   },
   "outputs": [],
   "source": [
    "LlavaBaseModel.get_vision_tower = get_vision_tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RW9YLSmBT4FG"
   },
   "outputs": [],
   "source": [
    "LlavaLlamaModel_model.initialize_vision_modules(\n",
    "    model_args=model_args,\n",
    "    fsdp=training_args.fsdp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3-wDbzZfnfD"
   },
   "outputs": [],
   "source": [
    "vision_tower = LlavaLlamaForCausalLM_model.get_vision_tower()\n",
    "print(\"vision_tower\\n\", vision_tower)\n",
    "vision_tower.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)\n",
    "\n",
    "data_args.image_processor = vision_tower.image_processor\n",
    "print(\"data_args.image_processor\\n\", data_args.image_processor)\n",
    "data_args.is_multimodal = True\n",
    "print(\"data_args.is_multimodal\\n\", data_args.is_multimodal) # True\n",
    "\n",
    "LlavaLlamaForCausalLM_model.config.image_aspect_ratio = data_args.image_aspect_ratio\n",
    "print(\"model.config.image_aspect_ratio\\n\", LlavaLlamaForCausalLM_model.config.image_aspect_ratio) # square\n",
    "LlavaLlamaForCausalLM_model.config.tokenizer_padding_side = tokenizer.padding_side\n",
    "print(\"model.config.tokenizer_padding_side\\n\", LlavaLlamaForCausalLM_model.config.tokenizer_padding_side) # right\n",
    "LlavaLlamaForCausalLM_model.config.tokenizer_model_max_length = tokenizer.model_max_length\n",
    "print(\"model.config.tokenizer_model_max_length\\n\", LlavaLlamaForCausalLM_model.config.tokenizer_model_max_length) # 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBa5nAaWi1M7"
   },
   "outputs": [],
   "source": [
    "LlavaLlamaForCausalLM_model.config.tune_mm_mlp_adapter = model_args.tune_mm_mlp_adapter\n",
    "training_args.tune_mm_mlp_adapter = model_args.tune_mm_mlp_adapter\n",
    "print(f\"【COND】 tune_mm_mlp_adapter={model_args.tune_mm_mlp_adapter}\") # True\n",
    "if model_args.tune_mm_mlp_adapter:\n",
    "    # 【ENTER】 tune_mm_mlp_adapter=True なので、この分岐に入る\n",
    "    print(\"【ENTER】if model_args.tune_mm_mlp_adapter:\")\n",
    "    # モデル全体の全パラメータを「学習不可（requires_grad=False）」にする\n",
    "    # これで通常の重みは全て凍結される\n",
    "    LlavaLlamaForCausalLM_model.requires_grad_(False)\n",
    "    for p in LlavaLlamaForCausalLM_model.get_model().mm_projector.parameters():\n",
    "        # mm_projector（画像特徴量→テキスト特徴量への変換層）の全パラメータだけを「学習可能（requires_grad=True）」に戻す\n",
    "        # これで mm_projector のみ学習されることになる\n",
    "        print(\"model.get_model().mm_projector.parameters()\", LlavaLlamaForCausalLM_model.get_model().mm_projector.parameters())\n",
    "        p.requires_grad = True\n",
    "    print(\"【EXIT】if model_args.tune_mm_mlp_adapter:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lj1_WjcpsnXB"
   },
   "outputs": [],
   "source": [
    "LlavaLlamaForCausalLM_model.config.mm_use_im_start_end = model_args.mm_use_im_start_end\n",
    "data_args.mm_use_im_start_end = model_args.mm_use_im_start_end\n",
    "training_args.mm_use_im_start_end = model_args.mm_use_im_start_end\n",
    "\n",
    "LlavaLlamaForCausalLM_model.config.mm_projector_lr = training_args.mm_projector_lr\n",
    "print(\"training_args.mm_projector_lr\", training_args.mm_projector_lr)\n",
    "\n",
    "LlavaLlamaForCausalLM_model.config.mm_use_im_patch_token = model_args.mm_use_im_patch_token\n",
    "print(\"model_args.mm_use_im_patch_token\", model_args.mm_use_im_patch_token)\n",
    "#LlavaLlamaForCausalLM_model.initialize_vision_tokenizer(model_args, tokenizer=tokenizer)\n",
    "#print(\"【EXIT】if model_args.vision_tower is not None:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLUf4wqcxKI4"
   },
   "outputs": [],
   "source": [
    "def rank0_print(*args):\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def rank0_print(*args)\")\n",
    "    print(\"args\\n\", args) # ('Formatting inputs...Skip in lazy mode',)\n",
    "    if local_rank == 0:\n",
    "        print(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "839mYcaPw2CZ"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "# URL: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/dataset.py\n",
    "class LazySupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str,\n",
    "                 tokenizer: transformers.PreTrainedTokenizer,\n",
    "                 data_args: DataArguments):\n",
    "\n",
    "        print(\"current file path\", \"llava/train/train.py\")\n",
    "        print(\"def LazySupervisedDataset.__init__(self, data_path, tokenizer, data_args)\")\n",
    "        print(\"data_path\\n\", data_path) # /content/LLaVA/blip_laion_cc_sbu_1.json\n",
    "        print(\"tokenizer\\n\", type(tokenizer)) # <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
    "        print(\"data_args\\n\", data_args) # DataArguments(data_path='/content/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/LLaVA/images', image_aspect_ratio='square')\n",
    "        super(LazySupervisedDataset, self).__init__()\n",
    "        list_data_dict = json.load(open(data_path, \"r\"))\n",
    "        # 今回は1サンプルだけなのでprintしても危険ではない\n",
    "        print(\"list_data_dict\", list_data_dict)\n",
    "\n",
    "        rank0_print(\"Formatting inputs...Skip in lazy mode\") # Formatting inputs...Skip in lazy mode\n",
    "        self.tokenizer = tokenizer\n",
    "        print(\"self.tokenizer\\n\", self.tokenizer)\n",
    "        self.list_data_dict = list_data_dict\n",
    "        print(\"self.list_data_dict\\n\", self.list_data_dict)\n",
    "        self.data_args = data_args\n",
    "        print(\"self.data_args\\n\", self.data_args)\n",
    "    \n",
    "    def __len__(self):\n",
    "\n",
    "        print(\"current file path\", \"llava/train/train.py\")\n",
    "        print(\"def LazySupervisedDataset.__len__(self)\")\n",
    "        return len(self.list_data_dict)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mro(LazySupervisedDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# だいぶ間が空いているので復習\n",
    "\"\"\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    model_max_length=training_args.model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(\"tokenizer\\n\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LazySupervisedDataset(data_path=data_args.data_path, tokenizer=tokenizer, data_args=data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.list_data_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "data_sample = train_dataset.list_data_dict[i]\n",
    "print(\"data_sample\\n\", data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'image' in data_sample:\n",
    "    image_file = data_sample['image']\n",
    "    print(\"image_file\\n\", image_file)\n",
    "    image_folder = train_dataset.data_args.image_folder\n",
    "    print(\"image_folder\\n\", image_folder)\n",
    "    image_path = os.path.join(image_folder, image_file)\n",
    "    print(\"image_path\\n\", image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = train_dataset.data_args.image_processor\n",
    "print(\"image_processor\\n\", image_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL: https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/image_processing_clip.py\n",
    "image = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n",
    "print(\"image_tensor_shape\\n\", image.shape)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "data_sample = train_dataset.list_data_dict[i]\n",
    "\n",
    "if isinstance(i, int):\n",
    "    data_sample_list = [data_sample]\n",
    "if 'image' in data_sample:\n",
    "    image_file = data_sample['image']\n",
    "    image_folder = train_dataset.data_args.image_folder\n",
    "    image_path = os.path.join(image_folder, image_file)\n",
    "    image_processor = train_dataset.data_args.image_processor\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load image {image_path}: {e}\")\n",
    "        raise e\n",
    "\n",
    "    image = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0 \n",
    "import copy\n",
    "dialog_pair = copy.deepcopy(data_sample[\"conversations\"])\n",
    "print(\"dialog_pair\\n\", dialog_pair)\n",
    "# __getitem__ では 1サンプルずつ取り出すので、基本的にリストの長さは1\n",
    "# pair は1組だが、リストにした方が便利な時があるのでリスト形式に変換する。\n",
    "# リストといっても、リスト形式にしたという意味で、複数サンプルがあるわけではないので注意\n",
    "# dialog_pair 自体がリストなので、二重リストになる。\n",
    "dialog_pair_list = [dialog_pair]\n",
    "print(\"dialog_pair_list\\n\", dialog_pair_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 見づらいので、もう一度見やすく定義する\n",
    "dialog_pair_list = [[\n",
    "    {'from': 'human', 'value': 'Render a clear and concise summary of the photo.\\n<image>'},\n",
    "    {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}\n",
    "]]\n",
    "\n",
    "print(\"len(dialog_pair_list)\\n\", len(dialog_pair_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"data_args\\n\", data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_multimodal = data_args.is_multimodal\n",
    "print(\"is_multimodal\\n\", is_multimodal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_pair = dialog_pair_list[0]\n",
    "print(\"pair current loop\\n\", dialog_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_dialog = dialog_pair[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sentence['value']\\n\", human_dialog['value'])\n",
    "print(\"\\nDEFAULT_IMAGE_TOKEN\\n\", DEFAULT_IMAGE_TOKEN)\n",
    "print(\"【COND】 if DEFAULT_IMAGE_TOKEN in sentence['value']:\", DEFAULT_IMAGE_TOKEN in human_dialog['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip() は先頭と末尾の空白文字（スペース、タブ、改行など）をすべて削除するPythonの標準メソッドです。\n",
    "# repleace で <image> (DEFAULT_IMAGE_TOKEN) を空文字に置換したあとに、strip()を適用して削除する。\n",
    "\n",
    "human_dialog['value'] = human_dialog['value'].replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
    "print(\"human_dialog['value'] after remove DEFAULT_IMAGE_TOKEN\\n\", human_dialog['value'])\n",
    "print(\"===\")\n",
    "\n",
    "human_dialog['value'] = DEFAULT_IMAGE_TOKEN + '\\n' + human_dialog['value']\n",
    "print(\"human_dialog['value'] after adding DEFAULT_IMAGE_TOKEN to the front\\n\", human_dialog['value'])\n",
    "print(\"===\")\n",
    "\n",
    "human_dialog['value'] = human_dialog['value'].strip()\n",
    "print(\"human_dialog['value'] after strip()\\n\", human_dialog['value'])\n",
    "print(\"===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"human_dialog['value']\\n\", human_dialog['value'])\n",
    "print(\"\\nDEFAULT_IMAGE_TOKEN\\n\", DEFAULT_IMAGE_TOKEN)\n",
    "print(\"【COND】 if DEFAULT_IMAGE_TOKEN in human_dialog['value']:\", DEFAULT_IMAGE_TOKEN in human_dialog['value'])\n",
    "if DEFAULT_IMAGE_TOKEN in human_dialog['value']: # False\n",
    "    # 【SKIP】\n",
    "    human_dialog['value'] = human_dialog['value'].replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
    "    print(\"human_dialog['value'] after remove DEFAULT_IMAGE_TOKEN\\n\", human_dialog['value'])\n",
    "    print(\"===\")\n",
    "\n",
    "    human_dialog['value'] = DEFAULT_IMAGE_TOKEN + '\\n' + human_dialog['value']\n",
    "    print(\"human_dialog['value'] after adding DEFAULT_IMAGE_TOKEN to the front\\n\", human_dialog['value'])\n",
    "    print(\"===\")\n",
    "\n",
    "    human_dialog['value'] = human_dialog['value'].strip()\n",
    "    print(\"human_dialog['value'] after strip()\\n\", human_dialog['value'])\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_dialog = dialog_pair[1]\n",
    "print(\"gpt_dialog current loop\\n\", gpt_dialog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gpt_dialog['value']\\n\", gpt_dialog['value'])\n",
    "print(\"\\nDEFAULT_IMAGE_TOKEN\\n\", DEFAULT_IMAGE_TOKEN)\n",
    "print(\"【COND】 if DEFAULT_IMAGE_TOKEN in gpt_dialog['value']:\", DEFAULT_IMAGE_TOKEN in gpt_dialog['value'])\n",
    "if DEFAULT_IMAGE_TOKEN in gpt_dialog['value']: # False\n",
    "    # 【SKIP】\n",
    "    gpt_dialog['value'] = gpt_dialog['value'].replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
    "    print(\"gpt_dialog['value'] after remove DEFAULT_IMAGE_TOKEN\\n\", gpt_dialog['value'])\n",
    "    print(\"===\")\n",
    "\n",
    "    gpt_dialog['value'] = DEFAULT_IMAGE_TOKEN + '\\n' + gpt_dialog['value']\n",
    "    print(\"gpt_dialog['value'] after adding DEFAULT_IMAGE_TOKEN\\n\", gpt_dialog['value'])\n",
    "    print(\"===\")\n",
    "\n",
    "    gpt_dialog['value'] = gpt_dialog['value'].strip()\n",
    "    print(\"gpt_dialog['value'] after strip()\\n\", gpt_dialog['value'])\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from typing import Dict\n",
    "\n",
    "def preprocess_multimodal(\n",
    "    dialog_pair: Sequence[str],\n",
    "    data_args: DataArguments\n",
    ") -> Dict:\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def preprocess_multimodal(dialog_pair, data_args)\")\n",
    "    print(\"dialog_pair\\n\", dialog_pair) # [{'from': 'human', 'value': 'Give a brief description of the image.\\n<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]\n",
    "    print(\"data_args\\n\", data_args) # DataArguments(data_path='/content/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/LLaVA/images', image_aspect_ratio='square')\n",
    "    for dialog in dialog_pair:\n",
    "        if DEFAULT_IMAGE_TOKEN in dialog['value']:\n",
    "            dialog['value'] = dialog['value'].replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
    "            dialog['value'] = DEFAULT_IMAGE_TOKEN + '\\n' + dialog['value']\n",
    "            dialog['value'] = dialog['value'].strip()\n",
    "    return dialog_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_pair= [\n",
    "    {'from': 'human', 'value': 'Render a clear and concise summary of the photo.\\n<image>'},\n",
    "    {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_pair = preprocess_multimodal(dialog_pair, data_args)\n",
    "print(\"dialog_pair\\n\", dialog_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 他のサンプルでも試してみる\n",
    "dialog_pair = [\n",
    "    {'from': 'human', 'value': '<image>\\nShare a concise interpretation of the image provided.'},\n",
    "    {'from': 'gpt', 'value': 'water pollution in the city'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_pair = preprocess_multimodal(dialog_pair, data_args)\n",
    "print(\"dialog_pair\\n\", dialog_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hv3nmqgMwVWa"
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "def __len__(self):\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def LazySupervisedDataset.__len__(self)\")\n",
    "    return len(self.list_data_dict)\n",
    "LazySupervisedDataset.__len__ = __len__\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<image>illustration of a summer holiday in bright colors . \\n\"\n",
    "image_token_index = IMAGE_TOKEN_INDEX # -200\n",
    "return_tensors = \"pt\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "# 例: TinyLlama用\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "print(\"prompt:\", prompt)\n",
    "print(\"tokenizer:\", tokenizer)\n",
    "print(\"image_token_index:\", image_token_index)\n",
    "print(\"return_tensors:\", return_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_before_image, text_after_image = prompt.split(\"<image>\")\n",
    "print(\"text_before_image\\n\", text_before_image) # Nothing\n",
    "print(\"text_after_image\\n\", text_after_image) # caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer(chunk).input_ids の 1 は、<s> という、BOS (Beginning Of Sentence) トークンのIDです。\n",
    "# 全く何もない場合でも、<s> という 特殊トークンとして扱われます。\n",
    "ids_before_image = tokenizer(text_before_image).input_ids\n",
    "print(\"ids_before_image\\n\", ids_before_image)\n",
    "ids_after_image = tokenizer(text_after_image).input_ids\n",
    "print(\"ids_after_image\\n\", ids_after_image)\n",
    "\n",
    "# [テキスト前] + [画像トークン] + [テキスト後]\n",
    "ids_after_image_no_bos = ids_after_image[1:]\n",
    "input_ids = ids_before_image + [image_token_index] + ids_after_image_no_bos\n",
    "print(\"input_ids\\n\", input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if return_tensors == \"pt\":\n",
    "    tokenized_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "\n",
    "print(\"tokenized_ids\\n\", tokenized_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None):\n",
    "    # <image> の前後で分割\n",
    "    text_before_image, text_after_image = prompt.split(\"<image>\")\n",
    "\n",
    "    # トークナイズ\n",
    "    ids_before_image = tokenizer(text_before_image).input_ids\n",
    "    ids_after_image = tokenizer(text_after_image).input_ids\n",
    "    ids_after_image_no_bos = ids_after_image[1:]\n",
    "\n",
    "    # [テキスト前] + [画像トークン] + [テキスト後]\n",
    "    input_ids = ids_before_image + [image_token_index] + ids_after_image_no_bos\n",
    "\n",
    "    if return_tensors == \"pt\":\n",
    "        return torch.tensor(input_ids, dtype=torch.long)\n",
    "    return input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<image>illustration of a summer holiday in bright colors .\\n'\"\n",
    "image_token_index = IMAGE_TOKEN_INDEX # -200\n",
    "return_tensors = \"pt\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "# 例: TinyLlama用\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "print(\"prompt:\", prompt)\n",
    "print(\"tokenizer:\", tokenizer)\n",
    "print(\"image_token_index:\", image_token_index)\n",
    "print(\"return_tensors:\", return_tensors)\n",
    "\n",
    "tokenizer_image_token(prompt, tokenizer, image_token_index, return_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# tokenizer(chunk).input_ids の 1 は、<s> という、BOS (Beginning Of Sentence) トークンのIDです。\n",
    "# 全く何もない場合でも、<s> という 特殊トークンとして扱われます。\n",
    "for chunk in prompt.split(\"<image>\"):\n",
    "    print(\"chunk\\n\", chunk)\n",
    "    print(\"tokenizer(chunk)\\n\", tokenizer(chunk))\n",
    "    print(\"tokenizer(chunk).input_ids\\n\", tokenizer(chunk).input_ids)\n",
    "    print(\"\")\n",
    "\n",
    "# セル2：<image>で分割してトークン化\n",
    "prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split(\"<image>\")]\n",
    "print(\"prompt_chunks =\", prompt_chunks)\n",
    "\n",
    "print(\"prompt_chunks\\n\", prompt_chunks)\n",
    "print(\"prompt_chunks[0]\\n\", prompt_chunks[0])\n",
    "print(\"prompt_chunks[0][0]\\n\", prompt_chunks[0][0])\n",
    "\n",
    "# ======================\n",
    "# セル3：画像トークンを間に挟む処理（関数なし）\n",
    "# ======================\n",
    "input_ids = []\n",
    "offset = 0\n",
    "\n",
    "# 最初のチャンクがBOSトークンで始まっていたら、その分をスキップ\n",
    "if len(prompt_chunks) > 0 and len(prompt_chunks[0]) > 0 and prompt_chunks[0][0] == tokenizer.bos_token_id:\n",
    "    offset = 1\n",
    "    input_ids.append(prompt_chunks[0][0])\n",
    "print(\"offset =\", offset)\n",
    "print(\"input_ids\\n\", input_ids)\n",
    "\n",
    "# チャンクと画像トークンを交互に並べる\n",
    "for i, chunk in enumerate(prompt_chunks):\n",
    "    # チャンクの実体を追加\n",
    "    print(\"current chunk\\n\", chunk)\n",
    "    input_ids.extend(chunk[offset:])\n",
    "    print(\"input_ids\\n\", input_ids)\n",
    "    # 最後以外には画像トークンを挿入\n",
    "    if i < len(prompt_chunks) - 1: # len(prompt_chunks) -1  = 2 -1 = 1 より、i=0\n",
    "        print(\"i\", i)\n",
    "        input_ids.extend([image_token_index])\n",
    "        print(f\"--> insert image_token_index ({image_token_index}) between chunks\")\n",
    "\n",
    "print(\"統合後の input_ids:\", input_ids)\n",
    "\n",
    "\n",
    "# ======================\n",
    "# セル4：return_tensors の処理\n",
    "# ======================\n",
    "import torch\n",
    "\n",
    "if return_tensors == \"pt\":\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "\n",
    "print(\"最終出力:\")\n",
    "print(input_ids)\n",
    "\n",
    "# セル4：offset処理と最初の BOS トークン確認\n",
    "input_ids = []\n",
    "offset = 0\n",
    "\n",
    "if len(prompt_chunks) > 0 and len(prompt_chunks[0]) > 0 and prompt_chunks[0][0] == tokenizer.bos_token_id:\n",
    "    offset = 1\n",
    "    input_ids.append(prompt_chunks[0][0])\n",
    "\n",
    "print(\"offset =\", offset)\n",
    "print(\"初期input_ids =\", input_ids)\n",
    "\n",
    "# セル6：return_tensorsの処理\n",
    "import torch\n",
    "\n",
    "if return_tensors is not None:\n",
    "    if return_tensors == \"pt\":\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported tensor type: {return_tensors}\")\n",
    "\n",
    "print(\"最終出力:\")\n",
    "print(input_ids)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None):\n",
    "\n",
    "    print(\"current file path\", \"llava/mm_utils.py\")\n",
    "    print(\"def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None)\")\n",
    "    print(\"prompt\\n\", prompt) # <image>the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair\n",
    "    print(\"tokenizer\\n\", tokenizer) #  LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)\n",
    "    print(\"image_token_index\\n\", image_token_index) # -200\n",
    "    print(\"return_tensors\\n\", return_tensors) # pt\n",
    "    prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split('<image>')]\n",
    "\n",
    "    def insert_separator(X, sep):\n",
    "        return [ele for sublist in zip(X, [sep]*len(X)) for ele in sublist][:-1]\n",
    "\n",
    "    input_ids = []\n",
    "    offset = 0\n",
    "    if len(prompt_chunks) > 0 and len(prompt_chunks[0]) > 0 and prompt_chunks[0][0] == tokenizer.bos_token_id:\n",
    "        offset = 1\n",
    "        input_ids.append(prompt_chunks[0][0])\n",
    "\n",
    "    for x in insert_separator(prompt_chunks, [image_token_index] * (offset + 1)):\n",
    "        input_ids.extend(x[offset:])\n",
    "\n",
    "    if return_tensors is not None:\n",
    "        if return_tensors == 'pt':\n",
    "            return torch.tensor(input_ids, dtype=torch.long)\n",
    "        raise ValueError(f'Unsupported tensor type: {return_tensors}')\n",
    "    print(\"input_ids (return)\\n\", input_ids)\n",
    "    return input_ids\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def preprocess_plain(\n",
    "    dialog_pair: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def preprocess_plain(dialog_pair, tokenizer)\")\n",
    "    print(\"dialog_pair\\n\", dialog_pair) # [[{'from': 'human', 'value': '<image>\\nGive a brief description of the image.'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]\n",
    "    print(\"tokenizer\\n\", type(tokenizer)) # <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
    "    # add end signal and concatenate together\n",
    "\n",
    "    human_dialog = dialog_pair[0]['value']\n",
    "    print(\"human_dialog\\n\", human_dialog) # <image>\\nGive a brief description of the image.\n",
    "    gpt_dialog = dialog_pair[1]['value']\n",
    "    print(\"gpt_dialog\\n\", gpt_dialog) # the divine queen in her elaborate\n",
    "    default_conversation = conv_templates[\"plain\"] # hard coding\n",
    "    sep_token = default_conversation.sep # separator_token \"\\n\"\n",
    "\n",
    "    prompt = DEFAULT_IMAGE_TOKEN + gpt_dialog + sep_token\n",
    "    print(\"prompt\\n\", prompt)\n",
    "    input_ids = tokenizer_image_token(prompt=prompt, tokenizer=tokenizer, return_tensors='pt')\n",
    "    target_ids = copy.deepcopy(input_ids)\n",
    "\n",
    "\n",
    "    tokenized_len = len(tokenizer_image_token(DEFAULT_IMAGE_TOKEN, tokenizer)) # prompt <image>\n",
    "    print(\"tokenized_len\\n\", tokenized_len) # 2\n",
    "    target_ids[:tokenized_len] = IGNORE_INDEX\n",
    "\n",
    "    # list処理はその後に重複があるのでコメントアウト\n",
    "    #input_ids_list = [input_ids]\n",
    "    #target_ids_list = [target_ids]\n",
    "\n",
    "    data_dict = dict(input_ids=input_ids, labels=target_ids)\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    has_image: bool = False\n",
    ") -> Dict:\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def preprocess(sources, tokenizer, has_image=False)\")\n",
    "    print(\"sources\\n\", sources) # [[{'from': 'human', 'value': '<image>\\nGive a brief description of the image.'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]\n",
    "    print(\"tokenizer\\n\", type(tokenizer)) # <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
    "    print(\"has_image\\n\", has_image) # True\n",
    "    \n",
    "    # Given a list of sources, each is a conversation list. This transform:\n",
    "    # 1. Add signal '### ' at the beginning each sentence, with end signal '\\n';\n",
    "    # 2. Concatenate conversations together;\n",
    "    # 3. Tokenize the concatenated conversation;\n",
    "    # 4. Make a deepcopy as the target. Mask human words with IGNORE_INDEX.\n",
    "    if default_conversation.sep_style == SeparatorStyle.PLAIN:\n",
    "        return preprocess_plain(sources, tokenizer) # True\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_pair = [\n",
    "    {'from': 'human', 'value': '<image>\\nRender a clear and concise summary of the photo.'},\n",
    "    {'from': 'gpt', 'value': 'illustration of a summer holiday in bright colors .'}\n",
    "]\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "# 例: TinyLlama用\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "data_dict = preprocess_plain(dialog_pair, tokenizer)\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from PIL import Image\n",
    "\n",
    "# Trainer > def _get_dataloader > dataloader = self.accelerator.prepare(DataLoader(dataset, **dataloader_params))\n",
    "def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def LazySupervisedDataset.__getitem__(self, i)\")\n",
    "    print(\"i\\n\", i) # 0\n",
    "    data_sample = self.list_data_dict[i]\n",
    "    dialog_pair = copy.deepcopy(data_sample[\"conversations\"])\n",
    "    \n",
    "    # 画像+テキストのサンプル\n",
    "    if 'image' in data_sample:\n",
    "        image_file = data_sample['image']\n",
    "        image_folder = self.data_args.image_folder\n",
    "        image_processor = self.data_args.image_processor      \n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        print(\"image_path\\n\", image_path)\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image: {e}\")\n",
    "            # 画像ファイルのパスがなければこのサンプルは無効としてスキップ\n",
    "            print(\"Skipping this sample due to image loading error.\")\n",
    "            return None\n",
    "        image = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n",
    "        dialog_pair = preprocess_multimodal(dialog_pair, self.data_args)\n",
    "        data_dict = preprocess_plain(dialog_pair, self.tokenizer)\n",
    "        data_dict['image'] = image\n",
    "    # テキストのみのサンプル\n",
    "    elif self.data_args.is_multimodal is False:\n",
    "        dialog_pair = copy.deepcopy(data_sample[\"conversations\"])\n",
    "        data_dict = preprocess_plain(dialog_pair, self.tokenizer)\n",
    "    # image does not exist in the data, but the data_args requires multimodal\n",
    "    else:\n",
    "        # 真っ黒の画像をダミー画像として作成する\n",
    "        crop_size = self.data_args.image_processor.crop_size\n",
    "        dummy_black_image = torch.zeros(3, crop_size['height'], crop_size['width'])\n",
    "\n",
    "        dialog_pair = copy.deepcopy(data_sample[\"conversations\"])\n",
    "        data_dict = preprocess_plain(dialog_pair, self.tokenizer)\n",
    "        data_dict['image'] = dummy_black_image\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LazySupervisedDataset.__getitem__ = __getitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import copy\n",
    "\n",
    "def preprocess_plain(s\n",
    "    sources: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def preprocess_plain(sources, tokenizer)\")\n",
    "    print(\"sources\\n\", sources) # [[{'from': 'human', 'value': '<image>\\nGive a brief description of the image.'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]\n",
    "    print(\"tokenizer\\n\", type(tokenizer)) # <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
    "    # add end signal and concatenate together\n",
    "    conversations = []\n",
    "    print(\"conversations initial\\n\", conversations) # []\n",
    "    for source in sources:\n",
    "        print(\"source current loop\\n\", source) \n",
    "        assert len(source) == 2\n",
    "        assert DEFAULT_IMAGE_TOKEN in source[0]['value']\n",
    "        source[0]['value'] = DEFAULT_IMAGE_TOKEN\n",
    "        conversation = source[0]['value'] + source[1]['value'] + default_conversation.sep\n",
    "        print(\"conversation current loop\\n\", conversation)\n",
    "        conversations.append(conversation)\n",
    "    print(\"conversations (final)\\n\", conversations) #  ['<image>the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair\\n']\n",
    "    # tokenize conversations\n",
    "    input_ids = [tokenizer_image_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations]\n",
    "    print(\"input_ids\\n\", input_ids) # [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114, 411,  2654, 11315,    13])]\n",
    "    for idx, tensor in enumerate(input_ids):\n",
    "        if hasattr(tensor, 'shape'):\n",
    "            print(f\"input_ids[{idx}].shape\\n\", tensor.shape) # torch.Size([24])\n",
    "    targets = copy.deepcopy(input_ids)\n",
    "    print(\"targets\\n\", targets) # [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114, 411,  2654, 11315,    13])]\n",
    "    for idx, tensor in enumerate(targets):\n",
    "        if hasattr(tensor, 'shape'):\n",
    "            print(f\"targets[{idx}].shape\\n\", tensor.shape) # torch.Size([24])\n",
    "    print(\"sources\\n\", sources) # [[{'from': 'human', 'value': '<image>'}, {'from': 'gpt', 'value': 'the divine queen in her elaborate masks canvas print featuring the face and hands of a woman with red hair'}]]\n",
    "    for target, source in zip(targets, sources):\n",
    "        tokenized_len = len(tokenizer_image_token(source[0]['value'], tokenizer)) # prompt <image>\n",
    "        target[:tokenized_len] = IGNORE_INDEX\n",
    "\n",
    "    print(\"input_ids (return)\\n\", input_ids) # [tensor([    1,  -200,   278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114, 411,  2654, 11315,    13])]\n",
    "    print(\"targets (return)\\n\", targets) #  [tensor([ -100,  -100,   278, 25616, 26624,   297,   902, 19430, 11105, 29879, 10508,  1596, 23425,   278,  3700,   322,  6567,   310,   263,  6114, 411,  2654, 11315,    13])]\n",
    "    return dict(input_ids=input_ids, labels=targets)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dict0 = train_dataset.__getitem__(0)\n",
    "sample_dict1 = train_dataset.__getitem__(1)\n",
    "instances = [sample_dict0, sample_dict1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "# 擬似データの input_ids, labels\n",
    "input_ids0 = torch.tensor([1, -200, 4094, 21180, 918, 297, 278, 4272, 13])\n",
    "labels0 = torch.tensor([-100, -100, 4094, 21180, 918, 297, 278, 4272, 13])\n",
    "image0 = torch.randn(3, 336, 336)\n",
    "\n",
    "input_ids1 = torch.tensor([1, -200, 8632, 362, 310, 263, 11801, 8753, 22394, 297, 11785, 11955, 869, 13])\n",
    "labels1 = torch.tensor([-100, -100, 8632, 362, 310, 263, 11801, 8753, 22394, 297, 11785, 11955, 869, 13])\n",
    "image1 = torch.randn(3, 336, 336)\n",
    "\n",
    "instances = [\n",
    "    {\n",
    "        \"input_ids\": input_ids0,\n",
    "        \"labels\": labels0,\n",
    "        \"image\": image0\n",
    "    },\n",
    "    {\n",
    "        \"input_ids\": input_ids1 ,\n",
    "        \"labels\": labels1,\n",
    "        \"image\": image1\n",
    "    }\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 例: TinyLlama用\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "def custom_decode(input_ids, tokenizer):\n",
    "    # -200 など特殊トークンは手動で置換\n",
    "    tokens = []\n",
    "    for id in input_ids.tolist():\n",
    "        if id == -200:\n",
    "            tokens.append(\"<image>\")\n",
    "        elif id == -100:\n",
    "            tokens.append(\"<IGNORE>\")\n",
    "        else:\n",
    "            tokens.append(tokenizer.decode([id], skip_special_tokens=False))\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "input_ids0 = instances[0]['input_ids']\n",
    "input_ids1 = instances[1]['input_ids']\n",
    "print(\"input_ids0:\", custom_decode(input_ids0, tokenizer))\n",
    "print(\"input_ids1:\", custom_decode(input_ids1, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データローダーが None を返すことがあるので、Noneのサンプルを除外。\n",
    "instances = [instance for instance in instances if instance is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, labels = tuple([instance[key] for instance in instances] \n",
    "                          for key in (\"input_ids\", \"labels\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"input_ids\\n\", input_ids)\n",
    "print(\"labels\\n\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_idsはtokenizerのpad_token_id(0)でパディング\n",
    "print(\"tokenizer.pad_token_id\\n\", tokenizer.pad_token_id)\n",
    "input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "    input_ids,\n",
    "    batch_first=True,\n",
    "    padding_value=tokenizer.pad_token_id\n",
    ")\n",
    "print(\"input_ids\\n\", input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelsはIGNORE_INDEX(-100)でパディング\n",
    "print(\"IGNORE_INDEX\\n\", IGNORE_INDEX)\n",
    "labels = torch.nn.utils.rnn.pad_sequence(\n",
    "    labels,\n",
    "    batch_first=True,\n",
    "    padding_value=IGNORE_INDEX\n",
    ")\n",
    "print(\"labels\\n\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = input_ids[:, :tokenizer.model_max_length]\n",
    "labels = labels[:, :tokenizer.model_max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"input_ids\\n\", input_ids)\n",
    "print(\"labels\\n\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask=input_ids.ne(tokenizer.pad_token_id)\n",
    "print(\"attention_mask\\n\", attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = dict(\n",
    "    input_ids=input_ids,\n",
    "    labels=labels,\n",
    "    attention_mask=attention_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'image' in instances[0]:\n",
    "    # 画像リストを抽出\n",
    "    images = [instance['image'] for instance in instances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_same_shape = all(\n",
    "    x is not None and x.shape == images[0].shape for x in images\n",
    ")\n",
    "\n",
    "all_same_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像をバッチ化 or リストのまま格納\n",
    "if all_same_shape:\n",
    "    batch['images'] = torch.stack(images)\n",
    "else:\n",
    "    # リストのまま渡すと困るのではないか？この疑問は未解決\n",
    "    batch['images'] = images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像のshapeを出力\n",
    "if isinstance(batch['images'], torch.Tensor):\n",
    "    print(\"batch['images'].shape\\n\", batch['images'].shape)\n",
    "else:\n",
    "    print(\"batch['images'] is a list, first shape:\", images[0].shape if images else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_collator とは、複数のサンプルをまとめてデータをバッチ化するもの、という意味です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2Th7s50xen5"
   },
   "outputs": [],
   "source": [
    "# Trainer > def _get_dataloader > dataloader_params = {...\"collate_fn\": data_collator,...}\n",
    "# self.accelerator.prepare(DataLoader(dataset, **dataloader_params)) で呼ばれる\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        print(\"current file path\", \"llava/train/train.py\")\n",
    "        print(\"def DataCollatorForSupervisedDataset.__call__(self, instances)\")\n",
    "        print(\"instances\\n\", instances)\n",
    "        #  [(torch.Size([24]), torch.Size([24]), torch.Size([3, 336, 336]))]\n",
    "        print(\"shape of each instance's input_ids and labels, and images(if any):\", [(x['input_ids'].shape, x['labels'].shape, x.get('image', None).shape if 'image' in x else None) for x in instances])\n",
    "        # データローダーが None を返すことがあるので、Noneのサンプルを除外。\n",
    "        instances = [x for x in instances if x is not None]\n",
    "        # input_idsとlabelsのそれぞれについてリストを作成。タプルをつくる。\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances]\n",
    "                                  for key in (\"input_ids\", \"labels\"))\n",
    "        # input_idsはtokenizerのpad_token_id(0)でパディング\n",
    "        print(\"self.tokenizer.pad_token_id\\n\", self.tokenizer.pad_token_id)\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids,\n",
    "            batch_first=True,\n",
    "            padding_value=self.tokenizer.pad_token_id)\n",
    "        # labelsはIGNORE_INDEX(-100)でパディング\n",
    "        print(\"IGNORE_INDEX\\n\", IGNORE_INDEX)\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels,\n",
    "                                                 batch_first=True,\n",
    "                                                 padding_value=IGNORE_INDEX)\n",
    "        input_ids = input_ids[:, :self.tokenizer.model_max_length]\n",
    "        print(\"input_ids.shape (after pad_sequence and truncate)\\n\", input_ids.shape)\n",
    "        print(\"input_ids (after pad_sequence and truncate)\\n\", input_ids)\n",
    "        labels = labels[:, :self.tokenizer.model_max_length]\n",
    "        print(\"labels.shape (after pad_sequence and truncate)\\n\", labels.shape)\n",
    "        print(\"labels (after pad_sequence and truncate)\\n\", labels)\n",
    "        # .ne() は \"not equal\" → pad_token_id(=0) じゃない部分を 1、pad 部分を 0 にする。モデルが pad 部分を読まないように制御するマスクです。\n",
    "        attention_mask=input_ids.ne(self.tokenizer.pad_token_id)\n",
    "        \n",
    "        batch = dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        if 'image' in instances[0]:\n",
    "            images = [instance['image'] for instance in instances]\n",
    "            if all(x is not None and x.shape == images[0].shape for x in images):\n",
    "                batch['images'] = torch.stack(images)\n",
    "            else:\n",
    "                batch['images'] = images\n",
    "            print(\"batch['images'].shape\\n\", batch['images'].shape)\n",
    "        \n",
    "        print(\"batch (return)\\n\", batch)\n",
    "        print(\"shape of each batch's input_ids and labels, and images(if any):\", [(batch['input_ids'].shape, batch['labels'].shape, batch.get('images', None).shape if 'images' in batch else None)])\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QjNj6B85wWA6"
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer,\n",
    "                                data_args) -> Dict:\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def make_supervised_data_module(tokenizer, data_args)\")\n",
    "    print(\"tokenizer\\n\", type(tokenizer))\n",
    "    print(\"data_args\\n\", data_args) #  DataArguments(data_path='/content/LLaVA/blip_laion_cc_sbu_1.json', lazy_preprocess=True, is_multimodal=True, image_folder='/content/LLaVA/images', image_aspect_ratio='square')\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    train_dataset = LazySupervisedDataset(tokenizer=tokenizer,\n",
    "                                data_path=data_args.data_path,\n",
    "                                data_args=data_args)\n",
    "    print(\"train_dataset\\n\", train_dataset) # <llava.train.train.LazySupervisedDataset object at 0x7ed6341f4880>\n",
    "    print(\"len(train_dataset)\\n\", len(train_dataset)) # 1\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "    print(\"data_collator\\n\", data_collator) # DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))\n",
    "    data_module = dict(train_dataset=train_dataset,\n",
    "                  eval_dataset=None,\n",
    "                  data_collator=data_collator)\n",
    "    print(\"def make_supervised_data_module: result (return)\\n\", data_module) # {'train_dataset': <llava.train.train.LazySupervisedDataset object at 0x7ed6341f4880>, 'eval_dataset': None, 'data_collator': DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))}\n",
    "    return data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TlaRN8vG5p99",
    "outputId": "a52c09b2-84d7-4d0b-b5b1-1f94af461408"
   },
   "outputs": [],
   "source": [
    "data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n",
    "print(\"data_module\\n\", data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    model_max_length=training_args.model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "print(\"tokenizer\\n\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LazySupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path, data_args=data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dict0 = train_dataset.__getitem__(0)\n",
    "sample_dict1 = train_dataset.__getitem__(1)\n",
    "instances = [sample_dict0, sample_dict1]\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "batch = data_collator(instances)\n",
    "print(\"batch\\n\", batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = batch['images']\n",
    "print(\"images shape\\n\", images.shape) # torch.Size([2, 3, 336, 336])\n",
    "print(\"images\\n\", images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def forward(self, images):\n",
    "    print(\"current file path\", \"llava/llava/model/multimodal_encoder/clip_encoder.py\")\n",
    "    print(\"def CLIPVisionTower.forward(self, images)\")\n",
    "    print(\"images shape\\n\", images.shape) # torch.Size([2, 3, 336, 336])\n",
    "    images = images.to(device=self.vision_tower.device, dtype=self.vision_tower.dtype)\n",
    "    vision_outputs = self.vision_tower(images, output_hidden_states=True)\n",
    "    print(\"after process vision_outputs\\n\", type(vision_outputs)) # 24層のtuple\n",
    "    vision_features = vision_outputs.hidden_states[self.select_layer]\n",
    "    if self.select_feature == 'patch':\n",
    "        vision_features = vision_features[:, 1:]\n",
    "    print(\"selected vision_feature shape\\n\", vision_features.shape)\n",
    "    return vision_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIPVisionTower.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlavaLlamaForCausalLM_model = LlavaLlamaForCausalLM.from_pretrained(model_args.model_name_or_path)\n",
    "# LlavaLlamaModel_model = self.get_model()\n",
    "CLIPVisionTower_model = LlavaLlamaModel_model.get_vision_tower()\n",
    "vision_features = CLIPVisionTower_model.forward(images)\n",
    "print(\"vision_features shape\\n\", vision_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vision_features.dtype)\n",
    "LlavaLlamaModel_model.mm_projector = LlavaLlamaModel_model.mm_projector.to(dtype=vision_features.dtype) # 2段目: Projectorの出力\n",
    "projected_vision_features = LlavaLlamaModel_model.mm_projector(vision_features)  # 2段目: Projectorの出力\n",
    "print(\"projected_vision_features shape\\n\", projected_vision_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images(self, images):\n",
    "    print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "    print(\"def LlavaBaseForCausalLM(ABC).encode_images(self, images)\")\n",
    "    LlavaLlamaModel_model = self.get_model()\n",
    "    CLIPVisionTower_model = LlavaLlamaModel_model.get_vision_tower()\n",
    "    vision_features = CLIPVisionTower_model(images)  # 1段目: Vision Encoderの出力\n",
    "    print(\"vision_features shape\\n\", vision_features.shape)\n",
    "    # dtypeを揃える\n",
    "    LlavaLlamaModel_model.mm_projector = LlavaLlamaModel_model.mm_projector.to(dtype=vision_features.dtype)\n",
    "    projected_vision_features = LlavaLlamaModel_model.mm_projector(vision_features)  # 2段目: Projectorの出力\n",
    "    print(\"projected_vision_features (return) shape\\n\", projected_vision_features.shape)\n",
    "    del LlavaLlamaModel_model, CLIPVisionTower_model\n",
    "    return projected_vision_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LlavaBaseForCausalLM.encode_images = encode_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = batch['images']\n",
    "print(\"images shape\\n\", images.shape) # torch.Size([2, 3, 336, 336])\n",
    "projected_vision_features = LlavaLlamaForCausalLM_model.encode_images(images)\n",
    "print(\"projected_vision_features shape\\n\", projected_vision_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# GPU上で管理されていたので、デバイスも合わせます\n",
    "device = training_args.device\n",
    "\n",
    "sample_dict0 = train_dataset.__getitem__(0)\n",
    "sample_dict1 = train_dataset.__getitem__(1)\n",
    "instances = [sample_dict0, sample_dict1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dict0 = train_dataset.__getitem__(0)\n",
    "sample_dict1 = train_dataset.__getitem__(1)\n",
    "instances = [sample_dict0, sample_dict1]\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "batch = data_collator(instances)\n",
    "print(\"batch\\n\", batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = training_args.device\n",
    "input_ids = batch['input_ids'].to(device)\n",
    "labels = batch['labels'].to(device)\n",
    "attention_mask = batch['attention_mask'].to(device)\n",
    "position_ids = None\n",
    "past_key_values = None\n",
    "image_sizes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 確認 ===\n",
    "print(\"input_ids\\n\", input_ids)\n",
    "print(\"labels\\n\", labels)\n",
    "print(\"images shape\\n\", images.shape)\n",
    "print(\"position_ids\\n\", position_ids)\n",
    "print(\"attention_mask\\n\", attention_mask)\n",
    "print(\"past_key_values\\n\", past_key_values)\n",
    "print(\"image_sizes\\n\", image_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_vision_features = LlavaLlamaForCausalLM_model.encode_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# これが True だと、MM-MLPアダプタ以外のパラメータは凍結（勾配計算しない）されます。\n",
    "print(\"training_args.tune_mm_mlp_adapter\\n\", training_args.tune_mm_mlp_adapter)\n",
    "# 画像トークンの前後に特別な開始・終了トークン（<im_start>, <im_end> など）を挿入するかどうかのフラグです。\n",
    "# これが True だと、画像部分を明示的に区切るトークンがプロンプトに追加され、モデルが画像領域をより明確に認識できるようになります。\n",
    "print(\"LlavaLlamaModel_model.config.mm_use_im_start_end\\n\", LlavaLlamaModel_model.config.mm_use_im_start_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Falseではなく、1と0で構成されている場合があるので、bool()を用いてTrueとFalseに変換する\n",
    "attention_mask = attention_mask.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the padding using attention_mask -- FIXME\n",
    "print(\"input_ids before mask\\n\", input_ids)\n",
    "print(\"labels before mask\\n\", labels)\n",
    "input_ids = [cur_input_ids[cur_attention_mask] for cur_input_ids, cur_attention_mask in zip(input_ids, attention_mask)]\n",
    "labels = [cur_labels[cur_attention_mask] for cur_labels, cur_attention_mask in zip(labels, attention_mask)]\n",
    "print(\"input_ids after mask\\n\", input_ids)\n",
    "print(\"labels after mask\\n\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input_embeds = []\n",
    "new_labels = []\n",
    "cur_image_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_input_ids = torch.tensor([0, 1, -200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, cur_input_ids in enumerate(input_ids):\n",
    "    print(\"cur_input_ids shape\\n\", cur_input_ids.shape)   # torch.Size([24])\n",
    "    print(\"cur_input_ids\\n\", cur_input_ids)\n",
    "\n",
    "    cur_image_token_idx = torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)[0].item()\n",
    "    print(\"cur_image_token_idx\\n\", cur_image_token_idx)  # [1]\n",
    "    cur_input_ids_len = cur_input_ids.shape[0]\n",
    "    cur_labels = labels[batch_idx]\n",
    "    print(\"cur_labels\\n\", cur_labels)\n",
    "\n",
    "    # 画像トークンが1つだけの場合に限定\n",
    "    # 1回目: 画像トークンの手前まで\n",
    "    cur_input_ids_noim_0 = cur_input_ids[:cur_image_token_idx]\n",
    "    cur_labels_noim_0 = cur_labels[:cur_image_token_idx]\n",
    "    # 2回目: 画像トークンより後\n",
    "    cur_input_ids_noim_1 = cur_input_ids[cur_image_token_idx+1:]\n",
    "    cur_labels_noim_1 = cur_labels[cur_image_token_idx+1:]\n",
    "    cur_input_ids_noim = [cur_input_ids_noim_0, cur_input_ids_noim_1]\n",
    "    cur_labels_noim = [cur_labels_noim_0, cur_labels_noim_1]\n",
    "\n",
    "    print(\"cur_input_ids_noim\\n\", cur_input_ids_noim)\n",
    "    print(\"cur_labels_noim\\n\", cur_labels_noim)\n",
    "\n",
    "    split_sizes = [cur_labels_noim_0.shape[0], cur_labels_noim_1.shape[0]]\n",
    "    print(\"split_sizes\\n\", split_sizes)  # [1, 22]\n",
    "    LlavaLlamaModel_model = LlavaLlamaForCausalLM_model.get_model()\n",
    "    cur_input_embeds = LlavaLlamaModel_model.embed_tokens(torch.cat(cur_input_ids_noim))\n",
    "    del LlavaLlamaModel_model\n",
    "    print(\"cur_input_embeds shape\\n\", cur_input_embeds.shape)  # torch.Size([23, 2048])\n",
    "    print(\"cur_input_embeds\\n\", cur_input_embeds)\n",
    "\n",
    "    cur_input_embeds_no_im = torch.split(cur_input_embeds, split_sizes, dim=0)\n",
    "    print(\"cur_input_embeds_no_im\\n\", cur_input_embeds_no_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for batch_idx, cur_input_ids in enumerate(input_ids):\n",
    "    print(\"cur_input_ids shape\\n\", cur_input_ids.shape)   # torch.Size([24])\n",
    "    print(\"cur_input_ids\\n\", cur_input_ids)\n",
    "\n",
    "    cur_image_token_idx = torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)[0]\n",
    "    print(\"cur_image_token_idx\\n\", cur_image_token_idx)  # [1]\n",
    "    cur_input_ids_len = cur_input_ids.shape[0]\n",
    "\n",
    "    cur_labels = labels[batch_idx]\n",
    "    print(\"cur_labels\\n\", cur_labels)\n",
    "\n",
    "    # 画像トークンが1つだけの場合に限定\n",
    "    if len(cur_image_token_idx) == 1:\n",
    "        idx = cur_image_token_idx.item()\n",
    "        # 1回目: 画像トークンの手前まで\n",
    "        cur_input_ids_noim_0 = cur_input_ids[:idx]\n",
    "        cur_labels_noim_0 = cur_labels[:idx]\n",
    "        # 2回目: 画像トークンより後\n",
    "        cur_input_ids_noim_1 = cur_input_ids[idx+1:]\n",
    "        cur_labels_noim_1 = cur_labels[idx+1:]\n",
    "        cur_input_ids_noim = [cur_input_ids_noim_0, cur_input_ids_noim_1]\n",
    "        cur_labels_noim = [cur_labels_noim_0, cur_labels_noim_1]\n",
    "    else:\n",
    "        # 画像トークンが0個 or 複数の場合は従来通り\n",
    "        cur_input_ids_noim = []\n",
    "        cur_labels_noim = []\n",
    "        prev_idx = -1\n",
    "        image_token_indices = [-1] + cur_image_token_idx.tolist() + [cur_input_ids_len]\n",
    "        for i in range(len(image_token_indices) - 1):\n",
    "            cur_input_ids_noim.append(cur_input_ids[image_token_indices[i]+1:image_token_indices[i+1]])\n",
    "            cur_labels_noim.append(cur_labels[image_token_indices[i]+1:image_token_indices[i+1]])\n",
    "\n",
    "    print(\"cur_input_ids_noim\\n\", cur_input_ids_noim)\n",
    "    print(\"cur_labels_noim\\n\", cur_labels_noim)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs_labels_for_multimodal(\n",
    "    self, input_ids, position_ids, attention_mask, past_key_values, labels,\n",
    "    images, image_sizes=None\n",
    "):\n",
    "    print(\"current file path\", \"llava/model/llava_arch.py\")\n",
    "    print(\"def LlavaMetaForCausalLM(ABC).prepare_inputs_labels_for_multimodal(self, input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes=None)\")  # not found\n",
    "    print(\"input_ids\\n\", input_ids)\n",
    "    print(\"position_ids\\n\", position_ids)  # None\n",
    "    print(\"attention_mask\\n\", attention_mask)\n",
    "    print(\"past_key_values\\n\", past_key_values)  # None\n",
    "    print(\"labels\\n\", labels)\n",
    "    print(\"image_sizes\\n\", image_sizes)  # None\n",
    "\n",
    "    projected_vision_features = self.encode_images(images)\n",
    "    print(\"projected_vision_features after encode_images shape \\n\", projected_vision_features.shape)  # torch.Size([1, 576, 2048])\n",
    "        \n",
    "    attention_mask_original_dtype = attention_mask.dtype\n",
    "    # bool型に変換する\n",
    "    attention_mask = attention_mask.bool()\n",
    "        \n",
    "    # remove the padding using attention_mask \n",
    "    input_ids = [cur_input_ids[cur_attention_mask] for cur_input_ids, cur_attention_mask in zip(input_ids, attention_mask)]\n",
    "    labels = [cur_labels[cur_attention_mask] for cur_labels, cur_attention_mask in zip(labels, attention_mask)]\n",
    "\n",
    "    new_input_embeds = []\n",
    "    new_labels = []\n",
    "    cur_image_idx = 0\n",
    "\n",
    "    for batch_idx, cur_input_ids in enumerate(input_ids):\n",
    "        print(\"cur_input_ids shape\\n\", cur_input_ids.shape)   # torch.Size([24])\n",
    "        print(\"cur_input_ids\\n\", cur_input_ids)\n",
    "\n",
    "        cur_image_token_idx = torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)[0].item()\n",
    "        print(\"cur_image_token_idx\\n\", cur_image_token_idx)  # [1]\n",
    "        cur_labels = labels[batch_idx]\n",
    "        print(\"cur_labels\\n\", cur_labels)\n",
    "\n",
    "        # 画像トークンが1つだけの場合に限定\n",
    "        # 1回目: 画像トークンの手前まで\n",
    "        cur_input_ids_noim_0 = cur_input_ids[:cur_image_token_idx]\n",
    "        cur_labels_noim_0 = cur_labels[:cur_image_token_idx]\n",
    "        # 2回目: 画像トークンより後\n",
    "        cur_input_ids_noim_1 = cur_input_ids[cur_image_token_idx+1:]\n",
    "        cur_labels_noim_1 = cur_labels[cur_image_token_idx+1:]\n",
    "        cur_input_ids_noim = [cur_input_ids_noim_0, cur_input_ids_noim_1]\n",
    "        cur_labels_noim = [cur_labels_noim_0, cur_labels_noim_1]\n",
    "\n",
    "        print(\"cur_input_ids_noim\\n\", cur_input_ids_noim)\n",
    "        print(\"cur_labels_noim\\n\", cur_labels_noim)\n",
    "        \n",
    "        split_sizes = [x.shape[0] for x in cur_labels_noim]\n",
    "        print(\"split_sizes\\n\", split_sizes)  # [1, 22]\n",
    "        cur_input_embeds = self.get_model().embed_tokens(torch.cat(cur_input_ids_noim))\n",
    "        print(\"cur_input_embeds shape\\n\", cur_input_embeds.shape)  # torch.Size([23, 2048])\n",
    "        print(\"cur_input_embeds\\n\", cur_input_embeds)\n",
    "        cur_input_embeds_noim = torch.split(cur_input_embeds, split_sizes, dim=0)\n",
    "        print(\"cur_input_embeds_no_im\\n\", cur_input_embeds_noim) # (torch.size[1, 2048], torch.size[22, 2048])\n",
    "\n",
    "\n",
    "        cur_new_input_embeds = []\n",
    "        cur_new_labels = []\n",
    "\n",
    "        cur_new_input_embeds.append(cur_input_embeds_noim[0])\n",
    "        cur_new_labels.append(cur_labels_noim[0])\n",
    "\n",
    "        cur_image_features = projected_vision_features[cur_image_idx]\n",
    "        cur_new_input_embeds.append(cur_image_features)\n",
    "        # cur_image_features.shape [576, 2048] より、 トークン長さ 576 が IGNORE_INDEX　でマスクされたlabelsを作成する。\n",
    "        cur_new_labels.append(torch.full((cur_image_features.shape[0],), IGNORE_INDEX, device=cur_labels.device, dtype=cur_labels.dtype))\n",
    "        cur_image_idx += 1\n",
    "\n",
    "        cur_new_input_embeds.append(cur_input_embeds_noim[1])\n",
    "        cur_new_labels.append(cur_labels_noim[1])\n",
    "\n",
    "        # cur_new_input_embeds\n",
    "        # [torch.Size([1, 2048]), torch.Size([576, 2048]), torch.Size([12, 2048])]\n",
    "        # cur_new_labels\n",
    "        # [torch.Size([1]), torch.Size([576]), torch.Size([12])]\n",
    " \n",
    "        cur_new_input_embeds = [x.to(self.device) for x in cur_new_input_embeds]\n",
    "        cur_new_input_embeds = torch.cat(cur_new_input_embeds)\n",
    "        cur_new_labels = [x.to(self.device) for x in cur_new_labels]\n",
    "        cur_new_labels = torch.cat(cur_new_labels)\n",
    "\n",
    "        print(\"cur_new_input_embeds (after cat) shape\\n\", cur_new_input_embeds.shape)  # torch.Size([589, 2048])\n",
    "        print(\"cur_new_labels (after cat) shape\\n\", cur_new_labels.shape)  # torch.Size([589])\n",
    "\n",
    "        new_input_embeds.append(cur_new_input_embeds) # [torch.Size([589, 2048])]\n",
    "        new_labels.append(cur_new_labels) # [torch.Size([589])]\n",
    "    \n",
    "    print(\"new_input_embeds for samples in batch shape\\n\", [x.shape for x in new_input_embeds])  \n",
    "    print(\"new_labels for samples in batch shape\\n\", [x.shape for x in new_labels])  \n",
    "    \n",
    "    # Truncate sequences to max length as image embeddings can make the sequence longer\n",
    "    tokenizer_model_max_length = getattr(self.config, 'tokenizer_model_max_length', None)\n",
    "    if tokenizer_model_max_length is not None:\n",
    "        new_input_embeds = [x[:tokenizer_model_max_length] for x in new_input_embeds]\n",
    "        new_labels = [x[:tokenizer_model_max_length] for x in new_labels]\n",
    "\n",
    "\n",
    "    # The length of the longest sample in a batch\n",
    "    max_len = max(x.shape[0] for x in new_input_embeds)\n",
    "    batch_size = len(new_input_embeds)\n",
    "    new_input_embeds_padded = []\n",
    "    new_labels_padded = torch.full((batch_size, max_len), IGNORE_INDEX, dtype=new_labels[0].dtype, device=new_labels[0].device)\n",
    "    print(\"new_labels_padded (before) shape\\n\", new_labels_padded.shape)\n",
    "    attention_mask = torch.zeros((batch_size, max_len), dtype=attention_mask.dtype, device=attention_mask.device)\n",
    "    print(\"attention_mask (before) shape\\n\", attention_mask.shape)  \n",
    "    \n",
    "    for i, (cur_new_embed, cur_new_labels) in enumerate(zip(new_input_embeds, new_labels)):\n",
    "        cur_len = cur_new_embed.shape[0]\n",
    "        # バッチ内で一番長いサンプルの長さ max_len に合わせるため、足りない分だけゼロ埋め（torch.zeros(...)）を右側に追加します。\n",
    "        # torch.cat((...), dim=0) で、元の埋め込みとゼロ埋めを縦方向（トークン方向）に連結します。\n",
    "        # こうすることで、全サンプルの埋め込みベクトルが [max_len, hidden_dim] の同じ形になり、torch.stack でバッチ化できます。\n",
    "        new_input_embeds_padded.append(torch.cat((\n",
    "            cur_new_embed,\n",
    "            torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device)\n",
    "        ), dim=0))\n",
    "\n",
    "        if cur_len > 0:\n",
    "            # padding_side は right なので、[0:cur_len] に代入して、右側を残す\n",
    "            new_labels_padded[i, :cur_len] = cur_new_labels \n",
    "            attention_mask[i, :cur_len] = True\n",
    "        \n",
    "        print(\"new_input_embeds_padded (so far) shape\\n\", [x.shape for x in new_input_embeds_padded])  # [torch.Size([599, 2048])]\n",
    "        print(\"new_labels_padded (so far) shape\\n\", new_labels_padded.shape)  # torch.Size([1, 599])                    \n",
    "        print(\"attention_mask (so far) shape\\n\", attention_mask.shape)  # torch.Size([1, 599])        \n",
    "\n",
    "    new_input_embeds = torch.stack(new_input_embeds_padded, dim=0)\n",
    "    print(\"new_input_embeds (after) shape\\n\", new_input_embeds.shape)  # torch.Size([1, 599, 2048])\n",
    "    new_labels = new_labels_padded\n",
    "    # 元の型に戻す\n",
    "    attention_mask = attention_mask.to(dtype=attention_mask_original_dtype)\n",
    "    print(\"past_key_values (return)\\n\", past_key_values)  # None\n",
    " \n",
    "    return None, position_ids, attention_mask, past_key_values, new_input_embeds, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LlavaBaseForCausalLM.prepare_inputs_labels_for_multimodal = prepare_inputs_labels_for_multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = training_args.device\n",
    "input_ids = batch['input_ids'].to(device)\n",
    "labels = batch['labels'].to(device)\n",
    "attention_mask = batch['attention_mask'].to(device)\n",
    "position_ids = None\n",
    "past_key_values = None\n",
    "image_sizes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    input_ids,\n",
    "    position_ids,\n",
    "    attention_mask,\n",
    "    past_key_values,\n",
    "    inputs_embeds,\n",
    "    labels\n",
    ") = LlavaLlamaForCausalLM_model.prepare_inputs_labels_for_multimodal(\n",
    "    input_ids,\n",
    "    position_ids,\n",
    "    attention_mask,\n",
    "    past_key_values,\n",
    "    labels,\n",
    "    images,\n",
    "    image_sizes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(\n",
    "    self,\n",
    "    input_ids: torch.LongTensor = None,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    labels: Optional[torch.LongTensor] = None,\n",
    "    use_cache: Optional[bool] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    images: Optional[torch.FloatTensor] = None,\n",
    "    image_sizes: Optional[List[List[int]]] = None,\n",
    "    return_dict: Optional[bool] = None,\n",
    ") -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "\n",
    "    print(\"current file path\", \"llava/llava/model/language_model/llava_llama.py\")\n",
    "    print(\"def LlavaLlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, image_sizes, return_dict)\")\n",
    "    print(\"input_ids.shape\\n\", input_ids.shape)\n",
    "    print(\"attention_mask.shape\\n\", attention_mask.shape)\n",
    "    print(\"position_ids\\n\", position_ids) # None\n",
    "    print(\"past_key_values\\n\", past_key_values) # None\n",
    "    print(\"inputs_embeds\\n\", inputs_embeds) # None\n",
    "    print(\"labels\\n\", labels)\n",
    "    print(\"use_cache\\n\", use_cache) # None\n",
    "    print(\"output_attentions\\n\", output_attentions) # None\n",
    "    print(\"output_hidden_states\\n\", output_hidden_states) # None\n",
    "    print(\"images shape\\n\", images.shape)\n",
    "    print(\"image_sizes\\n\", image_sizes) # None\n",
    "    print(\"return_dict\\n\", return_dict) # None\n",
    "\n",
    "    print(f\"【COND】 inputs_embeds_is_None={inputs_embeds is None}\") # True\n",
    "    if inputs_embeds is None:\n",
    "        # 【ENTER】\n",
    "        (\n",
    "            input_ids,\n",
    "            position_ids,\n",
    "            attention_mask,\n",
    "            past_key_values,\n",
    "            inputs_embeds,\n",
    "            labels\n",
    "        ) = self.prepare_inputs_labels_for_multimodal(\n",
    "            input_ids,\n",
    "            position_ids,\n",
    "            attention_mask,\n",
    "            past_key_values,\n",
    "            labels,\n",
    "            images,\n",
    "            image_sizes\n",
    "        )       \n",
    "\n",
    "    #  LlamaForCausalLM.forward(self, ...)で明示\n",
    "    # Trainer > def train > def inner_training_loop > def training_step > model(**inputs) > model.forward\n",
    "    result = LlamaForCausalLM.forward(\n",
    "        self,\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_values=past_key_values,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        labels=labels,\n",
    "        use_cache=use_cache,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict\n",
    "    )\n",
    "    print(\"Return of def LlavaLlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, image_sizes, return_dict)\")\n",
    "    #print(\"result of LlavaLlamaForCausalLM.forward (return)\\n\", result)\n",
    "    print(\"logits tensor shape  LlavaLlamaForCausalLM.forward\\n\", result.logits.shape) # torch.Size([1, 599, 32000])\n",
    "    print(\"loss (return)  LlavaLlamaForCausalLM.forward \\n\", result.loss) # tensor(9.2224, grad_fn=<NllLossBackward0>)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LlavaLlamaForCausalLM.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = training_args.device\n",
    "input_ids = batch['input_ids'].to(device)\n",
    "labels = batch['labels'].to(device)\n",
    "attention_mask = batch['attention_mask'].to(device)\n",
    "position_ids = None\n",
    "past_key_values = None\n",
    "image_sizes = None\n",
    "\n",
    "images = batch['images'].to(device)\n",
    "\n",
    "inputs = {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"attention_mask\": attention_mask,\n",
    "    \"labels\": labels,\n",
    "    \"images\": images,\n",
    "    \"position_ids\": None,\n",
    "    \"past_key_values\": None,\n",
    "    \"image_sizes\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = LlavaLlamaForCausalLM_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LlavaLlamaForCausalLM_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name, param_tensor in LlavaLlamaForCausalLM_model.named_parameters():\n",
    "    print(param_name, param_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# この関数 maybe_zero_3 は、DeepSpeedのZero Redundancy Optimizer（ZeRO）を使った分散学習時に、\n",
    "# パラメータ（param）を安全にCPUにコピーして保存できる形に変換するためのものです。\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "def maybe_zero_3(param, ignore_status=False, name=None):\n",
    "\n",
    "    print(\"current file path\", \"llava/train/llava_trainer.py\")\n",
    "    print(\"def maybe_zero_3(param, ignore_status=False, name=None)\")\n",
    "    print(\"param maybe_zero_3\\n\", param)\n",
    "    print(\"ignore_status maybe_zero_3\\n\", ignore_status)\n",
    "    print(\"name maybe_zero_3\\n\", name)\n",
    "    from deepspeed import zero\n",
    "    from deepspeed.runtime.zero.partition_parameters import ZeroParamStatus\n",
    "    # パラメータがZeRO管理下である場合\n",
    "    if hasattr(param, \"ds_id\"):\n",
    "        # パラメータが「利用可能でない」状態\n",
    "        if param.ds_status == ZeroParamStatus.NOT_AVAILABLE:\n",
    "            # 警告を出す。ただし、パラメータが「利用可能でない」状態を無視する場合は警告しない。\n",
    "            if not ignore_status:\n",
    "                logging.warning(f\"{name}: param.ds_status != ZeroParamStatus.NOT_AVAILABLE: {param.ds_status}\")\n",
    "        # 分散されているパラメータを集約。\n",
    "        with zero.GatheredParameters([param]):\n",
    "            # CPU上にコピー。\n",
    "            param = param.data.detach().cpu().clone()\n",
    "            logging.info(f\"{name}: param (after GatheredParameters)\\n{param}\")\n",
    "    else:\n",
    "        # パラメータがZeRO管理下ではない場合\n",
    "        # CPU上にコピー。\n",
    "        param = param.detach().cpu().clone()\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mm_adapter_state_maybe_zero_3(named_params, param_keywords):\n",
    "    print(\"current file path\", \"llava/train/llava_trainer.py\")\n",
    "    print(\"def get_mm_adapter_state_maybe_zero_3(named_params, keys_to_match)\")\n",
    "    print(\"named_params get_mm_adapter_state_maybe_zero_3\\n\", named_params)\n",
    "    print(\"keys_to_match get_mm_adapter_state_maybe_zero_3\\n\", param_keywords)\n",
    "    # 1. 指定したキーに一致するパラメータだけ抽出（初心者向けに分かりやすく）\n",
    "    matched_params = {}\n",
    "    for param_key, param_tensor in named_params:\n",
    "        for param_keyword in param_keywords:\n",
    "            if param_keyword in param_key:\n",
    "                matched_params[param_key] = param_tensor\n",
    "                break  # 一致したら次のkeyへ\n",
    "\n",
    "    # 2. DeepSpeed分散対応で安全にCPUにコピー（初心者向けにforループで記述）\n",
    "    adapter_state_dict = {}\n",
    "    for param_key, param_tensor in matched_params.items():\n",
    "        param_tensor_to_save = maybe_zero_3(param_tensor, ignore_status=True, name=param_key).cpu()\n",
    "        adapter_state_dict[param_key] = param_tensor_to_save\n",
    "    return adapter_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_keywords = ['mm_projector']\n",
    "model_params = LlavaLlamaForCausalLM_model.named_parameters()\n",
    "weight_to_save = get_mm_adapter_state_maybe_zero_3(model_params, param_keywords)\n",
    "print(\"weight_to_save\\n\", weight_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = training_args.output_dir\n",
    "print(\"output_dir\", output_dir)\n",
    "LlavaLlamaForCausalLM_model.config.save_pretrained(output_dir)\n",
    "torch.save(weight_to_save, os.path.join(output_dir, f'mm_projector.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./checkpoints\n",
    "!ls ./checkpoints/Tinyllava-1.1B-pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "class LLaVATrainer(Trainer):\n",
    "    # Trainer > _inner_training_loop > _maybe_log_save_evaluate > self._save_checkpoint(model, trial)\n",
    "    def _save_checkpoint(self, model, trial, metrics=None):\n",
    "\n",
    "        print(\"current file path\", \"llava/train/llava_trainer.py\")\n",
    "        print(\"def _save_checkpoint(self, model, trial, metrics=None)\")\n",
    "        print(\"self _save_checkpoint\\n\", self) # <llava.train.llava_trainer.LLaVATrainer object at 0x7ed6341f4490>\n",
    "        print(\"model _save_checkpoint\\n\", model)\n",
    "        print(\"trial _save_checkpoint\\n\", trial) # None\n",
    "        print(\"metrics _save_checkpoint\\n\", metrics) # None\n",
    "        print(f\"【COND】tune_mm_mlp_adapter={getattr(self.args, 'tune_mm_mlp_adapter', False)}\") # True\n",
    "        if getattr(self.args, 'tune_mm_mlp_adapter', False):\n",
    "            from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR # chcekpoint\n",
    "            checkpoint_folder = f\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\"\n",
    "            run_dir = self._get_output_dir(trial=trial)\n",
    "            output_dir = os.path.join(run_dir, checkpoint_folder)\n",
    "\n",
    "            # Only save Adapter\n",
    "            param_keywords = ['mm_projector'] # 'vision_resampler'\n",
    "            weight_to_save = get_mm_adapter_state_maybe_zero_3(self.model.named_parameters(), param_keywords)\n",
    "\n",
    "            if self.args.local_rank == 0 or self.args.local_rank == -1:\n",
    "                # モデルconfigファイルを保存\n",
    "                self.model.config.save_pretrained(output_dir)\n",
    "                # weight_to_save を mm_projector.binとして保存\n",
    "                torch.save(weight_to_save, os.path.join(output_dir, f'mm_projector.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = LLaVATrainer(model=LlavaLlamaForCausalLM_model, \n",
    "                       tokenizer=tokenizer,\n",
    "                       args=training_args,\n",
    "                       **data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer._save_checkpoint(model=LlavaLlamaForCausalLM_model, trial=None, metrics=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./checkpoints\n",
    "!ls ./checkpoints/Tinyllava-1.1B-pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer,\n",
    "                                   output_dir: str):\n",
    "\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str)\")\n",
    "    print(\"trainer safe_save_model_for_hf_trainer\\n\", type(trainer)) # <class 'llava.train.llava_trainer.LLaVATrainer'>\n",
    "    print(\"output_dir safe_save_model_for_hf_trainer\\n\", output_dir) # ./checkpoints/Tinyllava-1.1B-pretrain\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "\n",
    "    # A\n",
    "    # tune_mm_mlp_adapter=True の場合、mm_projector だけを抽出・保存します（Adapterのみ）。\n",
    "    # mm_projector だけだから、deepspeedかどうかは関係ない\n",
    "    if getattr(trainer.args, \"tune_mm_mlp_adapter\", False): # True\n",
    "        # Only save Adapter\n",
    "        param_keywords = ['mm_projector']     \n",
    "        model_params = trainer.model.named_parameters()\n",
    "        weight_to_save = get_mm_adapter_state_maybe_zero_3(model_params, param_keywords)\n",
    "        trainer.model.config.save_pretrained(output_dir)\n",
    "\n",
    "        current_folder = output_dir.split('/')[-1] # Tinyllava-1.1B-pretrain\n",
    "        parent_folder = os.path.dirname(output_dir) # ./checkpoints\n",
    "        if trainer.args.local_rank == 0 or trainer.args.local_rank == -1:\n",
    "            if current_folder.startswith('checkpoint-'): \n",
    "                # SKIP\n",
    "                mm_projector_folder = os.path.join(parent_folder, \"mm_projector\")\n",
    "                os.makedirs(mm_projector_folder, exist_ok=True)\n",
    "                torch.save(weight_to_save, os.path.join(mm_projector_folder, f'{current_folder}.bin'))\n",
    "            else:\n",
    "                # ENTER\n",
    "                torch.save(weight_to_save, os.path.join(output_dir, f'mm_projector.bin'))\n",
    "        return\n",
    "\n",
    "    # B\n",
    "    # tune_mm_mlp_adapter=False の場合、全モデル保存\n",
    "\n",
    "    # B1\n",
    "    # DeepSpeedが有効なら torch.cuda.synchronize() → trainer.save_model(output_dir) で全モデル保存。\n",
    "    # DeepSpeed（分散学習）を使っている場合は、CUDAの同期を取ってから HuggingFace Trainer の標準の save_model を呼びます。\n",
    "    if trainer.deepspeed:\n",
    "        torch.cuda.synchronize()\n",
    "        trainer.save_model(output_dir)\n",
    "        return\n",
    "\n",
    "    # B2\n",
    "    # should_save は HuggingFace Transformers の TrainingArguments で使われる内部フラグです。\n",
    "    # 分散学習やマルチGPU環境で「このプロセスがモデル保存を担当するかどうか」を判定します。\n",
    "    # 通常は local_rank == 0（メインプロセス）だけが保存します。\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {\n",
    "            key: value.cpu() for key, value in state_dict.items()\n",
    "        }\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgAHEvYzuB0U"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "def train():\n",
    "    print(\"current file path\", \"llava/train/train.py\")\n",
    "    print(\"def train()\")\n",
    "    global local_rank\n",
    "\n",
    "    parser = transformers.HfArgumentParser(\n",
    "        (ModelArguments, DataArguments, TrainingArguments)\n",
    "        )    \n",
    "    model_args, data_args, training_args = parser.parse_dict(args_dict)\n",
    "    local_rank = training_args.local_rank\n",
    "    compute_dtype = (torch.float16 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n",
    "    print(\"compute_dtype\\n\", compute_dtype)\n",
    "\n",
    "    print(f\"【COND】 vision_tower={model_args.vision_tower}\")\n",
    "    # 【ENTER】 vision_tower=openai/clip-vit-large-patch14-336 なので、この分岐に入る\n",
    "    if model_args.vision_tower is not None:\n",
    "        # PreTrainedModel.from_pretrained\n",
    "        model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            cache_dir=training_args.cache_dir\n",
    "        )\n",
    "\n",
    "    # 【ENTER】 gradient_checkpointing=True なので、この分岐に入る\n",
    "    if training_args.gradient_checkpointing:\n",
    "        # 【ENTER】 model に enable_input_require_grads メソッドがあるので、この分岐に入る\n",
    "        if hasattr(model, \"enable_input_require_grads\"):\n",
    "            # PreTrainedModel.enable_input_require_grads\n",
    "            # 元々 全ての重みについて True\n",
    "            model.enable_input_require_grads()\n",
    "\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "    )\n",
    "    \n",
    "    print(\"tokenizer defined by AutoTokenizer.from_pretrained \\n\", tokenizer)\n",
    "    # 【ENTER】 version=plain なので、この分岐に入る\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "    \n",
    "    # preprocess_plain で hard coding しているので省略\n",
    "    #if model_args.version in conv_templates:\n",
    "        #default_conversation = conv_templates[model_args.version]\n",
    "\n",
    "    # 【ENTER】 vision_tower=openai/clip-vit-large-patch14-336 なので、この分岐に入る\n",
    "    if model_args.vision_tower is not None:\n",
    "        model.get_model().initialize_vision_modules(\n",
    "            model_args=model_args,\n",
    "            fsdp=training_args.fsdp\n",
    "        )\n",
    "\n",
    "        vision_tower = model.get_vision_tower()\n",
    "        vision_tower.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)\n",
    "\n",
    "        data_args.image_processor = vision_tower.image_processor\n",
    "        data_args.is_multimodal = True\n",
    "        model.config.image_aspect_ratio = data_args.image_aspect_ratio\n",
    "        model.config.tokenizer_padding_side = tokenizer.padding_side\n",
    "        model.config.tokenizer_model_max_length = tokenizer.model_max_length\n",
    "        model.config.tune_mm_mlp_adapter = training_args.tune_mm_mlp_adapter = model_args.tune_mm_mlp_adapter\n",
    "\n",
    "        if model_args.tune_mm_mlp_adapter: # True\n",
    "            # 【ENTER】 tune_mm_mlp_adapter=True なので、この分岐に入る\n",
    "            # モデル全体の全パラメータを「学習不可（requires_grad=False）」にする\n",
    "            # これで通常の重みは全て凍結される\n",
    "            model.requires_grad_(False)\n",
    "            for param in model.get_model().mm_projector.parameters():\n",
    "                # mm_projector（画像特徴量→テキスト特徴量への変換層）の全パラメータだけを「学習可能（requires_grad=True）」に戻す\n",
    "                # これで mm_projector のみ学習されることになる\n",
    "                param.requires_grad = True\n",
    "\n",
    "        model.config.freeze_mm_mlp_adapter = training_args.freeze_mm_mlp_adapter\n",
    "        model.config.mm_use_im_start_end = data_args.mm_use_im_start_end = model_args.mm_use_im_start_end\n",
    "        model.config.mm_projector_lr = training_args.mm_projector_lr\n",
    "        training_args.use_im_start_end = model_args.mm_use_im_start_end\n",
    "        model.config.mm_use_im_patch_token = model_args.mm_use_im_patch_token\n",
    "    \n",
    "    # make_supervised_data_module は LazySupervisedDataset.__getitem__ より先に呼ばれますが、\n",
    "    # make_supervised_data_module を理解するためには\n",
    "    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n",
    "    print(\"data_module\\n\", data_module) # {'train_dataset': <llava.train.train.LazySupervisedDataset object at 0x7ed6341f4880>, 'eval_dataset': None, 'data_collator': DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))}\n",
    "\n",
    "    trainer = LLaVATrainer(model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    args=training_args,\n",
    "                    **data_module)\n",
    "    \n",
    "    if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n",
    "        trainer.train(resume_from_checkpoint=False)\n",
    "    else:\n",
    "        trainer.train()\n",
    "    trainer.save_state()\n",
    "\n",
    "    model.config.use_cache = True\n",
    "    print(\"model.config.use_cache = True\", model.config.use_cache) # True        \n",
    "    safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Jupyter (llava)",
   "language": "python",
   "name": "llava"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
